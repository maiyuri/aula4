[
    {
        "issnkey": "",
        "isbn": "9781450390668",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "amethodologyexplorationtomotivateteacherstoplacemathematicsatthecenterofatransdisciplinaryexperienceusingvideogamesandbigdatacombinedwiththe21stcenturyskills",
        "booktitle": "Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)",
        "doi": "10.1145/3486011.3486555",
        "author": [
            "Rocuts, Schweitzer",
            "Alier, Marc"
        ],
        "keywords": [
            "Connected mathematics, 21st Century Skills, creativity and collaborative work in education, transdisciplinary"
        ],
        "abstract": "Human knowledge is highly connected and under continuous and collaborative evolution, where all of us can co-create and contribute. But the execution of our standardized educational systems mostly offers a standardized experience and teaching practices that do not reflect this point. This is particularly acute in teachers and students with mathematics. In this research, I will review some strategies to help teachers to effectively use connected mathematics with the real and daily world, promoting transdisciplinary work with an emphasis on creativity and collaboration, and using the Game-Based Assessment methodology combined with the unique environment surrounding each teacher.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03600300",
        "isbn": null,
        "journal": "ACM Comput. Surv.",
        "publisher": "Association for Computing Machinery",
        "title": "edgelearningtheenablingtechnologyfordistributedbigdataanalyticsintheedge",
        "booktitle": null,
        "doi": "10.1145/3464419",
        "author": [
            "Zhang, Jie",
            "Qu, Zhihao",
            "Chen, Chenxi",
            "Wang, Haozhao",
            "Zhan, Yufeng",
            "Ye, Baoliu",
            "Guo, Song"
        ],
        "keywords": [
            "edge computing, federated learning, Edge learning, security and privacy, machine learning"
        ],
        "abstract": "Machine Learning (ML) has demonstrated great promise in various fields, e.g., self-driving, smart city, which are fundamentally altering the way individuals and organizations live, work, and interact. Traditional centralized learning frameworks require uploading all training data from different sources to a remote data server, which incurs significant communication overhead, service latency, and privacy issues.To further extend the frontiers of the learning paradigm, a new learning concept, namely, Edge Learning (EL) is emerging. It is complementary to the cloud-based methods for big data analytics by enabling distributed edge nodes to cooperatively training models and conduct inferences with their locally cached data. To explore the new characteristics and potential prospects of EL, we conduct a comprehensive survey of the recent research efforts on EL. Specifically, we first introduce the background and motivation. We then discuss the challenging issues in EL from the aspects of data, computation, and communication. Furthermore, we provide an overview of the enabling technologies for EL, including model training, inference, security guarantee, privacy protection, and incentive mechanism. Finally, we discuss future research opportunities on EL. We believe that this survey will provide a comprehensive overview of EL and stimulate fruitful future research in this field.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.282",
        "scimago_value": "2,079"
    },
    {
        "issnkey": "",
        "isbn": "9781450375061",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "dataanalyticsprojectmethodologieswhichonetochoose",
        "booktitle": "Proceedings of the 2020 International Conference on Big Data in Management",
        "doi": "10.1145/3437075.3437087",
        "author": [
            "Baijens, Jeroen",
            "Helms, Remko",
            "Kusters, Rob"
        ],
        "keywords": [
            "Project Methodologies, Project characteristics, Data Analytics"
        ],
        "abstract": "Developments in big data have led to an increase in data analytics projects conducted by organizations. Such projects aim to create value by improving decision making or enhancing business processes. However, many data analytics projects still fail to deliver the expected value. The use of process models or methodologies is recommended to increase the success rate of these projects. Nevertheless, organizations are hardly using them because they are considered too rigid and hard to implement. The existing methodologies often do not fit the specific project characteristics. Therefore, this research suggests grouping different project characteristics to identify the most appropriate project methodology for a specific type of project. More specifically, this research provides a structured description that helps to determine what type of project methodology works for different types of data analytics projects. The results of six different case studies show that continuous projects would benefit from an iterative methodology.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "19361955",
        "isbn": null,
        "journal": "J. Data and Information Quality",
        "publisher": "Association for Computing Machinery",
        "title": "ensuringdatareadinessforqualityrequirementswithhelpfromprocedurereuse",
        "booktitle": null,
        "doi": "10.1145/3428154",
        "author": [
            "Chirkova, Rada",
            "Doyle, Jon",
            "Reutter, Juan"
        ],
        "keywords": [
            "frameworks, Data and information quality, Big Data quality management processes, data cleaning in Big Data, and models, data integration in Big Data, Big Data quality in business process, Big Data quality and analytics"
        ],
        "abstract": "Assessing and improving the quality of data are fundamental challenges in Big-Data applications. These challenges have given rise to numerous solutions targeting transformation, integration, and cleaning of data. However, while schema design, data cleaning, and data migration are nowadays reasonably well understood in isolation, not much attention has been given to the interplay between standalone tools in these areas. In this article, we focus on the problem of determining whether the available data-transforming procedures can be used together to bring about the desired quality characteristics of the data in business or analytics processes. For example, to help an organization avoid building a data-quality solution from scratch when facing a new analytics task, we ask whether the data quality can be improved by reusing the tools that are already available, and if so, which tools to apply, and in which order, all without presuming knowledge of the internals of the tools, which may be external or proprietary.Toward addressing this problem, we conduct a formal study in which individual data cleaning, data migration, or other data-transforming tools are abstracted as black-box procedures with only some of the properties exposed, such as their applicability requirements, the parts of the data that the procedure modifies, and the conditions that the data satisfy once the procedure has been applied. As a proof of concept, we provide foundational results on sequential applications of procedures abstracted in this way, to achieve prespecified data-quality objectives, for the use case of relational data and for procedures described by standard relational constraints. We show that, while reasoning in this framework may be computationally infeasible in general, there exist well-behaved cases in which these foundational results can be applied in practice for achieving desired data-quality results on Big Data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389426",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "conductingmaliciouscybersecurityexperimentsoncrowdsourcingplatforms",
        "booktitle": "The 2021 3rd International Conference on Big Data Engineering",
        "doi": "10.1145/3468920.3468942",
        "author": [
            "Aljohani, Asmaa",
            "Jones, James"
        ],
        "keywords": [
            "Participants, Online experiments, Recruitment, Crowdsourcing, Cybersecurity"
        ],
        "abstract": "Evaluating the effectiveness of defense technologies mandates the inclusion of a human element, specifically if these technologies target human cognition and emotions. One of the biggest challenges that face researchers in the realm of behavioral cybersecurity is participant recruitment. Researchers often rely on college students, the general public, real-world hackers, or a hard-to-reach population (e.g., professional red teamers) to test the effectiveness of cybersecurity defense techniques. However, recruiting participants from these populations has drawbacks, including but not limited to: high cost, time constraints, and manageability and accessibility issues. This research explored the applicability of using two popular crowdsourcing platforms, Amazon Mechanical Turk and Prolific, to conduct web hacking experiments. Our study is the first to use crowdsourcing platforms to run hacking experiments for scientific purposes. While the recruitment is challenging, the paradigm of existing crowdsourcing platforms can be useful for understanding adversarial behavior, as it facilitates access to a diverse set of participants and allows researchers to conduct longitudinal and cross-cultural assessments. In particular, crowdsourcing platforms offer a great opportunity for cybersecurity researchers to investigate the Oppositional Human Factors (OHFs) in a manageable and flexible way.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389808",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "applicationofcollaborativefilteringrecommendationalgorithmininternetonlinecourses",
        "booktitle": "Proceedings of the 6th International Conference on Big Data and Computing",
        "doi": "10.1145/3469968.3469992",
        "author": [
            "Pan, Zhengjun",
            "Zhao, Lianfen",
            "Zhong, Xingyu",
            "Xia, Zitong"
        ],
        "keywords": [
            "Collaborative filtering algorithm, education platform, recommendation system, online course"
        ],
        "abstract": "Aiming at the problem that the overload of online education platform course resources leads to the difficulty of user selection, this paper mainly studies the improvement and application of collaborative filtering algorithm based on online course recommendation system, which organically combines personalized recommendation technology and online course system to meet the needs of users and online education platform. In the process of recommendation, firstly, user preferences are collected to establish a data model, and user login information and learning behavior information are used as implicit characteristics of user preferences. The loss rate of users in the computing platform is defined, the popularity of each course is calculated, and the relationship between users and courses is constructed, and the correlation and comparative analysis are carried out, Then, the traditional collaborative filtering algorithm is improved by introducing the implicit features after analysis, and the cosine similarity method is used to calculate the course similarity. Finally, the topN recommendation list is generated to get the recommendation results. Based on the desensitization data of an education platform, the experimental results show that the improved recommendation model can improve the precision of recommendation by introducing implicit features.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450375061",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "nonfunctionalrequirementsclassificationforaligningbusinesswithinformationsystems",
        "booktitle": "Proceedings of the 2020 International Conference on Big Data in Management",
        "doi": "10.1145/3437075.3437091",
        "author": [
            "Majthoub, Manar",
            "Odeh, Yousra",
            "Hijjawi, Mohammed"
        ],
        "keywords": [
            "System Model, Use Case, Business/IT Alignment, Quality Requirements, Non-Functional Requirements, Business Models, Business Process Model"
        ],
        "abstract": "Non-Functional Requirements (NFR) are defined as the desired quality requirements, such as availability, that restrict software product being developed where some external restrictions may apply. Since information systems have been introduced, organizations in the business world align their functional activities with systems without paying attention to quality-based alignment. Few research works have been conducted in order to classify and integrate the NFR with business or system models. But these classifications and integrations are only confined to either the business side or the system side, which in turn have caused in having a gap in mapping the classifications between the two sides. Because business models and system models mutually affect each other in many ways, their NFR integration and classification should be aligned with each other. Having a NFR alignment-based classification between business and information systems contributes to assist the stakeholders in reflecting the quality requirements at the business side for a particular task on the related tasks integrated with NFRs at the systems side. Also having an alignment-oriented classification contributes to trace quality/NFR-based changes from the business organization to its systems and vice versa.In this research, we propose a NFR classification for aligning quality requirements in business with their NFRs in information systems. The work in business side is represented through business process models designed using Business Process Model and Notation (BPMN) where the use case models represents the system side in this research. The proposed classification is demonstrated in both business and systems using the academic advising and registration case study at Applied Science University in Jordan.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384469",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "contrastivecurriculumlearningforsequentialuserbehaviormodelingviadataaugmentation",
        "booktitle": "Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management",
        "doi": "10.1145/3459637.3481905",
        "author": [
            "Bian, Shuqing",
            "Zhao, Wayne",
            "Zhou, Kun",
            "Cai, Jing",
            "He, Yancheng",
            "Yin, Cunxiang",
            "Wen, Ji-Rong"
        ],
        "keywords": [
            "user behavior modeling, contrastive curriculum learning"
        ],
        "abstract": "Within online platforms, it is critical to capture the semantics of sequential user behaviors for accurately modeling user interests. However, dynamic characteristics and sparse behaviors make it difficult to train effective user representations for sequential user behavior modeling.Inspired by the recent progress in contrastive learning, we propose a novel Contrastive Curriculum Learning framework for producing effective representations for modeling sequential user behaviors. We make important technical contributions in two aspects, namely data quality and sample ordering. Firstly, we design a model-based data generator by generating high-quality samples confirming to users' attribute information. Given a target user, it can leverage the fused attribute semantics for generating more close-to-real sequences. Secondly, we propose a curriculum learning strategy to conduct contrastive learning via an easy-to-difficult learning process. The core component is a learnable difficulty evaluator, which can score augmented sequences, and schedule them in curriculums. Extensive results on both public and industry datasets demonstrate the effectiveness of our approach on downstream tasks.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384650",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "towardssituationalawarenesswithmultimodalstreamingdatafusionserverlesscomputingapproach",
        "booktitle": "Proceedings of the International Workshop on Big Data in Emergent Distributed Environments",
        "doi": "10.1145/3460866.3461769",
        "author": [
            "Nesen, Alina",
            "Bhargava, Bharat"
        ],
        "keywords": [
            "multimodal machine learning, function-as-a-service, serverless computing"
        ],
        "abstract": "The availability of large quantities of data has given an impulse for methods and techniques to extract unseen useful knowledge and process it in a fast and scalable manner. In order to extract the most complete possible knowledge from the continuous data stream, it is necessary to use the heterogeneous data sources and process information from multiple modalities. The systems that utilize multimodal data must take advantage of the up-to-date approaches for data storage, usage, cleaning and storage. Neural networks and machine learning approaches are widely used for data-heavy software where pattern extractions and predictions need to be conducted while serverless computing frameworks are being increasingly used for machine learning solutions to optimize cost and speed of such systems. This work presents a framework for processing data from multimodal sources where the task of feature and pattern extraction is performed on a serverless computing platform. The use cases for public safety solutions to increase situational awareness are described and compared with other implementation approaches.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450388436",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "researchonarchitectureofbigdateanalysisplatformincloudenvironment",
        "booktitle": "2020 4th International Conference on Computer Science and Artificial Intelligence",
        "doi": "10.1145/3445815.3445820",
        "author": [
            "Jin, Liya",
            "Wang, Ronghui",
            "Wang, Xuan"
        ],
        "keywords": [
            "Analysis platform, Big data, Cloud computing, Data mining"
        ],
        "abstract": "The rapid development of big data has attracted extensive attention at home and abroad. Scientific and effective analysis and processing of big data is the core issue in the field of big data. The construction of a big data analysis platform in cloud environment can process complex data structures and highly correlated data, timely respond to user requests, realize intelligent and efficient data analysis, and mine more valuable data, providing technical support for the rapid construction of big data services.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450395588",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "acomparisonofmachinelearningalgorithmsinbloodglucosepredictionforpeoplewithtype1diabetes",
        "booktitle": "Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences",
        "doi": "10.1145/3500931.3500993",
        "author": [
            "Wang, Yiyang"
        ],
        "keywords": [
            "deep neural networks, deep learning, machine learning, reinforcement learning, type 1 diabetes, artificial intelligence"
        ],
        "abstract": "Diabetes is a metabolic disease with the characteristic of hyperglycemia. The pathogenic principle is derived from the defect of insulin secretion or the impairment of biological effects, or both. We use machine learning models and deep learning models for forecasting future blood glucose levels in this paper, and study the efficiency of detecting hypoglycemia and hyperglycemia events. The data set used is in-silico data generated from the UVA/PADOVA type 1 diabetes simulator. We aim to compare support vector machines, random forests, linear regression, K-Nearest Neighbors regression (KNN), XGBoosted trees and other deep learning models in terms of Root Mean Squared Error (RMSE), and other several evaluation metrics to study their effectiveness in predicting future blood sugar, and the accuracy rate of predicting hypoglycemia and hyperglycemia events. In this work, we found a bidirectional long-short term memory (LSTM) model with the best prediction effect, which can predict the blood glucose level of simulated patients with leading accuracy within 30 minutes (RMSE = 7.55\u00b10.19 [mg/dl], R2-SCORE=0.96). The hopeful results show that this method could have practical application value for self-management of blood glucose in patients with type 1 diabetes.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450388559",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "amapmatchingmethodforrestoringmovementrouteswithcellularsignalingdata",
        "booktitle": "2020 The 8th International Conference on Information Technology: IoT and Smart City",
        "doi": "10.1145/3446999.3447017",
        "author": [
            "Wang, Mo",
            "Wang, Jing",
            "Song, Yulun"
        ],
        "keywords": [
            "map matching, signaling data, human mobility, road networks"
        ],
        "abstract": "Cellular signaling data is a valuable and abundant data source to explore human mobility. Yet challenges remain to restore movement routes from signaling data due to its coarse positioning information. We propose an efficient map matching method based on road network topology. First, a customized spatial-temporal clustering algorithm ST-DBSCAN was employed to find stationary point clusters, which were later used to segment trips into sub-trips. The search space was then clipped with a fixed buffer zone along the line that connects the whole trip. Two optional strategies were provided to find the best matching routes with distance costs. Experiments on real-world data showed that both strategies achieved high map matching accuracies (88.2% and 94.3%). With Deep Mode, the method reached higher accuracy, while with longer computation time. The proposed method has the potential in solving practical problems, in the sense that it could be easily parallelized to deal with mass data.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390248",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "researchonpublishertopicselectionbasedondatamining",
        "booktitle": "2021 4th International Conference on Data Science and Information Technology",
        "doi": "10.1145/3478905.3478999",
        "author": [
            "Huang, Yongliang",
            "Yang, Shulin",
            "Li, Xiang",
            "Peng, Jiao",
            "Zhou, Meiqi"
        ],
        "keywords": [
            "Publishing topics, Decision tree, Data mining, Big data"
        ],
        "abstract": "As one of the important traditional industries in China, the printing and publishing industry is facing the current situation of the Internet era with the explosion of information and people's demands tend to be personalized and diversified.How to achieve accurate topic selection is the key.In this context, this paper combines the most popular big data technology with the traditional printing industry, improves the quality of the original data of the publishing house through data preprocessing technology, classifies different types of data by decision tree classifier, and finally completes the data mining.It provides a new thought and method for the topic planning of publishing industry.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384926",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "researchonsuggestionsofimprovingchineseopengovernmentdataininnovationofpublicgovernance",
        "booktitle": "DG.O2021: The 22nd Annual International Conference on Digital Government Research",
        "doi": "10.1145/3463677.3463687",
        "author": [
            "Li, Hongqin",
            "Zhai, Jun"
        ],
        "keywords": [
            "Data quality, Sharing and cooperation"
        ],
        "abstract": "This paper collects a large number of cases and makes a comparative analysis of the typical application of Chinese and American open government data for public governance. Through comparison, this paper finds the gap between China's open government data and the United States, and then analyzes the reasons. On this basis, through the investigation of advanced experience, this paper puts forward the suggestions of open government data to innovate public governance, including data catalogue compilation, data standard formulating, data quality assessment and open government data sharing cooperation, in order to improve Chinese open government data to innovate the public governance level.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03600300",
        "isbn": null,
        "journal": "ACM Comput. Surv.",
        "publisher": "Association for Computing Machinery",
        "title": "dataanalyticsforairtraveldataasurveyandnewperspectives",
        "booktitle": null,
        "doi": "10.1145/3469028",
        "author": [
            "Tian, Haiman",
            "Presa-Reyes, Maria",
            "Tao, Yudong",
            "Wang, Tianyi",
            "Pouyanfar, Samira",
            "Miguel, Alonso",
            "Luis, Steven",
            "Shyu, Mei-Ling",
            "Chen, Shu-Ching",
            "Iyengar, Sundaraja"
        ],
        "keywords": [
            "revenue management, Airline, big data"
        ],
        "abstract": "From the start, the airline industry has remarkably connected countries all over the world through rapid long-distance transportation, helping people overcome geographic barriers. Consequently, this has ushered in substantial economic growth, both nationally and internationally. The airline industry produces vast amounts of data, capturing a diverse set of information about their operations, including data related to passengers, freight, flights, and much more. Analyzing air travel data can advance the understanding of airline market dynamics, allowing companies to provide customized, efficient, and safe transportation services. Due to big data challenges in such a complex environment, the benefits of drawing insights from the air travel data in the airline industry have not yet been fully explored. This article aims to survey various components and corresponding proposed data analysis methodologies that have been identified as essential to the inner workings of the airline industry. We introduce existing data sources commonly used in the papers surveyed and summarize their availability. Finally, we discuss several potential research directions to better harness airline data in the future. We anticipate this study to be used as a comprehensive reference for both members of the airline industry and academic scholars with an interest in airline research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.282",
        "scimago_value": "2,079"
    },
    {
        "issnkey": "",
        "isbn": "9781450383431",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "autovalidateunsuperviseddatavalidationusingdatadomainpatternsinferredfromdatalakes",
        "booktitle": "Proceedings of the 2021 International Conference on Management of Data",
        "doi": "10.1145/3448016.3457250",
        "author": [
            "Song, Jie",
            "He, Yeye"
        ],
        "keywords": [
            "data pipelines, data quality, data validation, data lake"
        ],
        "abstract": "Complex data pipelines are increasingly common in diverse applications such as BI reporting and ML modeling. These pipelines often recur regularly (e.g., daily or weekly), as BI reports need to be refreshed, and ML models need to be retrained. However, it is widely reported that in complex production pipelines, upstream data feeds can change in unexpected ways, causing downstream applications to break silently that are expensive to resolve. Data validation has thus become an important topic, as evidenced by notable recent efforts from Google and Amazon, where the objective is to catch data quality issues early as they arise in the pipelines. Our experience on production data suggests, however, that on string-valued data, these existing approaches yield high false-positive rates and frequently require human intervention. In this work, we develop a corpus-driven approach to auto-validate machine-generated data by inferring suitable data-validation \"patterns'' that accurately describe the underlying data-domain, which minimizes false-positives while maximizing data quality issues caught. Evaluations using production data from real data lakes suggest that sj is substantially more effective than existing methods. Part of this technology ships as an Auto-Tag feature in Microsoft Azure Purview.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "19361955",
        "isbn": null,
        "journal": "J. Data and Information Quality",
        "publisher": "Association for Computing Machinery",
        "title": "businessintelligenceframeworkdesignandimplementationarealestatemarketcasestudy",
        "booktitle": null,
        "doi": "10.1145/3422669",
        "author": [
            "Fraihat, Salam",
            "Salameh, Walid",
            "Elhassan, Ammar",
            "Tahoun, Bushra",
            "Asasfeh, Maisa"
        ],
        "keywords": [
            "predictive analytics, real estate, Business intelligence, data quality"
        ],
        "abstract": "This article builds on previous work in the area of real-world applications of Business Intelligence (BI) technology. It illustrates the analysis, modeling, and framework design of a BI solution with high data quality to provide reliable analytics and decision support in the Jordanian real estate market. The motivation is to provide analytics dashboards to potential investors about specific segments or units in the market. The article ekxplains the design of a BI solution, including background market and technology investigation, problem domain requirements, solution architecture modeling, design and testing, and the usability of descriptive and predictive features. The resulting framework provides an effective BI solution with user-friendly market insights for investors with little or no market knowledge. The solution features predictive analytics based on established Machine Learning modeling techniques, analyzed and contrasted to select the optimum methodology and model combination for predicting market behavior to empower inexperienced users.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21508097",
        "isbn": null,
        "journal": "Proc. VLDB Endow.",
        "publisher": "VLDB Endowment",
        "title": "fastincrementaldiscoveryofpointwiseorderdependencies",
        "booktitle": null,
        "doi": "10.14778/3401960.3401965",
        "author": [
            "Tan, Zijing",
            "Ran, Ai",
            "Ma, Shuai",
            "Qin, Sheng"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Pointwise order dependencies (PODs) are dependencies that specify ordering semantics on attributes of tuples. POD discovery refers to the process of identifying the set \u03a3 of valid and minimal PODs on a given data set D. In practice D is typically large and keeps changing, and it is prohibitively expensive to compute \u03a3 from scratch every time. In this paper, we make a first effort to study the incremental POD discovery problem, aiming at computing changes \u0394\u03a3 to \u03a3 such that \u03a3 \u2295 \u0394\u03a3 is the set of valid and minimal PODs on D with a set \u0394D of tuple insertion updates. (1) We first propose a novel indexing technique for inputs \u03a3 and D. We give algorithms to build and choose indexes for \u03a3 and D, and to update indexes in response to \u0394D. We show that POD violations w.r.t. \u03a3 incurred by \u0394D can be efficiently identified by leveraging the proposed indexes, with a cost dependent on log(|D|). (2) We then present an effective algorithm for computing \u0394\u03a3, based on \u03a3 and identified violations caused by \u0394D. The PODs in \u03a3 that become invalid on D + \u0394D are efficiently detected with the proposed indexes, and further new valid PODs on D + \u0394D are identified by refining those invalid PODs in \u03a3 on D + \u0394D. (3) Finally, using both real-life and synthetic datasets, we experimentally show that our approach outperforms the batch approach that computes from scratch, up to orders of magnitude.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.047",
        "scimago_value": "0,946"
    },
    {
        "issnkey": "",
        "isbn": "9781450382977",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "federateddeepknowledgetracing",
        "booktitle": "Proceedings of the 14th ACM International Conference on Web Search and Data Mining",
        "doi": "10.1145/3437963.3441747",
        "author": [
            "Wu, Jinze",
            "Huang, Zhenya",
            "Liu, Qi",
            "Lian, Defu",
            "Wang, Hao",
            "Chen, Enhong",
            "Ma, Haiping",
            "Wang, Shijin"
        ],
        "keywords": [
            "data isolation, intelligent education, data quality evaluation, knowledge tracing, federated learning"
        ],
        "abstract": "Knowledge tracing is a fundamental task in intelligent education for tracking the knowledge states of students on necessary concepts. In recent years, Deep Knowledge Tracing (DKT) utilizes recurrent neural networks to model student learning sequences. This approach has achieved significant success and has been widely used in many educational applications. However, in practical scenarios, it tends to suffer from the following critical problems due to data isolation: 1) Data scarcity. Educational data, which is usually distributed across different silos (e.g., schools), is difficult to gather. 2) Different data quality. Students in different silos have different learning schedules, which results in unbalanced learning records, meaning that it is necessary to evaluate the learning data quality independently for different silos. 3) Data incomparability. It is difficult to compare the knowledge states of students with different learning processes from different silos. Inspired by federated learning, in this paper, we propose a novel Federated Deep Knowledge Tracing (FDKT) framework to collectively train high-quality DKT models for multiple silos. In this framework, each client takes charge of training a distributed DKT model and evaluating data quality by leveraging its own local data, while a center server is responsible for aggregating models and updating the parameters for all the clients. In particular, in the client part, we evaluate data quality incorporating different education measurement theories, and we construct two quality-oriented implementations based on FDKT, i.e., FDKTCTT and FDKTIRT-where the means of data quality evaluation follow Classical Test Theory and Item Response Theory, respectively. Moreover, in the server part, we adopt hierarchical model interpolation to uptake local effects for model personalization. Extensive experiments on real-world datasets demonstrate the effectiveness and superiority of the FDKT framework.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450380966",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "everyonewantstodothemodelworknotthedataworkdatacascadesinhighstakesai",
        "booktitle": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "doi": "10.1145/3411764.3445518",
        "author": [
            "Sambasivan, Nithya",
            "Kapania, Shivani",
            "Highfill, Hannah",
            "Akrong, Diana",
            "Paritosh, Praveen",
            "Aroyo, Lora"
        ],
        "keywords": [
            "data cascades, Uganda, data collectors, Data, high-stakes AI, ML, application-domain experts, Nigeria, raters, developers, India, AI, Kenya, data politics, data quality, Ghana, USA"
        ],
        "abstract": "AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades\u2014compounding events causing negative, downstream effects from data issues\u2014triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03600300",
        "isbn": null,
        "journal": "ACM Comput. Surv.",
        "publisher": "Association for Computing Machinery",
        "title": "informationintegrityarewethereyet",
        "booktitle": null,
        "doi": "10.1145/3436817",
        "author": [
            "Harley, Kelsey",
            "Cooper, Rodney"
        ],
        "keywords": [
            "Biba\u2019s model, Integrity, data quality, Clark-Wilson model, quality dimension, information integrity, noninterference, information flow, information trustworthiness, information security, security requirements, quality assessment, information quality"
        ],
        "abstract": "The understanding and promotion of integrity in information security has traditionally been underemphasized or even ignored. From implantable medical devices and electronic voting to vehicle control, the critical importance of information integrity to our well-being has compelled review of its treatment in the literature. Through formal information flow models, the data modification view, and the relationship to data quality, information integrity will be surveyed. Illustrations are given for databases and information trustworthiness. Integrity protection is advancing but lacks standardization in terminology and application. Integrity must be better understood, and pursued, to achieve devices and systems that are beneficial and safe for the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.282",
        "scimago_value": "2,079"
    },
    {
        "issnkey": "15335399",
        "isbn": null,
        "journal": "ACM Trans. Internet Technol.",
        "publisher": "Association for Computing Machinery",
        "title": "shorttermloadforecastingbyusingimprovedgepandabnormalloadrecognition",
        "booktitle": null,
        "doi": "10.1145/3447513",
        "author": [
            "Deng, Song",
            "Chen, Fulin",
            "Dong, Xia",
            "Gao, Guangwei",
            "Wu, Xindong"
        ],
        "keywords": [
            "Gene expression programming, abnormal load recognition, power load forecasting, probability distribution, adaptive evolution"
        ],
        "abstract": "Load forecasting in short term is very important to economic dispatch and safety assessment of power system. Although existing load forecasting in short-term algorithms have reached required forecast accuracy, most of the forecasting models are black boxes and cannot be constructed to display mathematical models. At the same time, because of the abnormal load caused by the failure of the load data collection device, time synchronization, and malicious tampering, the accuracy of the existing load forecasting models is greatly reduced. To address these problems, this article proposes a Short-Term Load Forecasting algorithm by using Improved Gene Expression Programming and Abnormal Load Recognition (STLF-IGEP_ALR). First, the Recognition algorithm of Abnormal Load based on Probability Distribution and Cross Validation is proposed. By analyzing the probability distribution of rows and columns in load data, and using the probability distribution of rows and columns for cross-validation, misjudgment of normal load in abnormal load data can be better solved. Second, by designing strategies for adaptive generation of population parameters, individual evolution of populations and dynamic adjustment of genetic operation probability, an Improved Gene Expression Programming based on Evolutionary Parameter Optimization is proposed. Finally, the experimental results on two real load datasets and one open load dataset show that compared with the existing abnormal data detection algorithms, the algorithm proposed in this article have higher advantages in missing detection rate, false detection rate and precision rate, and STLF-IGEP_ALR is superior to other short-term load forecasting algorithms in terms of the convergence speed, MAE, MAPE, RSME, and R2.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390248",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "researchonhorizontalintegrationschemeformasscustomizationdataquantityandqualityproblemhorizontalintegrationschemeformc",
        "booktitle": "2021 4th International Conference on Data Science and Information Technology",
        "doi": "10.1145/3478905.3478920",
        "author": [
            "Fei, Yiming",
            "Yuan, Xiaoyue",
            "Ren, Mengmeng",
            "Fan, Shuhai"
        ],
        "keywords": [
            "Mass Customization, Horizontal Integration, LiDAR Camera Technology, Data Quality"
        ],
        "abstract": "To solve the problem of Mass Customization Data Quantity and Quality Problem, a Horizontal Integration Scheme of MC is proposed. By using LiDAR technology to scan and identify parts information, the machining route and required parts of the workpiece are automatically planned by comparing and matching with the documents using STEP-NC standard, to realize the efficient acquisition and utilization of MC data and ensure the automation of enterprise production.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384131",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "missingdatapatternsfromtheorytoanapplicationinthesteelindustry",
        "booktitle": "33rd International Conference on Scientific and Statistical Database Management",
        "doi": "10.1145/3468791.3468841",
        "author": [
            "Bechny, Michal",
            "Sobieczky, Florian",
            "Zeindl, J\\\"{u}rgen",
            "Ehrlinger, Lisa"
        ],
        "keywords": [
            "Data Quality, Pattern Detection, Missing Data, Steel Industry."
        ],
        "abstract": "Missing data (MD) is a prevalent problem and can negatively affect the trustworthiness of data analysis. In industrial use cases, faulty sensors or errors during data integration are common causes for systematically missing values. The majority of MD research deals with imputation, i.e., the replacement of missing values with \u201cbest guesses\u201d. Most imputation methods require missing values to occur independently, which is rarely the case in industry. Thus, it is necessary to identify missing data patterns (i.e., systematically missing values) prior to imputation (1) to understand the cause of the missingness, (2) to gain deeper insight into the data, and (3) to choose the proper imputation technique. However, in literature, there is a wide varity of MD patterns without a common formalization. In this paper, we introduce the first formal definition of MD patterns. Building on this theory, we developed a systematic approach on how to automatically detect MD patterns in industrial data. The approach has been developed in cooperation with voestalpine Stahl GmbH, where we applied it to real-world data from the steel industry and demonstrated its efficacy with a simulation study.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10668888",
        "isbn": null,
        "journal": "The VLDB Journal",
        "publisher": "Springer-Verlag",
        "title": "abstathdascalabletoolforprofilingverylargeknowledgegraphs",
        "booktitle": null,
        "doi": "10.1007/s00778-021-00704-2",
        "author": [
            "Alva, Renzo",
            "Maurino, Andrea",
            "Palmonari, Matteo",
            "Ciavotta, Michele",
            "Spahiu, Blerina"
        ],
        "keywords": [
            "Data quality, Data profiling, Knowledge graph, Distributed processing engine, Data management"
        ],
        "abstract": "Processing large-scale and highly interconnected Knowledge Graphs (KG) is becoming crucial for many applications such as recommender systems, question answering, etc. Profiling approaches have been proposed to summarize large KGs with the aim to produce concise and meaningful representation so that they can be easily managed. However, constructing profiles and calculating several statistics such as cardinality descriptors or inferences are resource expensive. In this paper, we present ABSTAT-HD, a highly distributed profiling tool that supports users in profiling and understanding big and complex knowledge graphs. We demonstrate the impact of the new architecture of ABSTAT-HD by presenting a set of experiments that show its scalability with respect to three dimensions of the data to be processed: size, complexity and workload. The experimentation shows that our profiling framework provides informative and concise profiles, and can process and manage very large KGs.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.868",
        "scimago_value": "0,653"
    },
    {
        "issnkey": "15335399",
        "isbn": null,
        "journal": "ACM Trans. Internet Technol.",
        "publisher": "Association for Computing Machinery",
        "title": "dataopsforcyberphysicalsystemsgovernancetheairportpassengerflowcase",
        "booktitle": null,
        "doi": "10.1145/3432247",
        "author": [
            "Garriga, Martin",
            "Aarns, Koen",
            "Tsigkanos, Christos",
            "Tamburri, Damian",
            "Heuvel, Wjan"
        ],
        "keywords": [
            "DataOps, cyber-physical systems, big data, systems governance, Data-intensive systems, airport management"
        ],
        "abstract": "Recent advancements in information technology have ushered a new wave of systems integrating Internet technology with sensing, wireless communication, and computational resources over existing infrastructures. As a result, myriad complex, non-traditional Cyber-Physical Systems (CPS) have emerged, characterized by interaction among people, physical facilities, and embedded sensors and computers, all generating vast amounts of complex data. Such a case is encountered within a contemporary airport hall setting: passengers roaming, information systems governing various functions, and data being generated and processed by cameras, phones, sensors, and other Internet of Things technology. This setting has considerable potential of contributing to goals entertained by the CPS operators, such as airlines, airport operators/owners, technicians, users, and more. We model the airport setting as an instance of such a complex, data-intensive CPS where multiple actors and data sources interact, and generalize a methodology to support it and other similar systems. Furthermore, this article instantiates the methodology and pipeline for predictive analytics for passenger flow, as a characteristic manifestation of such systems requiring a tailored approach. Our methodology also draws from DataOps principles, using multi-modal and real-life data to predict the underlying distribution of the passenger flow on a flight-level basis (improving existing day-level predictions), anticipating when and how the passengers enter the airport and move through the check-in and baggage drop-off process. This allows to plan airport resources more efficiently while improving customer experience by avoiding passenger clumping at check-in and security. We demonstrate results obtained over a case from a major international airport in the Netherlands, improving up to 60% upon predictions of daily passenger flow currently in place.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383325",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "onpostselectioninferenceinabtesting",
        "booktitle": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "doi": "10.1145/3447548.3467129",
        "author": [
            "Deng, Alex",
            "Li, Yicheng",
            "Lu, Jiannan",
            "Ramamurthy, Vivek"
        ],
        "keywords": [
            "big data, empirical Bayes, winner's curse, machine learning, online metrics, post-selection inference, randomization, A/B testing, regression, bias correction"
        ],
        "abstract": "When interpreting A/B tests, we typically focus only on the statistically significant results and take them by face value. This practice, termed post-selection inference in the statistical literature, may negatively affect both point estimation and uncertainty quantification, and therefore hinder trustworthy decision making in A/B testing. To address this issue, in this paper we explore two seemingly unrelated paths, one based on supervised machine learning and the other on empirical Bayes, and propose post-selection inferential approaches that combine the strengths of both. Through large-scale simulated and empirical examples, we demonstrate that our proposed methodologies stand out among other existing ones in both reducing post-selection biases and improving confidence interval coverage rates, and discuss how they can be conveniently adjusted to real-life scenarios.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450388092",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "improveddataaccuracyassessmenttoolforinformationmanagementsystems",
        "booktitle": "2020 the 6th International Conference on Communication and Information Processing",
        "doi": "10.1145/3442555.3442579",
        "author": [
            "Maziku, Hellen"
        ],
        "keywords": [
            "Information Management Systems, Human Centered Design, Data Quality Assessment, Accuracy"
        ],
        "abstract": "Developing countries are increasingly taking advantage of the rapid advancement in ICT to replace paper-based operations with Information Management Systems (IMS) such as District Health Information Software (DHIS). While the adoption of IMS presents significant benefits, challenges exist in the quality of IMS data. Inaccurate, incomplete or redundant data in IMS has misled organisations into making incorrect decisions leading to customer dissatisfaction and high cost implications. There is an urgent need for IMS stakeholders to have mechanisms of assessing the quality of data prior to data analysis, data sharing or decision making. Data Accuracy Assessment Tool (DAAT) assesses and identifies errors in a pair of context related datasets. DAAT provides ability for Data Managers to easily compare datasets by choosing attributes of their interest from a pool of diverse attributes that define the data. Through reports and visualization, the tool reveals the accuracy of data in real time using metrics such as validity, completeness and duplication of data. DAAT is scalable since it can be integrated with any IMS such as DHIS. The tool has been tested using four years Voluntary Medical Male Circumcision (VMMC) program data from JHPIEGO's AIDSFree project in Tanzania.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383127",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "datapoisoningattacksanddefensestocrowdsourcingsystems",
        "booktitle": "Proceedings of the Web Conference 2021",
        "doi": "10.1145/3442381.3450066",
        "author": [
            "Fang, Minghong",
            "Sun, Minghao",
            "Li, Qi",
            "Gong, Neil",
            "Tian, Jin",
            "Liu, Jia"
        ],
        "keywords": [
            "Data poisoning attacks, crowdsourcing, truth discovery"
        ],
        "abstract": "A key challenge of big data analytics is how to collect a large volume of (labeled) data. Crowdsourcing aims to address this challenge via aggregating and estimating high-quality data (e.g., sentiment label for text) from pervasive clients/users. Existing studies on crowdsourcing focus on designing new methods to improve the aggregated data quality from unreliable/noisy clients. However, the security aspects of such crowdsourcing systems remain under-explored to date. We aim to bridge this gap in this work. Specifically, we show that crowdsourcing is vulnerable to data poisoning attacks, in which malicious clients provide carefully crafted data to corrupt the aggregated data. We formulate our proposed data poisoning attacks as an optimization problem that maximizes the error of the aggregated data. Our evaluation results on one synthetic and two real-world benchmark datasets demonstrate that the proposed attacks can substantially increase the estimation errors of the aggregated data. We also propose two defenses to reduce the impact of malicious clients. Our empirical results show that the proposed defenses can substantially reduce the estimation errors of the data poisoning attacks.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390255",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "cloudservicecontextandfeedbackfusionofproductdesigncreativedemandperceptiontechnology",
        "booktitle": "2021 4th International Conference on Information Systems and Computer Aided Education",
        "doi": "10.1145/3482632.3482675",
        "author": [
            "Sun, Wen"
        ],
        "keywords": [
            ""
        ],
        "abstract": "After experiencing the stages of document delivery service, information service and knowledge service, the traditional product design creative demand perception technology has gradually transformed to the cloud service stage driven by new technologies such as big data and cloud computing. With the advent of the era of big data and artificial intelligence, multi-source heterogeneous and massive data resources have specific fusion characteristics and application trends. The generation of new artificial intelligence technologies and methods is to respond to the above characteristics and trends. Multi-source heterogeneous resources and vast amounts of data driven product design requirements perception from the user requirements perception, perception technology content and creative product design requirements capturing perception technology scenario-based push these three core function implementation requirements perception technology cloud service mode, implement the product design requirements perception technology innovation process. Of this study was to study the cloud service situation and feedback the fusion of product design requirements perception technology, cloud service components analysis demand perception technology and its features, with case studies to explore data driven era product design demand perception technology to the cloud service model transformation of ideas, requirements for perception technology transformation provides scientific theory and practice.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450395588",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "researchprogressofuserportraittechnologyinmedicalfield",
        "booktitle": "Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences",
        "doi": "10.1145/3500931.3501016",
        "author": [
            "Gao, Mengke",
            "Zhang, Yan",
            "Gao, Yue"
        ],
        "keywords": [
            "User portrait, Medical treatment, Review"
        ],
        "abstract": "In recent years, due to the rise of big data mining and intelligent recommendation, user portrait technology has gradually become a hot topic. As a new data analysis method, user portrait technology aims to mine user characteristics from a large number of user behavior data, and complete the user information panorama of the monomer or group through information mining, so as to prepare for the realization of precision service and personalized recommendation in various industries. It has been widely used in personalized recommendation and precision recommendation, personalized service and intelligent service, group characteristics analysis, prediction analysis and auxiliary decision-making. User portrait technology in the domestic research started late, currently in e-commerce, commercial precision marketing, book recommendation management, network personalized search, video entertainment and other fields of application is relatively mature. However, the application of user portrait technology in the field of medical and health is still in the preliminary exploration stage, and the combination of medical big data and user portrait technology can vividly depict the portraits of patients, doctors, residents and other different groups in promoting health and disease prevention. According to the different characteristics extracted, it can provide reference for meeting the needs of patients and achieving precision medicine. Therefore, this study reviews the concept, elements, implementation process and application of user portrait technology in the medical field, and provides reference for the subsequent application research of user portrait technology in the medical field",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450385558",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "s2ceahybridcloudandedgeorchestratorforminingexascaledistributedstreams",
        "booktitle": "Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems",
        "doi": "10.1145/3465480.3466926",
        "author": [
            "Kourtellis, Nicolas",
            "Herodotou, Herodotos",
            "Grzenda, Maciej",
            "Wawrzyniak, Piotr",
            "Bifet, Albert"
        ],
        "keywords": [
            "cloud analytics, data stream analysis, edge analytics, stream mining, machine and deep learning"
        ],
        "abstract": "The explosive increase in volume, velocity, variety, and veracity of data generated by distributed and heterogeneous nodes such as IoT and other devices, continuously challenge the state of art in big data processing platforms and mining techniques. Consequently, it reveals an urgent need to address the ever-growing gap between this expected exascale data generation and the extraction of insights from these data. To address this need, this position paper proposes Stream to Cloud &amp; Edge (S2CE), a first of its kind, optimized, multi-cloud and edge orchestrator, easily configurable, scalable, and extensible. S2CE will enable machine and deep learning over voluminous and heterogeneous data streams running on hybrid cloud and edge settings, while offering the necessary functionalities for practical and scalable processing: data fusion and preprocessing, sampling and synthetic stream generation, cloud and edge smart resource management, and distributed processing.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450380621",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "adatacentriccomputingcurriculumforadatasciencemajor",
        "booktitle": "Proceedings of the 52nd ACM Technical Symposium on Computer Science Education",
        "doi": "10.1145/3408877.3432457",
        "author": [
            "Fekete, Alan",
            "Kay, Judy",
            "R\\\"{o}hm, Uwe"
        ],
        "keywords": [
            "data science, curriculum"
        ],
        "abstract": "Many universities are introducing a new major in Data Science into their offering, to reflect the explosive growth in this field and the career opportunities it provides. As a field Data Science has elements from Computer Science and from Statistics, and curricula plans differ widely, both in the balance between the CS and Stats aspects, and also in the emphasis within the computing topics. This paper reports on the curriculum that has been taught for three years now at the University of Sydney. In particular, we describe the approach of a sequence of computing subjects which were developed specifically for the major, in order to bring students over several years to a sophisticated understanding of the data-handling aspects of Data Science. Students also take traditional subjects from both CS (such as Data Structures or AI) and from Statistics (such as Learning from Data and Statistical Inference). The data-centric specially-designed subjects we discuss in this paper are (i) Informatics: Data and Computation (in the first year), (ii) Big Data and Data Diversity (in the second year), and then upper-division subjects on (iii) Data Science Platforms, and (iv) Human-in-the-Loop Data Analytics.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450391146",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "trackingurbanheartbeatandpolicycompliancethroughvisionandlanguagebasedsensing",
        "booktitle": "Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",
        "doi": "10.1145/3486611.3491133",
        "author": [
            "Chowdhury, Tahiya",
            "Ding, Qizhen",
            "Mandel, Ilan",
            "Ju, Wendy",
            "Ortiz, Jorge"
        ],
        "keywords": [
            "COVID-19, urban sensing, computer vision and language"
        ],
        "abstract": "Sensing activities at the city scale using big data can enable applications to improve the quality of citizen life. While there are approaches to sense the urban heartbeat using sound, vision, radio frequency (RF), and other sensors, capturing changes at urban scale using such sensing modalities is challenging. Due to the enormous amount of data they produce and the associated annotation and processing requirement, such data can be of limited use. In this paper, we present a vision-to-language modeling approach to capture patterns and transitions that occur in New York City from March 2020 to August 2020. We use the model on ~1 million street images captured by dashcams over 6 months. We then use the captions to train a language model based on Latent Dirichlet Allocation [4] and compare models from different periods using probabilistic distance measures. We observe distribution shifts in the model that correlate well with social distancing policies and are corroborated by different data sources, such as mobility traces. This language-based sensing introduces a new sensing modality to capture dynamics in the city with lower storage requirements and privacy concerns.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383127",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "scalableautoweighteddiscretemultiviewclustering",
        "booktitle": "Proceedings of the Web Conference 2021",
        "doi": "10.1145/3442381.3449956",
        "author": [
            "Yang, Longqi",
            "Zhang, Liangliang",
            "Tang, Yuhua"
        ],
        "keywords": [
            "binary coding, parameter selection, graph regularization, multi-view clustering"
        ],
        "abstract": "Multi-view clustering has been widely studied in machine learning, which uses complementary information to improve clustering performance. However, challenges remain when handling large-scale multi-view data due to the traditional approaches\u2019 high time complexity. Besides, the existing approaches suffer from parameter selection. Due to the lack of labeled data, parameter selection in practical clustering applications is difficult, especially in big data. In this paper, we propose a novel approach for large-scale multi-view clustering to overcome the above challenges. Our approach focuses on learning the low-dimensional binary embedding of multi-view data, preserving the samples\u2019 local structure during binary embedding, and optimizing the embedding and clustering in a unified framework. Furthermore, we proposed to learn the parameters using a combination of data-driven and heuristic approaches. Experiments on five large-scale multi-view datasets show that the proposed method is superior to the state-of-the-art in terms of clustering quality and running time.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450376556",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "predictivemaintenanceinindustry40",
        "booktitle": "Proceedings of the 10th International Conference on Information Systems and Technologies",
        "doi": "10.1145/3447568.3448537",
        "author": [
            "Sang, Go",
            "Xu, Lai",
            "Vrieze, Paul",
            "Bai, Yuewei",
            "Pan, Fangyu"
        ],
        "keywords": [
            "Predictive maintenance, Collaborative business process, Blockchain, Industrial data space, Industry 4.0, FIWARE"
        ],
        "abstract": "In the context of Industry 4.0, the manufacturing related processes have shifted from conventional processes within one organization to collaborative processes cross different organizations, for example, product design processes, manufacturing processes, and maintenance processes across different factories and enterprises. The application of Internet of things, i.e. smart devices and sensors increases collection and availability of diverse data. Advanced technologies such as big data analytics and cloud computing offer new opportunities for effective optimization of manufacturing related processes, e.g. predictive maintenance. Predictive maintenance provides a detailed examination of the detection, location and diagnosis of faults in related machineries using various analyses. RAMI4.0 is a framework for thinking about the various efforts that constitute Industry 4.0. It spans the entire product life cycle &amp; value stream axis, hierarchical structure axis and functional classification axis. The Industrial Data Space (now International Data Space) is a virtual data space using standards and common governance models to facilitate the secure exchange and easy linkage of data in business ecosystems. It thereby provides a basis for creating and using smart services and innovative business processes, while at the same time ensuring digital sovereignty of data owners. This paper looks at how to support predictive maintenance in the context of Industry 4.0? Especially, applying RAMI 4.0 architecture supports the predictive maintenance using FIWARE framework, which leads to deal with data exchanging among different organizations with different security requirements as well as modularizing of related functions.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15335399",
        "isbn": null,
        "journal": "ACM Trans. Internet Technol.",
        "publisher": "Association for Computing Machinery",
        "title": "incrementalgrouplevelpopularitypredictioninonlinesocialnetworks",
        "booktitle": null,
        "doi": "10.1145/3461839",
        "author": [
            "Wang, Jingjing",
            "Jiang, Wenjun",
            "Li, Kenli",
            "Wang, Guojun",
            "Li, Keqin"
        ],
        "keywords": [
            "information diffusion, tensor analysis, incremental approach, popularity prediction, Group level, online social networks"
        ],
        "abstract": "Predicting the popularity of web contents in online social networks is essential for many applications. However, existing works are usually under non-incremental settings. In other words, they have to rebuild models from scratch when new data occurs, which are inefficient in big data environments. It leads to an urgent need for incremental prediction, which can update previous results with new data and conduct prediction incrementally. Moreover, the promising direction of group-level popularity prediction has not been well treated, which explores fine-grained information while keeping a low cost. To this end, we identify the problem of incremental group-level popularity prediction, and propose a novel model IGPP to address it. We first predict the group-level popularity incrementally by exploiting the incremental CANDECOMP/PARAFCAC (CP) tensor decomposition algorithm. Then, to reduce the cumulative error by incremental prediction, we propose three strategies to restart the CP decomposition. To the best of our knowledge, this is the first work that identifies and solves the problem of incremental group-level popularity prediction. Extensive experimental results show significant improvements of the IGPP method over other works both in the prediction accuracy and the efficiency.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03600300",
        "isbn": null,
        "journal": "ACM Comput. Surv.",
        "publisher": "Association for Computing Machinery",
        "title": "automltodateandbeyondchallengesandopportunities",
        "booktitle": null,
        "doi": "10.1145/3470918",
        "author": [
            "Karmaker, Shubhra",
            "Hassan, Md.",
            "Smith, Micah",
            "Xu, Lei",
            "Zhai, Chengxiang",
            "Veeramachaneni, Kalyan"
        ],
        "keywords": [
            "interactive data science, Automated machine learning, predictive analytics, democratization of artificial intelligence"
        ],
        "abstract": "As big data becomes ubiquitous across domains, and more and more stakeholders aspire to make the most of their data, demand for machine learning tools has spurred researchers to explore the possibilities of automated machine learning (AutoML). AutoML tools aim to make machine learning accessible for non-machine learning experts (domain experts), to improve the efficiency of machine learning, and to accelerate machine learning research. But although automation and efficiency are among AutoML\u2019s main selling points, the process still requires human involvement at a number of vital steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training dataset, and selecting a promising machine learning technique. These steps often require a prolonged back-and-forth that makes this process inefficient for domain experts and data scientists alike and keeps so-called AutoML systems from being truly automatic. In this review article, we introduce a new classification system for AutoML systems, using a seven-tiered schematic to distinguish these systems based on their level of autonomy. We begin by describing what an end-to-end machine learning pipeline actually looks like, and which subtasks of the machine learning pipeline have been automated so far. We highlight those subtasks that are still done manually\u2014generally by a data scientist\u2014and explain how this limits domain experts\u2019 access to machine learning. Next, we introduce our novel level-based taxonomy for AutoML systems and define each level according to the scope of automation support provided. Finally, we lay out a roadmap for the future, pinpointing the research required to further automate the end-to-end machine learning pipeline and discussing important challenges that stand in the way of this ambitious goal.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.282",
        "scimago_value": "2,079"
    },
    {
        "issnkey": "",
        "isbn": "9781450390200",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "rulebaseddataverificationmethodinelectricityspotmarket",
        "booktitle": "2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "doi": "10.1145/3469213.3470246",
        "author": [
            "Wu, Yang",
            "Zou, Wentao",
            "Liu, Shuangquan",
            "Jiang, Yan",
            "Shao, Qizhuan",
            "Zhou, Han"
        ],
        "keywords": [
            ""
        ],
        "abstract": "In the electric power spot market, input data quality is critical to an accurate and reliable clearing result. Missing and abnormal data will lead to the result that the clearing algorithm diverges or the results mismatch reality. This would seriously affect power system security and reduce the efficiency of the market. Therefore, to ensure a smooth convergence of the algorithm and reasonable results, this paper proposes a rule-based verification method for the input data in the power spot market. Data integrity and logical verification rules are established. During the verification process, information will be divided into different warning levels and displayed so that users can modify relevant data according to the actual situation.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "19361955",
        "isbn": null,
        "journal": "J. Data and Information Quality",
        "publisher": "Association for Computing Machinery",
        "title": "developingaglobaldatabreachdatabaseandthechallengesencountered",
        "booktitle": null,
        "doi": "10.1145/3439873",
        "author": [
            "Neto, Nelson",
            "Madnick, Stuart",
            "Paula, Anchises",
            "Borges, Natasha"
        ],
        "keywords": [
            "data aggregation, data breach, semantics of data, privacy, Cyber security"
        ],
        "abstract": "If the mantra \u201cdata is the new oil\u201d of our digital economy is correct, then data leak incidents are the critical disasters in the online society. The initial goal of our research was to present a comprehensive database of data breaches of personal information that took place in 2018 and 2019. This information was to be drawn from press reports, industry studies, and reports from regulatory agencies across the world. This article identified the top 430 largest data breach incidents among more than 10,000 data breach incidents.In the process, we encountered many complications, especially regarding the lack of standardization of reporting. This article should be especially interesting to the readers of JDIQ because it describes both the range of data quality and consistency issues found as well as what was learned from the database created.The database that was created, available at https://www.databreachdb.com, shows that the number of data records breached in those top 430 incidents increased from around 4B in 2018 to more than 22B in 2019. This increase occurred despite the strong efforts from regulatory agencies across the world to enforce strict rules on data protection and privacy, such as the General Data Protection Regulation (GDPR) that went into effect in Europe in May 2018. Such regulatory effort could explain the reason why there is such a large number of data breach cases reported in the European Union when compared to the U.S. (more than 10,000 data breaches publicly reported in the U.S. since 2018, while the EU reported more than 160,0001 data breaches since May 2018). However, we still face the problem of an excessive number of breach incidents around the world.This research helps to understand the challenges of proper visibility of such incidents on a global scale. The results of this research can help government entities, regulatory bodies, security and data quality researchers, companies, and managers to improve the data quality of data breach reporting and increase the visibility of the data breach landscape around the world in the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383325",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "thethirdinternationalworkshoponsmartdataforblockchainanddistributedledgersdbd2021jointworkshopwithsigkdd2021trustday",
        "booktitle": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "doi": "10.1145/3447548.3469441",
        "author": [
            "Zhu, Feida",
            "Pei, Jian"
        ],
        "keywords": [
            "data governance, data pricing, data auditing, data asset, privacy, distributed ledger technology"
        ],
        "abstract": "Today's computing is characterized by an increasing degree of complexity, comprehensiveness and collaboration. The complexity can be observed by the wide application of gigantic models with a huge number of parameters and structures of an unprecedented level of sophistication. The comprehensiveness is best illustrated by the high heterogeneity of data both in terms of format and source. The collaboration, finally, becomes an obvious trend when computing systems grow more open and decentralized in which various entities interact to achieve collective intelligence with the presence of potentially malicious behavior. Trust, therefore, has become critical at multiple levels: At model level to assure its integrity, fairness and interpretability; At data level to safeguard data quality, compliance and privacy; At system level to govern resilience, performance and incentive. Moreover, the notion of trust has long been discussed in different domains in both academia and industry with different definition and understanding. The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD'21) will be held as a joint workshop with the special-themed \"Trust Day\" of KDD 2021, which has therefore aimed to bring together researchers, practitioners and experts from various communities to exchange and explore ideas, frontiers, opportunities and challenges under the broad theme of \"trust\" in a highly interdisciplinary manner.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "19361955",
        "isbn": null,
        "journal": "J. Data and Information Quality",
        "publisher": "Association for Computing Machinery",
        "title": "automatedannotationsforaidataandmodeltransparency",
        "booktitle": null,
        "doi": "10.1145/3460000",
        "author": [
            "Thirumuruganathan, Saravanan",
            "Kunjir, Mayuresh",
            "Ouzzani, Mourad",
            "Chawla, Sanjay"
        ],
        "keywords": [
            "data cleaning, Data transparency, machine learning"
        ],
        "abstract": "The data and Artificial Intelligence revolution has had a massive impact on enterprises, governments, and society alike. It is fueled by two key factors. First, data have become increasingly abundant and are often available openly. Enterprises have more data than they can process. Governments are spearheading open data initiatives by setting up data portals such as data.gov and releasing large amounts of data to the public. Second, AI engineering development is becoming increasingly democratized. Open source frameworks have enabled even an individual developer to engineer sophisticated AI systems. But with such ease of use comes the potential for irresponsible use of data.Ensuring that AI systems adhere to a set of ethical principles is one of the major problems of our age. We believe that data and model transparency has a key role to play in mitigating the deleterious effects of AI systems. In this article, we describe a framework to synthesize ideas from various domains such as data transparency, data quality, data governance among others to tackle this problem. Specifically, we advocate an approach based on automated annotations (of both data and the AI model), which has a number of appealing properties. The annotations could be used by enterprises to get visibility of potential issues, prepare data transparency reports, create and ensure policy compliance, and evaluate the readiness of data for diverse downstream AI applications. We propose a model architecture and enumerate its key components that could achieve these requirements. Finally, we describe a number of interesting challenges and opportunities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390255",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "constructingacomputermodelfordisciplinedatagovernanceusingthecontingencytheoryanddatamining",
        "booktitle": "2021 4th International Conference on Information Systems and Computer Aided Education",
        "doi": "10.1145/3482632.3484077",
        "author": [
            "Wang, Chunxia",
            "Xie, Jian"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Data governance is an important part of modernizing the governance capacity of universities. Discipline data governance plays an important role in promoting the development of university disciplines, and is a key factor in improving the governance of university disciplines, the science of educational decision-making and the effectiveness of management. It is an important way to promote the \"precision\" and \"science\" of discipline governance. In this paper, we construct a model of discipline data governance based on the Contingency Theory with a view to shedding light on discipline governance.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450387828",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "dynamicestimationmodelofinsuranceproductrecommendationbasedonnaivebayesianmodel",
        "booktitle": "Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies",
        "doi": "10.1145/3444370.3444575",
        "author": [
            "Zhang, Bo",
            "Kong, Dehua"
        ],
        "keywords": [
            "Naive Bayes, dynamic estimation, insurance products, recommendation"
        ],
        "abstract": "Aiming at the dynamic estimation of insurance product recommendation, considering the particularity and complexity of purchasing insurance product and the uncertainty of influencing factors, a dynamic estimation model of insurance product recommendation based on Naive Bayes is proposed. The model combines customer insurance information with machine learning. The results show that the naive Bayesian classification algorithm can be compared with the decision tree and neural network classification algorithm, showing high accuracy and high speed.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390606",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "computingcompetenciesforundergraduatedatasciencecurricula",
        "booktitle": null,
        "doi": null,
        "author": [
            "Force, ACM"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "book",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384926",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "boldinnationalbudgetplanningacomparisonofinternationalcasesboldinnationalbudgetplanning",
        "booktitle": "DG.O2021: The 22nd Annual International Conference on Digital Government Research",
        "doi": "10.1145/3463677.3463696",
        "author": [
            "Geci, Mentor",
            "CsAki, Csaba"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Anticipating the continues increase in quantity and consequently their importance, data are becoming a new currency. Overall purpose of this paper is to show the link between Big Open Linked Data (BOLD) and national budget planning. The methodology that follows is a qualitative case-study based approach leading to a comparative analysis of five cases. Research problems investigated are the commonalities and differences that may be identified in the handling of national budget data in developing countries, as well as best practices or potential \u2018lessons learned\u2019 from international cases of handling budget data as BOLD. In addition, the study investigates how Kosovo as a young developing country may benefit from the experiences of other countries. To create a framework of analysis, international cases are reviewed. Their comparison reveals that there are not that many commonalities among these developing countries in terms of issues and challenges regarding the use of BOLD in national budgeting. As it is mainly country specific, the approach used toward BOLD largely depends on the general landscape of each country. In comparison, although there is some progress made, Kosovo is still behind those countries in terms of applying BOLD.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384926",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "whatmattersinmaintainingeffectiveopengovernmentdatasystemstheroleofgovernmentmanagerialcapacityandpoliticalandlegalenvironment",
        "booktitle": "DG.O2021: The 22nd Annual International Conference on Digital Government Research",
        "doi": "10.1145/3463677.3463732",
        "author": [
            "Ahn, Michael",
            "Chu, Shengli"
        ],
        "keywords": [
            ""
        ],
        "abstract": "This paper aims to identify key institutional factors that contribute to effective open data systems. Rapid advancement in new technologies such as machine learning, algorithms, IoT, and Cloud Computing has amplified the importance of national open data systems. The availability of relevant public data has become a crucial factor in creating sophisticated machine learning platforms or algorithms that will have a considerable impact on national competitiveness. Effective national open data strategies will matter in shaping an environment that will facilitate data production, dissemination, and utilization. Using multiple sources of data that measure the qualities of open data systems and various political, governmental, and legal attributes at the national level, we seek to identify key institutional factors that contribute to robust open data policies and outcomes. Our findings point to the importance of the existence of a national open data strategy and support (especially \"open by default\" strategy), pre-existing e-government capability, and countries operating under full democracy with its guarantees to civil liberties and political freedom. In addition, the nature of the open data matters as different managerial, political, and demographic conditions affected the quality of different open data systems. Policy implications of our findings are discussed.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389853",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "researchondatagovernanceframeworkforfiredepartment",
        "booktitle": "The 5th International Conference on Computer Science and Application Engineering",
        "doi": "10.1145/3487075.3487110",
        "author": [
            "An, Zhenpeng",
            "Zhang, Di",
            "Liang, Yunjie"
        ],
        "keywords": [
            "Fire Department component",
            ", Data governance, Data standard system"
        ],
        "abstract": "This paper analyzes data governance elements, models and frameworks, provides a clear plan for data governance for fire department. Using the method of literature research, network investigation and conclude data system of fire departments, the china domestic and foreign research status of data governance is reviewed. We build the framework of data governance for fire department, including Data resource directory system, Data technology support system and Data standardization system. This paper preliminarily forms the framework of data governance for fire department. This framework was applied to the fire information planning work. The results indicate that based on the status and characteristics of fire industry, the implementation of this framework is effective and feasible, and it is also the basis of standard fire control data governance in future.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389228",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "machinelearningasaservicechallengesinresearchandapplications",
        "booktitle": "Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services",
        "doi": "10.1145/3428757.3429152",
        "author": [
            "Philipp, Robert",
            "Mladenow, Andreas",
            "Strauss, Christine",
            "V\\\"{o}lz, Alexander"
        ],
        "keywords": [
            "Machine Learning Platform, Machine Learning as a Service, Machine Learning, MLaaS, Machine Learning Services"
        ],
        "abstract": "This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "19361955",
        "isbn": null,
        "journal": "J. Data and Information Quality",
        "publisher": "Association for Computing Machinery",
        "title": "towardacompletedatavaluationprocesschallengesofpersonaldata",
        "booktitle": null,
        "doi": "10.1145/3447269",
        "author": [
            "Tufi\\c{s}, Mihnea",
            "Boratto, Ludovico"
        ],
        "keywords": [
            "Datasets, data valuation, data markets"
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384070",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "dataanalyticsreadinessmodelinindonesiangovernment",
        "booktitle": "6th International Conference on Sustainable Information Engineering and Technology 2021",
        "doi": "10.1145/3479645.3479669",
        "author": [
            "Sulistyowati, Ira",
            "Fransisca, Dyna",
            "Ruldeviyani, Yova"
        ],
        "keywords": [
            ""
        ],
        "abstract": "The development of information technology encourages the government to digitize business processes. It is generated with a large and varied volume from various data sources so that advanced data analytics (DA) is required to overcome this to support organization's data driven decision making. It's necessary to prepare DA based on DA readiness model so that the implementation of DA can run successfully. Whereas currently, there is limited study and no standard model for DA readiness. The focus of this study is to propose model readiness of implementing data analytics that is suitable in Indonesian government. The model refers to DA readiness model based on literature review on 15 papers relevant to DA readiness. Then it's verified by 7 experts. Furthermore, online survey was conducted to test the model that affects the readiness of implementing data analytics in Indonesian government. The survey results were analyzed using factor analysis. As a result, DA readiness model contains 4 dimensions, 11 factors, and 78 indicators where its dimensions consist of information system, organizational and cultural, organization structure and resource readiness. This model can describe 85% of the data analysis readiness requirements in the Indonesian government. In order to implement data analytics successfully, the government needs to improve the readiness of information systems, organizational and cultural, organizational structures, and resources.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390125",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "visualisingdevelopingnationshealthrecordsopportunitieschallengesandresearchagenda",
        "booktitle": "The 12th International Conference on Advances in Information Technology",
        "doi": "10.1145/3468784.3471607",
        "author": [
            "Umejiaku, Afamefuna",
            "Dang, Tommy"
        ],
        "keywords": [
            "Developing Nations, Health records, Visualisation"
        ],
        "abstract": "The benefits of effectively visualizing health records in huge volumes has resulted in health organizations, insurance companies, policy and decision makers, governments and drug manufactures\u2019 transformation in the way research is conducted. This has also played a key role in determining investment of resources. Health records contain highly valuable information; processing these records in large volumes is now possible due to technological advancement which allows for the extraction of highly valuable knowledge that has resulted in breakthroughs in scientific communities. To visualize health records in large volumes, the records need to be stored in electronic forms, properly documented, processed, and analyzed. A good visualization technique is used to present the analyzed information, allowing for effective knowledge extraction which is done in a secured manner protecting the privacy of the patients whose health records were used. As research and technological advancement have improved, the quality of knowledge extracted from health records have also improved; unfortunately, the numerous benefits of visualizing health records have only been felt in developed nations, unlike other sectors where technological advancement in developed nations have had similar impact in developing nations. This paper identifies the characteristics of health records and the challenges involved in processing large volumes of health records. This is to identify possible steps that could be taken for developing nations to benefit from visualizing health records in huge volumes.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26911922",
        "isbn": null,
        "journal": "ACM/IMS Trans. Data Sci.",
        "publisher": "Association for Computing Machinery",
        "title": "introductiontothespecialissueonlearningbasedsupportfordatascienceapplications",
        "booktitle": null,
        "doi": "10.1145/3450751",
        "author": [
            "Zhou, Ke",
            "Song, Jingkuan"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389662",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "astudyonthecurrentdevelopmentofartificialintelligenceineducationindustryinchina",
        "booktitle": "2021 7th International Conference on Education and Training Technologies",
        "doi": "10.1145/3463531.3463536",
        "author": [
            "Wan, Xinxin"
        ],
        "keywords": [
            "AI Education, Oral Assessment, Adaptive Learning, Smart Classroom, Education Informatization"
        ],
        "abstract": "This article first explained the definition of AI in education (AIEd) and reported findings regarding the current development of AIEd industry in the Chinese context. The research design is a context-specific case study using the supply and demand theoretical framework. From a demand-side perspective, the author made an in-depth analysis of the specific AI applications employed in different educational scenarios, including the automated speaking assessment system, the content-based image retrieval system, adaptive learning system, AI-supported classrooms, and AI-assisted campus safety system. For the supply analysis of the AIEd industry, this article summarized key AIEd industry chains and technologies currently widely used in China, obtaining the industry market scale through data collected from different sources. In addition, the iFLYTEK company, as a typical enterprise in the AIEd industry, was taken as a medium to conduct a case analysis. The employment of various AI applications in smart classrooms, smart exams, and smart terminals were comprehensively discussed. In a nutshell, this article discussed the development status and future trends of Chinese AIEd industry, with an aim to offer suggestions and implications for education practitioners.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383325",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "machinelearningrobustnessfairnessandtheirconvergence",
        "booktitle": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "doi": "10.1145/3447548.3470799",
        "author": [
            "Lee, Jae-Gil",
            "Roh, Yuji",
            "Song, Hwanjun",
            "Whang, Steven"
        ],
        "keywords": [
            "machine learning, robustness, convergence, fairness"
        ],
        "abstract": "Responsible AI becomes critical where robustness and fairness must be satisfied together. Traditionally, the two topics have been studied by different communities for different applications. Robust training is designed for noisy or poisoned data where image data is typically considered. In comparison, fair training primarily deals with biased data where structured data is typically considered. Nevertheless, robust training and fair training are fundamentally similar in considering that both of them aim at fixing the inherent flaws of real-world data. In this tutorial, we first cover state-of-the-art robust training techniques where most of the research is on combating various label noises. In particular, we cover label noise modeling, robust training approaches, and real-world noisy data sets. Then, proceeding to the related fairness literature, we discuss pre-processing, in-processing, and post-processing unfairness mitigation techniques, depending on whether the mitigation occurs before, during, or after the model training. Finally, we cover the recent trend emerged to combine robust and fair training in two flavors: the former is to make the fair training more robust (i.e., robust fair training), and the latter is to consider robustness and fairness as two equals to incorporate them into a holistic framework. This tutorial is indeed timely and novel because the convergence of the two topics is increasingly common, but yet to be addressed in tutorials. The tutors have extensive experience publishing papers in top-tier machine learning and data mining venues and developing machine learning platforms.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383325",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "towardsfairfederatedlearning",
        "booktitle": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "doi": "10.1145/3447548.3470814",
        "author": [
            "Zhou, Zirui",
            "Chu, Lingyang",
            "Liu, Changxin",
            "Wang, Lanjun",
            "Pei, Jian",
            "Zhang, Yong"
        ],
        "keywords": [
            "distributed learning, model fairness, data privacy, collaborative fairness, data leakage, federated learning"
        ],
        "abstract": "Federated learning has become increasingly popular as it facilitates collaborative training of machine learning models among multiple clients while preserving their data privacy. In practice, one major challenge for federated learning is to achieve fairness in collaboration among the participating clients, because different clients' contributions to a model are usually far from equal due to various reasons. Besides, as machine learning models are deployed in more and more important applications, how to achieve model fairness, that is, to ensure that a trained model has no discrimination against sensitive attributes, has become another critical desiderata for federated learning. In this tutorial, we discuss formulations and methods such that collaborative fairness, model fairness, and privacy can be fully respected in federated learning. We review the existing efforts and the latest progress, and discuss a series of potential directions.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390071",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "systemframeworkofintelligentconsultingsystemswithintellectualtechnology",
        "booktitle": "Proceedings of the 9th International Conference on Computer and Communications Management",
        "doi": "10.1145/3479162.3479167",
        "author": [
            "Suaprae, Phanintorn",
            "Nilsook, Prachyanun",
            "Wannapiroon, Panita"
        ],
        "keywords": [
            ""
        ],
        "abstract": "The purposes of this research were: 1) Analyze factors affecting the student retention of higher education students, 2) Develop intelligent consulting system models with intellectual technology for the student retention of higher education students, 3) Design intelligent consulting system architecture with intellectual technology for the student retention of higher education students, 4) Develop intelligent consulting systems with intellectual technology for the student retention of higher education students, and 5) Study the results of intelligent consultation systems with intellectual technology for the student retention of higher education students. An intelligent counseling system with intellectual technology for the student retention of higher education students is a system that can reduce students' mid-exit rates and increase student retention rates. The research has synthesized analysis of factors that affect Student retention applied to Cognitive technology, machine learning can provide accurate student retention forecasts. Counselors can know before students drop out.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383615",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "visualanalyticsforlargenetworkstheoryartandpractice",
        "booktitle": "ACM SIGGRAPH 2021 Courses",
        "doi": "10.1145/3450508.3464558",
        "author": [
            "Bednarz, Tomasz",
            "Hughes, Rowan",
            "Mathews, Alex",
            "Chen, Dawei",
            "Zhu, Liming",
            "Filonik, Daniel"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383325",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "allyouneedtoknowtobuildaproductknowledgegraph",
        "booktitle": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining",
        "doi": "10.1145/3447548.3470825",
        "author": [
            "Zalmout, Nasser",
            "Zhang, Chenwei",
            "Li, Xian",
            "Liang, Yan",
            "Dong, Xin"
        ],
        "keywords": [
            "taxonomy, data cleaning, information extraction, knowledge graphs"
        ],
        "abstract": "Knowledge graphs have been pivotal in supporting downstream applications like search, recommendation, and question answering, among others. Therefore, knowledge graphs have naturally become key enabling technologies in e-Commerce platforms. Developing a high coverage product knowledge graph is more challenging than generic knowledge graphs. The highly specific and complex domain, the sparsity of training data, along with the dynamic taxonomies and product types, can constrain the resulting knowledge graphs. In this tutorial we present best practices and ML innovations in industry towards building a scalable product knowledge graph. Contributions in this domain benefit from the general literature in areas including information extraction and data mining, tailored to address the specific characteristics of e-Commerce platforms.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390200",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "abuildingintegratedcontrolplatformorientedtowardsintelligentbuilding",
        "booktitle": "2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "doi": "10.1145/3469213.3470424",
        "author": [
            "Yan, Feng"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389242",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "towardsintelligentfeatureengineeringforriskbasedcustomersegmentationinbanking",
        "booktitle": "Proceedings of the 18th International Conference on Advances in Mobile Computing &amp; Multimedia",
        "doi": "10.1145/3428690.3429172",
        "author": [
            "Khadivizand, Sam",
            "Beheshti, Amin",
            "Sobhanmanesh, Fariborz",
            "Sheng, Quan",
            "Istanbouli, Elias",
            "Wood, Steven",
            "Pezaro, Damon"
        ],
        "keywords": [
            "risk-based customer segmentation, business process, feature engineering, banking processes"
        ],
        "abstract": "Business Processes, i.e., a set of coordinated tasks and activities to achieve a business goal, and their continuous improvements are key to the operation of any organization. In banking, business processes are increasingly dynamic as various technologies have made dynamic processes more prevalent. For example, customer segmentation, i.e., the process of grouping related customers based on common activities and behaviors, could be a data-driven and knowledge-intensive process. In this paper, we present an intelligent data-driven pipeline composed of a set of processing elements to move customers' data from one system to another, transforming the data into the contextualized data and knowledge along the way. The goal is to present a novel intelligent customer segmentation process which automates the feature engineering, i.e., the process of using (banking) domain knowledge to extract features from raw data via data mining techniques, in the banking domain. We adopt a typical scenario for analyzing customer transaction records, to highlight how the presented approach can significantly improve the quality of risk-based customer segmentation in the absence of feature engineering.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383196",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "aflexiblesecurityanalyticsservicefortheindustrialiot",
        "booktitle": "Proceedings of the 2021 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems",
        "doi": "10.1145/3445969.3450427",
        "author": [
            "Empl, Philip",
            "Pernul, G\\\"{u}nther"
        ],
        "keywords": [
            "security as a service, industrial IoT, security analytics"
        ],
        "abstract": "In Cloud Computing, the cloud serves as a central data hub for the Industrial Internet of Things' (IIoT) data and is deployed in diverse application fields, e.g., Smart Grid or Smart Manufacturing. Therefore, the aggregated and contextualized data is bundled in a central data hub, bringing tremendous cybersecurity advantages. Given the threat landscape in IIoT systems, especially SMEs (small and medium-sized enterprises) need to be prepared regarding their cybersecurity, react quickly, and strengthen their overall cybersecurity. For instance, with the application of machine learning algorithms, security-related data can be analyzed predictively in order to be able to ward off a potential attack at an early stage. Since modern reference architectures for IIoT systems, such as RAMI 4.0 or IIRA, consider cybersecurity approaches on a high level and SMEs lack financial funds and knowledge, this paper conceptualizes a security analytics service used as a security add-on to these reference architectures. Thus, this paper conceptualizes a flexible security analytics service that implements security capabilities with flexible analytical techniques that fit specific SMEs' needs. The security analytics service is also evaluated with a real-world use case.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450391238",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "fairinterfacesforgeospatialscientificdatasearches",
        "booktitle": "Proceedings of the 1st ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data",
        "doi": "10.1145/3486640.3491391",
        "author": [
            "Devarakonda, Ranjeet",
            "Guntupally, Kavya",
            "Thornton, Michele",
            "Wei, Yaxing",
            "Singh, Debjani",
            "Lunga, Dalton"
        ],
        "keywords": [
            "ARM Data Center, ORNL DAAC, FAIR data principle for scientific data, Geospatial search interfaces"
        ],
        "abstract": "Several factors must be considered in designing a highly accurate, reliable, scalable, and user-friendly geospatial data search interfaces. This paper examines four critical questions that ought to be considered during design phase: (1) Is the search interface or API that provides the search capability useable by both humans and machines? (2) Are the results consistent and reliable? (3) Is the output response format free to use, community-defined, and non-propriety? (4) Does the API clearly state the usage clauses? This paper discusses how certain data repositories at the US Department of Energy's Oak Ridge National Laboratory apply FAIR data principles to enable geospatial searches and address the above-mentioned questions.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450388825",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "modelofbusinessintelligenceappliedtheprincipleofcooperativesocietyinthebusinessforums",
        "booktitle": "2021 10th International Conference on Software and Computer Applications",
        "doi": "10.1145/3457784.3457820",
        "author": [
            "Al-Khowarizmi, Al-Khowarizmi",
            "Lubis, Muharman",
            "Ridho, Arif",
            "Fauzi, Fauzi",
            "Ramadhan, Ilham"
        ],
        "keywords": [
            "Business Intelligence, Cooperative Society, Business Forum, Model and Simulation"
        ],
        "abstract": "Business forums are activities between individuals and organizations that carry out the transactions on online media or within applications, which spread across countries. Along with the development of information technology towards business intelligence (BI), the business processes carried out in the business forum are modeled specifically in order to create an effort and attempt to follow the indicator and criteria from the industrial revolution 4.0. In this paper, a model is designed to combine three type of principles, namely the business forum, BI and the cooperative principle. Actually, cooperatives have been long abandoned since the existence of conventional and Islamic banking concept but it has kinship principle to divide the profits based on the size of the contribution given. Meanwhile, BI model is designed to obtain a formula from the cooperative principle, namely the residual income from operations where the transaction process is successfully implemented through the application to allocate a portion of the profits to the members based on the specified percent.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10725520",
        "isbn": null,
        "journal": "Interactions",
        "publisher": "Association for Computing Machinery",
        "title": "seeinglikeadatasetfromtheglobalsouth",
        "booktitle": null,
        "doi": "10.1145/3466160",
        "author": [
            "Sambasivan, Nithya"
        ],
        "keywords": [
            ""
        ],
        "abstract": "This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,247"
    },
    {
        "issnkey": "",
        "isbn": "9781450390255",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "researchonhighreliabilityintelligentsensinghealthservicesupportplatformandkeytechnologiesbasedonbiometricsandblockchainsecuritytechnology",
        "booktitle": "2021 4th International Conference on Information Systems and Computer Aided Education",
        "doi": "10.1145/3482632.3487461",
        "author": [
            "Tang, Xinzhong",
            "Zhuang, Bing",
            "Yao, Ying",
            "Dong, Xuesong"
        ],
        "keywords": [
            "blockchain, High-reliability, health services, deep learning, biological characteristics"
        ],
        "abstract": "A new type of high-reliability intelligent-sensing health service support platform and its key technologies are introduced in this article. By the technologies of automatic data collection, perceptual data removal, abnormal data detection, perceptual heterogeneous data identification, it is realizable to collect, analyze and process the health data of users. In order to solve the security problem in the process of data transmission, biometric identification and blockchain are used to realize the high-reliability transmission. At the end of the paper, a high-reliability intelligent-sensing health service support platform is built. And the implementation and service support process are expounded, indicating that the platform has high practical value.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390255",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "statisticsandmininganalysisoflightningmonitoringdatainpowergridbasedonclassicalmetrologymodel",
        "booktitle": "2021 4th International Conference on Information Systems and Computer Aided Education",
        "doi": "10.1145/3482632.3484010",
        "author": [
            "Yang, Rui"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Lightning, also known as lightning, is a strong catastrophic discharge phenomenon between clouds and between clouds and the ground in the process of atmospheric convection. There are two types: cloud flash (between clouds) and ground flash (between clouds and the earth). Lightning location system is a system that uses telemetry technology to monitor lightning activities in full-automatic, large-area, high-precision, continuous and real-time. By analyzing lightning location data collected for a long time, lightning accident points can be quickly located, the distribution of regional lightning activities can be counted, the development trend can be analyzed, and early warning can be carried out, which can provide reference for lightning protection research of ground buildings, thus reducing the harm to human activities. In this paper, the grid method is used to store and query lightning data based on the classical measurement model. Taking the lightning protection technology of transmission network as an example, the method and application of statistics and mining of lightning monitoring data in power grid are studied.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10725520",
        "isbn": null,
        "journal": "Interactions",
        "publisher": "Association for Computing Machinery",
        "title": "uxofdatamakingdataavailabledoesntmakeitusable",
        "booktitle": null,
        "doi": "10.1145/3448888",
        "author": [
            "Koesten, Laura",
            "Simperl, Elena"
        ],
        "keywords": [
            ""
        ],
        "abstract": "This forum provides a space to engage with the challenges of designing for intelligent algorithmic experiences. We invite articles that tackle the tensions between research and practice when integrating AI and UX design. We welcome interdisciplinary debate, artful critique, forward-looking research, case studies of AI in practice, and speculative design explorations. --- Juho Kim and Henriette Cramer, Editors",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,247"
    },
    {
        "issnkey": "",
        "isbn": "9781450389914",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "datamanagementinthedatalakeasystematicmapping",
        "booktitle": "Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "doi": "10.1145/3472163.3472173",
        "author": [
            "Zouari, Firas",
            "Kabachi, Nadia",
            "Boukadi, Khouloud",
            "Ghedira, Chirine"
        ],
        "keywords": [
            "Systematic mapping, Data lake, Data management"
        ],
        "abstract": "The computer science community is paying more and more attention to data due to its crucial role in performing analysis and prediction. Researchers have proposed many data containers such as files, databases, data warehouses, cloud systems, and recently data lakes in the last decade. The latter enables holding data in its native format, making it suitable for performing massive data prediction, particularly for real-time application development. Although data lake is well adopted in the computer science industry, its acceptance by the research community is still in its infancy stage. This paper sheds light on existing works for performing analysis and predictions on data placed in data lakes. Our study reveals the necessary data management steps, which need to be followed in a decision process, and the requirements to be respected, namely curation, quality evaluation, privacy-preservation, and prediction. This study aims to categorize and analyze proposals related to each step mentioned above.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03600300",
        "isbn": null,
        "journal": "ACM Comput. Surv.",
        "publisher": "Association for Computing Machinery",
        "title": "blockchainempowereddatadrivennetworksasurveyandoutlook",
        "booktitle": null,
        "doi": "10.1145/3446373",
        "author": [
            "Li, Xi",
            "Wang, Zehua",
            "Leung, Victor",
            "Ji, Hong",
            "Liu, Yiming",
            "Zhang, Heli"
        ],
        "keywords": [
            "blockchain, blockchain-empowered data-driven networks, networking technologies, Data-driven networks"
        ],
        "abstract": "The paths leading to future networks are pointing towards a data-driven paradigm to better cater to the explosive growth of mobile services as well as the increasing heterogeneity of mobile devices, many of which generate and consume large volumes and variety of data. These paths are also hampered by significant challenges in terms of security, privacy, services provisioning, and network management. Blockchain, which is a technology for building distributed ledgers that provide an immutable log of transactions recorded in a distributed network, has become prominent recently as the underlying technology of cryptocurrencies and is revolutionizing data storage and processing in computer network systems. For future data-driven networks (DDNs), blockchain is considered as a promising solution to enable the secure storage, sharing, and analytics of data, privacy protection for users, robust, trustworthy network control, and decentralized routing and resource managements. However, many important challenges and open issues remain to be addressed before blockchain can be deployed widely to enable future DDNs. In this article, we present a survey on the existing research works on the application of blockchain technologies in computer networks and identify challenges and potential solutions in the applications of blockchains in future DDNs. We identify application scenarios in which future blockchain-empowered DDNs could improve the efficiency and security, and generally the effectiveness of network services.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.282",
        "scimago_value": "2,079"
    },
    {
        "issnkey": "",
        "isbn": "9781450390156",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "dataprotectionmeasuresinesocietypolicyimplicationsofbritishdataprotectionacttochina",
        "booktitle": "2021 5th International Conference on E-Society, E-Education and E-Technology",
        "doi": "10.1145/3485768.3485770",
        "author": [
            "Guo, Yuanyuan"
        ],
        "keywords": [
            "E-society, Private information, Public policy, Data protection"
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450385510",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "reductioadabsurdumfromanaloguehypertexttodigitalhumanities",
        "booktitle": "Proceedings of the 32nd ACM Conference on Hypertext and Social Media",
        "doi": "10.1145/3465336.3475107",
        "author": [
            "Nurmikko-Fuller, Terhi",
            "Pickering, Paul"
        ],
        "keywords": [
            "hypertext, linked data, australian history, information aggregation, political history"
        ],
        "abstract": "In this paper we report on a complex and complete archive of historical primary sources that map the political landscape of the anglophone world in the mid-to late 1800s. The ruthless pragmatism applied to the construction of the initial Humanities dataset resulted in an analogue equivalent of a hypertext system, which has already resulted in published academic books and articles. Here, we describe the processes of a current project, which consists of the translation of this analogue information aggregation system into a graph database using Linked Data and semantic Web technologies.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390514",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "towardsimprovingidentityandaccessmanagementwiththeidmsecmanprocessframework",
        "booktitle": "Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "doi": "10.1145/3465481.3470055",
        "author": [
            "P\\\"{o}hn, Daniela",
            "Seeber, Sebastian",
            "Hanauer, Tanja",
            "Ziegler, Jule",
            "Schmitz, David"
        ],
        "keywords": [
            "Server, Security Management, Identity Management, Security"
        ],
        "abstract": "In today\u2019s networks, administrative access to Linux servers is commonly managed by Privileged Access Management (PAM). It is not only important to monitor these privileged accounts, but also to control segregation of duty and detect keys as well as accounts that potentially bypass PAM. Unprohibited access can become a business risk. In order to improve the security in a controlled manner, we establish IdMSecMan, a security management process tailored for identity and access management (IAM). Security management processes typically use the Deming Cycle or an adaption for continuous improvements of products, services, or processes within the network infrastructure. We adjust a security management process with visualization for IAM, which also shifts the focus from typical assets to the attacker. With the controlled cycles, the maturity of IAM is measured and can continually advance. This paper presents and applies the work in progress IdMSecMan to a motivating scenario in the field of Linux server. We evaluate our approach in a controlled test environment with first steps to roll it out in our data center. Last but not least, we discuss challenges and future work.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21508097",
        "isbn": null,
        "journal": "Proc. VLDB Endow.",
        "publisher": "VLDB Endowment",
        "title": "perfguarddeployingmlforsystemswithoutperformanceregressionsalmost",
        "booktitle": null,
        "doi": "10.14778/3484224.3484233",
        "author": [
            "Ammerlaan, Remmelt",
            "Antonius, Gilbert",
            "Friedman, Marc",
            "Hossain, H",
            "Jindal, Alekh",
            "Orenberg, Peter",
            "Patel, Hiren",
            "Qiao, Shi",
            "Ramani, Vijay",
            "Rosenblatt, Lucas",
            "Roy, Abhishek",
            "Shaffer, Irene",
            "Srinivasan, Soundarajan",
            "Weimer, Markus"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Modern data processing systems require optimization at massive scale, and using machine learning to optimize these systems (ML-for-systems) has shown promising results. Unfortunately, ML-for-systems is subject to over generalizations that do not capture the large variety of workload patterns, and tend to augment the performance of certain subsets in the workload while regressing performance for others. In this paper, we introduce a performance safeguard system, called PerfGuard, that designs pre-production experiments for deploying ML-for-systems. Instead of searching the entire space of query plans (a well-known, intractable problem), we focus on query plan deltas (a significantly smaller space). PerfGuard formalizes these differences, and correlates plan deltas to important feedback signals, like execution cost. We describe the deep learning architecture and the end-to-end pipeline in PerfGuard that could be used with general relational databases. We show that this architecture improves on baseline models, and that our pipeline identifies key query plan components as major contributors to plan disparity. Offline experimentation shows PerfGuard as a promising approach, with many opportunities for future improvement.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.047",
        "scimago_value": "0,946"
    },
    {
        "issnkey": "",
        "isbn": "9781450389853",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "digitaltwinbasedthreedimensionalvisualandglobalmonitoringofassemblyshopfloor",
        "booktitle": "The 5th International Conference on Computer Science and Application Engineering",
        "doi": "10.1145/3487075.3487147",
        "author": [
            "Zhang, Jiapeng",
            "Zhuang, Cunbo",
            "Liu, Jianhua",
            "Yuan, Kun",
            "Zhang, Jin",
            "Liu, Juan"
        ],
        "keywords": [
            "Assembly shop-floor, 3D visual monitoring, Global monitoring, Digital twin"
        ],
        "abstract": "Aiming at the requirements of rapid response and production efficiency improvement in the assembly shop-floor, a three-dimensional (3D) visual and global monitoring method for the assembly shop-floor based on the digital twin is proposed. The paper analyzes the monitoring objectives, objects, and methods of assembly shop-floor, and constructs a global monitoring framework of digital twin-based assembly shop-floor. Then, three key technologies of realizing global monitoring shop-floor are described: current-time data perception and collection, current-time information-driven digital twin generation, and state monitoring and optimization based on digital twin. Finally, a global monitoring prototype system is designed and developed to verify the effectiveness of the proposed method.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01635808",
        "isbn": null,
        "journal": "SIGMOD Rec.",
        "publisher": "Association for Computing Machinery",
        "title": "reportonthethirdinternationalworkshoponsemanticwebmeetshealthdatamanagementswh2020",
        "booktitle": null,
        "doi": "10.1145/3503780.3503792",
        "author": [
            "Kondylakis, Haridimos",
            "Stefanidis, Kostas",
            "Rao, Praveen"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Creating a holistic view of patient data comes with many challenges but also brings many benefits for disease prediction, prevention, diagnosis, and treatment. Especially in the COVID-19 era, this is more important than ever before. The third International Workshop on Semantic Web Meets Health Data Management (SWH) was aimed at bringing together an interdisciplinary audience who was interested in the fields of Semantic Web, data management, and health informatics. The workshop goal was to discuss the challenges in healthcare data management and to propose new solutions for the next generation of data-driven healthcare systems. In this article, we summarize the outcomes of the workshop, and we present a number of key observations and research directions that emerged from presentations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "0.775",
        "scimago_value": "0,372"
    },
    {
        "issnkey": "03600300",
        "isbn": null,
        "journal": "ACM Comput. Surv.",
        "publisher": "Association for Computing Machinery",
        "title": "mobilitytraceanalysisforintelligentvehicularnetworksmethodsmodelsandapplications",
        "booktitle": null,
        "doi": "10.1145/3446679",
        "author": [
            "Celes, Clayson",
            "Boukerche, Azzedine",
            "Loureiro, Antonio"
        ],
        "keywords": [
            "data mining, data analysis, routing, mobility, vanet, Vehicular networks, topology, survey"
        ],
        "abstract": "Intelligent vehicular networks emerge as a promising technology to provide efficient data communication in transportation systems and smart cities. At the same time, the popularization of devices with attached sensors has allowed the obtaining of a large volume of data with spatiotemporal information from different entities. In this sense, we are faced with a large volume of vehicular mobility traces being recorded. Those traces provide unprecedented opportunities to understand the dynamics of vehicular mobility and provide data-driven solutions. In this article, we give an overview of the main publicly available vehicular mobility traces; then, we present the main issues for preprocessing these traces. Also, we present the methods used to characterize and model mobility data. Finally, we review existing proposals that apply the hidden knowledge extracted from the mobility trace for vehicular networks. This article provides a survey on studies that use vehicular mobility traces and provides a guideline for the proposition of data-driven solutions in the domain of vehicular networks. Moreover, we discuss open research problems and give some directions to undertake them.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.282",
        "scimago_value": "2,079"
    },
    {
        "issnkey": "26921626",
        "isbn": null,
        "journal": "Digital Threats",
        "publisher": "Association for Computing Machinery",
        "title": "investigatingsharingofcyberthreatintelligenceandproposinganewdatamodelforenablingautomationinknowledgerepresentationandexchange",
        "booktitle": null,
        "doi": "10.1145/3458027",
        "author": [
            "Bromander, Siri",
            "Swimmer, Morton",
            "Muller, Lilly",
            "J\\o{}sang, Audun",
            "Eian, Martin",
            "Skj\\o{}tskift, Geir",
            "Borg, Fredrik"
        ],
        "keywords": [
            "security, ontology, Cyber threat intelligence, knowledge graph"
        ],
        "abstract": "For a strong, collective defense in the digital domain, we need to produce, consume, analyze, and share cyber threat intelligence. With an increasing amount of available information, we need automation to ensure adequate efficiency. We present the results from a questionnaire investigating the use of standards and standardization and how practitioners share and use cyber threat intelligence (CTI). We propose a strict data model for CTI that enables consumption of all relevant data, data validation, and analysis of consumed content. The main contribution of this article is insight into how CTI is shared and used by practitioners, and the strictness of the data model that enforces input of information and enables automation and deduction of new knowledge.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450391207",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "vtsvaprivacypreservingvehicletrajectorysimulationandvisualizationplatformusingdeepreinforcementlearning",
        "booktitle": "Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery",
        "doi": "10.1145/3486635.3491073",
        "author": [
            "Rao, Jinmeng",
            "Gao, Song",
            "Zhu, Xiaojin"
        ],
        "keywords": [
            "transportation, vehicle trajectory, data visualization, privacy protection, reinforcement learning"
        ],
        "abstract": "Trajectory data is among the most sensitive data and the society increasingly raises privacy concerns. In this demo paper, we present a privacy-preserving Vehicle Trajectory Simulation and Visualization (VTSV) web platform (demo video: https://youtu.be/NY5L4bu2kTU), which automatically generates navigation routes between given pairs of origins and destinations and employs a deep reinforcement learning model to simulate vehicle trajectories with customized driving behaviors such as normal driving, overspeed, aggressive acceleration, and aggressive turning. The simulated vehicle trajectory data contain high-sample-rate of attributes including GPS location, speed, acceleration, and steering angle, and such data are visualized in VTSV using streetscape.gl, an autonomous driving data visualization framework. Location privacy protection methods such as origin-destination geomasking and trajectory k-anonymity are integrated into the platform to support privacy-preserving trajectory data generation and publication. We design two application scenarios to demonstrate how VTSV performs location privacy protection and customize driving behavior, respectively. The demonstration shows that VTSV is able to mitigate data privacy, sparsity, and imbalance sampling issues, which offers new insights into driving trajectory simulation and GeoAI-powered privacy-preserving data publication.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389228",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "onarchitectureofegovernmentecosystemsfromeservicestoeparticipationiiwas2020keynote",
        "booktitle": "Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services",
        "doi": "10.1145/3428757.3429972",
        "author": [
            "Draheim, Dirk"
        ],
        "keywords": [
            "e-government, e-governance, public key infrastructures, digital transformation, X-Road, data governance, GAIA-X, persistent messaging, data exchange layers, Fiware, consent management"
        ],
        "abstract": "The \"digital transformation\" is perceived as the key enabler for increasing wealth and well-being by many in politics, media and among the citizens alike. In the same vein, e-Government steadily received and receives more and more attention. e-Government gives rise to complex, large-scale system landscapes consisting of many players and technological systems - and we call such system landscapes e-Government ecosystems. In this talk, we are interested in the architecture of e-Government ecosystems. \"Form ever follows function.\" Now, what is the function that determines e-Government? And what is the form in which it manifests? After briefly reviewing the purpose of e-Government from a democratic as well as a technocratic viewpoint, we will discover the primacy of the state's institutional design in the architecture of e-Government ecosystems. From there, we will arrive at the notion of data governance architecture, which provides the core of all system design efforts in e-Government. A data governance architecture maps data assets to accountable legal entities and represents the essence of co-designing institutions and technological systems. Against the background of what has been achieved, we review a series of established and emerging technologies that have been explicitly designed for or are otherwise relevant for building e-Government systems.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383431",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "aimeetsdatabaseai4dbanddb4ai",
        "booktitle": "Proceedings of the 2021 International Conference on Management of Data",
        "doi": "10.1145/3448016.3457542",
        "author": [
            "Li, Guoliang",
            "Zhou, Xuanhe",
            "Cao, Lei"
        ],
        "keywords": [
            "models, machine learning, database, AI"
        ],
        "abstract": "Database and Artificial Intelligence (AI) can benefit from each other. On one hand, AI can make database more intelligent (AI4DB). For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, learning-based techniques can alleviate this problem. On the other hand, database techniques can optimize AI models (DB4AI). For example, AI is hard to deploy, because it requires developers to write complex codes and train complicated models. Database techniques can be used to reduce the complexity of using AI models, accelerate AI algorithms and provide AI capability inside databases. DB4AI and AI4DB have been extensively studied recently. In this tutorial, we review existing studies on AI4DB and DB4AI. For AI4DB, we review the techniques on learning-based database configuration, optimization, design, monitoring, and security. For DB4AI, we review AI-oriented declarative language, data governance, training acceleration, and inference acceleration. Finally, we provide research challenges and future directions in AI4DB and DB4AI.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390231",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "potentialenergysavingestimationforretrofitbuildingwithashraegreatenergypredictoriiiusingmachinelearning",
        "booktitle": "Proceedings of the 2021 1st International Conference on Control and Intelligent Robotics",
        "doi": "10.1145/3473714.3473788",
        "author": [
            "Zhang, Jiamin"
        ],
        "keywords": [
            "green architecture, retrofit building, machine learning, energy saving"
        ],
        "abstract": "Energy is material basis for social development and conflicting issue of economic development around the world. With the continuous urbanization, the energy consumption of buildings will be further increased, accounting for about 40% of the total energy consumption eventually. Thus, enhancement of energy efficiency of the buildings has become an essential issue to reduce the amount of gas emission as well as fossil fuel consumption. Delivering a high-quality built environment in an energy efficient way is the crucial key to energy conservation. Energy efficiency retrofit for buildings is considered to be a promising way to achieve energy savings. Machine learning provides the ability to learn from data using multiple computer algorithms. This paper introduces several algorithms, including random forest and Light GBM, to analyze building energy consumption based on the data from Kaggle competition, providing discussion of improvement in model efficiency and economic analysis by simulating different scenarios. In addition, sensitivity analysis is conducted to show the influence of different parameters in models and metrics to quantify the accuracy of prediction are proposed. The results of this paper can help people understand quantitative influence of different variables on energy use and energy baseline models. Future works will incorporate more data type in order to enhance the performance of prediction.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicalissuesinmachinelearning",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447415",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "19361955",
        "isbn": null,
        "journal": "J. Data and Information Quality",
        "publisher": "Association for Computing Machinery",
        "title": "experiencealgorithmsandcasestudyforexplainingrepairswithuniformprofilesoveriotdata",
        "booktitle": null,
        "doi": "10.1145/3436239",
        "author": [
            "Liu, Zhicheng",
            "Zhang, Yang",
            "Huang, Ruihong",
            "Chen, Zhiwei",
            "Song, Shaoxu",
            "Wang, Jianmin"
        ],
        "keywords": [
            "data profiling, time series, Outlier explanation, outlier repairs"
        ],
        "abstract": "IoT data with timestamps are often found with outliers, such as GPS trajectories or sensor readings. While existing systems mostly focus on detecting temporal outliers without explanations and repairs, a decision maker may be more interested in the cause of the outlier appearance such that subsequent actions would be taken, e.g., cleaning unreliable readings or repairing broken devices or adopting a strategy for data repairs. Such outlier detection, explanation, and repairs are expected to be performed in either offline (batch) or online modes (over streaming IoT data with timestamps). In this work, we present TsClean, a new prototype system for detecting and repairing outliers with explanations over IoT data. The framework defines uniform profiles to explain the outliers detected by various algorithms, including the outliers with variant time intervals, and take approaches to repair outliers. Both batch and streaming processing are supported in a uniform framework. In particular, by varying the block size, it provides a tradeoff between computing the accurate results and approximating with efficient incremental computation. In this article, we present several case studies of applying TsClean in industry, e.g., how this framework works in detecting and repairing outliers over excavator water temperature data, and how to get reasonable explanations and repairs for the detected outliers in tracking excavators.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": null,
        "journal": "Proc. ACM Hum.-Comput. Interact.",
        "publisher": "Association for Computing Machinery",
        "title": "speculativedataworkampdashboardsdesigningalternativedatavisions",
        "booktitle": null,
        "doi": "10.1145/3434173",
        "author": [
            "Hockenhull, Michael",
            "Cohn, Marisa"
        ],
        "keywords": [
            "data visualization, speculative design, ethnography, business intelligence, data work"
        ],
        "abstract": "This paper studies data work in an organizational context, and suggests speculative data work as a useful concept and the speculative dashboard as a design concept, to better understand and support cooperative work. Drawing on fieldwork in a Danish public sector organisation, the paper identifies and conceptualizes the speculative data work performed around processes of digitalization and the push to become data-driven. The speculative dashboard is proposed as a design concept and opportunity for design, using practices from speculative design and research to facilitate speculation about data?its sources, visualizations, practices and infrastructures. It does so by hacking the 'genre' of the business intelligence data dashboard, and using it as a framework for the juxtaposition of different kinds of data, facilitating and encouraging speculation on alternative visions for data types and use. The paper contributes an empirical study of organizational use of and attitudes towards data, informing a novel design method and concept for co-speculating on alternative visions of and for organizational data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "24712566",
        "isbn": null,
        "journal": "ACM Trans. Priv. Secur.",
        "publisher": "Association for Computing Machinery",
        "title": "anovelhybridapproachformultidimensionaldataanonymizationforapachespark",
        "booktitle": null,
        "doi": "10.1145/3484945",
        "author": [
            "Bazai, Sibghat",
            "Jang-Jaccard, Julian",
            "Alavizadeh, Hooman"
        ],
        "keywords": [
            "Spark, resilient distributed dataset (RDD), data anonymization, multi-dimensional data, Mondrian"
        ],
        "abstract": "Multi-dimensional data anonymization approaches (e.g., Mondrian) ensure more fine-grained data privacy by providing a different anonymization strategy applied for each attribute. Many variations of multi-dimensional anonymization have been implemented on different distributed processing platforms (e.g., MapReduce, Spark) to take advantage of their scalability and parallelism supports. According to our critical analysis on overheads, either existing iteration-based or recursion-based approaches do not provide effective mechanisms for creating the optimal number of and relative size of resilient distributed datasets (RDDs), thus heavily suffer from performance overheads. To solve this issue, we propose a novel hybrid approach for effectively implementing a multi-dimensional data anonymization strategy (e.g., Mondrian) that is scalable and provides high-performance. Our hybrid approach provides a mechanism to create far fewer RDDs and smaller size partitions attached to each RDD than existing approaches. This optimal RDD creation and operations approach is critical for many multi-dimensional data anonymization applications that create tremendous execution complexity. The new mechanism in our proposed hybrid approach can dramatically reduce the critical overheads involved in re-computation cost, shuffle operations, message exchange, and cache management.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.909",
        "scimago_value": "0,743"
    },
    {
        "issnkey": "",
        "isbn": "9781450390163",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "atransformerbasedsalespredictionofsmartcontainerinnewretailera",
        "booktitle": "Proceedings of the 2021 5th International Conference on Deep Learning Technologies",
        "doi": "10.1145/3480001.3480017",
        "author": [
            "Jin, Ying",
            "Gao, Ming",
            "Yu, Jixiang"
        ],
        "keywords": [
            "Smart container, Transformer, Sales prediction"
        ],
        "abstract": "With the advent of the new retail era, the value of unmanned smart container is increasingly prominent. Fast and flexible self-service is favored by consumers. How to use accumulated historical sales data to predict sales in the future is an important part of smart container operation management. Reasonable sales prediction can not only reduce the inventory cost, but also reduce the shortage loss of the container. Based on the smart container sales data of Dalian Xiaode New Retail Co., Ltd., through detailed exploratory analysis in many aspects, this paper carries out the feature selection of sales prediction, and uses random forest, XGBoost, Transformer and other algorithms to predict sales. The experimental results show that the prediction accuracy of Transformer is better than traditional algorithms, whose MAPE is 14.67% lower than that of the worst one. Transformer can be well applied in the field of sales prediction of smart container. And in this experiment, compared with Transformer using sine and cosine functions for positional encoding, Transformer encoded by position index has better prediction performance and stronger stability.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "introduction",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447406",
        "author": [
            "Eslambolchilar, Parisa",
            "Komninos, Andreas",
            "Dunlop, Mark"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21606455",
        "isbn": null,
        "journal": "ACM Trans. Interact. Intell. Syst.",
        "publisher": "Association for Computing Machinery",
        "title": "towardresponsibleaianoverviewoffederatedlearningforusercenteredprivacypreservingcomputing",
        "booktitle": null,
        "doi": "10.1145/3485875",
        "author": [
            "Yang, Qiang"
        ],
        "keywords": [
            "machine learning, decentralized AI, user privacy, blockchain, privacy-preserving computing, Federated learning, data security, responsible AI"
        ],
        "abstract": "With the rapid advances of Artificial Intelligence (AI) technologies and applications, an increasing concern is on the development and application of responsible AI technologies. Building AI technologies or machine-learning models often requires massive amounts of data, which may include sensitive, user private information to be collected from different sites or countries. Privacy, security, and data governance constraints rule out a brute force process in the acquisition and integration of these data. It is thus a serious challenge to protect user privacy while achieving high-performance models. This article reviews recent progress of federated learning in addressing this challenge in the context of privacy-preserving computing. Federated learning allows global AI models to be trained and used among multiple decentralized data sources with high security and privacy guarantees, as well as sound incentive mechanisms. This article presents the background, motivations, definitions, architectures, and applications of federated learning as a new paradigm for building privacy-preserving, responsible AI ecosystems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390200",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "researchandapplicationofdigitalcollectionmethodofhumanmovement",
        "booktitle": "2021 2nd International Conference on Artificial Intelligence and Information Systems",
        "doi": "10.1145/3469213.3470272",
        "author": [
            "Hu, Yerong",
            "He, Xiangzhen",
            "Zhang, Yihao",
            "Zeng, Jia",
            "Yang, Huaiyuan",
            "Zhou, Shuaihang"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384070",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "camerabasedvegetationindexfromunmannedaerialvehicles",
        "booktitle": "6th International Conference on Sustainable Information Engineering and Technology 2021",
        "doi": "10.1145/3479645.3479661",
        "author": [
            "Kusnandar, Toni",
            "Surendro, Kridanto"
        ],
        "keywords": [
            "Unmanned Aerial Vehicle, Image Processing, Vegetation Index, Precission Agriculture"
        ],
        "abstract": "Agriculture assumes a vital role in human life because it provides food, feed for livestock, and bioenergy. The agricultural sector is expected to meet the needs of secure and nutritious food for the community at all times to boost productivity. Providing nutrition, water and light precisely and measuredly is an important effort in plant cultivation to produce quality. This effort can be materialized by implementing smart farming involving devices and information technology. Vast field surveillance or monitoring is made easy with the advent of unmanned aerial vehicle (UAV). Detection of plant condition can be achieved by obtaining Vegetation Index (VI) through camera imaging in UAVs which are more economic compared to multispectral or hyperspectral cameras. This study aims to obtain VI that is accurate but still economical, so that it can be utilized even by small-scale agriculture. The work that will be done is to conduct repair experiments at several stages of image processing to produce a new, more accurate VI. The research stages started from experiments on previous research, to finding new research opportunities in VI. Furthermore, the experiment was carried out with the addition of white balance value parameters and other UAV sensor parameters at the Pre-Processing stage to improve its quality. The hypothesis of adding white balance parameters should prove to be more accurate in correcting shooting in various light conditions. Next, try to modify the feature extraction algorithm using Color Extraction Edge Detection. Followed by modifying it using Back Propagation Neural Network to increase accuracy at the image processing stage. After synthesizing some of these experiments, a new formula or model VI using the camera on the UAV is expected to be produced. This research will contribute to the modification of methods or algorithms at the image processing stage to produce a corrected image in producing a new VI that is more accurate using a camera on a more economical UAV.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389884",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "multiscaleunetencoderdecodernetworkforbuildingextraction",
        "booktitle": "2021 3rd International Conference on Information Technology and Computer Communications",
        "doi": "10.1145/3473465.3473478",
        "author": [
            "Sun, Xiyan",
            "Xiao, Yu",
            "Ji, Yuanfa",
            "Huang, Jianhua",
            "Bai, Yang"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Buildings in remote sensing images have large scale differences and complex shapes. And there are often distractors with visual features similar to buildings in complex scenes. The traditional methods used to extract buildings are limited by the ability of feature representation, resulting in low accuracy and low universality. The semantic segmentation network based on the Encoder-Decoder structure can automatically learn multi-level building feature representation from the data set, and achieve end-to-end building extraction. UNet is a typical semantic segmentation Encoder-Decoder network, but UNet cannot explore enough building information. Small buildings are easy to be missed, large buildings with complex colors and shapes are incompletely extracted, boundary segmentation is inaccurate. And the network is easily affected by roads, trees, shadows and other distractors. Therefore, this article improves UNet and proposes a multi-scale Encoder-Decoder network to learn multi-scale and distinguishable features to better identify buildings and backgrounds. We experiment with the improved network and the classic U-Net on two data sets, and show that the multi-scale Encoder-Decoder network can effectively improve the accuracy of building extraction.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26911957",
        "isbn": null,
        "journal": "ACM Trans. Comput. Healthcare",
        "publisher": "Association for Computing Machinery",
        "title": "mobileandwearablesensingframeworksformhealthstudiesandapplicationsasystematicreview",
        "booktitle": null,
        "doi": "10.1145/3422158",
        "author": [
            "Kumar, Devender",
            "Jeuris, Steven",
            "Bardram, Jakob",
            "Dragoni, Nicola"
        ],
        "keywords": [
            "mobile sensing frameworks, mHealth sensing, mobile sensing, mHealth frameworks, wearable sensing"
        ],
        "abstract": "With the widespread use of smartphones and wearable health sensors, a plethora of mobile health (mHealth) applications to track well-being, run human behavioral studies, and clinical trials have emerged in recent years. However, the design, development, and deployment of mHealth applications is challenging in many ways. To address these challenges, several generic mobile sensing frameworks have been researched in the past decade. Such frameworks assist developers and researchers in reducing the complexity, time, and cost required to build and deploy health-sensing applications. The main goal of this article is to provide the reader with an overview of the state-of-the-art of health-focused generic mobile and wearable sensing frameworks. This review gives a detailed analysis of functional and non-functional features of existing frameworks, the health studies they were used in, and the stakeholders they support. Additionally, we also analyze the historical evolution, uptake, and maintenance after the initial release. Based on this analysis, we suggest new features and opportunities for future generic mHealth sensing frameworks.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03625915",
        "isbn": null,
        "journal": "ACM Trans. Database Syst.",
        "publisher": "Association for Computing Machinery",
        "title": "embeddedfunctionaldependenciesanddatacompletenesstailoreddatabasedesign",
        "booktitle": null,
        "doi": "10.1145/3450518",
        "author": [
            "Wei, Ziheng",
            "Link, Sebastian"
        ],
        "keywords": [
            "Boyce-Codd normal form, key, database design, updates, redundancy, decomposition, synthesis, missing value, functional dependency, normal form, third normal form"
        ],
        "abstract": "We establish a principled schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing redundant data value occurrences that may cause problems with processing data that meets the requirements. We establish axiomatic, algorithmic, and logical foundations for reasoning about embedded functional dependencies. These foundations enable us to introduce generalizations of Boyce-Codd and Third normal forms that avoid processing difficulties of any application data, or minimize these difficulties across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements, and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate the effectiveness of our framework for the acquisition of the constraints, the schema design process, and the performance of the schema designs in terms of updates and join queries.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21606455",
        "isbn": null,
        "journal": "ACM Trans. Interact. Intell. Syst.",
        "publisher": "Association for Computing Machinery",
        "title": "vadafvisualizationforabnormalclientdetectionandanalysisinfederatedlearning",
        "booktitle": null,
        "doi": "10.1145/3426866",
        "author": [
            "Meng, Linhao",
            "Wei, Yating",
            "Pan, Rusheng",
            "Zhou, Shuyue",
            "Zhang, Jianwei",
            "Chen, Wei"
        ],
        "keywords": [
            "Federated learning, anomaly detection, visual analytics"
        ],
        "abstract": "Federated Learning (FL) provides a powerful solution to distributed machine learning on a large corpus of decentralized data. It ensures privacy and security by performing computation on devices (which we refer to as clients) based on local data to improve the shared global model. However, the inaccessibility of the data and the invisibility of the computation make it challenging to interpret and analyze the training process, especially to distinguish potential client anomalies. Identifying these anomalies can help experts diagnose and improve FL models. For this reason, we propose a visual analytics system, VADAF, to depict the training dynamics and facilitate analyzing potential client anomalies. Specifically, we design a visualization scheme that supports massive training dynamics in the FL environment. Moreover, we introduce an anomaly detection method to detect potential client anomalies, which are further analyzed based on both the client model\u2019s visual and objective estimation. Three case studies have demonstrated the effectiveness of our system in understanding the FL training process and supporting abnormal client detection and analysis.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389136",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "sokdnaselforganisedknowledgedefinednetworksarchitectureforreliablerouting",
        "booktitle": "2021 The 4th International Conference on Information Science and Systems",
        "doi": "10.1145/3459955.3460617",
        "author": [
            "Gosh, Saptarshi",
            "EL, Brahim",
            "Dagiuklas, Tasos",
            "Iqbal, Muddesar"
        ],
        "keywords": [
            "Deep Learning, SDN, Routing, SON"
        ],
        "abstract": "\u201cWhen you are destined for an important appoint-ment, you would obviously opt for the most reliable route instead of the shortest in order to be well prepared\u201d. Modern networking is presently undergoing through a quantum leap. To cope up with ambitious demands and user expectations, it is becoming more complex both structurally and functionally. Software Defined Networking (SDN) happens to be an instance of such advancements. It has significantly leveraged the network programmability, abstraction, and automation. Eventually, with acceptance form all major network infrastructure such as 5G and Cloud, SDN is becoming the standard of future networking. Likewise, Machine Learning (ML) has become the trendiest skill-in-demand recently. With its superiority of analyzing data, makes it applicable for almost every possible domain. The attempt to applying the power of ML in networking has not been too long, it allows the network to be more intelligent and capable enough to take optimal decisions to address some of its native problems. This gives rise to Self- Organized Networking (SON). In this article, Routing using Deep Neural Network (DNN) on top of SDN is addressed. We proposed a Self-organized Knowledge Defined Network (SO-KDN) architecture and an intelligent routing algorithm, that reactively finds the most reliable route, i.e., a route having least probability of fluctuation. This reduces network overhead due to re-routing and optimizes traffic congestion. Experimental data show a mean 90% accurate forecast in reliability prediction.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicsandstatistics",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447411",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384735",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "excitingusefulworryingfuturisticpublicperceptionofartificialintelligencein8countries",
        "booktitle": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",
        "doi": "10.1145/3461702.3462605",
        "author": [
            "Kelley, Patrick",
            "Yang, Yongwei",
            "Heldreth, Courtney",
            "Moessner, Christopher",
            "Sedley, Aaron",
            "Kramm, Andreas",
            "Newman, David",
            "Woodruff, Allison"
        ],
        "keywords": [
            "artificial intelligence, public perception"
        ],
        "abstract": "As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": null,
        "journal": "Proc. ACM Hum.-Comput. Interact.",
        "publisher": "Association for Computing Machinery",
        "title": "wiringacityasociotechnicalperspectiveondeployingurbansensornetworks",
        "booktitle": null,
        "doi": "10.1145/3449252",
        "author": [
            "Van, Lucy",
            "Muller, Brian",
            "Voida, Stephen"
        ],
        "keywords": [
            "civic data, sociotechnical system, urban sensor networks, smart cities"
        ],
        "abstract": "We use a sociotechnical perspective to expand upon prior characterizations of deploying end-to-end urban sensor networks that focus primarily on the technical aspects of such systems. Via exploratory, semi-structured interviews with those deploying a number of urban sensor networks in a single American city, we identify ways that human decision-making and collaborative processes influence how these infrastructures are built. We synthesize these findings into a framework in which sociotechnical factors show up across the phases of data collection, management, analysis, and impacts within smart city projects. Each phase can display variability in immediacy, automation, geographic scope, and ownership. Finally, we use our situated work to discuss a generalizable tension within smart city projects between cross-domain data integration and fragmentation and provide implications for CSCW research, the design of smart city data platforms, and municipal policy.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383431",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "realtimedatainfrastructureatuber",
        "booktitle": "Proceedings of the 2021 International Conference on Management of Data",
        "doi": "10.1145/3448016.3457552",
        "author": [
            "Fu, Yupeng",
            "Soman, Chinmay"
        ],
        "keywords": [
            "streaming processing, real-time infrastructure"
        ],
        "abstract": "Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": null,
        "journal": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.",
        "publisher": "Association for Computing Machinery",
        "title": "vifinharnesspassivevibrationtocontinuousmicrofingerwritingwithacommoditysmartwatch",
        "booktitle": null,
        "doi": "10.1145/3448119",
        "author": [
            "Chen, Wenqiang",
            "Chen, Lin",
            "Ma, Meiyi",
            "Parizi, Farshid",
            "Patel, Shwetak",
            "Stankovic, John"
        ],
        "keywords": [
            "vibration intelligence, text input, wearable devices, micro finger writing"
        ],
        "abstract": "Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90% and 91% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26911922",
        "isbn": null,
        "journal": "ACM/IMS Trans. Data Sci.",
        "publisher": "Association for Computing Machinery",
        "title": "tabreformerunsupervisedrepresentationlearningforerroneousdatadetection",
        "booktitle": null,
        "doi": "10.1145/3447541",
        "author": [
            "Nashaat, Mona",
            "Ghosh, Aindrila",
            "Miller, James",
            "Quader, Shaikh"
        ],
        "keywords": [
            "transformers, data augmentation, bidirectional encoder, Error detection"
        ],
        "abstract": "Error detection is a crucial preliminary phase in any data analytics pipeline. Existing error detection techniques typically target specific types of errors. Moreover, most of these detection models either require user-defined rules or ample hand-labeled training examples. Therefore, in this article, we present TabReformer, a model that learns bidirectional encoder representations for tabular data. The proposed model consists of two main phases. In the first phase, TabReformer follows encoder architecture with multiple self-attention layers to model the dependencies between cells and capture tuple-level representations. Also, the model utilizes a Gaussian Error Linear Unit activation function with the Masked Data Model objective to achieve deeper probabilistic understanding. In the second phase, the model parameters are fine-tuned for the task of erroneous data detection. The model applies a data augmentation module to generate more erroneous examples to represent the minority class. The experimental evaluation considers a wide range of databases with different types of errors and distributions. The empirical results show that our solution can enhance the recall values by 32.95% on average compared with state-of-the-art techniques while reducing the manual effort by up to 48.86%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384278",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "thedilemmaofdigitaltransformationofchinashotelindustryandtheconstructionoftechnologyplatformasurveyofhotelsindustryinchina",
        "booktitle": "2021 4th International Conference on Information Management and Management Science",
        "doi": "10.1145/3485190.3485193",
        "author": [
            "Li, Yonghan",
            "Lv, Hongjiang"
        ],
        "keywords": [
            "management dilemma, digital transformation, Chinese hotel groups, technology platform"
        ],
        "abstract": "The digital economy has been a hot spot in social development in recent years, and all walks of life are facing the opportunities and challenges of digital transformation. Successful digital transformation can enable traditional industries to gain dynamic capabilities in a changing environment, thereby gaining a leading competitive advantage. The previous literature paid more attention to the digital transformation of traditional industries, but lacked enough attention to the hotel industry. Through the case analysis of several major hotel groups in China, this article has gained profound insights in the digital transformation, enriched the influence of digital technology on the organization and management changes of the hotel industry, and has enlightening significance for guiding the hotel industry's practice.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15455963",
        "isbn": null,
        "journal": "IEEE/ACM Trans. Comput. Biol. Bioinformatics",
        "publisher": "IEEE Computer Society Press",
        "title": "useofelectronichealthdatafordiseasepredictionacomprehensiveliteraturereview",
        "booktitle": null,
        "doi": "10.1109/TCBB.2019.2937862",
        "author": [
            "Hossain, Md.",
            "Khan, Arif",
            "Moni, Mohammad",
            "Uddin, Shahadat"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450387811",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "asurveyofpersonalizedrecommendationbasedonmachinelearningalgorithms",
        "booktitle": "Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering",
        "doi": "10.1145/3443467.3444711",
        "author": [
            "Tian, Luogeng",
            "Yang, Bailong",
            "Yin, Xinli",
            "Su, Yang"
        ],
        "keywords": [
            "Sparse matrix, Machine learning, Personalized recommendation, Graph Neural Networks"
        ],
        "abstract": "Personalized recommendation is a key technology to effectively solve the overload of online information and eliminate information islands. It is widely known as an important way to improve the quality of information services. However, cold start, data sparseness, algorithm performance, recommendation accuracy and surprise are still the key issues that restrict users' personalized recommendations. Firstly, we review the development trend of personalized information recommendation algorithms in the past 15 years. And then we propose a new classification method for users' personalized recommendation based on machine learning algorithms with cold start, data sparseness, and the performance of the algorithm as the main goals. On this basis, we summarize and compare the ideas, practices and conclusions of related machine learning algorithms. Finally, we further summarize the main advantages and disadvantages of the 10 kinds of personalized recommendation algorithms from the perspective of classification proposed, and look forward to the development directions, difficulties, focus and methods of personalized recommendation algorithms.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450384131",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "biosodaenablingnaturallanguagequestionansweringoverknowledgegraphswithouttrainingdata",
        "booktitle": "33rd International Conference on Scientific and Statistical Database Management",
        "doi": "10.1145/3468791.3469119",
        "author": [
            "Sima, Ana",
            "Farias, Tarcisio",
            "Anisimova, Maria",
            "Dessimoz, Christophe",
            "Robinson-Rechavi, Marc",
            "Zbinden, Erich",
            "Stockinger, Kurt"
        ],
        "keywords": [
            "Question Answering, Knowledge Graphs, Ranking"
        ],
        "abstract": "The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450386456",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "whoshouldgetmyprivatedatainwhichcaseevidenceinthewild",
        "booktitle": "Proceedings of Mensch Und Computer 2021",
        "doi": "10.1145/3473856.3473879",
        "author": [
            "Herbert, Franziska",
            "Schmidbauer-Wolf, Gina",
            "Reuter, Christian"
        ],
        "keywords": [
            "privacy, data sharing, survey, awareness"
        ],
        "abstract": "As a result of the ongoing digitalization of our everyday lives, the amount of data produced by everyone is steadily increasing. This happens through personal decisions and items, such as the use of social media or smartphones, but also through more and more data acquisition in public spaces, such as e.g., Closed Circuit Television. Are people aware of the data they are sharing? What kind of data do people want to share with whom? Are people aware if they have Wi-Fi, GPS, or Bluetooth activated as potential data sharing functionalities on their phone? To answer these questions, we conducted a representative online survey as well as face-to-face interviews with users in Germany. We found that most users wanted to share private data on premise with most entities, indicating that willingness to share data depends on who has access to the data. Almost half of the participants would be more willing to share data with specific entities (state bodies &amp; rescue forces) in the event that an acquaintance is endangered. For Wi-Fi and GPS the frequencies of self-reported and actual activation on the smartphone are almost equal, but 17% of participants were unaware of the Bluetooth status on their smartphone. Our research is therefore in line with other studies suggesting relatively low privacy awareness of users.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26911957",
        "isbn": null,
        "journal": "ACM Trans. Comput. Healthcare",
        "publisher": "Association for Computing Machinery",
        "title": "opportunitiesandbarriersforadoptionofadecisionsupporttoolforalzheimersdisease",
        "booktitle": null,
        "doi": "10.1145/3462764",
        "author": [
            "Bellio, Maura",
            "Furniss, Dominic",
            "Oxtoby, Neil",
            "Garbarino, Sara",
            "Firth, Nicholas",
            "Ribbens, Annemie",
            "Alexander, Daniel",
            "Blandford, Ann"
        ],
        "keywords": [
            "user-centred design, design-reality gap, Diffusion of innovation, healthcare, technology adoption"
        ],
        "abstract": "Clinical decision-support tools (DSTs) represent a valuable resource in healthcare. However, lack of Human Factors considerations and early design research has often limited their successful adoption. To complement previous technically focused work, we studied adoption opportunities of a future DST built on a predictive model of Alzheimer\u2019s Disease (AD) progression. Our aim is two-fold: exploring adoption opportunities for DSTs in AD clinical care, and testing a novel combination of methods to support this process. We focused on understanding current clinical needs and practices, and the potential for such a tool to be integrated into the setting, prior to its development. Our user-centred approach was based on field observations and semi-structured interviews, analysed through workflow analysis, user profiles, and a design-reality gap model. The first two are common practice, whilst the latter provided added value in highlighting specific adoption needs. We identified the likely early adopters of the tool as being both psychiatrists and neurologists based in research-oriented clinical settings. We defined ten key requirements for the translation and adoption of DSTs for AD around IT, user, and contextual factors. Future works can use and build on these requirements to stand a greater chance to get adopted in the clinical setting.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383431",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "dataprepedataskcentricexploratorydataanalysisforstatisticalmodelinginpython",
        "booktitle": "Proceedings of the 2021 International Conference on Management of Data",
        "doi": "10.1145/3448016.3457330",
        "author": [
            "Peng, Jinglin",
            "Wu, Weiyuan",
            "Lockhart, Brandon",
            "Bian, Song",
            "Yan, Jing",
            "Xu, Linghao",
            "Chi, Zhixuan",
            "Rzeszotarski, Jeffrey",
            "Wang, Jiannan"
        ],
        "keywords": [
            "data preparation, data exploration, exploratory data analysis, data profiling, statistical modeling, python"
        ],
        "abstract": "Exploratory Data Analysis (EDA) is a crucial step in any data science project. However, existing Python libraries fall short in supporting data scientists to complete common EDA tasks for statistical modeling. Their API design is either too low level, which is optimized for plotting rather than EDA, or too high level, which is hard to specify more fine-grained EDA tasks. In response, we propose DataPrep.EDA, a novel task-centric EDA system in Python. DataPrep.EDA allows data scientists to declaratively specify a wide range of EDA tasks in different granularity with a single function call. We identify a number of challenges to implement DataPrep.EDA, and propose effective solutions to improve the scalability, usability, customizability of the system. In particular, we discuss some lessons learned from using Dask to build the data processing pipelines for EDA tasks and describe our approaches to accelerate the pipelines. We conduct extensive experiments to compare DataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python. The experiments show that DataPrep.EDA significantly outperforms Pandas-profiling in terms of both speed and user experience. DataPrep.EDA is open-sourced as an EDA component of DataPrep: https://github.com/sfu-db/dataprep.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "authorsbiographiesindex",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447430",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383660",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "adaptivevisualizationsforenhanceddataunderstandingandinterpretation",
        "booktitle": "Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization",
        "doi": "10.1145/3450613.3459657",
        "author": [
            "Amyrotos, Christos"
        ],
        "keywords": [
            "personalization, adaptation, human factors, user modeling, data visualizations, business analytics"
        ],
        "abstract": "In a data driven economy where data volume and dimensions are explosively increasing, businesses rely on business intelligence and analytics (BI&amp;A) platforms for analysing their data and coming to beneficial decisions. With the ever-growing generation of data, the process of data analysis is becoming more complicated for the business users, as the exploration of more demanding use cases increases. While the existing BI&amp;A platforms provide myriads of data visualizations that support data exploration, none of those account for the user\u2019s individual differences, needs or requirements, and thus may hinder the user\u2019s understanding of visual data and consequently their decision-making processes. This work embarks on an interdisciplinary endeavour to introduce a human-centred adaptive data visualizations framework in the context of business, as the core of an adaptive data analytics platform, that aims to enhance the business user\u2019s decision making by increasing her understanding of data. The framework is built using a multi-dimensional human-centred user model that goes beyond traditional user characteristics and accounts for cognitive factors, domain expertise and experience and factors related to the business context i.e., data, visualizations and tasks; a data visualization engine that will recommend to the unique-user the best-fit data visualizations based on the abovementioned user model; and an intelligent data analytics component that enhances the efficiency and effectiveness of the data exploration process by leveraging user interactions during the explorations to further inform the user model on the user\u2019s expertise and experience.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "theinternetofeverythingintroducingprivacy",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447409",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicsandpersonalcontext",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447425",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicsandadaptivetouchinterfaces",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447427",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicalissuesofdigitalsignalprocessing",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447413",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10769757",
        "isbn": null,
        "journal": "J. Artif. Int. Res.",
        "publisher": "AI Access Foundation",
        "title": "asurveyontheexplainabilityofsupervisedmachinelearning",
        "booktitle": null,
        "doi": "10.1613/jair.1.12228",
        "author": [
            "Burkart, Nadia",
            "Huber, Marco"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.776",
        "scimago_value": "0,790"
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicsinautomotiveuserinterface",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447429",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicalissuesinprobabilistictextentry",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447421",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicsandsecuregestures",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447423",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "preface",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447405",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice provides a comprehensive resource on what has become the dominant paradigm for novel interaction design methods involving gesture, speech, text, and touch embedded in novel and emerging interfaces. These interfaces support smartphones, wearables, in-vehicle devices, virtual reality, robotic, the Internet of Things (IoT), brain\u2013computer interaction, and many other applications that are now highly competitive commercially.This edited collection is written by international experts and pioneers in the field of digital signal processing (DSP) and machine learning (ML) for interactive systems. It provides a textbook for students, and a reference and technology roadmap for developers and professionals working in interaction design on emerging platforms. This introductory textbook presents theory chapters on statistical grounding, signal processing, and ML foundations for guiding the development of novel interactive systems. Additional chapters discuss case studies on smart cities, brain\u2013computer interfaces (BCI), probabilistic text entry, secure gestures, personal context from mobile phones, building adaptive touch interfaces, and automotive user interfaces (UIs). The chapters on case studies also highlight an in-depth look at domain-specific language (DSL) and ML methods used, for example, in touch, gesture, electroencephalography (EEG), electrocardiography (ECG), and galvanic skin response (GSR) signals, or embedded sensor inputs. A common theme throughout is the ubiquitous support for humans as they go about their daily professional or personal activities.This introductory book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal\u2013multisensor systems. After each chapter an expert on the legal and ethical issues explores the wider ethical issues on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during interaction with novel platforms.Parisa Eslambolchilar, Andreas Komninos, and Mark D. Dunlop, March 2020AcknowledgmentsWe would like to thank our external reviewers for their valuable feedback throughout the writing process.",
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicsandsmartcities",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447417",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicalissuesinbraincomputerinterfaces",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447419",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450383097",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "towardsaccountabilityformachinelearningdatasetspracticesfromsoftwareengineeringandinfrastructure",
        "booktitle": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",
        "doi": "10.1145/3442188.3445918",
        "author": [
            "Hutchinson, Ben",
            "Smart, Andrew",
            "Hanna, Alex",
            "Denton, Emily",
            "Greer, Christina",
            "Kjartansson, Oddur",
            "Barnes, Parker",
            "Mitchell, Margaret"
        ],
        "keywords": [
            "datasets, machine learning, requirements engineering"
        ],
        "abstract": "Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390514",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "systemforcontinuouscollectionofcontextualinformationfornetworksecuritymanagementandincidenthandling",
        "booktitle": "Proceedings of the 16th International Conference on Availability, Reliability and Security",
        "doi": "10.1145/3465481.3470037",
        "author": [
            "Hus\\'{a}k, Martin",
            "La\\v{s}tovi\\v{c}ka, Martin",
            "Tovar\\v{n}\\'{a}k, Daniel"
        ],
        "keywords": [
            "Incident Response, Cybersecurity, Incident Handling, Network Monitoring, Cyber Situational Awareness"
        ],
        "abstract": "In this paper, we describe a system for the continuous collection of data for the needs of network security management. When a cybersecurity incident occurs in the network, the contextual information on the involved assets facilitates estimating the severity and impact of the incident and selecting an appropriate incident response. We propose a system based on the combination of active and passive network measurements and the correlation of the data with third-party systems. The system enumerates devices and services in the network and their vulnerabilities via fingerprinting of operating systems and applications. Further, the system pairs the hosts in the network with contacts on responsible administrators and highlights critical infrastructure and its dependencies. The system concentrates all the information required for common incident handling procedures and aims to speed up incident response, reduce the time spent on the manual investigation, and prevent errors caused by negligence or lack of information.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "ethicalissuesindigitalsignalprocessingandmachinelearning",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447407",
        "author": [
            "McMenemy, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21508097",
        "isbn": null,
        "journal": "Proc. VLDB Endow.",
        "publisher": "VLDB Endowment",
        "title": "gleanstructuredextractionsfromtemplaticdocuments",
        "booktitle": null,
        "doi": "10.14778/3447689.3447703",
        "author": [
            "Tata, Sandeep",
            "Potti, Navneet",
            "Wendt, James",
            "Costa, Lauro",
            "Najork, Marc",
            "Gunel, Beliz"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Extracting structured information from templatic documents is an important problem with the potential to automate many real-world business workflows such as payment, procurement, and payroll. The core challenge is that such documents can be laid out in virtually infinitely different ways. A good solution to this problem is one that generalizes well not only to known templates such as invoices from a known vendor, but also to unseen ones.We developed a system called Glean to tackle this problem. Given a target schema for a document type and some labeled documents of that type, Glean uses machine learning to automatically extract structured information from other documents of that type. In this paper, we describe the overall architecture of Glean, and discuss three key data management challenges : 1) managing the quality of ground truth data, 2) generating training data for the machine learning model using labeled documents, and 3) building tools that help a developer rapidly build and improve a model for a given document type. Through empirical studies on a real-world dataset, we show that these data management techniques allow us to train a model that is over 5 F1 points better than the exact same model architecture without the techniques we describe. We argue that for such information-extraction problems, designing abstractions that carefully manage the training data is at least as important as choosing a good model architecture.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.047",
        "scimago_value": "0,946"
    },
    {
        "issnkey": "15564681",
        "isbn": null,
        "journal": "ACM Trans. Knowl. Discov. Data",
        "publisher": "Association for Computing Machinery",
        "title": "neuralnetworksforentitymatchingasurvey",
        "booktitle": null,
        "doi": "10.1145/3442200",
        "author": [
            "Barlaug, Nils",
            "Gulla, Jon"
        ],
        "keywords": [
            "entity resolution, record linkage, Deep learning, data matching, entity matching"
        ],
        "abstract": "Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years, we have seen new methods based upon deep learning techniques for natural language processing emerge. In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.713",
        "scimago_value": "0,728"
    },
    {
        "issnkey": "",
        "isbn": null,
        "journal": "Proc. ACM Hum.-Comput. Interact.",
        "publisher": "Association for Computing Machinery",
        "title": "dodatasetshavepoliticsdisciplinaryvaluesincomputervisiondatasetdevelopment",
        "booktitle": null,
        "doi": "10.1145/3476058",
        "author": [
            "Scheuerman, Morgan",
            "Hanna, Alex",
            "Denton, Emily"
        ],
        "keywords": [
            "computer vision, machine learning, datasets, values in design, work practice"
        ],
        "abstract": "Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389914",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "multimodeldatamodelingandrepresentationstateoftheartandresearchchallenges",
        "booktitle": "Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "doi": "10.1145/3472163.3472267",
        "author": [
            "Holubova, Irena",
            "Contos, Pavel",
            "Svoboda, Martin"
        ],
        "keywords": [
            "Inter-model relationships, Category theory, Logical models, Multi-model data, Conceptual modeling"
        ],
        "abstract": "Following the current trend, most of the well-known database systems, being relational, NoSQL, or NewSQL, denote themselves as multi-model. This industry-driven approach, however, lacks plenty of important features of the traditional DBMSs. The primary problem is a design of an optimal multi-model schema and its sufficiently general and efficient representation. In this paper, we provide an overview and discussion of the promising approaches that could potentially be capable of solving these issues, along with a summary of the remaining open problems.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450380379",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "groupbasedpersonalizedsearchbyintegratingsearchbehaviourandfriendnetwork",
        "booktitle": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "doi": "10.1145/3404835.3462918",
        "author": [
            "Zhou, Yujia",
            "Dou, Zhicheng",
            "Wei, Bingzheng",
            "Xie, Ruobing",
            "Wen, Ji-Rong"
        ],
        "keywords": [
            "friend network, group formation, personalized search"
        ],
        "abstract": "The key to personalized search is to build the user profile based on historical behaviour. To deal with the users who lack historical data, group based personalized models were proposed to incorporate the profiles of similar users when re-ranking the results. However, similar users are mostly found based on simple lexical or topical similarity in search behaviours. In this paper, we propose a neural network enhanced method to highlight similar users in semantic space. Furthermore, we argue that the behaviour-based similar users are still insufficient to understand a new query when user's historical activities are limited. To tackle this issue, we introduce the friend network into personalized search to determine the closeness between users in another way. Since the friendship is often formed based on similar background or interest, there are plenty of personalized signals hidden in the friend network naturally. Specifically, we propose a friend network enhanced personalized search model, which groups the user into multiple friend circles based on search behaviours and friend relations respectively. These two types of friend circles are complementary to construct a more comprehensive group profile for refining the personalization. Experimental results show the significant improvement of our model over existing personalized search models.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "1049331x",
        "isbn": null,
        "journal": "ACM Trans. Softw. Eng. Methodol.",
        "publisher": "Association for Computing Machinery",
        "title": "spiautomatedidentificationofsecuritypatchesviacommits",
        "booktitle": null,
        "doi": "10.1145/3468854",
        "author": [
            "Zhou, Yaqin",
            "Siow, Jing",
            "Wang, Chenyu",
            "Liu, Shangqing",
            "Liu, Yang"
        ],
        "keywords": [
            "Machine learning, software security, deep learning"
        ],
        "abstract": "Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93% F1-score and precision of 86.24%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "internetofeverything",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447408",
        "author": [
            "Chatzigiannakis, Ioannis",
            "Tselios, Christos"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450367684",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "learningtohandleexceptions",
        "booktitle": "Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering",
        "doi": "10.1145/3324884.3416568",
        "author": [
            "Zhang, Jian",
            "Wang, Xu",
            "Zhang, Hongyu",
            "Sun, Hailong",
            "Pu, Yanjun",
            "Liu, Xudong"
        ],
        "keywords": [
            "code generation, deep learning, exception handling, neural network"
        ],
        "abstract": "Exception handling is an important built-in feature of many modern programming languages such as Java. It allows developers to deal with abnormal or unexpected conditions that may occur at runtime in advance by using try-catch blocks. Missing or improper implementation of exception handling can cause catastrophic consequences such as system crash. However, previous studies reveal that developers are unwilling or feel it hard to adopt exception handling mechanism, and tend to ignore it until a system failure forces them to do so. To help developers with exception handling, existing work produces recommendations such as code examples and exception types, which still requires developers to localize the try blocks and modify the catch block code to fit the context. In this paper, we propose a novel neural approach to automated exception handling, which can predict locations of try blocks and automatically generate the complete catch blocks. We collect a large number of Java methods from GitHub and conduct experiments to evaluate our approach. The evaluation results, including quantitative measurement and human evaluation, show that our approach is highly effective and outperforms all baselines. Our work makes one step further towards automated exception handling.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "securegesturescasestudy4",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447422",
        "author": [
            "Liu, Can",
            "Lindqvist, Janne"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23299290",
        "isbn": null,
        "journal": "IEEE/ACM Trans. Audio, Speech and Lang. Proc.",
        "publisher": "IEEE Press",
        "title": "audiovisualmultichannelintegrationandrecognitionofoverlappedspeech",
        "booktitle": null,
        "doi": "10.1109/TASLP.2021.3078883",
        "author": [
            "Yu, Jianwei",
            "Zhang, Shi-Xiong",
            "Wu, Bo",
            "Liu, Shansong",
            "Hu, Shoukang",
            "Geng, Mengzhe",
            "Liu, Xunying",
            "Meng, Helen",
            "Yu, Dong"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Automatic speech recognition (ASR) technologies have been significantly advanced in the past few decades. However, recognition of overlapped speech remains a highly challenging task to date. To this end, multi-channel microphone array data are widely used in current ASR systems. Motivated by the invariance of visual modality to acoustic signal corruption and the additional cues they provide to separate the target speaker from the interfering sound sources, this paper presents an audio-visual multi-channel based recognition system for overlapped speech. It benefits from a tight integration between a speech separation front-end and recognition back-end, both of which incorporate additional video input. A series of audio-visual multi-channel speech separation front-end components based on <italic>TF masking</italic>, <italic>Filter&amp;Sum</italic> and <italic>mask-based MVDR</italic> neural channel integration approaches are developed. To reduce the error cost mismatch between the separation and the recognition components, the entire system is jointly fine-tuned using a multi-task criterion interpolation of the scale-invariant signal to noise ratio (Si-SNR) with either the connectionist temporal classification (CTC), or lattice-free maximum mutual information (LF-MMI) loss function. Experiments suggest that: the proposed audio-visual multi-channel recognition system outperforms the baseline audio-only multi-channel ASR system by up to 8.04% (31.68% relative) and 22.86% (58.51% relative) absolute WER reduction on overlapped speech constructed using either simulation or replaying of the LRS2 dataset respectively. Consistent performance improvements are also obtained using the proposed audio-visual multi-channel recognition system when using occluded video input with the lip region randomly covered up to 60%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450380966",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "howweirdischi",
        "booktitle": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "doi": "10.1145/3411764.3445488",
        "author": [
            "Linxen, Sebastian",
            "Sturm, Christian",
            "Br\\\"{u}hlmann, Florian",
            "Cassau, Vincent",
            "Opwis, Klaus",
            "Reinecke, Katharina"
        ],
        "keywords": [
            "HCI research, WEIRD, geographic diversity, sample bias, generalizability"
        ],
        "abstract": "Computer technology is often designed in technology hubs in Western countries, invariably making it \u201cWEIRD\u201d, because it is based on the intuition, knowledge, and values of people who are Western, Educated, Industrialized, Rich, and Democratic. Developing technology that is universally useful and engaging requires knowledge about members of WEIRD and non-WEIRD societies alike. In other words, it requires us, the CHI community, to generate this knowledge by studying representative participant samples. To find out to what extent CHI participant samples are from Western societies, we analyzed papers published in the CHI proceedings between 2016-2020. Our findings show that 73% of CHI study findings are based on Western participant samples, representing less than 12% of the world\u2019s population. Furthermore, we show that most participant samples at CHI tend to come from industrialized, rich, and democratic countries with generally highly educated populations. Encouragingly, recent years have seen a slight increase in non-Western samples and those that include several countries. We discuss suggestions for further broadening the international representation of CHI participant samples.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450386944",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "spbpuide21proceedingsofthe3rdinternationalscientificconferenceoninnovationsindigitaleconomy",
        "booktitle": null,
        "doi": null,
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "proceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03600300",
        "isbn": null,
        "journal": "ACM Comput. Surv.",
        "publisher": "Association for Computing Machinery",
        "title": "generativeadversarialnetworksasurveytowardprivateandsecureapplications",
        "booktitle": null,
        "doi": "10.1145/3459992",
        "author": [
            "Cai, Zhipeng",
            "Xiong, Zuobin",
            "Xu, Honghui",
            "Wang, Peng",
            "Li, Wei",
            "Pan, Yi"
        ],
        "keywords": [
            "deep learning, privacy and security, Generative adversarial networks"
        ],
        "abstract": "Generative Adversarial Networks (GANs) have promoted a variety of applications in computer vision and natural language processing, among others, due to its generative model\u2019s compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey to summarize systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this article also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.282",
        "scimago_value": "2,079"
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "personalcontextfrommobilephonescasestudy5",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447424",
        "author": [
            "Wiese, Jason"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450380966",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "comparingperspectivesaroundhumanandtechnologysupportforcontacttracing",
        "booktitle": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "doi": "10.1145/3411764.3445669",
        "author": [
            "Lu, Xi",
            "L., Tera",
            "Jo, Eunkyung",
            "Hong, Hwajung",
            "Page, Xinru",
            "Chen, Yunan",
            "A., Daniel"
        ],
        "keywords": [
            "Public health, COVID-19, Crisis informatics, Contact tracing, Personal informatics, Self-tracking"
        ],
        "abstract": "Various contact tracing approaches have been applied to help contain the spread of COVID-19, with technology-based tracing and human tracing among the most widely adopted. However, governments and communities worldwide vary in their adoption of digital contact tracing, with many instead choosing the human approach. We investigate how people perceive the respective benefits and risks of human and digital contact tracing through a mixed-methods survey with 291 respondents from the United States. Participants perceived digital contact tracing as more beneficial for protecting privacy, providing convenience, and ensuring data accuracy, and felt that human contact tracing could help provide security, emotional reassurance, advice, and accessibility. We explore the role of self-tracking technologies in public health crisis situations, highlighting how designs must adapt to promote societal benefit rather than just self-understanding. We discuss how future digital contact tracing can better balance the benefits of human tracers and technology amidst the complex contact tracing process and context.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450380966",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "aiinglobalhealththeviewfromthefrontlines",
        "booktitle": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "doi": "10.1145/3411764.3445130",
        "author": [
            "Ismail, Azra",
            "Kumar, Neha"
        ],
        "keywords": [
            "Qualitative, India, AI, Healthcare, HCI4D, Social Good"
        ],
        "abstract": "There has been growing interest in the application of AI for Social Good, motivated by scarce and unequal resources globally. We focus on the case of AI in frontline health, a Social Good domain that is increasingly a topic of significant attention. We offer a thematic discourse analysis of scientific and grey literature to identify prominent applications of AI in frontline health, motivations driving this work, stakeholders involved, and levels of engagement with the local context. We then uncover design considerations for these systems, drawing from data from three years of ethnographic fieldwork with women frontline health workers and women from marginalized communities in Delhi (India). Finally, we outline an agenda for AI systems that target Social Good, drawing from literature on HCI4D, post-development critique, and transnational feminist theory. Our paper thus offers a critical and ethnographic perspective to inform the design of AI systems that target social impact.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "drivercognitiveloadclassificationbasedonphysiologicaldatacasestudy7",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447428",
        "author": [
            "He, Dengbo",
            "Risteska, Martina",
            "Donmez, Birsen",
            "Chen, Kaiyang"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "machinelearningbasics",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447414",
        "author": [
            "Chatzilygeroudis, Konstantinos",
            "Hatzilygeroudis, Ioannis",
            "Perikos, Isidoros"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10668888",
        "isbn": null,
        "journal": "The VLDB Journal",
        "publisher": "Springer-Verlag",
        "title": "asurveyonsemanticschemadiscovery",
        "booktitle": null,
        "doi": "10.1007/s00778-021-00717-x",
        "author": [
            "Kellou-Menouer, Kenza",
            "Kardoulakis, Nikolaos",
            "Troullinou, Georgia",
            "Kedad, Zoubida",
            "Plexousakis, Dimitris",
            "Kondylakis, Haridimos"
        ],
        "keywords": [
            "Schema discovery, Semantic web, Linked data, Irregular data"
        ],
        "abstract": "More and more weakly structured, and irregular data sources are becoming available every day. The schema of these sources is useful for a number of tasks, such as query answering, exploration and summarization. However, although semantic web data might contain schema information, in many cases this is completely missing or partially defined. In this paper, we present a survey of the state of the art on schema information extraction approaches. We analyze and classify these approaches into three families: (1) approaches that exploit the implicit structure of the data, without assuming that some explicit statements on the schema are provided in the dataset; (2) approaches that use the explicit schema statements contained in the dataset to complement and enrich the schema, and (3) those that discover structural patterns contained in a dataset. We compare these studies in terms of their approach, advantages and limitations. Finally we discuss the problems that remain open.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.868",
        "scimago_value": "0,653"
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "braincomputerinterfacingwithinteractivesystemscasestudy2",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447418",
        "author": [
            "Vourvopoulos, A.",
            "Niforatos, E.",
            "Badia, S.",
            "Liarokapis, Fotis"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "dspbasics",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447412",
        "author": [
            "Alexander, Jason",
            "Thanh, Chi"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "buildingadaptivetouchinterfacescasestudy6",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447426",
        "author": [
            "Buschek, Daniel",
            "Alt, Florian"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "combininginfrastructuresensorandtourismmarketdatainasmartcityprojectcasestudy1",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447416",
        "author": [
            "Komninos, Andreas",
            "Dunlop, Mark",
            "Wilson, John"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450380966",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "appsagainstthespreadprivacyimplicationsanduseracceptanceofcovid19relatedsmartphoneappsonthreecontinents",
        "booktitle": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
        "doi": "10.1145/3411764.3445517",
        "author": [
            "Utz, Christine",
            "Becker, Steffen",
            "Schnitzler, Theodor",
            "Farke, Florian",
            "Herbert, Franziska",
            "Schaewitz, Leonie",
            "Degeling, Martin",
            "D\\\"{u}rmuth, Markus"
        ],
        "keywords": [
            "digital contact tracing, privacy, COVID-19"
        ],
        "abstract": "The COVID-19 pandemic has fueled the development of smartphone applications to assist disease management. Many \u201ccorona apps\u201d require widespread adoption to be effective, which has sparked public debates about the privacy, security, and societal implications of government-backed health applications. We conducted a representative online study in Germany (n = 1003), the US (n = 1003), and China (n = 1019) to investigate user acceptance of corona apps, using a vignette design based on the contextual integrity framework. We explored apps for contact tracing, symptom checks, quarantine enforcement, health certificates, and mere information. Our results provide insights into data processing practices that foster adoption and reveal significant differences between countries, with user acceptance being highest in China and lowest in the US. Chinese participants prefer the collection of personalized data, while German and US participants favor anonymity. Across countries, contact tracing is viewed more positively than quarantine enforcement, and technical malfunctions negatively impact user acceptance.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "probabilistictextentrycasestudy3",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447420",
        "author": [
            "Vertanen, Keith"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "statisticalgrounding",
        "booktitle": "Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",
        "doi": "10.1145/3447404.3447410",
        "author": [
            "Arif, Ahmed"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inbook",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15564681",
        "isbn": null,
        "journal": "ACM Trans. Knowl. Discov. Data",
        "publisher": "Association for Computing Machinery",
        "title": "contextbasedevaluationofdimensionalityreductionalgorithmsexperimentsandstatisticalsignificanceanalysis",
        "booktitle": null,
        "doi": "10.1145/3428077",
        "author": [
            "Ghosh, Aindrila",
            "Nashaat, Mona",
            "Miller, James",
            "Quader, Shaikh"
        ],
        "keywords": [
            "Dimensionality reduction, statistical significance analysis, context-based evaluation, quality metrics"
        ],
        "abstract": "Dimensionality reduction is a commonly used technique in data analytics. Reducing the dimensionality of datasets helps not only with managing their analytical complexity but also with removing redundancy. Over the years, several such algorithms have been proposed with their aims ranging from generating simple linear projections to complex non-linear transformations of the input data. Subsequently, researchers have defined several quality metrics in order to evaluate the performances of different algorithms. Hence, given a plethora of dimensionality reduction algorithms and metrics for their quality analysis, there is a long-existing need for guidelines on how to select the most appropriate algorithm in a given scenario. In order to bridge this gap, in this article, we have compiled 12 state-of-the-art quality metrics and categorized them into 5 identified analytical contexts. Furthermore, we assessed 15 most popular dimensionality reduction algorithms on the chosen quality metrics using a large-scale and systematic experimental study. Later, using a set of robust non-parametric statistical tests, we assessed the generalizability of our evaluation on 40 real-world datasets. Finally, based on our results, we present practitioners\u2019 guidelines for the selection of an appropriate dimensionally reduction algorithm in the present analytical contexts.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.713",
        "scimago_value": "0,728"
    },
    {
        "issnkey": "",
        "isbn": "9781450390293",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "intelligentcomputingforinteractivesystemdesignstatisticsdigitalsignalprocessingandmachinelearninginpractice",
        "booktitle": null,
        "doi": null,
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": "Intelligent Computing for Interactive System Design provides a comprehensive resource on what has become the dominant paradigm in designing novel interaction methods, involving gestures, speech, text, touch and brain-controlled interaction, embedded in innovative and emerging human\u2013computer interfaces. These interfaces support ubiquitous interaction with applications and services running on smartphones, wearables, in-vehicle systems, virtual and augmented reality, robotic systems, the Internet of Things (IoT), and many other domains that are now highly competitive, both in commercial and in research contexts.This book presents the crucial theoretical foundations needed by any student, researcher, or practitioner working on novel interface design, with chapters on statistical methods, digital signal processing (DSP), and machine learning (ML). These foundations are followed by chapters that discuss case studies on smart cities, brain\u2013computer interfaces, probabilistic mobile text entry, secure gestures, personal context from mobile phones, adaptive touch interfaces, and automotive user interfaces. The case studies chapters also highlight an in-depth look at the practical application of DSP and ML methods used for processing of touch, gesture, biometric, or embedded sensor inputs. A common theme throughout the case studies is ubiquitous support for humans in their daily professional or personal activities.In addition, the book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal and multi-sensor systems. In a series of short additions to each chapter, an expert on the legal and ethical issues explores the emergent deep concerns of the professional community, on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during ubiquitous interaction with omnipresent computers.This carefully edited collection is written by international experts and pioneers in the fields of DSP and ML. It provides a textbook for students and a reference and technology roadmap for developers and professionals working on interaction design on emerging platforms.",
        "year": "2021",
        "type_publication": "book",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450380959",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "chiea21extendedabstractsofthe2021chiconferenceonhumanfactorsincomputingsystems",
        "booktitle": null,
        "doi": null,
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "proceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450380966",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "chi21proceedingsofthe2021chiconferenceonhumanfactorsincomputingsystems",
        "booktitle": null,
        "doi": null,
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "proceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389945",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "opportunitiesandchallengesofmarketinginthecontextofbigdata",
        "booktitle": "2021 Workshop on Algorithm and Big Data",
        "doi": "10.1145/3456389.3456390",
        "author": [
            "Cao, Shuangshuang"
        ],
        "keywords": [
            "Big data, Marketing, Personalized service"
        ],
        "abstract": "In the era of big data, under the conditions of rapid economic development in our country, various enterprises have also vigorously carried out marketing. In the context of big data, marketing research should be strengthened to effectively improve market. Market issues ensure that marketing has improved its status in the era of big data. This article has conducted a research and analysis on marketing in the context of big data. And then the opportunities and challenges of marketing in the context of big data has been explained, which gradually optimize the marketing implementation effect. The challenges faced by marketing has been understood which ensures that the big data model plays its best role in it.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389914",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "customizedeagerlazydatacleansingforsatisfactorybigdataveracity",
        "booktitle": "Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "doi": "10.1145/3472163.3472195",
        "author": [
            "Sahri, Soror",
            "Moussa, Rim"
        ],
        "keywords": [
            "Big data, Veracity"
        ],
        "abstract": "Big data systems are becoming mainstream for big data management either for batch processing or real-time processing. In order to extract insights from data, quality issues are very important to address, particularly. A veracity assessment model is consequently needed. In this paper, we propose a model which ties quality of datasets and quality of query resultsets. We particularly examine quality issues raised by a given dataset, order attributes along their fitness for use and correlate veracity metrics to business queries. We validate our work using the open dataset NYC taxi\u2019 trips.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389914",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "rigorousmeasurementmodelforvalidityofbigdatamegaapproach",
        "booktitle": "Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "doi": "10.1145/3472163.3472171",
        "author": [
            "Bhardwaj, Dave",
            "Ormandjieva, Olga"
        ],
        "keywords": [
            "Quality Characteristics (V's), Representational Theory of Measurement,, Validity, Big Data, Measurement Hierarchical Model"
        ],
        "abstract": "Big Data is becoming a substantial part of the decision-making processes in both industry and academia, especially in areas where Big Data may have a profound impact on businesses and society. However, as more data is being processed, data quality is becoming a genuine issue that negatively affects credibility of the systems we build because of the lack of visibility and transparency of the underlying data. Therefore, Big Data quality measurement is becoming increasingly necessary in assessing whether data can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses Big Data quality measurement modelling and automation by proposing a novel quality measurement framework for Big Data (MEGA) that objectively assesses the underlying quality characteristics of Big Data (also known as the V's of Big Data) at each step of the Big Data Pipelines. Five of the Big Data V's (Volume, Variety, Velocity, Veracity and Validity) are currently automated by the MEGA framework. In this paper, a new theoretically valid quality measurement model is proposed for an essential quality characteristic of Big Data, called Validity. The proposed measurement information model for Validity of Big Data is a hierarchy of 4 derived measures / indicators and 5 based measures. Validity measurement is illustrated on a running example.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389099",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "researchonsmarteducationserviceplatformbasedonbigdata",
        "booktitle": "Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science",
        "doi": "10.1145/3453187.3453340",
        "author": [
            "Hu, Zhifeng",
            "Zhao, Feng",
            "Zhao, Xiaona"
        ],
        "keywords": [
            "Information-oriented education, Big data, Smart education"
        ],
        "abstract": "The big data technology can be applied to build the education service platforms and construct the big data analysis and application system as well as the multi-dimensional perception system. The big data analysis assists in the teaching process and breaks the temporal and spatial restrictions of educational resources, to realize the diversification of educational resources and improve the effectiveness of teaching feedback. This paper proposes a smart education service platform based on big data, which can promote the organic integration of educational communication, educational research, learning activities, teaching affairs administration, and information infrastructures. At the same time, the platform provides smarter, more efficient, and accurate services for teaching.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450387750",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "avisualmethodforshipcloseencounterpatternrecognitionbasedonfuzzytheoryandbigdataintelligence",
        "booktitle": "2020 the 4th International Conference on Big Data Research (ICBDR'20)",
        "doi": "10.1145/3445945.3445962",
        "author": [
            "Zhao, Liangbin",
            "Fu, Xiuju"
        ],
        "keywords": [
            "Ship encounter, Visualization, AIS data, Fuzzy theory"
        ],
        "abstract": "As an important application of big data technology in the maritime field, big data driven visualization of ship encounter patterns helps to intuitively understand the risk situation in the water traffic. However traditional methods based on fixed thresholds do not consider the fuzziness of classification on ship encounter situations. We proposed a visual method to visualize the risk situations caused by the interaction between vessel traffic flows in more detail based on fuzzy theory and big data intelligence on large scale AIS data. A case study is conducted to verify the applicability based on the AIS data from Singapore Strait. Visualization results of density in grids show that the proposed method can effectively reflect the ship encounter patterns, which are consistent with the real situation and can show more valuable details for the safety assessment of water traffic.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450377119",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "abigdataarchitecturetoamultiplepurposeinhealthcaresurveillancethebraziliansyphiliscase",
        "booktitle": "Proceedings of the 10th Euro-American Conference on Telematics and Information Systems",
        "doi": "10.1145/3401895.3402092",
        "author": [
            "Silva, Rodrigo",
            "Ara\\'{u}jo, Jean",
            "Paiva, \\'{A}lvaro",
            "Medeiros, Ricardo",
            "Coutinho, Karilany",
            "Paiva, Jailton",
            "Roussanaly, Azim",
            "Boyer, Anne"
        ],
        "keywords": [
            "big data, epidemiology, healthcare surveillance, syphilis"
        ],
        "abstract": "For many decades society did need to monitor and assess the standard of living of the population. In the 1950s, the United Nations (UN) saw this need and proposed 12 areas that should be evaluated, the first of which is listed under \"Health and Demography\", which focuses on what is expressed as the level of a population's health. Decades have passed and great results have been gained from similar initiatives such as reducing mortality from infectious diseases and even eradicating some others. In the age of the digital society, needs have grown. Monitoring demands that once perished from data to become concrete now suffer from the opposite effect, the excess of data from everywhere. Healthcare systems around the world use many different information systems, collecting and generating hundreds of data at unimaginable speed. We are billions of people on the planet and most of us are connected to the virtual world, sharing information, experiences and events with some kind of cloud. In this information age, the ability to aggregate and process this data is a major factor in raising public health to a new level. The development of tools capable of analyzing a large volume of data in seconds and producing knowledge for targeted decision making can help in the fight against specific diseases, in the process of continuing education of professionals, in the formation of new professionals, in the elaboration of new policies. with the specific locoregional look, in the analysis of hidden trends in front of so much information faced in everyday life and other possibilities. The present work proposes an architecture capable of storing and manipulating seeking to standardize the variables in order to allow to correlate this large amount of data in a systematic way, providing to several services and researchers the possibility of consuming health, social, economic and educational data for the promotion of public health.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "2158656x",
        "isbn": null,
        "journal": "ACM Trans. Manage. Inf. Syst.",
        "publisher": "Association for Computing Machinery",
        "title": "novelmachinelearningforbigdataanalyticsinintelligentsupportinformationmanagementsystems",
        "booktitle": null,
        "doi": "10.1145/3469890",
        "author": [
            "Lv, Zhihan",
            "Lou, Ranran",
            "Feng, Hailin",
            "Chen, Dongliang",
            "Lv, Haibin"
        ],
        "keywords": [
            "lightGBM, intelligent support information system, accuracy rate, big data analysis, Machine learning"
        ],
        "abstract": "Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "19361955",
        "isbn": null,
        "journal": "J. Data and Information Quality",
        "publisher": "Association for Computing Machinery",
        "title": "editorialspecialissueonqualityassessmentandmanagementinbigdataparti",
        "booktitle": null,
        "doi": "10.1145/3449052",
        "author": [
            "Aljawarneh, Shadi",
            "Lara, Juan"
        ],
        "keywords": [
            "quality management, Quality assessment, big data"
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "19361955",
        "isbn": null,
        "journal": "J. Data and Information Quality",
        "publisher": "Association for Computing Machinery",
        "title": "editorialspecialissueonqualityassessmentandmanagementinbigdatapartii",
        "booktitle": null,
        "doi": "10.1145/3449056",
        "author": [
            "Aljawarneh, Shadi",
            "Lara, Juan"
        ],
        "keywords": [
            "quality management, Quality assessment, big data"
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450387828",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "humanresourcedataqualitymanagementbasedonmultipleregressionanalysis",
        "booktitle": "Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies",
        "doi": "10.1145/3444370.3444614",
        "author": [
            "Zhang, Yong"
        ],
        "keywords": [
            "Multiple regression analysis, human resources, data quality"
        ],
        "abstract": "The essence of human resource management informatization is data. Firstly, human information is transformed into data, and then the data can be used. This requires that the comprehensive, complete, timely and effective human resource data is entered into the system, and then the data is analyzed by using the human resource management system to provide decision support for the management. Due to the complexity of the internal law of objective things and the limitation of people's cognition, it is impossible to analyze the internal causality of the actual object and establish a mathematical model in accordance with the mechanism law. Therefore, when some mathematical models cannot be established by mechanism analysis, we usually adopt the method of collecting a large amount of data, and establish the model based on the statistical analysis of the data. Among them, the most widely used random model is statistical regression model.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450385015",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "applicationstrategiesofmedicalbigdatainhealtheconomicmanagement",
        "booktitle": "The Sixth International Conference on Information Management and Technology",
        "doi": "10.1145/3465631.3465664",
        "author": [
            "Yu, Xiaomu",
            "Yin, Yuelin"
        ],
        "keywords": [
            ""
        ],
        "abstract": "NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26911922",
        "isbn": null,
        "journal": "ACM/IMS Trans. Data Sci.",
        "publisher": "Association for Computing Machinery",
        "title": "deephashbasedrelevanceawaredataqualityassessmentforimagedarkdata",
        "booktitle": null,
        "doi": "10.1145/3420038",
        "author": [
            "Liu, Yu",
            "Wang, Yangtao",
            "Gao, Lianli",
            "Guo, Chan",
            "Xie, Yanzhao",
            "Xiao, Zhili"
        ],
        "keywords": [
            "CPR, data quality assessment, GAH, relevance, data mining, Resource allocation"
        ],
        "abstract": "Data mining can hardly solve but always faces a problem that there is little meaningful information within the dataset serving a given requirement. Faced with multiple unknown datasets, to allocate data mining resources to acquire more desired data, it is necessary to establish a data quality assessment framework based on the relevance between the dataset and requirements. This framework can help the user to judge the potential benefits in advance, so as to optimize the resource allocation to those candidates. However, the unstructured data (e.g., image data) often presents dark data states, which makes it tricky for the user to understand the relevance based on content of the dataset in real time. Even if all data have label descriptions, how to measure the relevance between data efficiently under semantic propagation remains an urgent problem. Based on this, we propose a Deep Hash-based Relevance-aware Data Quality Assessment framework, which contains off-line learning and relevance mining parts as well as an on-line assessing part. In the off-line part, we first design a Graph Convolution Network (GCN)-AutoEncoder hash (GAH) algorithm to recognize the data (i.e., lighten the dark data), then construct a graph with restricted Hamming distance, and finally design a Cluster PageRank (CPR) algorithm to calculate the importance score for each node (image) so as to obtain the relevance representation based on semantic propagation. In the on-line part, we first retrieve the importance score by hash codes and then quickly get the assessment conclusion in the importance list. On the one hand, the introduction of GCN and co-occurrence probability in the GAH promotes the perception ability for dark data. On the other hand, the design of CPR utilizes hash collision to reduce the scale of graph and iteration matrix, which greatly decreases the consumption of space and computing resources. We conduct extensive experiments on both single-label and multi-label datasets to assess the relevance between data and requirements as well as test the resources allocation. Experimental results show our framework can gain the most desired data with the same mining resources. Besides, the test results on Tencent1M dataset demonstrate the framework can complete the assessment with a stability for given different requirements.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390057",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "anempiricalstudyontheclassificationgradingsharingandopeningofhealthcarebigdatabasedoncurrentpoliciesandstandards",
        "booktitle": "2021 3rd International Conference on Intelligent Medicine and Image Processing",
        "doi": "10.1145/3468945.3468964",
        "author": [
            "Hou, Hanfang",
            "Fu, Qiang",
            "Zhang, Yang"
        ],
        "keywords": [
            "Healthcare big data, Grading, Opening, Sharing, Classification"
        ],
        "abstract": "This paper expounds the connotations and features of healthcare big data, as well as the concepts and logical relations for its classification, grading, sharing and opening. It also summarizes overseas policies related thereto and their status quo, and emphatically introduces main policies, laws, regulations and national standards regarding the classification, grading, sharing and opening of healthcare big data in China, as well as practices in three places represented by Shandong Province, Guangdong Province and Fuzhou. Finally, principles and practical suggestions for the classification, grading, sharing and opening of healthcare big data were proposed based on current polices, regulations and standards",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450388979",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "towardsthedesignofaconceptualframeworkfortheoperationofintensivecareunitsbasedonbigdataanalysis",
        "booktitle": "24th Pan-Hellenic Conference on Informatics",
        "doi": "10.1145/3437120.3437352",
        "author": [
            "Markopoulos, Dimitris",
            "Tsolakidis, Anastasios",
            "N., Nikitas",
            "Skourlas, Christos"
        ],
        "keywords": [
            "Conceptual Framework, Intensive Care Unit, Machine Learning, Big Data Analysis"
        ],
        "abstract": "The development of Big Data Analytics (BDA) technology and the maturity of the Machine Learning (ML) sector offer great opportunities for applications in Intensive Care Units (ICUs). This paper describes a Conceptual Framework and proposes its use in designing architectures and big data applications in ICUs. The Conceptual Framework is based on BDA,MLNatural Language Processing (NLP) and consists of the following subsystems: The \"Big Data Integration and ICUs\" module, the \"ICUs and critical care services\" module, the \"Use of standards and ICUs\" module, the \"Machine Learning and ICUs\" module, and the \u201cNLP and ICUs\u201d module. The framework is developed using Soft System Methodology (SSM) and Design Science Research Methodology (DSRM).",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390255",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "environmentalbigdatamodelandrecognitionofabnormalemissionfromenterprisedata",
        "booktitle": "2021 4th International Conference on Information Systems and Computer Aided Education",
        "doi": "10.1145/3482632.3484095",
        "author": [
            "Wu, Rui",
            "Cheng, Qian",
            "He, Lisong",
            "Cao, Zhenyu"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450389914",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "azonebaseddatalakearchitectureforiotsmallandbigdata",
        "booktitle": "Proceedings of the 25th International Database Engineering &amp; Applications Symposium",
        "doi": "10.1145/3472163.3472185",
        "author": [
            "Zhao, Yan",
            "Megdiche, Imen",
            "Ravat, Franck",
            "Dang, Vincent-nam"
        ],
        "keywords": [
            "Metadata, Stream IoT Data, Zone-based, Technical Architecture, Data Lake"
        ],
        "abstract": "Data lakes are supposed to enable analysts to perform more efficient and efficacious data analysis by crossing multiple existing data sources, processes and analyses. However, it is impossible to achieve that when a data lake does not have a metadata governance system that progressively capitalizes on all the performed analysis experiments. The objective of this paper is to have an easily accessible, reusable data lake that capitalizes on all user experiences. To meet this need, we propose an analysis-oriented metadata model for data lakes. This model includes the descriptive information of datasets and their attributes, as well as all metadata related to the machine learning analyzes performed on these datasets. To illustrate our metadata solution, we implemented a web application of data lake metadata management. This application allows users to find and use existing data, processes and analyses by searching relevant metadata stored in a NoSQL data store within the data lake. To demonstrate how to easily discover metadata with the application, we present two use cases, with real data, including datasets similarity detection and machine learning guidance.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390255",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "amethodofconstructingdistributedbigdataanalysismodelformachinelearningbasedoncloudcomputing",
        "booktitle": "2021 4th International Conference on Information Systems and Computer Aided Education",
        "doi": "10.1145/3482632.3484007",
        "author": [
            "Li, Jicai",
            "Liu, Dan"
        ],
        "keywords": [
            ""
        ],
        "abstract": "There are many big data analysis methods, and it is an effective method to build big data analysis model through machine learning. Big data is characterized by large data scale and long calculation cycle. In order to speed up the calculation speed and shorten the calculation cycle, distributed computing method is one of the effective methods to solve the above problems. With the wide application and rapid development of information technology, cloud computing as a new business computing model has attracted more and more attention. However, the security of cloud computing data storage model lacks reliability. Under the mainstream cloud computing and big data basic environment, building a better model from resource aggregation to analysis and mining, and modeling distributed big data analysis can provide high-reliability, high-security, high-efficiency analysis services for practical analysis and mining applications such as intelligence judgment, information deployment and control, stakeholder analysis and intelligent decision-making.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390255",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "researchontheapplicationofbigdatacloudcleaningsysteminphysicalfunctionsportstrainingmanagement",
        "booktitle": "2021 4th International Conference on Information Systems and Computer Aided Education",
        "doi": "10.1145/3482632.3487514",
        "author": [
            "Wang, Wenwen"
        ],
        "keywords": [
            ""
        ],
        "abstract": "The main purpose of physical function training is to solve the problems of many injuries, incorrect movement patterns, movement compensation and so on. In the era of big data, the scientific research field of sports training is gradually beginning to adopt the big data model for development, which will better form a new insight after data generation, collection, analysis and transformation. Therefore, this paper analyzes the application of big data intelligent cleaning system based on cloud computing in physical function training management. Students can quickly query running results through WEB query system, and administrators can download and modify students' results through WEB system. By using big data analysis technology, students are grouped according to their physical fitness test scores and BMI index, and their sports plans are formulated accordingly. A high-reliability real-time wireless receiving and sending sports monitoring system for big data is adopted, which realizes the functions of networking, unmanned and intelligent sports management.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450388825",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "identificationofbusinessintelligenceinbigdatamaintenanceofgovernmentsectorinputrajaya",
        "booktitle": "2021 10th International Conference on Software and Computer Applications",
        "doi": "10.1145/3457784.3457816",
        "author": [
            "Farhana, Ain",
            "Najib, Muhammad",
            "jalil, Rohaya",
            "Othman, Hajar",
            "Adnan, Yasmin"
        ],
        "keywords": [
            "Maintenance Management, Data Management,, Business Intelligence"
        ],
        "abstract": "This paper contributes significantly, which focuses on an intelligent system that lets the government make an integral part of decision-making and can be applied horizontally to solve the problems in maintenance practice through business intelligence. Accordingly, a real-time data management system for maintenance management is proposed in this paper. It looks at a real case study highlighting the need for proper data management in the government sector. Our findings bridge the gap of information technology inserted in government office buildings, with maintenance management being the domain. This paper demonstrates the underlying structure of the developed simulation model.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450385626",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "wouldyoulikeaquickpeekprovidingloggingsupporttomonitordataprocessinginbigdataapplications",
        "booktitle": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
        "doi": "10.1145/3468264.3468613",
        "author": [
            "Wang, Zehao",
            "Zhang, Haoxiang",
            "Chen, Tse-Hsun",
            "Wang, Shaowei"
        ],
        "keywords": [
            "Logging, Monitoring, Apache Spark"
        ],
        "abstract": "To analyze large-scale data efficiently, developers have created various big data processing frameworks (e.g., Apache Spark). These big data processing frameworks provide abstractions to developers so that they can focus on implementing the data analysis logic. In traditional software systems, developers leverage logging to monitor applications and record intermediate states to assist workload understanding and issue diagnosis. However, due to the abstraction and the peculiarity of big data frameworks, there is currently no effective monitoring approach for big data applications. In this paper, we first manually study 1,000 randomly sampled Spark-related questions on Stack Overflow to study their root causes and the type of information, if recorded, that can assist developers with motioning and diagnosis. Then, we design an approach, DPLOG, which assists developers with monitoring Spark applications. DPLOG leverages statistical sampling to minimize performance overhead and provides intermediate information and hint/warning messages for each data processing step of a chained method pipeline. We evaluate DPLOG on six benchmarking programs and find that DPLOG has a relatively small overhead (i.e., less than 10% increase in response time when processing 5GB data) compared to without using DPLOG, and reduce the overhead by over 500% compared to the baseline. Our user study with 20 developers shows that DPLOG can reduce the needed time to debug big data applications by 63% and the participants give DPLOG an average of 4.85/5 for its usefulness. The idea of DPLOG may be applied to other big data processing frameworks, and our study sheds light on future research opportunities in assisting developers with monitoring big data applications.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "9781450390255",
        "journal": null,
        "publisher": "Association for Computing Machinery",
        "title": "thearchitectureandsecuritydesignofbigdataplatformofthephysicalteachinginformationsystemincollegesanduniversities",
        "booktitle": "2021 4th International Conference on Information Systems and Computer Aided Education",
        "doi": "10.1145/3482632.3483061",
        "author": [
            "Han, Caibao"
        ],
        "keywords": [
            ""
        ],
        "abstract": "This paper constructs the logic structure of big data platform of physical teaching information system in Colleges and universities through Hadoop distributed system, which ensures the realization of management and monitoring function, data collection function, data storage and query function, data calculation function, security and management function. The security design is carried out from access control, data authorization and management, Hadoop security configuration to deal with the possible security problems of big data platform.",
        "year": "2021",
        "type_publication": "inproceedings",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "20960654",
        "isbn": null,
        "journal": "Big Data Mining and Analytics",
        "publisher": null,
        "title": "lotussqlsqlengineforhighperformancebigdatasystems",
        "booktitle": null,
        "doi": "10.26599/BDMA.2021.9020009",
        "author": [
            "Li, Xiaohan",
            "Yu, Bowen",
            "Feng, Guanyu",
            "Wang, Haojie",
            "Chen, Wenguang"
        ],
        "keywords": [
            "Structured Query Language",
            "Optimization",
            "Engines",
            "C++ languages",
            "Sparks",
            "Big Data",
            "Query processing",
            "big data",
            "C++",
            "Structured Query Language (SQL)",
            "query optimization"
        ],
        "abstract": "In recent years, Apache Spark has become the de facto standard for big data processing. SparkSQL is a module offering support for relational analysis on Spark with Structured Query Language (SQL). SparkSQL provides convenient data processing interfaces. Despite its efficient optimizer, SparkSQL still suffers from the inefficiency of Spark resulting from Java virtual machine and the unnecessary data serialization and deserialization. Adopting native languages such as C++ could help to avoid such bottlenecks. Benefiting from a bare-metal runtime environment and template usage, systems with C++ interfaces usually achieve superior performance. However, the complexity of native languages also increases the required programming and debugging efforts. In this work, we present LotusSQL, an engine to provide SQL support for dataset abstraction on a native backend Lotus. We employ a convenient SQL processing framework to deal with frontend jobs. Advanced query optimization technologies are added to improve the quality of execution plans. Above the storage design and user interface of the compute engine, LotusSQL implements a set of structured dataset operations with high efficiency and integrates them with the frontend. Evaluation results show that LotusSQL achieves a speedup of up to 9\u00d7 in certain queries and outperforms Spark SQL in a standard query benchmark by more than 2\u00d7 on average.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "blockchainwatermarkingforcompressivesensedimages",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3072196",
        "author": [
            "Li, Ming",
            "Zeng, Leilei",
            "Zhao, Le",
            "Yang, Renlin",
            "An, Dezhi",
            "Fan, Haiju"
        ],
        "keywords": [
            "Watermarking",
            "Blockchain",
            "Compressed sensing",
            "Big Data",
            "Image coding",
            "Privacy",
            "Peer-to-peer computing",
            "Blockchain",
            "IPFS",
            "compressed sensing",
            "watermarking",
            "data hiding"
        ],
        "abstract": "With the application of multimedia big data, the problems such as information leakage and data tampering have emerged. The security of images which is one of the most typical multimedia has become a major problem facing the large-scale open network environment. This paper proposed a blockchain-watermarking scheme to protect the privacy, integrity and availability of compressed sensed images, which effectively combines multimedia watermarking, compressed sensing, Interplanetary File System (IPFS) and blockchain technologies. Based on the reliable authentication of watermarking, the confidentiality protection of compressed sensing, the secure storage of IPFS, and the decentralization and non-tamperability of blockchain, the all-round security protection of the image big data based on compressive sensing can be realized. Experiments show that the proposed scheme is effective and feasible.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "researchontheimpactofbigdatacapabilitiesongovernmentssmartserviceperformanceempiricalevidencefromchina",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3056486",
        "author": [
            "Zhang, Airong",
            "Lv, Na"
        ],
        "keywords": [
            "Big Data",
            "Government",
            "Decision making",
            "Data models",
            "Mathematical model",
            "Technological innovation",
            "Information technology",
            "Big data system capabilities",
            "big data human capabilities",
            "big data management capabilities",
            "smart service performance",
            "structural equation model"
        ],
        "abstract": "The government of China seeks to improve e-government service quality and build a service-oriented government that citizens find satisfactory. To this end, big data is being used as a new tool of government service innovation. However, there is a lack of research on how big data affects the performance of government smart services. This article explores the influence mechanisms of government big data capabilities on the performance of smart service provision, utilizing the carding analysis of relevant literature, published both in China and abroad. To this end, a structural equation model was constructed. Using data from 289 valid questionnaires in Jiangsu, Shandong, Zhejiang, and other provinces and cities in China, the study tests internal mechanisms of big data capabilities and its effect on smart service performance. Following a new definition of government big data capability, the paper divides the capability into three dimensions: big data system capability, big data human capability and big data management capability. The main conclusions are as follows: (1) Big data management capability has a significant positive impact on big data human capability and big data system capability. (2) Big data system capability has a significant positive impact on big data human capability. (3) Big data system capability and big data management capability have a significant positive effect on smart service performance. (4) The impact of big data human capability on smart service performance is not however significant enough to bring about the improvements which the government seeks.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "27086240",
        "isbn": null,
        "journal": "Intelligent and Converged Networks",
        "publisher": null,
        "title": "truedatatestbedfor5gb5gintelligentnetwork",
        "booktitle": null,
        "doi": "10.23919/ICN.2021.0002",
        "author": [
            "Huang, Yongming",
            "Liu, Shengheng",
            "Zhang, Cheng",
            "You, Xiaohu",
            "Wu, Hequan"
        ],
        "keywords": [
            "Artificial intelligence",
            "5G mobile communication",
            "Optimization",
            "Wireless communication",
            "Engines",
            "Data acquisition",
            "Radio access networks",
            "true-data testbed",
            "wireless communication networks",
            "artificial intelligence (AI)",
            "big data",
            "internet of everything (IoE)"
        ],
        "abstract": "Future beyond fifth-generation (B5G) and sixth-generation (6G) mobile communications will shift from facilitating interpersonal communications to supporting internet of everything (IoE), where intelligent communications with full integration of big data and artificial intelligence (AI) will play an important role in improving network efficiency and provi di ng hi gh-quality servi ce. As a rapi d evolvi ng paradi gm, the AI-empowered mob i le communi cati ons demand large amounts of data acquired from real network environment for systematic test and verification. Hence, we build the world's first true-data testbed for 5G/B5G intelligent network (TTIN), which comprises 5G/B5G on-site experimental networks, data acquisition & data warehouse, and AI engine & network optimization. In the TTIN, true network data acquisition, storage, standardization, and analysis are available, which enable system-level online verification of B5G/6G-orientated key technologies and support data-driven network optimization through the closed-loop control mechanism. This paper elaborates on the system architecture and module design of TTIN. Detailed technical specifications and some of the established use cases are also showcased.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "developmentofusabilityenhancementmodelforunstructuredbigdatausingslr",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3089100",
        "author": [
            "Adnan, Kiran",
            "Akbar, Rehan",
            "Wang, Khor"
        ],
        "keywords": [
            "Usability",
            "Big Data",
            "Data models",
            "Data mining",
            "Bibliographies",
            "Protocols",
            "Systematics",
            "Big data",
            "data transformation",
            "data usability",
            "text data",
            "unstructured data",
            "usability enhancement"
        ],
        "abstract": "Unstructured text contains valuable information for a range of enterprise applications and informed decision making. Text analytics is used to extract valuable insights from unstructured big data. Among the most significant challenges of text analytics, quality and usability are critical in affecting the outcome of the analytical process. The enhancement in usability is important for the exploitation of unstructured data. Most of the existing literature focuses on the usability of structured data as compared to unstructured data whereas big data usability has been discussed merely in the context of its assessment. The existing approaches do not provide proper guidelines on the usability enhancement of unstructured data. In this study, a rigorous systematic literature review using PRISMA framework has been conducted to develop a model enhancing the usability of unstructured data bridging the research gap. The recent approaches and solutions for text analytics have been investigated thoroughly. The usability issues of unstructured text data and their consequences on data preparation for analytics have been identified. Defining the usability dimensions for unstructured big data, identification of the usability determinants, and developing a relationship between usability dimension and determinants to derive usability rules are the significant contributions of this research and are integrated to formulate the usability enhancement model. The proposed model is the major outcome of the research. It contributes to make unstructured data usable and facilitates the data preparation activities with more valuable data that eventually improve the analytical process.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "analyzingandevaluatingcriticalchallengesandpracticesforsoftwarevendororganizationstosecurebigdataoncloudcomputinganahpbasedsystematicapproach",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3100287",
        "author": [
            "Khan, Abudul",
            "Khan, Maseeh",
            "Khan, Javed",
            "Ahmad, Arshad",
            "Khan, Khalil",
            "Zamir, Muhammad",
            "Kim, Wonjoon",
            "Ijaz, Muhammad"
        ],
        "keywords": [
            "Cloud computing",
            "Security",
            "Big Data",
            "Software",
            "Organizations",
            "Social networking (online)",
            "STEM",
            "Security challenges",
            "big data",
            "cloud computing",
            "SLR",
            "vendor",
            "SPSS"
        ],
        "abstract": "Recently, its becomes easy to track down the data due to its availability in a large number. Although for data management, processing, and obtainability, cloud computing is considered a well-known approach for organizational development on the internet. Despite many advantages, cloud computing has still numerous security challenges that can affect the big-data usage on cloud computing. To find the security issues/challenges that are faced by software vendors\u2019 organizations we conducted a systematic literature review (SLR) through which we have find out 103 relevant research publications by developing a search string that is inspired by the research questions. This relevant data was comprised from different databases e.g. Google Scholar, IEEE Explore, ScienceDirect, ACM Digital Library, and SpringerLink. Furthermore, for the detailed literature review, we have accomplished all the steps in SLR, for example, development of SLR protocol, Initials and final assortment of the relevant data, data extraction, data quality assessment, and data synthesis. We identified fifteen (15) critical security challenges which are: data secrecy, geographical data location, unauthorized data access, lack of control, lack of data management, network-level issues, data integrity, data recovery, lack of trust, data sharing, data availability, asset issues, legal amenabilities, lack of quality, and lack of consistency. Furthermore, sixty four (64) standard practices are identified for these critical security challenges using the proposed SLR that could help vendor organizations to overcome the security challenges for big data. The findings of our research study demonstrate the resemblances and divergences in the identified security challenges in different periods, continents, databases, and methods. The proposed SLR will also support software vendor organizations for securing big data on the cloud computing platforms. This paper has the following content: in Section II, we have describe the Literature review; in Section III, research methodology is specified; in Section IV, the findings of the SLR and the analysis of result are discussed; in Section V, the limitations of this research are given; in Section VI, we discussed our conclusions and future work.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "frombigdatatodeepdatatosupportpeopleanalyticsforemployeeattritionprediction",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3074559",
        "author": [
            "Yahia, Nesrine",
            "Hlel, Jihen",
            "Colomo-Palacios, Ricardo"
        ],
        "keywords": [
            "Big Data",
            "Organizations",
            "Radio frequency",
            "Predictive models",
            "Support vector machines",
            "Data models",
            "Analytical models",
            "Deep people analytics",
            "employee attrition",
            "retention",
            "prediction",
            "interpretation",
            "policies recommendation"
        ],
        "abstract": "In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "towardspaddyricesmartfarmingareviewonbigdatamachinelearningandriceproductiontasks",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3069449",
        "author": [
            "Alfred, Rayner",
            "Obit, Joe",
            "Chin, Christie",
            "Haviluddin, Haviluddin",
            "Lim, Yuto"
        ],
        "keywords": [
            "Agriculture",
            "Digital agriculture",
            "Machine learning",
            "Machine learning algorithms",
            "Market research",
            "Big Data",
            "Internet of Things",
            "Rice production",
            "big data analytics",
            "Internet of Things",
            "machine learning",
            "smart farming",
            "precision agriculture",
            "agriculture supply chain"
        ],
        "abstract": "Big Data (BD), Machine Learning (ML) and Internet of Things (IoT) are expected to have a large impact on Smart Farming and involve the whole supply chain, particularly for rice production. The increasing amount and variety of data captured and obtained by these emerging technologies in IoT offer the rice smart farming strategy new abilities to predict changes and identify opportunities. The quality of data collected from sensors greatly influences the performance of the modelling processes using ML algorithms. These three elements (e.g., BD, ML and IoT) have been used tremendously to improve all areas of rice production processes in agriculture, which transform traditional rice farming practices into a new era of rice smart farming or rice precision agriculture. In this paper, we perform a survey of the latest research on intelligent data processing technology applied in agriculture, particularly in rice production. We describe the data captured and elaborate role of machine learning algorithms in paddy rice smart agriculture, by analyzing the applications of machine learning in various scenarios, smart irrigation for paddy rice, predicting paddy rice yield estimation, monitoring paddy rice growth, monitoring paddy rice disease, assessing quality of paddy rice and paddy rice sample classification. This paper also presents a framework that maps the activities defined in rice smart farming, data used in data modelling and machine learning algorithms used for each activity defined in the production and post-production phases of paddy rice. Based on the proposed mapping framework, our conclusion is that an efficient and effective integration of all these three technologies is very crucial that transform traditional rice cultivation practices into a new perspective of intelligence in rice precision agriculture. Finally, this paper also summarizes all the challenges and technological trends towards the exploitation of multiple sources in the era of big data in agriculture.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "constructfoodsafetytraceabilitysystemforpeopleshealthundertheinternetofthingsandbigdata",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3078536",
        "author": [
            "Zheng, Miaomiao",
            "Zhang, Shanshan",
            "Zhang, Yidan",
            "Hu, Baozhong"
        ],
        "keywords": [
            "Safety",
            "Big Data",
            "Internet of Things",
            "Production",
            "Radiofrequency identification",
            "Python",
            "Epidemics",
            "Two-dimensional code technology",
            "Internet of Things",
            "big data",
            "artificial intelligence",
            "food safety traceability system"
        ],
        "abstract": "In the context of epidemic prevention and control, food safety monitoring, data analysis and food safety traceability have become more important. At the same time, the most important reason for food safety issues is incomplete, opaque, and asymmetric information. The most fundamental way to solve these problems is to do a good job of traceability, and establish a reasonable and reliable food safety traceability system. The traceability system is currently an important means to ensure food quality and safety and solve the crisis of trust between consumers and the market. Research on food safety traceability systems based on big data, artificial intelligence and the Internet of Things provides ideas and methods to solve the problems of low credibility and difficult data storage in the application of traditional traceability systems. Therefore, this research takes rice as an example and proposes a food safety traceability system based on RFID two-dimensional code technology and big data storage technology in the Internet of Things. This article applies RFID technology to the entire system by analyzing the requirements of the system, designing the system database and database tables, encoding the two-dimensional code and generating the design for information entry. Using RFID radio frequency technology and the data storage function in big data to obtain information in the food production process. Finally, the whole process of food production information can be traced through the design of dynamic query platform and mobile terminal. In this research, the food safety traceability system based on big data and the Internet of Things guarantees the integrity, reliability and safety of traceability information from a technical level. This is an effective solution for enhancing the credibility of traceability information, ensuring the integrity of information, and optimizing the data storage structure.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "23327790",
        "isbn": null,
        "journal": "IEEE Transactions on Big Data",
        "publisher": null,
        "title": "hierarchicaldensitybasedclusteringusingmapreduce",
        "booktitle": null,
        "doi": "10.1109/TBDATA.2019.2907624",
        "author": [
            "Santos, Joelson",
            "Syed, Talat",
            "Naldi, Murilo",
            "Campello, Ricardo",
            "Sander, Joerg"
        ],
        "keywords": [
            "Clustering algorithms",
            "Partitioning algorithms",
            "Programming",
            "Data models",
            "Machine learning algorithms",
            "Big Data",
            "Computational modeling",
            "Density-based hierarchical clustering",
            "MapReduce",
            "big data"
        ],
        "abstract": "Hierarchical density-based clustering is a powerful tool for exploratory data analysis, which can play an important role in the understanding and organization of datasets. However, its applicability to large datasets is limited because the computational complexity of hierarchical clustering methods has a quadratic lower bound in the number of objects to be clustered. MapReduce is a popular programming model to speed up data mining and machine learning algorithms operating on large, possibly distributed datasets. In the literature, there have been attempts to parallelize algorithms such as Single-Linkage, which in principle can also be extended to the broader scope of hierarchical density-based clustering, but hierarchical clustering algorithms are inherently difficult to parallelize with MapReduce. In this paper, we discuss why adapting previous approaches to parallelize Single-Linkage clustering using MapReduce leads to very inefficient solutions when one wants to compute density-based clustering hierarchies. Preliminarily, we discuss one such solution, which is based on an exact, yet very computationally demanding, random blocks parallelization scheme. To be able to efficiently apply hierarchical density-based clustering to large datasets using MapReduce, we then propose a different parallelization scheme that computes an approximate clustering hierarchy based on a much faster, recursive sampling approach. This approach is based on HDBSCAN*, the state-of-the-art hierarchical density-based clustering algorithm, combined with a data summarization technique called data bubbles. The proposed method is evaluated in terms of both runtime and quality of the approximation on a number of datasets, showing its effectiveness and scalability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.344",
        "scimago_value": "0,959"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "faultjudgmentoftransmissioncablebasedonmultichanneldatafusionandtransferlearning",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3094231",
        "author": [
            "Zhang, Fujie",
            "Yao, Degui",
            "Zhang, Xiaofei",
            "Hu, Zhouming",
            "Zhu, Wenjun",
            "Ju, Yun"
        ],
        "keywords": [
            "Data models",
            "Communication cables",
            "Transfer learning",
            "Data integration",
            "Convolution",
            "Training",
            "Analytical models",
            "Quality of transmission cable",
            "transfer learning",
            "data fusion"
        ],
        "abstract": "Non-intrusive transmission cable monitoring is the latest advanced measurement technology for smart grids. It only samples the voltage on a certain part of the transmission cable, and uses intelligent algorithms to identify the quality, which has obvious advantages of low construction and maintenance costs. This paper established a model based on multi-channel data fusion and transfer learning to classify the quality of transmission cable. First, we used the ANSYS Maxwell simulation platform to obtain ten kinds of specific fault data, which solved the time cost of manual labeling. Then, we performed multi-channel data fusion on the original data, which strengthened the expression of important features and was more conducive to the training of the model. Next, we used Depthwise Separable Convolution (DSC) to speed up the learning of the model, and improve the accuracy of the classification. Finally, we transferred the model trained with simulation data into the real scene, realized the transfer from multi classes to two classes, the effectiveness was proved in experiments. The accuracy of the model built in the article to classify the quality of the transmission cables is 98.1%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "harmoniccharacteristicsdatadriventhdpredictionmethodforledsusingmeagrnnandimprovedadaboostalgorithm",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3059483",
        "author": [
            "Yang, Jingjian",
            "Ma, Hongyan",
            "Dou, Jiaming",
            "Guo, Rong"
        ],
        "keywords": [
            "Harmonic analysis",
            "Lighting",
            "Predictive models",
            "LED lamps",
            "Prediction algorithms",
            "Power quality",
            "Neurons",
            "LED lamps",
            "THD prediction",
            "ensemble learning",
            "mind evolution algorithm (MEA)",
            "generalized regression neural network (GRNN)"
        ],
        "abstract": "Light-emitting Diode (LED) lamps have been widely used due to versatility and energy efficiency. However, LEDs are nonlinear loads, the massive usage will inject harmonics into the lighting system, which has influenced the power quality. Total Harmonic Distortion (THD) is an important parameter to evaluate the power quality, but the prediction of THD for LEDs is a challenging task. This paper addresses this issue by designing harmonic characteristics detection experiment and using artificial intelligence algorithm. Firstly, LED lamps with different driving circuits were tested, the relevant data of each harmonic were sampled and analyzed. Then, a THD prediction method based on an improved AdaBoost algorithm is proposed. In this method, a Generalized Regression Neural Network (GRNN) model is established, and its parameters are optimized by Mind Evolution Algorithm (MEA) to improve the search ability of GRNN. On this basis, the AdaBoost algorithm is utilized to integrate multiple MEA-GRNN individuals to form a strong predictor, which improves the generalization ability of the model. To avoid the integration failure caused by improper selection of threshold value, a sigmoid adaptive factor is added to improve the accuracy of AdaBoost algorithm. Finally, the Ada-MEA-GRNN model is trained and simulated with the LED harmonic data collected by the experiment. The simulation results show that the prediction accuracy of the proposed method is better than BP and GRNN, which can reach 95.48%. Meanwhile, even if the input dimension is reduced, the error is still small.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "airqualitypredictionbasedonintegratedduallstmmodel",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3093430",
        "author": [
            "Chen, Hongqian",
            "Guan, Mengxi",
            "Li, Hui"
        ],
        "keywords": [
            "Air quality",
            "Atmospheric modeling",
            "Predictive models",
            "Data models",
            "Meteorology",
            "Time series analysis",
            "Forecasting",
            "Air quality prediction",
            "integrated dual model",
            "LSTM model with attention mechanism",
            "Seq2Seq technology",
            "XGBoosting tree"
        ],
        "abstract": "Air quality prediction is an important reference for meteorological forecast and air controlling, but over fitting often occurs in prediction algorithms based on a single model. Aiming at the complexity of air quality prediction, a prediction method based on integrated dual LSTM (Long Short-Term Memory) model was proposed in this paper. Firstly, the Seq2Seq (Sequence to Sequence) technology is used to establish a single-factor prediction model which can obtain the predicted value of each component in air quality data, independently. Each component of air quality is regarded as time series data in the forecasting process. Then, the LSTM model with attention mechanism is used as the multi-factor prediction model. The influencing factors of air quality, like the data of neighboring stations and weather data, are considered in the model. Finally, XGBoosting (eXtreme Gradient Boosting) tree is used to integrate two models. The final prediction results can be obtained by accumulating the predicted values of the optimal subtree nodes. Through evaluation and analysis using five evaluation methods, the proposed method has better performance in terms of error and model expression power. Compared with other various models, the precision of prediction data has been greatly improved in our model.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "animprovedpowerqualityevaluationforledlampbasedong1entropymethod",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3103052",
        "author": [
            "Dou, Jiaming",
            "Ma, Hongyan",
            "Yang, Jingjian",
            "Zhang, Yingda",
            "Guo, Rong"
        ],
        "keywords": [
            "Harmonic analysis",
            "LED lamps",
            "Power system harmonics",
            "Lighting",
            "Integrated circuit modeling",
            "Buildings",
            "Entropy",
            "Power quality",
            "evaluation approach",
            "light-emitting diode lamp",
            "sequential analysis method",
            "entropy weighting method"
        ],
        "abstract": "Nowadays, light emitting diode (LED) lamps have been widely utilized for lighting system due to its low-energy consumption. The harmonic emission standard is ignored by most of the manufacturers, high harmonic current will increase harmonic injection and cause fire risk. Existing research focuses on investigating harmonic emissions from several specific LED drivers, but a systematic evaluation approach is not given. The contribution of this paper proposed a LED harmonic evaluation in the management view, which can evaluate the harmonics of the LED lamps, accelerate the elimination of inferior LED lamps, and improve the power quality of distribution network. The evaluation approach combines G1 method and entropy method, which can make the weighting more scientific and rational. An evaluation model is established by collecting data, then the G1-entropy method is used to calculate the weights of harmonic characteristics in this model. Finally, we analyze and discuss the results, a specific evaluation approach is proposed, which can thoroughly and accurately represent the harmonic characteristics of LED lamps.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "anovelsimilarplayerclusteringmethodwithprivacypreservationforsportperformanceevaluationincloud",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3062735",
        "author": [
            "Ma, Rui",
            "Li, Jianqiang",
            "Xing, Baohui",
            "Zhao, Yuanyuan",
            "Liu, Yuwen",
            "Yan, Chao",
            "Yin, Hang"
        ],
        "keywords": [
            "Sports",
            "Privacy",
            "Data privacy",
            "Indexes",
            "Collaborative filtering",
            "Big Data",
            "Encoding",
            "Similar player clustering",
            "sport exercise score records",
            "SimHash",
            "privacy",
            "big data",
            "cloud platform"
        ],
        "abstract": "With the ever-increasing popularity of sports and health ideas, people are paying more attentions to gaining high-quality healthy life through various taking various sport items or exercises. Through observing and analyzing the past sport exercise score records, we can cluster the players into different categories, each of which share the same or similar sport preferences or performances. However, the sport exercise score records are often massive and often stored in different cloud platforms, which raise a big difficulty for time-efficient player clustering. Furthermore, the sport exercise score records are a kind of privacy for most players; therefore, it is often not rational or legal to release these sensitive data to the public for similar player clustering purpose. Considering the above two issues, we use SimHash, a kind of privacy-aware approximate neighbor search technique, for similar player clustering by analyzing the sport exercise score records distributed across different cloud platforms. Thus, we can realize privacy-aware similar player clustering through SimHash. At last, we provide a set of experiments to validate the advantages of our proposed privacy-aware similar player clustering algorithm. Reported experimental results show the effectiveness of our proposal in remedying the big data volume and privacy concerns in player clustering based on sport exercise score records.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "onlineincrementalminingbasedontrustedbehaviorinterval",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3130758",
        "author": [
            "Fang, Na",
            "Fang, Xianwen",
            "Lu, Ke",
            "Asare, Esther"
        ],
        "keywords": [
            "Data mining",
            "Analytical models",
            "Data models",
            "Heuristic algorithms",
            "Clustering algorithms",
            "Business",
            "Task analysis",
            "Online incremental mining",
            "trusted behavior interval",
            "event stream",
            "clustering",
            "process mining"
        ],
        "abstract": "Incremental mining improves the quality of process mining by analyzing the differences between event logs and a reference model to obtain valuable information to update the reference model. Existing incremental mining methods focus on offline logs by setting thresholds for analysis, which limits process mining efforts by the domain knowledge, log completeness, and business completion time. Aiming at these problems, a real-time incremental mining algorithm based on the trusted behavior interval is proposed to analyze online event streams for updating the reference model. First, a clustering technique to analyze an existing reference model selects the core structure of the model and calculates the trusted behavior interval. Then, the behavioral and structural relationships between the online event streams and the reference model are analyzed to obtain a valid candidate set. Based on this set, an incremental update algorithm is proposed to optimize the model structure to achieve an online dynamic update of the reference model. The proposed algorithm is implemented in PM4PY and Scikit-learn frameworks; a reasonable number of clusters is determined using the elbow method and validated with artificial and real data. Experimental results show that the algorithm improves the efficiency of incremental mining and enhances the quality of the model with both complete and incomplete data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "reliablefederatedlearningsystemsbasedonintelligentresourcesharingschemeforbigdatainternetofthings",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3101871",
        "author": [
            "Math, Sa",
            "Tam, Prohim",
            "Kim, Seokhoon"
        ],
        "keywords": [
            "Servers",
            "Internet of Things",
            "Reliability",
            "Computational modeling",
            "Cloud computing",
            "Quality of service",
            "Collaborative work",
            "Big data",
            "federated learning",
            "massive Internet of Things",
            "machine learning",
            "software-defined network"
        ],
        "abstract": "Federated learning (FL) is the up-to-date approach for privacy constraints Internet of Things (IoT) applications in next-generation mobile network (NGMN), 5th generation (5G), and 6th generation (6G), respectively. Due to 5G/6G is based on new radio (NR) technology, the multiple-input and multiple-output (MIMO) of radio services for heterogeneous IoT devices have been performed. The autonomous resource allocation and the intelligent quality of service class identity (IQCI) in mobile networks based on FL systems are obligated to meet the requirements of privacy constraints of IoT applications. In massive FL communications, the heterogeneous local devices propagate their local models and parameters over 5G/6G networks to the aggregation servers in edge cloud areas. Therefore, the assurance of network reliability is compulsory to facilitate end-to-end (E2E) reliability of FL communications and provide the satisfaction of model decisions. This paper proposed an intelligent lightweight scheme based on the reference software-defined networking (SDN) architecture to handle the massive FL communications between clients and aggregators to meet the mentioned perspectives. The handling method adjusts the model parameters and batches size of the individual client to reflect the apparent network conditions classified by the k-nearest neighbor (KNN) algorithm. The proposed system showed notable experimented metrics, including the E2E FL communication latency, throughput, system reliability, and model accuracy.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21511535",
        "isbn": null,
        "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "publisher": null,
        "title": "remotesensingestimationofchlorophyllaincaseiiwatersofcoastalareasthreebandmodelversusgeneticalgorithmartificialneuralnetworksmodel",
        "booktitle": null,
        "doi": "10.1109/JSTARS.2021.3066697",
        "author": [
            "Chen, Jinyue",
            "Chen, Shuisen",
            "Fu, Rao",
            "Wang, Chongyang",
            "Li, Dan",
            "Peng, Yongshi",
            "Wang, Li",
            "Jiang, Hao",
            "Zheng, Qiong"
        ],
        "keywords": [
            "Biological system modeling",
            "Water quality",
            "Reservoirs",
            "Estimation",
            "Water pollution",
            "Genetic algorithms",
            "Optical sensors",
            "Artificial neural networks (ANN)",
            "case-II waters",
            "chlorophyll-a (Chl-a)",
            "coastal areas",
            "genetic algorithm (GA)",
            "machine learning",
            "Sentinel 2 MSI",
            "three-band models (TBM)"
        ],
        "abstract": "Chlorophyll-a (Chl-a), an important indicator of phytoplankton biomass and eutrophication, is sensitive to water constitutes and optical characteristics. An integrated machine learning method of genetic algorithm and artificial neural networks (GA\u2013ANN) was developed to retrieve the concentration of Chl-a. In situ spectra and simultaneous water quality parameters of 107 samples from two reservoirs (Res) and coastal waters (CW) were used to calibrate GA\u2013ANN and three-band models (TBM) for comparison of Chl-a estimation. Both GA\u2013ANN and TBM methods perform well for the joint dataset (WGD) of Res and CW with the R2 exceeding 0.90, and the root mean square error (RMSE) of corresponding validation (N = 35) are 4.40 and 5.23 \u03bcg/L, respectively. Similarly, for independent dataset of Res (N = 45), GA\u2013ANN and TBM methods show robust performance: the R2 values are 0.87 and 0.80, respectively; and the corresponding RMSE values are 7.79 and 7.73 \u03bcg/L, respectively. For CW dataset (N = 62), the R2 values of two methods are 0.81 and 0.62, respectively; and the corresponding RMSE values are 0.79 and 1.32 \u03bcg/L, respectively. When the GA\u2013ANN and TBM models were applied to retrieve Chl-a concentration from the calibrated Sentinel 2 MSI reflectance data in two Res on October 20, 2019, however, the validated results of MSI-derived Chl-a concentrations using quasi-synchronous in situ data (N = 36) indicated that the GA\u2013ANN model outperforms TBM with higher R2 value (0.91 vs. 0.26) and smaller RMSE (4.41 vs. 13.85 \u03bcg/L) and mean absolute errors (3.40 vs. 11.87 \u03bcg/L) values. Although TBM has obvious overestimation of Chl-a concentration when applied to remote sensing image, we still thought that both GA\u2013ANN and TBM are useful methods for Chl-a estimation in case-II waters, and GA\u2013ANN performs marginally better with less deviation to measured Chl-a for multispectral remote sensing data. The ratio of TSS to Chl-a, experimental measurements, abundance of sampling points, and Chl-a concentration range are several important factors affecting the accuracy and robustness of GA\u2013ANN and TBM methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.784",
        "scimago_value": "1,246"
    },
    {
        "issnkey": "20960654",
        "isbn": null,
        "journal": "Big Data Mining and Analytics",
        "publisher": null,
        "title": "effectivedensitybasedclusteringalgorithmsforincompletedata",
        "booktitle": null,
        "doi": "10.26599/BDMA.2021.9020001",
        "author": [
            "Xue, Zhonghao",
            "Wang, Hongzhi"
        ],
        "keywords": [
            "Clustering algorithms",
            "Data mining",
            "Shape",
            "Big Data",
            "Bayes methods",
            "Optimization",
            "Clustering methods",
            "density-based clustering",
            "incomplete data",
            "clustering algorihtm"
        ],
        "abstract": "Density-based clustering is an important category among clustering algorithms. In real applications, manydatasets suffer from incompleteness. Traditional imputation technologies or other techniques for handling missingvalues are not suitable for density-based clustering and decrease clustering result quality. To avoid these problems, we develop a novel density-based clustering approach for incomplete data based on Bayesian theory, which conductsimputation and clustering concurrently and makes use of intermediate clustering results. To avoid the impact oflow-density areas inside non-convex clusters, we introduce a local imputation clustering algorithm, which aims toimpute points to high-density local areas. The performances of the proposed algorithms are evaluated using tensynthetic datasets and five real-world datasets with induced missing values. The experimental results show theeffectiveness of the proposed algorithms.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "researchonaproductqualitymonitoringmethodbasedonmultiscaleppyolo",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3085338",
        "author": [
            "Li, Yiting",
            "Huang, Haisong",
            "Chen, Qipeng",
            "Fan, Qingsong",
            "Quan, Huafeng"
        ],
        "keywords": [
            "Monitoring",
            "Production",
            "Feature extraction",
            "Object detection",
            "Kernel",
            "Convolution",
            "Quality assessment",
            "Quality monitoring",
            "PP-YOLO",
            "pruning",
            "deep learning"
        ],
        "abstract": "To monitor product quality in the production process in real time, this thesis proposes a quality monitoring model based on PaddlePaddle You Only Look Once (PP-YOLO). First, in the preprocessing stage, the data enhancement method and the K-means++ method are used to improve the robustness of the algorithm, and the generated anchor box can screen more refined features earlier. Second, ResNet50-vd with the deformable convolution idea is selected as the backbone of the detection model, the feature pyramid network structure and the composition of the loss function are improved, and the feature learning ability of the model is enhanced to enable it to detect multiple scales of defects. Finally, pruning is performed on the basis of the trained model to reduce the number of model parameters so that it can be deployed in industrial scenarios with limited hardware conditions. Experimental results show that the proposed quality monitoring model can meet the requirements for detection speed and accuracy in actual production, providing a new concept for the deployment of deep learning models in the industrial field.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "15580644",
        "isbn": null,
        "journal": "IEEE Transactions on Geoscience and Remote Sensing",
        "publisher": null,
        "title": "multisensordatafusionforcloudremovalinglobalandallseasonsentinel2imagery",
        "booktitle": null,
        "doi": "10.1109/TGRS.2020.3024744",
        "author": [
            "Ebel, Patrick",
            "Meraner, Andrea",
            "Schmitt, Michael",
            "Zhu, Xiao"
        ],
        "keywords": [
            "Clouds",
            "Optical imaging",
            "Cloud computing",
            "Earth",
            "Optical sensors",
            "Data integration",
            "Image reconstruction",
            "Cloud removal",
            "data fusion",
            "deep learning",
            "generative adversarial network (GAN)",
            "optical imagery",
            "synthetic aperture radar (SAR)-optical"
        ],
        "abstract": "The majority of optical observations acquired via spaceborne Earth imagery are affected by clouds. While there is numerous prior work on reconstructing cloud-covered information, previous studies are, oftentimes, confined to narrowly defined regions of interest, raising the question of whether an approach can generalize to a diverse set of observations acquired at variable cloud coverage or in different regions and seasons. We target the challenge of generalization by curating a large novel data set for training new cloud removal approaches and evaluate two recently proposed performance metrics of image quality and diversity. Our data set is the first publically available to contain a global sample of coregistered radar and optical observations, cloudy and cloud-free. Based on the observation that cloud coverage varies widely between clear skies and absolute coverage, we propose a novel model that can deal with either extreme and evaluate its performance on our proposed data set. Finally, we demonstrate the superiority of training models on real over synthetic data, underlining the need for a carefully curated data set of real observations. To facilitate future research, our data set is made available online.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.600",
        "scimago_value": "2,141"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "deeplearningandblockchainempoweredsecurityframeworkforintelligent5genablediot",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3077069",
        "author": [
            "Rathore, Shailendra",
            "Park, Jong",
            "Chang, Hangbae"
        ],
        "keywords": [
            "Security",
            "Internet of Things",
            "Data analysis",
            "Blockchain",
            "Reliability",
            "5G mobile communication",
            "Quality of service",
            "Internet of Things",
            "security attack detection",
            "edge computing",
            "fog computing",
            "blockchain",
            "software-defined networking"
        ],
        "abstract": "Recently, many IoT applications, such as smart transportation, healthcare, and virtual and augmented reality experiences, have emerged with fifth-generation (5G) technology to enhance the Quality of Service (QoS) and user experience. The revolution of 5G-enabled IoT supports distinct attributes, including lower latency, higher system capacity, high data rate, and energy saving. However, such revolution also delivers considerable increment in data generation that further leads to a major requirement of intelligent and effective data analytic operation across the network. Furthermore, data growth gives rise to data security and privacy concerns, such as breach and loss of sensitive data. The conventional data analytic and security methods do not meet the requirement of 5G-enabled IoT including its unique characteristic of low latency and high throughput. In this paper, we propose a Deep Learning (DL) and blockchain-empowered security framework for intelligent 5G-enabled IoT that leverages DL competency for intelligent data analysis operation and blockchain for data security. The framework's hierarchical architecture wherein DL and blockchain operations emerge across the four layers of cloud, fog, edge, and user is presented. The framework is simulated and analyzed, employing various standard measures of latency, accuracy, and security to demonstrate its validity in practical applications.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "acourseteacherrecommendationalgorithmbasedonimprovedlatentfactormodelandpersonalrank",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3101469",
        "author": [
            "Yao, Dunhong",
            "Deng, Xiaowu"
        ],
        "keywords": [
            "Education",
            "Prediction algorithms",
            "Matrix decomposition",
            "Correlation",
            "Sparse matrices",
            "Bipartite graph",
            "Data models",
            "LFM",
            "PersonalRank",
            "PRLFM",
            "teacher recommendation",
            "teaching quality"
        ],
        "abstract": "To scientifically and accurately recommend suitable teachers for university courses and improve teaching quality, designing an effective recommendation algorithm is necessary. Therefore, we construct quantitative models of teacher characteristics, course characteristics, and teaching evaluations under the theories and methods of education and build a sparse experimental data matrix based on the quantified data. On this basis, we propose a teacher recommendation algorithm (PRLFM) based on the improved latent factor model (LFM) and the improved PersonalRank algorithm. Firstly, the improved LFM is used to predict the evaluation scores of those courses that teachers have not taught. The scores which are higher than the specified threshold are used to fill the corresponding missing items in the sparse matrix to reduce the matrix's sparsity. Then, the bipartite graph model based on the teacher set and course set is constructed according to the filled experimental data matrix. The weight of edges in the bipartite graph is replaced by the teacher and course's evaluation score multiplied by the course difficulty, which reflects the correlation between course and evaluation score. Next, an improved probability transition matrix based on the bipartite graph is constructed. The access probability in the matrix is replaced by the node's out degree's reciprocal multiplied by the edge's weight. The correlation degree between the course and all teachers is quickly calculated using the matrix algorithm of PersonalRank. Finally, a teacher recommendation model is constructed to realize teachers' top-N recommendation by combining the correlation degree with teachers' characteristics. Experiments show that the PRLFM algorithm can effectively improve the accuracy of prediction and top-N recommendation. It solves the problem of lack of scientific basis in recommending suitable teachers for university courses and improving the teaching quality.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "15579964",
        "isbn": null,
        "journal": "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
        "publisher": null,
        "title": "rnaseqranrpackageforautomatedtwogrouprnaseqanalysisworkflow",
        "booktitle": null,
        "doi": "10.1109/TCBB.2019.2956708",
        "author": [
            "Chao, Kuan-Hao",
            "Hsiao, Yi-Wen",
            "Lee, Yi-Fang",
            "Lee, Chien-Yueh",
            "Lai, Liang-Chuan",
            "Tsai, Mong-Hsun",
            "Lu, Tzu-Pin",
            "Chuang, Eric"
        ],
        "keywords": [
            "Bioinformatics",
            "Genomics",
            "Quality assessment",
            "Pipelines",
            "Electronic mail",
            "RNA",
            "RNA-Seq",
            "analysis workflow",
            "pipeline",
            "R",
            "bioconductor",
            "transcriptome assembly",
            "differential expression analysis",
            "gene expression",
            "statistical analysis",
            "visualization"
        ],
        "abstract": "RNA-Seq analysis has revolutionized researchers\u2019 understanding of the transcriptome in biological research. Assessing the differences in transcriptomic profiles between tissue samples or patient groups enables researchers to explore the underlying biological impact of transcription. RNA-Seq analysis requires multiple processing steps and huge computational capabilities. There are many well-developed R packages for individual steps; however, there are few R/Bioconductor packages that integrate existing software tools into a comprehensive RNA-Seq analysis and provide fundamental end-to-end results in pure R environment so that researchers can quickly and easily get fundamental information in big sequencing data. To address this need, we have developed the open source R/Bioconductor package, RNASeqR. It allows users to run an automated RNA-Seq analysis with only six steps, producing essential tabular and graphical results for further biological interpretation. The features of RNASeqR include: six-step analysis, comprehensive visualization, background execution version, and the integration of both R and command-line software. RNASeqR provides fast, light-weight, and easy-to-run RNA-Seq analysis pipeline in pure R environment. It allows users to efficiently utilize popular software tools, including both R/Bioconductor and command-line tools, without predefining the resources or environments. RNASeqR is freely available for Linux and macOS operating systems from Bioconductor (https://bioconductor.org/packages/release/bioc/html/RNASeqR.html).",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.710",
        "scimago_value": "0,745"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "informationsecuritymonitoringandmanagementmethodbasedonbigdataintheinternetofthingsenvironment",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3064350",
        "author": [
            "Liang, Wuchao",
            "Li, Wenning",
            "Feng, Lili"
        ],
        "keywords": [
            "Monitoring",
            "Information security",
            "Business",
            "Pollution",
            "Internet of Things",
            "Big Data",
            "Information security",
            "big data",
            "evaluation model",
            "monitoring mechanism",
            "Internet of Things"
        ],
        "abstract": "As an important part of the new generation of information technology, the Internet of Things (IoT), with its ubiquitous connection and service characteristics, has penetrated into various fields of application and played an important role. In this paper, based on the study of the basic technology of the environmental Internet of Things, combined with the service-oriented technology architecture SOA, J2EE, multi-level system architecture MVC, real-time database and other technologies and project practice experience, summarized and proposed a kind of environmental quality monitoring integrated management platform design and implementation feasibility scheme. Firstly, the background of the era of big data is described in detail, the urgency and necessity of information security monitoring under the background of big data is clarified, and the three elements of information security monitoring mechanism, namely network monitoring personnel, environment and technology, are proposed, and the three elements as the starting point to establish the information security monitoring mechanism; Starting from the relevant monitoring strategies and safety monitoring technologies, this paper explains the basic principles of constructing the evaluation index system, and establishes the evaluation index system according to the key influencing factors of enterprise information security level in the environment of big data. AHP fuzzy comprehensive evaluation method is chosen on the basis of analyzing various comprehensive evaluation methods, and the weight of each evaluation index is determined and the comprehensive evaluation model is constructed. The establishment of information security monitoring and evaluation system, the use of information security monitoring and evaluation system, for information security monitoring work to provide reference standards. Finally, on the basis of the foregoing, relevant strategies for information security monitoring are proposed, and necessary suggestions are provided for information security work.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "alinkqualityestimationmethodforwirelesssensornetworksbasedondeepforest",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2020.3047648",
        "author": [
            "He, Mu",
            "Shu, Jian"
        ],
        "keywords": [
            "Estimation",
            "Signal to noise ratio",
            "Forestry",
            "Heuristic algorithms",
            "Wireless sensor networks",
            "Clustering algorithms",
            "Wireless sensor networks",
            "link quality estimation",
            "deep forest",
            "stratified sampling"
        ],
        "abstract": "In wireless sensor networks, sensor nodes, the miniature embedded devices, have limitation of energy, storage, computing, and etc. One of the tasks of the nodes is to use their limited resources to complete work efficiently. Choosing high quality link communication can effectively save energy. In this paper, we propose a link quality estimation model that is based on deep forest. To avoid a noise sample becoming a center point in the clustering, we use an improved K-medoids algorithm based on step increasing and optimizing medoids (INCK) when dividing the link quality grades. During the sample preprocessing stage, the Pauta criterion is used to delete the noise link samples, and we fill the mean value of each grade into the missing values. The feature extraction performance of deep forest is improved by combining the stratified sampling to change the unbalance distribution of link quality samples. And then the Stratified Sampling Cascade Forest link quality estimation (SCForest-LQE) is constructed by combining stratified sampling with cascade forest. The experiments are conducted in three real application scenarios. Compared with the existing six link quality estimation models, SCForest-LQE has better estimation performance and stability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "multimodalmedicalimagefusionbasedongaborrepresentationcombinationofmulticnnandfuzzyneuralnetwork",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3075953",
        "author": [
            "Wang, Lifang",
            "Zhang, Jin",
            "Liu, Yang",
            "Mi, Jia",
            "Zhang, Jiong"
        ],
        "keywords": [
            "Medical diagnostic imaging",
            "Computed tomography",
            "Gabor filters",
            "Image edge detection",
            "Feature extraction",
            "Filter banks",
            "Medical image fusion",
            "G-CNNs",
            "Gabor representation",
            "convolutional neural network",
            "fuzzy neural network"
        ],
        "abstract": "Aiming at the current multimodal medical image fusion methods that cannot fully characterize the complex textures and edge information of the lesion in the fused image, a method based on Gabor representation of multi-CNN combination and fuzzy neural network is proposed. This method first filters the CT and MR image sets through a set of Gabor filter banks with different proportions and directions to obtain different Gabor representations pairs of CT and MR, each pair of different Gabor representations is used to train the corresponding CNN to generate a G- CNN and multiple G- CNN form a G- CNN group, namely G- CNNs; then when fusing CT and MR images, CT and MR are represented by Gabors to get Gabor representation pairs firstly, each Gabor representation pair is put into the corresponding trained G- CNN for preliminary fusion, then use the fuzzy neural network to fuse multiple outputs of the G- CNNs to obtain the final fused image. Compared with the nine recent state-of-the-art multimodal fusion methods, the average mutual information of the three groups of experiments has increased by 13%, 10.3%, and 10% respectively; the average spatial frequency has increased by 10.3%, 20%, and 10.7%; the average standard deviation has increased respectively 12.4%, 10.8%, 14.4%; the average edge retention information increased by 33.5%, 22%, and 43%. The experimental results show that the proposed fusion method is significantly better than the other comparative fusion methods in objective evaluation and visual quality. It has the best performance on the four indicators and can better integrate the rich texture features and the clear edge information of the source images into the final fused image, which improves the quality of multimodal medical image fusion, and effectively assists doctors in disease diagnosis.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "amodifiedrandomforestbasedonkappameasureandbinaryartificialbeecolonyalgorithm",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3105796",
        "author": [
            "Zhang, Chen",
            "Wang, Xiaofeng",
            "Chen, Shengbing",
            "Li, Hong",
            "Wu, Xiaoxuan",
            "Zhang, Xin"
        ],
        "keywords": [
            "Random forests",
            "Vegetation",
            "Decision trees",
            "Artificial bee colony algorithm",
            "Forestry",
            "Training",
            "Radio frequency",
            "Random forest",
            "Kappa measure",
            "artificial bee colony algorithm",
            "haze prediction"
        ],
        "abstract": "Random forest (RF) is an ensemble classifier method, all decision trees participate in voting, some low-quality decision trees will reduce the accuracy of random forest. To improve the accuracy of random forest, decision trees with larger degree of diversity and higher classification accuracy are selected for voting. In this paper, the RF based on Kappa measure and the improved binary artificial bee colony algorithm (IBABC) are proposed. Firstly, Kappa measure is used for pre-pruning, and the decision trees with larger degree of diversity are selected from the forest. Then, the crossover operator and leaping operator are applied in ABC, and the improved binary ABC is used for secondary pruning, and the decision trees with better performance are selected for voting. The proposed method (Kappa+IBABC) are tested on a quantity of UCI datasets. Computational results demonstrate that Kappa+IBABC improves the performance on most datasets with fewer decision trees. The Wilcoxon signed-rank test is used to verify the significant difference between the Kappa+IBABC method and other pruning methods. In addition, Chinese haze pollution is becoming more and more serious. This proposed method is used to predict haze weather and has achieved good results.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21511535",
        "isbn": null,
        "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "publisher": null,
        "title": "theimpactsofimpervioussurfaceonwaterqualityintheurbanagglomerationsofmiddleandlowerreachesoftheyangtzerivereconomicbeltfromremotelysenseddata",
        "booktitle": null,
        "doi": "10.1109/JSTARS.2021.3106038",
        "author": [
            "Li, Zhihui",
            "Peng, Lu",
            "Wu, Feng"
        ],
        "keywords": [
            "Water quality",
            "Rivers",
            "Land surface",
            "Remote sensing",
            "Belts",
            "Monitoring",
            "Manganese",
            "Impervious surface",
            "rapid urbanization",
            "remote sensing",
            "segmented regression",
            "threshold",
            "water quality",
            "Yangtze River economic belt"
        ],
        "abstract": "The Urban Agglomerations of Middle and Lower Reaches of the Yangtze River Economic Belt (UAMLYREB) have experienced rapid and intense urbanization over the past decades with natural ecosystems being converted to impervious surfaces. Thus, impervious surfaces are recognized as critical parameters when considering the effect of urbanization on water quality. While understanding how the threshold of impervious surfaces affects water quality has been a hot topic, there has been little quantitative analysis on how such thresholds change during rapid urbanization periods across large urban areas. To remedy this deficiency, this article made use of remotely-sensed impervious surface area data and in situ water quality monitoring observations for the period 2000 to 2018 to quantitively derive the temporal variation in the thresholds of the percentage of the impervious surface area (PISA) when inferring the relationship between PISA and a set of water quality indicators for a selection of watersheds within the UAMLYREB. We employed segmented regression model to derive the nonlinear relationship between PISA, the water quality indicators, and the PISA-related thresholds. Our results indicate that PISA may be considered a useful water quality indicator over watershed spatial scales. We also found that the threshold effects differed between water quality indicators (DO, CODMn, NH3-N), where, except for NH3-N, the indicators showed a PISA threshold of 30.08 to 42.34%, with slight variations over the study period. These results imply that maintaining PISA to be around 30% in watershed areas may be sufficient to mitigate against water quality degradation during the urbanization process.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.784",
        "scimago_value": "1,246"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "anovelmethodtopredictlayingratebasedonmultipleenvironmentvariables",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3105189",
        "author": [
            "Yin, Hang",
            "Liu, Chuanyun",
            "Gao, Yacui",
            "Fan, Wenting",
            "Xiao, Bin",
            "Cao, Liang",
            "Hassan, Shahbaz",
            "Liu, Shuangyin"
        ],
        "keywords": [
            "Random forests",
            "Production",
            "Predictive models",
            "Logic gates",
            "Computer architecture",
            "Radio frequency",
            "Indexes",
            "Long short-term memory (LSTM)",
            "random forests (RF)",
            "egg laying rate",
            "feature importance selection"
        ],
        "abstract": "Realizing an accurate laying rate prediction based on environmental factors plays a vital role in livestock and poultry breeding. In this paper, multiple environmental factors were considered to improve the accuracy of egg production rate prediction. A method was proposed by combining the Random Forest (RF) and Long Short-Term Memory (LSTM) to analyze the impact of the external environmental factors on the laying rate. Firstly, using RF, feature importance selection was implemented on environmental factors affecting laying rate. Secondly, the extreme Gradient Boosting (XGBoost) was introduced as a comparison to evaluate the accuracy and reliability of the RF feature importance selection. Finally, by discarding the features with low importance one by one, the multi-variable RF-LSTM laying rate prediction was conducted. Experiment results showed that the proposed RF-LSTM method significantly improved the prediction accuracy on laying rate.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21511535",
        "isbn": null,
        "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "publisher": null,
        "title": "researchongenericopticalremotesensingproductsareviewofscientificexplorationtechnologyresearchandengineeringapplication",
        "booktitle": null,
        "doi": "10.1109/JSTARS.2021.3062411",
        "author": [
            "Liu, Yang",
            "Zuo, Xianyu",
            "Tian, Junfeng",
            "Li, Shenshen",
            "Cai, Kun",
            "Zhang, Wanjun"
        ],
        "keywords": [
            "Remote sensing",
            "Indexes",
            "Vegetation mapping",
            "Optical sensors",
            "Satellites",
            "Earth Observing System",
            "Reflectivity",
            "Generic optical remote sensing products (ORSPs)",
            "high-resolution earth observation system",
            "inversion technology",
            "quality evaluation",
            "remote sensing (RS) application",
            "RS product category",
            "validation"
        ],
        "abstract": "With the initial establishment of global earth observation system in various countries, more and more high-resolution remote sensing data of multisource, multitemporal, multiscale, and different types of satellites are obtained. It is urgent to explore the advanced basic theory of remote sensing information science, design high-performance generic key technologies of remote sensing information system and global positioning system, and study complex engineering system of remote sensing applications and geographic information system. In this article, the basic theory exploration, inversion technology research, and engineering application design and development of generic optical remote sensing product (ORSP) are systematically reviewed. We classify the ORSP scientifically, review the main algorithms and application scope of 16 kinds of generic ORSP, and expound the validation and quality evaluation methods of ORSP in engineering application. Furthermore, we analyze the current core problems and solutions, and prospects for the state-of-the-art research and the future development trend of generic ORSP. This will provide valuable reference for scientific research and construction of high-resolution earth observation system.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.784",
        "scimago_value": "1,246"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "adversarialreconstructionlossfordomaingeneralization",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3066041",
        "author": [
            "Bekkouch, Imad",
            "Nicolae, Drago\u015f",
            "Khan, Adil",
            "Kazmi, S.",
            "Khattak, Asad",
            "Ibragimov, Bulat"
        ],
        "keywords": [
            "Data models",
            "Deep learning",
            "Image reconstruction",
            "Training",
            "Computational modeling",
            "Feature extraction",
            "Transfer learning",
            "Computer vision",
            "deep learning",
            "domain adaptation",
            "domain generalization",
            "transfer learning"
        ],
        "abstract": "The biggest fear when deploying machine learning models to the real world is their ability to handle the new data. This problem is significant especially in medicine, where models trained on rich high-quality data extracted from large hospitals do not scale to small regional hospitals. One of the clinical challenges addressed in this work is magnetic resonance image generalization for improved visualization and diagnosis of hip abnormalities such as femoroacetabular impingement and dysplasia. Domain Generalization (DG) is a field in machine learning that tries to solve the model's dependency on the training data by leveraging many related but different data sources. We present a new method for DG that is both efficient and fast, unlike the most current state of art methods, which add a substantial computational burden making it hard to fine-tune. Our model trains an autoencoder setting on top of the classifier, but the encoder is trained on the adversarial reconstruction loss forcing it to forget style information while extracting features useful for classification. Our approach aims to force the encoder to generate domain-invariant representations that are still category informative by pushing it in both directions. Our method has proven universal and was validated on four different benchmarks for domain generalization, outperforming state of the art on RMNIST, VLCS and IXMAS with a 0.70% increase in accuracy and providing comparable results on PACS with a 0.02% difference. Our method was also evaluated for unsupervised domain adaptation and has shown to be quite an effective method against over-fitting.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "agradientbasedclusteringformultidatabasemining",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3050404",
        "author": [
            "Miloudi, Salim",
            "Wang, Yulin",
            "Ding, Wenjia"
        ],
        "keywords": [
            "Databases",
            "Itemsets",
            "Clustering algorithms",
            "Data models",
            "Prototypes",
            "Computer science",
            "Computational modeling",
            "Multi-database mining",
            "graph clustering",
            "dual gradient descent",
            "quasi-convex optimization",
            "similarity measure"
        ],
        "abstract": "Multinational corporations have multiple databases distributed throughout their branches, which store millions of transactions per day. For business applications, identifying disjoint clusters of similar and relevant databases contributes to learning the common buying patterns among customers and also increases the profits by targeting potential clients in the future. This process is called clustering, which is an important unsupervised technique for big data mining. In this article, we present an effective approach to search for the optimal clustering of multiple transaction databases in a weighted undirected similarity graph. To assess the clustering quality, we use dual gradient descent to minimize a constrained quasi-convex loss function whose parameters will determine the edges needed to form the optimal database clusters in the graph. Therefore, finding the global minimum is guaranteed in a finite and short time compared with the existing non-convex objectives where all possible candidate clusterings are generated to find the ideal clustering. Moreover, our algorithm does not require specifying the number of clusters a priori and uses a disjoint-set forest data structure to maintain and keep track of the clusters as they are updated. Through a series of experiments on public data samples and precomputed similarity matrices, we show that our algorithm is more accurate and faster in practice than the existing clustering algorithms for multi-database mining.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "astackeddenoisingautoencoderandlongshorttermmemoryapproachwithrulebasedrefinementtoextractvalidsemantictrajectories",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3080288",
        "author": [
            "Yustiawan, Yoga",
            "Ramadhan, Hani",
            "Kwon, Joonho"
        ],
        "keywords": [
            "Semantics",
            "Trajectory",
            "Location awareness",
            "Hidden Markov models",
            "Deep learning",
            "Noise measurement",
            "Indoor environment",
            "Deep learning",
            "indoor localization",
            "the Internet of Things",
            "rule-based refinement",
            "semantic trajectories"
        ],
        "abstract": "Indoor location-based services have been widely investigated to take advantage of semantic trajectories for providing user oriented services in indoor environments. Although indoor semantic trajectories can provide seamless understanding to users regarding the provided location-based services, studies on the application of deep learning approaches for robust and valid semantic indoor localization are lacking. In this study, we combined a stacked denoising autoencoder and long short term memory technique with a rule-based refinement method applying a rule-based hidden Markov model (HMM) to perform robust and valid semantic trajectory extraction. In particular, our rule-based HMM approach incorporates a direct set of rules into HMM to resolve invalid movements of the extracted semantic trajectories and is extensible to various deep learning techniques. We compared the performance of our proposed approach with that of other cutting-edge deep learning approaches on two different real-world data sets. The experimental results demonstrate the feasibility of our proposed approach to produce more robust and valid semantic trajectories.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "ssiisecuredandhighqualitysteganographyusingintelligenthybridoptimizationalgorithmsforiot",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3089357",
        "author": [
            "Dhawan, Sachin",
            "Chakraborty, Chinmay",
            "Frnda, Jaroslav",
            "Gupta, Rashmi",
            "Rana, Arun",
            "Pani, Subhendu"
        ],
        "keywords": [
            "Security",
            "Particle swarm optimization",
            "Encryption",
            "Image edge detection",
            "Payloads",
            "Optimization",
            "Medical diagnostic imaging",
            "Steganography",
            "encryption",
            "embedding process",
            "salp swarm optimization",
            "hybrid fuzzy neural network"
        ],
        "abstract": "Internet of Things (IoT) is a domain where the transfer of big data is taking place every single second. The security of these data is a challenging task; however, security challenges can be mitigated with cryptography and steganography techniques. These techniques are crucial when dealing with user authentication and data privacy. In the proposed work, a highly secured technique is proposed using IoT protocol and steganography. This work proposes an image steganography procedure by utilizing the combination of various algorithms that build the security of the secret data by utilizing Binary bit-plane decomposition (BBPD) based image encryption technique. Thereafter a Salp Swarm Optimization Algorithm (SSOA) based adaptive embedding process is proposed to increase the payload capacity by setting different parameters in the steganographic embedding function for edge and smooth blocks. Here the SSOA algorithm is used to localize the edge and smooth blocks efficiently. Then, the hybrid Fuzzy Neural Network with a backpropagation learning algorithm is used to enhance the quality of the stego images. Then these stego images are transferred to the destination in the highly secured protocol of IoT. The proposed steganography technique shows better results in terms of security, image quality, and payload capacity in comparison with the existing state of art methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "pluggablemicronetworkforlayerconfigurationrelayinadynamicdeepneuralsurface",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3110709",
        "author": [
            "Khan, Farhat",
            "Aziz, Izzatdin",
            "Akhir, Emilia"
        ],
        "keywords": [
            "Convolution",
            "Training",
            "Logic gates",
            "Feature extraction",
            "Computational modeling",
            "Adaptive systems",
            "Relays",
            "Convolution neural network",
            "deep learning",
            "dynamic neural structure",
            "micronetwork",
            "multilayer perceptron"
        ],
        "abstract": "The classical convolution neural network architecture adheres to static declaration procedures, which means that the shape of computation is usually predefined and the computation graph is fixed. In this research, the concept of a pluggable micronetwork, which relaxes the static declaration constraint by dynamic layer configuration relay, is proposed. The micronetwork consists of several parallel convolutional layer configurations and relays only the layer settings, incurring a minimum loss. The configuration selection logic is based on the conditional computation method, which is implemented as an output layer of the proposed micronetwork. The proposed micronetwork is implemented as an independent pluggable unit and can be used anywhere on the deep learning decision surface with no or minimal configuration changes. The MNIST, FMNIST, CIFAR-10 and STL-10 datasets have been used to validate the proposed research. The proposed technique is proven to be efficient and achieves appropriate validity of the research by obtaining state-of-the-art performance in fewer iterations with wider and compact convolution models. We also naively attempt to discuss the involved computational complexities in these advanced deep neural structures.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21511535",
        "isbn": null,
        "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "publisher": null,
        "title": "adaptiveresourceoptimizededgefederatedlearninginrealtimeimagesensingclassifications",
        "booktitle": null,
        "doi": "10.1109/JSTARS.2021.3120724",
        "author": [
            "Tam, Prohim",
            "Math, Sa",
            "Nam, Chaebeen",
            "Kim, Seokhoon"
        ],
        "keywords": [
            "Computational modeling",
            "Sensors",
            "Data models",
            "Servers",
            "Real-time systems",
            "Adaptation models",
            "Resource management",
            "Convolutional neural networks (CNN)",
            "deep q-learning (DQL)",
            "federated learning (FL)",
            "quality of service (QoS)",
            "real-time image classifications"
        ],
        "abstract": "With the exponential growth of the Internet of things (IoT) in remote sensing image applications, network resource orchestration and data privacy are significant aspects to handle in bigdata cellular networks. The image data sharing procedure toward central cloud servers in order to perform real-time classifications has leaked client personalization and heavily burdened the communication networks. Thus, the deployment of IoT image sensors in privacy-constrained sectors requires an optimized federated learning (FL) scheme to efficiently consider both aspects of securing data privacy and maximizing the model accuracy with sufficient communication and computation resources. In this article, an adaptive model communication scheme with virtual resource optimization for edge FL is proposed by converging a deep q-learning algorithm to enforce a self-learning agent interacting with network functions virtualization orchestrator and software-defined networking based architecture. The agent targets to optimize the resource control policy of virtual multi-access edge computing entities in virtualized infrastructure manager. The proposed scheme trains the learning model and weighs the optimal actions for particular network states by using an epsilon-greedy strategy. In the exploitation phase, the scheme considers multiple spatial-resolution sensing conditions and allocates computation offloading resources for global multiconvolutional neural networks model aggregation based on the congestion states. In the simulation results, the quality of service and global collaborative model performance metrics were evaluated in terms of delay, packet drop ratios, packet delivery ratios, loss values, and overall accuracy.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.784",
        "scimago_value": "1,246"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "localgeneralizedmultigranulationvariableprecisiontoleranceroughsetsanditsattributereduction",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3124339",
        "author": [
            "Zhou, Yueli",
            "Lin, Guoping"
        ],
        "keywords": [
            "Rough sets",
            "Information systems",
            "Data models",
            "Approximation algorithms",
            "Granular computing",
            "Computational modeling",
            "Big Data",
            "Tolerance relation",
            "local rough sets",
            "attribute reduction",
            "set-valued information systems"
        ],
        "abstract": "In the era of big data, as for an important granular computing model, rough set model is an important tool for us to deal with data. As a kind of extension of classical rough sets, multigranulation rough sets have two forms, including optimistic and pessimistic cases. However, these two models have their shortcomings, one is too loose, and the other is too strict. To overcome the above shortcomings, based on the concept of local multigranulation tolerance rough sets in set-valued information systems, the local generalized multigranulation variable precision tolerance rough sets model by introducing characteristic function is established. Then the related properties are studied and proved. In addition, we define the concepts of lower approximate quality, inner and outer importance of attribute according to different granularity structures in set-valued decision information systems because different granularity structures have different effectives on the decision classes. Finally, the local attribute reduction algorithm and the global attribute reduction algorithm of local generalized multigranulation variable precision tolerance rough sets in set-valued decision information systems are given, and the effectiveness of the algorithms is proved by using UCI data sets.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "15579662",
        "isbn": null,
        "journal": "IEEE Transactions on Instrumentation and Measurement",
        "publisher": null,
        "title": "qualityofmeasurementinformationindecisionmaking",
        "booktitle": null,
        "doi": "10.1109/TIM.2020.3047954",
        "author": [
            "Petri, Dario",
            "Carbone, Paolo",
            "Mari, Luca"
        ],
        "keywords": [
            "Syntactics",
            "Semantics",
            "Pragmatics",
            "Measurement uncertainty",
            "Decision making",
            "Pollution measurement",
            "Big Data",
            "Decision-making",
            "measurement",
            "measurement information (MI)",
            "quality management",
            "semiotic criteria",
            "semiotics"
        ],
        "abstract": "This article introduces a general-purpose framework aimed at capturing the elusive concept of quality of measurement information (MI), a critical issue for both researchers and practitioners when dealing with MI-enabled decision-making. The framework is a blueprint for the definition, assessment, communication, and improvement of MI quality, as analyzed through a set of general criteria, classified according to the syntactic, semantic, and pragmatic layers of semiotics, as suggested in the ISO 8000-8:2015 technical standard. The top-down analysis, where each criterion is specified in terms of characteristics and each characteristic in terms of domain-related indicators, is complemented with a bottom-up synthesis and operationalized by means of a flowchart. An application example, about the quality of information provided by the networks of measurement instruments reporting pollutants in the air, is presented to test the usefulness and the limitations of the framework.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "ensemblinganddynamicassetselectionforriskcontrolledstatisticalarbitrage",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3059187",
        "author": [
            "Carta, Salvatore",
            "Consoli, Sergio",
            "Podda, Alessandro",
            "Recupero, Diego",
            "Stanciu, Maria"
        ],
        "keywords": [
            "Predictive models",
            "Forecasting",
            "Data models",
            "Machine learning algorithms",
            "Heuristic algorithms",
            "Prediction algorithms",
            "Portfolios",
            "Stock market forecast",
            "statistical arbitrage",
            "machine learning",
            "ensemble learning"
        ],
        "abstract": "In recent years, machine learning algorithms have been successfully employed to leverage the potential of identifying hidden patterns of financial market behavior and, consequently, have become a land of opportunities for financial applications such as algorithmic trading. In this paper, we propose a statistical arbitrage trading strategy with two key elements: an ensemble of regression algorithms for asset return prediction, followed by a dynamic asset selection. More specifically, we construct an extremely heterogeneous ensemble ensuring model diversity by using state-of-the-art machine learning algorithms, data diversity by using a feature selection process, and method diversity by using individual models for each asset, as well models that learn cross-sectional across multiple assets. Then, their predictive results are fed into a quality assurance mechanism that prunes assets with poor forecasting performance in the previous periods. We evaluate the approach on historical data of component stocks of the S&P500 index. By performing an in-depth risk-return analysis, we show that this setup outperforms highly competitive trading strategies considered as baselines. Experimentally, we show that the dynamic asset selection enhances overall trading performance both in terms of return and risk. Moreover, the proposed approach proved to yield superior results during both financial turmoil and massive market growth periods, and it showed to have general application for any risk-balanced trading strategy aiming to exploit different asset classes.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "processmodelenhancementthroughcapturingimportantbehaviorsandratingtracevariants",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3121997",
        "author": [
            "Wang, Mimi",
            "He, Xudong",
            "Zhao, Peihai"
        ],
        "keywords": [
            "Data mining",
            "Task analysis",
            "Solid modeling",
            "Roads",
            "Reactive power",
            "Power capacitors",
            "Process mining",
            "process discovery",
            "filtering infrequent behaviors",
            "event log preprocessing",
            "process model enhancement"
        ],
        "abstract": "In the field of process discovery, it is worth noting that most process discovery algorithms assume that event logs are clean, i.e., event logs should not contain infrequent behaviors. However, real-life event logs often contain infrequent behaviors (i.e., outliers) and lead to quality issues of the discovered process model. On the other hand, driven by recent trends such as big data and process automation, the volume of event data is rapidly increasing: an event log may contain billions of event data. Unfortunately, some process mining algorithms and platforms may have difficulties handling such event logs. The ever-increasing size of event data and infrequent behaviors in the event log are two main challenges in the field of process discovery nowadays. However, little research has been conducted on simultaneously filtering infrequent behaviors and decreasing the size of the event log: Various filtering methods can filter infrequent behaviors, whereas the volume of the filtered log is still considerable. On the other hand, sampling methods can reduce the size of the event log, but the processed event log may still contain infrequent behaviors. Therefore, this paper proposes a technique to simultaneously filter infrequent behaviors and control the volume of input logs by capturing important behaviors and rating trace variants. Our experiments show that our approach can significantly improve the quality of the discovered process models. Furthermore, our approach can obtain a better process model from 0.001% trace variants than the complete event log and significantly improves the runtime of discovery algorithms.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "bitierdifferentialprivacyforpreciseauctionbasedpeoplecentriciotservice",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3067138",
        "author": [
            "Tian, Yuan",
            "Song, Biao",
            "Ma, Tinghuai",
            "Al-Dhelaan, Abdullah",
            "Al-Dhelaan, Mohammed"
        ],
        "keywords": [
            "Differential privacy",
            "Internet of Things",
            "Data privacy",
            "Privacy",
            "Sensors",
            "Incentive schemes",
            "Databases",
            "Data protection",
            "Internet of Things",
            "differential privacy",
            "crowd sensing IoT system"
        ],
        "abstract": "With the fast proliferation of device sensing and computing, crowed sensing has become the building block of the Internet of things. Consequently, various data collection and incentive mechanisms are investigated for people-centric services. In this paper, we have investigated the problem of privacy-aware people-centric IoT service based on a tailored auction approach. We applied a bi-tier differential privacy methodology on the data collected from crowdsensing IoT devices. A corresponding pricing scheme is also proposed to ensure the property of incentive compatibility, precise service data, and anonymized query results. Comparing to traditional privacy-aware auction schemes which only focus on the cost, our corresponding precise privacy-aware auction scheme provides a tailored IoT service based on the customers' request. The proposed trial query technique is able to provide a precise assessment of service quality, thus improves the efficiency of the people-centric IoT service. The customer could enjoy the convenience of service evaluation before making a bid, while the actual service data is anonymized to guarantee the service providers' interests. We evaluate the proposed bi-tier differential privacy schema for auction-based service by conducting extensive simulations. The experimental results show that our proposed method yields higher data utility and accuracy for the IoT service customers with privacy concerns.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "areviewonelectronicnosecoherenttaxonomyclassificationmotivationschallengesrecommendationsanddatasets",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3090165",
        "author": [
            "Al-Dayyeni, Wisam",
            "Al-Yousif, Shahad",
            "Taher, Mayada",
            "Al-Faouri, Ahmad",
            "Tahir, Nooritawati",
            "Jaber, Mustafa",
            "Ghabban, Fahad",
            "Najm, Ihab",
            "Alfadli, Ibrahim",
            "Ameerbakhsh, Omair",
            "Mnati, Mohannad",
            "Al-Shareefi, Nael",
            "Saleh, Abbadullah"
        ],
        "keywords": [
            "Pattern recognition",
            "Electronic noses",
            "Olfactory",
            "Taxonomy",
            "Food industry",
            "Sensor arrays",
            "Quality assessment",
            "Artificial olfaction",
            "electronic nose",
            "feature classification",
            "food quality",
            "machine learning",
            "pattern recognition"
        ],
        "abstract": "Context: Quality Control (QC) has been constantly an essential concern in many fields like food industry production, medical drugs, environmental protection, and so on. An odor or flavor, as a global fingerprint, can be implemented as a non-invasive mechanism for quality assurance. This computer-based approach can assure accurate detection and precise identification of the product quality or manufactured goods. Objective: This paper aims to achieve a systematic review about e-nose by introducing the achievements made by researchers in this area, to summarize their findings, to provide motivations and challenges to new researchers in the field of e-nose. Methods: The articles that were being utilized in the e-nose field were systematically achieved using three search engines: The online library of IEEE Explore, Web of Science and Science Direct for time span of 7 years (from 2013 to 2020). Both medical literature reviews and technical reviews were considered in the criteria of the research for wider understanding in the field of e-nose. The articles were categorized according to the objective of the research and projected into four classes. Upon completion of screening process 333 research papers using the exclusion and inclusion conditions, as the final set 54 articles were selected. Results: The taxonomy of this research was classified into four categories. The first one included the suggested methods that introduced the utilization of the e-nose for classification purposes (9/54 papers). The second category comprises the methods related to the development of e-nose (24/54 papers). The third one included the review studies about the e-nose (8/54 papers). The fourth group comprises comparative studies and evaluation (13/54 papers). Discussion: This systematic review contributes for a clearer understanding and a full insight in the e- nose research field by surveying and categorizing pertinent research efforts. Conclusion: This review paper will help to address the up-to-date research opportunities, challenges, problems, motivations and recommendations related to the utilization of e-nose in all fields of sciences and industries.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "aselfadaptingtaskschedulingalgorithmforcontainercloudusinglearningautomata",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3078773",
        "author": [
            "Zhu, Lilu",
            "Huang, Kai",
            "Hu, Yanfeng",
            "Tai, Xianqing"
        ],
        "keywords": [
            "Task analysis",
            "Containers",
            "Job shop scheduling",
            "Cloud computing",
            "Optimal scheduling",
            "Learning automata",
            "Dynamic scheduling",
            "Container cloud",
            "learning automata",
            "self-adapting scheduling",
            "reward-penalty strategy"
        ],
        "abstract": "With the rapid development of cloud computing and container technology, more and more applications are deployed to the cloud, and the scale of cloud platform is expanding. Due to the large number of container instances running in the platform, complex dependency relationship, fast version iteration and other characteristics, the update of business can often cause the change of the whole cloud resource environment, which triggers the repetitive scheduling problem of related tasks and affects stability of the business. In this paper, we propose a self-adapting task scheduling algorithm (ADATSA) using learning automata to solve these problems. Firstly, we design a learning automata model and objective function for the system on task scheduling problem. Then, we realize an effective reward-penalty mechanism for scheduling actions in combination with the idle state of resources and the running state of tasks in the current environment. Meanwhile, the environment is modeled by cluster, node and task, and the probability of action selected is optimized by scheduling execution, thus enhancing the adaptability to the cloud environment of the scheduling and accelerating convergence. Finally, we construct a framework of task load monitoring with buffer queue to achieve dynamic scheduling based on priority. The experimental part verifies the effectiveness of proposed algorithm with different angles such as resource imbalance degree, resource residual degree and QoS. Compared with other learning automata scheduling models such as LAEAS, non-automata technology based algorithms such as PSOS and K8S scheduling engine, ADATSA shows the better performance of environment adaptability, resource optimization efficiency and QoS in dynamic scheduling. The theoretical analysis was consistent with the experimental results.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "deeplearningbasedfaultdiagnosisofphotovoltaicsystemsacomprehensivereviewandenhancementprospects",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3110947",
        "author": [
            "Mansouri, Majdi",
            "Trabelsi, Mohamed",
            "Nounou, Hazem",
            "Nounou, Mohamed"
        ],
        "keywords": [
            "Circuit faults",
            "Feature extraction",
            "Tools",
            "Iron",
            "Data models",
            "Photovoltaic systems",
            "Deep learning",
            "Fault diagnosis",
            "deep learning",
            "photovoltaic systems"
        ],
        "abstract": "Photovoltaic (PV) systems are subject to failures during their operation due to the aging effects and external/environmental conditions. These faults may affect the different system components such as PV modules, connection lines, converters/inverters, which can lead to a decrease in the efficiency, performance, and further system collapse. Thus, a key factor to be taken into consideration in high-efficiency grid-connected PV systems is the fault detection and diagnosis (FDD). The performance of the FDD method depends mainly on the quality of the extracted features including real-time changes, phase changes, trend changes, and faulty modes. Thus, the data representation learning is the core stage of intelligent FDD techniques. Recently, due to the enhancement of computing capabilities, the increase of the big data use, and the development of effective algorithms, the deep learning (DL) tool has witnessed a great success in data science. Therefore, this paper proposes an extensive review on deep learning based FDD methods for PV systems. After a brief description of the DL-based strategies, techniques for diagnosing PV systems proposed in recent literature are overviewed and analyzed to point out their differences, advantages and limits. Future research directions towards the improvement of the performance of the DL-based FDD techniques are also discussed. This review paper aims to systematically present the development of DL-based FDD for PV systems and provide guidelines for future research in the field.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "uncertaintyawaredeeplearningarchitecturesforhighlydynamicairqualityprediction",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3052429",
        "author": [
            "Mokhtari, Ichrak",
            "Bechkit, Walid",
            "Rivano, Herv\u00e9",
            "Yaici, Mouloud"
        ],
        "keywords": [
            "Atmospheric modeling",
            "Predictive models",
            "Air pollution",
            "Forecasting",
            "Vehicle dynamics",
            "Uncertainty",
            "Deep learning",
            "Conv-LSTM",
            "spatio-temporel prediction",
            "highly dynamic air quality",
            "accidental pollutant release",
            "uncertainty",
            "FFT-07",
            "WSN"
        ],
        "abstract": "Forecasting air pollution is considered as an essential key for early warning and control management of air pollution, especially in emergency situations, where big amounts of pollutants are quickly released in the air, causing considerable damages. Predicting pollution in such situations is particularly challenging due to the strong dynamic of the phenomenon and the various spatio-temporal factors affecting air pollution dispersion. In addition, providing uncertainty estimates of prediction makes the forecasting model more trustworthy, which helps decision-makers to take appropriate actions with more confidence regarding the pollution crisis. In this study, we propose a multi-point deep learning model based on convolutional long short term memory (ConvLSTM) for highly dynamic air quality forecasting. ConvLSTM architectures combines long short term memory (LSTM) and convolutional neural network (CNN), which allows to mine both temporal and spatial data features. In addition, uncertainty quantification methods were implemented on top of our model's architecture and their performances were further excavated. We conduct extensive experimental evaluations using a real and highly dynamic air pollution data set called Fusion Field Trial 2007 (FFT07). The results demonstrate the superiority of our proposed deep learning model in comparison to state-of-the-art methods including machine and deep learning techniques. Finally, we discuss the results of the uncertainty techniques and we derive insights.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "detectionandrecoveryofhighertamperedimagesusingnovelfeatureandcompressionstrategy",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3072314",
        "author": [
            "Tohidi, Faranak",
            "Paul, Manoranjan",
            "Hooshmandasl, Mohammad"
        ],
        "keywords": [
            "Watermarking",
            "Image coding",
            "Feature extraction",
            "Authentication",
            "Discrete cosine transforms",
            "Data mining",
            "Tampered image",
            "image recovery",
            "image authentication",
            "feature extraction",
            "watermarking",
            "image compression"
        ],
        "abstract": "Due to the availability of powerful image-editing software and the growing amount of multimedia data that is transmitted via the Internet, integrity verifications and confidentiality of the data are becoming critical issues. However, currently, the accuracy of detecting and the recovery capability of the tampered images by the existing methods through watermarking strategy is still not at the required level, especially at a higher tampered rate. This paper proposes a new blind and fragile watermarking method to detect tampering and better recovery of tampered images. To improve the quality of both the watermarked and the recovered images, a new feature extraction scheme is introduced which will produce a short but comprehensive recovery code using a new compression strategy. If a block in the image tampers, the proposed embedded feature allows the original data to be extracted for recovery. To overcome tamper coincidence, every block\u2019s watermarked data contains not only the recovery code belonging to the block itself but also its neighbor\u2019s data as a second layer of recovery. Various size blocks were investigated to see the performance and compare their efficiency for recovering an image after different tampering rates. The test showed the smaller block sizes may be more suitable for locating tampering, where the bigger ones are more suitable when the tampering rate is higher. The bigger block sizes in the proposed method can recover an image even after a 60% tampering rate with high quality (more than 31 dB). The experimental results prove that the proposed method can have better efficiency for detecting tampering, and recovery of the original image, compared to the relevant existing methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "animprovedcnnbasedappleappearancequalityclassificationmethodwithsmallsamples",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3077567",
        "author": [
            "Sun, Li",
            "Liang, Kaibo",
            "Song, Yanxing",
            "Wang, Yuzhi"
        ],
        "keywords": [
            "Support vector machines",
            "Image segmentation",
            "Data models",
            "Feature extraction",
            "Training",
            "Mathematical model",
            "Image recognition",
            "Apple quality classification",
            "SVM",
            "DCGAN",
            "ResNet50"
        ],
        "abstract": "Apple quality classification is an important means to refine apple sales market and promote apple sales. At present, most of classification methods based on a convolutional neural network (CNN) depend on the quantity of training samples to get good performance. But due to the lack of large-scale public apple appearance dataset, it is a big challenge to obtain high accuracy of apple appearance quality classification with small samples. Therefore, we propose an improved method based on CNN for apple appearance, quality classification with small samples. Firstly, support vector machine (SVM) is used for image segmentation to avoid the decrease of recognition accuracy caused by environmental noise. Secondly, the segmented image data are input into deep convolutional generative adversarial networks (DCGAN) model, which is used for data expansion. Thirdly, the improved ResNet50 (Imp-ResNet50) is proposed as follows: Replace the fully-connected layer with global average pooling layer; Add the dropout algorithm and batch normalization algorithm at the fully-connected layer; Replace the activation function ReLU with Swish. Through comparative experiments with 360 apple images, we verify the performance of the proposed method including the training image quality, the running time, and classification accuracy. The result shows that the proposed method can obtain high quality training samples and reduce the running time of the method effectively. At the same time, it can realize higher classification accuracy that is up to 96.5%, which is higher than the previous classification method.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21511535",
        "isbn": null,
        "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "publisher": null,
        "title": "evaluationofthelandsatbasedcanadianwetlandinventorymapusingmultiplesourceschallengesoflargescalewetlandclassificationusingremotesensing",
        "booktitle": null,
        "doi": "10.1109/JSTARS.2020.3036802",
        "author": [
            "Amani, Meisam",
            "Brisco, Brian",
            "Mahdavi, Sahel",
            "Ghorbanian, Arsalan",
            "Moghimi, Armin",
            "DeLancey, Evan",
            "Merchant, Michael",
            "Jahncke, Raymond",
            "Fedorchuk, Lee",
            "Mui, Amy",
            "Fisette, Thierry",
            "Kakooei, Mohammad",
            "Ahmadi, Seyed",
            "Leblon, Brigitte",
            "LaRocque, Armand"
        ],
        "keywords": [
            "Wetlands",
            "Remote sensing",
            "Earth",
            "Monitoring",
            "Artificial satellites",
            "Biodiversity",
            "Synthetic aperture radar",
            "Big data",
            "Canada",
            "Google Earth Engine",
            "Landsat",
            "remote sensing (RS)",
            "wetlands"
        ],
        "abstract": "The first Canadian wetland inventory (CWI) map, which was based on Landsat data, was produced in 2019 using the Google Earth Engine (GEE) big data processing platform. The proposed GEE-based method to create the preliminary CWI map proved to be a cost, time, and computationally efficient approach. Although the initial effort to produce the CWI map was valuable with a 71% overall accuracy (OA), there were several inevitable limitations (e.g., low-quality samples for the training and validation of the map). Therefore, it was important to comprehensively investigate those limitations and develop effective solutions to improve the accuracy of the Landsat-based CWI (L-CWI) map. Over the past year, the L-CWI map was shared with several governmental, academic, environmental nonprofit, and industrial organizations. Subsequently, valuable feedback was received on the accuracy of this product by comparing it with various in situ data, photo-interpreted reference samples, land cover/land use maps, and high-resolution aerial images. It was generally observed that the accuracy of the L-CWI map was lower relative to the other available products. For example, the average OA in four Canadian provinces using in situ data was 60%. Moreover, including reliable in situ data, using an object-based classification method, and adding more optical and synthetic aperture radar datasets were identified as the main practical solutions to improve the CWI map in the future. Finally, limitations and solutions discussed in this study are applicable to any large-scale wetland mapping using remote sensing methods, especially to CWI generation using optical satellite data in GEE.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.784",
        "scimago_value": "1,246"
    },
    {
        "issnkey": "10044132",
        "isbn": null,
        "journal": "Journal of Systems Engineering and Electronics",
        "publisher": null,
        "title": "rfcafeatureselectionalgorithmforsoftwaredefectprediction",
        "booktitle": null,
        "doi": "10.23919/JSEE.2021.000032",
        "author": [
            "Xiaolong, Xu",
            "Wen, Chen",
            "Xinheng, Wang"
        ],
        "keywords": [
            "Software",
            "Software algorithms",
            "Prediction algorithms",
            "Feature extraction",
            "Partitioning algorithms",
            "Correlation",
            "Clustering algorithms",
            "software defect prediction (SDP)",
            "feature selection",
            "cluster"
        ],
        "abstract": "Software defect prediction (SDP) is used to perform the statistical analysis of historical defect data to find out the distribution rule of historical defects, so as to effectively predictdefects in the new software. However, there are redundant and irrelevant features in the software defect datasets affecting the performance of defect predictors. In order to identify and remove the redundant and irrelevant features in software defectdatasets, we propose Relief F-based clustering (RFC), a cluster-based feature selection algorithm. Then, the correlation between features is calculated based on the symmetric uncertainty. According to the correlation degree, RFC partitions features into kclusters based on the k-medoids algorithm, and finally selects the representative features from each cluster to form the final feature subset. In the experiments, we compare the proposed RFC with classical feature selection algorithms on nine National Aeronautics and Space Administration (NASA) software defectprediction datasets in terms of area under curve (AUC) and F-value. The experimental results show that RFC can effectively improve the performance of SDP.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "multilayoutunstructuredinvoicedocumentsdatasetadatasetfortemplatefreeinvoiceprocessinganditsevaluationusingaiapproaches",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3096739",
        "author": [
            "Baviskar, Dipali",
            "Ahirrao, Swati",
            "Kotecha, Ketan"
        ],
        "keywords": [
            "Task analysis",
            "Layout",
            "Organizations",
            "Artificial intelligence",
            "Data mining",
            "Standards organizations",
            "Text analysis",
            "Artificial Intelligence (AI)",
            "information extraction",
            "key field extraction",
            "named entity recognition (NER)",
            "template-free invoice processing",
            "unstructured data"
        ],
        "abstract": "The daily transaction of an organization generates a vast amount of unstructured data such as invoices and purchase orders. Managing and analyzing unstructured data is a costly affair for the organization. Unstructured data has a wealth of hidden valuable information. Extracting such insights automatically from unstructured documents can significantly increase the productivity of an organization. Thus, there is a huge demand to develop a tool that can automate the extraction of key fields from unstructured documents. Researchers have used different approaches for extracting key fields, but the lack of annotated and high-quality datasets is the biggest challenge. Existing work in this area has used standard and custom datasets for extracting key fields from unstructured documents. Still, the existing datasets face some serious challenges, such as poor-quality images, domain-related datasets, and a lack of data validation approaches to evaluate data quality. This work highlights the detailed process flow for end-to-end key fields extraction from unstructured documents. This work presents a high-quality, multi-layout unstructured invoice documents dataset assessed with a statistical data validation technique. The proposed multi-layout unstructured invoice documents dataset is highly diverse in invoice layouts to generalize key field extraction tasks for unstructured documents. The proposed multi-layout unstructured invoice documents dataset is evaluated with various feature extraction techniques such as Glove, Word2Vec, FastText, and AI approaches such as BiLSTM and BiLSTM-CRF. We also present the comparative analysis of feature extraction techniques and AI approaches on the proposed multi-layout unstructured invoice document dataset. We attained the best results with BiLSTM-CRF model.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "researchonadmmreconstructionalgorithmofphotoacoustictomographywithlimitedsamplingdata",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3104154",
        "author": [
            "Wang, Zhao-Xu",
            "Wang, Hao-Quan",
            "Ren, Shi-Lei"
        ],
        "keywords": [
            "Image reconstruction",
            "Absorption",
            "Mathematical model",
            "Detectors",
            "Biological tissues",
            "TV",
            "Photoacoustic imaging",
            "Photoacoustic imaging",
            "alternating direction method of multipliers",
            "total variation",
            "image reconstruction"
        ],
        "abstract": "Photoacoustic imaging is a new non-destructive biomedical imaging method. When limited independent data is available, the restoration of the initial pressure rise distribution is often an ill-posed problem. In this paper, based on the study of photoacoustic effects, the sparse prior information of photoacoustic images is integrated into the reconstruction process by using the compressed sensing (CS) theory and the L2 norm optimization technique, combining the augmented Langrange weighting of the alternating direction method of multipliers (ADMM) with the total variation (TV) minimization problem, and the reconstruction artifacts are effectively eliminated. The simulation data from the real numerical model show that compared with the common time reversal algorithm, interpolation algorithm and truncated back projection algorithm, the total variational regularization method based on ADMM can effectively improve the quality of reconstructed images under the condition of limited viewing angles and incomplete projection data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "arcycleganimprovedcycleganforstyletransferringoffruitimages",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3068094",
        "author": [
            "Chen, Hongqian",
            "Guan, Mengxi",
            "Li, Hui"
        ],
        "keywords": [
            "Training",
            "Convolution",
            "Generators",
            "Image reconstruction",
            "Image recognition",
            "Generative adversarial networks",
            "Diseases",
            "Image generation",
            "style transferring",
            "attribute registration",
            "image registration",
            "improved CycleGAN"
        ],
        "abstract": "CycleGAN can realize image translation and style transferring among unpaired images. However, it will easily generate inappropriate image results when the number and shapes of objects in the style offering image and the source image are greatly different. The paper proposed an improved network, named arCycleGAN, which introduced the mechanism of attribute registration into CycleGAN to solve the problem. The arCycleGAN can transfer the freshness styles from the style offering images to the unpaired input source images. The generated target images will have the freshness attributes of the style offering images, while maintaining the shapes and key features of the input source images. The realization of mechanism of attribute registration consists of three modules. The first module is attribute recognition module, which can identify and label the attributes of objects in images. The second module is image pre-screening module, which selects appropriate image subset as screened training set from raw image set according to the attributes of the input source images. The third module is similarity matching module, which matches the images in screened training set based on the similarity. The generator and discriminator in the new network are similar to that in the CycleGAN network. Experimental results demonstrate the effectiveness and better performance of the arCycleGAN. Compared with the CycleGAN, the new network can generate more convincing images. It can generate the target images of similar quality based on a smaller training set and less training time than the original CycleGAN. For generating images of similar quality, the number of images in the required training set can be reduced by 50%, while training time is reduced by 5.8%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "sustainablemarineecosystemsdeeplearningforwaterqualityassessmentandforecasting",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3109216",
        "author": [
            "Gamb\u00edn, \u00c1ngel",
            "Angelats, Eduard",
            "Gonz\u00e1lez, Jes\u00fas",
            "Miozzo, Marco",
            "Dini, Paolo"
        ],
        "keywords": [
            "Sea measurements",
            "Water quality",
            "Ecosystems",
            "Aquaculture",
            "Forecasting",
            "Feature extraction",
            "Europe",
            "Sustainable coastal management",
            "sustainable aquaculture",
            "remote sensing",
            "artificial intelligence",
            "machine learning",
            "water quality",
            "blue economy"
        ],
        "abstract": "An appropriate management of the available resources within oceans and coastal regions is vital to guarantee their sustainable development and preservation, where water quality is a key element. Leveraging on a combination of cross-disciplinary technologies including Remote Sensing (RS), Internet of Things (IoT), Big Data, cloud computing, and Artificial Intelligence (AI) is essential to attain this aim. In this paper, we review methodologies and technologies for water quality assessment that contribute to a sustainable management of marine environments. Specifically, we focus on Deep Leaning (DL) strategies for water quality estimation and forecasting. The analyzed literature is classified depending on the type of task, scenario and architecture. Moreover, several applications including coastal management and aquaculture are surveyed. Finally, we discuss open issues still to be addressed and potential research lines where transfer learning, knowledge fusion, reinforcement learning, edge computing and decision-making policies are expected to be the main involved agents.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "cottonwarehousingimprovementforbalemanagementsystembasedonneutrosophicclassifier",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3126790",
        "author": [
            "Halawa, Waleed",
            "Darwish, Saad",
            "Elzoghabi, Adel"
        ],
        "keywords": [
            "Cotton",
            "Fabrics",
            "Clustering algorithms",
            "Yarn",
            "Fuzzy logic",
            "Manuals",
            "Warehousing",
            "Bale management system",
            "cotton lay-down",
            "cotton warehousing",
            "neutrosophic clustering"
        ],
        "abstract": "One of the big factors affecting yarn quality is the cotton mix. There is always a considerable variation in the fiber characteristics from one bale to another, even within the same lot. This variation will result in the yarn quality difference, which leads to many fabric defects if the bales are mixed in an uncontrolled manner. The bale management system is based on the categorization of cotton bales according to their fiber quality characteristics. It includes the measurement of the fiber characteristics concerning each bale by using a High Volume Instrument (HVI). The separation of bales into categories for cotton lay-down to achieve balanced bale mixes must be based on a robust clustering algorithm. This paper discusses the utilization of the neutrosophic classifier, for the first time, to categorize the cotton in the warehouse. Although the traditional categorizing method using fuzzy logic came out with some satisfying results, it was missing the way of excluding the outlier\u2019s data points (off-quality bales) which can affect the fabric quality. Neutrosophic classifier deals with cotton bale\u2019s data type by excluding some bale data points that affect the fabric quality through falsity and indeterminacy membership functions to increase the accuracy of the bale management system. Our proposed method has been tested on mill cotton data. The results have been compared with the results of the traditional fuzzy logic algorithms and revealed higher accuracy.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "qualityawaredevopsresearchwheredowestand",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3064867",
        "author": [
            "Alnafessah, Ahmad",
            "Gias, Alim",
            "Wang, Runan",
            "Zhu, Lulai",
            "Casale, Giuliano",
            "Filieri, Antonio"
        ],
        "keywords": [
            "Software",
            "Testing",
            "Artificial intelligence",
            "Computer architecture",
            "Tools",
            "Production",
            "Software architecture",
            "DevOps",
            "CI/CD",
            "infrastructure as code",
            "testing",
            "artificial intelligence",
            "verification"
        ],
        "abstract": "DevOps is an emerging paradigm that reduces the barriers between developers and operations teams to offer continuous fast delivery and enable quick responses to changing requirements within the software life cycle. A significant volume of activity has been carried out in recent years with the aim of coupling DevOps stages with tools and methods to improve the quality of the produced software and the underpinning delivery methodology. While the research community has produced a sustained effort by conducting numerous studies and innovative development tools to support quality analyses within DevOps, there is still a limited cohesion between the research themes in this domain and a shortage of surveys that holistically examine quality engineering work within DevOps. In this paper, we address the gap by comprehensively surveying existing efforts in this area, categorizing them according to the stage of the DevOps lifecycle to which they primarily contribute. The survey holistically spans across all the DevOps stages, identify research efforts to improve architectural design, modeling and infrastructure-as-code, continuous-integration/continuous-delivery (CI/CD), testing and verification, and runtime management. Our analysis also outlines possible directions for future work in quality-aware DevOps, looking in particular at AI for DevOps and DevOps for AI software.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "representationlearningwithdualautoencoderformultilabelclassification",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3096194",
        "author": [
            "Zhu, Yi",
            "Yang, Yang",
            "Li, Yun",
            "Qiang, Jipeng",
            "Yuan, Yunhao",
            "Zhang, Runmei"
        ],
        "keywords": [
            "Training",
            "Manifolds",
            "Independent component analysis",
            "Data models",
            "Correlation",
            "Linear programming",
            "Multi-label classification",
            "dual autoencoder",
            "RICA",
            "manifold regularization",
            "representation learning"
        ],
        "abstract": "Multi-label classification aims to deal with the problem that an object may be associated with one or more labels, which is a more difficult task due to the complex nature of multi-label data. The crucial problem of multi-label classification is the more robust and higher-level feature representation learning, which can reduce non-helpful feature attributes from the input space prior to training. In recent years, deep learning methods based on autoencoders have achieved excellent performance in multi-label classification for the advantages of powerful representations learning ability and fast convergence speed. However, most existing autoencoder-based methods only rely on the single autoencoder model, which pose challenges for multi-label feature representations learning and fail to measure similarities between data spaces. To address this problem, in this paper, we propose a novel representation learning method with dual autoencoder for multi-label classification. Compared to the existing autoencoder-based methods, our proposed method can capture different characteristics and more abstract features from data by the serially connection of two different types of autoencoders. More specifically, firstly, the algorithm of Reconstruction Independent Component Analysis (RICA) in sparse autoencoder is trained on patches on all training and test dataset for robust global feature representations learning. Secondly, with the output of RICA, stacked autoencoder with manifold regularization (SAMR) is introduced to ameliorate the quality of multi-label features learning. Comprehensive experiments on several real-world data sets demonstrate the effectiveness of our proposed approach compared with several competing state-of-the-art methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "predictiveanalyticsforoctanenumberanovelhybridapproachofkpcaandgspsosvrmodel",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3077028",
        "author": [
            "Li, Baosheng",
            "Qin, Chuandong"
        ],
        "keywords": [
            "Support vector machines",
            "Petroleum",
            "Prediction algorithms",
            "FCC",
            "Principal component analysis",
            "Predictive models",
            "Spectroscopy",
            "Gasoline octane number",
            "kernel principal component analysis",
            "support vector regression",
            "particle swarm optimization"
        ],
        "abstract": "Octane number is the most important indicator of reflecting the combustion performance, and a great deal of research has been devoted to improving it. In this paper, a new analytical framework is proposed to predict octane number, kernel principal component analysis (KPCA) is used to reduce the dimension of the variables in the process of Fluid Catalytic Cracking (FCC), support vector regression (SVR) is used to construct the gasoline octane number prediction model and the particle swarm optimization algorithm (PSO) is used to select the optimal combination of parameters for the model. The experiments show that the octane number can be improved under a given production environment with a guaranteed desulfurization effect of gasoline products. Furthermore, several key attributes that have a significantly positive or negative correlation with the improvement of gasoline product quality are identified through computing the feature score. The findings can help engineers adjust operational variables to obtain a series of high-quality products.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "pm25predictionusinggeneticalgorithmbasedfeatureselectionandencoderdecodermodel",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3072280",
        "author": [
            "Nguyen, Minh",
            "Le, Phi",
            "Nguyen, Kien",
            "Le, Van",
            "Nguyen, Thanh-Hung",
            "Ji, Yusheng"
        ],
        "keywords": [
            "Predictive models",
            "Atmospheric modeling",
            "Genetic algorithms",
            "Feature extraction",
            "Air quality",
            "Decoding",
            "Statistics",
            "PM 25",
            "genetic algorithm",
            "feature selection",
            "long short-term memory",
            "encoder-decoder model"
        ],
        "abstract": "The concentration of fine particulate matter (PM2.5), which represents inhalable particles with diameters of 2.5 micrometers and smaller, is a vital air quality index. Such particles can penetrate deep into the human lungs and severely affect human health. This paper studies accurate PM2.5 prediction, which can potentially contribute to reducing or avoiding the negative consequences. Our approach\u2019s novelty is to utilize the genetic algorithm (GA) and an encoder-decoder (E-D) model for PM2.5 prediction. The GA benefits feature selection and remove outliers to enhance the prediction accuracy. The encoder-decoder model with long short-term memory (LSTM), which relaxes the restrictions between the input and output of the model, can be used to effectively predict the PM2.5 concentration. We evaluate the proposed model on air quality datasets from Hanoi and Taiwan. The evaluation results show that our model achieves excellent performance. By merely using the E-D model, we can obtain more accurate (up to 53.7%) predictions than those of previous works. Moreover, the GA in our model has the advantage of obtaining the optimal feature combination for predicting the PM2.5 concentration. By combining the GA-based feature selection algorithm and the E-D model, our proposed approach further improves the accuracy by at least 13.7%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "20960654",
        "isbn": null,
        "journal": "Big Data Mining and Analytics",
        "publisher": null,
        "title": "asurveyonalgorithmsforintelligentcomputingandsmartcityapplications",
        "booktitle": null,
        "doi": "10.26599/BDMA.2020.9020029",
        "author": [
            "Tong, Zhao",
            "Ye, Feng",
            "Yan, Ming",
            "Liu, Hong",
            "Basodi, Sunitha"
        ],
        "keywords": [
            "Smart cities",
            "Urban areas",
            "Cloud computing",
            "Task analysis",
            "Edge computing",
            "Statistics",
            "Sociology",
            "cyber physical systems",
            "Internet of Things (IoT)",
            "intelligent computing algorithm",
            "Quality of Service(QoS)",
            "smart city"
        ],
        "abstract": "With the rapid development of human society, the urbanization of the world's population is also progressing rapidly. Urbanization has brought many challenges and problems to the development of cities. For example, the urban population is under excessive pressure, various natural resources and energy are increasingly scarce, and environmental pollution is increasing, etc. However, the original urban model has to be changed to enable people to live in greener and more sustainable cities, thus providing them with a more convenient and comfortable living environment. The new urban framework, the smart city, provides excellent opportunities to meet these challenges, while solving urban problems at the same time. At this stage, many countries are actively responding to calls for smart city development plans. This paper investigates the current stage of the smart city. First, it introduces the background of smart city development and gives a brief definition of the concept of the smart city. Second, it describes the framework of a smart city in accordance with the given definition. Finally, various intelligent algorithms to make cities smarter, along with specific examples, are discussed and analyzed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21511535",
        "isbn": null,
        "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "publisher": null,
        "title": "understandingthedriversoflandsurfacetemperaturebasedonmultisourcedataaspatialeconometricperspective",
        "booktitle": null,
        "doi": "10.1109/JSTARS.2021.3129842",
        "author": [
            "Liu, Menghang",
            "Ma, Haitao",
            "Bai, Yu"
        ],
        "keywords": [
            "Land surface temperature",
            "Land surface",
            "Surface topography",
            "Remote sensing",
            "Vegetation mapping",
            "Statistics",
            "Sociology",
            "Beijing",
            "direct and indirect effects",
            "land surface temperature (LST)",
            "spatial durbin model (SDM)"
        ],
        "abstract": "Urban thermal condition has seriously affected the quality of residents\u2019 daily life and triggered some environmental issues. Understanding spatial patterns of land surface temperature (LST) and its driving mechanism is important for the sustainable development of cities. Taking Beijing as an example, this study employed spatial econometric models to investigate spatial and temporal heterogeneity of LST from 2014 to 2018 based on multisource remote sensing and statistical data. The global autocorrelation Moran's I index showed the existence of significant positive correlations of LST among regions, indicating the regions with high thermal environments are spatially adjacent. The temperature of a region would increase by more than 0.6% for every 1% increase in LST of surrounding areas based on the spatial Durbin model. In terms of spatial interactions of influencing factors, elevation, normalized difference vegetation index, modified normalized difference water index, nighttime light, and fossil energy consumption of neighbors exhibited significantly positive spatial agglomeration effects on local LST, whereas albedo, GDP, and population density of adjacent areas had negative effects on LST in local areas. Particularly, the indirect effects of drivers were greater than their direct effects, indicating urban thermal condition was an interregional issue and joint control measures should be adopted to mitigate the urban heat island effects as a whole.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.784",
        "scimago_value": "1,246"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "monthlyelectricitydemandpatternsandtheirrelationshipwiththeeconomicsectorandgeographiclocation",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3089443",
        "author": [
            "Luque, Joaquin",
            "Personal, Enrique",
            "Garcia-Delgado, Antonio",
            "Leon, Carlos"
        ],
        "keywords": [
            "Economics",
            "Companies",
            "Meters",
            "Meter reading",
            "Energy consumption",
            "Supply chains",
            "Energy demand",
            "customer profiling",
            "data engineering",
            "big data applications",
            "statistical learning",
            "pattern analysis"
        ],
        "abstract": "In a highly competitive and liberalized energy market, where the retail of electricity is open to many potential companies, it is essential to have tools that help make decisions and guide the design of marketing strategies. In this sense, it is essential for retailers to know the behavior of their customers to correctly define their commercial strategies. One of the most commonly used methods for this is the characterization of their consumption profiles. Fortunately, for regulatory reasons, in some countries, the monthly electricity demand of each customer is openly available to any competitor. This paper explores whether this information, especially the economic sector and geographic location of a client, is useful for determining the client\u2019s demand profile. Specifically, data on electricity demand in Spain from more than 27 million users and for a period of 3 years are analyzed. For this purpose, the electricity consumption of every client is grouped by month and normalized. The resulting demand profiles are later clustered according to different criteria. The main finding of the research is that the combined information on economic activity and location definitely enables prediction of the demand profile. Additionally, profile quality metrics are defined and obtained for the entire dataset. The resulting profiles have a mean dispersion of 10% and a confidence interval of \u00b117%. To clarify the use of these metrics, several examples are detailed, showing how this profile information can be used to improve the marketing decision-making process for electricity retailers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "19430655",
        "isbn": null,
        "journal": "IEEE Photonics Journal",
        "publisher": null,
        "title": "improvementofimageclassificationbymultipleopticalscattering",
        "booktitle": null,
        "doi": "10.1109/JPHOT.2021.3109016",
        "author": [
            "Gao, Xinyu",
            "Li, Yi",
            "Qiu, Yanqing",
            "Mao, Bangning",
            "Chen, Miaogen",
            "Meng, Yanlong",
            "Zhao, Chunliu",
            "Kang, Juan",
            "Guo, Yong",
            "Shen, Changyu"
        ],
        "keywords": [
            "Scattering",
            "Optical scattering",
            "Optical imaging",
            "Adaptive optics",
            "Liquid crystal displays",
            "Optical reflection",
            "Optical distortion",
            "Optical computing",
            "machine learning",
            "random media",
            "feedforward neural networks"
        ],
        "abstract": "Multiple optical scattering occurs when light propagates in a non-uniform medium. During the multiple scattering, images were distorted and the spatial information they carried became scrambled. However, the image information is not lost but presents in the form of speckle patterns (SPs). In this study, we built up an optical random scattering system based on an liquid crystal display (LCD) and an RGB laser source. We found that the image classification can be improved by the help of random scattering which is considered as a feedforward neural network to extracts features from image. Along with the ridge classification deployed on computer, we achieved excellent classification accuracy higher than 94%, for a variety of data sets covering medical, agricultural, environmental protection and other fields. In addition, the proposed optical scattering system has the advantages of high speed, low power consumption, and miniaturization, which is suitable for deploying in edge computing applications.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "sportlocationbaseduserclusteringwithprivacypreservationinwirelessiotdrivenhealthcare",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3051051",
        "author": [
            "Zhang, Qiyun",
            "Zhang, Yuan",
            "Li, Caizhong",
            "Yan, Chao",
            "Duan, Yucong",
            "Wang, Hao"
        ],
        "keywords": [
            "Quality of service",
            "Sports",
            "Medical services",
            "Wireless communication",
            "Communication system security",
            "Wireless sensor networks",
            "Privacy",
            "Sport location",
            "user clustering",
            "privacy",
            "healthcare service",
            "simhash",
            "wireless network"
        ],
        "abstract": "The gradual prevalence of Internet of Things (IoT) and wireless communication technologies has enabled the wide adoption of various smart devices (e.g., smart watches) in provisioning the healthcare services to massive users. Besides monitoring the real-time health signals or conditions of users, smart devices can also record a series of sport-related user information such as user location information at a certain time point. The location sequence information is valuable to cluster the users who share the similar sport preferences or habits and therefore, is also playing a key role in providing wireless healthcare services to these users. However, the user location information is often sensitive to certain wireless users as they decline to reveal their daily sport behavior patterns to others. In this situation, a natural challenge is raised in securing the sensitive user location information while mining the users\u2019 daily sport behavior patterns and provisioning better healthcare services to the users. Considering this challenge, we take advantage of the well-known SimHash technique to protect users\u2019 location privacy while clustering the users who share similar sport preferences or habits for better healthcare services. At last, we validate the feasibility of the proposal through a set of simulated experiments conducted on a real-world dataset. Reported results demonstrate that our solution performs better than the other two competitive ones while securing user location information.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "agenericapproachforwheatdiseaseclassificationandverificationusingexpertopinionforknowledgebaseddecisions",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3058582",
        "author": [
            "Haider, Waleej",
            "Rehman, Aqeel-Ur",
            "Durrani, Nouman",
            "Rehman, Sadiq"
        ],
        "keywords": [
            "Diseases",
            "Agriculture",
            "Insects",
            "Production",
            "Machine learning algorithms",
            "Knowledge based systems",
            "Genetics",
            "Agricultural DSS",
            "artificial intelligence",
            "agricultural knowledge management",
            "classification of crop diseases",
            "machine learning",
            "wheat crop diseases"
        ],
        "abstract": "Crop diseases have mainly affected crop production due to the lack of modern approaches for disease identification. For many years, farmers have identified various crop diseases and have local knowledge about disease management. However, the local knowledge of one agricultural region is not utilized in other regions due to the unavailability of knowledge sharing platforms. Agricultural research also suggests that crop production has mainly decreased due to diseases, methods of cultivation, irrigation, and lack of local agricultural knowledge. In this research, the experience of agricultural experts, farmers, and cultivators is gathered through a crowd-sourced platform. The data is then processed for various disease identification. Hence, timely identification of various crop diseases can benefit farmers to apply relevant management methods. In literature, researchers have proposed various methods for disease management, mostly based on the classification of crop diseases using Machine Learning (ML) algorithms. However, these algorithms are unable to give trustful results due to static data provisioning and the dynamic nature of various diseases in different agricultural regions. Further, the agricultural expert's experience is also not considered in verifying the classification results. To identify the dynamic nature of wheat diseases, we acquired high-quality images and symptoms-based text data from farmers, domain experts, and users using a crowd-sourced platform. Different augmentation techniques were also used to enhance the size of training data. In this paper, a modern generic approach has been proposed for the identification and classification of wheat diseases using Decision Trees (DT) and different deep learning models. Also, results of both algorithms were then verified by domain experts that improved the decision trees accuracy by 28.5%, CNN accuracy by 4.3% (leading to 97.2%), and resulted in decision rules for wheat diseases in a knowledge-based system.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "trajectoryoptimizationandpowerallocationalgorithminmbsassistedcellfreemassivemimosystems",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3054652",
        "author": [
            "An, Jue",
            "Zhao, Feng"
        ],
        "keywords": [
            "Wireless communication",
            "Base stations",
            "System performance",
            "Simulation",
            "Massive MIMO",
            "Unmanned aerial vehicles",
            "User experience",
            "Cell-free massive MIMO",
            "mobile base station",
            "trajectory optimization",
            "power allocation"
        ],
        "abstract": "As the mobile networks become even denser and the traffic demand is increasing drastically, it is becoming more heterogeneous. Recently, it is intensive to investigate the Cell-Free Massive multiple-input multiple-output Massive (MIMO) system, where a large number of access points (APs) simultaneously serve a much smaller number of users, and the APs and users are clustered to provide good service for all users. This paper designs a mobile base station (MBS)-assisted Cell-Free Massive MIMO system: Utilizing an MBS deployed on the unmanned aerial vehicle (UAV) to form an air-ground heterogeneous system with the Cell-Free Massive MIMO system and offload part of traffic to the MBS to further improve the system performance. To provide better service to all users, this paper design an MBS flight trajectory optimal method to improve the wireless coverage performance and user experience, and proposed a joint power allocation algorithm based on the consideration of the fairness of the service quality. The simulation results indicate that the performance of the system designed in this paper has a significant improvement compared with the normal Cell-Free Massive MIMO system.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "15582256",
        "isbn": null,
        "journal": "Proceedings of the IEEE",
        "publisher": null,
        "title": "asurveyofcybersecurityofdigitalmanufacturing",
        "booktitle": null,
        "doi": "10.1109/JPROC.2020.3032074",
        "author": [
            "Mahesh, Priyanka",
            "Tiwari, Akash",
            "Jin, Chenglu",
            "Kumar, Panganamala",
            "Reddy, A.",
            "Bukkapatanam, Satish",
            "Gupta, Nikhil",
            "Karri, Ramesh"
        ],
        "keywords": [
            "Smart manufacturing",
            "Digital systems",
            "Intelligent sensors",
            "Process control",
            "Sensor systems",
            "Service robots",
            "Computer crime",
            "Fourth Industrial Revolution",
            "Robot sensing systems",
            "Machine components",
            "Virtual manufacturing",
            "Digital manufacturing (DM)"
        ],
        "abstract": "The Industry 4.0 concept promotes a digital manufacturing (DM) paradigm that can enhance quality and productivity, which reduces inventory and the lead time for delivering custom, batch-of-one products based on achieving convergence of additive, subtractive, and hybrid manufacturing machines, automation and robotic systems, sensors, computing, and communication networks, artificial intelligence, and big data. A DM system consists of embedded electronics, sensors, actuators, control software, and interconnectivity to enable the machines and the components within them to exchange data with other machines, components therein, the plant operators, the inventory managers, and customers. This article presents the cybersecurity risks in the emerging DM context, assesses the impact on manufacturing, and identifies approaches to secure DM.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.961",
        "scimago_value": "2,383"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "edgeawaremultilevelinteractivenetworkforsalientobjectdetectionofstripsteelsurfacedefects",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3124814",
        "author": [
            "Zhou, Xiaofei",
            "Fang, Hao",
            "Fei, Xiaobo",
            "Shi, Ran",
            "Zhang, Jiyong"
        ],
        "keywords": [
            "Strips",
            "Steel",
            "Image edge detection",
            "Feature extraction",
            "Decoding",
            "Computational modeling",
            "Object detection",
            "Salient object detection",
            "surface defects",
            "multi-level feature",
            "fusion",
            "edge"
        ],
        "abstract": "The performance of the salient object detection of strip surface defects has been promoted largely by deep learning based models. However, due to the complexity of strip surface defects, the existing models perform poorly in the challenging scenes such as noise disturbance, and low contrast between defect regions and background. Meanwhile, the detection results of existing models often suffer from coarse boundary details. Therefore, we propose a novel saliency model, namely an Edge-aware Multi-level Interactive Network, to detect the defects from the strip steel surface. Concretely, our model adopts the U-shape architecture where the two crucial points are the interactive feature integration and the edge-guided saliency fusion. Firstly, except the skip connection that combines the same stage of encoder and decoder, we deploy another connection, where the features from adjacent levels of encoder are transferred to the same stage of decoder. By this way, we are able to provide an effective fusion of multi-level deep features, yielding a well depiction for defects. Secondly, to give well-defined boundaries for prediction results, we add the edge extraction branch after each decoder block, where the progressive feature aggregation endows the edge with precise details and complete object cues. Meanwhile, together with the edge extraction branches, we deploy the saliency prediction branch at each decoder stage. After that, coupled with the fine edge information, we fuse all outputs of saliency prediction branches into the final saliency map, where the edge cue steers the saliency result to pay more attention to the boundary details. Following this way, we can provide a high-quality saliency map which can accurately locate and segment the defects. Extensive experiments are performed on the public dataset, and the results prove the effectiveness and robustness of our model which consistently outperforms the state-of-the-art models.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "anovelefficientsecureanderrorrobustschemeforinternetofthingsusingcompressivesensing",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3064700",
        "author": [
            "Kuldeep, Gajraj",
            "Zhang, Qi"
        ],
        "keywords": [
            "Internet of Things",
            "Encryption",
            "Forward error correction",
            "Image coding",
            "Sensors",
            "Wireless communication",
            "Wireless sensor networks",
            "Error robust encryption",
            "joint compression and error recovery",
            "projection matrix",
            "wireless body area network",
            "resource-constrained",
            "Industrial Internet of Things"
        ],
        "abstract": "In most of existing Internet of Things (IoT) applications, data compression, data encryption and error/erasure correction are implemented separately. To achieve reliable communication, in particular, in harsh wireless environment with strong interference, error/erasure correction codes with higher correction capability or Automatic repeat request (ARQ) scheme are desirable but at the cost of increasing complexity and energy consumption. Due to resource-constrained IoT device, it is often challenging to implement all of them. In this paper, we propose a novel lightweight efficient secure error-robust scheme, ENCRUST, which is able to achieve these three functions using simple matrix multiplication. ENCRUST is built on the new theoretical foundation of projection-based encoding presented in this paper, by leveraging the sparsity inherent in the signal. We perform theoretical analysis and experimental study of the proposed scheme in comparison with the conventional schemes. It shows that the proposed scheme can work in low SINR range and the reconstructed signal quality shows graceful degradation. Furthermore, we apply the proposed scheme on real-life electrocardiogram (ECG) dataset and images. The results demonstrate that ENCRUST achieves decent compression, information secrecy as well as strong error recovery in one go.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21511535",
        "isbn": null,
        "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "publisher": null,
        "title": "multiobjectivetaskschedulingforenergyefficientcloudimplementationofhyperspectralimageclassification",
        "booktitle": null,
        "doi": "10.1109/JSTARS.2020.3036896",
        "author": [
            "Sun, Jin",
            "Li, Heng",
            "Zhang, Yi",
            "Xu, Yang",
            "Zhu, Yaoqin",
            "Zang, Qitao",
            "Wu, Zebin",
            "Wei, Zhihui"
        ],
        "keywords": [
            "Cloud computing",
            "Hyperspectral imaging",
            "Task analysis",
            "Scheduling",
            "Scheduling algorithms",
            "Energy consumption",
            "Computational modeling",
            "Energy consumption",
            "hyperspectral image classification",
            "makespan",
            "multiobjective optimization",
            "task scheduling"
        ],
        "abstract": "Cloud computing has become a promising solution to efficient processing of remotely sensed big data, due to its high-performance and scalable computing capabilities. However, existing cloud solutions generally involve the problems of low resource utilization and high energy consumption when processing large-scale remote sensing datasets, affecting the quality-of-service of the cloud system. Aiming at hyperspectral image classification applications, this article proposes an energy-efficient cloud implementation by employing a multiobjective task scheduling algorithm. We first present a parallel computing mechanism for a fusion-based classification method based on Apache Spark. With the general classification flow represented by a workflow model, we formulate a multiobjective scheduling framework that jointly minimizes the total execution time as well as energy consumption. We further develop an effective scheduling algorithm to solve the multiobjective optimization problem and produce a set of Pareto-optimal solutions, providing the tradeoff between computational efficiency and energy efficiency. Experimental results demonstrate that the multiobjective scheduling approach proposed in this work can substantially reduce the execution time and energy consumption for performing large-scale hyperspectral image classification on Spark. In addition, our proposed algorithm can generate better tradeoff solutions to the multiobjective scheduling problem as compared to competing scheduling algorithms.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.784",
        "scimago_value": "1,246"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "constrainedgenerativeadversarialnetworks",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3054822",
        "author": [
            "Chao, Xiaopeng",
            "Cao, Jiangzhong",
            "Lu, Yuqin",
            "Dai, Qingyun",
            "Liang, Shangsong"
        ],
        "keywords": [
            "Training",
            "Nash equilibrium",
            "Generators",
            "Standards",
            "Generative adversarial networks",
            "Games",
            "Gallium nitride",
            "Generative adversarial networks",
            "Nash equilibrium",
            "Lipschitz constraint"
        ],
        "abstract": "Generative Adversarial Networks (GANs) are a powerful subclass of generative models. Yet, how to effectively train them to reach Nash equilibrium is a challenge. A number of experiments have indicated that one possible solution is to bound the function space of the discriminator. In practice, when optimizing the standard loss function without limiting the discriminator's output, the discriminator may suffer from lack of convergence. To be able to reach the Nash equilibrium in a faster way during training and obtain better generative data, we propose constrained generative adversarial networks, GAN-C, where a constraint on the discriminator's output is introduced. We theoretically prove that our proposed loss function shares the same Nash equilibrium as the standard one, and our experiments on mixture of Gaussians, MNIST, CIFAR-10, STL-10, FFHQ, and CAT datasets show that our loss function can better stabilize training and yield even better high-quality images.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "15580644",
        "isbn": null,
        "journal": "IEEE Transactions on Geoscience and Remote Sensing",
        "publisher": null,
        "title": "studyofsystematicbiasinmeasuringsurfacedeformationwithsarinterferometry",
        "booktitle": null,
        "doi": "10.1109/TGRS.2020.3003421",
        "author": [
            "Ansari, Homa",
            "De, Francesco",
            "Parizzi, Alessandro"
        ],
        "keywords": [
            "Strain",
            "Fading channels",
            "Time series analysis",
            "Synthetic aperture radar",
            "Systematics",
            "Decorrelation",
            "Moisture",
            "Big Data",
            "deformation estimation",
            "differential interferometric synthetic aperture radar (SAR) (DInSAR)",
            "distributed scatterers (DSs)",
            "error analysis",
            "near real-time (NRT) processing",
            "phase inconsistencies",
            "signal decorrelation",
            "time-series analysis"
        ],
        "abstract": "This article investigates the presence of a new interferometric signal in multilooked synthetic aperture radar (SAR) interferograms that cannot be attributed to the atmospheric or Earth-surface topography changes. The observed signal is short-lived and decays with the temporal baseline; however, it is distinct from the stochastic noise attributed to temporal decorrelation. The presence of such a fading signal introduces a systematic phase component, particularly in short temporal baseline interferograms. If unattended, it biases the estimation of Earth surface deformation from SAR time series. Here, the contribution of the mentioned phase component is quantitatively assessed. The biasing impact on the deformation-signal retrieval is further evaluated. A quality measure is introduced to allow the prediction of the associated error with the fading signals. Moreover, a practical solution for the mitigation of this physical signal is discussed; special attention is paid to the efficient processing of Big Data from modern SAR missions such as Sentinel-1 and NISAR. Adopting the proposed solution, the deformation bias is shown to decrease significantly. Based on these analyses, we put forward our recommendations for efficient and accurate deformation-signal retrieval from large stacks of multilooked interferograms.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.600",
        "scimago_value": "2,141"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "urbanrailtrainschedulingwithsmoothingenergyconsumptionpeaksandsynchronizationtimeminimizationusingnoveltimeshiftcontrolscheme",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3078569",
        "author": [
            "Zhou, Jin",
            "Guo, Xin",
            "Li, Feng"
        ],
        "keywords": [
            "Rails",
            "Synchronization",
            "Energy consumption",
            "Sun",
            "Rail transportation",
            "Programming",
            "Optimization",
            "Energy consumption",
            "synchronization time",
            "time-shift control scheme",
            "congestion"
        ],
        "abstract": "Considering operators of the urban rail transit systems are often faced with cost control, passengers required high service quality, which has inter-affection between each other in congestion issues for the peak period. A good cooperative timetable associated with time-based shift radios is proposed to achieve a mutually beneficial win-win situation for operators required low energy consumption (costs), and passengers required short waiting time in high peak level with different time-shift control schemes by shifting passengers' travel times. By seeking the optimal shift radios, we focus on generating a favorable train schedule by taking the optimal decisions in the presence of trade-offs between two conflicting objectives. Subsequently, an improved non-dominated sorting in genetic algorithms (INSGA-II) was presented to solve the multi-objective programming model. Finally, the computational results show that the optimized time-shift control scheme brings a significant effect on reducing congestion during the peak periods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "multiviewghostfreeimageenhancementforinthewildimageswithunknownexposureandgeometry",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3057167",
        "author": [
            "Khan, Rizwan",
            "Akram, Adeel",
            "Mehmood, Atif"
        ],
        "keywords": [
            "Lighting",
            "Cameras",
            "Image restoration",
            "Dynamic range",
            "Estimation",
            "Noise reduction",
            "Image color analysis",
            "Multi-view images",
            "feature matching",
            "virtual images",
            "exposure fusion"
        ],
        "abstract": "The multiview low dynamic range images captured with sparse camera arrangement under ill-lighting conditions contain highlighted and shadow regions due to over-exposed and under-exposed regions. The processing of these images produces contrast distortion, and it is challenging to maintain relative brightness with color consistency. Moreover, the disparity map estimation faces the challenges of holes and artifacts due to a wide baseline and poor visibility, with a shared view of vision. In this article, we propose a multiview ghost-free image enhancement strategy for in-the-wild images with unknown exposure and geometry. We address the complex geometric alignment problem for a wide variational baseline among multiple sparsely arranged cameras. The features among multiple viewpoints are detected and matched for the image restoration. The restored image contains highlighted and shadow regions with a color imbalance problem. We synthesize virtual images following the intensity mapping function, which compensates for the relative brightness and color distortions. Finally, we fuse all the images to obtain high-quality images. The proposed method is more frequent and feasible for future multiview systems with varying baselines without relying on disparity maps. The experimental results demonstrate that the proposed method outperformed the state-of-the-art approaches.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "parallelrouteoptimizationandserviceassuranceinenergyefficientsoftwaredefinedindustrialiotnetworks",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3056931",
        "author": [
            "Njah, Yosra",
            "Cheriet, Mohamed"
        ],
        "keywords": [
            "Quality of service",
            "Optimization",
            "Routing",
            "Industrial Internet of Things",
            "Energy consumption",
            "Production",
            "Delays",
            "Industrial Internet of Things (IIoT)",
            "software-defined networking (SDN)",
            "multiprogrammability",
            "traffic engineering",
            "Quality of Service (QoS)",
            "energy awareness",
            "resource optimization"
        ],
        "abstract": "In recent years, the Industrial world has been embracing new digital technology, including the internet of things (IoT) paradigm that promises revolutionizing-prospects in numerous industrial applications. However, many deployment challenges related to real-time big data analytics, service assurance, resource optimization, energy consumption, and security awareness are raised. In this work, we focus on service assurance and resource optimization, including energy consumption challenges over Industrial Internet of Things (IIoT)-based environments since the existing network routing algorithms cannot meet the strict heterogeneous quality of service (QoS) requirements of industrial communications while optimizing resources. We take advantage of the flexibility and programmability offered by the promising software-defined networking paradigm, and we propose a centralized route optimization and service assurance scheme, named ROSA, over a multi-layer programmable industrial architecture. The proposed solution supports a wide range of heterogeneous flows, such as ultra-reliable low-latency communications (URLLC) and bandwidth-sensitive services. The routing optimization problems are formulated as multi-constrained shortest path problems. The Lagrangian Relaxation approach is used to solve the . Hence, we deploy a pair of parallel routing algorithms run according to the flow type to ensure QoS requirements, efficiently allocate constrained resources, and enhance the overall network energy consumption. We conduct extensive simulations to validate the proposed ROSA scheme. The experimental results show promising performance in terms of reducing bandwidth utilization by up to 22%, end-to-end delay at least by 21%, packet loss by more than 19%, flow violation by about 16%, and energy consumption up to 14% as compared to well-known benchmarks in QoS provisioning and energy-aware routing problem.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "efficientweaklysupervisedobjectdetectionwithpseudoannotations",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3099497",
        "author": [
            "Yuan, Qingsheng",
            "Sun, Gang",
            "Liang, Jianming",
            "Leng, Biao"
        ],
        "keywords": [
            "Annotations",
            "Proposals",
            "Training",
            "Object detection",
            "Detectors",
            "Feature extraction",
            "Streaming media",
            "Object detection",
            "weakly-supervised learning",
            "data augmentation",
            "mixed-supervision"
        ],
        "abstract": "Weakly-supervised object detection (WSOD) has attracted lots of attention in recent years. However, there is still a big gap between WSOD and generic object detection. The main barriers to the efficiency of WSOD are the ineffective data augmentations and inaccurate bounding box predictions. Given only image-level annotations, it is hard for WSOD to effectively utilize variant data augmentations and accurately regress the bounding boxes. Although a fully-supervised object detector can be trained using annotations generated from the weakly-supervised object detector, the performance is still severely limited due to the low quality of mined pseudo annotations. This paper proposes an efficient WSOD method with pseudo annotations (EWPA) to make better use of imperfect annotations. With the assistance of pseudo annotations, EWPA can effectively regress more accurate bounding boxes while the traditional WSOD can only locate the salient parts of an object. Furthermore, pseudo annotations can help design more complex data augmentations, driving the network to learn more discriminative feature representations. Extensive experiments are conducted on PASCAL VOC 2007 and 2012 datasets and validate the effectiveness of EWPA.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "anovelgabasedoptimizedapproachforregionalmultimodalmedicalimagefusionwithsuperpixelsegmentation",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3094972",
        "author": [
            "Duan, Junwei",
            "Mao, Shuqi",
            "Jin, Junwei",
            "Zhou, Zhiguo",
            "Chen, Long",
            "Chen, C."
        ],
        "keywords": [
            "Image fusion",
            "Image segmentation",
            "Feature extraction",
            "Medical diagnostic imaging",
            "Transforms",
            "Clustering algorithms",
            "Genetic algorithms",
            "Multimodal medical image fusion",
            "superpixel segmentation",
            "genetic algorithm",
            "log-gabor filter",
            "sum modified laplacian"
        ],
        "abstract": "For multimodal medical image fusion problems, most of the existing fusion approaches are based on pixel-level. However, the pixel-based fusion method tends to lose local and spatial information as the relationships between pixels are not considered appropriately, which has much influence on the quality of the fusion results. To address this issue, a region-based multimodal medical image fusion framework is proposed based on superpixel segmentation and a post-processing optimization method in this paper. In this framework, the average image of the source medical images is firstly obtained by a weighted averaging method. To effectively obtain homogeneous regions and preserve the complete information of image details, the fast linear spectral clustering(LSC) superpixel algorithm is carried out to segment the average image and get superpixel labels. For each region of the medical images, log-gabor filter(LGF) and sum modified laplacian(SML) are adopted to extract texture feature and contrast feature for the measurement of region importance. The most important regions are selected and the decision map is generated by comparison. Moreover, to get a more accurate decision map, a new post-processing optimized method based on genetic algorithm(GA) is given. A weighted strategy is applied to the extracted features and the weighting factor can be adaptively adjusted by GA. The effectiveness of the proposed fusion method is validated by conducting experiments on eight pairs of medical images from diverse modalities. In addition, seven other mainstream medical image fusion methods are adopted for comparing the performance of fusion. Experimental results in terms of qualitative and quantitative evaluation demonstrate that the proposed method can achieve state-of-the-art performance for multimodal medical image fusion problems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "bilevelmultiobjectivegraywolfalgorithmbasedonpackettransportnetworkoptimization",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3130280",
        "author": [
            "Wang, Chunzhi",
            "Li, Xing",
            "Wang, Zaoning"
        ],
        "keywords": [
            "Optimization",
            "Linear programming",
            "Optical fibers",
            "Heuristic algorithms",
            "Bandwidth",
            "Task analysis",
            "Packet transport network",
            "bilevel programming",
            "multi-objective gray wolf algorithm",
            "label switching path",
            "committed information rate"
        ],
        "abstract": "Packet transport network (PTN), as an efficient transmission network technology in mobile communications in the big data era, is used by more and more communication operators. The existing PTN resource utilization rate is low, the network security is poor, so the existing PTN needs to be optimized in all aspects. For the optimization of the PTN, it is necessary to consider the decision of both the operator user and the service product supplier. Therefore, this paper proposes a bilevel multi-objective gray wolf algorithm based on PTN optimization problem. The operator user is the upper-level decision maker, and the objective function is to pay the product supplier the lowest cost. The product supplier is the lower-level decision maker, it mainly includes two major objective functions. The first objective function is to maximize the Label switching path overlap rate(LSPOR) evaluation score to solve the abnormal Label Switching Path (LSP) problem in the network, and the second is to maximize the committed bandwidth with utilizing rate(CBWUR) evaluation score to solve the problem of excessive Committed Information Rate(CIR) bandwidth usage in the network. According to the three scale network situation in Hubei, China, the improved multi-objective gray wolf algorithm is used to solve the PTN bilevel programming problem. The experimental results show that compared with the initial network, the optimized network size dropped by 125314 hops on average, the LSPOR increased by 13.64%, and the CBWUR increased by 3.7%. This model not only improves the utilization of network resources, but also reduces the cost to be paid by superior decision makers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21511535",
        "isbn": null,
        "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "publisher": null,
        "title": "arbitraryshapedbuildingboundaryawaredetectionwithpixelaggregationnetwork",
        "booktitle": null,
        "doi": "10.1109/JSTARS.2020.3017934",
        "author": [
            "Jiang, Xin",
            "Zhang, Xinchang",
            "Xin, Qinchuan",
            "Xi, Xu",
            "Zhang, Pengcheng"
        ],
        "keywords": [
            "Feature extraction",
            "Buildings",
            "Image segmentation",
            "Semantics",
            "Remote sensing",
            "Optimization",
            "Image edge detection",
            "Boundary quality",
            "building extraction",
            "high resolution",
            "structural similarity (SSIM)"
        ],
        "abstract": "Large-scale building extraction is an essential work in the field of a remote sensing image analysis. The high-resolution image extraction methods based on deep learning have achieved state-of-the-art performance. However, most of the previous work has focused on region accuracy rather than boundary quality. Aiming at the low-accuracy problems and incomplete boundary of the building extraction method, we propose a predictive optimization architecture, BAPANet. Notably, the architecture consists of an encoder\u2013decoder network, and residual refinement modules responsible for prediction, and refinement. The objective function optimizes the network in the form of three levels (pixel, feature map, and patch) by fusing three loss functions: binary cross-entropy, intersection over-union, and structural similarity. The five public datasets\u2019 experimental results show that the extraction method in this article has high region accuracy, and the boundary of buildings is clear and complete.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.784",
        "scimago_value": "1,246"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "areviewoncommunitydetectioninlargecomplexnetworksfromconventionaltodeeplearningmethodsacallfortheuseofparallelmetaheuristicalgorithms",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3095335",
        "author": [
            "Al-Andoli, Mohammed",
            "Tan, Shing",
            "Cheah, Wooi",
            "Tan, Sin"
        ],
        "keywords": [
            "Deep learning",
            "Computational modeling",
            "Complex networks",
            "Social networking (online)",
            "Optimization",
            "Big Data",
            "Terminology",
            "Community detection",
            "deep learning",
            "complex networks",
            "meta-heuristic algorithms",
            "parallel computing"
        ],
        "abstract": "Complex networks (CNs) have gained much attention in recent years due to their importance and popularity. The rapid growth in the size of CNs leads to more difficulties in the analysis of CNs tasks. Community Detection (CD) is an important multidisciplinary research area where many machine/deep learning-based methods have been applied to map CNs into a low-dimensional representation for extracting information similarity among members of CNs. Currently, Deep Learning (DL) is one of the promising methods to extract knowledge and learn information from high dimensional space and represent it in low dimensional space. However, designing an accurate and efficient DL-based CD method especially when dealing with large CNs is always an on-going research endeavor to pursue. Meta-Heuristic (MH) algorithms have shown their potentials in improving DL models in terms of solution quality and computational cost. In addition, parallel computing is a feasible solution for building efficient DL models. The algorithmic principle of MH is parallel in nature; however, its computation framework in DL training that is reported in the literature is not really implemented in a parallel computing setup. In this paper, we present a systematic review of CD in CNs from conventional machine learning to DL methods and point out the gap of applying DL-based CD methods in large CNs. In addition, the relevant studies on DL with parallel and MH approaches are reviewed and their implications on DL models are highlighted to prospect effective solutions to overcome the challenges of DL-based CD methods. We also point out research challenges in the field of CD and suggest possible future research directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "corpulyzeranovelframeworkforbuildinglowresourcelanguagecorpora",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3049793",
        "author": [
            "Tahir, Bilal",
            "Mehmood, Muhammad"
        ],
        "keywords": [
            "Buildings",
            "Tools",
            "Task analysis",
            "Social networking (online)",
            "Crawlers",
            "Computational modeling",
            "Vocabulary",
            "Common crawl",
            "web crawling",
            "text corpus",
            "corpus analysis",
            "regional languages corpora"
        ],
        "abstract": "The rapid proliferation of artificial intelligence has led to the development of sophisticated cutting-edge systems in natural language processing and computational linguistics domains. These systems heavily rely on high-quality dataset/corpora for the training of deep-learning algorithms to develop precise models. The preparation of a high-quality gold standard corpus for natural language processing on a large scale is a challenging task due to the need of huge computational resources, accurate language identification models, and precise content parsing tools. This task is further exacerbated in case of regional languages due to the scarcity of web content. In this article, we propose a generic framework of Corpus Analyzer - Corpulyzer - a novel framework for building low resource language corpora. Our framework consists of corpus generation and corpus analyzer module. We demonstrate the efficacy of our framework by creating a high-quality large scale corpus for the Urdu language as a case study. Leveraging dataset from Common Crawl Corpus (CCC), first, we prepare a list of seed URLs by filtering the Urdu language webpages. Next, we use Corpulyzer to crawl the World-Wide-Web (WWW) over a period of four years (2016-2020). We build Urdu web corpus \u201cUrduWeb20\u201d that consists of 8.0 million Urdu webpages crawled from 6,590 websites. In addition, we propose Low-Resource Language (LRL) website scoring algorithm and content-size filter for language-focused crawling to achieve optimal use of computational resources. Moreover, we analyze UrduWeb20 using variety of traditional metrics such as web-traffic-rank, URL depth, duplicate documents, and vocabulary distribution along with our newly defined content-richness metrics. Furthermore, we compare different characteristics of our corpus with three datasets of CCC. In general, we observe that contrary to CCC that focuses on crawling the limited number of webpages from highly ranked Urdu websites, Corpulyzer performs an in-depth crawling of Urdu content-rich websites. Finally, we made available Corpulyzer framework for the research community for corpus building.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "asystematicreviewonnomavariantsfor5gandbeyond",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3081601",
        "author": [
            "Budhiraja, Ishan",
            "Kumar, Neeraj",
            "Tyagi, Sudhanshu",
            "Tanwar, Sudeep",
            "Han, Zhu",
            "Piran, Md.",
            "Suh, Doug"
        ],
        "keywords": [
            "NOMA",
            "5G mobile communication",
            "Cooperative communication",
            "Wireless networks",
            "Quality of service",
            "Interference",
            "Device-to-device communication",
            "NOMA",
            "OMA",
            "uplink",
            "downlink",
            "device-to-device",
            "machine-to-machine"
        ],
        "abstract": "Over the last few years, interference has been a major hurdle for successfully implementing various end-user applications in the fifth-generation (5G) of wireless networks. During this era, several communication protocols and standards have been developed and used by the community. However, interference persists, keeping given quality of service (QoS) provision to end-users for different 5G applications. To mitigate the issues mentioned above, in this paper, we present an in-depth survey of state-of-the-art non-orthogonal multiple access (NOMA) variants having power and code domains as the backbone for interference mitigation, resource allocations, and QoS management in the 5G environment. These are future smart communication and supported by device-to-device (D2D), cooperative communication (CC), multiple-input and multiple-output (MIMO), and heterogeneous networks (HetNets). From the existing literature, it has been observed that NOMA can resolve most of the issues in the existing proposals to provide contention-based grant-free transmissions between different devices. The key differences between the orthogonal multiple access (OMA) and NOMA in 5G are also discussed in detail. Moreover, several open issues and research challenges of NOMA-based applications are analyzed. Finally, a comparative analysis of different existing proposals is also discussed to provide deep insights to the readers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "empiricalcomparisonofthefeatureevaluationmethodsbasedonstatisticalmeasures",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3058428",
        "author": [
            "\u0141ysiak, Adam",
            "Szmajda, Miros\u0142aw"
        ],
        "keywords": [
            "Feature extraction",
            "Visualization",
            "Task analysis",
            "Satellite broadcasting",
            "Probability distribution",
            "Prediction algorithms",
            "Classification",
            "dimensionality reduction",
            "distribution overlap",
            "feature evaluation",
            "feature extraction",
            "feature selection",
            "filter methods",
            "machine learning",
            "overlap coefficient",
            "pattern recognition"
        ],
        "abstract": "One of the most important classification problems is selecting proper features, i.e. features that describe the classified object in the most straightforward way possible. Then, one of the biggest challenges of the feature selection is the evaluation of the feature\u2019s quality. There is a plethora of feature evaluation methods in the literature. This paper presents the results of a comparison between nine selected feature evaluation methods, both existing in literature and newly defined. To make a comparison, features from ten various sets were evaluated by every method. Then, from every feature set, best subset (according to each method) was chosen. Those subsets then were used to train a set of classifiers (including decision trees and forests, linear discriminant analysis, naive Bayes, support vector machines, k nearest neighbors and an artificial neural network). The maximum accuracy of those classifiers, as well as the standard deviation between their accuracies, were used as a quality measures of each particular method. Furthermore, it was determined, which method is the most universal in terms of the data set, i.e. for which method, obtained accuracies were dependent on the feature set the least. Finally, computation time of each method was compared. Results indicated that for applications with limited computational power, method based on the average overlap between feature\u2019s values seem best suited. It led to high accuracies and proved to be fast to compute. However, if the data set is known to be normally distributed, method based on two-sample ${t}$ -test may be preferable.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "15258955",
        "isbn": null,
        "journal": "IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control",
        "publisher": null,
        "title": "feasibilityofmodelassistedprobabilityofdetectionprinciplesforstructuralhealthmonitoringsystemsbasedonguidedwavesforfiberreinforcedcomposites",
        "booktitle": null,
        "doi": "10.1109/TUFFC.2021.3084898",
        "author": [
            "Tsch\u00f6ke, Kilian",
            "Mueller, Inka",
            "Memmolo, Vittorio",
            "Moix-Bonet, Maria",
            "Moll, Jochen",
            "Lugovtsova, Yevgeniya",
            "Golub, Mikhail",
            "Venkat, Ramanan",
            "Schubert, Lars"
        ],
        "keywords": [
            "Reliability",
            "Acoustics",
            "Monitoring",
            "Industries",
            "Standards",
            "Inspection",
            "Automotive engineering",
            "Acousto ultrasonics (AUs)",
            "automotive industry",
            "damage detection",
            "elastodynamic finite integration technique (EFIT)",
            "reliability assessment"
        ],
        "abstract": "In many industrial sectors, structural health monitoring (SHM) is considered as an addition to nondestructive testing (NDT) that can reduce maintenance effort during the lifetime of a technical facility, structural component, or vehicle. A large number of SHM methods are based on ultrasonic waves, whose properties change depending on structural health. However, the wide application of SHM systems is limited due to the lack of suitable methods to assess their reliability. The evaluation of the system performance usually refers to the determination of the probability of detection (POD) of a test procedure. Up until now, only a few limited methods exist to evaluate the POD of SHM systems, which prevents them from being standardized and widely accepted in the industry. The biggest hurdle concerning the POD calculation is the large number of samples needed. A POD analysis requires data from numerous identical structures with integrated SHM systems. Each structure is then damaged at different locations and with various degrees of severity. All of these are connected to high costs. Therefore, one possible way to tackle this problem is to perform computer-aided investigations. In this work, the POD assessment procedure established in NDT according to the Berens model is adapted to guided wave-based SHM systems. The approach implemented here is based on solely computer-aided investigations. After efficient modeling of wave propagation phenomena across an automotive component made of a carbon-fiber-reinforced composite, the POD curves are extracted. Finally, the novel concept of a POD map is introduced to look into the effect of damage position on system reliability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.725",
        "scimago_value": "1,159"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "automatedsewerdefectsdetectionusingstylebasedgenerativeadversarialnetworksandfinetunedwellknowncnnclassifier",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3073915",
        "author": [
            "Situ, Zuxiang",
            "Teng, Shuai",
            "Liu, Hanlin",
            "Luo, Jinhua",
            "Zhou, Qianqian"
        ],
        "keywords": [
            "Training",
            "Generators",
            "Generative adversarial networks",
            "Deep learning",
            "Transfer learning",
            "Image quality",
            "Computational modeling",
            "Automated detection",
            "image synthesis",
            "sewer defects",
            "well-known CNN classifiers",
            "StyleGANs"
        ],
        "abstract": "Automated sewer defects detection has become an important trend for better management and maintenance of urban sewer systems. Deep learning technology has developed rapidly and offers an innovative solution for automated detection in engineering applications. However, insufficient data and unbalanced samples have proposed a big challenge to deep learning model training. This study adopts the state-of-the-art Style-based Generative Adversarial Networks (StyleGANs) model and compares the performances of its two variants in producing high-quality synthetic sewer defects images. Seven well-known CNN models are further fine-tuned and trained using the synthetic images for automated sewer defects detection to examine the effects of StyleGANs on augmenting the detection performance. Results show that both StyleGANs are efficient in producing high-quality images with various styles and high-level details for multiple types of sewer defects. Specifically, the StyleGAN2-Adaptive Discriminator Augmentation (StyleGAN2-ADA) with the aid of Freeze Discriminator (Freeze-D) yields the best model performance. Among the adopted CNN classifiers, Inception_v3 achieves the highest detection accuracy. The mean detection accuracy is 94% (with a specific accuracy of 99.7%, 97%, 95.3% and 84% for tree root, residential wall, disjoint and obstacle, respectively) and confirms the reliability of the StyleGANs' performance. The study shows that StyleGANs provide a promising method to alleviate the limited and uneven dataset problem and can improve the deep learning model performance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "socialawareloadbalancingsystemforcrowdsincellularnetworks",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3100459",
        "author": [
            "Torres, Renato",
            "Fortes, Sergio",
            "Baena, Eduardo",
            "Barco, Raquel"
        ],
        "keywords": [
            "Load management",
            "Load modeling",
            "Social networking (online)",
            "Optimization",
            "Heuristic algorithms",
            "Cellular networks",
            "Prediction algorithms",
            "Social events",
            "mobile communication",
            "fuzzy control",
            "load balancing",
            "social-awareness",
            "communication system operations and management"
        ],
        "abstract": "Over time, load balancing systems in cellular networks have been key to avoiding overload problems in the network and to maintaining a correct resource allocation and performance. However, the classical approaches were not designed for the dynamism generated by user behavior. The big crowds of users at certain social venues are one of the main concerns of mobile operators due to the load imbalance generated. Additionally, the mobility of users who attend social events (e.g., sports events, concerts, etc.) greatly impacts network performance due to its high correlation with network traffic. The availability to inform about events, particularly regarding venue location (e.g., stadiums, concert halls, convention centers) is exponentially growing thanks to its proliferation in social networks through geolocation databases and other functionalities. Therefore, the present work proposes a novel load balancing system integrating a fuzzy logic controller algorithm with social-awareness, which considers the relative position between cell sites and the social event venue in order to configure the network parameters. This approach is evaluated for different configurations of load balancing methods simulated on an urban macro scenario, mitigating the impact of the number of users per cell without degrading the signal quality. In this way, results show that social event data information plus soft or aggressive transmission power changes in cells can help to maintain the balance in the number of users per cell during mass events.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21682275",
        "isbn": null,
        "journal": "IEEE Transactions on Cybernetics",
        "publisher": null,
        "title": "evolutionarydivideandconqueralgorithmforvirusspreadingcontrolovernetworks",
        "booktitle": null,
        "doi": "10.1109/TCYB.2020.2975530",
        "author": [
            "Zhao, Tian-Fang",
            "Chen, Wei-Neng",
            "Kwong, Sam",
            "Gu, Tian-Long",
            "Yuan, Hua-Qiang",
            "Zhang, Jie",
            "Zhang, Jun"
        ],
        "keywords": [
            "Optimization",
            "Viruses (medical)",
            "Resource management",
            "Computer science",
            "Genetic algorithms",
            "Complex networks",
            "Cooperative coevolution (CC)",
            "evolutionary algorithm (EA)",
            "networked system",
            "resource allocation",
            "spreading control"
        ],
        "abstract": "The control of virus spreading over complex networks with a limited budget has attracted much attention but remains challenging. This article aims at addressing the combinatorial, discrete resource allocation problems (RAPs) in virus spreading control. To meet the challenges of increasing network scales and improve the solving efficiency, an evolutionary divide-and-conquer algorithm is proposed, namely, a coevolutionary algorithm with network-community-based decomposition (NCD-CEA). It is characterized by the community-based dividing technique and cooperative coevolution conquering thought. First, to reduce the time complexity, NCD-CEA divides a network into multiple communities by a modified community detection method such that the most relevant variables in the solution space are clustered together. The problem and the global swarm are subsequently decomposed into subproblems and subswarms with low-dimensional embeddings. Second, to obtain high-quality solutions, an alternative evolutionary approach is designed by promoting the evolution of subswarms and the global swarm, in turn, with subsolutions evaluated by local fitness functions and global solutions evaluated by a global fitness function. Extensive experiments on different networks show that NCD-CEA has a competitive performance in solving RAPs. This article advances toward controlling virus spreading over large-scale networks.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "11.448",
        "scimago_value": "3,109"
    },
    {
        "issnkey": "21682208",
        "isbn": null,
        "journal": "IEEE Journal of Biomedical and Health Informatics",
        "publisher": null,
        "title": "hospitaladmissionlocationpredictionviadeepinterpretablenetworksfortheyearroundimprovementofemergencypatientcare",
        "booktitle": null,
        "doi": "10.1109/JBHI.2020.2990309",
        "author": [
            "El-Bouri, Rasheed",
            "Eyre, David",
            "Watkinson, Peter",
            "Zhu, Tingting",
            "Clifton, David"
        ],
        "keywords": [
            "Hospitals",
            "Training",
            "Neural networks",
            "Deep learning",
            "Optimization",
            "Machine learning algorithms",
            "Machine learning algorithms",
            "multi-layer neural networks",
            "patient flow",
            "hospitals"
        ],
        "abstract": "Objective: This paper presents a deep learning method of predicting where in a hospital emergency patients will be admitted after being triaged in the Emergency Department (ED). Such a prediction will allow for the preparation of bed space in the hospital for timely care and admission of the patient as well as allocation of resource to the relevant departments, including during periods of increased demand arising from seasonal peaks in infections. Methods: The problem is posed as a multi-class classification into seven separate ward types. A novel deep learning training strategy was created that combines learning via curriculum and a multi-armed bandit to exploit this curriculum post-initial training. Results: We successfully predict the initial hospital admission location with area-under-receiver-operating-curve (AUROC) ranging between 0.60 to 0.78 for the individual wards and an overall maximum accuracy of 52% where chance corresponds to 14% for this seven-class setting. Our proposed network was able to interpret which features drove the predictions using a `network saliency' term added to the network loss function. Conclusion: We have proven that prediction of location of admission in hospital for emergency patients is possible using information from triage in ED. We have also shown that there are certain tell-tale tests which indicate what space of the hospital a patient will use. Significance: It is hoped that this predictor will be of value to healthcare institutions by allowing for the planning of resource and bed space ahead of the need for it. This in turn should speed up the provision of care for the patient and allow flow of patients out of the ED thereby improving patient flow and the quality of care for the remaining patients within the ED.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.772",
        "scimago_value": "1,293"
    },
    {
        "issnkey": "21511535",
        "isbn": null,
        "journal": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "publisher": null,
        "title": "influencingfactorsofspatialheterogeneityoflandsurfacetemperatureinnanjingchina",
        "booktitle": null,
        "doi": "10.1109/JSTARS.2021.3105582",
        "author": [
            "Fan, Qiang",
            "Song, Xiaonan",
            "Shi, Yue",
            "Gao, Rui"
        ],
        "keywords": [
            "Land surface temperature",
            "Urban areas",
            "Remote sensing",
            "Earth",
            "Production",
            "Artificial satellites",
            "Temperature",
            "Geographically weighted regression (GWR)",
            "human settlements",
            "land surface temperature (LST)",
            "Nanjing",
            "spatial heterogeneity"
        ],
        "abstract": "The environment and climate significantly affect the land surface temperature (LST) of a city. Previous studies have revealed that LST exhibits significant spatial heterogeneity primarily caused by a combination of natural factors and human activities. Based on this, the introduction of point of interest data of the \u201cproduction-living-ecological space\u201d divides the influencing pattern into a comprehensive description of human activities supplemented by natural factors, resulting in the precise influencing factors of spatial heterogeneity of LST. Taking Nanjing (Jiangsu Province, China) as a case study, this study uses Landsat-8 remote sensing images, point of interest data, and other data to establish a geographically weighted regression model that combines natural factors and human activities. The main research results are as follows: First, the LST of Nanjing ranged from 19.9 \u00b0C to 47.6 \u00b0C, whereas the distribution trend was \u201clow at both ends and high in the middle.\u201d Second, there is no multicollinearity of the influencing factors, the fitting degree of LST and each influencing factor reached 0.87. The regression coefficients were high and exhibited both positive and negative values, implying that spatial heterogeneity exists among the influencing factors and LST. Finally, the ranking of how all factors influence the LST followed the order of water area > forest and grassland > ecological space > slope > production space > elevation > living space. The research results have practical significance for improving the quality of life of urban residents and providing a critical theoretical basis for optimizing urban human settlements.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.784",
        "scimago_value": "1,246"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "fuzzymultilevelimagethresholdingbasedonimprovedcoyoteoptimizationalgorithm",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3060749",
        "author": [
            "Li, Linguo",
            "Sun, Lijuan",
            "Xue, Yu",
            "Li, Shujing",
            "Huang, Xuwen",
            "Mansour, Romany"
        ],
        "keywords": [
            "Image segmentation",
            "Optimization",
            "Linear programming",
            "Entropy",
            "Particle swarm optimization",
            "Histograms",
            "Heuristic algorithms",
            "Coyote optimization algorithm",
            "information entropy",
            "image segmentation",
            "multilevel thresholding"
        ],
        "abstract": "Due to the computational complexity of multilevel image thresholding, Swarm Intelligence Optimization Algorithm (SIOA) has been widely applied to improve the calculation efficiency. Therefore, more and more attention has been paid to exploring the application of the latest SIOA in multilevel segmentation. This article takes Otsu and fuzzy entropy as the objective functions, using Coyote Optimization Algorithm (COA) for multilevel thresholds optimization selection, through fuzzy median aggregation of local neighborhood information and then forms the Fuzzy Coyote Optimization Algorithm (FCOA), so that the thresholding image segmentation can be achieved in the end. To prevent the COA algorithm from falling into the local optimum, this article follows the differential evolution strategy adopted by the standard COA, using the number of iterations to construct the differential scaling factor to form the Improved Coyote Optimization Algorithm (ICOA). The experimental results show that fuzzy Kapur entropy and fuzzy median value aggregation-based ICOA(FICOA) achieves better image segmentation quality. Compared with Grey Wolf Optimizer (GWO), Fuzzy Modified Quick Artificial Bee Colony and Aggregation Algorithm (FMQABCA) and Fuzzy Modified Discrete Grey Wolf Optimizer and Aggregation Algorithm (FMDGWOA), FCOA and FICOA have certain advantages in visual effects of image segmentation and PSNR, FSIM evaluation indices. Particularly compared with GWO (also a wolf evolutionary algorithm), FICOA shows significant advantages.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "bshtisbuffersizingforheterogeneoustrafficandintegratedsystem",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3102423",
        "author": [
            "Shi, Huaifeng",
            "Pan, Chengsheng",
            "Wang, Yingzhi"
        ],
        "keywords": [
            "Job shop scheduling",
            "Quality of service",
            "Queueing analysis",
            "Markov processes",
            "Diffserv networks",
            "Bandwidth",
            "Videos",
            "xsSRD traffic",
            "LRD traffic",
            "integrated scheduling",
            "buffer sizing"
        ],
        "abstract": "Buffer sizing for switching and routing devices is of significance for guaranteeing the Quality of Service (QoS) of critical services on the Internet of Things (IoT), continuously evolving scheduling mechanisms and complex traffic characteristics pose new challenges for the traditional method of static buffer sizing based on rule-of-thumb. In this paper, the scope of buffer sizing is extended from a basic scheduling system under homogeneous arrival traffic input to an integrated scheduling system under heterogeneous arrival traffic input which is more ubiquitous. In this context, Voices, videos and other heterogeneous data in the IoT are categorized into short-range-dependent (SRD) and long-range-dependent (LRD) traffic, and the integrated scheduling system is decomposed into single-server-single-queue (SSSQ) systems by not only decoupling the complex dependencies among heterogeneous traffic inputs but also taking the impact of SRD and LRD traffic burstiness on the buffer sizing into account. On this basis, expressions for the relation between the minimum buffer size and the maximum overflow probability are presented. The numerical analysis results and simulation analysis results reveal that the average arrival rate, traffic burst level and scheduling priority are positively correlated with the required buffer size, and once the overflow probability is set, the minimum buffer size can be determined correspondingly. The achievements of this paper will provide theoretical guidance for IoT manufacturers and technicians to set buffers more reasonably and use resources more efficiently.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "15580210",
        "isbn": null,
        "journal": "IEEE Transactions on Neural Systems and Rehabilitation Engineering",
        "publisher": null,
        "title": "areliablefalldetectionsystembasedonanalyzingthephysicalactivitiesofolderadultslivinginlongtermcarefacilities",
        "booktitle": null,
        "doi": "10.1109/TNSRE.2021.3133616",
        "author": [
            "Saleh, Majd",
            "Abbas, Manuel",
            "Prud\u2019Homm, Joaquim",
            "Somme, Dominique",
            "Le, R\u00e9gine"
        ],
        "keywords": [
            "Fall detection",
            "Sensitivity",
            "Temperature measurement",
            "Sea measurements",
            "Pressure measurement",
            "Performance evaluation",
            "Detectors",
            "Fall detection",
            "wearable sensors",
            "machine learning",
            "elderly health care"
        ],
        "abstract": "Fall detection systems are designed in view to reduce the serious consequences of falls thanks to the early automatic detection that enables a timely medical intervention. The majority of the state-of-the-art fall detection systems are based on machine learning (ML). For training and performance evaluation, they use some datasets that are collected following predefined simulation protocols i.e. subjects are asked to perform different types of activities and to repeat them several times. Apart from the quality of simulating the activities, protocol-based data collection results in big differences between the distribution of the activities of daily living (ADLs) in these datasets in comparison with the actual distribution in real life. In this work, we first show the effects of this problem on the sensitivity of the ML algorithms and on the interpretability of the reported specificity. Then, we propose a reliable design of an ML-based fall detection system that aims at discriminating falls from the ambiguous ADLs. The latter are extracted from 400 days of recorded activities of older adults experiencing their daily life. The proposed system can be used in neck- and wrist-worn fall detectors. In addition, it is invariant to the rotation of the wearable device. The proposed system shows 100% of sensitivity while it generates an average of one false positive every 25 days for the neck-worn device and an average of one false positive every 3 days for the wrist-worn device.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "pansharpeningbasedonpanchromaticcolorizationusingworldview2",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3104321",
        "author": [
            "Xiong, Zhangxi",
            "Guo, Qing",
            "Liu, Mingliang",
            "Li, An"
        ],
        "keywords": [
            "Spatial resolution",
            "Feature extraction",
            "Training",
            "Deep learning",
            "Decoding",
            "Image resolution",
            "Convolution",
            "Pan-sharpening",
            "deep learning",
            "multispectral image",
            "panchromatic image",
            "image colorization",
            "loss function"
        ],
        "abstract": "In order to overcome the lack of the multispectral image (MS) and adequately preserve the spatial information of panchromatic (PAN) image and the spectral information of MS image, this study proposes a method which adds the spectral information of the prior MS to the prior PAN during training, and only the posterior PAN is needed for predicting. Firstly, we introduce the autoencoder model based on image colorization and discuss its feasibility in the field of multi-band remote sensing image pan-sharpening. Then, the image quality evaluation functions including spatial and spectral indexes are formed as the loss function to control the image colorization model. Because the loss function contains spatial and spectral evaluation indexes, it could directly calculate the loss between the network output and the label considering characteristics of remote sensing images. Besides, the training data in our model is original PAN, this means that it is not necessary to make the simulated degraded MS and PAN data for training which is a big difference from most existing deep learning pan-sharpening methods. The new loss function including the spectral and spatial quality instead of the general MSE (mean square error), only the original PAN instead of the simulated degraded MS + PAN to be inputted, only the spectral feature instead of the direct fusion result to be learned, these three aspects change the current learning framework and optimization rule of deep learning pan-sharpening. Finally, thousands of remote sensing images from different scenes are adopted to make the training dataset to verify the effectiveness of the proposed method. In addition, we selected seven representative pan-sharpening algorithms and four widely recognized objective fusion metrics to evaluate and compare the performance on the WorldView-2 experimental data. The results show that the proposed method achieves optimal performance in terms of both the subjective visual effect and the object assessment.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "amultivocalliteraturereviewongrowingsocialengineeringbasedcyberattacksthreatsduringthecovid19pandemicchallengesandprospectivesolutions",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2020.3048839",
        "author": [
            "Hijji, Mohammad",
            "Alam, Gulzar"
        ],
        "keywords": [
            "COVID-19",
            "Pandemics",
            "Organizations",
            "Standards organizations",
            "Computer hacking",
            "Buildings",
            "Social networking (online)",
            "Multivocal literature review",
            "social engineering",
            "COVID-19",
            "security and privacy",
            "prospective solutions",
            "cyber-attacks and threats"
        ],
        "abstract": "The novel coronavirus (COVID-19) pandemic has caused a considerable and long-lasting social and economic impact on the world. Along with other potential challenges across different domains, it has brought numerous cybersecurity challenges that must be tackled timely to protect victims and critical infrastructure. Social engineering\u2013based cyber-attacks/threats are one of the major methods for creating turmoil, especially by targeting critical infrastructure, such as hospitals and healthcare services. Social engineering\u2013based cyber-attacks are based on the use of psychological and systematic techniques to manipulate the target. The objective of this research study is to explore the state-of-the-art and state-of-the-practice social engineering\u2013based techniques, attack methods, and platforms used for conducting such cybersecurity attacks and threats. We undertake a systematically directed Multivocal Literature Review (MLR) related to the recent upsurge in social engineering\u2013based cyber-attacks/threats since the emergence of the COVID-19 pandemic. A total of 52 primary studies were selected from both formal and grey literature based on the established quality assessment criteria. As an outcome of this research study; we discovered that the major social engineering\u2013based techniques used during the COVID-19 pandemic are phishing, scamming, spamming, smishing, and vishing, in combination with the most used socio-technical method: fake emails, websites, and mobile apps used as weapon platforms for conducting successful cyber-attacks. Three types of malicious software were frequently used for system and resource exploitation are; ransomware, trojans, and bots. We also emphasized the economic impact of cyber-attacks performed on different organizations and critical infrastructure in which hospitals and healthcare were on the top targeted infrastructures during the COVID-19 pandemic. Lastly, we identified the open challenges, general recommendations, and prospective solutions for future work from the researcher and practitioner communities by using the latest technology, such as artificial intelligence, blockchain, and big data analytics.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "peertopeerenergytradingmechanismbasedonblockchainandmachinelearningforsustainableelectricalpowersupplyinsmartgrid",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3060457",
        "author": [
            "Jamil, Faisal",
            "Iqbal, Naeem",
            [],
            "Ahmad, Shabir",
            "Kim, Dohyeun"
        ],
        "keywords": [
            "Blockchain",
            "Smart contracts",
            "Predictive models",
            "Crowdsourcing",
            "Machine learning",
            "Peer-to-peer computing",
            "Energy consumption",
            "Energy trading",
            "energy prediction",
            "predictive analysis",
            "machine learning",
            "blockchain"
        ],
        "abstract": "It is expected that peer to peer energy trading will constitute a significant share of research in upcoming generation power systems due to the rising demand of energy in smart microgrids. However, the on-demand use of energy is considered a big challenge to achieve the optimal cost for households. This paper proposes a blockchain-based predictive energy trading platform to provide real-time support, day-ahead controlling, and generation scheduling of distributed energy resources. The proposed blockchain-based platform consists of two modules; blockchain-based energy trading and smart contract enabled predictive analytics modules. The blockchain module allows peers with real-time energy consumption monitoring, easy energy trading control, reward model, and unchangeable energy trading transaction logs. The smart contract enabled predictive analytics module aims to build a prediction model based on historical energy consumption data to predict short-term energy consumption. This paper uses real energy consumption data acquired from the Jeju province energy department, the Republic of Korea. This study aims to achieve optimal power flow and energy crowdsourcing, supporting energy trading among the consumer and prosumer. Energy trading is based on day-ahead, real-time control, and scheduling of distributed energy resources to meet the smart grid\u2019s load demand. Moreover, we use data mining techniques to perform time-series analysis to extract and analyze underlying patterns from the historical energy consumption data. The time-series analysis supports energy management to devise better future decisions to plan and manage energy resources effectively. To evaluate the proposed predictive model\u2019s performance, we have used several statistical measures, such as mean square error and root mean square error on various machine learning models, namely recurrent neural networks and alike. Moreover, we also evaluate the blockchain platform\u2019s effectiveness through hyperledger calliper in terms of latency, throughput, and resource utilization. Based on the experimental results, the proposed model is effectively used for energy crowdsourcing between the prosumer and consumer to attain service quality.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "autonomousconfigurationofcommunicationsystemsforiotsmartnodessupportedbymachinelearning",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3081794",
        "author": [
            "Gl\u00f3ria, Andr\u00e9",
            "Sebasti\u00e3o, Pedro"
        ],
        "keywords": [
            "Protocols",
            "Machine learning",
            "Wireless communication",
            "Peer-to-peer computing",
            "Logic gates",
            "Batteries",
            "Zigbee",
            "Wireless communications",
            "edge computing",
            "Internet of Things",
            "machine learning",
            "random forest",
            "sustainability"
        ],
        "abstract": "Machine Learning brings intelligence services to IoT systems, with Edge Computing contributing for edge nodes to be part of these services, allowing data to be processed directly in the nodes in real time. This paper introduces a new way of creating a self-configurable IoT node, in terms of communications, supported by machine learning and edge computing, in order to achieve a better efficiency in terms of power consumption, as well as a comparison between regression models and between deploying them in edge or cloud fashions, with a real case implementation. The correct choice of protocol and configuration parameters can make the difference between a device battery lasting 100 times more. The proposed method predicts the energy consumption and quality of signal using regressions based on node location, distance and obstacles and the transmission power used. With an accuracy of 99.88% and a margin of error of 1.504 mA for energy consumption and 98.68% and a margin of error of 1.9558 dBm for link quality, allowing the node to use the best transmission power values for reliability and energy efficiency. With this it is possible to achieve a network that can reduce up to 68% the energy consumption of nodes while only compromising in 7% the quality of the network. Besides that, edge computing proves to be a better solution when energy efficient nodes are needed, as less messages are exchanged, and the reduced latency allows nodes to be configured in less time.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "ballmotioncontrolinthetabletennisrobotsystemusingtimeseriesdeepreinforcementlearning",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3093340",
        "author": [
            "Yang, Luo",
            "Zhang, Haibo",
            "Zhu, Xiangyang",
            "Sheng, Xinjun"
        ],
        "keywords": [
            "Sports",
            "Robots",
            "Trajectory",
            "Sports equipment",
            "Motion control",
            "Estimation",
            "Atmospheric modeling",
            "Ball motion control",
            "reinforcement learning",
            "spin velocity estimation",
            "table tennis robot"
        ],
        "abstract": "One of the biggest challenges hindering a table tennis robot to play as well as a professional player is the ball\u2019s accurate motion control, which depends on various factors such as the incoming ball\u2019s position, linear, spin velocity and so forth. Unfortunately, some factors are almost impossible to be directly measured in real practice, such as the ball\u2019s spin velocity, which is difficult to be estimated from vision due to the little texture on the ball\u2019s surface. To perform accurate motion control in table tennis, this study proposes to learn a ball stroke strategy to guarantee desirable \u201ctarget landing location\u201d and the \u201cover-net height\u201d which are two key indicators to evaluate the quality of a stroke. To overcome the spin velocity challenge, a deep reinforcement learning (DRL) based stroke approach is developed with the spin velocity estimation capability, through which the system can predict the relative spin velocity of the ball and stroke it back accurately by iteratively learning from the robot-environment interactions. To pre-train the DRL-based strategy effectively, this paper develops a virtual table tennis playing environment, through which various simulated data can be collected. For the real table tennis robot implementation, experimental results demonstrate the superior performance of the proposed control strategy compared to that of the traditional aerodynamics-based method with an average landing error around 80mm and the landing-within-table probability higher than 70%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "ieeeaccessspecialsectioneditorialtowardsmartcitieswithiotbasedoncrowdsensing",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3106756",
        "author": [
            "Wang, Kun",
            "Wang, Zhibo",
            "Song, Ye-Qiong",
            "Yang, Dejun",
            "He, Shibo",
            "Wang, Wei"
        ],
        "keywords": [
            ""
        ],
        "abstract": "The proliferation of the Internet of Things (IoT) has paved the way for the future of smart cities. The large volume of data over the IoT can enable decision-making for various applications such as smart transportation, smart parking, and smart lighting. The key to the success of smart cities is data collection and aggregation over the IoT. Recently, crowdsensing has become a new data collection paradigm over the IoT, which can realize large-scale and fine-grained data collection with low cost for various applications. For example, we can leverage the power of the crowd to build a real-time noise map with microphones on smartphones. Despite the advantages of crowdsensing and the IoT, there are many challenges to utilize crowdsensing over the IoT for smart cities, such as how to allocate tasks to appropriate users to provide high-quality sensing data, how to incentivize users to participate in crowdsourcing, how to detect the reliability of the crowdsourced data, and how to protect the privacy of users.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "ieeeaccessspecialsectioneditorialurbancomputingandintelligence",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3111669",
        "author": [
            "Zhu, Rongbo",
            "Liu, Lu",
            "Ma, Maode",
            "Li, Hongxiang",
            "Mao, Shiwen"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Urban computing utilizes unobtrusive and ubiquitous sensing technologies, advanced data management and analytics models, and novel visualization methods to create win-win-win solutions which intelligently improve people\u2019s lives, urban environments, and city operation systems. With the help of cloud computing, the Internet of Things, device-to-device (D2D) communication, artificial intelligence (AI), big data, and urban computing and intelligence will bridge the gap of ubiquitous sensing, intelligent computing, cooperative communication, and mass data management technologies, to create novel solutions that improve urban environments, human life quality, and smart city systems. Thus, urban computing and intelligence has recently attracted significant attention from industry and academia for building smart cities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "21693536",
        "isbn": null,
        "journal": "IEEE Access",
        "publisher": null,
        "title": "ieeeaccessspecialsectioneditorialadvancedartificialintelligencetechnologiesforsmartmanufacturing",
        "booktitle": null,
        "doi": "10.1109/ACCESS.2021.3106717",
        "author": [
            "Yau, Her-Terng",
            "Prior, Stephen",
            "Wang, Yang",
            "Li, Yunhua"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Industry 4.0, also known as the fourth industrial revolution, is an area that many scientists and manufacturers are pursuing. Industry 4.0 consists of many topics such as the Internet of things (IoT), big data, cloud computing, smart manufacturing, and so on. Smart manufacturing is a crucial and valuable topic which aims at developing advanced techniques to improve the quality and costs of manufacturing. Through sensors, networks, and high-performance computers, powerful algorithms for smart manufacturing can be developed and implemented. Thanks to an innovative variety of sensors, reliable, and high-resolution information can be collected and utilized. Networks allow signals to be exchanged quickly between sensors, machines, and computers. Artificial intelligence (AI) requires huge computation power. Modern computers provide graphic cards with parallel computing, breaking this restriction. Algorithms related to smart manufacturing will be more complicated than before. As a result, this Special Section aims to speed up the development of smart manufacturing, attract the attention of communities, and disseminate novel research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.367",
        "scimago_value": "0,587"
    },
    {
        "issnkey": "14629011",
        "isbn": null,
        "journal": "Environmental Science & Policy",
        "publisher": null,
        "title": "abigdataandartificialintelligenceframeworkforsmartandpersonalizedairpollutionmonitoringandhealthmanagementinhongkong",
        "booktitle": null,
        "doi": "10.1016/j.envsci.2021.06.011",
        "author": [
            "Li, Victor",
            "Lam, Jacqueline",
            "Han, Yang",
            "Chow, Kenyon"
        ],
        "keywords": [
            "Air Pollution Monitoring, Health Management, Artificial Intelligence, Big Data, PM, Personalization, Smart Behavioural Intervention, Health and Well-being Improvement"
        ],
        "abstract": "All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "exploringfuturechallengesforbigdatainthehumanitariandomain",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2020.09.035",
        "author": [
            "Bell, David",
            "Lycett, Mark",
            "Marshan, Alaa",
            "Monaghan, Asmat"
        ],
        "keywords": [
            "Big data, Veracity, Granularity, Heterogeneous datasets, Humanitarian, Value"
        ],
        "abstract": "This paper examines the challenges of leveraging big data in the humanitarian sector in support of UN Sustainable Development Goal 17 \u201cPartnerships for the Goals\u201d. The full promise of Big Data is underpinned by a tacit assumption that the heterogeneous \u2018exhaust trail\u2019 of data is contextually relevant and sufficiently granular to be mined for value. This promise, however, relies on relationality \u2013 that patterns can be derived from combining different pieces of data that are of corresponding detail or that there are effective mechanisms to resolve differences in detail. Here, we present empirical work integrating eight heterogeneous datasets from the humanitarian domain to provide evidence of the inherent challenge of complexity resulting from differing levels of data granularity. In clarifying this challenge, we explore the reasons why it is manifest, discuss strategies for addressing it and, as our principal contribution, identify five propositions to guide future research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "bigdataandhumanresourcemanagementresearchanintegrativereviewandnewdirectionsforfutureresearch",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2021.04.019",
        "author": [
            "Zhang, Yucheng",
            "Xu, Shan",
            "Zhang, Long",
            "Yang, Mengxi"
        ],
        "keywords": [
            "Human resource management research, Big data, Integrative review, Inductive and deductive paradigms"
        ],
        "abstract": "The lack of sufficient big data-based approaches impedes the development of human resource management (HRM) research and practices. Although scholars have realized the importance of applying a big data approach to HRM research, clear guidance is lacking regarding how to integrate the two. Using a clustering algorithm based on the big data research paradigm, we first conduct a bibliometric review to quantitatively assess and scientifically map the evolution of the current big data HRM literature. Based on this systematic review, we propose a general theoretical framework described as \u201cInductive (Prediction paradigm: Data mining/Theory building) vs. Deductive (Explanation paradigm: Theory testing)\u201d. In this framework, we discuss potential research questions, their corresponding levels of analysis, relevant methods, data sources and software. We then summarize the general procedures for conducting big data research within HRM research. Finally, we propose a future agenda for applying big data approaches to HRM research and identify five promising HRM research topics at the micro, meso and macro levels along with three challenges and limitations that HRM scholars may face in the era of big data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "theconvergenceofbigdataandaccountinginnovativeresearchopportunities",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.121171",
        "author": [
            "Ibrahim, Awad",
            "Elamer, Ahmed",
            "Ezat, Amr"
        ],
        "keywords": [
            "Big data, Analytics, Accounting, Data science, Business intelligence"
        ],
        "abstract": "This study aims to develop accounting standards, curriculums, and research to cope with the rapid development of big data. The study presents several potential convergence points between big data and different accounting techniques and theories. The study discusses how big data can overcome the data limitations of six accounting issues: financial reporting, performance measurement, audit evidence, risk management, corporate budgeting and activity-based techniques. It presents six exciting research questions for future research. Then, the study explains the potential convergence between big data and agency theory, stakeholders theory, and legitimacy theory. This theoretical study develops new convergence points between big data and accounting by reviewing the literature and proposing new ideas and research questions. The conclusion indicates a significant convergence between big data and accounting on the premise that data is the heart of accounting. Big data and advanced analytics have the potential to overcome the data limitations of accounting techniques that require estimations and predictions. A remarkable convergence is argued between big data and three accounting theories. Overall, the study presents helpful insights to members of the accounting and auditing community on the potential of big data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "02786125",
        "isbn": null,
        "journal": "Journal of Manufacturing Systems",
        "publisher": null,
        "title": "adigitaltwinbasedbigdatavirtualandrealfusionlearningreferenceframeworksupportedbyindustrialinternettowardssmartmanufacturing",
        "booktitle": null,
        "doi": "10.1016/j.jmsy.2020.11.012",
        "author": [
            "Wang, Pei",
            "Luo, Ming"
        ],
        "keywords": [
            "Virtual and real fusion learning, Big data learning and analysis models, Digital twin, Industrial internet, Smart manufacturing"
        ],
        "abstract": "Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.633",
        "scimago_value": "2,310"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822132-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter10bigdatastewardshipanddatalakes",
        "booktitle": "Data Stewardship (Second Edition)",
        "doi": "10.1016/B978-0-12-822132-7.00010-3",
        "author": [
            "Plotkin, David"
        ],
        "keywords": [
            "Big data, data lake, unstructured data, zone"
        ],
        "abstract": "Big Data Governance and big data stewardship are not so different from what we\u2019ve been doing prior to the advent of big data and data lakes. Most of the same roles still need to be filled, and accountability for making data decisions is even more important because of the vast quantity of data, the many ways in which it can be changed, and increased consequences of \u201cgetting it wrong\u201d due to not only the large quantities of data and metadata, but also the speed at which the data can change.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822066-5",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter6bigdataanalyticsandprocesssafety",
        "booktitle": "Process Safety and Big Data",
        "doi": "10.1016/B978-0-12-822066-5.00001-7",
        "author": [
            "Valeev, Sagit",
            "Kondratyeva, Natalya"
        ],
        "keywords": [
            "Analytics, Machine learning, Prediction, Clustering, Classification, Regression, Time series, Text analysis, Image analysis"
        ],
        "abstract": "The chapter discusses the basics of big data analytics and the features of using analytical models in the field of process safety and risk management. The definition and basic principles of data analytics are necessary to understand the analytical techniques. The requirements for input data and the properties of analytical models are important for effective analytics. The concept, basic components, and varieties of machine learning are discussed. We consider such basic machine learning algorithms as clustering, classification, and regression. As advanced methods of data analytics, time series analysis methods, text analysis, and image analysis are proposed. Examples of the application of data analytics for risk management in the framework of process safety are considered.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13891286",
        "isbn": null,
        "journal": "Computer Networks",
        "publisher": null,
        "title": "ablockchainbasedtradingsystemforbigdata",
        "booktitle": null,
        "doi": "10.1016/j.comnet.2021.107994",
        "author": [
            "Hu, Donghui",
            "Li, Yifan",
            "Pan, Lixuan",
            "Li, Meng",
            "Zheng, Shuli"
        ],
        "keywords": [
            "Big data trading, Blockchain, Smart contract, Proxy re-encryption, Price negotiation, Value reward"
        ],
        "abstract": "Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users\u2019 application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.474",
        "scimago_value": "0,798"
    },
    {
        "issnkey": "26659727",
        "isbn": null,
        "journal": "Environmental and Sustainability Indicators",
        "publisher": null,
        "title": "harnessingartificialintelligenceandbigdataforsdgsandprosperousurbanfutureinsouthasia",
        "booktitle": null,
        "doi": "10.1016/j.indic.2021.100127",
        "author": [
            "Arfanuzzaman, Md."
        ],
        "keywords": [
            "Artificial intelligence, Big data, Climate resilience, Data infrastructure, South Asia, SDG, Technological readiness, Urban transformation"
        ],
        "abstract": "Artificial intelligence (AI) and big data solutions are currently being utilized to offer low cost and efficient solutions in solving pressing urban socio-economic and environmental problems globally. The study found big data and AI have the potentiality to solve the common urban problems in South Asia and upsurge the efficiency of urban industries, increase competitiveness and productivity of the human and natural resources, reduce the cost of urban service delivery, and build climate resilience. The study has assessed the current AI and big data initiatives and technologies in mitigating the urban development challenges and their potentiality for scaling up in South Asian cities. The study also examined the latest innovations in AI and big data solutions for SDG monitoring and implementation in South Asia and their implication for transformational change. The study suggested that South Asia can harness the maximum benefit of AI and big data technologies by building big data and associated IT infrastructure, advancing research and innovations with regional cooperation, enhancing technological readiness, and eliminating week enabling conditions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01403664",
        "isbn": null,
        "journal": "Computer Communications",
        "publisher": null,
        "title": "abigdatabasedarchitectureforcollaborativenetworkssupplychainsmixednetwork",
        "booktitle": null,
        "doi": "10.1016/j.comcom.2021.05.008",
        "author": [
            "Tamym, Lahcen",
            "Benyoucef, Lyes",
            "{Nait Sidi Moh}, Ahmed",
            "{El Ouadghiri}, Moulay"
        ],
        "keywords": [
            "Big data architecture, Collaborative networks, Enterprises network, Supply chain network, Flexibility, Robustness"
        ],
        "abstract": "Nowadays, the world knows a high-speed development and evolution of technologies, vulnerable economic environments, market changes, and personalised consumer trends. The issue and challenge related to enterprises networks design are more and more critical. These networks are often designed for short terms since their strategies must be competitive and better adapted to the environment, social and economical changes. As a solution, to design a flexible and robust network, it is necessary to deal with the trade-off between conflicting qualitative and quantitative criteria such as cost, quality, delivery time, and competition, etc. To this end, using Big Data (BD) as emerging technology will enhance the real performances of these kinds of networks. Moreover, even if the literature is rich with BD models and frameworks developed for a single supply chain network (SCN), there is a real need to scale and extend these BD models to networked supply chains (NSCs). To do so, this paper proposes a BD architecture to drive a mixed-network of SCs that collaborate in serial and parallel fashions. The collaboration is set up by sharing their resources, capabilities, competencies, and information to imitate a unique organisation. The objective is to increase internal value to their shareholders (where value is seen as wealth) and deliver better external value to the end-customer (where value represents customer satisfaction). Within a mixed-network of SCs, both values are formally calculated considering both serial and parallel networks configurations. Besides, some performance factors of the proposed BD architecture such as security, flexibility, robustness and resilience are discussed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.167",
        "scimago_value": "0,627"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820714-7",
        "journal": null,
        "publisher": "Gulf Professional Publishing",
        "title": "chapter9globalpracticeofaiandbigdatainoilandgasindustry",
        "booktitle": "Machine Learning and Data Science in the Oil and Gas Industry",
        "doi": "10.1016/B978-0-12-820714-7.00009-1",
        "author": [
            "Qing, Wu"
        ],
        "keywords": [
            "artificial intelligence, big data, digital core, digital rock physics, multiphase flow physics information, oil recovery, molecule advanced planning and scheduling, system (MAPS), crude oil selection, crude oil property prediction, process optimization, CCR unit, FCC unit, ethylene cracking unit, correlation analysis, the anomaly detection, parameters optimization analysis, prediction analysis, equipment preventive maintenance, distributed equipment health monitoring system, time series, early warning system for equipment fault monitoring, residual life prediction for equipment"
        ],
        "abstract": "This chapter introduces typical cases of artificial intelligence and big data application in oil and gas industry. In the upstream field, it introduces how to combine digital rock physics with big data and AI to optimize recovery efficiency. Digital core (also known as digital petrophysics\u2014DRP) technology enables more reliable physical information about pore-scale multiphase flows to determine the cause of low recovery and provides new ways for different injection solutions to improve reservoir performance. Combined with digital rock technology and AI, we can integrate the characteristics of digital rock databases into logging and well data, and use a variety of advanced classification techniques to identify the remaining oil and gas potential. Through the multi-phase flow simulation, the multi-scale model can predict the best injection method for maximum recovery under different conditions and propose possible solutions to optimize crude oil production. In the downstream field, the application of AI and big data analysis in planning and scheduling systems, process unit optimization, preventive maintenance of equipment, and other aspects is introduced. Among them, the molecular-level advanced planning and scheduling system (MAPS) can realize the cost performance measurement under different production schemes for potential types of processable crude oil, which is conducive to more accurate selection of crude oil and prediction of crude oil properties. In addition, the whole process simulation can be used to understand the product quality changes under different crude oil blending schemes and different unit operating conditions, which is conducive to timely adjusting the product blending schemes according to economic benefits or ex-factory demands. The operation conditions of secondary units and even the properties of mixed crude oil can be deduced according to different product quality requirements. In the process of optimization, the Continuous Catalytic Reforming (CCR) unit in the refinery process, for example, introduces the application of large data analysis, including correlation analysis, single index detection, multidimensional data anomaly detection, and the parameters of the single objective optimization, a multi-objective parameter optimization analysis, unstructured data analysis, and forecast analysis based on material properties. Good practices in CCR units have also been extended to other oil refining and chemical units, such as Fluid Catalytic Cracking (FCC) and ethylene cracking. In terms of equipment preventive maintenance, it introduced how to integrated application of Internet of things, deep machine learning, knowledge map and other technology to build real-time and on-line distributed equipment health monitoring and early warning system, for early detection of equipment hidden danger, early warning, early treatment of providing effective means, guarantee equipment run healthy and stable for a long period of time, to reduce unplanned downtime losses. In particular, the establishment of equipment prediction model based on time series and AI can realize effective monitoring and early warning of equipment faults such as shaft displacement, shaft fracture, shell cracking, power overload, and prediction of equipment remaining life.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "07475632",
        "isbn": null,
        "journal": "Computers in Human Behavior",
        "publisher": null,
        "title": "bigdatamanagementcapabilitiesinthehospitalitysectorserviceinnovationandcustomergeneratedonlinequalityratings",
        "booktitle": null,
        "doi": "10.1016/j.chb.2021.106777",
        "author": [
            "Shamim, Saqib",
            "Yang, Yumei",
            "Zia, Najam",
            "Shah, Mahmood"
        ],
        "keywords": [
            "Big data management, Dynamic capabilities, Service innovation, Knowledge creation, Customer generated online quality rating, Hospitality"
        ],
        "abstract": "Despite the wide usage of big data in tourism and the hospitality sector, little research has been done to understand the role of organizations\u2019 capability of managing big data in value creation. This study bridges this gap by investigating how big data management capabilities lead to service innovation and high online quality ratings. Instead of treating big data management as a whole, we access big data management capabilities at the strategic and operational level. Using a sample of 202 hotels in Pakistan, we collected the primary data for big data capabilities, knowledge creation and service innovation; the secondary data about quality rating were collected from Booking.com. Structural equation modelling through SmartPLS was used for data analysis. The results indicated that big data management capabilities lead to high online quality ratings through the mediation of knowledge creation and service innovation. We contribute to the current literature by empirically testing how strategic level big data capabilities enable the firm to add value in innovativeness and positive online quality ratings through acquiring, contextualizing, experimenting and applying big data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.829",
        "scimago_value": "2,108"
    },
    {
        "issnkey": "23521864",
        "isbn": null,
        "journal": "Environmental Technology & Innovation",
        "publisher": null,
        "title": "predictionmodelofecologicalenvironmentalwaterdemandbasedonbigdataanalysis",
        "booktitle": null,
        "doi": "10.1016/j.eti.2020.101196",
        "author": [
            "Zhao, Lihong"
        ],
        "keywords": [
            "Big data analysis, Ecological environment, Water demand, Prediction"
        ],
        "abstract": "The existing prediction model of eco-environmental water demand has the problem of large prediction error. In order to solve the above problems, the prediction model of eco-environmental water demand is constructed based on big data analysis. In order to reduce the prediction error of the ecological environment water demand prediction model, the framework of the ecological environment water demand prediction model is built. On this basis, the principal component analysis method is used to select the auxiliary variables of the model. Based on the selected auxiliary variables, the minimum monthly average flow method is used to analyze the basic water demand of the ecological environment, the leakage water demand and the water surface evaporation ecological environment water demand, so as to analyze based on the results, the water demand of ecological environment is predicted by big data analysis technology, and the prediction of water demand of ecological environment is realized. The experimental results show that compared with the existing ecological environment water demand prediction model, the prediction error of the model is within 19.3, which fully shows that the constructed ecological environment water demand prediction model has better prediction effect and can provide a certain reference value for the actual use of water resources.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,866"
    },
    {
        "issnkey": "22145796",
        "isbn": null,
        "journal": "Big Data Research",
        "publisher": null,
        "title": "enhancingprecisionmedicineabigdatadrivenapproachforthemanagementofgenomicdata",
        "booktitle": null,
        "doi": "10.1016/j.bdr.2021.100253",
        "author": [
            "Le\u00f3n, Ana",
            "Pastor, \u00d3scar"
        ],
        "keywords": [
            "Big Data, Genomics, Computer science, Theory and methods"
        ],
        "abstract": "The management of the exponential growth of data that Next Generation Sequencing techniques produce has become a challenge for researchers that are forced to delve into an ocean of complex data in order to extract new insights to unravel the secrets of human diseases. Initially, this can be faced as a Big Data-related problem, but the genomic data have particular and relevant challenges that make them different from other Big Data working domains. Genomic data are much more heterogeneous; they are spread in hundreds of repositories, represented in multiple formats, and have different levels of quality. In addition, getting meaningful conclusions from genomic data requires considering all of the relevant surrounding knowledge that is under continuous evolution. In this scenario, the precise identification of what makes Genome Data Management so different is essential in order to provide effective Big Data-based solutions. Genomic projects require dealing with the technological problems associated with data management, nomenclature standards, and quality issues that only robust Information Systems that use Big Data techniques can provide. The main contribution of this paper is to present a Big Data-driven approach for managing genomic data, that is adapted to the particularities of the domain and to show its applicability to improve genetic diagnoses, which is the core of the development of accurate Precision Medicine.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.578",
        "scimago_value": "0,565"
    },
    {
        "issnkey": "07437315",
        "isbn": null,
        "journal": "Journal of Parallel and Distributed Computing",
        "publisher": null,
        "title": "sparkdqefficientgenericbigdataqualitymanagementondistributeddataparallelcomputation",
        "booktitle": null,
        "doi": "10.1016/j.jpdc.2021.05.012",
        "author": [
            "Gu, Rong",
            "Qi, Yang",
            "Wu, Tongyu",
            "Wang, Zhaokang",
            "Xu, Xiaolong",
            "Yuan, Chunfeng",
            "Huang, Yihua"
        ],
        "keywords": [
            "Parallel data quality algorithms, Distributed system, Data quality management system, Multi-tasks scheduling, Big data"
        ],
        "abstract": "In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.734",
        "scimago_value": "0,638"
    },
    {
        "issnkey": "01989715",
        "isbn": null,
        "journal": "Computers, Environment and Urban Systems",
        "publisher": null,
        "title": "analyticsoflocationbasedbigdataforsmartcitiesopportunitieschallengesandfuturedirections",
        "booktitle": null,
        "doi": "10.1016/j.compenvurbsys.2021.101712",
        "author": [
            "Huang, Haosheng",
            "Yao, Xiaobai",
            "Krisp, Jukka",
            "Jiang, Bin"
        ],
        "keywords": [
            "Location-based big data (LocBigData), Smart cities, Data analytics, State-of-the-art review, Research agenda, Geodata"
        ],
        "abstract": "The growing ubiquity of location/activity sensing technologies and location-based services (LBS) has led to a large volume and variety of location-based big data (LocBigData), such as location tracking or sensing data, social media data, and crowdsourced geographic information. The increasing availability of such LocBigData has created unprecedented opportunities for research on urban systems and human environments in general. In this article, we first review the common types of LocBigData: mobile phone network data, GPS data, Location-based social media data, LBS usage/log data, smart card travel data, beacon log data (WiFi or Bluetooth), and camera imagery data. Secondly, we describe the opportunities fueled by LocBigData for the realization of smart cities, mainly via answering questions ranging from \u201cwhat happened\u201d and \u201cwhy did it happen\u201d to \u201cwhat's likely to happen in the future\u201d and \u201cwhat to do next\u201d. Thirdly, pitfalls of dealing with LocBigData are summarized, such as high volume/velocity/variety; non-random sampling; messy and not clean data; and correlations rather than causal relationships. Finally, we review the state-of-the-art research trends in this field, and conclude the article with a list of open research challenges and a research agenda for LocBigData research to help achieve the vision of smart and sustainable cities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.324",
        "scimago_value": "1,549"
    },
    {
        "issnkey": "0740624x",
        "isbn": null,
        "journal": "Government Information Quarterly",
        "publisher": null,
        "title": "adataqualityapproachtotheidentificationofdiscriminationriskinautomateddecisionmakingsystems",
        "booktitle": null,
        "doi": "10.1016/j.giq.2021.101619",
        "author": [
            "Vetr\u00f2, Antonio",
            "Torchiano, Marco",
            "Mecati, Mariachiara"
        ],
        "keywords": [
            "Automated decision making, Data ethics, Data quality, Data bias, Algorithm fairness, Digital policy, Digital governance"
        ],
        "abstract": "Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation \u2013 or even amplification \u2013 of bias in the input data of ADM systems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0169023x",
        "isbn": null,
        "journal": "Data & Knowledge Engineering",
        "publisher": null,
        "title": "conceptualmodelingintheeraofbigdataandartificialintelligenceresearchtopicsandintroductiontothespecialissue",
        "booktitle": null,
        "doi": "10.1016/j.datak.2021.101911",
        "author": [
            "Trujillo, Juan",
            "Davis, Karen",
            "Du, Xiaoyong",
            "Damiani, Ernesto",
            "Storey, Veda"
        ],
        "keywords": [
            "Conceptual modeling, Big Data, Machine learning, Artificial Intelligence"
        ],
        "abstract": "Since the first version of the Entity\u2013Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER\u201918) held in Xi\u2019an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02642751",
        "isbn": null,
        "journal": "Cities",
        "publisher": null,
        "title": "detectingthetrueurbanpolycentricpatternofchinesecitiesinmorphologicaldimensionsamultiscaleanalysisbasedongeospatialbigdata",
        "booktitle": null,
        "doi": "10.1016/j.cities.2021.103298",
        "author": [
            "Lv, Yongqiang",
            "Zhou, Lin",
            "Yao, Guobiao",
            "Zheng, Xinqi"
        ],
        "keywords": [
            "Polycentricity, Urban centers, Multi-scale, Street blocks, Geospatial big data, Chinese cities"
        ],
        "abstract": "With current decentralization trends and polycentric planning efforts, the urban spatial structures of Chinese cities have been changing tremendously. To detect the true urban polycentric pattern of Chinese cities, this article analyzed the urban polycentricity characteristics of 294 cities. The natural cities were delineated by points of interest (POIs), and road networks constituted street blocks. Based on check-in data and new spatial units, centers within both metropolitan areas and central cities were identified and examined. We discovered that all Chinese cities have at least one natural city in their metropolitan areas because of rapid urban sprawl. Although a monocentric structure is still the most common urban spatial structure, 110 Chinese cities displayed different degrees of polycentricity at the metropolitan level. Many natural cities beyond central cities contribute to polycentric development at the metropolitan level. Central cities have maintained their original vitality and importance, most Chinese cities have dispersed urban structures in central cities, and 45 central cities are polycentric. The spatial structures in metropolitan areas are more polycentric than those in central cities. The only 36 cities with polycentric urban structures at both the metropolitan and central city levels are all national or regional central cities in eastern China.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.835",
        "scimago_value": "1,771"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822226-3",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter2deeplearninginbigdataanddatamining",
        "booktitle": "Trends in Deep Learning Methodologies",
        "doi": "10.1016/B978-0-12-822226-3.00002-7",
        "author": [
            "Sharma, Deepak",
            "Tokas, Bhanu",
            "Adlakha, Leo"
        ],
        "keywords": [
            "Aspect extraction, Big data, CRISP-DM, Customer relations, Data mining, Data visualization, Deep learning, Distributed computing, LSTM, Machine learning"
        ],
        "abstract": "The growth of the digital age has led to a colossal leap in data generated by the average user. This growing data has several applications: businesses can use it to give a more personalized touch to their services, governments can use it to better allocate their funds, and companies can utilize it to select the best candidates for a job. While these applications may seem extremely enticing, there are a couple of problems that must be solved first, namely, data collection and extraction of useful patterns from the data. The disciplines of data mining and big data deal with these problems, respectively. But, as we have already discussed, the amount of data is so vast that any manual approach is extremely time intensive and costly. Thus this limits the potential outcomes from this data. This problem has been solved by the application of deep learning. Deep learning has allowed us to automate processes that were not only time intensive but also mentally arduous. It has achieved better than human accuracy in several types of discriminative and recognition tasks making it a viable alternative to inefficient human labor. Deep learning plays a vital role in this analysis and has enabled several businesses to comprehend customer needs and accordingly improve their own services, thus giving them the opportunity to outdo their competitors. Similarly, deep learning has also been instrumental in analyzing the trends and associations of securities in the financial market. It has even helped to create fraud detection and loan underwriting applications, which have contributed to making financial institutions more transparent and efficient. Apart from directly improving the efficiency in these fields, deep learning has also been instrumental in improving the fields of data mining and big data. Machine learning algorithms can actually utilize the existing data to predict the unknowns, including future trends in data. Due to its potential applications the field of machine learning is deeply interconnected with data mining. Nevertheless, machine learning algorithms are often heavily dependent on the availability of huge datasets to ensure useful accuracy. Deep learning algorithms have allowed the different components of data (i.e., multimedia data) in the data mining process itself to be identified. Similarly, semantic indexing and tagging algorithms have allowed the processes of big data to speed up. In this chapter, we will discuss the applications of deep learning in these fields and give a brief overview of the concepts involved.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22142126",
        "isbn": null,
        "journal": "Journal of Information Security and Applications",
        "publisher": null,
        "title": "guidelinesforgdprcomplianceinbigdatasystems",
        "booktitle": null,
        "doi": "10.1016/j.jisa.2021.102896",
        "author": [
            "Rhahla, Mouna",
            "Allegue, Sahar",
            "Abdellatif, Takoua"
        ],
        "keywords": [
            "The General Data Protection Regulation (GDPR), Big Data analytics, Privacy, Security"
        ],
        "abstract": "The implementation of the GDPR that aims at protecting European citizens\u2019 privacy is still a real challenge. In particular, in Big Data systems where data are voluminous and heterogeneous, it is hard to track data evolution through its complex life cycle ranging from collection, ingestion, storage and analytics. In this context, from 2016 to 2021 research has been conducted and several security tools designed. However, they are either specific to particular applications or address partially the regulation articles. To identify the covered parts, the missed ones and the necessary metrics for comparing different works, we propose a framework for GDPR compliance. The framework identifies the main components for the regulation implementation by mapping requirements aligned with GDPR\u2019s provisions to IT design requirements. Based on this framework, we compare the main GDPR solutions in the Big Data domain and we propose a guideline for GDPR verification and implementation in Big Data systems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.872",
        "scimago_value": "0,610"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "bigdataasatoolhelpfulincommunicationmanagement",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.09.293",
        "author": [
            "Smalec, Agnieszka"
        ],
        "keywords": [
            "Big Data, marketing communication, management, data processing, collection, communication management"
        ],
        "abstract": "The boundaries between the online and offline worlds have become irretrievably blurred, especially as mobile devices have proliferated. As a result, more and more activities are transferred to the Internet. Every activity in the network leaves a trace, which is why the volume of available data is growing rapidly. The amount of increasing information affects all market participants, and the necessity to constantly collect and process large amounts of data becomes an everyday reality. The aim of the article is to present the concept of big data and to indicate examples of the use of big data to manage marketing communication with the environment. It should be emphasized that not only data transfer devices, but also human interaction contribute to the creation of very large data sets. Acquiring and correctly interpreting them plays an important role in market entities in terms of management, including communication management. Contemporary multi-directional communication, including communication in a hypermedia environment, creates new challenges and threats. The article was prepared based on a literature review, research reports and an analysis of secondary sources. It also outlines the practical implications. The considerations provided are the basis for further activities and empirical research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "14629011",
        "isbn": null,
        "journal": "Environmental Science & Policy",
        "publisher": null,
        "title": "aiforsocialgoodaiandbigdataapproachesforenvironmentaldecisionmaking",
        "booktitle": null,
        "doi": "10.1016/j.envsci.2021.09.001",
        "author": [
            "Li, Victor",
            "Lam, Jacqueline",
            "Cui, Jiahuan"
        ],
        "keywords": [
            ""
        ],
        "abstract": "AI and big data technologies have been increasingly deployed to process complex, heterogeneous, high-resolution environmental data, and generate results at greater speeds and higher accuracies to facilitate environmental decision-making. However, current attempts to develop reliable AI and big data technologies for environmental decision-making are still inadequate. In this special issue, AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making, we attempt to address the following important questions: What are the conditions for AI and big data technologies to facilitate environmental decision-making? How can AI and big data be used to facilitate environmental decision-making? Do AI and big data serve those most at risk of environmental pollution? Who should own and govern AI and big data? This special issue brings together researchers in relevant fields of AI and environmental science to address these pertinent questions. First, we will review the existing works which attempt to address these four questions. Second, we summarize the significance and novelty of six articles included in our special issue in addressing these four questions. Finally, we highlight the important principles of AI for Social Good, which can help distinguish good from bad environmental decisions based on AI and big data technologies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22147853",
        "isbn": null,
        "journal": "Materials Today: Proceedings",
        "publisher": null,
        "title": "anefficientapproachformanufacturingprocessusingbigdataanalytics",
        "booktitle": null,
        "doi": "10.1016/j.matpr.2021.05.146",
        "author": [
            "Mishra, Devendra",
            "Upadhyay, Arvind",
            "Sharma, Sanjiv"
        ],
        "keywords": [
            "Manufacturing process, Bigdata, Structured data, Unstructured data"
        ],
        "abstract": "Manufacturing is a technique that produce finished goods after taking supplies, raw materials and ingredients. Manufacturing process is frequently used to produce food, chemicals and other things those have very important place in human\u2019s life. This manufacturing process involve data for analysis and management of the process, but in current scenario the data that are generated by the process is increasing day by day. This huge amount of data in known as big data. Big data is difficult to handle by traditional data management tools. Data that are generated by the manufacturing process collected by the logs records and may be as structured or unstructured. Generally analysis is performed by structured data. Unstructured data also provide good insights in the manufacturing process if analysed in proper manner. This paper involved an efficient approach of big data analysis for manufacturing process.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,341"
    },
    {
        "issnkey": "26665468",
        "isbn": null,
        "journal": "Energy and AI",
        "publisher": null,
        "title": "transformationsoftrustinsocietyasystematicreviewofhowaccesstobigdatainenergysystemschallengesscandinavianculture",
        "booktitle": null,
        "doi": "10.1016/j.egyai.2021.100079",
        "author": [
            "Godoy, Jaqueline",
            "Otrel-Cass, Kathrin",
            "Toft, Kristian"
        ],
        "keywords": [
            "Surveillance capitalism, Smart meters, Energy transition, Trust, Data Ethics, Big Data"
        ],
        "abstract": "In the era of information technology and big data, the extraction, commodification, and control of personal information is redefining how people relate and interact. However, the challenges that big data collection and analytics can introduce in trust-based societies, like those of Scandinavia, are not yet understood. For instance, in the energy sector, data generated through smart appliances, like smart metering devices, can have collateral implications for the end-users. In this paper, we present a systematic review of scientific articles indexed in Scopus to identify possible relationships between the practices of collecting, processing, analysing, and using people's data and people's responses to such practices. We contextualise this by looking at research about Scandinavian societies and link this to the academic literature on big data and trust, big data and smart meters, data ethics and the energy sector, surveillance capitalism, and subsequently performing a reflexive thematic analysis. We broadly situate our understanding of culture in this context on the interactions between cognitive norms, material culture, and energy practices. Our analysis identified a number of articles discussing problems and solutions to do with the practices of surveillance capitalism. We also found that research addresses these challenges in different ways. While some research focuses on technological amendments to address users\u2019 privacy protection, only few examine the fundamental ethical questions that discuss how big data practices may change societies and increase their vulnerability. The literature suggests that even in highly trusting societies, like the ones found in Scandinavian countries, trust can be undermined and weakened.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-817962-8",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter3machinelearningandbigdatainpediatriclaboratorymedicine",
        "booktitle": "Biochemical and Molecular Basis of Pediatric Disease (Fifth Edition)",
        "doi": "10.1016/B978-0-12-817962-8.00018-4",
        "author": [
            "Haymond, Shannon",
            "Julian, Randall",
            "Gill, Emily",
            "Master, Stephen"
        ],
        "keywords": [
            "Artificial intelligence, Big data, Laboratory medicine, Machine learning, Pediatrics, Regulation"
        ],
        "abstract": "Clinical laboratories generate a large number of test results, creating opportunities for improved data management and the use of analytics. Aggregate analyses of these data have potential diagnostic value but require labs to utilize computational tools for the analysis of high-dimensional data. Machine learning can be used to aid decision-making, whether for clinical or operational purposes, using a variety of algorithms to analyze complex data sets and make reliable predictions. This chapter discusses key concepts related to big data and its application to pediatric laboratory medicine. Machine learning workflows, concepts, common algorithms, and related infrastructure requirements are also covered.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00104825",
        "isbn": null,
        "journal": "Computers in Biology and Medicine",
        "publisher": null,
        "title": "clinicalnotesasprognosticmarkersofmortalityassociatedwithdiabetesmellitusfollowingcriticalcarearetrospectivecohortanalysisusingmachinelearningandunstructuredbigdata",
        "booktitle": null,
        "doi": "10.1016/j.compbiomed.2021.104305",
        "author": [
            "{De Silva}, Kushan",
            "Mathews, Noel",
            "Teede, Helena",
            "Forbes, Andrew",
            "J\u00f6nsson, Daniel",
            "Demmer, Ryan",
            "Enticott, Joanne"
        ],
        "keywords": [
            "Critical care, Diabetes, Electronic health records, LASSO, Machine learning, Mortality, Natural language processing, Prognosis, Text mining"
        ],
        "abstract": "Background Clinical notes are ubiquitous resources offering potential value in optimizing critical care via data mining technologies. Objective To determine the predictive value of clinical notes as prognostic markers of 1-year all-cause mortality among people with diabetes following critical care. Materials and methods Mortality of diabetes patients were predicted using three cohorts of clinical text in a critical care database, written by physicians (n = 45253), nurses (159027), and both (n = 204280). Natural language processing was used to pre-process text documents and LASSO-regularized logistic regression models were trained and tested. Confusion matrix metrics of each model were calculated and AUROC estimates between models were compared. All predictive words and corresponding coefficients were extracted. Outcome probability associated with each text document was estimated. Results Models built on clinical text of physicians, nurses, and the combined cohort predicted mortality with AUROC of 0.996, 0.893, and 0.922, respectively. Predictive performance of the models significantly differed from one another whereas inter-rater reliability ranged from substantial to almost perfect across them. Number of predictive words with non-zero coefficients were 3994, 8159, and 10579, respectively, in the models of physicians, nurses, and the combined cohort. Physicians\u2019 and nursing notes, both individually and when combined, strongly predicted 1-year all-cause mortality among people with diabetes following critical care. Conclusion Clinical notes of physicians and nurses are strong and novel prognostic markers of diabetes-associated mortality in critical care, offering potentially generalizable and scalable applications. Clinical text-derived personalized risk estimates of prognostic outcomes such as mortality could be used to optimize patient care.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.589",
        "scimago_value": "0,884"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-819200-9",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter6psychologicaltargetingintheageofbigdata",
        "booktitle": "Measuring and Modeling Persons and Situations",
        "doi": "10.1016/B978-0-12-819200-9.00015-6",
        "author": [
            "Appel, Ruth",
            "Matz, Sandra"
        ],
        "keywords": [
            "Psychological targeting, Psychological profiling, Psychologically-informed interventions, Big Data, Digital footprints, Methods, Ethics, Privacy, Contextual integrity"
        ],
        "abstract": "Advances in the collection, storage, and processing of large amounts of user data have given rise to psychological targeting, which we define as the process of extracting individuals\u2019 psychological characteristics from their digital footprints in order to target them with psychologically-informed interventions at scale. In this chapter, we introduce a two-stage framework of psychological targeting consisting of (1) psychological profiling and (2) psychologically-informed interventions. We summarize the most important research findings in relation to the two stages and discuss important methodological opportunities and pitfalls. To help researchers make the most of the opportunities, we also provide practical advice on how to deal with some of the potential pitfalls. Finally, we highlight ethical opportunities and challenges and offer some suggestions for addressing these challenges. If done right, psychological targeting has the potential to advance our scientific understanding of human nature and to enhance the well-being of individuals and society at large.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22145796",
        "isbn": null,
        "journal": "Big Data Research",
        "publisher": null,
        "title": "abigdataanalyticsarchitectureforsmartcitiesandsmartcompanies",
        "booktitle": null,
        "doi": "10.1016/j.bdr.2021.100192",
        "author": [
            "Fugini, Mariagrazia",
            "Finocchi, Jacopo",
            "Locatelli, Paolo"
        ],
        "keywords": [
            "Big Data platforms, Unstructured data, Text analytics, Machine learning, Virtual enterprises, Smart communities"
        ],
        "abstract": "This paper presents the approach to Big Data Analytics (BDA) developed in the SIBDA (Sistema Innovativo Big Data Analytics) Project. The project aim is to study and develop innovative solutions in the field of BDA for three companies cooperating in a temporary association of enterprises. We discuss elements of Big Data tackled in the project, namely document processing, mass e-mail applications and Internet of Things sensor networks, to be integrated into a shared platform of common assets and services for the three cooperating companies. We comment about the \u201cBig Data Journey\u201d status in Italy reported by Osservatorio Politecnico di Milano. Then, the paper presents the SIBDA project approach and requirements, outlines the adopted architecture and provides implementation hints, along with some experiments and considerations on the use of the proposed architecture for Smart Cities and Smart Enterprises and Communities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.578",
        "scimago_value": "0,565"
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "bigdatadriveninternetofthingsforcreditevaluationandearlywarninginfinance",
        "booktitle": null,
        "doi": "10.1016/j.future.2021.06.003",
        "author": [
            "Wen, Chunhui",
            "Yang, Jinhai",
            "Gan, Liu",
            "Pan, Yang"
        ],
        "keywords": [
            "Internet of Things finance, Credit evaluation and early warning, Factor analysis, Particle swarm optimization, Big data driven"
        ],
        "abstract": "The big data technology framework has been successfully used in the Internet of Things, and the financial industry also hopes to use the advanced technology of big data to integrate and improve internal and external data related to credit risks. Relying on more efficient machine learning algorithms to get a reasonable prediction of credit risk can reduce the self-generated losses of the Internet of Things finance and increase profits. This article uses distributed search engine technology to customize web crawlers to obtain the required bank card and transaction data from the multi-source heterogeneous data of the Internet of Things financial industry, design the corresponding Spark parallel algorithm to preprocess the data, and establish an inverted table and two Level index file provides data source for big data analysis platform. After the data source is determined, the Mutually Exclusive Collectively Exhaustive (MECE) analysis method is combined with the scores of many financial business experts in the industry to obtain a set of candidate indicators and quantification methods for the financial credit risk evaluation of the Internet of Things, and analyze the correlation of indicators and risk grading. The random forest algorithm in the big data machine learning library is used to select the feature of the candidate index set, and a multi-level spatial association rule algorithm based on the Hash structure is designed to mine the financial risk information of the Internet of Things, and build a credit risk assessment and intelligent early warning model. This paper selects 26 indicators of Internet of Things finance as the research objects, uses SPSS26.0 software to perform sample Kaiser\u2013Meyer\u2013Olkin (KMO) test and Bartlett sphere test on the original data, and describes the results of factor analysis in detail. The particle swarm algorithm is introduced into the parameter optimization of random forest, and the financial credit risk assessment model of the Internet of Things is established. The results show that this method can significantly reduce the probability of banks making the first and second error rates when evaluating the credit risk of financing the Internet of Things Finance. This is conducive to the smooth development of the Internet of Things financial business for banks, which enables banks to enhance their own profitability while effectively reducing losses due to incorrect credit provision.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "14439506",
        "isbn": null,
        "journal": "Heart, Lung and Circulation",
        "publisher": null,
        "title": "aversatilebigdatahealthsystemforaustraliadrivingimprovementsincardiovascularhealth",
        "booktitle": null,
        "doi": "10.1016/j.hlc.2021.04.023",
        "author": [
            "Paige, Ellie",
            "Doyle, Kerry",
            "Jorm, Louisa",
            "Banks, Emily",
            "Hsu, Meng-Ping",
            "Nedkoff, Lee",
            "Briffa, Tom",
            "Cadilhac, Dominique",
            "Mahoney, Ray",
            "Verjans, Johan",
            "Dwivedi, Girish",
            "Inouye, Michael",
            "Figtree, Gemma"
        ],
        "keywords": [
            "Big data, Datasets, Cardiovascular disease, National platform"
        ],
        "abstract": "Cardiovascular diseases (CVD) are leading causes of death and morbidity in Australia and worldwide. Despite improvements in treatment, there remain large gaps in our understanding to prevent, treat and manage CVD events and associated morbidities. This article lays out a vision for enhancing CVD research in Australia through the development of a Big Data system, bringing together the multitude of rich administrative and health datasets available. The article describes the different types of Big Data available for CVD research in Australia and presents an overview of the potential benefits of a Big Data system for CVD research and some of the major challenges in establishing the system for Australia. The steps for progressing this vision are outlined.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "24058963",
        "isbn": null,
        "journal": "IFAC-PapersOnLine",
        "publisher": null,
        "title": "frombigdatatosmartdataapplicationtoperformancemanagement",
        "booktitle": null,
        "doi": "10.1016/j.ifacol.2021.08.100",
        "author": [
            "Souifi, Amel",
            "Boulanger, Zohra",
            "Zolghadri, Marc",
            "Barkallah, Maher",
            "Haddar, Mohamed"
        ],
        "keywords": [
            "Big Data, Smart Data, Performance Management"
        ],
        "abstract": "In the context of digitalization, some companies are considering a transition to Industry 4.0 to ensure greater flexibility, productivity and responsiveness. The implementation of a relevant performance management system is then a real necessity to measure the degree of achievement of these objectives. In the era of Industry 4.0, the potential access to large amounts of data, i.e. Big Data, poses new challenges to the design and implementation of these systems. With the exponential growth of data generated from different sources, there is a need for extensive exploitation of data for performance management. Given the large volume of data, the speed at which it is generated and the variety of data sources, the manufacturing sector is facing with the challenge of creating value from large data sets. This paper introduces some potential benefits of Big Data for business and in particular its role in performance management systems. However, the key idea is that Big Data are not always neither available nor necessary. Authors focus on the concept of smart data, the result of the transformation of Big Data, and define a set of necessary and sufficient conditions the data should satisfy to be considered as Smart. The paper presents some methods of smart data extraction. Such smart data will be used to feed the performance management system in order to obtain more accurate, timely and representative key performance indicators.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,308"
    },
    {
        "issnkey": "25426605",
        "isbn": null,
        "journal": "Internet of Things",
        "publisher": null,
        "title": "anextendedmetalearningapproachforautomatingmodelselectioninbigdataenvironmentsusingmicroserviceandcontainervirtualizationztechnologies",
        "booktitle": null,
        "doi": "10.1016/j.iot.2021.100432",
        "author": [
            "Shahoud, Shadi",
            "Winter, Moritz",
            "Khalloof, Hatem",
            "Duepmeier, Clemens",
            "Hagenmeyer, Veit"
        ],
        "keywords": [
            "Meta learning, Machine learning, Microservice, Web-based applications, Big data"
        ],
        "abstract": "For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new concept for enhancing the predictive performance of meta learning classification models by generating new meta examples is introduced. Our concept is realized and evaluated in a microservice-based meta learning framework. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture. In this demonstration and for evaluation purpose, time series model selection is taken as a use case for applying meta learning. It is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, our new concept for generating new meta examples enhances the predictive performance of the meta learner up to 16.77% and 27.07% in the case of using the original and encoded representation forms of meta features respectively. The recommendation of the most appropriate forecasting model results in a well acceptable low framework overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in the context of Big Data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22147853",
        "isbn": null,
        "journal": "Materials Today: Proceedings",
        "publisher": null,
        "title": "aframeworkbasedonbwmforbigdataanalyticsbdabarriersinmanufacturingsupplychains",
        "booktitle": null,
        "doi": "10.1016/j.matpr.2021.03.374",
        "author": [
            "Sharma, Vikrant",
            "Kumar, Atul",
            "Kumar, Mukesh"
        ],
        "keywords": [
            "Big data analytics, Barriers, Manufacturing supply chains, Best worst method (BWM)"
        ],
        "abstract": "Due to its potential utility, Big Data (BD) recently attracted researchers and practitioners in decision-making. Big Data analytics (BDA) becomes more common among manufacturing companies because it lets them gain insight and make decisions based on BD. Given the importance of both BD and BDA, this study aims to identify and analyse essential BDA adoption barriers in supply chains. This study explores the current knowledge base using a BWM (Best Worst Method) to discuss these barriers. Data were obtained from five Indian manufacturing companies. Research findings show that data-related barriers are most significant. The findings will help managers understand the exact nature of the challenges and possible advantages of the BDA and implement BDA policies for the growth and output of supply chain operations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,341"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822060-3",
        "journal": null,
        "publisher": "Academic Press",
        "title": "prefaceartificialintelligenceandbigdataanalyticsforsmarthealthcareadigitaltransformationofhealthcareprimer",
        "booktitle": "Artificial Intelligence and Big Data Analytics for Smart Healthcare",
        "doi": "10.1016/B978-0-12-822060-3.00018-8",
        "author": [
            "Lytras, Miltiadis",
            "Visvizi, Anna",
            "Sarirete, Akila",
            "Chui, Kwok"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "experimentingwithbigdatacomputingforscalingdataqualityawarequeryprocessing",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2021.114858",
        "author": [
            "Cisneros-Cabrera, Sonia",
            "Michailidou, Anna-Valentini",
            "Sampaio, Sandra",
            "Sampaio, Pedro",
            "Gounaris, Anastasios"
        ],
        "keywords": [
            "Data quality-aware queries, Big data computing, Empirical evaluation"
        ],
        "abstract": "Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "26663899",
        "isbn": null,
        "journal": "Patterns",
        "publisher": null,
        "title": "addressingbiasinbigdataandaiforhealthcareacallforopenscience",
        "booktitle": null,
        "doi": "10.1016/j.patter.2021.100347",
        "author": [
            "Norori, Natalia",
            "Hu, Qiyang",
            "Aellen, Florence",
            "Faraci, Francesca",
            "Tzovara, Athina"
        ],
        "keywords": [
            "artificial intelligence, deep learning, health care, bias, open science, participatory science, data standards"
        ],
        "abstract": "Summary Artificial intelligence (AI) has an astonishing potential in assisting clinical decision making and revolutionizing the field of health care. A major open challenge that AI will need to address before its integration in the clinical routine is that of algorithmic bias. Most AI algorithms need big datasets to learn from, but several groups of the human population have a long history of being absent or misrepresented in existing biomedical datasets. If the training data is misrepresentative of the population variability, AI is prone to reinforcing bias, which can lead to fatal outcomes, misdiagnoses, and lack of generalization. Here, we describe the challenges in rendering AI algorithms fairer, and we propose concrete steps for addressing bias using tools from the field of open science.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822060-3",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter10spatiotemporalbigdatadrivenvesseltrafficriskestimationforpromotingmaritimehealthcarelessonslearntfromanotherdomainthanhealthcare",
        "booktitle": "Artificial Intelligence and Big Data Analytics for Smart Healthcare",
        "doi": "10.1016/B978-0-12-822060-3.00006-1",
        "author": [
            "Feng, Zikun",
            "Li, Yan",
            "Liu, Zhao",
            "Liu, Ryan"
        ],
        "keywords": [
            "Data mining, maritime healthcare, maritime risk estimation, automatic identification system (AIS), maritime safety, artificial intelligence"
        ],
        "abstract": "With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and global maritime economy. Therefore it is of vital significance to study the risk of ship collision using big data mining techniques. The big data\u2013driven computational results are beneficial for guaranteeing smart maritime healthcare in the fields of ocean engineering and maritime management. This chapter proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is first improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are then computed using the Monte Carlo simulation strategy. For the sake of better understanding, the kernel density estimation method is finally adopted to visually generate the ship collision risk in maps. Experimental results on realistic spatiotemporal big data have illustrated the effectiveness of the proposed method in crowded inland waterways.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822884-5",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter2bigdatainpersonalizedhealthcare",
        "booktitle": "Big Data in Psychiatry #x0026; Neurology",
        "doi": "10.1016/B978-0-12-822884-5.00017-9",
        "author": [
            "Wang, Lidong",
            "Alexander, Cheryl"
        ],
        "keywords": [
            "Big data, Precision healthcare, Personalized healthcare, Big Data analytics, Telepsychiatry, Deep learning"
        ],
        "abstract": "Big data technologies enable correlation of multiple data sources into a coherent view. Big data and Big Data analytics have been used in public health, electronic consultation (e-consultation), real-time telediagnosis, precision healthcare, and personalized healthcare. e-Consultation is one aspect of telemedicine related to remote communication between medical specialists and clinicians, or clinicians and patients. It is generally implemented via the Internet or mobile communication devices (e.g., smartphone) and often generates big data. The concepts, characteristics, methods, emerging technologies, and software platforms or tools of big data and Big Data analytics are introduced in this chapter. Big data and applications in general healthcare are presented. Specifically, big data in precision healthcare and personalized healthcare are introduced. Challenges of big data and Big Data analytics in personalized healthcare are also outlined.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15698432",
        "isbn": null,
        "journal": "International Journal of Applied Earth Observation and Geoinformation",
        "publisher": null,
        "title": "integratingremotesensingandgeospatialbigdataforurbanlandusemappingareview",
        "booktitle": null,
        "doi": "10.1016/j.jag.2021.102514",
        "author": [
            "Yin, Jiadi",
            "Dong, Jinwei",
            "Hamm, Nicholas",
            "Li, Zhichao",
            "Wang, Jianghao",
            "Xing, Hanfa",
            "Fu, Ping"
        ],
        "keywords": [
            "Integration methods, Urban functional zone classification, Urban management, Land use"
        ],
        "abstract": "Remote Sensing (RS) has been used in urban mapping for a long time; however, the complexity and diversity of urban functional patterns are difficult to be captured by RS only. Emerging Geospatial Big Data (GBD) are considered as the supplement to RS data, and help to contribute to our understanding of urban lands from physical aspects (i.e., urban land cover) to socioeconomic aspects (i.e., urban land use). Integrating RS and GBD could be an effective way to combine physical and socioeconomic aspects with great potential for high-quality urban land use classification. In this study, we reviewed the existing literature and focused on the state-of-the-art and perspective of the urban land use categorization by integrating RS and GBD. Specifically, the commonly used RS features (e.g., spectral, textural, temporal, and spatial features) and GBD features (e.g., spatial, temporal, semantic, and sequence features) were identified and analyzed in urban land use classification. The integration strategies for RS and GBD features were categorized into feature-level integration (FI) and decision-level integration (DI). To be more specific, the FI method integrates the RS and GBD features and classifies urban land use types using the integrated feature sets; the DI method processes RS and GBD independently and then merges the classification results based on decision rules. We also discussed other critical issues, including analysis unit setting, parcel segmentation, parcel labeling of land use types, and data integration. Our findings provide a retrospect of different features from RS and GBD, strategies of RS and GBD integration, and their pros and cons, which could help to define the framework for future urban land use mapping and better support urban planning, urban environment assessment, urban disaster monitoring and urban traffic analysis.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.933",
        "scimago_value": "1,623"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "roleofinstitutionalpressuresandresourcesintheadoptionofbigdataanalyticspoweredartificialintelligencesustainablemanufacturingpracticesandcirculareconomycapabilities",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2020.120420",
        "author": [
            "Bag, Surajit",
            "Pretorius, Jan",
            "Gupta, Shivam",
            "Dwivedi, Yogesh"
        ],
        "keywords": [
            "Big data, Artificial intelligence, Industry 4.0, Circular economy, Sustainable manufacturing"
        ],
        "abstract": "ABSTRACT The significance of big data analytics-powered artificial intelligence has grown in recent years. The literature indicates that big data analytics-powered artificial intelligence has the ability to enhance supply chain performance, but there is limited research concerning the reasons for which firms engaging in manufacturing activities adopt big data analytics-powered artificial intelligence. To address this gap, our study employs institutional theory and resource-based view theory to elucidate the way in which automotive firms configure tangible resources and workforce skills to drive technological enablement and improve sustainable manufacturing practices and furthermore develop circular economy capabilities. We tested the research hypothesis using primary data collected from 219 automotive and allied manufacturing companies operating in South Africa. The contribution of this work lies in the statistical validation of the theoretical framework, which provides insight regarding the role of institutional pressures on resources and their effects on the adoption of big data analytics-powered artificial intelligence, and how this affects sustainable manufacturing and circular economy capabilities under the moderating effects of organizational flexibility and industry dynamism.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "01674048",
        "isbn": null,
        "journal": "Computers & Security",
        "publisher": null,
        "title": "marismabidapatternintegratedriskanalysisforbigdata",
        "booktitle": null,
        "doi": "10.1016/j.cose.2020.102155",
        "author": [
            "Rosado, David",
            "Moreno, Julio",
            "S\u00e1nchez, Luis",
            "Santos-Olmo, Antonio",
            "Serrano, Manuel",
            "Fern\u00e1ndez-Medina, Eduardo"
        ],
        "keywords": [
            "Big data, Risk assessment, Risk analysis, Information security, Security standards"
        ],
        "abstract": "Data is one of the most important assets for all types of companies, which have undoubtedly grown their quantity and the ways of exploiting them. Big Data appears in this context as a set of technologies that manage data to obtain information that supports decision-making. These systems were not conceived to be secure, resulting in significant risks that must be controlled. Security risks in Big Data must be analyzed and managed in an appropriate manner to protect the system and secure the information and the data being handled. This paper proposes a risk analysis approach for Big Data environments, which is based on a security analysis methodology called MARISMA (Methodology for the Analysis of Risks on Information System), supported by a technological environment in the cloud (eMARISMA tool) already used by numerous clients. Both MARISMA and eMARISMA are specifically designed to be easily adapted to particular contexts, such as Big Data. Our proposal, called MARISMA-BiDa, is based on the main related standards, such as ISO/IEC 27,000 and 31,000, or the NIST Big Data reference architecture or ENISA and CSA recommendations for Big Data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,861"
    },
    {
        "issnkey": "26671026",
        "isbn": null,
        "journal": "Intelligent Medicine",
        "publisher": null,
        "title": "thecriticalneedtoestablishstandardsfordataqualityinintelligentmedicine",
        "booktitle": null,
        "doi": "10.1016/j.imed.2021.04.004",
        "author": [
            "Li, Ruiyang",
            "Yang, Yahan",
            "Lin, Haotian"
        ],
        "keywords": [
            "Artificial intelligence, Intelligent medicine, Big data, Data collection, Data storage, Data management"
        ],
        "abstract": "Medical artificial intelligence (AI) is an important technical asset to support medical supply-side reforms and national development in the big data era. Clinical data from multiple disciplines represent building blocks for the development and application of AI-aided diagnostic and treatment systems based on medical big data. However, the inconsistent quality of these data resources in AI research leads to waste and inefficiencies. Therefore, it is crucial that the field formulates the requirements and content related to data processing as part of the development of intelligent medicine. To promote medical AI research worldwide, the \u201cBelt and Road\u201d International Ophthalmic Artificial Intelligence Research and Development Alliance will establish a series of expert recommendations for data quality in intelligent medicine.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26669900",
        "isbn": null,
        "journal": "Computer Methods and Programs in Biomedicine Update",
        "publisher": null,
        "title": "dataqualityawaregenomicdataintegration",
        "booktitle": null,
        "doi": "10.1016/j.cmpbup.2021.100009",
        "author": [
            "Bernasconi, Anna"
        ],
        "keywords": [
            "Data quality, Data integration, Data curation, Genomic datasets, Metadata, Interoperability"
        ],
        "abstract": "Genomic data are growing at unprecedented pace, along with new protocols, update polices, formats and guidelines, terminologies and ontologies, which are made available every day by data providers. In this continuously evolving universe, enforcing quality on data and metadata is increasingly critical. While many aspects of data quality are addressed at each individual source, we focus on the need for a systematic approach when data from several sources are integrated, as such integration is an essential aspect for modern genomic data analysis. Data quality must be assessed from many perspectives, including accessibility, currency, representational consistency, specificity, and reliability. In this article we review relevant literature and, based on the analysis of many datasets and platforms, we report on methods used for guaranteeing data quality while integrating heterogeneous data sources. We explore several real-world cases that are exemplary of more general underlying data quality problems and we illustrate how they can be resolved with a structured method, sensibly applicable also to other biomedical domains. The overviewed methods are implemented in a large framework for the integration of processed genomic data, which is made available to the research community for supporting tertiary data analysis over Next Generation Sequencing datasets, continuously loaded from many open data sources, bringing considerable added value to biological knowledge discovery.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09505849",
        "isbn": null,
        "journal": "Information and Software Technology",
        "publisher": null,
        "title": "bigdataanalyticsinagilesoftwaredevelopmentasystematicmappingstudy",
        "booktitle": null,
        "doi": "10.1016/j.infsof.2020.106448",
        "author": [
            "Biesialska, Katarzyna",
            "Franch, Xavier",
            "Munt\u00e9s-Mulero, Victor"
        ],
        "keywords": [
            "Agile software development, Software analytics, Data analytics, Machine learning, Artificial intelligence, Literature review"
        ],
        "abstract": "Context: Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity. Objective: Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA). Method: As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019. Results: In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics. Conclusions: As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.730",
        "scimago_value": "0,606"
    },
    {
        "issnkey": "22145796",
        "isbn": null,
        "journal": "Big Data Research",
        "publisher": null,
        "title": "riskpredictionofrenalfailureforchronicdiseasepopulationbasedonelectronichealthrecordbigdata",
        "booktitle": null,
        "doi": "10.1016/j.bdr.2021.100234",
        "author": [
            "Yang, Yujie",
            "Li, Ye",
            "Chen, Runge",
            "Zheng, Jing",
            "Cai, Yunpeng",
            "Fortino, Giancarlo"
        ],
        "keywords": [
            "Renal failure, Risk prediction, Electronic health record, Health big data, Machine learning"
        ],
        "abstract": "Renal failure is a fatal disease raising global concerns. Previous risk models for renal failure mostly rely on the diagnosis of chronic kidney disease, which lacks obvious clinical symptoms and thus is mostly undiagnosed, causing significant omission of high-risk patients. In this paper, we proposed a framework to predict the risk of renal failure directly from a big data repository of chronic disease population without prerequisite diagnosis of chronic kidney disease. The electronic health records of 42,256 patients with hypertension or diabetes in Shenzhen Health Information Big Data Platform were collected, with 398 suffered from renal failure during a 3-year follow-up. Five state-of-the-art machine learning methods are utilized to build risk prediction models of renal failure for chronic disease population. Extensive experimental results show that the proposed framework achieves quite well performance. Particularly, the XGBoost obtains the best performance with an area under receiving-operating-characteristics curve (AUC) of 0.9139. By analyzing the effect of risk factors, we identified that serum creatine, age, urine acid, systolic blood pressure, and blood urea nitrogen are the top five factors associated with renal failure risk. Compared with existing models, our model can be deployed into routine chronic disease management procedures and enable more preemptive, widely-covered screening of renal risks, which would in turn reduce the damage caused by the disease through timely intervention.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.578",
        "scimago_value": "0,565"
    },
    {
        "issnkey": "03608352",
        "isbn": null,
        "journal": "Computers & Industrial Engineering",
        "publisher": null,
        "title": "abigdatadrivenrootcauseanalysissystemapplicationofmachinelearninginqualityproblemsolving",
        "booktitle": null,
        "doi": "10.1016/j.cie.2021.107580",
        "author": [
            "Ma, Qiuping",
            "Li, Hongyan",
            "Thorstenson, Anders"
        ],
        "keywords": [
            "Quality management, Data mining, Machine Learning, Multi-class classification, Neural Network"
        ],
        "abstract": "Root cause analysis for quality problem solving is critical to improve product quality performance and reduce the quality risk for manufacturers. Subjective conventional methods have been applied frequently in past decades. However, due to increasingly complex product and supply chain structures, diverse working conditions, and massive amounts of components, accuracy and efficiency of root cause analysis are progressively challenged in practice. Therefore, data-driven root cause analysis methods have attracted attention lately. In this paper, taking advantage of the availability of big operations data and the rapid development of data science, we design a big data-driven root cause analysis system utilizing Machine Learning techniques to improve the performance of root cause analysis. More specifically, we first propose a conceptual framework of the big data-driven root cause analysis system including three modules of Problem Identification, Root Cause Identification, and Permanent Corrective Action. Furthermore, in the Problem Identification Module, we construct a unified feature-based approach to describe multiple and different types of quality problems by applying a data mining method. In the Root Cause Identification Module, we use supervised Machine Learning (classification) methods to automatically predict the root causes of multiple quality problems. Finally, we illustrate the accuracy and efficiency of the proposed system and algorithms based on actual quality data from a case company. This study contributes to the literature from the following aspects: (i) the integrated system and algorithms can be used directly to develop a computer application to manage and solve quality problems with high concurrences and complexities in any manufacturing process; (ii) a general procedure and method are provided to formulate and describe a large quantity and different types of quality problems; (iii) compared with traditional methods, it is demonstrated using real case data that manufacturing companies can save significant time and cost with our proposed data-driven root cause analysis system; (iv) this study not only aims at improving the quality problem solving practices for a complex manufacturing process but also bridges a gap between the theoretical development of Machining Learning methods and their application in the operations management domain.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,315"
    },
    {
        "issnkey": "08867798",
        "isbn": null,
        "journal": "Tunnelling and Underground Space Technology",
        "publisher": null,
        "title": "tunnelboringmachinestbmperformancepredictionacasestudyusingbigdataanddeeplearning",
        "booktitle": null,
        "doi": "10.1016/j.tust.2020.103636",
        "author": [
            "Feng, Shangxin",
            "Chen, Zuyu",
            "Luo, Hua",
            "Wang, Shanyong",
            "Zhao, Yufei",
            "Liu, Lipeng",
            "Ling, Daosheng",
            "Jing, Liujie"
        ],
        "keywords": [
            "TBM performance prediction, Deep belief network (DBN), Yingsong Water Diversion Project, Field penetration index prediction"
        ],
        "abstract": "This work explores the potential for predicting TBM performance using deep learning. It focuses on a 17.5-km-long tunnel excavated for the Yingsong Water Diversion Project in Northeastern China with its 728 days\u2019 continuous monitoring of mechanical data. The prediction uses the deep belief network (DBN) proposed by Hinton et al. (2006),on the penetration rate, cutter rotation speed, torque, and thrust force. Field Penetration Index (FPI) is introduced to quantify TBM performance in the field. The DBN algorithm trains on nth number of preceding elements and predicts the performance of the n + 1th element. Prior to the implementation of the DBN, a pilot test was performed to find the optimal values for the network structural parameters (number of input nodes, number of hidden layers, number of nodes in the hidden layers, and learning rate). Predictions on FPIs in all the three rock types were then proceeded with good agreement with the field measured data. The mean relative errors for the predicted measured FPIs are generally less than 0.15 and the correlation coefficients (R) can be higher than 0.78. The predicted and measured FPI values along the length of the tunnel graphically follow the same trends. These results confirm the usefulness of big data and the deep learning in predicting TBM performance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.915",
        "scimago_value": "2,172"
    },
    {
        "issnkey": "01679236",
        "isbn": null,
        "journal": "Decision Support Systems",
        "publisher": null,
        "title": "dmn4dqwhendataqualitymeetsdmn",
        "booktitle": null,
        "doi": "10.1016/j.dss.2020.113450",
        "author": [
            "Valencia-Parra, \u00c1lvaro",
            "Parody, Luisa",
            "Varela-Vaca, \u00c1ngel",
            "Caballero, Ismael",
            "G\u00f3mez-L\u00f3pez, Mar\u00eda"
        ],
        "keywords": [
            "Data usability, Data quality, Decision model and notation, Data quality rule, Data quality assessment, Data quality measurement"
        ],
        "abstract": "To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.795",
        "scimago_value": "1,564"
    },
    {
        "issnkey": "0740624x",
        "isbn": null,
        "journal": "Government Information Quarterly",
        "publisher": null,
        "title": "isbigdatausedbycitiesunderstandingthenatureandantecedentsofbigdatausebymunicipalities",
        "booktitle": null,
        "doi": "10.1016/j.giq.2021.101600",
        "author": [
            "Ali, Hamza",
            "Titah, Ryad"
        ],
        "keywords": [
            "Big data use, Big data use in municipalities, Smart cities, E-government, Municipal use of IT, Digital government"
        ],
        "abstract": "It is estimated that by 2050, 70% of the population will be urban (Nations Unies, 2014). This massive urbanization has created unprecedented challenges for cities and city managers which has led many of them to look for technological solutions to address them, including the use of Big Data, which is among the most considered technological support to help improve the overall operational and service delivery of cities. It is estimated that around 7 billion connected objects will soon be implemented in cities worldwide which will produce an unprecedented and massive amount of real-time data that will have to be managed, used, and analyzed effectively. If this massive amount of data is effectively managed and used, it can provide important benefits and produce real positive impacts on the functioning of cities. Nonetheless, despite these benefits, only a few cities are able to use and exploit big data, and some studies have shown that less than 0.5% of all the available data has been explored. The objective of this study is to understand the factors that influence cities to use big data and the nature of such use. Based on a field survey involving 106 municipalities, this study investigates the antecedents of big data use by cities and shows how different sets of antecedents influence three different types of big data use by cities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "bigdataandfirmmarketingperformancefindingsfromknowledgebasedview",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.120986",
        "author": [
            "Gupta, Shivam",
            "Justy, Th\u00e9o",
            "Kamboj, Shampy",
            "Kumar, Ajay",
            "Kristoffersen, Eivind"
        ],
        "keywords": [
            "Big data analytics, Artificial intelligence, Marketing performance, Knowledge-based view"
        ],
        "abstract": "A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms\u2019 strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms\u2019 activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "22147853",
        "isbn": null,
        "journal": "Materials Today: Proceedings",
        "publisher": null,
        "title": "bigdataindustry40andcyberphysicalsystemsintegrationasmartindustrycontext",
        "booktitle": null,
        "doi": "10.1016/j.matpr.2020.07.170",
        "author": [
            "Singh, Harpreet"
        ],
        "keywords": [
            "Agile management, Heterogeneity, Internet-of-things, Smart factory, Smart manufacturing"
        ],
        "abstract": "The advancements in the industries have paved the way for the distributed establishment of the big data volumes, cyber-physical systems, and industrie 4.0. The perspectives of modules are integrated with the shop-floor monitoring and controlled by computational paradigms, and digital computational spaces. The performance rises after introducing an intelligent and automated manufacturing industry into the next-generation industry. The scope of this paper is to address the state-of-the-art technologies and phases such as digital twins, big data analytics, artificial intelligence, and internet-of-things. The research challenges are examined with attention on data integrity, data quality, data privacy, data availability, data scalability, data transformation, legitimate and monitoring issues, and governance. Lastly, potential research issues that need considerable research efforts are summarized. We believe that this paper is presenting the research directions for researchers in the area of smart industry towards its integration for the advancements of the industrial sector, and agile management. Some surprising development as industry 4.0 integration with socio-technical systems was found in designing the architecture of vertical, horizontal, and end-to-end integration mechanisms.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,341"
    },
    {
        "issnkey": "07365853",
        "isbn": null,
        "journal": "Telematics and Informatics",
        "publisher": null,
        "title": "bigdataanalyticsmeetssocialmediaasystematicreviewoftechniquesopenissuesandfuturedirections",
        "booktitle": null,
        "doi": "10.1016/j.tele.2020.101517",
        "author": [
            "{Bazzaz Abkenar}, Sepideh",
            "{Haghi Kashani}, Mostafa",
            "Mahdipour, Ebrahim",
            "Jameii, Seyed"
        ],
        "keywords": [
            "Social networks, Big data, Content analysis, Sentiment analysis, Systematic literature review"
        ],
        "abstract": "Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5 V\u2019s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.182",
        "scimago_value": "1,567"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822884-5",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter4challengesandsolutionsforbigdatainpersonalizedhealthcare",
        "booktitle": "Big Data in Psychiatry #x0026; Neurology",
        "doi": "10.1016/B978-0-12-822884-5.00016-7",
        "author": [
            "Hulsen, Tim"
        ],
        "keywords": [
            "Big data, Precision medicine, Personalized healthcare, Data science, Big data analytics"
        ],
        "abstract": "\u201cBig data\u201d is a term that has been used often in the past decade to describe datasets that are extremely large and complex so that traditional software is unable to store and analyze them in an accurate way. It can refer to \u201clong data,\u201d \u201cwide data,\u201d and both. Big data is of increasing importance in healthcare as well: new methods dedicated to improving data collection, storage, cleaning, processing, and interpretation for medical research continue to be developed. Exploiting new tools and methods to extract meaning from large volume information has the potential to drive real change in clinical practice, and combining this novel data-driven research with the classical hypothesis-driven research will have a large impact on personalized healthcare. However, significant challenges remain. Here we discuss the challenges (and possible solutions) posed to biomedical research by our increasing ability to collect, store, and analyze large datasets. Important challenges include: (1) the need for standardization of data content, format, and clinical definitions, adhering to the FAIR guiding principles; (2) the need for collaborative networks with sharing of both data and expertise, for example through a federated approach; (3) stricter privacy and ethics regulations, in particular the GDPR in the European Union; and (4) a need to reconsider how and when analytic methodology (data science) is taught to medical researchers. Overcoming these challenges will help to make a success of the use of big data in medical and translational research.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02650568",
        "isbn": null,
        "journal": "Natural Product Reports",
        "publisher": null,
        "title": "benefitingfrombigdatainnaturalproductsimportanceofpreservingfoundationalskillsandprioritizingdataquality",
        "booktitle": null,
        "doi": "10.1039/d1np00061f",
        "author": [
            "Cech, Nadja",
            "Medema, Marnix",
            "Clardy, Jon"
        ],
        "keywords": [
            ""
        ],
        "abstract": "ABSTRACT Systematic, large-scale, studies at the genomic, metabolomic, and functional level have transformed the natural product sciences. Improvements in technology and reduction in cost for obtaining spectroscopic, chromatographic, and genomic data coupled with the creation of readily accessible curated and functionally annotated data sets have altered the practices of virtually all natural product research laboratories. Gone are the days when the natural products researchers were expected to devote themselves exclusively to the isolation, purification, and structure elucidation of small molecules. We now also engage with big data in taxonomic, genomic, proteomic, and/or metabolomic collections, and use these data to generate and test hypotheses. While the oft stated aim for the use of large-scale -omics data in the natural products sciences is to achieve a rapid increase in the rate of discovery of new drugs, this has not yet come to pass. At the same time, new technologies have provided unexpected opportunities for natural products chemists to ask and answer new and different questions. With this viewpoint, we discuss the evolution of big data as a part of natural products research and provide a few examples of how discoveries have been enabled by access to big data. We also draw attention to some of the limitations in our existing engagement with large datasets and consider what would be necessary to overcome them.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "13.423",
        "scimago_value": "2,703"
    },
    {
        "issnkey": "03050483",
        "isbn": null,
        "journal": "Omega",
        "publisher": null,
        "title": "bigdatadrivensupplychaindesignandapplicationsforblockchainanactionresearchusingcasestudyapproach",
        "booktitle": null,
        "doi": "10.1016/j.omega.2021.102452",
        "author": [
            "Sundarakani, Balan",
            "Ajaykumar, Aneesh",
            "Gunasekaran, Angappa"
        ],
        "keywords": [
            "Big data architecture, Action research, Case study research, Blockchain adoption, Supply chain management"
        ],
        "abstract": "Blockchain appears to still be nascent in its growth and a relatively untapped asset. This research investigates the need of blockchain in Industry 4.0 environment from Big Data perspective in supply chain management. The research method used in this study involves a combination of an Action Research method and Case Study research. More specifically, the action research method was applied in two industry case studies that implemented and tested the designed architecture in a global logistics environment. Case Study A examined the blockchain application in cross-border cargo movements whereas Case Study B investigated the application in a liquid chemical logistics company serving to petroleum industries. Our research analysis has identified that the Case A subject had disconnected systems and services for blockchain wherein the big data interactions had failed (failure case). Whereas in Case B, the company has achieved nearly 25% increase in revenue through its customer service after the blockchain implementation and thereby reduction in paperwork and carbon emissions (success case). This research contributes to the advancement of the body of knowledge to big data and blockchain by identifying key implementation guideline and issues for blockchain in supply chain management. Further, action-based research coupled with a case study approach has been used to evaluate the application aspects of the architecture's scalability and functionality of bigdata and blockchain in supply chain management.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,500"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818015-0",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter1dataqualityandthedatadependentworld",
        "booktitle": "Executing Data Quality Projects (Second Edition)",
        "doi": "10.1016/B978-0-12-818015-0.00021-9",
        "author": [
            "McGilvray, Danette"
        ],
        "keywords": [
            "Data-dependent world, data-driven, assets, technology, legal and regulatory tsunami, Internet of Things (IoT), big data, artificial intelligence (AI), machine learning (ML), The Leader\u2019s Data Manifesto, data literacy, change, COVID-19"
        ],
        "abstract": "Chapter 1 addresses topics in our world today, shows how they are dependent on data and information, why data quality is more relevant and critical now than ever before, and how the Ten Steps methodology will help. Topics include COVID-19, the legal and regulatory tsunami, big data, Internet of Things (IoT), 5G, artificial intelligence (AI) and machine learning (ML). Our data-dependent world is broader than, yet encompasses, being data-driven. Data and information are assets to be managed and are compared to how human and financial resources are managed. The Leader\u2019s Data Manifesto is introduced as a starting point for conversations about the importance of managing data and information assets. While the Ten Steps methodology is meant for specific audiences, three recommendations were provided that anyone, in any organization, can do to help raise awareness of the importance of data quality: add data and information to the conversation, increase data literacy in the workplace, and include data (quality) management in learning institutions at all levels.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26667649",
        "isbn": null,
        "journal": "Data Science and Management",
        "publisher": null,
        "title": "aninterviewwithshouyangwangresearchfrontierofbigdatadriveneconomicandfinancialforecasting",
        "booktitle": null,
        "doi": "10.1016/j.dsm.2021.01.001",
        "author": [
            "Wang, Shouyang"
        ],
        "keywords": [
            "Big data, Economic forecasting, Data mining, Spatio-temporal"
        ],
        "abstract": "The development of big data generation, acquisition, storage, processing, and other technologies has greatly enriched our sensory world and fundamentally changed the basis of traditional economic and financial forecasting. Unexpected events in the economic and financial fields challenge our confidence in the performance of forecasting models. Obviously, the big data-driven decision theories and analysis methods are different from the traditional methods. In view of the important role of big data-driven economic and financial forecasting in social stability, innovative development, and sustainability, the research frontiers of big data-driven economic and financial forecasting in the future include: feature mining of complex economic systems with big data representation; accurate real-time correction of theories and methods of dynamic forecasting and early warning; general paradigm of big data forecasting research; formation and process of big data-driven economic and financial system management mechanism, etc. Systematic research on such issues will contribute to the formation of decision-making theories and research systems in the context of big data, thus improving the adaptability and scientificity of management decisions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09507051",
        "isbn": null,
        "journal": "Knowledge-Based Systems",
        "publisher": null,
        "title": "titanaknowledgebasedplatformforbigdataworkflowmanagement",
        "booktitle": null,
        "doi": "10.1016/j.knosys.2021.107489",
        "author": [
            "Ben\u00edtez-Hidalgo, Antonio",
            "Barba-Gonz\u00e1lez, Crist\u00f3bal",
            "Garc\u00eda-Nieto, Jos\u00e9",
            "Guti\u00e9rrez-Moncayo, Pedro",
            "Paneque, Manuel",
            "Nebro, Antonio",
            "Mar, Mar\u00eda",
            "Aldana-Montes, Jos\u00e9",
            "Navas-Delgado, Ismael"
        ],
        "keywords": [
            "Big Data analytics, Semantics, Knowledge extraction"
        ],
        "abstract": "Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.038",
        "scimago_value": "1,587"
    },
    {
        "issnkey": "03608352",
        "isbn": null,
        "journal": "Computers & Industrial Engineering",
        "publisher": null,
        "title": "optimaltimingofbigdataapplicationinatwoperioddecisionmodelwithnewproductsales",
        "booktitle": null,
        "doi": "10.1016/j.cie.2021.107550",
        "author": [
            "Yang, Lei",
            "Jiang, Anqian",
            "Zhang, Jiahua"
        ],
        "keywords": [
            "Supply chain management, Optimal strategy, Big data application, Two-period model, Social welfare"
        ],
        "abstract": "We study a firm's strategy in adopting big data technology to motivate consumer demand over two periods. In the first period, the firm designs a product to sell to the market and determines whether to apply big data to attract more consumers. In the second period, the firm designs a new product and determines whether to sell the old product and the new product simultaneously, where big data can also be applied in this period to stimulate more demands. We formulate this problem into four models considering whether the firm adopts big data in the first period and/or the second period, and whether the firm only sells the new product or sells both the old and new products in the second period. We find that the firm prefers to apply big data over both periods when the cost is low, only over the second period when the cost is median and will not apply big data when the cost is high. Interestingly, only applying big data over the first period also may bring the most profits with heterogeneous big data coefficients. Furthermore, applying big data in the second period is the better choice for the social welfare.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,315"
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "abigdatacentricarchitecturemetamodelforindustry40",
        "booktitle": null,
        "doi": "10.1016/j.future.2021.06.020",
        "author": [
            "{L\u00f3pez Mart\u00ednez}, Patricia",
            "Dint\u00e9n, Ricardo",
            "Drake, Jos\u00e9",
            "Zorrilla, Marta"
        ],
        "keywords": [
            "Data-centric architecture, Data-intensive applications, Model-based development, Industry 4.0, Big data, Metamodel"
        ],
        "abstract": "The effective implementation of Industry 4.0 requires the reformulation of industrial processes in order to achieve the vertical and horizontal digitalization of the value chain. For this purpose, it is necessary to provide tools that enable their successful implementation. This paper therefore proposes a data-centric, distributed, dynamically scalable reference architecture that integrates cutting-edge technologies being aware of the existence of legacy technology typically present in these environments. In order to make its implementation easier, we have designed a metamodel that collects the description of all the elements involved in a digital platform (data, resources, applications and monitoring metrics) as well as the necessary information to configure, deploy and execute applications on it. Likewise, we provide a tool compliant to the metamodel that automates the generation of configuration, deployment and launch files and their corresponding transference and execution in the nodes of the platform. We show the flexibility, extensibility and validity of our software artefacts through their application in two case studies, one addressed to preprocess and store pollution data and the other one, more complex, which simulates the management of an electric power distribution of a smart city.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01641212",
        "isbn": null,
        "journal": "Journal of Systems and Software",
        "publisher": null,
        "title": "dataqualitycertificationusingisoiec25012industrialexperiences",
        "booktitle": null,
        "doi": "10.1016/j.jss.2021.110938",
        "author": [
            "Gualo, Fernando",
            "Rodriguez, Mois\u00e9s",
            "Verdugo, Javier",
            "Caballero, Ismael",
            "Piattini, Mario"
        ],
        "keywords": [
            "Data quality evaluation process, Data quality certification, Data quality management, ISO/IEC 25012, ISO/IEC 25024, ISO/IEC 25040"
        ],
        "abstract": "The most successful organizations in the world are data-driven businesses. Data is at the core of the business of many organizations as one of the most important assets, since the decisions they make cannot be better than the data on which they are based. Due to this reason, organizations need to be able to trust their data. One important activity that helps to achieve data reliability is the evaluation and certification of the quality level of organizational data repositories. This paper describes the results of the application of a data quality evaluation and certification process to the repositories of three European organizations belonging to different sectors. We present findings from the point of view of both the data quality evaluation team and the organizations that underwent the evaluation process. In this respect, several benefits have been explicitly recognized by the involved organizations after achieving the data quality certification for their repositories (e.g., long-term organizational sustainability better internal knowledge of data, and a more efficient management of data quality). As a result of this experience, we have also identified a set of best practices aimed to enhance the data quality evaluation process.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.829",
        "scimago_value": "0,642"
    },
    {
        "issnkey": "01663615",
        "isbn": null,
        "journal": "Computers in Industry",
        "publisher": null,
        "title": "bigdataanalyticsimplementationchallengesinindianmanufacturingsupplychains",
        "booktitle": null,
        "doi": "10.1016/j.compind.2020.103368",
        "author": [
            "Raut, Rakesh",
            "Yadav, Vinay",
            "Cheikhrouhou, Naoufel",
            "Narwane, Vaibhav",
            "Narkhede, Balkrishna"
        ],
        "keywords": [
            "Big data analytics, DEMATEL, Indian manufacturing supply chains, Interpretive structural modeling, MICMAC analysis"
        ],
        "abstract": "Big Data Analytics (BDA) has attracted significant attention from both academicians and practitioners alike as it provides several ways to improve strategic, tactical and operational capabilities to eventually create a positive impact on the economic performance of organizations. In the present study, twelve significant barriers against BDA implementation are identified and assessed in the context of Indian manufacturing Supply Chains (SC). These barriers are modeled using an integrated two-stage approach, consisting of Interpretive Structural Modeling (ISM) in the first stage and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in the second stage. The approach developed provides the interrelationships between the identified constructs and their intensities. Moreover, Fuzzy MICMAC technique is applied to analyze the high impact (i.e., high driving power) barriers. Results show that four constructs, namely lack of top management support, lack of financial support, lack of skills, and lack of techniques or procedures, are the most significant barriers. This study aids policy-makers in conceptualizing the mutual interaction of the barriers for developing policies and strategies to improve the penetration of BDA in manufacturing SC.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.635",
        "scimago_value": "1,432"
    },
    {
        "issnkey": "18771327",
        "isbn": null,
        "journal": "Orthopaedics and Trauma",
        "publisher": null,
        "title": "introductiontobigdataintraumaandorthopaedics",
        "booktitle": null,
        "doi": "10.1016/j.mporth.2021.01.004",
        "author": [
            "Koziara, Michal",
            "Gaukroger, Andrew",
            "Hing, Caroline",
            "Eardley, Will"
        ],
        "keywords": [
            "Big Data, clinical database, GDPR, healthcare, orthopaedics, privacy"
        ],
        "abstract": "The enormous amount of data created daily within healthcare has become so complex, that it cannot be effectively handled by routine analytical methods. Such large data sets can be processed, looking for correlation not otherwise obvious in smaller patient samples. Further advances in terms of data processing as well as significant infrastructure and personnel investments are required to fully reap the benefits of Big Data, in terms of research and financial sense. However, despite its popularity and promise, Big Data in orthopaedics has attracted a number of criticisms, not only in terms of data input and processing, but particularly with regards to analysis of the output, which are explored within the article. Moreover, use of Big Data within healthcare carries the ethical question of privacy and consent.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,194"
    },
    {
        "issnkey": "2452414x",
        "isbn": null,
        "journal": "Journal of Industrial Information Integration",
        "publisher": null,
        "title": "cbi40acrosslayerapproachforbigdatagatheringforactivemonitoringandmaintenanceinthemanufacturingindustry40",
        "booktitle": null,
        "doi": "10.1016/j.jii.2021.100236",
        "author": [
            "Faheem, Muhammad",
            "Butt, Rizwan",
            "Ali, Rashid",
            "Raza, Basit",
            "Ngadi, Md.",
            "Gungor, Vehbi"
        ],
        "keywords": [
            "Internet of things, Industry 4.0, Big data, Multi-channel communication, Wireless sensor network"
        ],
        "abstract": "Industry 4.0 (I4.0) defines a new paradigm to produce high-quality products at the low cost by reacting quickly and effectively to changing demands in the highly volatile global markets. In Industry 4.0, the adoption of Internet of Things (IoT)-enabled Wireless Sensors (WSs) in the manufacturing processes, such as equipment, machining, assembly, material handling, inspection, etc., generates a huge volume of data known as Industrial Big Data (IBD). However, the reliable and efficient gathering and transmission of this big data from the source sensors to the floor inspection system for the real-time monitoring of unexpected changes in the production and quality control processes is the biggest challenge for Industrial Wireless Sensor Networks (IWSNs). This is because of the harsh nature of the indoor industrial environment that causes high noise, signal fading, multipath effects, heat and electromagnetic interference, which reduces the transmission quality and trigger errors in the IWSNs. Therefore, this paper proposes a novel cross-layer data gathering approach called CBI4.0 for active monitoring and control of manufacturing processes in the Industry 4.0. The key aim of the proposed CBI4.0 scheme is to exploit the multi-channel and multi-radio architecture of the sensor network to guarantee quality of service (QoS) requirements, such as higher data rates, throughput, and low packet loss, corrupted packets, and latency by dynamically switching between different frequency bands in the Multichannel Wireless Sensor Networks (MWSNs). By performing several simulation experiments through EstiNet 9.0 simulator, the performance of the proposed CBI4.0 scheme is compared against existing studies in the automobile Industry 4.0. The experimental outcomes show that the proposed scheme outperforms existing schemes and is suitable for effective control and monitoring of various events in the automobile Industry 4.0.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0160791x",
        "isbn": null,
        "journal": "Technology in Society",
        "publisher": null,
        "title": "modellingandanalysisofbigdataplatformgroupadoptionbehaviourbasedonsocialnetworkanalysis",
        "booktitle": null,
        "doi": "10.1016/j.techsoc.2021.101570",
        "author": [
            "Lei, Zhimei",
            "Chen, Yandan",
            "Lim, Ming"
        ],
        "keywords": [
            "Big data, Platforms, Technology adoption, Corporate group behaviour, Social network analysis"
        ],
        "abstract": "Due to the importance of big data technology in decision-making, production and service provision, enterprises have adopted various big data technologies and platforms to improve their operational efficiency. However, the number of enterprises that have adopted big data is not promising. The purpose of this study is to explore the current status of big data adoption by Chinese enterprises and to reveal the possible factors that hinder big data adoption from the group behaviour network perspective. Based on a real case survey of 54 big data platforms (BDPs), four types of networks\u2014i.e., the enterprise-platform network, enterprise network, platform network and industry similarity and difference (ISD) network\u2014are constructed and analysed on the basis of social network analysis (SNA). This study finds that among Chinese enterprises, the level and scope of big data adoption are generally low and are imbalanced among industries; the cognitive level and adoption behaviour of enterprises on BDPs are inconsistent, the compatibility of BDPs is different, and the density and distance-based cohesion of networks are weak; although the current big data adoption behaviours of Chinese enterprises have formed some structural features, core-periphery structures and maximal complete cliques are found, and the current network structure has little impact on individual enterprises and platforms; enterprises in the same industry prefer to adopt the same kind of big data technology or platform. Based on these findings, several strategies and suggestions to improve big data adoption are provided.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820203-6",
        "journal": null,
        "publisher": "Academic Press",
        "title": "2bigdataanalyticsforhealthcaretheoryandapplications",
        "booktitle": "Applications of Big Data in Healthcare",
        "doi": "10.1016/B978-0-12-820203-6.00008-4",
        "author": [
            "Bachhety, Shivam",
            "Kapania, Shivani",
            "Jain, Rachna"
        ],
        "keywords": [
            "Big Data, healthcare, Big Data Analytics, Hadoop"
        ],
        "abstract": "In the past 10 years, the healthcare industry is growing at a remarkable rate. The healthcare industry is generating enormous amounts of data in terms of volume, velocity, and variety. Big Data methodologies in healthcare can not only increase the business value but will also add to the improvement of healthcare services. Several techniques can be implemented to develop early disease diagnose systems and improve treatment procedures using detailed analysis over time. In such a situation, Big data Analytics proposes to connect intricate databases to achieve more useful results. In this chapter, we will discuss the procedure of big data analytics in the healthcare sector with some practical applications along with its challenges. We will also have a look at the various big data techniques and their tools for implementation. We conclude this chapter with a discussion on potential opportunities for analytics in the healthcare sector.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10478310",
        "isbn": null,
        "journal": "The Journal of High Technology Management Research",
        "publisher": null,
        "title": "bigdataandsmartdatatwointerdependentandsynergisticdigitalpolicieswithinavirtuousdataexploitationloop",
        "booktitle": null,
        "doi": "10.1016/j.hitech.2021.100406",
        "author": [
            "Lacam, Jean-S\u00e9bastien",
            "Salvetat, David"
        ],
        "keywords": [
            "Big data, Smart data, Volume, Velocity, Variety, Automotive distribution"
        ],
        "abstract": "This research examines for the first time the relationship between Big data and Smart data among French automotive distributors. Many low-tech firms engage in these data policies to improve their decisions and performance through the predictive capacities of their data. A discussion emerges in the literature according to which an effective policy lies in the conversion of a mass of raw data into so-called intelligent data. In order to understand better this digital transition, we question the transformation of data policies practiced in low-tech firms through the founding model of 3Vs (Volume, Variety and Velocity of data). First of all, this empirical study of 112 French automotive distributors develops the existing literature by proposing an original and detailed typology of the data policies practiced (Low data, Big data and Smart data). Secondly, after specifying the elements of the differences between the quantitative nature of Big data and the qualitative nature of Smart data, our results reveal and analyse for the first time the existence of their synergistic relationship. Companies transform their Big data approach into Smart data when they move from massive exploitation to intelligent exploitation of their data. The phenomenon is part of a high-end loop data exploitation. Initially, the exploitation of intelligent data can only be done by extracting a sample from a large raw data pool previously made by a Big data policy. Secondly, the organization's raw data pool is in turn enriched by the repayment of contributions made by the Smart data approach. Thus, this study develops three important ways. First off, we identify, detail and compare the current data policies of a traditional industry. Secondly, we reveal and explain the evolution of digital practices within organizations that now combine both quantitative and qualitative data exploitation. Finally, our results guide decision-makers towards the synergistic and the legitimate association of different forms of data management for better performance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,684"
    },
    {
        "issnkey": "15689972",
        "isbn": null,
        "journal": "Autoimmunity Reviews",
        "publisher": null,
        "title": "digitalhealthbigdataandsmarttechnologiesforthecareofpatientswithsystemicautoimmunediseaseswheredowestand",
        "booktitle": null,
        "doi": "10.1016/j.autrev.2021.102864",
        "author": [
            "Bergier, Hugo",
            "Duron, Lo\u00efc",
            "Sordet, Christelle",
            "Kawka, Lou",
            "Schlencker, Aur\u00e9lien",
            "Chasset, Fran\u00e7ois",
            "Arnaud, Laurent"
        ],
        "keywords": [
            "Autoimmune diseases, Digital technology, Big data, Delivery of health care, Telemedicine"
        ],
        "abstract": "The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.754",
        "scimago_value": "2,621"
    },
    {
        "issnkey": "",
        "isbn": "978-0-08-102909-1",
        "journal": null,
        "publisher": "Academic Press",
        "title": "freshoutlookonnumericalmethodsforgeodynamicspart2bigdatahpceducation",
        "booktitle": "Encyclopedia of Geology (Second Edition)",
        "doi": "10.1016/B978-0-08-102908-4.00111-9",
        "author": [
            "Morra, Gabriele",
            "Yuen, David",
            "Tufo, Henry",
            "Knepley, Matthew"
        ],
        "keywords": [
            "Big Data, High performance computing, Education, Modeling, Geodynamics"
        ],
        "abstract": "Since 2015 much has developed in geodynamical modeling because of the arrival of Big Data. We present in two parts an overview of numerical techniques but also a scan of the new opportunities in this age of Big Data and prepare the community for the coming decade, the roaring twenties, when Data Analytics will reign. We begin with a review of traditional numerical methods (Part I), followed by a survey of the current techniques used for data analytics and high-performance computing (HPC) (Part II). Our aim is to cover topics of machine learning, neural networks and deep learning, unsupervised learning as well as the role that HPC will play in the Big Data era, especially in hardware of various calibers. Finally, we will address the need for education of students and professionals, in particular, on the use of the emerging programming languages and the importance of scientific software communities.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00457906",
        "isbn": null,
        "journal": "Computers & Electrical Engineering",
        "publisher": null,
        "title": "insightsintodemandsidemanagementwithbigdataanalyticsinelectricityconsumersbehaviour",
        "booktitle": null,
        "doi": "10.1016/j.compeleceng.2020.106902",
        "author": [
            "Oprea, Simona-Vasilica",
            "B\u00e2ra, Adela",
            "Tudoric\u0103, Bogdan",
            "C\u0103linoiu, Maria",
            "Botezatu, Mihai"
        ],
        "keywords": [
            "Big data, Machine learning, Smart meters, Electricity consumption, Clustering, Questionnaire analytics"
        ],
        "abstract": "The consumption data from smart meters and complex questionnaires reveals the electricity consumers\u2019 willingness to adapt their lifestyle to reduce or change their behaviour in electricity usage to flatten the peak in electricity consumption and release the stress in the power grid. Thus, the electricity consumption can support the enforcement of tariff and demand response strategies. Although the plethora of complex, unstructured and heterogeneous data is collected from various devices connected to the Internet, smart meters, plugs, sensors and complex questionnaires, there is an undoubted challenge to handle the data flow that does not provide much information as it remains unprocessed. Therefore, in this paper, we propose an innovative methodology that organizes and extracts valuable information from the increasing volume of data, such as data about the electricity consumption measured and recorded at 30 min intervals, as well as data collected from complex questionnaires.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,630"
    },
    {
        "issnkey": "00489697",
        "isbn": null,
        "journal": "Science of The Total Environment",
        "publisher": null,
        "title": "anupdatedmodelreadyemissioninventoryforguangdongprovincebyincorporatingbigdataandmappingontomultiplechemicalmechanisms",
        "booktitle": null,
        "doi": "10.1016/j.scitotenv.2020.144535",
        "author": [
            "Huang, Zhijiong",
            "Zhong, Zhuangmin",
            "Sha, Qinge",
            "Xu, Yuanqian",
            "Zhang, Zhiwei",
            "Wu, Lili",
            "Wang, Yuzheng",
            "Zhang, Lihang",
            "Cui, Xiaozhen",
            "Tang, MingShuang",
            "Shi, Bowen",
            "Zheng, Chuanzeng",
            "Li, Zhen",
            "Hu, Mingming",
            "Bi, Linlin",
            "Zheng, Junyu",
            "Yan, Min"
        ],
        "keywords": [
            "Emission inventory, Guangdong Province, Ship emissions, Big data, VOCs speciation"
        ],
        "abstract": "An accurate characterization of spatial-temporal emission patterns and speciation of volatile organic compounds (VOCs) for multiple chemical mechanisms is important to improving the air quality ensemble modeling. In this study, we developed a 2017-based high-resolution (3 km \u00d7 3 km) model-ready emission inventory for Guangdong Province (GD) by updating estimation methods, emission factors, activity data, and allocation profiles. In particular, a full-localized speciation profile dataset mapped to five chemical mechanisms was developed to promote the determination of VOC speciation, and two dynamic approaches based on big data were used to improve the estimation of ship emissions and open fire biomass burning (OFBB). Compared with previous emissions, more VOC emissions were classified as oxygenated volatile organic compound (OVOC) species, and their contributions to the total ozone formation potential (OFP) in the Pearl River Delta (PRD) region increased by 17%. Formaldehyde became the largest OFP species in GD, accounting for 11.6% of the total OFP, indicating that the model-ready emission inventory developed in this study is more reactive. The high spatial-temporal variability of ship sources and OFBB, which were previously underestimated, was also captured by using big data. Ship emissions during typhoon days and holidays decreased by 23\u201355%. 95% of OFBB emissions were concentrated in 9% of the GD area and 31% of the days in 2017, demonstrating their strong spatial-temporal variability. In addition, this study revealed that GD emissions have changed rapidly in recent years due to the leap-forward control measures implemented, and thus, they needed to be updated regularly. All of these updates led to a 5\u201317% decrease in the emission uncertainty for most pollutants. The results of this study provide a reference for how to reduce uncertainties in developing model-ready emission inventories.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.963",
        "scimago_value": "1,795"
    },
    {
        "issnkey": "2352152x",
        "isbn": null,
        "journal": "Journal of Energy Storage",
        "publisher": null,
        "title": "bigdatadrivenvehiclebatterymanagementmethodanovelcyberphysicalsystemperspective",
        "booktitle": null,
        "doi": "10.1016/j.est.2020.102064",
        "author": [
            "Li, Shuangqi",
            "Zhao, Pengfei"
        ],
        "keywords": [
            "Electric vehicles, Battery energy storage, Cyber-physical battery management system, Big data, Deep learning"
        ],
        "abstract": "The establishment of an accurate battery model is of great significance to improve the reliability of electric vehicles (EVs). However, the battery is a complex electrochemical system with hardly observable and simulatable internal chemical reactions, and it is challenging to estimate the state of battery accurately. This paper proposes a novel flexible and reliable battery management method based on the battery big data platform and Cyber-Physical System (CPS) technology. First of all, to integrate the battery big data resources in the cloud, a Cyber-physical battery management framework is defined and served as the basic data platform for battery modeling issues. And to improve the quality of the collected battery data in the database, this work reports the first attempt to develop an adaptive data cleaning method for the cloud battery management issue. Furthermore, a deep learning algorithm-based feature extraction model, as well as a feature-oriented battery modeling method, is developed to mitigate the under-fitting problem and improve the accuracy of the cloud-based battery model. The actual operation data of electric buses is used to validate the proposed methodologies. The maximum data restoring error can be limited within 1.3% in the experiments, which indicates that the proposed data cleaning method is able to improve the cloud battery data quality effectively. Meanwhile, the maximum SoC estimation error in the proposed feature-oriented battery modeling method is within 2.47%, which highlights the effectiveness of the proposed method.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "massivescalecarbonpollutioncontrolandbiologicalfusionunderbigdatacontext",
        "booktitle": null,
        "doi": "10.1016/j.future.2021.01.002",
        "author": [
            "Liu, Yi",
            "Xu, Jie",
            "Yi, Weijie"
        ],
        "keywords": [
            "Internet-scale network, Dense subgraph mining, Low carbon, Multiple features, Biologically-aware"
        ],
        "abstract": "In the modern society, there are a rich number of low-carbon enterprises that the explicitly/implicitly collaborated. Effectively understanding the mechanism of their complex cooperative relationships is becoming an urgent and significant problem in information processing and management. Traditionally, these cooperation behavior are analyzed in a holistic and non-quantitative way, where the complicated relationships among various enterprises cannot be well represented. In this work, we propose to understand the low-carbon entrepreneurs\u2019 cooperation by leveraging a massive-scale dense subgraph mining technique, based on which an evolutionary graphical model is built to dynamically represent such complex relationships. More specifically, given million-scale low-carbon enterprises, we first extract multiple biologically-aware features (e.g., production value and carbon emission) to represent each of them. Based on this, a massive-scale affinity network is constructed to characterize the relationships among these enterprises. Based on this, an efficient subgraph mining algorithm (called graph shift) is deployed to discover the neighbors for each enterprise. Finally, based on the discovered neighbors of each enterprise, we can build a graphical model to represent the relationships among explicitly/implicitly-connected enterprises. The flows of multiple attributes (benefit exchange and resources swap) can be modeled effectively. To demonstrate the usefulness of our method, we manually label the attributes of 20,000 enterprises, based on which a classification model is trained by encoding the neighboring attributes of each enterprise. Comparative results have clearly demonstrated the competitiveness of our method. Moreover, visualization results can reveal the effectiveness of our method in uncovering the intrinsic distributions/correlations of million-scale enterprises.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03062619",
        "isbn": null,
        "journal": "Applied Energy",
        "publisher": null,
        "title": "investmentsindataqualityevaluatingimpactsoffaultydataonassetmanagementinpowersystems",
        "booktitle": null,
        "doi": "10.1016/j.apenergy.2020.116057",
        "author": [
            "Koziel, Sylvie",
            "Hilber, Patrik",
            "Westerlund, Per",
            "Shayesteh, Ebrahim"
        ],
        "keywords": [
            "Asset management, Component replacement, Data quality costs, Electric power distribution, Optimization, Trade-off"
        ],
        "abstract": "Data play an essential role in asset management decisions. The amount of data is increasing through accumulating historical data records, new measuring devices, and communication technology, notably with the evolution toward smart grids. Consequently, the management of data quantity and quality is becoming even more relevant for asset managers to meet efficiency and reliability requirements for power grids. In this work, we propose an innovative data quality management framework enabling asset managers (i) to quantify the impact of poor data quality, and (ii) to determine the conditions under which an investment in data quality improvement is required. To this end, an algorithm is used to determine the optimal year for component replacement based on three scenarios, a Reference scenario, an Imperfect information scenario, and an Investment in higher data quality scenario. Our results indicate that (i) the impact on the optimal year of replacement is the highest for middle-aged components; (ii) the profitability of investments in data quality improvement depends on various factors, including data quality, and the cost of investment in data quality improvement. Finally, we discuss the implementation of the proposed models to control data quality in practice, while taking into account real-world technological and economic limitations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.746",
        "scimago_value": "3,035"
    },
    {
        "issnkey": "00457906",
        "isbn": null,
        "journal": "Computers & Electrical Engineering",
        "publisher": null,
        "title": "bigdataanalyticsasaservicebridgingthegapbetweensecurityexpertsanddatascientists",
        "booktitle": null,
        "doi": "10.1016/j.compeleceng.2021.107215",
        "author": [
            "Ardagna, Claudio",
            "Bellandi, Valerio",
            "Damiani, Ernesto",
            "Bezzi, Michele",
            "Hebert, Cedric"
        ],
        "keywords": [
            "Artificial intelligence, Big Data Analytics, Machine learning, Security and privacy"
        ],
        "abstract": "We live in an interconnected and pervasive world where huge amount of data are collected every second. Fully exploiting data through advanced analytics, machine learning and artificial intelligence, becomes crucial for businesses, from micro to large enterprises, resulting in a key advantage (or shortcoming) in the global market competition, as well as in a strong market driver for business analytics solutions. This scenario is deeply changing the security landscape, introducing new risks and threats that affect security and privacy of systems, on one side, and safety of users, on the other side. Many domains that can benefit from novel solutions based on data analytics have stringent security requirements to fulfill. The Energy domain\u2019s Smart Grid is a major example of systems at the crossroads of security and data-driven intelligence. The Smart Grid plays a crucial role in modern energy infrastructure. However, it must face two major challenges related to security: managing front-end intelligent devices such as power assets and smart meters securely, and protecting the huge amount of data received from these devices. Starting from these considerations, setting up proper analytics is a complex problem because security controls could have the undesired side effect of decreasing the accuracy of the analytics themselves. This is even more critical when the configuration of security controls is let to the security expert, who often has only basic skills in data science. In this paper, we propose a solution based on the concept of Model-Based Big Data Analytics-as-a-Service (MBDAaaS) that bridges the gap between security experts and data scientists. Our solution acts as a middleware allowing a security expert and a data scientist to collaborate to the deployment of an analytics addressing their needs.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,630"
    },
    {
        "issnkey": "13665545",
        "isbn": null,
        "journal": "Transportation Research Part E: Logistics and Transportation Review",
        "publisher": null,
        "title": "bigdataanalyticsasamediatorinleanagileresilientandgreenlargpracticeseffectsonsustainablesupplychains",
        "booktitle": null,
        "doi": "10.1016/j.tre.2020.102170",
        "author": [
            "Raut, Rakesh",
            "Mangla, Sachin",
            "Narwane, Vaibhav",
            "Dora, Manoj",
            "Liu, Mengqi"
        ],
        "keywords": [
            "Big data analytics, Manufacturing firms, Supply chain and logistics management, LARG, Business performance and innovation, Sustainability"
        ],
        "abstract": "The effect of big data on the lean, agile, resilient, and green (LARG) supply chain has not been explored much in the literature. This study investigates the role of \u2018Big Data Analytics\u2019 (BDA) as a mediator between \u2018sustainable supply chain business performance\u2019 and key factors, namely, lean practices, social practices, environmental practices, organisational practices, supply chain practices, financial practices, and total quality management. A sample of 297 responses from thirty-seven Indian manufacturing firms was collected. The paper is beneficial for managers and practitioners to understand supply chain analytics, and it addresses challenges in the management of LARG practices to contribute to a sustainable supply chain.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.875",
        "scimago_value": "2,042"
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "bigdataanalyticsfordefaultpredictionusinggraphtheory",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2021.114840",
        "author": [
            "Y\u0131ld\u0131r\u0131m, Mustafa",
            "Okay, Feyza",
            "\u00d6zdemir, Suat"
        ],
        "keywords": [
            "Big data analytics, Graph theory, Machine learning, Default prediction, SHAP value"
        ],
        "abstract": "With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010\u20132018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "01419331",
        "isbn": null,
        "journal": "Microprocessors and Microsystems",
        "publisher": null,
        "title": "searchqueryofenglishtranslationtextbasedonembeddedsystemandbigdata",
        "booktitle": null,
        "doi": "10.1016/j.micpro.2021.103928",
        "author": [
            "Li, Zhihong"
        ],
        "keywords": [
            "Cross-language information retrieval, Optical character recognition, Embedded applications"
        ],
        "abstract": "Cross-Language Information Retrieval (CLIR) the purpose of another language (target language), a collection of documents written question from one language (source language).CLIR employees publish documents based on user queries, dictionary translation, machine translation methods, and promotion problems. Through various detection methods and translation, this word applies to single words, translation, transliteration of names, including transcription and translation and disambiguation. CLIR Recovery in Question Semantics Technology is the most appropriate) Translation to retrieve documents (dictionary related method) based on English Question Concentrations and Question translation. To the proposed model of translating Arabic to English, and provides the high accuracy Optical Character Recognition (OCR) errors in handling orthography, expanding outside and transliteration dubbed gives higher accuracy in resolving ambiguities. Thus, the single question expands with additional meanings and related words that improve significantly with semantic input. However, the documents related to the questions recover those cross language boundaries. The development of large data systems is completely different from the actual goal of small (traditional, structured) data system development. When the in-depth learning technology is developed, face some space and environmental barriers in different laboratory environments. To describe the requirements when running embedded applications on computers for deep learning.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.525",
        "scimago_value": "0,323"
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "firmlevelcapabilitiestowardsbigdatavaluecreation",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2020.07.036",
        "author": [
            "Brinch, Morten",
            "Gunasekaran, Angappa",
            "{Fosso Wamba}, Samuel"
        ],
        "keywords": [
            "Big data, Supply chain management, Operations management, Value creation, Business analytics, Capabilities"
        ],
        "abstract": "Big data has played an increasingly important role in using data to improve business value. In response to several big data challenges, the purpose of this study is to identify firm-level capabilities required to create value from big data. The adjacent theories of business process management and IT business value underpinned the study, together with an in-depth case study that led to the identification of twenty-four types of capabilities related to IT, process, performance, human, strategic, and organizational practices. The findings confirmed the application of practices and capabilities of adjacent theories, as well as certain practices and attributes that were both changed and reinforced at the intersection of big data. As an outstanding additional support to the extant big data studies, this work empirically confirms and portrays hitherto unexplored capabilities of big data and set their roles, thus providing a holistic overview of firm-level capabilities that are required for big data value creation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "00298018",
        "isbn": null,
        "journal": "Ocean Engineering",
        "publisher": null,
        "title": "marinebigdataanalysisofshipsfortheenergyefficiencychangesofthehullandmaintenanceevaluationbasedontheiso19030standard",
        "booktitle": null,
        "doi": "10.1016/j.oceaneng.2021.108953",
        "author": [
            "Shaw, Heiu-Jou",
            "Lin, Cheng-Kuan"
        ],
        "keywords": [
            "Energy efficiency management, ISO 19030, Hull and propeller maintenance"
        ],
        "abstract": "This study analyzes the energy efficiency of ships based on ISO 19030, which is a standard for the measurement of changes in hull and propeller performance. The goal is to provide energy efficiency management with digital indicators that have not been easily provided. The ship navigation information platform (SNIP) is developed to determine the dynamic information of each ship, including the fuel consumption, ship speed, horsepower of the engine, rotation speed of the engine, wind direction, and wind speed. In addition, model test data and computational fluid dynamics (CFD) calculation data are applied to calculate the energy efficiency performance indicators. Finally, the relationship of the speed through water and the speed over ground enables us to modify the effects of the ocean currents. The results verify that these indicators can be used as a reference for performance monitoring and maintenance prediction of international maritime affairs.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.795",
        "scimago_value": "1,321"
    },
    {
        "issnkey": "10619518",
        "isbn": null,
        "journal": "Journal of International Accounting, Auditing and Taxation",
        "publisher": null,
        "title": "correlatesoftheinternalauditfunctionsuseofdataanalyticsinthebigdataeraglobalevidence",
        "booktitle": null,
        "doi": "10.1016/j.intaccaudtax.2020.100357",
        "author": [
            "Rakipi, Romina",
            "{De Santis}, Federica",
            "D'Onza, Giuseppe"
        ],
        "keywords": [
            "Internal audit, Data analytics, Big data, Soft skills, Fraud detection, IT audit"
        ],
        "abstract": "In the big data era, internal audit functions (IAFs) should innovate their techniques so as to add value to their organizations. The use of data analytics (DA) increases IAFs\u2019 ability to extract value from big data, helping IAFs to enhance their activities\u2019 efficiency and effectiveness. We use responses from 1,681 Chief Audit Executives (CAEs) in 82 countries to investigate the correlates of IAFs\u2019 DA usage. From the literature, we identify five main variables expected to be associated with IAFs\u2019 DA use. We find a positive and significant association between DA use and (i) the IAF reporting to the audit committee (AC) and (ii) CAEs\u2019 ability to build positive relationships with managers. These findings suggest that IAF independence and CAEs\u2019 soft skills are important to innovate IAF techniques favoring DA use. We also find a positive association between DA use and IAFs\u2019 involvement in the assurance of enterprise risk management, fraud detection, and IT risk audit activities. Our findings contribute to the internal auditing and DA literatures, and should be of interest to CAEs, ACs, corporate boards, and professional associations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,444"
    },
    {
        "issnkey": "26666065",
        "isbn": null,
        "journal": "The Lancet Regional Health - Western Pacific",
        "publisher": null,
        "title": "healthadjustedlifeexpectancyhaleinchongqingchina2017anartificialintelligenceandbigdatamethodestimatingtheburdenofdiseaseatcitylevel",
        "booktitle": null,
        "doi": "10.1016/j.lanwpc.2021.100110",
        "author": [
            "Ruan, Xiaowen",
            "Li, Yue",
            "Jin, Xiaohui",
            "Deng, Pan",
            "Xu, Jiaying",
            "Li, Na",
            "Li, Xian",
            "Liu, Yuqi",
            "Hu, Yiyi",
            "Xie, Jingwen",
            "Wu, Yingnan",
            "Long, Dongyan",
            "He, Wen",
            "Yuan, Dongsheng",
            "Guo, Yifei",
            "Li, Heng",
            "Huang, He",
            "Yang, Shan",
            "Han, Mei",
            "Zhuang, Bojin",
            "Qian, Jiang",
            "Cao, Zhenjie",
            "Zhang, Xuying",
            "Xiao, Jing",
            "Xu, Liang"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Background A universally applicable approach that provides standard HALE measurements for different regions has yet to be developed because of the difficulties of health information collection. In this study, we developed a natural language processing (NLP) based HALE estimation approach by using individual-level electronic medical records (EMRs), which made it possible to calculate HALE timely in different temporal or spatial granularities. Methods We performed diagnostic concept extraction and normalisation on 13\u202299 million EMRs with NLP to estimate the prevalence of 254 diseases in WHO Global Burden of Disease Study (GBD). Then, we calculated HALE in Chongqing, 2017, by using the life table technique and Sullivan's method, and analysed the contribution of diseases to the expected years \u201clost\u201d due to disability (DLE). Findings Our method identified a life expectancy at birth (LE0) of 77\u20229 years and health-adjusted life expectancy at birth (HALE0) of 71\u20227 years for the general Chongqing population of 2017. In particular, the male LE0 and HALE0 were 76\u20223 years and 68\u20229 years, respectively, while the female LE0 and HALE0 were 80\u20220 years and 74\u20224 years, respectively. Cerebrovascular diseases, cancers, and injuries were the top three deterioration factors, which reduced HALE by 2\u202267, 2\u202215, and 1\u202219 years, respectively. Interpretation The results demonstrated the feasibility and effectiveness of EMRs-based HALE estimation. Moreover, the method allowed for a potentially transferable framework that facilitated a more convenient comparison of cross-sectional and longitudinal studies on HALE between regions. In summary, this study provided insightful solutions to the global ageing and health problems that the world is facing. Funding National Key R and D Program of China (2018YFC2000400).",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01692607",
        "isbn": null,
        "journal": "Computer Methods and Programs in Biomedicine",
        "publisher": null,
        "title": "managementofmedicalandhealthbigdatabasedonintegratedlearningbasedhealthcaresystemareviewandcomparativeanalysis",
        "booktitle": null,
        "doi": "10.1016/j.cmpb.2021.106293",
        "author": [
            "Ye, Yuguang",
            "Shi, Jianshe",
            "Zhu, Daxin",
            "Su, Lianta",
            "Huang, Jianlong",
            "Huang, Yifeng"
        ],
        "keywords": [
            "Integrated learning, Health care system, Elaboration Likelihood Machine, System design, Medical big data, Internet of Medical Things"
        ],
        "abstract": "Purpose We present a Health Care System (HCS) based on integrated learning to achieve high-efficiency and high-precision integration of medical and health big data, and compared it with an internet-based integrated system. Method The method proposed in this paper adopts the Bagging integrated learning method and the Extreme Learning Machine (ELM) prediction model to obtain a high-precision strong learning model. In order to verify the integration efficiency of the system, we compare it with the Internet-based health big data integration system in terms of integration volume, integration efficiency, and storage space capacity. Results The HCS based on integrated learning relies on the Internet in terms of integration volume, integration efficiency, and storage space capacity. The amount of integration is proportional to the time and the integration time is between 170-450 ms, which is only half of the comparison system; whereby the storage space capacity reaches 8.3\u00d728TB. Conclusion The experimental results show that the integrated learning-based HCS integrates medical and health big data with high integration volume and integration efficiency, and has high space storage capacity and concurrent data processing performance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.428",
        "scimago_value": "0,924"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821633-0",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter3bigdatabasedframeworksforhealthcaresystems",
        "booktitle": "Demystifying Big Data, Machine Learning, and Deep Learning for Healthcare Analytics",
        "doi": "10.1016/B978-0-12-821633-0.00003-9",
        "author": [
            "Ilmudeen, Aboobucker"
        ],
        "keywords": [
            "Big data, Frameworks, Healthcare, Healthcare systems"
        ],
        "abstract": "Today, the term big data involves various applications, technologies, architectures, services, and standards in the healthcare domain. The advanced technology and healthcare systems have been extremely knotted together in recent times. As the adoption of wearable biosensors and their applications have begun across the world, eHealth and mHealth have emerged. Hence, wearable sensor devices produce organized and unorganized big data that cannot be easily processed and analyzed due to its complexity; that hinders effective medical decision making. Modern advances in healthcare systems have increased the size of health records such as through electronic health records, patient care, clinical reports, regulations, and compliance requirements. However, the present data-processing technologies are not capable of handling the growing amount of large datasets. This chapter discusses theoretical illustrations that pinpoint various aspects of big data-related frameworks and proposes a large data-based conceptual framework in healthcare systems.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-08-102672-4",
        "journal": null,
        "publisher": "Elsevier",
        "title": "transportmodesandbigdata",
        "booktitle": "International Encyclopedia of Transportation",
        "doi": "10.1016/B978-0-08-102671-7.10601-3",
        "author": [
            "Budnitz, Hannah",
            "Tranos, Emmanouil",
            "Chapman, Lee"
        ],
        "keywords": [
            "Application programming interfaces, Automation, Big data, Crowd-sourced, Digitization, Geolocation, Information and communication technology, Intelligent transport systems, Internet of things, Location-based services, Mobility as a service, Real time information systems, Timestamp, Vehicle telematics"
        ],
        "abstract": "Transport modes and big data considers the characteristics of \u201cBig Data\u201d as described by the 5 \u201cVs,\u201d Volume, Velocity, Variety, Veracity, and Value. It reviews the many sources of big data within the transport sector by modal group, and the sources of big data from the Information and Communication Technology (ICT) sector that may be applied to the understanding of transport. It briefly considers the uses, advantages, and disadvantages of these data sources, and the challenges to analyzing and interpreting them. It concludes that Big Data is necessary to prepare for the uncertain future of transport, but recognizes the challenges to applying it accurately and effectively to transport problems and policies.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "researchonbigdataanalysismodelofmultienergypowergenerationconsideringpollutantemissionempiricalanalysisfromshanxiprovince",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.128154",
        "author": [
            "Ren, Dongfang",
            "Guo, Xiaopeng",
            "Li, Cunbin"
        ],
        "keywords": [
            "Multi-energy power generation, Pollutant emission, Big data analysis, Renewable energy generation, Thermal power"
        ],
        "abstract": "With the development of the integrated energy Internet, energy structure optimization and emission reduction have led to higher requirements for developing various energy sources to enable coordinated and sustainable development. However, data-mining methods are rarely used to study the coordination of multi-energy generation in published research results. In this study, from the perspective of power industry emissions, coordinated generation of various energy sources, and balance of power generation and consumption, a data-mining algorithm was used to analyze the development of thermal power, hydropower, wind power, waste heat, gas, and other power sources. The chi-square automatic interaction detection tree (CHAID), logistic regression, and two-step clustering methods were applied. The results show that: a) CO2 and SO2 emissions were mainly affected by thermal power generation, whereas NOx emissions were jointly affected by thermal power, garbage power, and gas-fired power, and the emissions of various pollutants increased with an increase in power consumption. The optimal power-generation scheme under minimum emission can be obtained. b) There was a strong correlation between thermal power generation and residential electricity consumption, and renewable energy (wind energy, photovoltaic, hydropower) exhibited the highest correlation with the electricity consumption of the tertiary industry, which indicates that renewable energy generation can be promoted by managing electricity consumption in the tertiary industry. c) When the electricity demand of all users was small, the proportion of renewable energy power generation increased; in contrast, the thermal power generation was larger. This indicates the importance of improving the sustainable and stable power supply of renewable energy. This study provides a data analysis model for the coordinated development of multiple energies, which will contribute to the decision-making basis for controlling power emissions, improving the utilization rate of renewable energy, and optimizing the energy structure.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "07365845",
        "isbn": null,
        "journal": "Robotics and Computer-Integrated Manufacturing",
        "publisher": null,
        "title": "abigdatadrivenframeworkforsustainableandsmartadditivemanufacturing",
        "booktitle": null,
        "doi": "10.1016/j.rcim.2020.102026",
        "author": [
            "Majeed, Arfan",
            "Zhang, Yingfeng",
            "Ren, Shan",
            "Lv, Jingxiang",
            "Peng, Tao",
            "Waqar, Saad",
            "Yin, Enhuai"
        ],
        "keywords": [
            "Big data, Additive manufacturing, Sustainable manufacturing, Smart manufacturing, Optimization"
        ],
        "abstract": "From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.666",
        "scimago_value": "1,561"
    },
    {
        "issnkey": "09609822",
        "isbn": null,
        "journal": "Current Biology",
        "publisher": null,
        "title": "fungalbiodiversityandconservationmycologyinlightofnewtechnologybigdataandchangingattitudes",
        "booktitle": null,
        "doi": "10.1016/j.cub.2021.06.083",
        "author": [
            "Lofgren, Lotus",
            "Stajich, Jason"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Summary Fungi have successfully established themselves across seemingly every possible niche, substrate, and biome. They are fundamental to biogeochemical cycling, interspecies interactions, food production, and drug bioprocessing, as well as playing less heroic roles as difficult to treat human infections and devastating plant pathogens. Despite community efforts to estimate and catalog fungal diversity, we have only named and described a minute fraction of the fungal world. The identification, characterization, and conservation of fungal diversity is paramount to preserving fungal bioresources, and to understanding and predicting ecosystem cycling and the evolution and epidemiology of fungal disease. Although species and ecosystem conservation are necessarily the foundation of preserving this diversity, there is value in expanding our definition of conservation to include the protection of biological collections, ecological metadata, genetic and genomic data, and the methods and code used for our analyses. These definitions of conservation are interdependent. For example, we need metadata on host specificity and biogeography to understand rarity and set priorities for conservation. To aid in these efforts, we need to draw expertise from diverse fields to tie traditional taxonomic knowledge to data obtained from modern -omics-based approaches, and support the advancement of diverse research perspectives. We also need new tools, including an updated framework for describing and tracking species known only from DNA, and the continued integration of functional predictions to link genetic diversity to functional and ecological diversity. Here, we review the state of fungal diversity research as shaped by recent technological advancements, and how changing viewpoints in taxonomy, -omics, and systematics can be integrated to advance mycological research and preserve fungal biodiversity.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "artificialintelligencetechniquesforenablingbigdataservicesindistributionnetworksareview",
        "booktitle": null,
        "doi": "10.1016/j.rser.2021.111459",
        "author": [
            "Barja-Martinez, Sara",
            "Arag\u00fc\u00e9s-Pe\u00f1alba, M\u00f2nica",
            "Munn\u00e9-Collado, \u00cdngrid",
            "Lloret-Gallego, Pau",
            "Bullich-Massagu\u00e9, Eduard",
            "Villafafila-Robles, Roberto"
        ],
        "keywords": [
            "Machine learning, Deep learning, Smart grid, Distribution grid, Smart energy service"
        ],
        "abstract": "Artificial intelligence techniques lead to data-driven energy services in distribution power systems by extracting value from the data generated by the deployed metering and sensing devices. This paper performs a holistic analysis of artificial intelligence applications to distribution networks, ranging from operation, monitoring and maintenance to planning. The potential artificial intelligence techniques for power system applications and needed data sources are identified and classified. The following data-driven services for distribution networks are analyzed: topology estimation, observability, fraud detection, predictive maintenance, non-technical losses detection, forecasting, energy management systems, aggregated flexibility services and trading. A review of the artificial intelligence methods implemented in each of these services is conducted. Their interdependencies are mapped, proving that multiple services can be offered as a single clustered service to different stakeholders. Furthermore, the dependencies between the AI techniques with each energy service are identified. In recent years there has been a significant rise of deep learning applications for time series prediction tasks. Another finding is that unsupervised learning methods are mainly being applied to customer segmentation, buildings efficiency clustering and consumption profile grouping for non-technical losses detection. Reinforcement learning is being widely applied to energy management systems design, although more testing in real environments is needed. Distribution network sensorization should be enhanced and increased in order to obtain larger amounts of valuable data, enabling better service outcomes. Finally, the future opportunities and challenges for applying artificial intelligence in distribution grids are discussed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820203-6",
        "journal": null,
        "publisher": "Academic Press",
        "title": "4healthcareandmedicalbigdataanalytics",
        "booktitle": "Applications of Big Data in Healthcare",
        "doi": "10.1016/B978-0-12-820203-6.00005-9",
        "author": [
            "Ristevski, Blagoj",
            "Savoska, Snezana"
        ],
        "keywords": [
            "Big Data, medical and healthcare Big Data, Big Data Analytics, databases, healthcare information systems"
        ],
        "abstract": "In the era of big data, a huge volume of heterogeneous healthcare and medical data are generated daily. These heterogeneous data, that are stored in diverse data formats, have to be integrated and stored in a standard way and format to perform suitable efficient and effective data analysis and visualization. These data, which are generated from different sources such as mobile devices, sensors, lab tests, clinical notes, social media, demographics data, diverse omics data, etc., can be structured, semistructured, or unstructured. These varieties of data structures require these big data to be stored not only in the standard relational databases but also in NoSQL databases. To provide effective data analysis, suitable classification and standardization of big data in medicine and healthcare are necessary, as well as excellent design and implementation of healthcare information systems. Regarding the security and privacy of the patient\u2019s data, we suggest employing suitable data governance policies. Additionally, we suggest choosing of proper software development frameworks, tools, databases, in-database analytics, stream computing and data mining algorithms (supervised, unsupervised and semisupervised) to reveal valuable knowledge and insights from these healthcare and medical big data. Ultimately we propose the development of not only patient-oriented but also decision- and population-centric healthcare information systems.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822884-5",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter8theuseofbigdatainpsychiatrytheroleofadministrativedatabases",
        "booktitle": "Big Data in Psychiatry #x0026; Neurology",
        "doi": "10.1016/B978-0-12-822884-5.00009-X",
        "author": [
            "Gon\u00e7alves-Pinho, Manuel",
            "Freitas, Alberto"
        ],
        "keywords": [
            "Administrative database, Mental Health, Secondary data, Psychiatry, Research design"
        ],
        "abstract": "Administrative databases (AD) are repositories of administrative and clinical data related to patient contact episodes with all sorts of health facilities (primary care, hospitals, pharmacies, etc.). The use of AD data is increasing in Mental Health research as the advantages of using AD surpass some of the difficulties Mental health researchers find when using data from other sources (clinical trials, cohort studies, etc.). The large number of patients/contact episodes available, the systematic and broad register, and the fact that AD provides real-world data are some of the pros in using AD data. There are some methodological aspects that must be addressed when using this type of databases in order to provide solid and valid results. The possibility of clinical and administrative errors in an AD is a reality when using secondary data in Mental Health Research, and diagnostic code validation studies may be performed to estimate clinical and administrative accuracy. This chapter described in detail the pros and cons of using secondary data in mental health research and specifies the methodological steps a researcher must follow in order to find valid conclusions in AD from a clinical point of view.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02684012",
        "isbn": null,
        "journal": "International Journal of Information Management",
        "publisher": null,
        "title": "usingbigdataforcoinnovationprocessesmappingthefieldofdatadriveninnovationproposingtheoreticaldevelopmentsandprovidingaresearchagenda",
        "booktitle": null,
        "doi": "10.1016/j.ijinfomgt.2021.102347",
        "author": [
            "Bresciani, Stefano",
            "Ciampi, Francesco",
            "Meli, Francesco",
            "Ferraris, Alberto"
        ],
        "keywords": [
            "Big data, Co-innovation, Open innovation, Bibliometric analysis, Literature review"
        ],
        "abstract": "This is the first systematic literature review concerning the interconnections between big data (BD) and co-innovation. It uses BD as a common perspective of analysis as well as a concept aggregating different research streams (open innovation, co-creation and collaborative innovation). The review is based on the results of a bibliographic coupling analysis performed with 51 peer-reviewed papers published before the end of 2019. Three thematic clusters were discovered, which respectively focused on BD as a knowledge creation enabler within co-innovation contexts, BD as a driver of co-innovation processes based on customer engagement, and the impact of BD on co-innovation within service ecosystems. The paper theoretically argues that the use of BD, in addition to enhancing intentional and direct collaborative innovation processes, allows the development of passive and unintentional co-innovation that can be implemented through indirect relationships between the collaborative actors. This study also makes eleven unique research propositions concerning further theoretical developments and managerial implementations in the field of BD-driven co-innovation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "14.098",
        "scimago_value": "2,770"
    },
    {
        "issnkey": "23524847",
        "isbn": null,
        "journal": "Energy Reports",
        "publisher": null,
        "title": "visualrecognitionprocessingofpowermonitoringdatabasedonbigdatacomputing",
        "booktitle": null,
        "doi": "10.1016/j.egyr.2021.09.205",
        "author": [
            "Qian, Jianguo",
            "Zhu, Bingquan",
            "Li, Ying",
            "Shi, Zhengchai"
        ],
        "keywords": [
            "Power control data, Monitoring, Visual identification, Iterative screening, CARIMA"
        ],
        "abstract": "The operation control of power units is usually carried out by the control personnel with the help of distributed control system. Although it can ensure the safety of unit operation and meet the requirements of power generation loads, the economy of unit operation and the accuracy of control process still need to be further improved. Therefore, by designing multiple view mapping and association, it provides interactive visualization support for relevant experts in the key links of model establishment and evaluation. In the exploration stage of estimating model parameter, the user can get the delay range by line chart and focus + context technology, while in the model screening stage, the user can provide the combination of screening views, selecting the model by its accuracy on different data sets, and finding the model anomalies by the model structure view. Besides, in the model evaluation stage, the user can get the delay range by predicting line chart and model accuracy radar chart. In addition, the method in this paper keeps between 4.2\u20137.2 in most distributions, and the maximum value is 18. The time series trend of the data segment is consistent, and the absolute value of the weight coefficient is basically 0 after being superimposed, which has great advantages compared with other methods, proving the effective results of the research content in this paper.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.870",
        "scimago_value": "1,199"
    },
    {
        "issnkey": "03601323",
        "isbn": null,
        "journal": "Building and Environment",
        "publisher": null,
        "title": "buildinglifespanpredictionforlifecycleassessmentandlifecyclecostusingmachinelearningabigdataapproach",
        "booktitle": null,
        "doi": "10.1016/j.buildenv.2021.108267",
        "author": [
            "Ji, Sukwon",
            "Lee, Bumho",
            "Yi, Mun"
        ],
        "keywords": [
            "Building life span, Life cycle cost, Life cycle assessment, Big data, Machine learning, Deep neural network"
        ],
        "abstract": "Life cycle assessment (LCA) and life cycle cost (LCC) are two primary methods used to assess the environmental and economic feasibility of building construction. An estimation of the building's life span is essential to carrying out these methods. However, given the diverse factors that affect the building's life span, it was estimated typically based on its main structural type. However, different buildings have different life spans. Simply assuming that all buildings with the same structural type follow an identical life span can cause serious estimation errors. In this study, we collected 1,812,700 records describing buildings built and demolished in South Korea, analysed the actual life span of each building, and developed a building life-span prediction model using deep-learning and traditional machine learning. The prediction models examined in this study produced root mean square errors of 3.72\u20134.6 and the coefficients of determination of 0.932\u20130.955. Among those models, a deep-learning based prediction model was found the most powerful. As anticipated, the conventional method of determining a building's life expectancy using a discrete set of specific factors and associated assumptions of life span did not yield realistic results. This study demonstrates that an application of deep learning to the LCA and LCC of a building is a promising direction, effectively guiding business planning and critical decision making throughout the construction process.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.456",
        "scimago_value": "1,736"
    },
    {
        "issnkey": "25426605",
        "isbn": null,
        "journal": "Internet of Things",
        "publisher": null,
        "title": "conceptualizationandscalableexecutionofbigdataworkflowsusingdomainspecificlanguagesandsoftwarecontainers",
        "booktitle": null,
        "doi": "10.1016/j.iot.2021.100440",
        "author": [
            "Nikolov, Nikolay",
            "Dessalk, Yared",
            "Khan, Akif",
            "Soylu, Ahmet",
            "Matskin, Mihhail",
            "Payberah, Amir",
            "Roman, Dumitru"
        ],
        "keywords": [
            "Big data workflows, Internet of Things, Domain-specific languages, Software containers"
        ],
        "abstract": "Big Data processing, especially with the increasing proliferation of Internet of Things (IoT) technologies and convergence of IoT, edge and cloud computing technologies, involves handling massive and complex data sets on heterogeneous resources and incorporating different tools, frameworks, and processes to help organizations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, requires taking advantage of Cloud infrastructures\u2019 elasticity for scalability. In this article, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies, message-oriented middleware (MOM), and a domain-specific language (DSL) to enable highly scalable workflow execution and abstract workflow definition. We demonstrate our system in a use case and a set of experiments that show the practical applicability of the proposed approach for the specification and scalable execution of Big Data workflows. Furthermore, we compare our proposed approach\u2019s scalability with that of Argo Workflows \u2013 one of the most prominent tools in the area of Big Data workflows \u2013 and provide a qualitative evaluation of the proposed DSL and overall approach with respect to the existing literature.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22145796",
        "isbn": null,
        "journal": "Big Data Research",
        "publisher": null,
        "title": "editorialtothespecialissueonbigdatainindustrialandcommercialapplications",
        "booktitle": null,
        "doi": "10.1016/j.bdr.2021.100244",
        "author": [
            "Lundberg, Lars",
            "Grahn, H\u00e5kan",
            "Cardellini, Valeria",
            "Polze, Andreas",
            "Shirinbab, Sogand"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.578",
        "scimago_value": "0,565"
    },
    {
        "issnkey": "00457906",
        "isbn": null,
        "journal": "Computers & Electrical Engineering",
        "publisher": null,
        "title": "satcepmonitoranairqualitymonitoringsoftwarearchitecturecombiningcomplexeventprocessingwithsatelliteremotesensing",
        "booktitle": null,
        "doi": "10.1016/j.compeleceng.2021.107257",
        "author": [
            "Semlali, Badr-Eddine",
            "Amrani, Chaker",
            "Ortiz, Guadalupe",
            "Boubeta-Puig, Juan",
            "Garcia-de-Prado, Alfonso"
        ],
        "keywords": [
            "Remote sensing, Satellite sensors, Air quality, Complex event processing, Big data, Decision-making"
        ],
        "abstract": "Air pollution is a major problem today that causes serious damage to human health. Urban areas are the most affected by the degradation of air quality caused by anthropogenic gas emissions. Although there are multiple proposals for air quality monitoring, in most cases, two limitations are imposed: the impossibility of processing data in Near Real-Time (NRT) for remote sensing approaches and the impossibility of reaching areas of limited accessibility or low network coverage for ground data approaches. We propose a software architecture that efficiently combines complex event processing with remote sensing data from various satellite sensors to monitor air quality in NRT, giving support to decision-makers. We illustrate the proposed solution by calculating the air quality levels for several areas of Morocco and Spain, extracting and processing satellite information in NRT. This study also validates the air quality measured by ground stations and satellite sensor data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,630"
    },
    {
        "issnkey": "20010370",
        "isbn": null,
        "journal": "Computational and Structural Biotechnology Journal",
        "publisher": null,
        "title": "aiapplicationsinfunctionalgenomics",
        "booktitle": null,
        "doi": "10.1016/j.csbj.2021.10.009",
        "author": [
            "Caudai, Claudia",
            "Galizia, Antonella",
            "Geraci, Filippo",
            "{Le Pera}, Loredana",
            "Morea, Veronica",
            "Salerno, Emanuele",
            "Via, Allegra",
            "Colombo, Teresa"
        ],
        "keywords": [
            "Artificial intelligence, Functional genomics, Genomics, Proteomics, Epigenomics, Transcriptomics, Epitranscriptomics, Metabolomics, Machine learning, Deep learning"
        ],
        "abstract": "We review the current applications of artificial intelligence (AI) in functional genomics. The recent explosion of AI follows the remarkable achievements made possible by \u201cdeep learning\u201d, along with a burst of \u201cbig data\u201d that can meet its hunger. Biology is about to overthrow astronomy as the paradigmatic representative of big data producer. This has been made possible by huge advancements in the field of high throughput technologies, applied to determine how the individual components of a biological system work together to accomplish different processes. The disciplines contributing to this bulk of data are collectively known as functional genomics. They consist in studies of: i) the information contained in the DNA (genomics); ii) the modifications that DNA can reversibly undergo (epigenomics); iii) the RNA transcripts originated by a genome (transcriptomics); iv) the ensemble of chemical modifications decorating different types of RNA transcripts (epitranscriptomics); v) the products of protein-coding transcripts (proteomics); and vi) the small molecules produced from cell metabolism (metabolomics) present in an organism or system at a given time, in physiological or pathological conditions. After reviewing main applications of AI in functional genomics, we discuss important accompanying issues, including ethical, legal and economic issues and the importance of explainability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.271",
        "scimago_value": "1,908"
    },
    {
        "issnkey": "23525509",
        "isbn": null,
        "journal": "Sustainable Production and Consumption",
        "publisher": null,
        "title": "evaluatingandrankingsecondarydatasourcestobeusedinthebrazilianlcadatabasesicvbrasil",
        "booktitle": null,
        "doi": "10.1016/j.spc.2020.09.021",
        "author": [
            "{Shirosaki Mar\u00e7al de Souza}, Luri",
            "Nunes, Andr\u00e9a",
            "Giusti, Gabriela",
            "Saavedra, Yovana",
            "Rodrigues, Thiago",
            "{Nunes Braga}, Tiago",
            "{Lopes Silva}, Diogo"
        ],
        "keywords": [
            "Qualidata guide, Data quality, Data format, Life Cycle Inventory, Weighting factors"
        ],
        "abstract": "The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System \u2013 the \u201cSICV Brasil\u201d launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.032",
        "scimago_value": "1,019"
    },
    {
        "issnkey": "00457906",
        "isbn": null,
        "journal": "Computers & Electrical Engineering",
        "publisher": null,
        "title": "towardsthedefinitionofaninformationqualitymetricforinformationfusionmodels",
        "booktitle": null,
        "doi": "10.1016/j.compeleceng.2020.106907",
        "author": [
            "Paggi, Horacio",
            "Soriano, Javier",
            "Lara, Juan",
            "Damiani, Ernesto"
        ],
        "keywords": [
            "Adaptive Peer-to-Peer systems, Information fusion, Uncertain information handling, Information quality metric"
        ],
        "abstract": "Managing information quality has become important in cyber-physical systems dealing with big data. In this regard, different models have been proposed, mainly in flat peer-to-peer networks, in which exchanging information efficiently is a key aspect due to scarce resources. However, little research has been conducted on information quality metrics for cyber-physical scenarios. In this paper, we propose an information quality metric and show its application to an information fusion model. It is a \u201cmodel-oriented quality metric\u201d since it allows non-predefined variants on its configuration depending on the application domain. The model was tested on several simulations using open datasets. The results obtained in the performance of the model confirm the validity of the information quality metric, proposed in this paper, on which the model is based. The model may have a wide variety of applications such as mobile recommendation or decision making in critical environments (emergencies, war, and so on).",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,630"
    },
    {
        "issnkey": "22147853",
        "isbn": null,
        "journal": "Materials Today: Proceedings",
        "publisher": null,
        "title": "astudyonartificialintelligenceformonitoringsmartenvironments",
        "booktitle": null,
        "doi": "10.1016/j.matpr.2021.06.046",
        "author": [
            "D., Karthika"
        ],
        "keywords": [
            "Big data analytics, Machine learning, Internet of things, Smart cities, Wireless technologies"
        ],
        "abstract": "Wireless networking has made enormous improvements. These developments have brought about new paradigms of wireless networking and communications Environmental protection has been in recent years a more intelligent and linked system for all facets of a global city. With the rise in data gathering, machine learning (ML) approaches can be used to boost the knowledge and the skill of an application. As the numbers increase and technology develops, the number of available data increases. Smart collection and interpretation of these Big Data is the underground to the rising of smart Internet of Things IoT apps. This study discovers the diverse methods of machine learning that resolve data difficulties in smart cities. The discussion takes place on applications such as air quality, water pollution, radiation pollution, smart buildings, smart transport, etc., which pose genuine environmental challenges. Adequate monitoring is needed to ensure sustainable growth in the world by safeguarding a healthy society. The potential and challenges in particular the role of machine learning technology for the Internet of Things and Big Data Analytics.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,341"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818015-0",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter5structuringyourproject",
        "booktitle": "Executing Data Quality Projects (Second Edition)",
        "doi": "10.1016/B978-0-12-818015-0.00001-3",
        "author": [
            "McGilvray, Danette"
        ],
        "keywords": [
            "Solution Development Life Cycle (SDLC), Agile, Scrum, sequential, waterfall, project objectives, project team, project roles, project timing, project approach"
        ],
        "abstract": "It is essential that those using the Ten Steps do a good job of organizing their work. This chapter guides readers\u2019 choices when setting up their projects and assembling a project team. Three general types of projects are detailed: 1) Focused data quality improvement project, 2) Data quality activities in another project, such as application development, data migration or integration of any kind, and 3) Ad hoc use of data quality steps, activities, or techniques from the Ten Steps. Additional information is given for incorporating data quality activities into another project using various SDLCs (solution/system/software life cycles). Relevant data quality activities from the Ten Steps can be incorporated into any SDLC that is the basis for the larger project (Agile, sequential, hybrid, etc.). To that end, several tables list data governance, stewardship, data quality and readiness activities and where they would take place in typical SDLC phases. A table with Agile Scrum activities are cross-referenced to the same SDLC phases. The chapter concludes with general tips for project timing, communication, and engagement.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26668270",
        "isbn": null,
        "journal": "Machine Learning with Applications",
        "publisher": null,
        "title": "machinelearningpredictionsforlosttimeinjuriesinpowertransmissionanddistributionprojects",
        "booktitle": null,
        "doi": "10.1016/j.mlwa.2021.100158",
        "author": [
            "Oyedele, Ahmed",
            "Ajayi, Anuoluwapo",
            "Oyedele, Lukumon"
        ],
        "keywords": [
            "Hazard assessment, Deep learning, Big data predictive analytics, Power infrastructure, Zero count data"
        ],
        "abstract": "Although advanced machine learning algorithms are predominantly used for predicting outcomes in many fields, their utilisation in predicting incident outcome in construction safety is still relatively new. This study harnesses Big Data with Deep Learning to develop a robust safety management system by analysing unstructured incident datasets consisting of 168,574 data points from power transmission and distribution projects delivered across the UK from 2004 to 2016. This study compared Deep Learning performance with popular machine learning algorithms (support vector machine, random forests, multivariate adaptive regression splines, generalised linear model, and their ensembles) concerning lost time injury and risk assessment in power utility projects. Deep Learning gave the best prediction for safety outcomes with high skills (AUC = 0.95, R2 = 0.88, and multi-class ROC = 0.93), thus outperforming the other algorithms. The results from this study also highlight the significance of quantitative analysis of empirical data in safety science and contribute to an enhanced understanding of injury patterns using predictive analytics in conjunction with safety experts\u2019 perspectives. Additionally, the results will enhance the skills of safety managers in the power utility domain to advance safety intervention efforts.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26671026",
        "isbn": null,
        "journal": "Intelligent Medicine",
        "publisher": null,
        "title": "constructionofanartificialintelligencesystemindermatologyeffectivenessandconsiderationofchineseskinimagedatabasecsid",
        "booktitle": null,
        "doi": "10.1016/j.imed.2021.04.003",
        "author": [
            "Li, Chengxu",
            "Fei, Wenmin",
            "Han, Yang",
            "Ning, Xiaoli",
            "Wang, Ziyi",
            "Li, Keke",
            "Xue, Ke",
            "Xu, Jingkai",
            "Yu, Ruixing",
            "Meng, Rusong",
            "Xu, Feng",
            "Ma, Weimin",
            "Cui, Yong"
        ],
        "keywords": [
            "Artificial intelligence, Dermatology, Skin image, Chinese Skin Image Database"
        ],
        "abstract": "After more than 60 years of development, artificial intelligence (AI) has been widely used in various fields. Especially in recent years, with the development of deep learning, AI has made many remarkable achievements in the medical field. Dermatology, as a clinical discipline with morphology as its main feature, is particularly suitable for the development of AI. The rapid development of skin imaging technology has helped dermatologists to assist in the diagnosis of diseases and has greatly improved the accuracy of diagnosis. Skin imaging data have natural big data attributes, which is important for AI research. The establishment of the Chinese Skin Image Database (CSID) has solved many problems such as isolated data islands and inconsistent data quality. Based on the CSID, many pioneering achievements have been made in the research and development of AI-assisted decision-making software, the establishment of expert organizations, personnel training, scientific research, and so on. At present, there are still many problems with AI in the field of dermatology, such as clinical validation, medical device licensing, interdisciplinary, and standard formulation, which urgently need to be solved by joint efforts of all parties.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01692607",
        "isbn": null,
        "journal": "Computer Methods and Programs in Biomedicine",
        "publisher": null,
        "title": "automateddataqualitycontrolinfdopabrainpetimagingusingdeeplearning",
        "booktitle": null,
        "doi": "10.1016/j.cmpb.2021.106239",
        "author": [
            "Pontoriero, Antonella",
            "Nordio, Giovanna",
            "Easmin, Rubaida",
            "Giacomel, Alessio",
            "Santangelo, Barbara",
            "Jahuar, Sameer",
            "Bonoldi, Ilaria",
            "Rogdaki, Maria",
            "Turkheimer, Federico",
            "Howes, Oliver",
            "Veronese, Mattia"
        ],
        "keywords": [
            "FDOPA, PET, quality control, QC, convolutional neural networks"
        ],
        "abstract": "ABSTRACT Introduction. With biomedical imaging research increasingly using large datasets, it becomes critical to find operator-free methods to quality control the data collected and the associated analysis. Attempts to use artificial intelligence (AI) to perform automated quality control (QC) for both single-site and multi-site datasets have been explored in some neuroimaging techniques (e.g. EEG or MRI), although these methods struggle to find replication in other domains. The aim of this study is to test the feasibility of an automated QC pipeline for brain [18F]-FDOPA PET imaging as a biomarker for the dopamine system. Methods. Two different Convolutional Neural Networks (CNNs) were used and combined to assess spatial misalignment to a standard template and the signal-to-noise ratio (SNR) relative to 200 static [18F]-FDOPA PET images that had been manually quality controlled from three different PET/CT scanners. The scans were combined with an additional 400 scans, in which misalignment (200 scans) and low SNR (200 scans) were simulated. A cross-validation was performed, where 80% of the data were used for training and 20% for validation. Two additional datasets of [18F]-FDOPA PET images (50 and 100 scans respectively with at least 80% of good quality images) were used for out-of-sample validation. Results. The CNN performance was excellent in the training dataset (accuracy for motion: 0.86 \u00b1 0.01, accuracy for SNR: 0.69 \u00b1 0.01), leading to 100% accurate QC classification when applied to the two out-of-sample datasets. Data dimensionality reduction affected the generalizability of the CNNs, especially when the classifiers were applied to the out-of-sample data from 3D to 1D datasets. Conclusions. This feasibility study shows that it is possible to perform automatic QC of [18F]-FDOPA PET imaging with CNNs. The approach has the potential to be extended to other PET tracers in both brain and non-brain applications, but it is dependent on the availability of large datasets necessary for the algorithm training.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.428",
        "scimago_value": "0,924"
    },
    {
        "issnkey": "2352152x",
        "isbn": null,
        "journal": "Journal of Energy Storage",
        "publisher": null,
        "title": "remainingusefullifepredictionwithprobabilitydistributionforlithiumionbatteriesbasedonedgeandcloudcollaborativecomputation",
        "booktitle": null,
        "doi": "10.1016/j.est.2021.103342",
        "author": [
            "Zhou, Yong",
            "Gu, Huanghui",
            "Su, Teng",
            "Han, Xuebing",
            "Lu, Languang",
            "Zheng, Yuejiu"
        ],
        "keywords": [
            "Battery life prediction, RVM optimization prediction, Parameter transfer: RUL probability prediction"
        ],
        "abstract": "This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2\u03c3 range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00190578",
        "isbn": null,
        "journal": "ISA Transactions",
        "publisher": null,
        "title": "dataconsistencymethodofheterogeneouspoweriotbasedonhybridmodel",
        "booktitle": null,
        "doi": "10.1016/j.isatra.2021.01.056",
        "author": [
            "Jiang, Haoyu",
            "Chen, Kai",
            "Ge, Quanbo",
            "Xu, Jinqiang",
            "Fu, Yingying",
            "Li, Chunxi"
        ],
        "keywords": [
            "Power IOT system, Hybrid model, Heterogeneous data consistency, Machine learning combination method"
        ],
        "abstract": "The data of the power Internet of Things (IOT) system is transferred from the IaaS layer to the SaaS layer. The general data preprocessing method mainly solves the problem of big data anomalies and missing at the PaaS layer, but it still lacks the ability to judge the high error data that meets the timing characteristics, making it difficult to deal with heterogeneous power inconsistent issues. This paper shows this phenomenon and its physical mechanism, showing the difficulty of building a quantitative model forward. A data-driven method is needed to form a hybrid model to correct the data. The research object is the electricity meter data on both sides of a commercial building transformer, which comes from different power IOT systems. The low-voltage side was revised based on the high-voltage side. Compared with the correction method based on purely using neural networks, the combined method, Linear Regression (LS) + Differential Evolution (DE) + Extreme Learning Machine (ELM), further reduces the deviation from approximately 4% to 1%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.468",
        "scimago_value": "1,147"
    },
    {
        "issnkey": "23522496",
        "isbn": null,
        "journal": "Food Webs",
        "publisher": null,
        "title": "integratingtrophicdatafromtheliteraturethedanuberiverfoodweb",
        "booktitle": null,
        "doi": "10.1016/j.fooweb.2021.e00203",
        "author": [
            "Patonai, Katalin",
            "Jord\u00e1n, Ferenc"
        ],
        "keywords": [
            "Aggregation, Danube River, Food web, Incomplete data, Taxonomy"
        ],
        "abstract": "In the era of bioinformatics and big data, ecological research depends on large and easily accessible databases that make it possible to construct complex system models. Open-access data repositories for food webs via publications and ecological databases (e.g. EcoBase) are becoming increasingly common, yet certain ecosystem types are underrepresented (e.g. rivers). In this paper, we compile the trophic connections (predator-prey relationships) for the Danube River ecosystem as gathered from globally available literature data. Data are analyzed by Danube regions separately (Upper, Middle, Lower Danube) as well as an integrated master network version. The master version has been aggregated into larger taxonomic categories. Local and global metrics were used to analyze and compare each network. We find disparity between regions (the Middle Danube having most nodes, but still quite heterogenous), we identify the most important trophic groups, and explain ways on evaluating missing data using each aggregation stage. This data-driven approach, summarizing our presently documented knowledge, can be used for preparing preliminary models and to further refine the Danube River food web in the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,847"
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "examiningthedeterminantsofsuccessfuladoptionofdataanalyticsinhumanresourcemanagementaframeworkforimplications",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2021.03.054",
        "author": [
            "Shet, Sateesh.V.",
            "Poddar, Tanuj",
            "{Wamba Samuel}, Fosso",
            "Dwivedi, Yogesh"
        ],
        "keywords": [
            "Human resource analytics, HRM analytics, People analytics, Adoption of HR analytics, Challenges, Implementation of HR analytics, Big data, Data analytics, Framework synthesis"
        ],
        "abstract": "Data analytics has gained importance in human resource management (HRM) for its ability to provide insights based on data-driven decision-making processes. However, integrating an analytics-based approach in HRM is a complex process, and hence, many organizations are unable to adopt HR Analytics (HRA). Using a framework synthesis approach, we first identify the challenges that hinder the practice of HRA and then develop a framework to explain the different factors that impact the adoption of HRA within organizations. This study identifies the key aspects related to the technological, organizational, environmental, data governance, and individual factors that influence the adoption of HRA. In addition, this paper determines 23 sub-dimensions of these five factors as the crucial aspects for successfully implementing and practicing HRA within organizations. We also discuss the implications of the framework for HR leaders, HR Managers, CEOs, IT Managers and consulting practitioners for effective adoption of HRA in organization.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "predictivemaintenancesystemforproductionlinesinmanufacturingamachinelearningapproachusingiotdatainrealtime",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2021.114598",
        "author": [
            "Ayvaz, Serkan",
            "Alpay, Koray"
        ],
        "keywords": [
            "Predictive maintenance, Internet of things, Manufacturing systems, Artificial intelligence, Machine learning, Big data"
        ],
        "abstract": "In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "00298018",
        "isbn": null,
        "journal": "Ocean Engineering",
        "publisher": null,
        "title": "advanceddataanalyticsforshipperformancemonitoringunderlocalizedoperationalconditions",
        "booktitle": null,
        "doi": "10.1016/j.oceaneng.2021.109392",
        "author": [
            "Bui, Khanh",
            "Perera, Lokukaluge"
        ],
        "keywords": [
            "Big data analytics, Machine learning, Ship performance monitoring, Energy efficiency, Emission control, Data anomaly detection"
        ],
        "abstract": "Improving the operational energy efficiency of existing ships is attracting considerable interests to reduce the environmental footprint due to air emissions. As the shipping industry is entering into Shipping 4.0 with digitalization as a disruptive force, an intriguing area in the field of ship\u2019s operational energy efficiency is big data analytics. This paper proposes a big data analytics framework for ship performance monitoring under localized operational conditions with the help of appropriate data analytics together with domain knowledge. The proposed framework is showcased through a data set obtained from a bulk carrier pertaining the detection of data anomalies, the investigation of the ship\u2019s localized operational conditions, the identification of the relative correlations among parameters and the quantification of the ship\u2019s performance in each of the respective conditions. The novelty of this study is to provide a KPI (i.e. key performance indicator) for ship performance quantification in order to identify the best performance trim-draft mode under the engine modes of the case study ship. The proposed framework has the features to serve as an operational energy efficiency measure to provide data quality evaluation and decision support for ship performance monitoring that is of value for both ship operators and decision-makers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.795",
        "scimago_value": "1,321"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "howdodatascientistsandmanagersinfluencemachinelearningvaluecreation",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.01.228",
        "author": [
            "Ferreira, Humberto",
            "Ruivo, Pedro",
            "Reis, Carolina"
        ],
        "keywords": [
            "machine learning, business value, data scientists, managers, people factor"
        ],
        "abstract": "Corporations are leveraging machine learning (ML) to create business value (BV). So, it becomes relevant to not only ponder the antecedents that influence the ML BV process but also, the main actors that influence the creation of such value within organizations: data scientists and managers. Grounded in the dynamic-capabilities theory, a model is proposed and tested with 319 responses to a survey. While for both groups, platform maturity and data quality are equally important factors for financial performance, information intensity is an equally important factor for organizational performance. On one hand, data scientists care more about the catalytic effect of data quality on the relationship between platform maturity and financial performance, and the compatibility factor for organizational performance. On the other hand, managers care more about the feasibility factor for financial performance. The findings presented here offer insights on how data scientists and managers perceive the ML BV creation process.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "22145796",
        "isbn": null,
        "journal": "Big Data Research",
        "publisher": null,
        "title": "smartpharmaceuticalmanufacturingensuringendtoendtraceabilityanddataintegrityinmedicineproduction",
        "booktitle": null,
        "doi": "10.1016/j.bdr.2020.100172",
        "author": [
            "Leal, F\u00e1tima",
            "Chis, Adriana",
            "Caton, Simon",
            "Gonz\u00e1lez\u2013V\u00e9lez, Horacio",
            "Garc\u00eda\u2013G\u00f3mez, Juan",
            "Dur\u00e1, Marta",
            "S\u00e1nchez\u2013Garc\u00eda, Angel",
            "S\u00e1ez, Carlos",
            "Karageorgos, Anthony",
            "Gerogiannis, Vassilis",
            "Xenakis, Apostolos",
            "Lallas, Efthymios",
            "Ntounas, Theodoros",
            "Vasileiou, Eleni",
            "Mountzouris, Georgios",
            "Otti, Barbara",
            "Pucci, Penelope",
            "Papini, Rossano",
            "Cerrai, David",
            "Mier, Mariola"
        ],
        "keywords": [
            "ALCOA, Blockchain, Data anaytics, Data quality, Intelligent agents, Smart contracts"
        ],
        "abstract": "Production lines in pharmaceutical manufacturing generate numerous heterogeneous data sets from various embedded systems which control the multiple processes of medicine production. Such data sets should arguably ensure end-to-end traceability and data integrity in order to release a medicine batch, which is uniquely identified and tracked by its batch number/code. Consequently, auditable computerised systems are crucial on pharmaceutical production lines, since the industry is becoming increasingly regulated for product quality and patient health purposes. This paper describes the EU-funded SPuMoNI project, which aims to ensure the quality of large amounts of data produced by computerised production systems in representative pharmaceutical environments. Our initial results include significant progress in: (i) end-to-end verification taking advantage of blockchain properties and smart contracts to ensure data authenticity, transparency, and immutability; (ii) data quality assessment models to identify data behavioural patterns that can violate industry practices and/or international regulations; and (iii) intelligent agents to collect and manipulate data as well as perform smart decisions. By analysing multiple sensors in medicine production lines, manufacturing work centres, and quality control laboratories, our approach has been initially evaluated using representative industry-grade pharmaceutical manufacturing data sets generated at an IT environment with regulated processes inspected by regulatory and government agencies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.578",
        "scimago_value": "0,565"
    },
    {
        "issnkey": "07475632",
        "isbn": null,
        "journal": "Computers in Human Behavior",
        "publisher": null,
        "title": "consumerinteractionwithcuttingedgetechnologiesimplicationsforfutureresearch",
        "booktitle": null,
        "doi": "10.1016/j.chb.2021.106761",
        "author": [
            "Ameen, Nisreen",
            "Hosany, Sameer",
            "Tarhini, Ali"
        ],
        "keywords": [
            "Consumer interaction, Cutting-edge technologies, Artificial intelligence, Virtual reality and augmented reality, Robotics, Wearable technology, Big data analytics"
        ],
        "abstract": "This article provides an overview of extant literature addressing consumer interaction with cutting-edge technologies. Six focal cutting-edge technologies are identified: artificial intelligence, augmented reality, virtual reality, wearable technology, robotics and big data analytics. Our analysis shows research on consumer interaction with cutting-edge technologies is at a nascent stage, and there are several gaps requiring attention. To further advance knowledge, our article offers avenues for future interdisciplinary research addressing implications of consumer interaction with cutting-edge technologies. More specifically, we propose six main areas for future research namely: rethinking consumer behaviour models, identifying behavioural differences among different generations of consumers, understanding how consumers interact with automated services, ethics, privacy and the blackbox, consumer security concerns and consumer interaction with new-age technologies during and after a major global crisis such as the COVID-19 pandemic.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.829",
        "scimago_value": "2,108"
    },
    {
        "issnkey": "09581669",
        "isbn": null,
        "journal": "Current Opinion in Biotechnology",
        "publisher": null,
        "title": "thepotentialofremotesensingandartificialintelligenceastoolstoimprovetheresilienceofagricultureproductionsystems",
        "booktitle": null,
        "doi": "10.1016/j.copbio.2020.09.003",
        "author": [
            "Jung, Jinha",
            "Maeda, Murilo",
            "Chang, Anjin",
            "Bhandari, Mahendra",
            "Ashapure, Akash",
            "Landivar-Bowles, Juan"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Modern agriculture and food production systems are facing increasing pressures from climate change, land and water availability, and, more recently, a pandemic. These factors are threatening the environmental and economic sustainability of current and future food supply systems. Scientific and technological innovations are needed more than ever to secure enough food for a fast-growing global population. Scientific advances have led to a better understanding of how various components of the agricultural system interact, from the cell to the field level. Despite incredible advances in genetic tools over the past few decades, our ability to accurately assess crop status in the field, at scale, has been severely lacking until recently. Thanks to recent advances in remote sensing and Artificial Intelligence (AI), we can now quantify field scale phenotypic information accurately and integrate the big data into predictive and prescriptive management tools. This review focuses on the use of recent technological advances in remote sensing and AI to improve the resilience of agricultural systems, and we will present a unique opportunity for the development of prescriptive tools needed to address the next decade\u2019s agricultural and human nutrition challenges.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "unleashingtheconvergenceamiddigitalizationandsustainabilitytowardspursuingthesustainabledevelopmentgoalssdgsaholisticreview",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2020.122204",
        "author": [
            "{Del R\u00edo Castro}, Gema",
            "{Gonz\u00e1lez Fern\u00e1ndez}, Mar\u00eda",
            "{Uruburu Colsa}, \u00c1ngel"
        ],
        "keywords": [
            "Sustainability, Sustainable development goals (SDGs), Digitalization, ICT, Big data, Artificial intelligence"
        ],
        "abstract": "The Sustainable Development Goals (SDGs) within the United Nations 2030 Agenda emerged in 2015, becoming an unprecedented global compass for navigating extant sustainability challenges. Nevertheless, it still represents a nascent field enduring uncertainties and complexities. In this regard, the interplay between digitalization and sustainability unfolds bright opportunities for shaping a greener economy and society, paving the way towards the SDGs. However, little evidence exists so far, about a genuine contribution of digital paradigms to sustainability. Besides, their role to tackle the SDGs research gaps remains unexplored. Thus, a holistic characterization of the aforementioned topics has not been fully explored in the emerging literature, deserving further research. The article endeavors a twofold purpose: (1) categorizing the main SDGs research gaps; (2) coupled with a critical exploration of the potential contribution of digital paradigms, particularly Big Data and Artificial Intelligence, towards overcoming the aforesaid caveats and pursuing the 2030 Agenda. Ultimately, the study seeks to bridge literature gaps by providing a first-of-its-kind overview on the SDGs and their nexus with digitalization, while unraveling policy implications and future research directions. The methodology has consisted of a systematic holistic review and in-depth qualitative analysis of the literature on the realms of the SDGs and digitalization. Our findings evidence that the SDGs present several research gaps, namely: flawed understanding of complexities and interlinkages; design shortcomings and imbalances; implementation and governance hurdles; unsuitable indicators and assessment methodologies; truncated adoption and off-target progress; unclear responsibilities and lacking coordination; untapped role of technological innovation and knowledge management. Moreover, our results show growing expectations about the added value brought by digitalization for pursuing the SDGs, through novel data sources, enhanced analytical capacities and collaborative digital ecosystems. However, current research and practice remains in early-stage, pointing to ethical, social and environmental controversies, along with policy caveats, which merit additional research. In light of the findings, the authors suggest a first-approach exploration of research and policy implications. Results suggest that further multidisciplinary research, dialogue and concerted efforts for transformation are required. Reframing the Agenda, while aligning the sustainable development and digitalization policies, seems advisable to ensure a holistic sustainability. The findings aim at guiding and stimulating further research and science-policy dialogue on the promising nexus amid the SDGs and digitalization.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "22119736",
        "isbn": null,
        "journal": "Tourism Management Perspectives",
        "publisher": null,
        "title": "comparingdifferencesinthespatiotemporalpatternsbetweenresidenttouristsandnonresidenttouristsusinghotelcheckinregisters",
        "booktitle": null,
        "doi": "10.1016/j.tmp.2021.100860",
        "author": [
            "Xu, Yuquan",
            "Ran, Xiaobin",
            "Liu, Yuewen",
            "Huang, Wei"
        ],
        "keywords": [
            "Big data, Spatiotemporal patterns, Resident tourists and non-resident tourists, Multiple city travel, Hotel check-in registers"
        ],
        "abstract": "Previous research studied the spatiotemporal patterns in different visitor segments but lacks evidence of the segmentation of resident tourists and non-resident tourists in multi-city travel. To fill this gap, this study conducts a big data study using hotel check-in registers. The exploratory data analysis visualizes the spatiotemporal patterns and the differences between resident tourists and non-resident tourists. Then, the spatiotemporal patterns are measured by the length of stay and the number of visited cities. The regression shows that both the length of stay and the number of visited cities of non-resident tourists are higher than those of resident tourists. Moreover, non-resident tourists reduce their length of stay and their number of visited cities more than resident tourists on three-day holidays, while they increase their number of visited cities less than resident tourists on seven-day holidays. This study has significant implications for understanding spatiotemporal patterns and visitors' segmentations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.586",
        "scimago_value": "1,454"
    },
    {
        "issnkey": "09575820",
        "isbn": null,
        "journal": "Process Safety and Environmental Protection",
        "publisher": null,
        "title": "safetyintelligenceasanessentialperspectiveforsafetymanagementintheeraofsafety40fromatheoreticaltoapracticalframework",
        "booktitle": null,
        "doi": "10.1016/j.psep.2020.10.008",
        "author": [
            "Wang, Bing"
        ],
        "keywords": [
            "Safety intelligence (SI), Safety big data, Safety 4.0, Safety management, Safety decision-making"
        ],
        "abstract": "In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science\u2014a new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "dataacquisitionandpreparationenablingdataanalyticsprojectswithinproduction",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.107",
        "author": [
            "Schock, Christoph",
            "Dumler, Jonas",
            "Doepper, Frank"
        ],
        "keywords": [
            "Data Analytics, CRISP-DM, Data Acquisition, Data Preparation, Feature Engineering, Process Monitoring, Condition Monitoring"
        ],
        "abstract": "The increasing amount of available data in production systems is associated with great potential for process optimization. Due to lack of a data analytics methodology and low data quality within production these potentials often remain unused. Therefore, in this paper we present a model for data acquisition and data preparation including feature engineering for characteristic sensor signals of production machines. The model allows the extraction of relevant process information from the signal, which can be used for monitoring, KPI tracking, trend analysis and anomaly detection. The approach is evaluated on an industrial turning process.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "smartstandardsconceptfortheautomatedtransferofstandardcontentsintoamachineactionableform",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.05.025",
        "author": [
            "Ehring, Dominik",
            "Luttmer, Janosch",
            "Pluhnau, Robin",
            "Nagarajah, Arun"
        ],
        "keywords": [
            "SMART Standards, knowledge representation, automatic extraction, transfer, 3M model of Duisburg"
        ],
        "abstract": "Standards are - not directly visible to everyone \u2013 omnipresent in nearly every development process. In times of digitalization, where buzzwords such as \"connectivity of machines\", \"artificial intelligence\", \u201cbig data\u201d, \u201ccloud computing\u201d or \u201csmart factories\u201d are often used, companies are still confronted with problems in handling standards throughout the entire product lifecycle. Today\u2019s way of working with standards is characterized by manual viewing of documents, whereby a user searches for relevant information, such as formulas, and has to transfer this information to his process, method or tool. This manual process results in an increased time, loss of quality due to faulty manual transmission of information, a high adjustment effort for updates of standards and no guarantee for traceability. In order to reduce and minimize errors and needed time for work with information stored within standards, there is a need for a new form of knowledge representation for standards with sufficient data quality to ensure standard-compliant development activities. Consequently, there is a need for machine-actionable standards to ensure autonomous and efficient processes, whereby the effort for preparation is less than the benefit. The question arises how classified standards content can be represented in a machine-actionable way without loss of information. This paper shows a concept for the automatic extraction of standards content and their transfer into a machine-actionable knowledge representation. The concept, which is based on the \u201c3M Framework of Duisburg\u201d and thus answers questions of modularization, modeling and management, consists of six steps \"extraction\", \"modeling\", \u201cmodification\u201d, \"fusion and storage\", \"provision\" and \"application\", to digitalize existing content, is presented and discussed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "10848045",
        "isbn": null,
        "journal": "Journal of Network and Computer Applications",
        "publisher": null,
        "title": "asurveyontheroleofinternetofthingsforadoptingandpromotingagriculture40",
        "booktitle": null,
        "doi": "10.1016/j.jnca.2021.103107",
        "author": [
            "Raj, Meghna",
            "Gupta, Shashank",
            "Chamola, Vinay",
            "Elhence, Anubhav",
            "Garg, Tanya",
            "Atiquzzaman, Mohammed",
            "Niyato, Dusit"
        ],
        "keywords": [
            ""
        ],
        "abstract": "There is a rapid increase in the adoption of emerging technologies like the Internet of Things (IoT), Unmanned Aerial Vehicles (UAV), Internet of Underground Things (IoUT), Data analytics in the agriculture domain to meet the increased food demand to cater to the increasing population. Agriculture 4.0 is set to revolutionize agriculture productivity by using Precision Agriculture (PA), IoT, UAVs, IoUT, and other technologies to increase agriculture produce for growing demographics while addressing various farm-related issues. This survey provides a comprehensive overview of how multiple technologies such as IoT, UAVs, IoUT, Big Data Analytics, Deep Learning Techniques, and Machine Learning methods can be used to manage various farm-related operations. For each of these technologies, a detailed review is done on how the technology is being used in Agriculture 4.0. These discussions include an overview of relevant technologies, their use cases, existing case studies, and research works that demonstrate the use of these technologies in Agriculture 4.0. This paper also highlights the various future research gaps in the adoption of these technologies in Agriculture 4.0.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "makingdataplatformssmarterwithmoses",
        "booktitle": null,
        "doi": "10.1016/j.future.2021.06.031",
        "author": [
            "Francia, Matteo",
            "Gallinucci, Enrico",
            "Golfarelli, Matteo",
            "Leoni, Anna",
            "Rizzi, Stefano",
            "Santolini, Nicola"
        ],
        "keywords": [
            "Data lake, Metadata, Big data, Data platform"
        ],
        "abstract": "The rise of data platforms has enabled the collection and processing of huge volumes of data, but has opened to the risk of losing their control. Collecting proper metadata about raw data and transformations can significantly reduce this risk. In this paper we propose MOSES, a technology-agnostic, extensible, and customizable framework for metadata handling in big data platforms. The framework hinges on a metadata repository that stores information about the objects in the big data platform and the processes that transform them. MOSES provides a wide range of functionalities to different types of users of the platform. Differently from previous high-level proposals, MOSES is fully implemented and it was not conceived for a specific technology. Besides discussing the rationale and the features of MOSES, in this paper we describe its implementation and we test it on a real case study. The ultimate goal is to take a significant step forward towards proving that metadata handling in big data platforms is feasible and beneficial.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03787788",
        "isbn": null,
        "journal": "Energy and Buildings",
        "publisher": null,
        "title": "dataminingcubesforbuildingsagenericframeworkformultidimensionalanalyticsofbuildingperformancedata",
        "booktitle": null,
        "doi": "10.1016/j.enbuild.2021.111195",
        "author": [
            "Leprince, Julien",
            "Miller, Clayton",
            "Zeiler, Wim"
        ],
        "keywords": [
            "Data mining, Data cube, Generic method, Multidimensional analytics, Machine learning, Building data"
        ],
        "abstract": "Over the last decade, collecting massive volumes of data has been made all the more accessible, pushing the building sector to embrace data mining as a powerful tool for harvesting the potential of big data analytics. However repetitive challenges still persist emerging from the need for a common analytical frame, effective application- and insight-driven targeted data selection, as well as benchmarked-supported claims. This study addresses these concerns by putting forward a generic stepwise multidimensional data mining framework tailored to building data, leveraging the dimensional-structures of data cubes. Using the open Building Data Genome Project 2 set, composed of 3053 energy meters from 1636 buildings, we provide an online, open access, implementation illustration of our method applied to automated pattern identification. We define a 3-dimensional building cube echoing typical analytical frames of interest, namely, bottom-up, top-down and temporal drill-in approaches. Our results highlight the importance of application and insight driven mining for effective dimensional-frame targeting. Impactful visualizations were developed allowing practical human inspection, paving the path towards more interpretable analytics.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.879",
        "scimago_value": "1,737"
    },
    {
        "issnkey": "02786125",
        "isbn": null,
        "journal": "Journal of Manufacturing Systems",
        "publisher": null,
        "title": "datascienceskillsanddomainknowledgerequirementsinthemanufacturingindustryagapanalysis",
        "booktitle": null,
        "doi": "10.1016/j.jmsy.2021.07.007",
        "author": [
            "Li, Guoyan",
            "Yuan, Chenxi",
            "Kamarthi, Sagar",
            "Moghaddam, Mohsen",
            "Jin, Xiaoning"
        ],
        "keywords": [
            "Industry 4.0, Labor market analysis, Skills gap, Data science"
        ],
        "abstract": "Manufacturing has adopted technologies such as automation, robotics, industrial Internet of Things (IoT), and big data analytics to improve productivity, efficiency, and capabilities in the production environment. Modern manufacturing workers not only need to be adept at the traditional manufacturing technologies but also ought to be trained in the advanced data-rich computer-automated technologies. This study analyzes the data science and analytics (DSA) skills gap in today's manufacturing workforce to identify the critical technical skills and domain knowledge required for data science and intelligent manufacturing-related jobs that are highly in-demand in today's manufacturing industry. The gap analysis conducted in this paper on Emsi job posting and profile data provides insights into the trends in manufacturing jobs that leverage data science, automation, cyber, and sensor technologies. These insights will be helpful for educators and industry to train the next generation manufacturing workforce. The main contribution of this paper includes (1) presenting the overall trend in manufacturing job postings in the U.S., (2) summarizing the critical skills and domain knowledge in demand in the manufacturing sector, (3) summarizing skills and domain knowledge reported by manufacturing job seekers, (4) identifying the gaps between demand and supply of skills and domain knowledge, and (5) recognize opportunities for training and upskilling workforce to address the widening skills and knowledge gap.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.633",
        "scimago_value": "2,310"
    },
    {
        "issnkey": "26671026",
        "isbn": null,
        "journal": "Intelligent Medicine",
        "publisher": null,
        "title": "thenationalmulticenterartificialintelligentmyopiapreventionandcontrolproject",
        "booktitle": null,
        "doi": "10.1016/j.imed.2021.05.001",
        "author": [
            "Wang, Xun",
            "Yang, Yahan",
            "Wu, Yuxuan",
            "Wei, Wenbin",
            "Dong, Li",
            "Li, Yang",
            "Tan, Xingping",
            "Cao, Hankun",
            "Zhang, Hong",
            "Ma, Xiaodan",
            "Jiang, Qin",
            "Zhou, Yunfan",
            "Yang, Weihua",
            "Li, Chaoyu",
            "Gu, Yu",
            "Ding, Lin",
            "Qin, Yanli",
            "Chen, Qi",
            "Li, Lili",
            "Lian, Mingyue",
            "Ma, Jin",
            "Cui, Dongmei",
            "Huang, Yuanzhou",
            "Liu, Wenyan",
            "Yang, Xiao",
            "Yu, Shuiming",
            "Chen, Jingjing",
            "Wang, Dongni",
            "Lin, Zhenzhe",
            "Yan, Pisong",
            "Lin, Haotian"
        ],
        "keywords": [
            "Myopia prevention and control, Artificial intelligent, National multicenter project"
        ],
        "abstract": "In recent years, the incidence of myopia has increased at an alarming rate among children and adolescents in China. The exploration of an effective prevention and control method for myopia is in urgent need. With the development of information technology in the past decade, artificial intelligence with the Internet of Things technology (AIoT) is characterized by strong computing power, advanced algorithm, continuous monitoring, and accurate prediction of long-term progression. Therefore, big data and artificial intelligence technology have the potential to be applied to data mining of myopia etiology and prediction of myopia occurrence and development. More recently, there has been a growing recognition that myopia study involving AIoT needs to undergo a rigorous evaluation to demonstrate robust results.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "traindelaypredictionintunisianrailwaythroughlightgbmmodel",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.08.101",
        "author": [
            "Laifa, Hassiba",
            "khcherif, Raoudha",
            "{Ben Ghezalaa}, Henda"
        ],
        "keywords": [
            "Delay prediction, Data Analysis, Machine learning, LightGBM"
        ],
        "abstract": "Train delays are one of the most important problems in the railway systems across the world, which urges the development of predictive analysis-based approaches to estimate it. In fact, with the advanced big data analysis and machine learning tools and technologies, the train delay-prediction systems can process and extract useful information from the large historical train movement data collected by the railway information system. Besides, accurate prediction of train delays can help train dispatchers make decisions through timetable rescheduling and service reliability improving. We propose, in this manuscript, a machine-learning model that captures the relationship between the arrival delay of passenger trains and the various characteristics of the railway system. We also apply, for the first time, lightGBM regressor based on optimal hyper-parameters to predict train delays. To evaluate the introduced model performance, the latter is compared with that of some other widely used existing models. Its R-squared, RMSE and RME were also compared with those of Support Vector Machine, Random Forest, XGBboost and Artificial Neural Network models. Statistical comparison indicates that the LightGBM outperforms the other models and is the fastest.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "15740137",
        "isbn": null,
        "journal": "Computer Science Review",
        "publisher": null,
        "title": "machinelearninganddeeplearninginsmartmanufacturingthesmartgridparadigm",
        "booktitle": null,
        "doi": "10.1016/j.cosrev.2020.100341",
        "author": [
            "Kotsiopoulos, Thanasis",
            "Sarigiannidis, Panagiotis",
            "Ioannidis, Dimosthenis",
            "Tzovaras, Dimitrios"
        ],
        "keywords": [
            "Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid"
        ],
        "abstract": "Industry 4.0 is the new industrial revolution. By connecting every machine and activity through network sensors to the Internet, a huge amount of data is generated. Machine Learning (ML) and Deep Learning (DL) are two subsets of Artificial Intelligence (AI), which are used to evaluate the generated data and produce valuable information about the manufacturing enterprise, while introducing in parallel the Industrial AI (IAI). In this paper, the principles of the Industry 4.0 are highlighted, by giving emphasis to the features, requirements, and challenges behind Industry 4.0. In addition, a new architecture for AIA is presented. Furthermore, the most important ML and DL algorithms used in Industry 4.0 are presented and compiled in detail. Each algorithm is discussed and evaluated in terms of its features, its applications, and its efficiency. Then, we focus on one of the most important Industry 4.0 fields, namely the smart grid, where ML and DL models are presented and analyzed in terms of efficiency and effectiveness in smart grid applications. Lastly, trends and challenges in the field of data analysis in the context of the new Industrial era are highlighted and discussed such as scalability, cybersecurity, and big data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.872",
        "scimago_value": "1,646"
    },
    {
        "issnkey": "22141804",
        "isbn": null,
        "journal": "Sensing and Bio-Sensing Research",
        "publisher": null,
        "title": "digitallivestockfarming",
        "booktitle": null,
        "doi": "10.1016/j.sbsr.2021.100408",
        "author": [
            "Neethirajan, Suresh",
            "Kemp, Bas"
        ],
        "keywords": [
            "Precision Livestock Farming, digitalization, Digital Technologies in Livestock Systems, sensor technology, big data, blockchain, data models, livestock agriculture"
        ],
        "abstract": "As the global human population increases, livestock agriculture must adapt to provide more livestock products and with improved efficiency while also addressing concerns about animal welfare, environmental sustainability, and public health. The purpose of this paper is to critically review the current state of the art in digitalizing animal agriculture with Precision Livestock Farming (PLF) technologies, specifically biometric sensors, big data, and blockchain technology. Biometric sensors include either noninvasive or invasive sensors that monitor an individual animal\u2019s health and behavior in real time, allowing farmers to integrate this data for population-level analyses. Real-time information from biometric sensors is processed and integrated using big data analytics systems that rely on statistical algorithms to sort through large, complex data sets to provide farmers with relevant trending patterns and decision-making tools. Sensors enabled blockchain technology affords secure and guaranteed traceability of animal products from farm to table, a key advantage in monitoring disease outbreaks and preventing related economic losses and food-related health pandemics. Thanks to PLF technologies, livestock agriculture has the potential to address the abovementioned pressing concerns by becoming more transparent and fostering increased consumer trust. However, new PLF technologies are still evolving and core component technologies (such as blockchain) are still in their infancy and insufficiently validated at scale. The next generation of PLF technologies calls for preventive and predictive analytics platforms that can sort through massive amounts of data while accounting for specific variables accurately and accessibly. Issues with data privacy, security, and integration need to be addressed before the deployment of multi-farm shared PLF solutions becomes commercially feasible.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,770"
    },
    {
        "issnkey": "02684012",
        "isbn": null,
        "journal": "International Journal of Information Management",
        "publisher": null,
        "title": "identifyinginfluencersonsocialmedia",
        "booktitle": null,
        "doi": "10.1016/j.ijinfomgt.2020.102246",
        "author": [
            "Harrigan, Paul",
            "Daly, Timothy",
            "Coussement, Kristof",
            "Lee, Julie",
            "Soutar, Geoffrey",
            "Evers, Uwana"
        ],
        "keywords": [
            "Influencers, Market mavens, Big data, Social media, Twitter"
        ],
        "abstract": "The increased availability of social media big data has created a unique challenge for marketing decision-makers; turning this data into useful information. One of the significant areas of opportunity in digital marketing is influencer marketing, but identifying these influencers from big data sets is a continual challenge. This research illustrates how one type of influencer, the market maven, can be identified using big data. Using a mixed-method combination of both self-report survey data and publicly accessible big data, we gathered 556,150 tweets from 370 active Twitter users. We then proposed and tested a range of social-media-based metrics to identify market mavens. Findings show that market mavens (when compared to non-mavens) have more followers, post more often, have less readable posts, use more uppercase letters, use less distinct words, and use hashtags more often. These metrics are openly available from public Twitter accounts and could integrate into a broad-scale decision support system for marketing and information systems managers. These findings have the potential to improve influencer identification effectiveness and efficiency, and thus improve influencer marketing.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "14.098",
        "scimago_value": "2,770"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "thechallengeofprivacyandsecuritywhenusingtechnologytotrackpeopleintimesofcovid19pandemic",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.01.281",
        "author": [
            "Smidt, Hermanus",
            "Jokonya, Osden"
        ],
        "keywords": [
            "COVID 19, tracking, society, technology, privacy"
        ],
        "abstract": "Since the start of the Coronavirus disease 2019 (COVID-19) governments and health authorities across the world have find it very difficult in controlling infections. Digital technologies such as artificial intelligence (AI), big data, cloud computing, blockchain and 5G have effectively improved the efficiency of efforts in epidemic monitoring, virus tracking, prevention, control and treatment. Surveillance to halt COVID-19 has raised privacy concerns, as many governments are willing to overlook privacy implications to save lives. The purpose of this paper is to conduct a focused Systematic Literature Review (SLR), to explore the potential benefits and implications of using digital technologies such as AI, big data and cloud to track COVID-19 amongst people in different societies. The aim is to highlight the risks of security and privacy to personal data when using technology to track COVID-19 in societies and identify ways to govern these risks. The paper uses the SLR approach to examine 40 articles published during 2020, ultimately down selecting to the most relevant 24 studies. In this SLR approach we adopted the following steps; formulated the problem, searched the literature, gathered information from studies, evaluated the quality of studies, analysed and integrated the outcomes of studies while concluding by interpreting the evidence and presenting the results. Papers were classified into different categories such as technology use, impact on society and governance. The study highlighted the challenge for government to balance the need of what is good for public health versus individual privacy and freedoms. The findings revealed that although the use of technology help governments and health agencies reduce the spread of the COVID-19 virus, government surveillance to halt has sparked privacy concerns. We suggest some requirements for government policy to be ethical and capable of commanding the trust of the public and present some research questions for future research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "02673649",
        "isbn": null,
        "journal": "Computer Law & Security Review",
        "publisher": null,
        "title": "legalaspectsofdatacleansinginmedicalai",
        "booktitle": null,
        "doi": "10.1016/j.clsr.2021.105587",
        "author": [
            "St\u00f6ger, Karl",
            "Schneeberger, David",
            "Kieseberg, Peter",
            "Holzinger, Andreas"
        ],
        "keywords": [
            "Data cleansing, Data quality, Medical AI, Medical devices"
        ],
        "abstract": "Data quality is of paramount importance for the smooth functioning of modern data-driven AI applications with machine learning as a core technology. This is also true for medical AI, where malfunctions due to \"dirty data\" can have particularly dramatic harmful implications. Consequently, data cleansing is an important part in improving the usability of (Big) Data for medical AI systems. However, it should not be overlooked that data cleansing can also have negative effects on data quality if not performed carefully. This paper takes an interdisciplinary look at some of the technical and legal challenges of data cleansing against the background of European medical device law, with the key message that technical and legal aspects must always be considered together in such a sensitive context.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,815"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "thefutureofinternetofthingsinagricultureplanthighthroughputphenotypicplatform",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2020.123651",
        "author": [
            "Fan, Jiangchuan",
            "Zhang, Ying",
            "Wen, Weiliang",
            "Gu, Shenghao",
            "Lu, Xianju",
            "Guo, Xinyu"
        ],
        "keywords": [
            "Internet of things in agriculture, Big data, High-throughput phenotype, Data mining"
        ],
        "abstract": "With continuous collaborative research in sensor technology, communication technology, plant science, computer science and engineering science, Internet of Things (IoT) in agriculture has made a qualitative leap through environmental sensor networks, non-destructive imaging, spectral analysis, robotics, machine vision and laser radar technology. Physical and chemical analysis can continuously obtain environmental data, experimental metadata (including text, image and spectral, 3D point cloud and real-time growth data) through integrated automation platform equipment and technical means. Based on data on multi-scale, multi-environmental and multi-mode plant traits that constitute big data on plant phenotypes, genotype\u2013phenotype\u2013envirotype relationship in the omics system can be explored deeply. Detailed information on the formation mechanism of specific biological traits can promote the process of functional genomics, plant molecular breeding and efficient cultivation. This study summarises the development background, research process and characteristics of high-throughput plant phenotypes. A systematic review of the research progress of IoT in agriculture and plant high-throughput phenotypes is conducted, including the acquisition and analysis of plant phenotype big data, phenotypic trait prediction and multi-recombination analysis based on plant phenomics. This study proposes key techniques for current plant phenotypes, and looks forward to the research on plant phenotype detection technology in the field environment, fusion and data mining of plant phenotype multivariate data, simultaneous observation of multi-scale phenotype platform and promotion of a comprehensive high-throughput phenotype technology.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "artificialintelligenceinsustainableenergyindustrystatusquochallengesandopportunities",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.125834",
        "author": [
            "Ahmad, Tanveer",
            "Zhang, Dongdong",
            "Huang, Chao",
            "Zhang, Hongcai",
            "Dai, Ningyi",
            "Song, Yonghua",
            "Chen, Huanxin"
        ],
        "keywords": [
            "Artificial intelligence, Renewable energy, Energy demand, Decision making, Big data, Energy digitization"
        ],
        "abstract": "The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study\u2019s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "09696989",
        "isbn": null,
        "journal": "Journal of Retailing and Consumer Services",
        "publisher": null,
        "title": "bigsocialdataandcustomerdecisionmakinginvegetarianrestaurantsacombinedmachinelearningmethod",
        "booktitle": null,
        "doi": "10.1016/j.jretconser.2021.102630",
        "author": [
            "Nilashi, Mehrbakhsh",
            "Ahmadi, Hossein",
            "Arji, Goli",
            "Alsalem, Khalaf",
            "Samad, Sarminah",
            "Ghabban, Fahad",
            "Alzahrani, Ahmed",
            "Ahani, Ali",
            "Alarood, Ala"
        ],
        "keywords": [
            "Online reviews, Food quality, Vegetarian friendly restaurants, Text mining, Segmentation"
        ],
        "abstract": "Customers increasingly use various social media to share their opinion about restaurants service quality. Big data collected from social media provides a data platform to improve the service quality of restaurants through customers' online reviews, where online reviews are a trustworthy and reliable source that helps consumers to evaluate food quality. Developing methods for effective evaluation of customer-generated reviews of restaurant services is important. This study develops a new method through effective learning techniques for customer segmentation and their preferences prediction in vegetarian friendly restaurants. The method is developed through text mining (Latent Dirichlet Allocation), cluster analysis (Self Organizing Map) and predictive learning technique (Classification and Regression Trees) to reveal the customer\u2019 satisfaction levels from the service quality in vegetarian friendly restaurants. Based on the obtained results of our experiments on the data vegetarian friendly restaurants in Bangkok, the models constructed by Classification and Regression Trees were able to give an accurate prediction of customers' preferences on the basis of restaurants' quality factors. The results showed that customers\u2019 online reviews analysis can be an effective way for customers segmentation to predict their preferences and help the restaurant managers to set priority instructions for service quality improvements.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.135",
        "scimago_value": "1,568"
    },
    {
        "issnkey": "22131388",
        "isbn": null,
        "journal": "Sustainable Energy Technologies and Assessments",
        "publisher": null,
        "title": "aframeworkofenergyconsumptiondrivendiscretemanufacturingsystem",
        "booktitle": null,
        "doi": "10.1016/j.seta.2021.101336",
        "author": [
            "Zhang, Tao",
            "Ji, Weixi",
            "Qiu, Yongtao"
        ],
        "keywords": [
            "Energy-efficient optimization, Discrete manufacturing system, Data preprocessing, Data mining"
        ],
        "abstract": "Because of big data on energy consumption, there is a lack of research on the discrete manufacturing system. The discrete manufacturing system has plenty of multi-source and heterogeneous data; it was challenging to collect real-time data. Recently, low carbon and green manufacturing is a hot field; especially, it can save electrical energy. This paper proposes a significant energy consumption data of a data-driven analysis framework, which promoting the energy efficiency of discrete manufacturing plant, equipment, and workshop production process. Firstly, put forward the evaluation standards of energy efficiency for discrete manufacturing shops. Then make energy-consumption data preprocessing. Efficiency optimization of big data mining method is put forward based on grid computing function. Design the discrete manufacturing system energy-consumption parameter values, then summarizes prediction algorithms and models in order to predict the results and the trends. Finally, introduce the application of a mobile phone shell manufacturing shop to verify the proposed framework. Further research will focus on energy-consumption data mining processing.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.353",
        "scimago_value": "1,040"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-819412-6",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter13artificialintelligenceforfloodobservation",
        "booktitle": "Earth Observation for Flood Applications",
        "doi": "10.1016/B978-0-12-819412-6.00013-4",
        "author": [
            "Wang, Ruo-Qian"
        ],
        "keywords": [
            "artificial intelligence, natural language processing, computer vision, machine learning, Big Data, Internet of Things, crowdsourcing, citizen science, surveillance video"
        ],
        "abstract": "Artificial intelligence (AI) is fundamentally changing our society, benefiting from the big data revolution and dramatical declination in the Internet of Things (IoT) costs. Flood research and applications will progress with this emerging technology, as AI is creating new flood data sources, enhancing our capability to analyze the data, and improving our accuracy of flood predictions. This chapter introduces the basic concepts of AI and its technical frontier. Using the method of \u201creview of the reviews\u201d with example highlight the emerging AI applications in the field of flood hazards is summarized in terms of the data sources, including crowdsourcing and surveillance camera videos. The use of the AI-enabled big data is also discussed. The opportunities and barriers of this new technology are summarized. At the end of the chapter, the trend and the research gaps are identified in this field.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26671026",
        "isbn": null,
        "journal": "Intelligent Medicine",
        "publisher": null,
        "title": "standardizationofcollectionstorageannotationandmanagementofdatarelatedtomedicalartificialintelligence",
        "booktitle": null,
        "doi": "10.1016/j.imed.2021.11.002",
        "author": [
            "Yang, Yahan",
            "Li, Ruiyang",
            "Xiang, Yifan",
            "Lin, Duoru",
            "Yan, Anqi",
            "Chen, Wenben",
            "Li, Zhongwen",
            "Lai, Weiyi",
            "Wu, Xiaohang",
            "Wan, Cheng",
            "Bai, Wei",
            "Huang, Xiucheng",
            "Li, Qiang",
            "Deng, Wenrui",
            "Liu, Xiyang",
            "Lin, Yucong",
            "Yan, Pisong",
            "Lin, Haotian"
        ],
        "keywords": [
            "Artificial intelligence, Big data, Intelligent medicine, Data collection, Data storage, Data annotation, Data management"
        ],
        "abstract": "Medical artificial intelligence (AI) and big data technology have rapidly advanced in recent years, and they are now routinely used for image-based diagnosis. China has a massive amount of medical data. However, a uniform criteria for medical data quality have yet to be established. Therefore, this review aimed to develop a standardized and detailed set of quality criteria for medical data collection, storage, annotation, and management related to medical AI. This will greatly improve the process of medical data resource sharing and the use of AI in clinical medicine.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0307904x",
        "isbn": null,
        "journal": "Applied Mathematical Modelling",
        "publisher": null,
        "title": "theroleofsurrogatemodelsinthedevelopmentofdigitaltwinsofdynamicsystems",
        "booktitle": null,
        "doi": "10.1016/j.apm.2020.09.037",
        "author": [
            "Chakraborty, S.",
            "Adhikari, S.",
            "Ganguli, R."
        ],
        "keywords": [
            "Digital twin, Vibration, Response, Frequency, Surrogate"
        ],
        "abstract": "Digital twin technology has significant promise, relevance and potential of widespread applicability in various industrial sectors such as aerospace, infrastructure and automotive. However, the adoption of this technology has been slower due to the lack of clarity for specific applications. A discrete damped dynamic system is used in this paper to explore the concept of a digital twin. As digital twins are also expected to exploit data and computational methods, there is a compelling case for the use of surrogate models in this context. Motivated by this synergy, we have explored the possibility of using surrogate models within the digital twin technology. In particular, the use of Gaussian process (GP) emulator within the digital twin technology is explored. GP has the inherent capability of addressing noisy and sparse data and hence, makes a compelling case to be used within the digital twin framework. Cases involving stiffness variation and mass variation are considered, individually and jointly, along with different levels of noise and sparsity in data. Our numerical simulation results clearly demonstrate that surrogate models, such as GP emulators, have the potential to be an effective tool for the development of digital twins. Aspects related to data quality and sampling rate are analysed. Key concepts introduced in this paper are summarised and ideas for urgent future research needs are proposed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-816078-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "complexityofpatientdatainprimarycarepractice",
        "booktitle": "Systems Medicine",
        "doi": "10.1016/B978-0-12-801238-3.11590-9",
        "author": [
            "\u0160vab, Igor"
        ],
        "keywords": [
            "Big data, Digital health, Family medicine, Genomics, Primary care"
        ],
        "abstract": "The chapter deals with the practical implications of big data in clinical practice, especially primary care. Family medicine has always advocated individualized approach to patient care. Medicine is changing rapidly for different reasons. One of the reasons is the development of new technologies which is going to radically change medical practice in the future. One of the key changes will involve the importance and practice of data management. The traditional data management that was based on paper records is being changed to the electronic medical record which offers great potential for patient management. This transition will also give rise to new challenges to the practising physician. We are facing the challenge of new sources of data, their increase and variety. Currently, all these data is stored in different locations and there is no consensus whether one single profession is going to take responsibility for managing the data of the patient. If this is decided, the primary care practice would seem a logical solution. In order to do this would involve challenges to healthcare policy and infrastructure. The big data approach to medical care gives rise to new ethical challenges that we would have to address. Existing and future physicians will have to be educated in order to address all these issues for the benefit of their patients. Nevertheless, the physicians should still remember that even with the vast development of precision medicine, the patient will still be more than just a collection of data.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02784319",
        "isbn": null,
        "journal": "International Journal of Hospitality Management",
        "publisher": null,
        "title": "constructionofaservicequalityscalefortheonlinefooddeliveryindustry",
        "booktitle": null,
        "doi": "10.1016/j.ijhm.2021.102938",
        "author": [
            "Cheng, Ching-Chan",
            "Chang, Ya-Yuan",
            "Chen, Cheng-Ta"
        ],
        "keywords": [
            "Online food delivery, Service quality, Big data analytic, OFD service quality scale"
        ],
        "abstract": "The main purpose of this study is based on qualitative and quantitative research procedures, and integrates the key service factors for the online food delivery (OFD) industry extracted by Internet Big Data Analytics (IBDA) to construct a OFD service quality scale (OFD-SERV). This study takes OFD customers in Taipei City as the objects. The results show that 20 key service factors for the OFD industry are extracted through IBDA. The OFD-SERV scale contains six dimensions including reliability, maintenance of meal quality and hygiene, assurance, security, system operation and traceability, a total of 28 items. The results from the structural equation modeling showed that the reliability, assurance and system operation have a positive impact on customer satisfaction. Finally, the findings provide knowledge and inspiration for the current OFD, and enable OFD operators and future researchers to more accurately identify the deficiency of service quality.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.237",
        "scimago_value": "2,321"
    },
    {
        "issnkey": "08957967",
        "isbn": null,
        "journal": "Seminars in Vascular Surgery",
        "publisher": null,
        "title": "artificialintelligencesroleinvascularsurgerydecisionmaking",
        "booktitle": null,
        "doi": "10.1053/j.semvascsurg.2021.10.005",
        "author": [
            "Zarkowsky, Devin",
            "Stonko, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": "ABSTRACT Artificial intelligence (AI) is the next great advance informing medical science. Several disciplines, including vascular surgery, use AI-based decision-making tools to improve clinical performance. Although applied widely, AI functions best when confronted with voluminous, accurate data. Consistent, predictable analytic technique selection also challenges researchers. This article contextualizes AI analyses within evidence-based medicine, focusing on \u201cbig data\u201d and health services research, as well as discussing opportunities to improve data collection and realize AI's promise.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01497189",
        "isbn": null,
        "journal": "Evaluation and Program Planning",
        "publisher": null,
        "title": "advancesinmonitoringandevaluationinlowandmiddleincomecountries",
        "booktitle": null,
        "doi": "10.1016/j.evalprogplan.2021.101994",
        "author": [
            "Thomas, James",
            "Doherty, Kathy",
            "Watson-Grant, Stephanie",
            "Kumar, Manish"
        ],
        "keywords": [
            "Monitoring and evaluation, Health information systems, Developing countries"
        ],
        "abstract": "Data to inform and improve health care systems in low- and middle-income countries (LMICs) has been facilitated by the development of monitoring and evaluation (M&E) systems. The drivers of change in M&E systems over the last 50 years have included a series of health concerns that have animated global donors (e.g., family planning, vaccination campaigns, and HIV/AIDS); the data requirements of donors; improved national economies enabling LMICs to invest more in M&E systems; and rapid advances in digital technologies. Progress has included the training and expansion of an M&E workforce, the creation of systems for data collection and use, and processes for assessing and ensuring data quality. Controversies have included the development of disease-specific systems that do not coordinate with each other, and a growing burden on health care deliverers to collect data for a proliferating number of health and process indicators. Digital technologies offer the promise of real time data and quick adaptation but also raise ethical and privacy concerns. The desire for speed can cast large-scale evaluations, considered by some to be the gold standard, in an unfavorable light as slow and expensive. Accordingly, there is a growing demand for speedy evaluations that rely on routine health information systems and privately collected \u201cbig data\u201d from electronic health records and social media.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.849",
        "scimago_value": "0,555"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818015-0",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter4thetenstepsprocess",
        "booktitle": "Executing Data Quality Projects (Second Edition)",
        "doi": "10.1016/B978-0-12-818015-0.00006-2",
        "author": [
            "McGilvray, Danette"
        ],
        "keywords": [
            "business needs, information environment, information life cycle, data quality dimensions, business impact techniques, root causes, improvement, correction, prevention, controls, monitor, communicate, ethics, change management"
        ],
        "abstract": "This chapter contains the step-by-step guide for creating, assessing, improving, sustaining, and managing information and data quality. Concrete instructions, sample output and templates, and practical advice for executing every step of the Ten Steps Process are provided. A step summary table gives an at-a-glance overview of objectives, purpose, inputs and outputs, techniques and tools, communication, and checkpoints for each step. The Ten Steps Process was designed to be flexible. Suggestions are given to help the reader select and adjust the Ten Steps to various situations, business needs, and data quality issues. The layout allows for quick reference with an easy-to-use format highlighting key concepts and definitions, important checkpoints, communication activities, best practices, and warnings. The experience of actual clients and users of the Ten Steps are highlighted in callout boxes called Ten Steps in Action.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0308521x",
        "isbn": null,
        "journal": "Agricultural Systems",
        "publisher": null,
        "title": "enhancingtheabilityofagriculturetocopewithmajorcrisesordisasterswhattheexperienceofcovid19teachesus",
        "booktitle": null,
        "doi": "10.1016/j.agsy.2020.103023",
        "author": [
            "Lioutas, Evagelos",
            "Charatsari, Chrysanthi"
        ],
        "keywords": [
            "Agriculture, COVID-19, Major crises, Smart technology, Community marketing, Resilience"
        ],
        "abstract": "The COVID-19 outbreak was an unprecedented situation that uncovered forgotten interconnections and interdependencies between agriculture, society, and economy, whereas it also brought to the fore the vulnerability of agrifood production to external disturbances. Building upon the ongoing experience of the COVID-19 pandemic, in this short communication, we discuss three potential mechanisms that, in our opinion, can mitigate the impacts of major crises or disasters in agriculture: resilience-promoting policies, community marketing schemes, and smart farming technology. We argue that resilience-promoting policies should focus on the development of crisis management plans and enhance farmers' capacity to cope with external disturbances. We also stress the need to promote community marketing conduits that ensure an income floor for farmers while in parallel facilitating consumer access to agrifood products when mainstream distribution channels under-serve them. Finally, we discuss some issues that need to be solved to ensure that smart technology and big data can help farmers overcome external shocks.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "objectsdetectiontowardcomplicatedhighremotebasketballsportsbyleveragingdeepcnnarchitecture",
        "booktitle": null,
        "doi": "10.1016/j.future.2021.01.020",
        "author": [
            "Liu, Long"
        ],
        "keywords": [
            "Object detection, Sport action recognition, Image recognition, Basketball recognition"
        ],
        "abstract": "The analysis of high-difficulty action recognition technology in basketball is mainly to identify and analyze the physical behavior of basketball players in the video to complete the technical action. The purpose of video recognition is to provide an important guarantee for improving the level of basketball training. The current target recognition technology has achieved some results. It shows that the application of target detection technology in basketball sports scene is of great significance and can improve the effect of sports training. However, traditional sports target recognition is limited by technology and injury, and the analysis of difficult sports skills is limited by the scene, dynamic background and technology, and cannot achieve the desired effect. This is not conducive to the improvement of athletes\u2019 skills. Therefore, this article aims to develop a big data motion target detection system based on deep convolutional neural network for sports difficult motion image recognition. More specifically, we use the high discriminative power of the convolutional neural network to extract images to perform computational preprocessing for the recognition of each human motion image in the video stream. Then, the skeleton recognition algorithm based on LSTM is used to detect the key points of the human body, which is of great significance for modeling different movements. Finally, we developed an object detection system to reconstruct each movement. By selecting five groups of highly difficult actions that are likely to cause sports injuries to conduct experimental research, the results prove the effectiveness of the target detection system we proposed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01692607",
        "isbn": null,
        "journal": "Computer Methods and Programs in Biomedicine",
        "publisher": null,
        "title": "areviewofriskpredictionmodelsincardiovasculardiseaseconventionalapproachvsartificialintelligentapproach",
        "booktitle": null,
        "doi": "10.1016/j.cmpb.2021.106190",
        "author": [
            "{Mohd Faizal}, Aizatul",
            "Thevarajah, T.",
            "Khor, Sook",
            "Chang, Siow-Wee"
        ],
        "keywords": [
            "Cardiovascular diseases, Risk prediction, Artificial intelligence, Machine learning, Deep learning"
        ],
        "abstract": "Cardiovascular disease (CVD) is the leading cause of death worldwide and is a global health issue. Traditionally, statistical models are used commonly in the risk prediction and assessment of CVD. However, the adoption of artificial intelligent (AI) approach is rapidly taking hold in the current era of technology to evaluate patient risks and predict the outcome of CVD. In this review, we outline various conventional risk scores and prediction models and do a comparison with the AI approach. The strengths and limitations of both conventional and AI approaches are discussed. Besides that, biomarker discovery related to CVD are also elucidated as the biomarkers can be used in the risk stratification as well as early detection of the disease. Moreover, problems and challenges involved in current CVD studies are explored. Lastly, future prospects of CVD risk prediction and assessment in the multi-modality of big data integrative approaches are proposed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.428",
        "scimago_value": "0,924"
    },
    {
        "issnkey": "26670917",
        "isbn": null,
        "journal": "Journal of Urban Mobility",
        "publisher": null,
        "title": "mainchallengesandopportunitiestodynamicroadspaceallocationfromstatictodynamicurbandesigns",
        "booktitle": null,
        "doi": "10.1016/j.urbmob.2021.100008",
        "author": [
            "Valen\u00e7a, Gabriel",
            "Moura, Filipe",
            "{Morais de S\u00e1}, Ana"
        ],
        "keywords": [
            "Dynamic road space allocation, Big data, Smart cities, Intelligent transportation systems, Information and communication technology, Urban planning"
        ],
        "abstract": "Urban planning has focused on reallocating road space from automobile to more sustainable transport modes in many cities worldwide. Mostly in urban areas, road space (from fa\u00e7ade to fa\u00e7ade) is highly disputed by different urban activities and functions. Nonetheless, there are varying demand periods during the day in which road space is underutilized due to its static design. Underutilized spaces could be used for other mobility or access purposes to improve efficiency. Sensing road space, using big data and transport demand management tools, may characterize different demand patterns, adapt the road space dynamically and, ultimately, promote efficiency in using a scarce resource, such as urban road space. This approach also reinforces short-term flexibility in urban planning, allowing for better responses to unpredictable events. This paper defines the concept of dynamic road space allocation by discussing the previous literature on dynamic allocation of space. We propose a methodological framework and discuss the technological solutions as well as the many challenges of implementing dynamic road space allocation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00489697",
        "isbn": null,
        "journal": "Science of The Total Environment",
        "publisher": null,
        "title": "digitalizationtoachievesustainabledevelopmentgoalsstepstowardsasmartgreenplanet",
        "booktitle": null,
        "doi": "10.1016/j.scitotenv.2021.148539",
        "author": [
            "Mondejar, Maria",
            "Avtar, Ram",
            "Diaz, Heyker",
            "Dubey, Rama",
            "Esteban, Jes\u00fas",
            "G\u00f3mez-Morales, Abigail",
            "Hallam, Brett",
            "Mbungu, Nsilulu",
            "Okolo, Chukwuebuka",
            "Prasad, Kumar",
            "She, Qianhong",
            "Garcia-Segura, Sergi"
        ],
        "keywords": [
            "Digitalization, Food-water-energy nexus, Internet of things, Geographic information system (GIS), Sustainable development"
        ],
        "abstract": "Digitalization provides access to an integrated network of unexploited big data with potential benefits for society and the environment. The development of smart systems connected to the internet of things can generate unique opportunities to strategically address challenges associated with the United Nations Sustainable Development Goals (SDGs) to ensure an equitable, environmentally sustainable, and healthy society. This perspective describes the opportunities that digitalization can provide towards building the sustainable society of the future. Smart technologies are envisioned as game-changing tools, whereby their integration will benefit the three essential elements of the food-water-energy nexus: (i) sustainable food production; (ii) access to clean and safe potable water; and (iii) green energy generation and usage. It then discusses the benefits of digitalization to catalyze the transition towards sustainable manufacturing practices and enhance citizens' health wellbeing by providing digital access to care, particularly for the underserved communities. Finally, the perspective englobes digitalization benefits by providing a holistic view on how it can contribute to address the serious challenges of endangered planet biodiversity and climate change.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.963",
        "scimago_value": "1,795"
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "dataqualityawaretaskoffloadinginmobileedgecomputinganoptimalstoppingtheoryapproach",
        "booktitle": null,
        "doi": "10.1016/j.future.2020.12.017",
        "author": [
            "Alghamdi, Ibrahim",
            "Anagnostopoulos, Christos",
            "Pezaros, Dimitrios"
        ],
        "keywords": [
            "Mobile edge computing, Tasks offloading, Data quality, Optimal stopping theory, Sequential decision making"
        ],
        "abstract": "An important use case of the Mobile Edge Computing (MEC) paradigm is task and data offloading. Computational offloading is beneficial for a wide variety of mobile applications on different platforms including autonomous vehicles and smartphones. With the envision deployment of MEC servers along the roads and while mobile nodes are moving and having certain tasks (or data) to be offloaded to edge servers, choosing an appropriate time and an ideally suited MEC server to guarantee the Quality of Service (QoS) is challenging. We tackle the data quality-aware offloading sequential decision making problem by adopting the principles of Optimal Stopping Theory (OST) to minimize the expected processing time. A variety of OST stochastic models and their applications to the offloading decision making problem are investigated and assessed. A performance evaluation is provided using simulation approach and real world data sets together with the assessment of baseline deterministic and stochastic offloading models. The results show that the proposed OST models can significantly minimize the expected processing time for analytics task execution and can be implemented in the mobile nodes efficiently.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23519789",
        "isbn": null,
        "journal": "Procedia Manufacturing",
        "publisher": null,
        "title": "quality40greenblackandmasterblackbeltcurricula",
        "booktitle": null,
        "doi": "10.1016/j.promfg.2021.06.085",
        "author": [
            "Escobar, Carlos",
            "Chakraborty, Debejyo",
            "McGovern, Megan",
            "Macias, Daniela",
            "Morales-Menendez, Ruben"
        ],
        "keywords": [
            "Quality 4.0, Certification, Smart manufacturing, Artificial intelligence, Big data"
        ],
        "abstract": "Industrial Big Data (IBD) and Artificial Intelligence (AI) are propelling the new era of manufacturing - smart manufacturing. Manufacturing companies can competitively position themselves amongst the most advanced and influential companies by successfully implementing Quality 4.0 practices. Despite the global impact of COVID-19 and the low deployment success rate, industrialization of the AI mega-trend has dominated the business landscape in 2020. Although these technologies have the potential to advance quality standards, it is not a trivial task. A significant portion of quality leaders do not yet have a clear deployment strategy and universally cite difficulty in harnessing such technologies. The lack of people power is one of the biggest challenges. From a career development standpoint, the higher-educated employees (such as engineers) are the most exposed to, and thus affected by, these new technologies. 79% of young professionals have reported receiving training outside of formal schooling to acquire the necessary skills for Industry 4.0. Strategically investing in training is thus important for manufacturing companies to generate value from IBD and AI. Following the path traced by Six Sigma, this article presents a certification curricula for Green, Black, and Master Black Belts. The proposed curriculum combines six areas of knowledge: statistics, quality, manufacturing, programming, learning, and optimization. These areas, along with an ad hoc 7-step problem solving strategy, must be mastered to obtain a certification. Certified professionals will be well positioned to deploy Quality 4.0 technologies and strategies. They will have the capacity to identify engineering intractable problems that can be formulated as machine learning problems and successfully solve them. These certifications are an efficient and effective way for professionals to advance in their career and thrive in Industry 4.0.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,504"
    },
    {
        "issnkey": "09255273",
        "isbn": null,
        "journal": "International Journal of Production Economics",
        "publisher": null,
        "title": "smartproductservicesystemhierarchicalmodelinbankingindustryunderuncertainties",
        "booktitle": null,
        "doi": "10.1016/j.ijpe.2021.108244",
        "author": [
            "Tseng, Ming-Lang",
            "Bui, Tat-Dat",
            "Lan, Shulin",
            "Lim, Ming",
            "Mashud, Abu"
        ],
        "keywords": [
            "Smart product-service systems, Digital technology, Sustainable innovation, Fuzzy delphi method, Decision-making trial and evaluation laboratory (DEMATEL), Diffusion of innovation theory"
        ],
        "abstract": "This study adopts the diffusion of innovation theory as to develop the smart product service system model in banking industry due to prior studies are lacking in identifying the attributes. The smart product service system functions are bearing high uncertainty and system complexity; hence, the hybrid method of fuzzy Delphi method and fuzzy decision-making trial and evaluation laboratory to construct a valid hierarchical model and identified the causal interrelationships among the attributes. The smart product service system hierarchical model with eight aspects and 41 criteria are proposed enriching the existing literature and that identify appropriate strategies to achieve operational performance. The results show that seven aspects and 22 criteria are determined as the valid hierarchical model. The institutional compression, digital platform operation, and e-knowledge management are the causing aspects helps to form smart product service system operational performance in high uncertainty. For practices, the banking decision-makers should develop innovative actions relied on the forcible compression, cyber-physical systems, industrial big data, cloud service allocation and sharing, and transparency improvement as they are most importance criteria playing a decisive role in a successful SPSS. This provides guidelines for banking industry practice in Taiwan encouraging the miscellany of digital technology accomplishment for sustainable target.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.885",
        "scimago_value": "2,406"
    },
    {
        "issnkey": "00652423",
        "isbn": null,
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapterfourtranslationalbiomarkersintheeraofprecisionmedicine",
        "booktitle": null,
        "doi": "10.1016/bs.acc.2020.08.002",
        "author": [
            "Bravo-Merodio, Laura",
            "Acharjee, Animesh",
            "Russ, Dominic",
            "Bisht, Vartika",
            "Williams, John",
            "Tsaprouni, Loukia",
            "Gkoutos, Georgios"
        ],
        "keywords": [
            "Translational biomarkers, Omics, Big data, Artificial intelligence, Clinical trials"
        ],
        "abstract": "In this chapter we discuss the past, present and future of clinical biomarker development. We explore the advent of new technologies, paving the way in which health, medicine and disease is understood. This review includes the identification of physicochemical assays, current regulations, the development and reproducibility of clinical trials, as well as, the revolution of omics technologies and state-of-the-art integration and analysis approaches.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": "5.394",
        "scimago_value": "1,330"
    },
    {
        "issnkey": "03608352",
        "isbn": null,
        "journal": "Computers & Industrial Engineering",
        "publisher": null,
        "title": "processminingforsixsigmautilisingdigitaltraces",
        "booktitle": null,
        "doi": "10.1016/j.cie.2020.107083",
        "author": [
            "Kregel, I.",
            "Stemann, D.",
            "Koch, J.",
            "Coners, A."
        ],
        "keywords": [
            "process mining, six sigma, DMAIC, big data analytics, data science, project management"
        ],
        "abstract": "Six Sigma is one of the most successful quality management philosophies of the past 20 years. However, the current challenges facing companies, such as rising process and supply chain complexity, as well as high volumes of unstructured data, cannot easily be answered by relying on traditional Six Sigma tools. Instead, the Process Mining (PM) technology using big data analytics promises valuable support for 6S and its data analysis capabilities. The article presents a design science research project in which a method for the integration of PM in Six Sigma\u2019s DMAIC project structure was developed. This method could be extended, refined and tested during three evaluation cycles: an expert evaluation with Six Sigma professionals, a technical experiment and finally a multi case study in a company. The method therefore was eventually endorsed by 6S experts and successfully applied in a first pilot setting. This article presents the first developed method for the integration of PM and Six Sigma. It follows the recommendations of many researchers to test Six Sigma as an application field of PM as well as using the potential of big data analytics. The method can be used by researchers and practitioners alike to implement, test and verify its design in organisations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,315"
    },
    {
        "issnkey": "03787206",
        "isbn": null,
        "journal": "Information & Management",
        "publisher": null,
        "title": "streamsofdigitaldataandcompetitiveadvantagethemediationeffectsofprocessefficiencyandproducteffectiveness",
        "booktitle": null,
        "doi": "10.1016/j.im.2021.103451",
        "author": [
            "Raguseo, Elisabetta",
            "Pigni, Federico",
            "Vitari, Claudio"
        ],
        "keywords": [
            "Streams of big data, Process efficiency, Product effectiveness, Competitive advantage"
        ],
        "abstract": "Firms can achieve a competitive advantage by leveraging real-time Digital Data Streams (DDSs). The ability to profit from DDSs is emerging as a critical competency for firms and a novel area for Information Technology (IT) investments. We examine the relationship between DDS readiness and competitive advantage by studying the mediation effect of product effectiveness and process efficiency. The research model is tested with data obtained from 302 companies, and the results confirm the existence of the mediation effects. Interestingly, we confirm that competitive advantage is more significantly impacted by IT investments affecting product effectiveness than those affecting process efficiency.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,147"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "towardsabusinessanalyticscapabilityforthecirculareconomy",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.120957",
        "author": [
            "Kristoffersen, Eivind",
            "Mikalef, Patrick",
            "Blomsma, Fenna",
            "Li, Jingyue"
        ],
        "keywords": [
            "Digital circular economy, Sustainability, Big data analytics, Competitive advantage, Resource-based view, Expert interviews"
        ],
        "abstract": "Digital technologies are growing in importance for accelerating firms\u2019 circular economy transition. However, so far, the focus has primarily been on the technical aspects of implementing these technologies with limited research on the organizational resources and capabilities required for successfully leveraging digital technologies for circular economy. To address this gap, this paper explores the business analytics resources firms should develop and how these should be orchestrated towards a firm-wide capability. The paper proposes a conceptual model highlighting eight business analytics resources that, in combination, build a business analytics capability for the circular economy and how this relates to firms\u2019 circular economy implementation, resource orchestration capability, and competitive performance. The model is based on the results of a thematic analysis of 15 semi-structured expert interviews with key positions in industry. Our approach is informed by and further develops, the theory of the resource-based view and the resource orchestration view. Based on the results, we develop a deeper understanding of the importance of taking a holistic approach to business analytics when leveraging data and analytics towards a more efficient and effective digital-enabled circular economy, the smart circular economy.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "13191578",
        "isbn": null,
        "journal": "Journal of King Saud University - Computer and Information Sciences",
        "publisher": null,
        "title": "improvingoutliersdetectionindatastreamsusinglicsandvoting",
        "booktitle": null,
        "doi": "10.1016/j.jksuci.2019.08.003",
        "author": [
            "Benjelloun, Fatima-Zahra",
            "Oussous, Ahmed",
            "Bennani, Amine",
            "Belfkih, Samir",
            "{Ait Lahcen}, Ayoub"
        ],
        "keywords": [
            "Data streams, Outlier detection, High-dimensional data, Big data mining, Intrusion detection"
        ],
        "abstract": "Detecting outliers in real-time is increasingly important for many real-world applications such as detecting abnormal heart activity, intrusions to systems, spams or abnormal credit card transactions. However, detecting outliers in data streams rises many challenges such as high-dimensionality, dynamic data distribution and unpredictable relationships. Our simulations demonstrate that some advanced solutions still show drawbacks. In this paper, first, we improve the capacity to detect outliers of both micro-clusters based algorithms (MCOD) and distance-based algorithms (Abstract-C and Exact-Storm) known for their performance. This is by adding a layer called LiCS that classifies online the K-nearest-neighbors (Knn) of each node based on their evolutionary status. This layer aggregates the results and uses a count threshold to better classify nodes. Experiments on SpamBase datasets confirmed that our technique enhances the accuracy and the precision of such algorithm and helps to reduce the unclassified nodes.Second, we propose a hybrid solution based on iterative majority voting and our LiCS. Experiments on real data proves that it outperforms discussed algorithms in terms of accuracy, precision and sensitivity in detecting outliers. It also minimizes the issue of unclassified instances and consolidate the different outputs of algorithms.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "analysisofbarrierstoindustry40adoptioninmanufacturingorganizationsanismapproach",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.01.010",
        "author": [
            "Kumar, Pramod",
            "Bhamu, Jaiprakash",
            "Sangwan, Kuldip"
        ],
        "keywords": [
            "Industry 4.0, interpretive structural modeling, digital manufacturing, barriers, MICMAC analysis"
        ],
        "abstract": "Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "09242716",
        "isbn": null,
        "journal": "ISPRS Journal of Photogrammetry and Remote Sensing",
        "publisher": null,
        "title": "lidarshedsnewlightonplantphenomicsforplantbreedingandmanagementrecentadvancesandfutureprospects",
        "booktitle": null,
        "doi": "10.1016/j.isprsjprs.2020.11.006",
        "author": [
            "Jin, Shichao",
            "Sun, Xiliang",
            "Wu, Fangfang",
            "Su, Yanjun",
            "Li, Yumei",
            "Song, Shiling",
            "Xu, Kexin",
            "Ma, Qin",
            "Baret, Fr\u00e9d\u00e9ric",
            "Jiang, Dong",
            "Ding, Yanfeng",
            "Guo, Qinghua"
        ],
        "keywords": [
            "Lidar, Traits, Phenomics, Breeding, Management, Multi-omics"
        ],
        "abstract": "Plant phenomics is a new avenue for linking plant genomics and environmental studies, thereby improving plant breeding and management. Remote sensing techniques have improved high-throughput plant phenotyping. However, the accuracy, efficiency, and applicability of three-dimensional (3D) phenotyping are still challenging, especially in field environments. Light detection and ranging (lidar) provides a powerful new tool for 3D phenotyping with the rapid development of facilities and algorithms. Numerous efforts have been devoted to studying static and dynamic changes of structural and functional phenotypes using lidar in agriculture. These progresses also improve 3D plant modeling across different spatial\u2013temporal scales and disciplines, providing easier and less expensive association with genes and analysis of environmental practices and affords new insights into breeding and management. Beyond agriculture phenotyping, lidar shows great potential in forestry, horticultural, and grass phenotyping. Although lidar has resulted in remarkable improvements in plant phenotyping and modeling, the synthetization of lidar-based phenotyping for breeding and management has not been fully explored. We identify three main challenges in lidar-based phenotyping development: 1) developing low cost, high spatial\u2013temporal, and hyperspectral lidar facilities, 2) moving into multi-dimensional phenotyping with an endeavor to generate new algorithms and models, and 3) embracing open source and big data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.979",
        "scimago_value": "2,960"
    },
    {
        "issnkey": "13865056",
        "isbn": null,
        "journal": "International Journal of Medical Informatics",
        "publisher": null,
        "title": "predictionofearlychildhoodobesitywithmachinelearningandelectronichealthrecorddata",
        "booktitle": null,
        "doi": "10.1016/j.ijmedinf.2021.104454",
        "author": [
            "Pang, Xueqin",
            "Forrest, Christopher",
            "L\u00ea-Scherban, F\u00e9lice",
            "Masino, Aaron"
        ],
        "keywords": [
            "Data quality control, Early childhood obesity, Electronic health record, Machine learning, Prediction"
        ],
        "abstract": "Objective This study compares seven machine learning models developed to predict childhood obesity from age > 2 to \u2264 7 years using Electronic Healthcare Record (EHR) data up to age 2 years. Materials and methods EHR data from of 860,510 patients with 11,194,579 healthcare encounters were obtained from the Children\u2019s Hospital of Philadelphia. After applying stringent quality control to remove implausible growth values and including only individuals with all recommended wellness visits by age 7 years, 27,203 (50.78 % male) patients remained for model development. Seven machine learning models were developed to predict obesity incidence as defined by the Centers for Disease Control and Prevention (age/sex adjusted BMI>95th percentile). Model performance was evaluated by multiple standard classifier metrics and the differences among seven models were compared using the Cochran's Q test and post-hoc pairwise testing. Results XGBoost yielded 0.81 (0.001) AUC, which outperformed all other models. It also achieved statistically significant better performance than all other models on standard classifier metrics (sensitivity fixed at 80 %): precision 30.90 % (0.22 %), F1-socre 44.60 % (0.26 %), accuracy 66.14 % (0.41 %), and specificity 63.27 % (0.41 %). Discussion and conclusion Early childhood obesity prediction models were developed from the largest cohort reported to date. Relative to prior research, our models generalize to include males and females in a single model and extend the time frame for obesity incidence prediction to 7 years of age. The presented machine learning model development workflow can be adapted to various EHR-based studies and may be valuable for developing other clinical prediction models.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "07430167",
        "isbn": null,
        "journal": "Journal of Rural Studies",
        "publisher": null,
        "title": "imprecisionfarmingexaminingtheinaccuracyandrisksofdigitalagriculture",
        "booktitle": null,
        "doi": "10.1016/j.jrurstud.2021.07.024",
        "author": [
            "Visser, Oane",
            "Sippel, Sarah",
            "Thiemann, Louis"
        ],
        "keywords": [
            "Digital agriculture, Smart farming, Precision agriculture, Accuracy, Big data"
        ],
        "abstract": "The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows \u2018doing more with less\u2019 through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often \u2018precisely inaccurate\u2019, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of \u2018ultra-precise\u2019 digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a \u2018precision trap\u2019. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of \u2018precision traps\u2019 increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging \u2018precision divide\u2019: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like \u2018imprecision farming\u2019.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.849",
        "scimago_value": "1,497"
    },
    {
        "issnkey": "26666030",
        "isbn": null,
        "journal": "International Journal of Intelligent Networks",
        "publisher": null,
        "title": "frameworkofdataanalyticsandintegratingknowledgemanagement",
        "booktitle": null,
        "doi": "10.1016/j.ijin.2021.09.004",
        "author": [
            "Schaefer, Camilla",
            "Makatsaria, Ana"
        ],
        "keywords": [
            "Data analytics, Knowledge management, Big data, Business intelligence, Data discovery"
        ],
        "abstract": "Big data is significantly dependent on technologies such as cloud computing, machine learning and statistical models. However, its significance is becoming more dependent on human qualities e.g. judgment, value, intuition and experience. Therefore, the human knowledge presents a basis for knowledge management and big data, which are a major element of data analytics. This research contribution applies the process of Data, Information, Knowledge and Perception hierarchy as a structure to evaluate the end-users\u2019 process. The framework in incorporating data analytics and display a conceptual data analytics process (with three phases) evaluated as knowledge management, including the creation, discovery and application of knowledge. Knowledge conversion theories are applicable in data analytics to emphasize on the typically overlooked organizational and human aspects, which are critical to the efficiency of data analytics. The synergy and alignment between knowledge management and data analytics is fundamental in fostering innovations and collaboration.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "1674862x",
        "isbn": null,
        "journal": "Journal of Electronic Science and Technology",
        "publisher": null,
        "title": "impactofcoronaviruspandemiccrisisontechnologiesandcloudcomputingapplications",
        "booktitle": null,
        "doi": "10.1016/j.jnlest.2020.100059",
        "author": [
            "Alashhab, Ziyad",
            "Anbar, Mohammed",
            "Singh, Manmeet",
            "Leau, Yu-Beng",
            "Al-Sai, Zaher",
            "{Abu Alhayja\u2019a}, Sami"
        ],
        "keywords": [
            "Big data privacy, Cloud computing (CC) applications, COVID-19, Digital transformation, Security challenge, Work from home"
        ],
        "abstract": "In light of the COVID-19 outbreak caused by the novel coronavirus, companies and institutions have instructed their employees to work from home as a precautionary measure to reduce the risk of contagion. Employees, however, have been exposed to different security risks because of working from home. Moreover, the rapid global spread of COVID-19 has increased the volume of data generated from various sources. Working from home depends mainly on cloud computing (CC) applications that help employees to efficiently accomplish their tasks. The cloud computing environment (CCE) is an unsung hero in the COVID-19 pandemic crisis. It consists of the fast-paced practices for services that reflect the trend of rapidly deployable applications for maintaining data. Despite the increase in the use of CC applications, there is an ongoing research challenge in the domains of CCE concerning data, guaranteeing security, and the availability of CC applications. This paper, to the best of our knowledge, is the first paper that thoroughly explains the impact of the COVID-19 pandemic on CCE. Additionally, this paper also highlights the security risks of working from home during the COVID-19 pandemic.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00200255",
        "isbn": null,
        "journal": "Information Sciences",
        "publisher": null,
        "title": "anticollusiondataauctionmechanismbasedonsmartcontract",
        "booktitle": null,
        "doi": "10.1016/j.ins.2020.10.053",
        "author": [
            "Xiong, Wei",
            "Xiong, Li"
        ],
        "keywords": [
            "Data auction mechanism, Anti-collusion, Smart contract, Blockchain, Ethereum"
        ],
        "abstract": "Due to the uncertainty of the value of big data, it is difficult to directly give a reasonable price for big data. Auction is an effective method of distributing goods to the bidder with the highest valuation. Hence, the use of auction strategy can not only guarantee the interests of data sellers, but also conform to market principles. However, existing data auction mechanisms are centralized. It is hard to build trust among sellers, buyers and auctioneers. An open and anonymous online environment may cause entities involved in data auctions to collude to manipulate the results of data auctions. This will cause the price of auction data to fail to reach a fair and truthful level. Therefore, the first anti-collusion data auction mechanism based on smart contract is proposed. Through a well-designed anti-collusion data auction algorithm, mutual distrust and rational buyers and sellers safely participate in the data auction without a trusted third party. The data auction mechanism designed in the smart contract can effectively prevent collusion and realize the fairness and truthfulness of data auction. The webpack in the Truffle Boxes is used to implement the data auction mechanism, and the anti-collusion property of the mechanism has been verified. The source code of the smart contract has been uploaded to GitHub.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.795",
        "scimago_value": "1,524"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820273-9",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter17ethicalandlegalchallenges",
        "booktitle": "Machine Learning in Cardiovascular Medicine",
        "doi": "10.1016/B978-0-12-820273-9.00017-8",
        "author": [
            "Tat, Emily",
            "Rabbat, Mark"
        ],
        "keywords": [
            "Artificial intelligence, Autonomy, Big data, Black box, Ethics, Informed consent, Liability, Privacy, Safety"
        ],
        "abstract": "As the technology of artificial intelligence (AI) grows in cardiovascular medicine, so do the ethical and legal challenges that come with it. Currently, the medical community is ill-informed of what these challenges entail, and policy and ethical guidelines are lacking. Physicians and policy makers should be informed of these issues to minimize harm and promote patient care. Three overarching themes relating to the data, the algorithms, and the results comprise the foundation of these challenges and will be discussed in this chapter. The introduction of big data raises concern for patient privacy and security, with issues of data quality and inconsistent medical records. There is also risk for biases in the algorithms that could worsen health disparities or skew results for financial gain. Finally, the archetypal \u201cblack box\u201d algorithm, questions of legal liability, and what happens when humans and machine disagree are discussed in depth. Ultimately, a code of ethics in the coming integration of AI is needed to ensure the preservation of human rights.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "anapproachtodatastructuringandpredictiveanalysisindiscretemanufacturing",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.224",
        "author": [
            "\u00d8ien, Christian",
            "Dransfeld, Sebastian"
        ],
        "keywords": [
            "Anomaly Detection, Predictive Maintenance, Discrete Manufacturing, Big Data Analytics, Adaptive Self-learning Systems"
        ],
        "abstract": "In discrete manufacturing the variation in process parameters and duration is often large. Common data storage and analytics systems primarily store data in univariate time series, and when analysing machine components of strongly varying lifetime and behaviour this causes a challenge. This paper presents a data structure and an analysis method for outlier detection which intends to deal with this challenge, as an alternative to predictive maintenance which often requires more data with higher quality than what is available. A case study in aluminium extrusion billet manufacturing is used to demonstrate the approach, predominantly detecting anomalies at the end of a critical component\u2019s lifetime.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "01403664",
        "isbn": null,
        "journal": "Computer Communications",
        "publisher": null,
        "title": "towardshighqualitymobilecrowdsensingincentivemechanismdesignbasedonfinegrainedabilityreputation",
        "booktitle": null,
        "doi": "10.1016/j.comcom.2021.09.026",
        "author": [
            "Luo, Zhuangye",
            "Xu, Jia",
            "Zhao, Pengcheng",
            "Yang, Dejun",
            "Xu, Lijie",
            "Luo, Jian"
        ],
        "keywords": [
            "Mobile crowdsensing, Incentive mechanism, Quality-aware, Reputation system"
        ],
        "abstract": "Mobile crowdsensing has become an efficient paradigm for performing large-scale sensing tasks. Many quality-aware incentive mechanisms for mobile crowdsensing have been proposed. However, most of them measure the data quality by one single metric from a specific perspective. Moreover, they usually use the real-time quality, which cannot provide sufficient incentive for the workers with long-term high quality. In this paper, we refine the generalized data quality into the fine-grained ability requirement. We present a mobile crowdsensing system to achieve the fine-grained quality control, and formulate the problem of maximizing the social cost such that the fine-grained ability requirement of all sensing tasks can be satisfied. To stimulate the workers with long-term high quality, we design two ability reputation systems to assess workers\u2019 fine-grained abilities online. The incentive mechanism based on the reverse auction and fine-grained ability reputation system is proposed. We design a greedy algorithm to select the winners and determine the payment based on the bids and fine-grained ability reputation of workers. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed mechanisms achieve computational efficiency, individual rationality, truthfulness, whitewashing proof, and guaranteed approximation. Moreover, the designed mechanisms show prominent advantage in terms of social cost and average ability achievement ratio.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.167",
        "scimago_value": "0,627"
    },
    {
        "issnkey": "09204105",
        "isbn": null,
        "journal": "Journal of Petroleum Science and Engineering",
        "publisher": null,
        "title": "automatedpressuretransientanalysisacloudbasedapproach",
        "booktitle": null,
        "doi": "10.1016/j.petrol.2020.107627",
        "author": [
            "Guo, Yonggui",
            "Mohamed, Ibrahim",
            "Zidane, Ali",
            "Panchal, Yashesh",
            "Abou-Sayed, Omar",
            "Abou-Sayed, Ahmed"
        ],
        "keywords": [
            "Pressure transient analysis, Cloud computing, ISIP, Flow regime, G-function, Web-based application"
        ],
        "abstract": "Pressure transient analysis provides essential information to evaluate the dimensions of injection induced fractures, permeability damage near the wellbore, and pressure elevation in the injection horizon. For injection wells, shut-in data can be collected and analyzed after each injection cycle to evaluate the well injectivity and predict the well longevity. However, any interactive analysis of the pressure data could be subjective and time-consuming. In this study a novel cloud-based approach to automatically analyzing pressure data is presented, which aims to improve the reliability and efficiency of pressure transient analysis. There are two fundamental requirements for automated pressure transient analysis: 1) Pressure data needs to be automatically retrieved from field sites and fed to the analyzer; 2) The engineer can automatically select instantaneous shut-in pressure (ISIP), identify flow regimes, and determine the fracture closure point if any. To meet these requirements as well as to take advantage of cloud storage and computing technologies, a web-based application has been developed to pull real time injection data from any field sites and push it to a cloud database. A built-in pressure transient workflow has been also proposed to detect any stored or real-time pressure data and perform pressure analysis automatically if the required data is available. The automated pressure transient analysis technology has been applied to multiple injection projects. In general, the analysis results including formation and fracture properties (i.e. permeability, fracture half length, skin factor, and fracture closure pressure) are comparable to results from interactive analysis. Any discrepancies are mainly caused by poor data quality. Issues such as inconsistent selections of ISIP and different slopes defined for pre and after closure analyses also contribute to the divergence. Overall, the automated pressure transient analysis provides consistent results as the exact same criteria are applied to the pressure data, and analysis results are independent of the analyzer's experience and knowledge. As data from oil/gas industry increases exponentially over time, automated data transmission, storage, analysis and access are becoming necessary to maximize the value of the data and reduce operation cost. The automated pressure transient analysis presented here demonstrates that cloud storage and computing combined with automated analysis tools is a viable way to overcome big data challenges faced by oil/gas industry professionals.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.346",
        "scimago_value": "0,975"
    },
    {
        "issnkey": "26671026",
        "isbn": null,
        "journal": "Intelligent Medicine",
        "publisher": null,
        "title": "thinkingontheinformatizationdevelopmentofchinashealthcaresysteminthepostcovid19era",
        "booktitle": null,
        "doi": "10.1016/j.imed.2021.03.004",
        "author": [
            "Zhang, Ming",
            "Dai, Danyun",
            "Hou, Siliang",
            "Liu, Wei",
            "Gao, Feng",
            "Xu, Dong",
            "Hu, Yu"
        ],
        "keywords": [
            "Coronavirus disease 2019, Healthcare system, Informatization"
        ],
        "abstract": "With the application of Internet of Things, big data, cloud computing, artificial intelligence, and other cutting-edge technologies, China's medical informatization is developing rapidly. In this paper, we summaried the role of information technology in healthcare sector's battle against the Coronavirus disease 2019 (COVID-19) from the perspectives of early warning and monitoring, screening and diagnosis, medical treatment and scientific research, analyzes the bottlenecks of the development of information technology in the post-COVID-19 era, and puts forward feasible suggestions for further promoting the construction of medical informatization from the perspectives of sharing, convenience, and safety.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "25890042",
        "isbn": null,
        "journal": "iScience",
        "publisher": null,
        "title": "comprehensivestrategiesofmachinelearningbasedquantitativestructureactivityrelationshipmodels",
        "booktitle": null,
        "doi": "10.1016/j.isci.2021.103052",
        "author": [
            "Mao, Jiashun",
            "Akhtar, Javed",
            "Zhang, Xiao",
            "Sun, Liang",
            "Guan, Shenghui",
            "Li, Xinyu",
            "Chen, Guangming",
            "Liu, Jiaxin",
            "Jeon, Hyeon-Nae",
            "Kim, Min",
            "No, Kyoung",
            "Wang, Guanyu"
        ],
        "keywords": [
            "Data analysis in structural biology, Machine learning, Structural biology"
        ],
        "abstract": "Summary Early quantitative structure-activity relationship (QSAR) technologies have unsatisfactory versatility and accuracy in fields such as drug discovery because they are based on traditional machine learning and interpretive expert features. The development of Big Data and deep learning technologies significantly improve the processing of unstructured data and unleash the great potential of QSAR. Here we discuss the integration of wet experiments (which provide experimental data and reliable verification), molecular dynamics simulation (which provides mechanistic interpretation at the atomic/molecular levels), and machine learning (including deep learning) techniques to improve QSAR models. We first review the history of traditional QSAR and point out its problems. We then propose a better QSAR model characterized by a new iterative framework to integrate machine learning with disparate data input. Finally, we discuss the application of QSAR and machine learning to many practical research fields, including drug development and clinical trials.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.458",
        "scimago_value": "1,805"
    },
    {
        "issnkey": "23528648",
        "isbn": null,
        "journal": "Digital Communications and Networks",
        "publisher": null,
        "title": "towardintelligentwirelesscommunicationsdeeplearningbasedphysicallayertechnologies",
        "booktitle": null,
        "doi": "10.1016/j.dcan.2021.09.014",
        "author": [
            "Liu, Siqi",
            "Wang, Tianyu",
            "Wang, Shaowei"
        ],
        "keywords": [
            "Data-driven, Deep learning, Physical layer, Wireless communications"
        ],
        "abstract": "Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.797",
        "scimago_value": "1,082"
    },
    {
        "issnkey": "26665182",
        "isbn": null,
        "journal": "Current Research in Behavioral Sciences",
        "publisher": null,
        "title": "aidingproenvironmentalbehaviormeasurementbyinternetofthings",
        "booktitle": null,
        "doi": "10.1016/j.crbeha.2021.100055",
        "author": [
            "Xia, Ziqian",
            "Liu, Yurong"
        ],
        "keywords": [
            "Pro-environmental behavior, Internet of Things, Measurement, Big data, Environmental psychology"
        ],
        "abstract": "Promoting pro-environmental behavior is an effective means of reducing carbon emissions at the individual end, but the measurement of behavior has long been a problem for scholars. Especially in environmental psychology community, the complexity of social policies and habitat implies greater difficulty in measuring. Due to the limitations of traditional questionnaire, laboratory, and naturalistic observation methods, environmental psychologists need more realistic, accurate, and cost-effective ways to measure behavior. The rapid development of IoT technology lights up the hope for achieving this goal, and its large-scale popularization will bring great changes to the research community. This paper reviews the current methods and their limitations, proposes a framework for measuring behavior using IoT devices, and points out its future research directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "2352152x",
        "isbn": null,
        "journal": "Journal of Energy Storage",
        "publisher": null,
        "title": "stateofhealthestimationbasedonrealdataofelectricvehiclesconcerninguserbehavior",
        "booktitle": null,
        "doi": "10.1016/j.est.2021.102867",
        "author": [
            "He, Zhigang",
            "Shen, Xiaoyu",
            "Sun, Yanyan",
            "Zhao, Shichao",
            "Fan, Bin",
            "Pan, Chaofeng"
        ],
        "keywords": [
            "Electric vehicles, SOH, User behavior, LWLR, LSTM"
        ],
        "abstract": "State of health (SOH) of lithium-ion battery pack directly determines the driving mileage and output power of the electric vehicle. With the development of big data storage and analysis technology, using big data to off-line estimate battery pack SOH is more feasible than before. This paper proposes a SOH estimation method based on real data of electric vehicles concerning user behavior. The charging capacity is calculated by historical charging data, and locally weighted linear regression (LWLR) algorithm is used to qualitatively characterize the capacity decline trend. The health features are extracted from historical operating data, maximal information coefficient (MIC) algorithm is used to measure the correlation between health features and capacity. Then, long and short-term memory (LSTM)-based neural network will further learn the nonlinear degradation relationship between capacity and health features. Bayesian optimization algorithm is used to ensure the generalization of the model when different electric vehicles produce different user behaviors. The estimation method is validated by the 300 days historical dataset from 100 vehicles with different driving behavior. The results indicates that the maximum relative error of estimating SOH is 0.2%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-817390-9",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter10fromgreenpeastostevecitizenscienceengagementinspacescience",
        "booktitle": "Space Science and Public Engagement",
        "doi": "10.1016/B978-0-12-817390-9.00009-9",
        "author": [
            "Fortson, Lucy"
        ],
        "keywords": [
            "Citizen Science, Crowdsourcing, Machine learning, Volunteer engagement"
        ],
        "abstract": "The past two decades has seen a tremendous rise in citizen science and crowdsourcing techniques as a means to carry out ground-breaking research while at the same time engage the general public in the wonders of space science. This article reviews some of the recent advances made in this realm as well as lessons learned from the unique perspective of the author\u2019s role as a cofounder of the Zooniverse citizen science platform and practicing astrophysics researcher. I briefly describe the factors that led to the recent rise of citizen science including the formation of governance bodies at national and international levels, and the adoption by Federal Agencies within the United States government. I address concerns raised by research colleagues on the validity of citizen science as a research methodology, and then describe several key metrics for the success of citizen science including the link between data quality and publications, and the critical role that motivation and engagement of volunteer participants play in project success. I use the Green Pea galaxies discovered by Galaxy Zoo volunteers and an aurora-like phenomenon known as STEVE discovered by Aurorasaurus volunteers as examples of how, with the right tools and support, non-professional volunteers can make key contributions to space science. I then describe the role that machine learning can play when judiciously teamed with citizen scientists to tackle the ever-growing challenge of big data and close with some reflections on what it takes to support and manage a large platform like the Zooniverse.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "08867798",
        "isbn": null,
        "journal": "Tunnelling and Underground Space Technology",
        "publisher": null,
        "title": "perspectivesofbigexperimentaldatabaseandartificialintelligenceintunnelfireresearch",
        "booktitle": null,
        "doi": "10.1016/j.tust.2020.103691",
        "author": [
            "Zhang, Xiaoning",
            "Wu, Xiqiang",
            "Park, Younggi",
            "Zhang, Tianhang",
            "Huang, Xinyan",
            "Xiao, Fu",
            "Usmani, Asif"
        ],
        "keywords": [
            "Big data, Empirical model, Deep learning, Critical event, Smart firefighting"
        ],
        "abstract": "Tunnel fire is one of the most severe global fire hazards and causes a significant amount of economic losses and casualties every year. Over the last 50 years, numerous full-scale and reduced-scale tunnel fire tests, as well as numerical simulations have been conducted to quantify the critical fire events and key parameters to guide the fire safety design of the tunnel. In light of the recent advances in big data and artificial intelligence, this paper aims to establish a database that contains all existing experimental data of tunnel fire, based on an extensive literature review on tunnel fire tests. This tunnel-fire database summarizes seven key parameters of flame, ventilation, and smoke in that is open access at a GitHub site: https://github.com/PolyUFire/Tunnel_Fire_Database. The test conditions, experimental phenomena, and data of each literature work were organized and categorized in a standard format that could be conveniently accessed and continuously updated. Based on this database, machine learning is applied to predict the critical ventilation velocity of a tunnel fire as a demonstration. The review of the current database not only reveals more valuable information and hidden problems in the conventional collection of test data, but also provides new directions in future tunnel fire research. The established database and methodology help promote the application of artificial intelligence and smart firefighting in tunnel fire safety.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.915",
        "scimago_value": "2,172"
    },
    {
        "issnkey": "01403664",
        "isbn": null,
        "journal": "Computer Communications",
        "publisher": null,
        "title": "blockchainmanagementandmachinelearningadaptationforiotenvironmentin5gandbeyondnetworksasystematicreview",
        "booktitle": null,
        "doi": "10.1016/j.comcom.2021.07.009",
        "author": [
            "Miglani, Arzoo",
            "Kumar, Neeraj"
        ],
        "keywords": [
            "Blockchain, Machine learning, Federated learning, Internet of Things, Deep learning, 5G, 6G"
        ],
        "abstract": "Keeping in view of the constraints and challenges with respect to big data analytics along with security and privacy preservation for 5G and B5G applications, the integration of machine learning and blockchain, two of the most promising technologies of the modern era is inevitable. In comparison to the traditional centralized techniques for security and privacy preservation, blockchain uses decentralized consensus algorithms for verification and validation of different transactions which are supposed to become an integral part of blockchain network. Starting with the existing literature survey, we introduce the basic concepts of blockchain and machine learning in this article. Then, we presented a comprehensive taxonomy for integration of blockchain and machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning algorithms usage in blockchain based applications. Finally, we provide recommendations for future use cases of these emerging technologies in 5G and B5G technologies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.167",
        "scimago_value": "0,627"
    },
    {
        "issnkey": "25889141",
        "isbn": null,
        "journal": "Clinical eHealth",
        "publisher": null,
        "title": "applicationofartificialintelligenceinrenaldisease",
        "booktitle": null,
        "doi": "10.1016/j.ceh.2021.11.003",
        "author": [
            "Yao, Lijing",
            "Zhang, Hengyuan",
            "Zhang, Mengqin",
            "Chen, Xing",
            "Zhang, Jun",
            "Huang, Jiyi",
            "Zhang, Lu"
        ],
        "keywords": [
            "Artificial intelligence (AI), Machine learning (ML), Artificial neural network (ANN), Convolution neural network (CNN), Nephrology"
        ],
        "abstract": "Artificial intelligence (AI) has been applied widely in almost every area of our daily lives, due to the growth of computing power, advances in methods and techniques, and the explosion of data, it also plays a critical role in academic disciplines, medicine is not an exception. AI can augment the intelligence of clinicians in diagnosis, prognosis, and treatment decisions.Kidney disease causes great economic burden worldwide, with both acute kidney injury and chronic kidney disease bringing about high morbidity and mortality. Outstanding challenges in nephrology may be addressed by leveraging big data and AI.In this review, we summarized advances in machine learning (ML), artificial neural network (ANN), convolution neural network (CNN) and deep learning (DL), with a special focus on acute kidney injury (AKI), chronic kidney disease (CKD), end-stage renal disease (ESRD), dialysis, kidney transplantation and nephropathology. AI may not be anticipated to replace the nephrologists\u2019 medical decision-making for now, but instead assisting them in providing optimal personalized therapy for patients.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26670100",
        "isbn": null,
        "journal": "Environmental Challenges",
        "publisher": null,
        "title": "necessityofmakingwatersmartforproactiveinformeddecisiveactionsacasestudyoftheuppervaalcatchmentsouthafrica",
        "booktitle": null,
        "doi": "10.1016/j.envc.2021.100100",
        "author": [
            "{du Plessis}, Anja"
        ],
        "keywords": [
            "Data quality, Decisive action, Smart water management, Water quality, South Africa, Upper vaal catchment"
        ],
        "abstract": "The need for informed management of water resources has been continuously highlighted worldwide. Societies are increasingly faced with water quality challenges globally which directly translate into multifaceted challenges. South Africa has acknowledged that water is not receiving the attention and status it deserves. Wastage is rife and degradation widespread. The sustainability of South Africa's freshwater resources has reached a critical point and requires decisive action. Vast amounts of water quality data, varying in quality, is available however the seemingly lack of integrative data management has led to reactive planning and questionable decisions. The paper highlights the necessity for making water smart through a case study of the Upper Vaal catchment. The quality of available government data is mostly of an acceptable standard according to the evaluated data dimensions and elements. The practical application of determining hydrological responses to predict possible water quality changes towards land cover change in the Vaal river catchment emphasises that there is suitable data available and highlights the value of Smart Water Management (SWM). SWM can enable improved integrated water resource management by increasing sharing and effective use of real-time data of acceptable quality to promote proactive unambiguous strategies and decisions focused on overall improved water management and the evasion of a future water predicament.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09213449",
        "isbn": null,
        "journal": "Resources, Conservation and Recycling",
        "publisher": null,
        "title": "sustainablesupplychainmanagementtrendsinworldregionsadatadrivenanalysis",
        "booktitle": null,
        "doi": "10.1016/j.resconrec.2021.105421",
        "author": [
            "Tsai, Feng",
            "Bui, Tat-Dat",
            "Tseng, Ming-Lang",
            "Ali, Mohd",
            "Lim, Ming",
            "Chiu, Anthony"
        ],
        "keywords": [
            "Sustainable supply chain management, Data-driven analysis, Fuzzy Delphi method, Entropy weight method, Fuzzy decision-making trial and evaluation laboratory"
        ],
        "abstract": "This study proposes a data-driven analysis that describes the overall situation and reveals the factors hindering improvement in the sustainable supply chain management field. The literature has presented a summary of the evolution of sustainable supply chain management across attributes. Prior studies have evaluated different parts of the supply chain as independent entities. An integrated systematic assessment is absent in the extant literature and makes it necessary to identify potential opportunities for research direction. A hybrid of data-driven analysis, the fuzzy Delphi method, the entropy weight method and fuzzy decision-making trial and evaluation laboratory is adopted to address uncertainty and complexity. This study contributes to locating the boundary of fundamental knowledge to advance future research and support practical execution. Valuable direction is provided by reviewing the existing literature to identify the critical indicators that need further examination. The results show that big data, closed-loop supply chains, industry 4.0, policy, remanufacturing, and supply chain network design are the most important indicators of future trends and disputes. The challenges and gaps among different geographical regions is offered that provides both a local viewpoint and a state-of-the-art advanced sustainable supply chain management assessment.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23519789",
        "isbn": null,
        "journal": "Procedia Manufacturing",
        "publisher": null,
        "title": "productionreschedulingthroughproductqualityprediction",
        "booktitle": null,
        "doi": "10.1016/j.promfg.2021.07.022",
        "author": [
            "Frye, Maik",
            "Gyulai, D\u00e1vid",
            "Bergmann, J\u00falia",
            "Schmitt, Robert"
        ],
        "keywords": [
            "Machine Learning, Production Scheduling, Product Quality Prediction, Data Quality"
        ],
        "abstract": "In production management, efficient scheduling is key towards smooth and balanced production. Scheduling can be well-supported by real-time data acquisition systems, resulting in decisions that rely on actual or predicted status of production environment and jobs in progress. Utilizing advanced monitoring systems, prediction-based rescheduling method is proposed that can react on in-process scrap predictions, performed by machine learning algorithms. Based on predictions, overall production can be rescheduled with higher efficiency, compared to rescheduling after completion of the whole machining process with realization of scrap. Series of numerical experiments are presented to demonstrate potentials in prediction-based rescheduling, with early-stage scrap detection.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,504"
    },
    {
        "issnkey": "15684946",
        "isbn": null,
        "journal": "Applied Soft Computing",
        "publisher": null,
        "title": "anintegratedmultinodehadoopframeworktopredicthighriskfactorsofdiabetesmellitususingamultilevelmapreducebasedfuzzyclassifiermmrfcandmodifieddbscanalgorithm",
        "booktitle": null,
        "doi": "10.1016/j.asoc.2021.107423",
        "author": [
            "Ramsingh, J.",
            "Bhuvaneswari, V."
        ],
        "keywords": [
            "Fuzzy classifier, MDBSCAN, MapReduce, Hadoop, Diabetes mellitus"
        ],
        "abstract": "In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people\u2019s food habits, physical activity are extracted from social media using the API\u2019s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55% of people are in a high-risk group with positive sentiments on the data extracted. More than 70% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,290"
    },
    {
        "issnkey": "00916749",
        "isbn": null,
        "journal": "Journal of Allergy and Clinical Immunology",
        "publisher": null,
        "title": "developingandevaluatingapediatricasthmaseveritycomputablephenotypederivedfromelectronichealthrecords",
        "booktitle": null,
        "doi": "10.1016/j.jaci.2020.11.045",
        "author": [
            "Peer, Komal",
            "Adams, William",
            "Legler, Aaron",
            "Sandel, Megan",
            "Levy, Jonathan",
            "Boynton-Jarrett, Ren\u00e9e",
            "Kim, Chanmin",
            "Leibler, Jessica",
            "Fabian, M."
        ],
        "keywords": [
            "Asthma, electronic health records, big data, respiratory function tests, selection bias, health care disparities, delivery of health care, observer variation, National Heart, Lung, and Blood Institute (US), pediatrics"
        ],
        "abstract": "Background Extensive data available in electronic health records (EHRs) have the potential to improve asthma care and understanding of factors influencing asthma outcomes. However, this work can be accomplished only when the EHR data allow for accurate measures of severity, which at present are complex and inconsistent. Objective Our aims were to create and evaluate a standardized pediatric asthma severity phenotype based in clinical asthma guidelines for use in EHR-based health initiatives and studies and also to examine the presence and absence of these data in relation to patient characteristics. Methods We developed an asthma severity computable phenotype and compared the concordance of different severity components contributing to the phenotype to trends in the literature. We used multivariable logistic regression to assess the presence of EHR data relevant to asthma severity. Results The asthma severity computable phenotype performs as expected in comparison with national statistics and the literature. Severity classification for a child is maximized when based on the long-term medication regimen component and minimized when based only on the symptom data component. Use of the severity phenotype results in better, clinically grounded classification. Children for whom severity could be ascertained from these EHR data were more likely to be seen for asthma in the outpatient setting and less likely to be older or Hispanic. Black children were less likely to have lung function testing data present. Conclusion We developed a pragmatic computable phenotype for pediatric asthma severity that is transportable to other EHRs.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822060-3",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter13asupportvectormachinebasedvoicedisordersdetectionusinghumanvoicesignal",
        "booktitle": "Artificial Intelligence and Big Data Analytics for Smart Healthcare",
        "doi": "10.1016/B978-0-12-822060-3.00014-0",
        "author": [
            "Leung, Pak",
            "Chui, Kwok",
            "Lo, Kenneth",
            "{de Pablos}, Patricia"
        ],
        "keywords": [
            "Artificial intelligence, big data, human voice, imbalanced classification, machine learning, medical screening, smart city, smart healthcare, support vector machine, voice disorders"
        ],
        "abstract": "Voice disorders are common diseases; most of the people have had experienced in their life. Voice disorder sufferers are usually not seeking medical consultation attributable to time-consuming and costly medical expenditure. Recently, researchers have proposed various machine learning algorithms for rapid detection of voice disorders based on the analysis of human voice. In this chapter, we have taken the pronunciation of vowel /a/ as the input of support vector machine algorithm. The research problem is formulated as binary classification which output will be either healthy or pathological status. Our work achieves an accuracy of 69.3% (sensitivity of 83.3% and specificity of 33.3%) which improves by 6.4%\u201319.3% compared with existing works. The implication of research work suggests tackling the imbalanced classification by adding penalty or generating new training data to class of smaller size. Everybody could contribute the voice signal of vowel /a/ and serving as big data pool.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22145796",
        "isbn": null,
        "journal": "Big Data Research",
        "publisher": null,
        "title": "datasciencemethodologiescurrentchallengesandfutureapproaches",
        "booktitle": null,
        "doi": "10.1016/j.bdr.2020.100183",
        "author": [
            "Martinez, I\u00f1igo",
            "Viles, Elisabeth",
            "{G. Olaizola}, Igor"
        ],
        "keywords": [
            "Data science, Big data, Data science methodology, Project life-cycle, Organizational impacts, Knowledge management"
        ],
        "abstract": "Data science has employed great research efforts in developing advanced analytics, improving data models and cultivating new algorithms. However, not many authors have come across the organizational and socio-technical challenges that arise when executing a data science project: lack of vision and clear objectives, a biased emphasis on technical issues, a low level of maturity for ad-hoc projects and the ambiguity of roles in data science are among these challenges. Few methodologies have been proposed on the literature that tackle these type of challenges, some of them date back to the mid-1990, and consequently they are not updated to the current paradigm and the latest developments in big data and machine learning technologies. In addition, fewer methodologies offer a complete guideline across team, project and data & information management. In this article we would like to explore the necessity of developing a more holistic approach for carrying out data science projects. We first review methodologies that have been presented on the literature to work on data science projects and classify them according to the their focus: project, team, data and information management. Finally, we propose a conceptual framework containing general characteristics that a methodology for managing data science projects with a holistic point of view should have. This framework can be used by other researchers as a roadmap for the design of new data science methodologies or the updating of existing ones.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.578",
        "scimago_value": "0,565"
    },
    {
        "issnkey": "17554365",
        "isbn": null,
        "journal": "Epidemics",
        "publisher": null,
        "title": "challengesinmodelingtheemergenceofnovelpathogens",
        "booktitle": null,
        "doi": "10.1016/j.epidem.2021.100516",
        "author": [
            "Glennon, Emma",
            "Bruijning, Marjolein",
            "Lessler, Justin",
            "Miller, Ian",
            "Rice, Benjamin",
            "Thompson, Robin",
            "Wells, Konstans",
            "Metcalf, C."
        ],
        "keywords": [
            "Immune landscape, Genotype to phenotype map, Big data, Data integration, Fundamental theory, Health system functioning"
        ],
        "abstract": "The emergence of infectious agents with pandemic potential present scientific challenges from detection to data interpretation to understanding determinants of risk and forecasts. Mathematical models could play an essential role in how we prepare for future emergent pathogens. Here, we describe core directions for expansion of the existing tools and knowledge base, including: using mathematical models to identify critical directions and paths for strengthening data collection to detect and respond to outbreaks of novel pathogens; expanding basic theory to identify infectious agents and contexts that present the greatest risks, over both the short and longer term; by strengthening estimation tools that make the most use of the likely range and uncertainties in existing data; and by ensuring modelling applications are carefully communicated and developed within diverse and equitable collaborations for increased public health benefit.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.396",
        "scimago_value": "2,023"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "evaluatingthecriticalsuccessfactorsofdataintelligenceimplementationinthepublicsectorusinganalyticalhierarchyprocess",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.121180",
        "author": [
            "Merhi, Mohammad"
        ],
        "keywords": [
            "Data intelligence, Systems implementation, Data analytics, Success factors, Public sector, AHP"
        ],
        "abstract": "This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "leveragingmachinelearningintheglobalfightagainstmoneylaunderingandterrorismfinancinganaffordancesperspective",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2020.10.012",
        "author": [
            "Canhoto, Ana"
        ],
        "keywords": [
            "Big data, Artificial intelligence, Machine learning, Algorithm, Customer profiling, Financial services, Anti-money laundering, United Nations, Sustainable development goals"
        ],
        "abstract": "Financial services organisations facilitate the movement of money worldwide, and keep records of their clients\u2019 identity and financial behaviour. As such, they have been enlisted by governments worldwide to assist with the detection and prevention of money laundering, which is a key tool in the fight to reduce crime and create sustainable economic development, corresponding to Goal 16 of the United Nations Sustainable Development Goals. In this paper, we investigate how the technical and contextual affordances of machine learning algorithms may enable these organisations to accomplish that task. We find that, due to the unavailability of high-quality, large training datasets regarding money laundering methods, there is limited scope for using supervised machine learning. Conversely, it is possible to use reinforced machine learning and, to an extent, unsupervised learning, although only to model unusual financial behaviour, not actual money laundering.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "01403664",
        "isbn": null,
        "journal": "Computer Communications",
        "publisher": null,
        "title": "thegroundwaterpotentialassessmentsystembasedoncloudcomputingacasestudyinislandsregion",
        "booktitle": null,
        "doi": "10.1016/j.comcom.2021.06.028",
        "author": [
            "Wang, Daqing",
            "Xu, Haoli",
            "Shi, Yue",
            "Ding, Zhibin",
            "Deng, Zhengdong",
            "Liu, Zhixin",
            "Xu, Xingang",
            "Lu, Zhao",
            "Wang, Guangyuan",
            "Cheng, Zijian",
            "Zhao, Xiaoning"
        ],
        "keywords": [
            "Big data, Cloud computing, Remote sensing, Groundwater potential, Bedrock islands"
        ],
        "abstract": "Today\u2019s intelligent system based on cloud computing platform can realize \u201cunattended\u201d, real-time monitoring observation and forecast by remote sensing. In order to import the development and efficiency of groundwater potential assessment(GPA) by remote sensing, the cloud computing platform was tried to use in the computing GPA. In this study, the Pearl River Estuary islands region(China) was selected as the study area. The slope, aspect, water-density(WD), land surface temperature(LST), NDVI and NDWI were used as the GPA indexes, which have been used before. Considering the similar geological and geomorphological conditions of the islands area, the analytic hierarchy process (AHP) method and these indexes can be used to assess GPA in the remote sensing cloud computing platform efficiently and conveniently. The results of the assessment were in good agreement with the actual hydrogeological map. Besides, the other intelligent algorithms can also be applied in this platform. Finally, this study realized the rapid \u201cunattended\u201d and \u201creal-time monitoring\u201d groundwater potential assessment, and carried out a multi-level GPA. It will be of certain reference significance to the exploitation of groundwater in the island area, which has realized convenient and efficient processing and analysis of data anytime and anywhere. At the same time, attention must be paid to the security of data and the maintenance of the system.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.167",
        "scimago_value": "0,627"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "thescienceofstatisticsversusdatasciencewhatisthefuture",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.121111",
        "author": [
            "Hassani, Hossein",
            "Beneki, Christina",
            "Silva, Emmanuel",
            "Vandeput, Nicolas",
            "Madsen, Dag"
        ],
        "keywords": [
            "Perspective, Science, Statistics, Data science, Similarities, Differences, Pragmatism"
        ],
        "abstract": "The importance and relevance of the discipline of statistics with the merits of the evolving field of data science continues to be debated in academia and industry. Following a narrative literature review with over 100 scholarly and practitioner-oriented publications from statistics and data science, this article generates a pragmatic perspective on the relationships and differences between statistics and data science. Some data scientists argue that statistics is not necessary for data science as statistics delivers simple explanations and data science delivers results. Therefore, this article aims to stimulate debate and discourse among both academics and practitioners in these fields. The findings reveal the need for stakeholders to accept the inherent advantages and disadvantages within the science of statistics and data science. The science of statistics enables data science (aiding its reliability and validity), and data science expands the application of statistics to Big Data. Data scientists should accept the contribution and importance of statistics and statisticians must humbly acknowledge the novel capabilities made possible through data science and support this field of study with their theoretical and pragmatic expertise. Indeed, the emergence of data science does pose a threat to statisticians, but the opportunities for synergies are far greater.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "26659174",
        "isbn": null,
        "journal": "Measurement: Sensors",
        "publisher": null,
        "title": "metrologyforthedigitalage",
        "booktitle": null,
        "doi": "10.1016/j.measen.2021.100232",
        "author": [
            "Eichst\u00e4dt, Sascha",
            "Keidel, Anke",
            "Tesch, Julia"
        ],
        "keywords": [
            "Digital transformation, Digital certificates, Systemic metrology, Big data, Industry 4.0, Open science, FAIR"
        ],
        "abstract": "Based on digital technologies, big data, artificial intelligence and machine-readable information, the digital transformation rapidly changes society, industries, and economies. Metrology as a central element of international trade, for confidence in measurements and part of the quality infrastructure is facing several challenges and opportunities in these developments. In this contribution we discuss some of the key challenges and a potential future role of metrology in the digital age. We address metrological principles for confidence in data and Algorithms, cyber-physical systems, FAIR data and metrology, and the role of metrology in the digital transformation in the quality infrastructure.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23519789",
        "isbn": null,
        "journal": "Procedia Manufacturing",
        "publisher": null,
        "title": "towardstheimplementationofthedigitaltwinincmminspectionprocessopportunitieschallengesandproposals",
        "booktitle": null,
        "doi": "10.1016/j.promfg.2021.07.033",
        "author": [
            "Gaha, Raoudha",
            "Durupt, Alexandre",
            "Eynard, Benoit"
        ],
        "keywords": [
            "Digital Twin, CMM, inspection, Model-based-defintion, Digital thread"
        ],
        "abstract": "The use of Digital Twin (DT) is adopted by manufacturers and have positive effects on the product manufacturing process. The aim of this paper is to define a Coordinate Measuring Machine (CMM) inspection DT model, based on inspection process digitalized functionalities, from one side, and Industry 4.0 opportunities (Digital thread, Big data, etc.), from the other side. A review about DT definition, is firstly presented. Secondly, we review related studies based on existing DT orientations and usages for CMM inspection. Thirdly, challenges related to the variation management are presented. Finally, a discussion about possible DT functionalities and opportunities is conducted then a CMM inspection DT model is presented.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,504"
    },
    {
        "issnkey": "15700232",
        "isbn": null,
        "journal": "Journal of Chromatography B",
        "publisher": null,
        "title": "anintegratedapproachtouncoverqualitymarkersofstirbakingsemencuscutawithsaltsolutionpreventingrecurrentspontaneousabortionbasedonchemicalandmetabolomicprofiling",
        "booktitle": null,
        "doi": "10.1016/j.jchromb.2021.122727",
        "author": [
            "Wang, Xiaoli",
            "Gao, Haiyan",
            "Tan, Song",
            "Xu, Chao",
            "Xu, Fengqing",
            "Wang, Tongsheng",
            "Chu, Jijun",
            "Han, Yanquan",
            "Wu, Deling",
            "Jin, Chuanshan"
        ],
        "keywords": [
            ", Stir-baking with salt solution, Metabolomics, UHPLC-Q-TOF-MS"
        ],
        "abstract": "The previous research of clinical big data mining showed that stir-baking Semen Cuscuta with salt solution (YP) ranked the first in the usage rate of treating abortion caused by kidney deficiency. At the same time, pharmacodynamic studies also showed that YP has better effect on improving recurrent spontaneous abortion (RSA) compared to raw products of Semen Cuscuta (SP). However, there were few studies on the biomarkers of YP improving RSA. In this study, the chemical and metabonomic profiling were used to screen the quality markers of YP on improving RSA. Firstly, a metabolomics study was carried out to select representative biomarkers of RSA. The ultra-high performance liquid chromatography coupled with electrospray ionization-quadrupole-time of flight-mass spectrometry (UPLC-ESI-Q-TOF-MS) technique was used to investigate the components of exogenous and endogenous in serum of rats after administrated with YP and SP. As a result, 14 differential compounds were identified between the serum of rats administrated SP and YP. Compared to SP, there was an upward trend in YP of the compounds including kaempferol-3-glucuronide, iso-kaempferol-3-glucuronide, (1S) \u221211-hydroxyhexadecanoic acid and 3-phenylpropionic acid. Meanwhile, there was a reducing trend in YP of the compounds including kaempferol 3-arabinofuranoside, apigenin-3-O-glucoside, hyperoside, caffeic acid-\u03b2-D glucoside, dicaffeoylquinic acid, linoleic acid, 3,4-dicaffeoylquinic acid, caffeic acid, palmitic acid and methyl myristate. 12 biomarkers for RSA indication were identified. SP and YP have a certain effect on the endogenous biomarker. The regulation effect of YP was higher than that of SP. The main metabolic pathways included phenylalanine, tyrosine and tryptophan biosynthesis, glycerophospholipid metabolism, fatty acid biosynthesis, sphingolipid metabolism, biosynthesis of unsaturated fatty acids. This study demonstrated a promising way to elucidate the active chemical and endogenous material basis of TCM.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09255273",
        "isbn": null,
        "journal": "International Journal of Production Economics",
        "publisher": null,
        "title": "theeffectsofbusinessanalyticscapabilityoncirculareconomyimplementationresourceorchestrationcapabilityandfirmperformance",
        "booktitle": null,
        "doi": "10.1016/j.ijpe.2021.108205",
        "author": [
            "Kristoffersen, Eivind",
            "Mikalef, Patrick",
            "Blomsma, Fenna",
            "Li, Jingyue"
        ],
        "keywords": [
            "Digital circular economy, Big data analytics, Circular economy capability, Sustainability, Policy implication, Digitalization"
        ],
        "abstract": "Today, most organizations are undergoing a digital transformation. At the same time, the gravity of environmental issues has put sustainability and the circular economy at the top of corporate agendas. To this end, information systems, in particular business analytics, are being highlighted as essential enablers of an accelerated circular economy transition. However, effectively managing this joint transformation is a challenge. Firms struggle to identify which organizational resources they should target and how those should be leveraged towards a firm-wide business analytics capability for circular economy. To address these questions, this study draws on recent literature dealing with smart circular economy and business analytics capabilities along with the resource-based and resource orchestration view to (1) create an instrument to measure firms\u2019 business analytics capability for circular economy, and (2) examine the relationship among a circular economy-specific business analytics capability, circular economy implementation, resource orchestration capability, and firm performance. The proposed research model was tested using partial least squares structural equation modeling of survey data from 125 top-level managers at companies across Europe. The results show that firms with a strong business analytics capability have an increased resource orchestration capability and a greater ability to excel in the circular economy, resulting in improved organizational performance in building a more sustainable competitive advantage in an increasingly competitive business landscape. The effect of business analytics capability on firm performance is not direct but fully mediated through resource orchestration capability and circular economy implementation. The results empirically validate the proposed research model and offer pathways to future information systems research streams to support the operationalization of circular strategies. The study provides the first empirical evidence of a business analytics capability for circular economy and its effect on firm performance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.885",
        "scimago_value": "2,406"
    },
    {
        "issnkey": "22124209",
        "isbn": null,
        "journal": "International Journal of Disaster Risk Reduction",
        "publisher": null,
        "title": "socialmediabaseddisasterresearchdevelopmenttrendsandobstacles",
        "booktitle": null,
        "doi": "10.1016/j.ijdrr.2021.102095",
        "author": [
            "Tang, Jiting",
            "Yang, Saini",
            "Wang, Weiping"
        ],
        "keywords": [
            "Social media, Disaster research, Obstacles, Data applicability"
        ],
        "abstract": "Social media, as a new data source, is a promising field in disaster research. Despite doubts about its validity, a growing number of research institutes and commercial companies are exploring the potential of social media in disaster risk management. To understand the development and trends in this domain, a bibliometric analysis was performed using 1573 related published articles in Web of Science between 1991 and 2019. We found that (1) the number of annual publications and new research institutes in this field grew rapidly but seems to have become saturated in recent years. (2) The main research force is independent universities with limited cooperation, and a knowledge network has not yet been formed in this arena. (3) Research hotspots evolve in the path of \u201cconceptualization - refinement - application\u201d. Due to the three features of social media data, namely, timeliness, subjectivity, and disequilibrium, obstacles still exist in applicable disaster types and population representativeness. We anticipate new scientific advances emerging to overcome some technical difficulties. Knowledge sharing, advanced computer science, and multiorganizational cooperation will benefit this arena. These findings indicate potential directions for the development of and innovation in social media-based disaster research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.320",
        "scimago_value": "1,161"
    },
    {
        "issnkey": "20967209",
        "isbn": null,
        "journal": "Blockchain: Research and Applications",
        "publisher": null,
        "title": "bdriveablockchainbaseddistributediotnetworkforsmarturbantransportation",
        "booktitle": null,
        "doi": "10.1016/j.bcra.2021.100033",
        "author": [
            "Zia, Mohammed"
        ],
        "keywords": [
            "Blockchain, Open data, Urban traffic, Internet of Things, Vehicle navigation"
        ],
        "abstract": "In this paper, I present B-DRIVE\u2014a blockchain-based distributed IoT (Internet of Things) network for smart urban transportation. The network is designed to connect a large fleet of IoT devices, installed on various vehicles and roadside infrastructures, to distributed data storage centers, called as Full-Nodes, to log and disseminate sensor generated data. It connects devices from around the city to multiple Full-Nodes to log timestamped data into the blockchain. These sensors vary from GPS (Global Positioning System), air quality meter, gyrometer to speed cameras in order to facilitate efficient urban mobility. The three identified hardware layers that comprise the network are the IoT layer, Storage layer, and User layer. They consist of Moving/Static-Nodes, Full-Nodes, and Smart devices, respectively. The Moving/Static-Nodes are primarily made up of moving vehicles and road-side infrastructures, respectively, thus acting as various data sources. Whereas, Full-Nodes and Smart devices are institutions and mobile phones, acting as data handler/disseminator and navigator/data visualizer, respectively. The data, or data blocks, received by Full-Nodes get appended into Full and Running-Blockchain, meant for specific purposes. The network is designed to be free from any block mining activity. It provides open access to anonymous sensor data to end-users, especially scientists, policy-makers and entrepreneurs, to develop innovative urban transportation solutions. It is believed that a system like B-DRIVE, along with existing VANETs (Vehicular Ad-hoc NETworks), is capable of answering some of the current urban transportation issues around traffic congestion, navigation, and vehicle parking. Other applications of blockchain data could vary from user activity mapping to VGI (volunteered geographic information) data quality assessment. Two identified limitations of the presented architecture are the low processing power of current IoT devices and the lack of urban IoT infrastructure.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09265805",
        "isbn": null,
        "journal": "Automation in Construction",
        "publisher": null,
        "title": "posturerelateddatacollectionmethodsforconstructionworkersareview",
        "booktitle": null,
        "doi": "10.1016/j.autcon.2020.103538",
        "author": [
            "Yu, Yantao",
            "Umer, Waleed",
            "Yang, Xincong",
            "Antwi-Afari, Maxwell"
        ],
        "keywords": [
            "Behavior-based safety (BBS), Computer vision, Construction worker, Deep learning, Motion sensor, Occupational safety and health (OSH), Pose estimation"
        ],
        "abstract": "Construction workers' posture-related data is closely connected with their safety, health, and productivity performance. The importance of posture-related data has drawn the attention of researchers in construction management and other fields. Accordingly, many data collection methods have been developed and applied to collect posture-related data. Despite the importance of workers' posture-related data, there lacks a review of previous data collection methods in the construction industry. This paper fills the research gap by reviewing previous methods to collect posture-related data for construction workers via 1) summarizing working principles and applications of posture-related data collection in construction management, which demonstrates the extensive use of motion sensors and Red-Green-Blue (RGB) cameras in posture-related data collection, 2) comparing the above methods based on data quality and feasibility on construction sites, which reveals the reason why motion sensors and RGB cameras have been prevalent in previous studies, 3) revealing research gaps of posture-related data collection tools and applications, and providing possible future research directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.700",
        "scimago_value": "1,837"
    },
    {
        "issnkey": "13665545",
        "isbn": null,
        "journal": "Transportation Research Part E: Logistics and Transportation Review",
        "publisher": null,
        "title": "tointroduceastorebrandornotrolesofmarketinformationinsupplychains",
        "booktitle": null,
        "doi": "10.1016/j.tre.2021.102334",
        "author": [
            "Shi, Chun-lai",
            "Geng, Wei"
        ],
        "keywords": [
            "Store brand, National brand, Market information, Information accuracy, Information sharing"
        ],
        "abstract": "Retailers in multiple market segments have opposing business practices regarding whether to introduce a store brand. Assuming that manufacturers and retailers have symmetric knowledge on market information, prior literature has shown that retailers have incentive to introduce a store brand in cases in which the store brand intensively competes with the national brand. Information asymmetry, however, is prevalent in supply chains. In the era of big data, the latest advances in technology intensify information asymmetry by allowing retailers to access market information with improved accuracy. To address such information asymmetry, some retailers share information with manufacturers, while others do not. The gap between prior studies and current industrial practices motivates us to explore the roles of market information in supply chains regarding store brand introduction. In this research, we consider a two-echelon supply chain in which the retailer has an advantage over the manufacturer in accessing market information. In the focal supply chain, four decision scenarios are present, each of which deviates from the others on the retailer\u2019s decision on information sharing and store brand introduction. By evaluating and comparing supply chain performances in these decision scenarios, we demonstrate the influence of information accuracy on the incentive for the retailer to introduce a store brand and the mitigation effect of information sharing on such influence. In particular, we contribute to the literature by finding that, surprisingly, high information accuracy prevents store brand introduction even if brand competition is intensive. This research bridges the gap between prior research and industrial practices and is the first to consider information asymmetry and information sharing in the context of store brand introduction. Our findings contribute to theories of store brand introduction and provide information sharing for practitioners as a managerial tool to realize the full potential of store brands.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.875",
        "scimago_value": "2,042"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818015-0",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter3keyconcepts",
        "booktitle": "Executing Data Quality Projects (Second Edition)",
        "doi": "10.1016/B978-0-12-818015-0.00009-8",
        "author": [
            "McGilvray, Danette"
        ],
        "keywords": [
            "Framework for Information Quality (FIQ), information life cycle, data life cycle, lineage, data quality dimensions, business impact techniques, data categories, master data, data specifications, metadata, data standards, reference data, data models, business rules, data governance, data stewardship, Ten Steps Process, data quality improvement cycle"
        ],
        "abstract": "This chapter introduces fundamental ideas, the understanding of which, will aid data quality work and use of the Ten Steps Process. Information, like financial and human resources, must be properly managed throughout its life cycle to get the full use and benefit from it, so the information life cycle is discussed with the acronym POSMAD as an easy way to remember the six phases of the information life cycle: Plan, Obtain, Store and Share, Maintain, Apply, and Dispose. POSMAD plus several additional concepts are summarized in the Framework for Information Quality (FIQ), which provides an at-a-glance view of the components necessary to have high quality information. Other concepts central to understanding and managing data are discussed, such as data quality dimensions, business impact techniques, data categories, and data specifications (with a focus on metadata, data standards, reference data, data models, and business rules), data governance and stewardship. An overview of each of the steps in the Ten Steps Process is given, along with their relationship to the concepts.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15698432",
        "isbn": null,
        "journal": "International Journal of Applied Earth Observation and Geoinformation",
        "publisher": null,
        "title": "largescalecropmappingfrommultisourceopticalsatelliteimageriesusingmachinelearningwithdiscretegrids",
        "booktitle": null,
        "doi": "10.1016/j.jag.2021.102485",
        "author": [
            "Yan, Shuai",
            "Yao, Xiaochuang",
            "Zhu, Dehai",
            "Liu, Diyou",
            "Zhang, Lin",
            "Yu, Guojiang",
            "Gao, Bingbo",
            "Yang, Jianyu",
            "Yun, Wenju"
        ],
        "keywords": [
            "Crop Mapping, Large-Scale, Remote Sensing, Machine Learning, Discrete Grids"
        ],
        "abstract": "The spatial distribution of crops is an important agricultural parameter, which is used to derive important information about crop productivity and food security. However, crop mapping on a large scale is challenging due to the low spatio-temporal information of satellite data, sparse sampling, and poor computational efficiency for massive data. To alleviate these problems, this study proposes a method based on discrete grids with machine learning to integrate GaoFen-1 and Sentinel-2 imagery. First, the proposed method fuses multi-source satellite data with similar observation characteristics to improve the spatial and temporal coverage of satellites. Second, a data augmentation technique based on a discrete grid framework was proposed to solve the problem of sparse samples. Finally, a machine learning algorithm in a discrete grid was introduced to improve processing efficiency and ensure the crop classification precision of large-scale remote sensing images. An experiment in the Sanjiang Plain area (approximately 108900 km2) of Northeast China showed that the proposed scheme benefited from a high spatio-temporal multi-source dataset and achieved good performance. Compared with a single data source, the accuracy of crop mapping using multi-source optical remote sensing data is higher, attaining up to 86 and 88 % in 2017 and 2018, respectively. Furthermore, the advantages of machine learning in discrete grids over large-scale areas are validated by evaluating the accuracy of different classifiers, which indicates the suitability of discrete grids in data augmentation and large-scale crop mapping. Finally, discrete grid technology offers a possibility for crop mapping over large-scale areas, and improves the processing efficiency of remote sensing big data. The findings in this study can contribute to studies on large-scale crop classification and serve as a reference to them.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.933",
        "scimago_value": "1,623"
    },
    {
        "issnkey": "23529148",
        "isbn": null,
        "journal": "Informatics in Medicine Unlocked",
        "publisher": null,
        "title": "deeplearningapplicationsforiotinhealthcareasystematicreview",
        "booktitle": null,
        "doi": "10.1016/j.imu.2021.100550",
        "author": [
            "Bolhasani, Hamidreza",
            "Mohseni, Maryam",
            "Rahmani, Amir"
        ],
        "keywords": [
            "Deep learning, Internet of things, Healthcare, Medical imaging, Wearable device, Systematic literature review"
        ],
        "abstract": "In machine learning, deep learning is the most popular topic having a wide range of applications such as computer vision, natural language processing, speech recognition, visual object detection, disease prediction, drug discovery, bioinformatics, biomedicine, etc. Of these applications, health care and medical science-related applications are dramatically on the rise. The tremendous big data growth, the Internet of Things (IoT), connected devices, and high-performance computers utilizing GPUs and TPUs are the main reasons why deep learning is so popular. Based on their specific tasks, medical IoT, digital images, electronic health record (EHR) data, genomic data, and central medical databases are the primary data sources for deep learning systems. Several potential issues such as privacy, QoS optimization, and deployment indicate the pivotal part of deep learning. In this paper, deep learning for IoT applications in health care systems is reviewed based on the Systematic Literature Review (SLR). This paper investigates the related researches, selected from among 44 published research papers, conducted within a period of ten years \u2013 2010 to 2020. Firstly, theoretical concepts and ideas of deep learning and technical taxonomy are proposed. Afterwards, major deep learning applications for IoT in health care and medical sciences are presented through analyzing the related works. Later, the main idea, advantages, disadvantages, and limitations of each study are discussed, preceding suggestions for further research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,440"
    },
    {
        "issnkey": "01697161",
        "isbn": null,
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter4applicationofdatahandlingtechniquestopredictpavementperformance",
        "booktitle": "Data Science: Theory and Applications",
        "doi": "10.1016/bs.host.2020.07.001",
        "author": [
            "Saride, Sireesh",
            "Peddinti, Pranav",
            "Basha, B."
        ],
        "keywords": [
            "Data handling, Automation, User interface (UI), Regression, Fatigue, Rutting, Pavement"
        ],
        "abstract": "The present study discusses the design of pavements and the importance of big data handling in improving their performance. A comprehensive framework based on a simple natural language processing technique is presented to reduce the computational time and error in data handling for pavement applications. The application of the proposed method to automate a graphical user interface (UI) adopted in pavement design is demonstrated. The proposed method was found to reduce the run-time by about 83% as compared to the conventional procedures. The proposed framework is highly flexible and can be adapted to extract data from various file formats and automate UIs at ease. To present the potential of this framework, about 0.2 million data sets representing pavement geometry and material properties were generated using language processing algorithms. Further, robust non-linear regression equations for calculating pavement damage in terms of fatigue and rutting strains were developed by using automated data processing through the pavement design interface.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": "0,125"
    },
    {
        "issnkey": "0160791x",
        "isbn": null,
        "journal": "Technology in Society",
        "publisher": null,
        "title": "areviewofsocialmediabasedpublicopinionanalyseschallengesandrecommendations",
        "booktitle": null,
        "doi": "10.1016/j.techsoc.2021.101724",
        "author": [
            "Dong, Xuefan",
            "Lian, Ying"
        ],
        "keywords": [
            "Public opinion, Social media, PRISMA, Challenges, Recommendations"
        ],
        "abstract": "Compared with survey polls, social media can yield a better and more comprehensive understanding of public perceptions of special topics in a more scientific manner. However, despite this advantage, there seem to be limited investigations into the challenges in social media-based public opinion analysis. This study offers an understanding of the challenges in this field and some corresponding recommendations. Through a systematic literature review, we identify 54 papers to analyze and discuss issues related to data collection, data quality, and data mining. This paper summarizes a framework for social media-based public opinion analysis as well as the commonly employed data mining methodologies. We found that collecting public opinion data from Facebook and Weibo is difficult because of their restricted application programming interface and measures against Web Crawler. How to effectively and conveniently delete invalid data and how to design data mining methods for social media data, especially for those in Chinese, are still two main challenges in social media-based public opinion analysis. We claim that using multiple data sources, optimizing keyword settings, enhancing interdisciplinary cooperation, and paying more attention to the functional role of social media can benefit the development of social media-based public opinion analysis. This study also highlights the potential risks of releasing the personal information of the public in the use of social media data in research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23524847",
        "isbn": null,
        "journal": "Energy Reports",
        "publisher": null,
        "title": "evaluatingneuralnetworkandlinearregressionphotovoltaicpowerforecastingmodelsbasedondifferentinputmethods",
        "booktitle": null,
        "doi": "10.1016/j.egyr.2021.10.125",
        "author": [
            "AlShafeey, Mutaz",
            "Cs\u00e1ki, Csaba"
        ],
        "keywords": [
            "Solar energy, Photovoltaic technology, Prediction model, Multiple regression, Artificial neural network, Prediction accuracy"
        ],
        "abstract": "As Photovoltaic (PV) energy is impacted by various weather variables such as solar radiation and temperature, one of the key challenges facing solar energy forecasting is choosing the right inputs to achieve the most accurate prediction. Weather datasets, past power data sets, or both sets can be utilized to build different forecasting models. However, operators of grid-connected PV farms do not always have full sets of data available to them especially over an extended period of time as required by key techniques such as multiple regression (MR) or artificial neural network (ANN). Therefore, the research reported here considered these two main approaches of building prediction models and compared their performance when utilizing structural, time-series, and hybrid methods for data input. Three years of PV power generation data (of an actual farm) as well as historical weather data (of the same location) with several key variables were collected and utilized to build and test six prediction models. Models were built and designed to forecast the PV power for a 24-hour ahead horizon with 15 min resolutions. Results of comparative performance analysis show that different models have different prediction accuracy depending on the input method used to build the model: ANN models perform better than the MR regardless of the input method used. The hybrid input method results in better prediction accuracy for both MR and ANN techniques, while using the time-series method results in the least accurate forecasting models. Furthermore, sensitivity analysis shows that poor data quality does impact forecasting accuracy negatively especially for the structural approach.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.870",
        "scimago_value": "1,199"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-823395-5",
        "journal": null,
        "publisher": "Academic Press",
        "title": "16dataduplicationusingamazonwebservicescloudstorage",
        "booktitle": "Data Deduplication Approaches",
        "doi": "10.1016/B978-0-12-823395-5.00006-9",
        "author": [
            "Rao, M."
        ],
        "keywords": [
            "Data duplication, cloud computing, Amazon Web Services, data storage, fault tolerance"
        ],
        "abstract": "Since Big Data has become a worldwide phenomenon, both decision-makers and experts in data and analysis have been concerned about data scalability. They have been too reliant on the theory of better data in the digital age. Data duplication is an occurrence where a business has more than one copy of the same source of data. To a layman, data replication appears like a basic issue which could be avoided by any professional data scientist or experienced network administrator. Needless to say, the replication of data is also a very common issue. Cloud computing is a big part of the business sector today, where computing services are supplied to consumers via the Internet on demand. Data storage is one of the most common services provided by cloud computing. The key benefit of using cloud storage from the perspective of consumers is that they are only liable for the required quantity of data, which can be scaled and decreased on demand, to minimize their costs in buying and maintaining their data infrastructure. A decrease in data sizes will help providers minimize costs by using large storage facilities and minimize energy usage by rising data size in cloud computing. It has contributed to the implementation of data replication technology to increase cloud storage capacity. Because of the volatile existence of data in cloud storage and cloud overtime shifts, some data chunks can also be read in time but cannot be accessed in another time. Many databases can be accessed or modified simultaneously by several users and others need a high consistency level for accuracy. The author has suggested a dynamic cloud storage virtualization scheme to maximize storage capacity and preserve fault tolerance redundancy.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03050483",
        "isbn": null,
        "journal": "Omega",
        "publisher": null,
        "title": "learningfromthepasttoshapethefutureacomprehensivetextmininganalysisoformsreviews",
        "booktitle": null,
        "doi": "10.1016/j.omega.2020.102388",
        "author": [
            "Romero-Silva, Rodrigo",
            "{de Leeuw}, Sander"
        ],
        "keywords": [
            "Operations research, Management science, Text mining, Bibliometric analysis, Emerging trends, Literature reviews"
        ],
        "abstract": "This paper provides an overview of the evolution and state-of-the-art of the Operations Research and Management Science (OR/MS) subject area from 1956 to 2019. Using text mining techniques on the content of the title, abstract, and author keywords of papers classified by the Web of Science as literature review studies in OR/MS, we found that there are 76 topical consolidated clusters in the field covering a wide range of reviewed topics. Since 2015, reviews on supply chain risk management and big data analytics have had the highest impact in the field, whereas topics such as Industry 4.0, socio-technical systems, social networks, green supply, sustainable supply chain, and resilience engineering have all received significant attention from researchers. Reviews on analytic hierarchy process were found to be the most impactful overall, showing the high relevance of multi-criteria decision making in the current research and practice contexts. Furthermore, a text mining analysis of the papers citing OR/MS literature reviews showed that optimization continues to be one of the most highly influential methodological contributions of OR/MS to other research areas and that topics such as circular economy, carbon emissions, and social commerce have yet to find some traction in OR/MS research, suggesting future research and multidisciplinary opportunities for the field. Results also show that the research area of Public Administration has been greatly influenced by OR/MS reviews as 16% of all the papers published in that field have cited at least one of the 1744 review papers included in this study. Finally, a summary table of published structured literature reviews per topic (benchmarks, classifications, taxonomies) is presented as a short bibliography of OR/MS review papers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,500"
    },
    {
        "issnkey": "15698432",
        "isbn": null,
        "journal": "International Journal of Applied Earth Observation and Geoinformation",
        "publisher": null,
        "title": "afeasibleframeworktodownscalenppviirsnighttimelightimageryusingmultisourcespatialvariablesandgeographicallyweightedregression",
        "booktitle": null,
        "doi": "10.1016/j.jag.2021.102513",
        "author": [
            "Ye, Yang",
            "Huang, Linyan",
            "Zheng, Qiming",
            "Liang, Chenxin",
            "Dong, Baiyu",
            "Deng, Jinsong",
            "Han, Xiuzhen"
        ],
        "keywords": [
            "Nighttime light (NTL), Downscaling, Geographically weighted regression (GWR), Impervious surface detection"
        ],
        "abstract": "The cloud-free monthly composite of global nighttime light (NTL) data of the Suomi National Polar-orbiting Partnership with the Visible Infrared Imaging Radiometer Suite (NPP-VIIRS) day/night band (DNB) provides indispensable indications of human activities and settlements. However, the coarse spatial resolution (15 arc sec) of NTL imagery greatly restricts its application potential. This study proposes a feasible framework to downscale NPP-VIIRS NTL using muti-source spatial variables and geographically weighted regression (GWR) method. High-resolution auxiliary variables were acquired from the Landsat 8 OLI/ TIRS and social media platforms. GWR-based downscaling procedures were consequently implemented to obtain NTL at a 100-m resolution. The downscaled NTL data were validated against Loujia1-01 imagery based on the coefficient of determination (R2) and root-mean-square error (RMSE). The results suggest that the data quality was suitably improved after downscaling, yielding higher R2 (0.604 vs. 0.568) and lower RMSE (8.828 vs. 9.870 nW/cm2/sr) values than those of the original NTL data. Finally, the NTL was extendedly applied to detect impervious surfaces, and the downscaled NTL had higher accuracy than the original NTL. Therefore, this study facilitates data quality improvement of NPP-VIIRS NTL imagery by downscaling, thus enabling more accurate applications.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.933",
        "scimago_value": "1,623"
    },
    {
        "issnkey": "15517144",
        "isbn": null,
        "journal": "Contemporary Clinical Trials",
        "publisher": null,
        "title": "riskbasedcentralizeddatamonitoringofclinicaltrialsatthetimeofcovid19pandemic",
        "booktitle": null,
        "doi": "10.1016/j.cct.2021.106368",
        "author": [
            "Afroz, Most",
            "Schwarber, Grant",
            "Bhuiyan, Mohammad"
        ],
        "keywords": [
            "Centralize data monitoring, Risk based data monitoring"
        ],
        "abstract": "Objectives COVID-19 pandemic caused several alarming challenges for clinical trials. On-site source data verification (SDV) in the multicenter clinical trial became difficult due to travel ban and social distancing. For multicenter clinical trials, centralized data monitoring is an efficient and cost-effective method of data monitoring. Centralized data monitoring reduces the risk of COVID-19 infections and provides additional capabilities compared to on-site monitoring. The key steps for on-site monitoring include identifying key risk factors and thresholds for the risk factors, developing a monitoring plan, following up the risk factors, and providing a management plan to mitigate the risk. Methods For analysis purposes, we simulated data similar to our clinical trial data. We classified the data monitoring process into two groups, such as the Supervised analysis process, to follow each patient remotely by creating a dashboard and an Unsupervised analysis process to identify data discrepancy, data error, or data fraud. We conducted several risk-based statistical analysis techniques to avoid on-site source data verification to reduce time and cost, followed up with each patient remotely to maintain social distancing, and created a centralized data monitoring dashboard to ensure patient safety and maintain the data quality. Conclusion Data monitoring in clinical trials is a mandatory process. A risk-based centralized data review process is cost-effective and helpful to ignore on-site data monitoring at the time of the pandemic. We summarized how different statistical methods could be implemented and explained in SAS to identify various data error or fabrication issues in multicenter clinical trials.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "08954356",
        "isbn": null,
        "journal": "Journal of Clinical Epidemiology",
        "publisher": null,
        "title": "pcornet2020currentstateaccomplishmentsandfuturedirections",
        "booktitle": null,
        "doi": "10.1016/j.jclinepi.2020.09.036",
        "author": [
            "Forrest, Christopher",
            "McTigue, Kathleen",
            "Hernandez, Adrian",
            "Cohen, Lauren",
            "Cruz, Henry",
            "Haynes, Kevin",
            "Kaushal, Rainu",
            "Kho, Abel",
            "Marsolo, Keith",
            "Nair, Vinit",
            "Platt, Richard",
            "Puro, Jon",
            "Rothman, Russell",
            "Shenkman, Elizabeth",
            "Waitman, Lemuel",
            "Williams, Neely",
            "Carton, Thomas"
        ],
        "keywords": [
            "Distributed data network, Clinical research network, Health plan research network, PCORnet, Pragmatic clinical trials, Electronic health records, Big data, Learning health system"
        ],
        "abstract": "Objective To describe PCORnet, a clinical research network developed for patient-centered outcomes research on a national scale. Study Design and Setting Descriptive study of the current state and future directions for PCORnet. We conducted cross-sectional analyses of the health systems and patient populations of the 9 Clinical Research Networks and 2 Health Plan Research Networks that are part of PCORnet. Results Within the Clinical Research Networks, electronic health data are currently collected from 337 hospitals, 169,695 physicians, 3,564 primary care practices, 338 emergency departments, and 1,024 community clinics. Patients can be recruited for prospective studies from any of these clinical sites. The Clinical Research Networks have accumulated data from 80 million patients with at least one visit from 2009 to 2018. The PCORnet Health Plan Research Network population of individuals with a valid enrollment segment from 2009 to 2019 exceeds 60 million individuals, who on average have 2.63 years of follow-up. Conclusion PCORnet\u2019s infrastructure comprises clinical data from a diverse cohort of patients and has the capacity to rapidly access these patient populations for pragmatic clinical trials, epidemiological research, and patient-centered research on rare diseases.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.437",
        "scimago_value": "2,993"
    },
    {
        "issnkey": "02638762",
        "isbn": null,
        "journal": "Chemical Engineering Research and Design",
        "publisher": null,
        "title": "machinelearningonsustainableenergyareviewandoutlookonrenewableenergysystemscatalysissmartgridandenergystorage",
        "booktitle": null,
        "doi": "10.1016/j.cherd.2021.08.013",
        "author": [
            "Rangel-Martinez, Daniel",
            "Nigam, K.D.P.",
            "Ricardez-Sandoval, Luis"
        ],
        "keywords": [
            "Machine learning, Artificial neural networks, Renewable energies, Catalysis, Power systems, Sustainability, Energy efficiency"
        ],
        "abstract": "This study presents a broad view of the current state of the art of ML applications in the manufacturing sectors that have a considerable impact on sustainability and the environment, namely renewable energies (solar, wind, hydropower, and biomass), smart grids, the industry of catalysis and power storage and distribution. Artificial neural networks are the most preferred techniques over other ML algorithms because of their generalization capabilities. Demands for ML techniques in the energy sectors will increase considerably in the coming years, since there is a growing demand of academic programmes related to artificial intelligence in science, math, and engineering. Data generation, management, and safety are expected to play a key role for the successful implementation of ML algorithms that can be shared by major stakeholders in the energy sector, thereby promoting the development of ambitious energy management projects. New algorithms for producing reliable data and the addition of other sources of information (e.g., novel sensors) will enhance flow of information between ML and systems. It is expected that unsupervised and reinforcement learning will take a central role in the energy sector, but this will depend on the expansion of other major fields in data science such as big data analytics. Massive implementations, specialized algorithms, and new technologies like 5G will promote the development of sustainable applications of ML in non-industrial applications for energy management.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15684946",
        "isbn": null,
        "journal": "Applied Soft Computing",
        "publisher": null,
        "title": "modulo9modelbasedlearningformissingdataimputation",
        "booktitle": null,
        "doi": "10.1016/j.asoc.2021.107167",
        "author": [
            "Ngueilbaye, Alladoumbaye",
            "Wang, Hongzhi",
            "Mahamat, Daouda",
            "Junaidu, Sahalu"
        ],
        "keywords": [
            "Data quality, Machine learning algorithms, Missing data, Modulo 9"
        ],
        "abstract": "Missing Values Management is one of the challenges faced by Data Analysts. Therefore, the creation of effective data models will be the right decision for missing data imputation. However, learning, training, and Data Analysis must be implemented through machine learning algorithms. Missing Data is a problem with no feedback or variables. This problem (missing data) can result in serious Data Analysis, which may eventually lead to erroneous conclusions. This research paper first studies how missing data can affect Machine Learning Algorithms, and decision-making based on the Data Analysis\u2019s output. Secondly, it proposes Modulo 9 as a novel method for handling missing data problems. The proposed novel method is assessed with wide-ranging experiments compared with robust Machine Learning techniques such as Support Vector Machine (SVM) Algorithm, Linear Regression (LR), K-Nearest Neighbors (KNN), Na\u00efve Bayes (NB), Support Vector Classifier (SVC), Linear Support Vector Classifier (LSVC), Random Forest Classifier (RFC), Decision Tree Regressor (DTR), Deletion Method, Multi-Layer Perceptron (MLP), and the Mean Value. The results show that the novel method outperforms the eleven (11) existing methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,290"
    },
    {
        "issnkey": "13596446",
        "isbn": null,
        "journal": "Drug Discovery Today",
        "publisher": null,
        "title": "systematicriskidentificationandassessmentusinganewriskmapinpharmaceuticalrd",
        "booktitle": null,
        "doi": "10.1016/j.drudis.2021.06.015",
        "author": [
            "Schuhmacher, Alexander",
            "Brieke, Clara",
            "Gassmann, Oliver",
            "Hinder, Markus",
            "Hartl, Dominik"
        ],
        "keywords": [
            "Pharmaceutical, Research and development (R&D), Artificial intelligence, Drug discovery, Drug development, Risk"
        ],
        "abstract": "Delivering transformative therapies to patients while maintaining growth in the pharmaceutical industry requires an efficient use of research and development (R&D) resources and technologies to develop high-impact new molecular entities (NMEs). However, increasing global R&D competition in the pharmaceutical industry, growing impact of generics and biosimilars, more stringent regulatory requirements, as well as cost-constrained reimbursement frameworks challenge current business models of leading pharmaceutical companies. Big data-based analytics and artificial intelligence (AI) approaches have disrupted various industries and are having an increasing impact in the biopharmaceutical industry, with the promise to improve and accelerate biopharmaceutical R&D processes. Here, we systematically analyze, identify, assess, and categorize key risks across the drug discovery and development value chain using a new risk map approach, providing a comprehensive risk\u2013reward analysis for pharmaceutical R&D.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-824536-1",
        "journal": null,
        "publisher": "Academic Press",
        "title": "6limitationsandchallengesonthediagnosisofcovid19usingradiologyimagesanddeeplearning",
        "booktitle": "Data Science for COVID-19",
        "doi": "10.1016/B978-0-12-824536-1.00007-1",
        "author": [
            "K\u0131zrak, Merve",
            "M\u00fcft\u00fco\u011flu, Z\u00fcmr\u00fct",
            "Y\u0131ld\u0131r\u0131m, T\u00fclay"
        ],
        "keywords": [
            "COVID-19, Data privacy, Deep learning, Differential privacy, EfficientNet, Explainable artificial intelligence, Radiology imaging, Small data"
        ],
        "abstract": "The world is facing a great threat nowadays. The COVID-19 virus outbreak that occurred in Wuhan in China in December 2019 continues to increase in the middle of 2020. Within the scope of this epidemic, different contents of data are published and products for improving the treatment process. One of the major symptoms of COVID-19 epidemic disease, which was revealed by the World Health Organization, is intense cough and breathing difficulties. Chest X-ray (CXR) and computing tomography (CT) images of patients infected with COVID-19 are also a type of data that allows data scientists to work with healthcare professionals during this struggle. Fast evaluation of these images by experts is important in the days when the epidemic has suffered. This chapter focuses on artificial intelligence (AI) for a successful and rapid diagnostic recommendation as part of these deadly epidemic prevention efforts that have emerged. As a study case, a dataset of 373 CXR images, 139 of which were COVID-19 infected, collected from open sources, was used for diagnosis with deep learning approaches of COVID-19. The use of EfficientNet, an up-to-date and robust deep learning model for education, offers the possibility to become infected with an accuracy of 94.7%. Nevertheless, some limitations must be considered when producing AI solutions by making use of medical data. Using these results, a perspective is provided on the limitations of deep learning models in the diagnosis of COVID-19 from radiology images for data quality, amount of data, data privacy, explainability, and robust solutions.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13648152",
        "isbn": null,
        "journal": "Environmental Modelling & Software",
        "publisher": null,
        "title": "ageneralconceptualframeworkformultidimensionalspatiotemporaldatasets",
        "booktitle": null,
        "doi": "10.1016/j.envsoft.2021.105096",
        "author": [
            "Baumann, Peter"
        ],
        "keywords": [
            "Coverage, Spatio-temporal field, Datacube, Conceptual model, Standards, ISO, OGC"
        ],
        "abstract": "In the era of ubiquitous data collection and generation, demands are high to make these data accessible as widely as possible, with as little effort and as much power and flexibility as ever possible. On Earth data, this holds in particular for pixel data and point clouds, some of the main \u201cBig Data\u201d today. Coverages represent a unifying concept for space/time-varying data, especially for spatio-temporal gridded data, nowadays often called \u201cdatacubes\". Coverage standards exist, however, their fundaments appear in places technically outdated, imprecise, and not suitable for the full spectrum of data. Due to this lack there is a danger of missing interoperability goals and impeding future-directed \u201cBig Earth Data\u201d services. We introduce the conceptual coverage model of the forthcoming ISO 19123-1 standard. It is generic, supporting all spatio-temporal dimensions in a unified manner, and is compatible with the existing coverage implementation standards of OGC and ISO. We demonstrate feasibility through concrete service examples.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,828"
    },
    {
        "issnkey": "26665573",
        "isbn": null,
        "journal": "Computers and Education Open",
        "publisher": null,
        "title": "perspectivesonthechallengesofgeneralizabilitytransparencyandethicsinpredictivelearninganalytics",
        "booktitle": null,
        "doi": "10.1016/j.caeo.2021.100060",
        "author": [
            "Mathrani, Anuradha",
            "Susnjak, Teo",
            "Ramaswami, Gomathy",
            "Barczak, Andre"
        ],
        "keywords": [
            "Learning analytics, Generalizability, Interpretability, Feature extraction, Transparency, Ethics protocol"
        ],
        "abstract": "Educational institutions need to formulate a well-established data-driven plan to get long-term value from their learning analytics (LA) strategy. By tracking learners\u2019 digital traces and measuring learners\u2019 performance, institutions can discern consequential learning trends via use of predictive models to enhance their instructional services. However, questions remain on how the proposed LA system is suitable, meaningful, and justifiable. In this concept paper, we examine generalizability and transparency of the internals of predictive models, alongside the ethical challenges in using learners\u2019 data for building predictive capabilities. Model generalizability or transferability is hindered by inadequate feature representation, small and imbalanced datasets, concept drift, and contextually un-related domains. Additional challenges relate to trustworthiness and social acceptance of these models since algorithmic-driven models are difficult to interpret by themselves. Further, ethical dilemmas are faced in engaging with learners\u2019 data while developing and deploying LA systems at an institutional level. We propose methodologies for apprehending these challenges by establishing efforts for managing transferability and transparency, and further assessing the ethical standing on justifiable use of the LA strategy. This study showcases underlying relationships that exist between constructs pertaining to learners\u2019 data and the predictive model. We suggest the use of appropriate evaluation techniques and setting up research ethics protocols, since without proper controls in place, the model outcome would not be portable, transferable, trustworthy, or admissible as a responsible outcome. This concept paper has theoretical and practical implications for future inquiry in the burgeoning field of learning analytics.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0264410x",
        "isbn": null,
        "journal": "Vaccine",
        "publisher": null,
        "title": "exploringelectronichealthrecordstoestimatetheextentofcatchupimmunisationandfactorsassociatedwithunderimmunisationamongrefugeesandasylumseekersinsoutheastqueensland",
        "booktitle": null,
        "doi": "10.1016/j.vaccine.2021.09.026",
        "author": [
            "Nyanchoga, Mercy",
            "Lee, Patricia",
            "Barbery, Gaery"
        ],
        "keywords": [
            "Vaccination, Catch-up immunisation, Refugee, Migrant Health, Digital Health, Electronic Health Records"
        ],
        "abstract": "Background Australia is one of the leading countries resettling people from refugee-like backgrounds. Catch-up immunisation is a key priority in this cohort. However, few studies have included asylum seekers and the adult age group in their study sample. In addition, Electronic Health Records (EHR) has recently been recognised as a vital tool in big data analysis with the capacity to contribute to informed strategic decision making. As such, the main aim of this study is to explore EHR routinely used in a specialised refugee clinic in South East Queensland to estimate the extent of catch-up immunisation and assess the factors associated with under-immunisation among refugees and asylum seekers. Methods A quantitative study involving a secondary data analysis on a pre-existing dataset was undertaken. Relevant data was extracted from the EHR in the clinic. SPSS was used to perform Statistical data analysis. Results The majority of clients originated from Papua New Guinea, followed by Iran and Afghanistan. When assessing the uptake of catch-up immunisations among refugees and asylum seekers, MMR (Measles-Mumps-Rubella), Polio and DTP (Diphtheria-Tetanus-Pertussis) had the highest uptake, while HPV (Human Papilloma Virus), Pneumococcal and Hib (Haemophilus influenza type b) immunisations had the lowest uptake. Binary logistic regression revealed that the younger patients, the refugees (compared to asylum seekers) and those with a longer residential duration in Australia are at a higher risk of being under-immunised. Conclusion This study indicates that the broader group of immigrants, and in particular refugees and asylum seekers, do not represent a homogenous group in terms of immunisation coverage, and that each cohort should be carefully considered during immunisation interventions and strategies. This will be particularly important during targeted health promotions and future immunisation programs in this cohort.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "14391791",
        "isbn": null,
        "journal": "Basic and Applied Ecology",
        "publisher": null,
        "title": "crossdisciplinaryapproachesforbetterresearchthecaseofbirdsandbats",
        "booktitle": null,
        "doi": "10.1016/j.baae.2021.06.010",
        "author": [
            "Maas, Bea",
            "Ocampo-Ariza, Carolina",
            "Whelan, Christopher"
        ],
        "keywords": [
            "Agricultural biodiversity, Collaborative conservation, Ecosystem functions, Ecosystem services, Knowledge co-production, Sustainable agriculture, Transdisciplinary research"
        ],
        "abstract": "Across a wide range of disciplines, mounting evidence points to solutions for addressing the global biodiversity and climate crisis through sustainable land use development. Managing ecosystem services offers promising potential of combining environmental, economic, and social interests in this process. Achieving sustainability, however, requires collaboration across disciplines, or in short \u201ccross-disciplinary\u201d approaches. Multi-, inter- and transdisciplinary approaches are often used as synonyms, although they are defined by different levels of integrating results and perspectives. We highlight challenges and opportunities related to these cross-disciplinary approaches by using research on bird- and bat-mediated ecosystem services as a case - with a focus on sustainable agricultural development. Examples from transdisciplinary collaborations show how more integrative and inclusive approaches promote the implementation of basic and applied ecological research into land use practices. Realizing this opportunity requires strong partnerships between science, practice and policy, as well as integration of diverse skills and perspectives. If appropriately funded and guided, this effort is rewarded by improved data quality, more targeted concepts, as well as improvement implementation and impact of sustainability research and practice. We outline a stepwise approach for developing these processes and highlight case studies from bird and bat research to inspire cross-disciplinary approaches within and beyond ecology.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23529385",
        "isbn": null,
        "journal": "Remote Sensing Applications: Society and Environment",
        "publisher": null,
        "title": "asimpleandrobustwetlandclassificationapproachbyusingopticalindicesunsupervisedandsupervisedmachinelearningalgorithms",
        "booktitle": null,
        "doi": "10.1016/j.rsase.2021.100569",
        "author": [
            "Ahmed, Kazi",
            "Akter, Simu",
            "Marandi, Andres",
            "Sch\u00fcth, Christoph"
        ],
        "keywords": [
            "Wetland classification, Optical index, Machine learning, K-means cluster, Support vector machine classification"
        ],
        "abstract": "Wetlands are important for their peat reservoir, dynamic land cover and natural resources, ecological and hydrological regimes, fossil fuels reservoir, and crucial carbon storage. The global wetlands are decreasing since 1800 due to climatic phenomena and human activities. Wetland mapping with satellite data is not new but an ongoing challenge due to its precision relies on data quality and data classification schemes. The accuracy of such mapping is emerging due to the gradual establishment of satellite data and subsequent data modeling technologies, i.e., big data modeling with machine learning (ML) algorithms. Our study introduced a simple, scalable, and robust wetland classification by applying unsupervised (K-means cluster \u2013 KMC) and supervised (Support vector machine classification \u2013 SVMc) ML algorithms. We used Landsat optical data to model normalized difference vegetation index (NDVI) and normalized difference water index (NDWI), as the primary inputs for KMC. Later KMC data were supervised by SVMc with training data from 20 field observations. The accuracy tests insights that both optical indices have considerably less error and all SVMc models have about 99% accuracy. The 1 to 1 validation insights that our wetland classification presented more detail wetland cover areas than reference data. The test case SVMc showed that Class 1 and 2 are optimally fitting, Class 3 and 4 are overfitting, and Class 5 is underfitting. Furthermore, the sensitivity analysis insight that all SVMc models are optimally fitting and SVMc is more sensitive to NDVI than NDWI. From SVMc models we can see that Selisoo bog in Estonia lost a considerable amount of wetland covers including water bodies, and more large forest covers are taking wetland areas, i.e., mixed forest and coniferous trees. Our methodological approach insights a simple and robust wetland classification based on advanced unsupervised and supervised ML algorithms despite some unavoidable limitation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,703"
    },
    {
        "issnkey": "02751062",
        "isbn": null,
        "journal": "Chinese Astronomy and Astrophysics",
        "publisher": null,
        "title": "surfacerotationoflamostkeplerlirichgiantstars",
        "booktitle": null,
        "doi": "10.1016/j.chinastron.2021.02.003",
        "author": [
            "Ming-hao, DU",
            "Shao-lan, BI",
            "Jian-rong, SHI",
            "Hong-liang, YAN"
        ],
        "keywords": [
            "Stars: rotation, Stars: abundance, stars: chemically peculiar, method: data analysis"
        ],
        "abstract": "The years-long high-precision photometric data observed by Kepler satellite combining with huge amount of spectra observed by LAMOST provide a great opportunity to study the relations between surface rotation and lithium abundance of li-rich giants. In this study, we cross match the Kepler data with li-rich giants catalog from LAMOST, and obtain 619 common sources. Then we measure 36 rotation periods from the full set of 295 li-rich giants with good data quality which consists of two sub-samples. The rotation periods of 14 stars was extracted from 205 stars with evolutionary stages determined using asteroseismology, including 11 core helium-burning stars (HeBs), 2 red giant branch stars (RGBs), and 1 unclassified star. All the super lithium-rich giant stars (A(Li)>3.3 dex) are HeBs in our sample. The remaining 90 giants do not have evolutionary stages confirmed, and in this sub-sample, 22 giants have their rotation period measured. The rotation detection rate of the former sub-sample is 6.8%, which is significantly higher than the detection rate of a large giant sample in previous studies (2.08%). With the surface rotation period measured, we confirm the relation between stellar rotation and lithium enrichment of giants. Meanwhile, we find that the less Li-enriched stars have a relatively dispersed distribution of rotation periods, and the giants with a high Li enrichment is concentrated on the rapidly rotating area, which is consistent with earlier studies. The present work also shows a jump at A(Li)\u22483.3 dex in the relation between rotation period and Li abundance that coincidently is the boundary between Li-rich giants and super Li-rich giants which may indicate the different mechanisms. The rotation periods of super Li-rich giants become shorter as the lithium enrichment increases. This correlation provides an evidence for the rotational induced extra-mixing mechanism responsible for Li enrichment of giants.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,132"
    },
    {
        "issnkey": "08954356",
        "isbn": null,
        "journal": "Journal of Clinical Epidemiology",
        "publisher": null,
        "title": "logisticregressionandmachinelearningpredictedpatientmortalityfromlargesetsofdiagnosiscodescomparably",
        "booktitle": null,
        "doi": "10.1016/j.jclinepi.2020.12.018",
        "author": [
            "Cowling, Thomas",
            "Cromwell, David",
            "Bellot, Alexis",
            "Sharples, Linda",
            "{van der Meulen}, Jan"
        ],
        "keywords": [
            "Machine learning, Regression analysis, Big data, Electronic health records, International Classification of Diseases, Comorbidity, Prognosis"
        ],
        "abstract": "Objective The objective of the study was to compare the performance of logistic regression and boosted trees for predicting patient mortality from large sets of diagnosis codes in electronic healthcare records. Study Design and Setting We analyzed national hospital records and official death records for patients with myocardial infarction (n = 200,119), hip fracture (n = 169,646), or colorectal cancer surgery (n = 56,515) in England in 2015\u20132017. One-year mortality was predicted from patient age, sex, and socioeconomic status, and 202 to 257 International Classification of Diseases 10th Revision codes recorded in the preceding year or not (binary predictors). Performance measures included the c-statistic, scaled Brier score, and several measures of calibration. Results One-year mortality was 17.2% (34,520) after myocardial infarction, 27.2% (46,115) after hip fracture, and 9.3% (5,273) after colorectal surgery. Optimism-adjusted c-statistics for the logistic regression models were 0.884 (95% confidence interval [CI]: 0.882, 0.886), 0.798 (0.796, 0.800), and 0.811 (0.805, 0.817). The equivalent c-statistics for the boosted tree models were 0.891 (95% CI: 0.889, 0.892), 0.804 (0.802, 0.806), and 0.803 (0.797, 0.809). Model performance was also similar when measured using scaled Brier scores. All models were well calibrated overall. Conclusion In large datasets of electronic healthcare records, logistic regression and boosted tree models of numerous diagnosis codes predicted patient mortality comparably.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.437",
        "scimago_value": "2,993"
    },
    {
        "issnkey": "16747755",
        "isbn": null,
        "journal": "Journal of Rock Mechanics and Geotechnical Engineering",
        "publisher": null,
        "title": "comparisonofmachinelearningmethodsforgroundsettlementpredictionwithdifferenttunnelingdatasets",
        "booktitle": null,
        "doi": "10.1016/j.jrmge.2021.08.006",
        "author": [
            "Tang, Libin",
            "Na, SeonHong"
        ],
        "keywords": [
            "Surface settlement, Tunnel construction, Machine learning (ML), Hyperparameter optimization, Cross-validation (CV)"
        ],
        "abstract": "This study integrates different machine learning (ML) methods and 5-fold cross-validation (CV) method to estimate the ground maximal surface settlement (MSS) induced by tunneling. We further investigate the applicability of artificial intelligent (AI) based prediction through a comparative study of two tunnelling datasets with different sizes and features. Four different ML approaches, including support vector machine (SVM), random forest (RF), back-propagation neural network (BPNN), and deep neural network (DNN), are utilized. Two techniques, i.e. particle swarm optimization (PSO) and grid search (GS) methods, are adopted for hyperparameter optimization. To assess the reliability and efficiency of the predictions, three performance evaluation indicators, including the mean absolute error (MAE), root mean square error (RMSE), and Pearson correlation coefficient (R), are calculated. Our results indicate that proposed models can accurately and efficiently predict the settlement, while the RF model outperforms the other three methods on both datasets. The difference in model performance on two datasets (Datasets A and B) reveals the importance of data quality and quantity. Sensitivity analysis indicates that Dataset A is more significantly affected by geological conditions, while geometric characteristics play a more dominant role on Dataset B.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.338",
        "scimago_value": "1,470"
    },
    {
        "issnkey": "09507051",
        "isbn": null,
        "journal": "Knowledge-Based Systems",
        "publisher": null,
        "title": "truemeanvaluediscoveryovermultipledatasourceswithunknownreliabilitydegrees",
        "booktitle": null,
        "doi": "10.1016/j.knosys.2021.107036",
        "author": [
            "Ye, Songtao",
            "Wang, Junjie",
            "Fan, Hongjie",
            "Zhang, Zhiqiang"
        ],
        "keywords": [
            "Multiple data sources, Source reliabilities, True mean value discovery, Confidence interval"
        ],
        "abstract": "In the era of big data, we are committed to obtaining the observations of target objects from a wider range of data sources. As the number of data sources increases, we expect that more trustworthy statistical parameters can be estimated from the multi-source observations, for example, the population mean. However, the reliability of data sources rarely attracts our attention, because the hypothesis testing seems to be an effective tool for determining whether a given estimate is acceptable. In practice, the noisy observations from different unreliable data sources may have different statistical characteristic parameters, and these parameters are unknown. It makes the condition that observations should be identically distributed in hypothesis testing no longer tenable. Therefore, a poor estimate of the population mean may be accepted, as the hypothesis testing is performed over the multi-source observations. To address this issue, in this paper, we propose a true mean value discovery algorithm in which we can use multi-source observations to determine whether an estimated population mean should be rejected. Additionally, the reliability degree of each data source can be estimated using the proposed algorithm. By removing incorrect observations provided by unreliable sources, we can obtain more reliable estimates of true population means. Experiments on three real-world tasks demonstrate that the proposed method outperforms state-of-the-art approaches.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.038",
        "scimago_value": "1,587"
    },
    {
        "issnkey": "03608352",
        "isbn": null,
        "journal": "Computers & Industrial Engineering",
        "publisher": null,
        "title": "improvingsupplychainresiliencethroughindustry40asystematicliteraturereviewundertheimpressionsofthecovid19pandemic",
        "booktitle": null,
        "doi": "10.1016/j.cie.2021.107452",
        "author": [
            "Spieske, Alexander",
            "Birkel, Hendrik"
        ],
        "keywords": [
            "Industry 4.0, Supply chain risk management, Supply chain resilience, Supply chain disruption, Digital supply chain, Literature review"
        ],
        "abstract": "The COVID-19 pandemic is one of the most severe supply chain disruptions in history and has challenged practitioners and scholars to improve the resilience of supply chains. Recent technological progress, especially industry 4.0, indicates promising possibilities to mitigate supply chain risks such as the COVID-19 pandemic. However, the literature lacks a comprehensive analysis of the link between industry 4.0 and supply chain resilience. To close this research gap, we present evidence from a systematic literature review, including 62 papers from high-quality journals. Based on a categorization of industry 4.0 enabler technologies and supply chain resilience antecedents, we introduce a holistic framework depicting the relationship between both areas while exploring the current state-of-the-art. To verify industry 4.0\u2019s resilience opportunities in a severe supply chain disruption, we apply our framework to a use case, the COVID-19-affected automotive industry. Overall, our results reveal that big data analytics is particularly suitable for improving supply chain resilience, while other industry 4.0 enabler technologies, including additive manufacturing and cyber-physical systems, still lack proof of effectiveness. Moreover, we demonstrate that visibility and velocity are the resilience antecedents that benefit most from industry 4.0 implementation. We also establish that industry 4.0 holistically supports pre-disruption resilience measures, enabling more effective proactive risk management. Both research and practice can benefit from this study. While scholars may analyze resilience potentials of under-explored enabler technologies, practitioners can use our findings to guide industry 4.0 investment decisions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,315"
    },
    {
        "issnkey": "1470160x",
        "isbn": null,
        "journal": "Ecological Indicators",
        "publisher": null,
        "title": "assessmentofspatialtemporalchangesofecologicalenvironmentqualitybasedonrseiandgeeacasestudyinerhailakebasinyunnanprovincechina",
        "booktitle": null,
        "doi": "10.1016/j.ecolind.2021.107518",
        "author": [
            "Xiong, Yuan",
            "Xu, Weiheng",
            "Lu, Ning",
            "Huang, Shaodong",
            "Wu, Chao",
            "Wang, Leiguang",
            "Dai, Fei",
            "Kou, Weili"
        ],
        "keywords": [
            "Spatial-temporal changes, Erhai Lake Basin, Ecological environment quality, Spatial auto-correlation analysis, Google Earth Engine"
        ],
        "abstract": "The Erhai Lake Basin is an area with the active economic and social development of agriculture and tourism, facing increasingly prominent environmental problems with rapid urbanization. Assessing spatial\u2013temporal changes in ecological environment quality objectively and quantitatively in a timely fashion is crucial for environmental protection and policymaking. First, we selected the high-quality Landsat imagery acquired at the same time phase in the years of 1999, 2004, 2009, 2014, and 2019 respectively. Second, the remote sensing-based ecological index (RSEI) was constructed by using Landsat 5 TM and Landsat 8 OLI/TIRS imagery based on Google Earth Engine (GEE) platform. Third, we assessed the spatial\u2013temporal changes and spatial autocorrelation of ecological environment quality in our study area based on five RSEI maps. The mean values of RSEI in 1999, 2004, 2009, 2014, and 2019 were 0.513 0.457, 0.462, 0.506, and 0.509, respectively, which indicated that the overall ecological environment quality of the Erhai Lake Basin degraded from 1999 to 2009 and promoted from 2009 to 2019. The worst degradation occurred between 1999 and 2004, about 27.12% of the total area was degraded, and the greatest improvement occurred between 2009 and 2014, about 26.42% of the total area was improved. The Globalmoran's I value ranged from 0.662 to 0.783 in 1999\u20132019, which indicated that the spatial distribution of ecological environment quality was positively correlated. The cluster map of local indicator of spatial association of RSEI show that the high-high points were mainly located in the western and southern high-altitude area of the study area, and the low-low points were mainly distributed in lakeside area, where populations were dense and human activities were frequent. This study provides a promising approach to assess the spatial\u2013temporal changes in ecological environment quality based on RSEI and GEE, which is critical to investigate the interactions between human activities and ecosystem services in basin systems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "24520144",
        "isbn": null,
        "journal": "Gene Reports",
        "publisher": null,
        "title": "machinelearningapproachesforclassificationofcolorectalcancerwithandwithoutfeatureselectionmethodonmicroarraydata",
        "booktitle": null,
        "doi": "10.1016/j.genrep.2021.101419",
        "author": [
            "Nazari, Elham",
            "Aghemiri, Mehran",
            "Avan, Amir",
            "Mehrabian, Amin",
            "Tabesh, Hamed"
        ],
        "keywords": [
            "Colon cancer, Machine learning, Ensemble, Deep learning, Diagnosis, Classification, AdaBoost classifier, XGBclassifier, LightGBM, Random forest classifier, Big data, Advanced method, Feature, Microarray data, Gene, Early cancer diagnosis"
        ],
        "abstract": "To differentiate cancer cells from healthy cells, cancer research is focused on machine learning techniques because the high-accuracy classification provides the basis for early diagnosis and will lead to the appropriate treatment methods selection and costs reduction. Particularly in colon cancer, which is the most life-threatening cancer after lung cancer, early diagnosis can be extremely effective. In this paper, the lightGBM based Relief attribute evaluation and DNN based Relief attribute evaluation algorithm were proposed to differentiate between non-cancer cells and cancer cells in colon cancer compared to 20 other machine learning techniques. The data that was used involved 111 patients' microarray data including 22,278 features. Python and Weka software was used to implement the methods. Accuracy, precision, MCC, recall, and AUC indices were used for evaluation. Among the methods of the decision tree, decision stump, LMT, Na\u00efve Bayes, bagging, AdaBoost, QDA, DNN, etc. the proposed methods with 100% accuracy performed the classification work. The results of comparing the methods would prove that the proposed method can be an effective way to separate cells, particularly for this type of data, which has a high diversity. Accordingly, the development of state-of-the-art methods using existing models should focus on the current research field to be useful in research areas related to diseases, especially cancers, and to have an effective performance in the benchmark.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,235"
    },
    {
        "issnkey": "09333657",
        "isbn": null,
        "journal": "Artificial Intelligence in Medicine",
        "publisher": null,
        "title": "adatascienceapproachtodrugsafetysemanticandvisualminingofadversedrugeventsfromclinicaltrialsofpaintreatments",
        "booktitle": null,
        "doi": "10.1016/j.artmed.2021.102074",
        "author": [
            "Lamy, Jean-Baptiste"
        ],
        "keywords": [
            "Data mining, Ontology, Visual analytics, Glyph, Drug safety, Adverse drug events, Pain treatments, Painkillers"
        ],
        "abstract": "Clinical trials are the basis of Evidence-Based Medicine. Trial results are reviewed by experts and consensus panels for producing meta-analyses and clinical practice guidelines. However, reviewing these results is a long and tedious task, hence the meta-analyses and guidelines are not updated each time a new trial is published. Moreover, the independence of experts may be difficult to appraise. On the contrary, in many other domains, including medical risk analysis, the advent of data science, big data and visual analytics allowed moving from expert-based to fact-based knowledge. Since 12 years, many trial results are publicly available online in trial registries. Nevertheless, data science methods have not yet been applied widely to trial data. In this paper, we present a platform for analyzing the safety events reported during clinical trials and published in trial registries. This platform is based on an ontological model including 582 trials on pain treatments, and uses semantic web technologies for querying this dataset at various levels of granularity. It also relies on a 26-dimensional flower glyph for the visualization of the Adverse Drug Events (ADE) rates in 13 categories and 2 levels of seriousness. We illustrate the interest of this platform through several use cases and we were able to find back conclusions that were initially found during meta-analyses. The platform was presented to four experts in drug safety, and is publicly available online, with the ontology of pain treatment ADE.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.326",
        "scimago_value": "0,980"
    },
    {
        "issnkey": "22132198",
        "isbn": null,
        "journal": "The Journal of Allergy and Clinical Immunology: In Practice",
        "publisher": null,
        "title": "artificialintelligencemachinelearninginrespiratorymedicineandpotentialroleinasthmaandcopddiagnosis",
        "booktitle": null,
        "doi": "10.1016/j.jaip.2021.02.014",
        "author": [
            "Kaplan, Alan",
            "Cao, Hui",
            "FitzGerald, J.",
            "Iannotti, Nick",
            "Yang, Eric",
            "Kocks, Janwillem",
            "Kostikas, Konstantinos",
            "Price, David",
            "Reddel, Helen",
            "Tsiligianni, Ioanna",
            "Vogelmeier, Claus",
            "Pfister, Pascal",
            "Mastoridis, Paul"
        ],
        "keywords": [
            "Asthma, Artificial intelligence, COPD, Diagnosis, Machine learning, Respiratory disease"
        ],
        "abstract": "Artificial intelligence (AI) and machine learning, a subset of AI, are increasingly used in medicine. AI excels at performing well-defined tasks, such as image recognition; for example, classifying skin biopsy lesions, determining diabetic retinopathy severity, and detecting brain tumors. This article provides an overview of the use of AI in medicine and particularly in respiratory medicine, where it is used to evaluate lung cancer images, diagnose fibrotic lung disease, and more recently is being developed to aid the interpretation of pulmonary function tests and the diagnosis of a range of obstructive and restrictive lung diseases. The development and validation of AI algorithms requires large volumes of well-structured data, and the algorithms must work with variable levels of data quality. It is important that clinicians understand how AI can function in the context of heterogeneous conditions such as asthma and chronic obstructive pulmonary disease where diagnostic criteria overlap, how AI use fits into everyday clinical practice, and how issues of patient safety should be addressed. AI has a clear role in providing support for doctors in the clinical workplace, but its relatively recent introduction means that confidence in its use still has to be fully established. Overall, AI is expected to play a key role in aiding clinicians in the diagnosis and management of respiratory diseases in the future, and it will be exciting to see the benefits that arise for patients and doctors from its use in everyday clinical practice.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.861",
        "scimago_value": "1,731"
    },
    {
        "issnkey": "13837621",
        "isbn": null,
        "journal": "Journal of Systems Architecture",
        "publisher": null,
        "title": "commercialhypervisorbasedtasksandboxingmechanismsareunsecuredbutwecanfixit",
        "booktitle": null,
        "doi": "10.1016/j.sysarc.2021.102114",
        "author": [
            "Huo, Dongdong",
            "Cao, Chen",
            "Liu, Peng",
            "Wang, Yazhe",
            "Li, Mingxuan",
            "Xu, Zhen"
        ],
        "keywords": [
            "Cyber\u2013Physical\u2013Social Systems, Internet of Things devices, RTOS task sandboxing, Intra-Mode Privilege Separation"
        ],
        "abstract": "Cyber\u2013Physical\u2013Social Systems are frequently prescribed for providing valuable information on personalized services. The foundation of these services is big data which must be trustily collected and efficiently processed. Though High Performance Computing and Communication technique makes great contributions to addressing the issue of data processing, its effectiveness still relies on the veracity of data generated from Internet of Things (IoT) devices. Nevertheless, IoT devices, as basic production facilities to ensure data\u2019s security, are unable to deploy expensive security extensions. Consequently, it causes the implementation of the task sandboxing, the fundamental security mechanism in Real-Time Operating Systems (RTOSs), much simpler and more vulnerable. In this paper, we take ARM Mbed uVisor as an example system, utilizing hypervisor-based task sandboxing mechanisms, and presents three new findings: First, we discover vulnerabilities against Mbed task sandboxing, which can be exploited to compromise system-maintained data structure to manipulate any tasks\u2019 data. Second, we present LIPS (Lightweight Intra-Mode Privilege Separation), building a special protection domain to isolate particular system-maintained data structures. Finally, thorough evaluation and experimental tests show the efficiency of LIPS to defeat these attacks, with small runtime overheads and good portability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.777",
        "scimago_value": "0,598"
    },
    {
        "issnkey": "00128252",
        "isbn": null,
        "journal": "Earth-Science Reviews",
        "publisher": null,
        "title": "deeplearningforgeologicalhazardsanalysisdatamodelsapplicationsandopportunities",
        "booktitle": null,
        "doi": "10.1016/j.earscirev.2021.103858",
        "author": [
            "Ma, Zhengjing",
            "Mei, Gang"
        ],
        "keywords": [
            "Geological hazards, Earth observation data, Deep learning, Landslide detection, Seismic phase picking"
        ],
        "abstract": "As natural disasters are induced by geodynamic activities or abnormal changes in the environment, geological hazards tend to wreak havoc on the environment and human society. Recently, the dramatic increase in the volume of various types of Earth observation \u2018big data\u2019 from multiple sources, and the rapid development of deep learning as a state-of-the-art data analysis tool, have enabled novel advances in geological hazard analysis, with the ultimate aim to mitigate the devastation associated with these hazards. Motivated by numerous applications, this paper presents an overview of the advances in the utilization of deep learning for geological hazard analysis. First, six commonly available Earth observation data sources are described, e.g., unmanned aerial vehicles, satellite platforms, and in-situ monitoring systems. Second, the deep learning background and six typical deep learning models are introduced, such as convolutional neural networks and recurrent neural networks. Third, focusing on six typical geological hazards, i.e., landslides, debris flows, rockfalls, avalanches, earthquakes, and volcanoes, the deep learning applications for geological hazard analysis are reviewed, and common application paradigms are summarized. Finally, the challenges and opportunities for the application of deep learning models for geological hazard analysis are highlighted, with the aim to inspire further related research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "12.413",
        "scimago_value": "3,893"
    },
    {
        "issnkey": "0038092x",
        "isbn": null,
        "journal": "Solar Energy",
        "publisher": null,
        "title": "typicaldailyprofilesanovelapproachforphotovoltaicsperformanceassessmentcasestudyonlargescalesystemsinchile",
        "booktitle": null,
        "doi": "10.1016/j.solener.2021.07.007",
        "author": [
            "Ascencio-V\u00e1squez, Juli\u00e1n",
            "Osorio-Aravena, Juan",
            "Brecl, Kristijan",
            "Mu\u00f1oz-Cer\u00f3n, Emilio",
            "Topi\u010d, Marko"
        ],
        "keywords": [
            "Photovoltaic, Performance, PV system, Typical Daily Profiles, KPIs"
        ],
        "abstract": "A growing photovoltaic industry shows exponential deployment worldwide, and it is expected to largely contribute to the energy transition. Because PV technologies will play a major role in achieving global sustainable development and climate goals, driving more energy-efficient scenarios will require efficient approaches to evaluate systems, especially when dealing with big data of a large region or portfolio. In this work, a novel approach for the PV performance assessment of photovoltaic systems, called \u201cTypical Daily Profiles\u201d (TDP), is presented. This approach is tested on the entire PV fleet operating in Chile from 2014 to 2019. The TDP approach can help to calculate key performance indicators, identify the mounting configuration of PV systems, and to detect major technical issues. A detailed validation carried on the Chilean PV fleet confirms the TDP approach's capabilities to provide accurate PV performance results, neglecting external factors such as failures, grid curtailment, or poor operation activities. Chile was chosen due to the large variety of climate zones in its unique geography, which helps to understand the PV performance under different environmental conditions. Besides, this study reveals the immense potential of PV technologies in Chile compared to mature PV markets worldwide. On an annual basis, their unit capacity factors can reach up to 38%, performance ratios above 90%, and the highest energy yield close to 3350 kWh/kWp. Proving to be an accurate tool, the Typical Daily Profiles approach can be easily applied to other PV portfolios in different regions, and a periodical execution could help to identify and understand long-term performance losses.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01681699",
        "isbn": null,
        "journal": "Computers and Electronics in Agriculture",
        "publisher": null,
        "title": "towardsautomatedgreenhouseastateoftheartreviewongreenhousemonitoringmethodsandtechnologiesbasedoninternetofthings",
        "booktitle": null,
        "doi": "10.1016/j.compag.2021.106558",
        "author": [
            "Li, Haixia",
            "Guo, Yu",
            "Zhao, Huajian",
            "Wang, Yang",
            "Chow, David"
        ],
        "keywords": [
            "Agriculture greenhouse building, Environment, Energy saving, Internet of Things (IoT), Monitoring system"
        ],
        "abstract": "As a controllable environment, greenhouse has less resource consumption and emission than field crop production and reduced greenhouse gas emissions from agricultural production. Besides, the greenhouse with an intelligent monitoring system has better energy-saving and emission reduction effects. Simultaneously, the intelligent monitoring system can predict the extreme greenhouse environment in advance, reduce diseases and insect pests, reduce the use of pesticides and fertilizers, and provide high-quality food. Researchers are becoming more and more interested in greenhouse monitoring systems, and how to put them into production correctly and effectively is a major challenge. This paper aims to review the intelligent greenhouse monitoring system systematically, serve the data transmission and server processing subsystems by identifying, listing and further explaining the greenhouse environmental parameters and studying the overall design of the greenhouse monitoring system. According to the characteristics of each component of the system, the paper makes a comparative study and obtains its development trend, summarizes the current popular technology and the future development trend of the intelligent monitoring system, and provides support for the research of greenhouse monitoring system. It was found that multi-parameter monitoring is beneficial to achieve effective greenhouse control, and wireless technology has gradually replaced wired mode for data transmission in the environment both inside and outside the greenhouse. Notably, deep learning, big data, and other advanced technologies used in greenhouse monitoring are considered valuable developments, further refine unmanned greenhouse management, and further improve greenhouse construction's energy utilization.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.565",
        "scimago_value": "1,208"
    },
    {
        "issnkey": "00489697",
        "isbn": null,
        "journal": "Science of The Total Environment",
        "publisher": null,
        "title": "theresponsesofweatheringcarbonsinktoecohydrologicalprocessesinglobalrocks",
        "booktitle": null,
        "doi": "10.1016/j.scitotenv.2021.147706",
        "author": [
            "Xi, Huipeng",
            "Wang, Shijie",
            "Bai, Xiaoyong",
            "Tang, Hong",
            "Luo, Guangjie",
            "Li, Huiwen",
            "Wu, Luhua",
            "Li, Chaojun",
            "Chen, Huan",
            "Ran, Chen",
            "Luo, Xuling"
        ],
        "keywords": [
            "Rock chemical weathering, Eco-hydrology, Global change, Big data"
        ],
        "abstract": "Eco-hydrological processes affect the chemical weathering carbon sink (CS) of rocks. However, due to data quality limitations, the magnitude of the CS of rocks and their responses to eco-hydrological processes are not accurately understood. Therefore, based on Global Erosion Model for CO2 fluxes (GEM-CO2 model), hydrological site data, and multi-source remote sensing data, we produced a 0.05\u00b0 \u00d7 0.05\u00b0 resolution dataset of CS for 11 types of rocks from 2001 to 2018. The results show that the total amount of CS of global rocks is 0.32 \u00b1 0.02 Pg C, with an average flux of 2.7 t C km\u22122 yr\u22121, accounting for 53% and 3% of the \u201cmissing\u201d carbon sink and fossil fuel emissions, respectively. This is 23% higher than previous research results, which may be due to the increased resolution. Although about 60% of the CS of global rocks are in a stable state, there are obvious differences among rocks. For example, the CS of carbonate rocks exhibited a significant increase (0.30 Tg C/yr), while the CS of siliceous clastic sedimentary rocks exhibited a significant decrease (\u22120.06 Tg C/yr). Although temperature is an important factor affecting the CS, the proportion of soil moisture in arid and temperate climate zones is higher (accounting for 24%), which is 3.6 times that of temperature. Simulations based on representative concentration pathways scenarios indicate that the global CS of rocks may increase by about 28% from 2050 to 2100. In short, we produced a set of high-resolution datasets for the CS of global rocks, which makes up for the lack of datasets in previous studies and improves our understanding of the magnitude and spatial pattern of the CS and its responses to eco-hydrological processes.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.963",
        "scimago_value": "1,795"
    },
    {
        "issnkey": "15740137",
        "isbn": null,
        "journal": "Computer Science Review",
        "publisher": null,
        "title": "conceptualandempiricalcomparisonofdimensionalityreductionalgorithmspcakpcaldamdssvdlleisomapleicatsne",
        "booktitle": null,
        "doi": "10.1016/j.cosrev.2021.100378",
        "author": [
            "Anowar, Farzana",
            "Sadaoui, Samira",
            "Selim, Bassant"
        ],
        "keywords": [
            "Dimension reduction, Optimal set of features, Data quality, High-dimensional datasets, Correlation metrics, Classification accuracy, Run-time"
        ],
        "abstract": "Feature Extraction Algorithms (FEAs) aim to address the curse of dimensionality that makes machine learning algorithms incompetent. Our study conceptually and empirically explores the most representative FEAs. First, we review the theoretical background of many FEAs from different categories (linear vs. nonlinear, supervised vs. unsupervised, random projection-based vs. manifold-based), present their algorithms, and conduct a conceptual comparison of these methods. Secondly, for three challenging binary and multi-class datasets, we determine the optimal sets of new features and assess the quality of the various transformed feature spaces in terms of statistical significance and power analysis, and the FEA efficacy in terms of classification accuracy and speed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.872",
        "scimago_value": "1,646"
    },
    {
        "issnkey": "09507051",
        "isbn": null,
        "journal": "Knowledge-Based Systems",
        "publisher": null,
        "title": "aclassificationframeworkformultivariatecompositionaldatawithdirichletfeatureembedding",
        "booktitle": null,
        "doi": "10.1016/j.knosys.2020.106614",
        "author": [
            "Gu, Jie",
            "Cui, Bin",
            "Lu, Shan"
        ],
        "keywords": [
            "Multivariate compositional data, Classification, Feature embedding, Dirichlet distribution, Support vector machine"
        ],
        "abstract": "Compositional data which contain relative or structure information of a whole occur commonly in many disciplines and practical scenarios. Yet relatively few works are available for multivariate compositional data classification with different numbers of parts using machine learning. This is because compositional data is inherently constrained to unit sum, resulting in the existing methods cannot be directly applied. Particularly, the multivariate analysis methods for compositional data variables with unequal sizes of parts are not sufficiently investigated. Moreover, to design a good classification model is indeed a complicated work. Except for the learning algorithm, data quality is also an essential determinant, which is rarely been concerned. In this paper, we propose an effective framework for multivariate compositional data classification. Specifically, the Dirichlet feature embedding is proposed to implement on the original compositional data features with the goal of removing the constraint and obtaining high quality training data, as well as reducing the dimension. Support vector machine is then used to build the classification model. Results of simulation study and real-world dataset show our proposed method can achieve good performances.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.038",
        "scimago_value": "1,587"
    },
    {
        "issnkey": "0967070x",
        "isbn": null,
        "journal": "Transport Policy",
        "publisher": null,
        "title": "predictingweatherinduceddelaysofhighspeedrailandaviationinchina",
        "booktitle": null,
        "doi": "10.1016/j.tranpol.2020.11.008",
        "author": [
            "Chen, Zhenhua",
            "Wang, Yuxuan",
            "Zhou, Lei"
        ],
        "keywords": [
            "High-speed rail, Aviation, Extreme weather event, Machine learning, Big data"
        ],
        "abstract": "High-speed rail (HSR) has become a competitive mode with aviation for medium-distance intercity travel, given the massive deployment of the HSR infrastructure network in China. While the travel experience with both HSR and air has become more convenient, the systems\u2019 operational reliability in terms of punctuality remains a key concern, especially during disruptive events, such as under severe weather conditions. Although previous studies have attempted to investigate the impact of severe weather events on the operational performance of transportation systems, there is still a lack of ability to forecast to what extent the performance of different transportation systems may vary under various conditions. This study develops an integrated modeling framework that allows us to predict the performance of weather-induced delays of different transportation systems, including HSR and aviation. By applying machine-learning methods to real-world transportation performance data, the study examines the robustness of the method, variations of data characteristics and the different applications of the predictive modeling system. Overall, the concept and modeling framework provide important implications for the improvement of transportation system resilience to various severe weather-related disruptions through the understanding of the impact and its predictability of the system performance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22113835",
        "isbn": null,
        "journal": "Acta Pharmaceutica Sinica B",
        "publisher": null,
        "title": "insightintochemicalbasisoftraditionalchinesemedicinebasedonthestateofthearttechniquesofliquidchromatographymassspectrometry",
        "booktitle": null,
        "doi": "10.1016/j.apsb.2021.02.017",
        "author": [
            "Yu, Yang",
            "Yao, Changliang",
            "Guo, De-an"
        ],
        "keywords": [
            "Liquid chromatography\u2212mass spectrometry, Qualitative analysis, Traditional Chinese medicine, Data acquisition, Data post-processing"
        ],
        "abstract": "Traditional Chinese medicine (TCM) has been an indispensable source of drugs for curing various human diseases. However, the inherent chemical diversity and complexity of TCM restricted the safety and efficacy of its usage. Over the past few decades, the combination of liquid chromatography with mass spectrometry has contributed greatly to the TCM qualitative analysis. And novel approaches have been continuously introduced to improve the analytical performance, including both the data acquisition methods to generate a large and informative dataset, and the data post-processing tools to extract the structure-related MS information. Furthermore, the fast-developing computer techniques and big data analytics have markedly enriched the data processing tools, bringing benefits of high efficiency and accuracy. To provide an up-to-date review of the latest techniques on the TCM qualitative analysis, multiple data-independent acquisition methods and data-dependent acquisition methods (precursor ion list, dynamic exclusion, mass tag, precursor ion scan, neutral loss scan, and multiple reaction monitoring) and post-processing techniques (mass defect filtering, diagnostic ion filtering, neutral loss filtering, mass spectral trees similarity filter, molecular networking, statistical analysis, database matching, etc.) were summarized and categorized. Applications of each technique and integrated analytical strategies were highlighted, discussion and future perspectives were proposed as well.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0169409x",
        "isbn": null,
        "journal": "Advanced Drug Delivery Reviews",
        "publisher": null,
        "title": "automationanddatadrivendesignofpolymertherapeutics",
        "booktitle": null,
        "doi": "10.1016/j.addr.2020.11.009",
        "author": [
            "Upadhya, Rahul",
            "Kosuri, Shashank",
            "Tamasi, Matthew",
            "Meyer, Travis",
            "Atta, Supriya",
            "Webb, Michael",
            "Gormley, Adam"
        ],
        "keywords": [
            "Polymer chemistry, High throughput screening, Automation, Machine learning, Artificial intelligence, Drug delivery, Gene delivery"
        ],
        "abstract": "Polymers are uniquely suited for drug delivery and biomaterial applications due to tunable structural parameters such as length, composition, architecture, and valency. To facilitate designs, researchers may explore combinatorial libraries in a high throughput fashion to correlate structure to function. However, traditional polymerization reactions including controlled living radical polymerization (CLRP) and ring-opening polymerization (ROP) require inert reaction conditions and extensive expertise to implement. With the advent of air-tolerance and automation, several polymerization techniques are now compatible with well plates and can be carried out at the benchtop, making high throughput synthesis and high throughput screening (HTS) possible. To avoid HTS pitfalls often described as \u201cfishing expeditions,\u201d it is crucial to employ intelligent and big data approaches to maximize experimental efficiency. This is where the disruptive technologies of machine learning (ML) and artificial intelligence (AI) will likely play a role. In fact, ML and AI are already impacting small molecule drug discovery and showing signs of emerging in drug delivery. In this review, we present state-of-the-art research in drug delivery, gene delivery, antimicrobial polymers, and bioactive polymers alongside data-driven developments in drug design and organic synthesis. From this insight, important lessons are revealed for the polymer therapeutics community including the value of a closed loop design-build-test-learn workflow. This is an exciting time as researchers will gain the ability to fully explore the polymer structural landscape and establish quantitative structure-property relationships (QSPRs) with biological significance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-816078-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "biomedicalandclinicalresearchdatamanagement",
        "booktitle": "Systems Medicine",
        "doi": "10.1016/B978-0-12-801238-3.11621-6",
        "author": [
            "Ganzinger, Matthias",
            "Glaab, Enrico",
            "Kerssemakers, Jules",
            "Nahnsen, Sven",
            "Sax, Ulrich",
            "Schaadt, Nadine",
            "Schapranow, Matthieu-P.",
            "Tiede, Thorsten"
        ],
        "keywords": [
            "Bioinformatics, Data integration, Data management, Data quality, Data sharing, Medical informatics"
        ],
        "abstract": "Systems medicine is an interdisciplinary approach in medicine that relies on computational models based on data from a variety of sources. Typically, such sources include clinical and biomedical data with heterogeneous data definitions that are sometimes not even structured in a useful way. Consequently, the systematic management of data is an important element for the successful implementation of systems medicine in both research and clinical application. In this article, we provide an overview over the following selected aspects of data management:\u2022Integration of multiple data sources\u2022IT infrastructures\u2022Data protection regulations\u2022Data history and data quality\u2022Data sharing/FAIR principles\u2022Use and access policies The presented best practices and experiences result from several systems medicine projects in which the authors have participated. They can be considered as recommendations for future projects in order to quickly set up data management infrastructures for systems medicine.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02648377",
        "isbn": null,
        "journal": "Land Use Policy",
        "publisher": null,
        "title": "quantifyingphysicalandpsychologicalperceptionsofurbanscenesusingdeeplearning",
        "booktitle": null,
        "doi": "10.1016/j.landusepol.2021.105762",
        "author": [
            "Zhang, Yonglin",
            "Li, Shanlin",
            "Dong, Rencai",
            "Deng, Hongbing",
            "Fu, Xiao",
            "Wang, Chenxing",
            "Yu, Tianshu",
            "Jia, Tianxia",
            "Zhao, Jingzhu"
        ],
        "keywords": [
            "Complex perceptions, Cityscapes, Image big data, Deep learning, Massive street-view datasets, Human-oriented"
        ],
        "abstract": "The complicated relationship between urban scenes and public perceptions has long been a concern in many disciplines. Previous studies have lacked human-oriented technical paths and high-throughput datasets to quantify physical and psychological perceptions in different land-use scenarios. This paper adopts a novel transfer learning approach to quantify the six types of landsense indices (LSIs) as psychological perception metrics and employs panoptic segmentation to parameterize the view index (VI) and the number of foreground instances (NFIs) as physical perception measures. Then, a quantitative analysis is conducted in Beijing\u2019s six Ring Road areas, and the connections between people\u2019s physical and psychological perceptions of heterogeneous land use are explored. The landsense maps can depict the distribution of LSIs and facilitate the understanding of complex perceptions distributed at a large scale. The regression model shows that natural landscapes (trees, grasses, and mountains) in the Beijing built-up area exhibit an overall positive performance. Moreover, for several block-level land uses, industrial scenery is related to overall negative psychological feelings. Parks and green spaces are positively related to psychological perceptions, because of the greater exposure opportunities to natural landscapes for residents. The framework in this research has potential in assisting urban planning and land-use management, and it enriches the datasets with extensive information, thereby improving the psychological perceptions of urban scenes from residents\u2019 perspectives. The novel approaches in this paper take a step forward in quantifying and understanding the public perceptions of urban landscapes.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.398",
        "scimago_value": "1,668"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822828-9",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter19prospectofdatascienceandartificialintelligenceforpatientspecificneuroprostheses",
        "booktitle": "Somatosensory Feedback for Neuroprosthetics",
        "doi": "10.1016/B978-0-12-822828-9.00005-8",
        "author": [
            "Yalug, Buse",
            "Arslan, Dilek",
            "Ozturk-Isik, Esin"
        ],
        "keywords": [
            "Artificial intelligence, machine learning, deep learning, patient-specific neuroprosthesis, big data"
        ],
        "abstract": "Machine learning and its subfield deep learning have recently gained interest in scientific research community due to their ability to analyze and learn from big data. In this chapter, we discuss the capabilities, limitations, and current applications of unsupervised and supervised machine learning methods in addition to more recent deep learning techniques for the design and control of patient-specific neuroprostheses. Furthermore, we speculate on what they could promise for future applications.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "25901982",
        "isbn": null,
        "journal": "Transportation Research Interdisciplinary Perspectives",
        "publisher": null,
        "title": "driversviewsondriverlessvehiclespublicperspectivesondefiningandusingautonomouscars",
        "booktitle": null,
        "doi": "10.1016/j.trip.2021.100446",
        "author": [
            "Schneble, Christophe",
            "Shaw, David"
        ],
        "keywords": [
            "Smart cars, Definition, Autonomous vehicles, Decision-making, Big data"
        ],
        "abstract": "Objectives To investigate how members of the public define autonomous vehicles, including perceived advantages, disadvantages and reliability. Methods A series of qualitative interviews were conducted with 16 members of the public in Switzerland who were recruited online and through snowballing. Interviews were transcribed and coded, before being subjected to thematic analysis. Results Three main themes emerged from the interviews. These included differing perceptions of the level of self-automation of smart cars, across a spectrum from partial automation to fully autonomous vehicles; a variety of views on the perceived pros and cons of smart cars, including fewer accidents and potential loss of freedom; and opinions concerning whether a car would be \u2018better than human\u2019, and related issues including whether such cars should or could be subject to human override. Almost all participants saw the introduction of truly smart cars as beneficial, although many said they would not be early adopters and would always want humans to have the ability to take control. Significance Our results reveal generally positive attitudes to smart cars, but also some poor understanding of the different levels of automation that such cars can have, and potentially unrealistic expectations regarding the capacity of humans to re-assume control in emergencies. These factors must be considered by legislators and others involved in the introduction of smart cars onto our roads.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,383"
    },
    {
        "issnkey": "03767388",
        "isbn": null,
        "journal": "Journal of Membrane Science",
        "publisher": null,
        "title": "deepspatialrepresentationlearningofpolyamidenanofiltrationmembranes",
        "booktitle": null,
        "doi": "10.1016/j.memsci.2020.118910",
        "author": [
            "Zhang, Ziyang",
            "Luo, Yingtao",
            "Peng, Huawen",
            "Chen, Yu",
            "Liao, Rong-Zhen",
            "Zhao, Qiang"
        ],
        "keywords": [
            "Nanofiltration, Thin film composite membranes, Feature engineering, Machine learning, Data augmentation, Molecular vibration"
        ],
        "abstract": "Machine learning overfitting caused by data scarcity greatly limits the application of chemical artificial intelligence in membrane materials. As the original data for thin film polyamide nanofiltration membranes is limited, here we propose to extract the natural features of monomer molecular structures and rationally distort them to augment the data availability. This few-shot learning method allows a chemical engineering project to leverage the powerful fit of deep learning without big data at the outset, which is advantageous over traditional machine learning models. The rejection and flux predictions of polyamide nanofiltration membranes are practiced by the molecular augmentation in deep learning. Convergence of loss function indicates that the model is effectively optimized. Correlation coefficients over 0.80 and the mean relative error below 5% prove an accurate prediction of nanofiltration performance. The success of predicting nanofiltration membrane performances is widely instructive for molecule and material science.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.742",
        "scimago_value": "1,929"
    },
    {
        "issnkey": "25895370",
        "isbn": null,
        "journal": "EClinicalMedicine",
        "publisher": null,
        "title": "estimatingcancersurvivalandprevalencewiththemedicalinsurancesystembasedcancersurveillancesystemmiscassanempiricalstudyinchina",
        "booktitle": null,
        "doi": "10.1016/j.eclinm.2021.100756",
        "author": [
            "Tian, Hongrui",
            "Hu, Yanjun",
            "Li, Qingxiang",
            "Lei, Liang",
            "Liu, Zhen",
            "Liu, Mengfei",
            "Guo, Chuanhai",
            "Liu, Fangfang",
            "Liu, Ying",
            "Pan, Yaqi",
            "dos-Santos-Silva, Isabel",
            "He, Zhonghu",
            "Ke, Yang"
        ],
        "keywords": [
            "Cancer surveillance, Survival, Prevalence, Health-related big data, China"
        ],
        "abstract": "Background We aimed to establish a new approach for surveillance of cancer prevalence and survival in China, based on the Medical-Insurance-System-based Cancer Surveillance System (MIS-CASS). Methods We constructed a standard procedure for data collection, cleaning, processing, linkage, verification, analysis, and estimation of cancer prevalence and survival (including both actual observations and model estimates) by conjoint use of medical insurance claims data and all-cause death surveillance data. As a proof-of-principle study, we evaluated the performance of this surveillance approach by estimating the latest prevalence and survival for upper gastrointestinal cancers in Hua County, a high-risk region for oesophageal cancer in China. Findings In Hua County, the age-standardised relative 5-year survival was 39\u00b72% (male: 36\u00b78%; female: 43\u00b76%) for oesophageal cancer and 33\u00b73% (male: 29\u00b76%; female: 43\u00b74%) for stomach cancer. For oesophageal cancer, better survival was observed in patients of 45\u201364 years compared with national average estimates, and women of <75 years had better survival than men. The 5-year prevalence rate in Hua County was 99\u00b78/100,000 (male: 105\u00b79/100,000; female: 93\u00b73/100,000) for oesophageal cancer and 41\u00b75/100,000 (male: 57\u00b74/100,000; female: 24\u00b75/100,000) for stomach cancer. For both of these cancers, the prevalence burden peaked at 65\u201379 years. The model estimates for survival and prevalence were close to the observations in real investigation, with a relative difference of less than 4\u00b75%. Interpretation This novel approach allows accurate estimation of cancer prevalence and survival with a short delay, which has great potential for regular use in general Chinese populations, especially those not covered by cancer registries. Funding The National Key R&D Program of China (2016YFC0901404), the National Science & Technology Fundamental Resources Investigation Program of China (2019FY101102), the National Natural Science Foundation of China (82073626), the Taikang Yicai Public Health and Epidemic Control Fund (TKYC-GW-2020), the Beijing-Tianjin-Hebei Basic Research Cooperation Project (J200016), and the Digestive Medical Coordinated Development Center of Beijing Hospitals Authority (XXZ0204).",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,915"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820567-9",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter9datainthecontextofconnectedandautomatedvehicles",
        "booktitle": "Connected and Automated Vehicles",
        "doi": "10.1016/B978-0-12-820567-9.00006-0",
        "author": [
            "Ponnaluri, Raj",
            "Alluri, Priyanka"
        ],
        "keywords": [
            "SAE J2735, CAV Data, Data architecture, Data governance, V2X data exchange platform"
        ],
        "abstract": "This chapter discusses several key components of CAV data and a V2X Data Exchange Platform. It begins by discussing SAE J2735 and the commonly used message data sets: Basic Safety Messages, Traveler Information Messages, Signal Request Messages, Signal Status Messages, Signal Phase and Timing Messages, and MAP data. Next, the following six data quality dimensions in the context of CAV are presented: standardization, hygiene, timeliness, completeness, validity, and accuracy and precision. The CAV data architecture, along with cloud services, data lake, and data edge, is then discussed. An overview of data security, governance, and change management is then provided. Finally, a blueprint for the V2X Data Exchange Platform is presented.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "24519294",
        "isbn": null,
        "journal": "Chem",
        "publisher": null,
        "title": "transitiontosustainablechemistrythroughdigitalization",
        "booktitle": null,
        "doi": "10.1016/j.chempr.2021.09.012",
        "author": [
            "Fantke, Peter",
            "Cinquemani, Claudio",
            "Yaseneva, Polina",
            "{De Mello}, Jonathas",
            "Schwabe, Henning",
            "Ebeling, Bjoern",
            "Lapkin, Alexei"
        ],
        "keywords": [
            "safe and sustainable by design, artificial intelligence, big data, green transition, sustainable development, machine learning"
        ],
        "abstract": "Summary Modern chemistry is the backbone of our society, but it is also a major contributor to global environmental pollution and the ongoing climate crisis. The transition toward a sustainable future requires a radical transformation of how chemistry is designed, developed, and used. This represents a \u201cbreak it or make it\u201d challenge for the chemical industry with significant technology lock-in and high entry barriers to radical innovations. We propose that urgently required systemic changes in chemical industry, research and development (R&D), chemicals assessment and management, and education to advance sustainable chemistry are attainable through increased and more rapid adoption of digitalization and new digital tools. This will enable flexible data exchange, increased transparency of information flows along cross-country chemical, material, and product life cycles, and chemistries that are safe and sustainable by design, addressing the complexity of chemicals-environment-health interactions and lowering the costs of entry into chemical R&D and manufacture, and new, more sustainable and collaborative business models.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "22.804",
        "scimago_value": "7,057"
    },
    {
        "issnkey": "25895370",
        "isbn": null,
        "journal": "eClinicalMedicine",
        "publisher": null,
        "title": "brainstructuralassociationswithdepressioninalargeearlyadolescentsampletheabcdstudy",
        "booktitle": null,
        "doi": "10.1016/j.eclinm.2021.101204",
        "author": [
            "Shen, Xueyi",
            "MacSweeney, Niamh",
            "Chan, Stella",
            "Barbu, Miruna",
            "Adams, Mark",
            "Lawrie, Stephen",
            "Romaniuk, Liana",
            "McIntosh, Andrew",
            "Whalley, Heather"
        ],
        "keywords": [
            "Big data, Adolescent depression, Adolescent Brain and Cognitive Development Study, Brain structure"
        ],
        "abstract": "Background Depression is the leading cause of disability worldwide with > 50% of cases emerging before the age of 25 years. Large-scale neuroimaging studies in depression implicate robust structural brain differences in the disorder. However, most studies have been conducted in adults and therefore, the temporal origins of depression-related imaging features remain largely unknown. This has important implications for understanding aetiology and informing timings of potential intervention. Methods Here, we examine associations between brain structure (cortical metrics and white matter microstructural integrity) and depression ratings (from caregiver and child), in a large sample (N = 8634) of early adolescents (9 to 11 years old) from the US-based, Adolescent Brain and Cognitive Development (ABCD) Study\u00ae. Data was collected from 2016 to 2018. Findings We report significantly decreased global cortical and white matter metrics, and regionally in frontal, limbic and temporal areas in adolescent depression (Cohen's d = -0\u22c5018 to -0\u22c5041, \u03b2 = -0\u00b7019 to -0\u22c5057). Further, we report consistently stronger imaging associations for caregiver-reported compared to child-reported depression ratings. Divergences between reports (caregiver vs child) were found to significantly relate to negative socio-environmental factors (e.g., family conflict, absolute \u03b2 = 0\u22c5048 to 0\u22c5169). Interpretation Depression ratings in early adolescence were associated with similar imaging findings to those seen in adult depression samples, suggesting neuroanatomical abnormalities may be present early in the disease course, arguing for the importance of early intervention. Associations between socio-environmental factors and reporter discrepancy warrant further consideration, both in the wider context of the assessment of adolescent psychopathology, and in relation to their role in aetiology. Funding Wellcome Trust (References: 104036/Z/14/Z and 220857/Z/20/Z) and the Medical Research Council (MRC, Reference: MC_PC_17209).",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,915"
    },
    {
        "issnkey": "01681699",
        "isbn": null,
        "journal": "Computers and Electronics in Agriculture",
        "publisher": null,
        "title": "livestockdataisitthereandisitfairasystematicreviewoflivestockfarmingdatasetsinaustralia",
        "booktitle": null,
        "doi": "10.1016/j.compag.2021.106365",
        "author": [
            "Bahlo, Christiane",
            "Dahlhaus, Peter"
        ],
        "keywords": [
            "Livestock data quality, Systematic data review, FAIR data, FAIR assessment, Precision livestock farming, Extensive livestock farming"
        ],
        "abstract": "The global adoption of the FAIR principles for scientific data: findable, accessible, interoperable and reusable, has been relatively slow in agriculture, compared to other disciplines. A recent review of the literature showed that the use of precision farming technologies and the development and adoption of open data standards was particularly low in extensive livestock farming. However, a plethora of public datasets exist that have the potential to be used to inform precision farming decision tools. Using extensive livestock farming in Australia as example, we investigate the quantity and quality of datasets available via a systematic dataset review. This systematic review of datasets begins with a search of open data catalogues and querying these to find datasets. Software scripts are developed and used to query the Application Programming Interfaces (APIs) of many of the large data catalogues in Australia, while catalogues without public APIs are queried manually via available web portals. Following the systematic search, a combined list of all datasets is collated and tested for FAIRness and other quality metrics. The contribution of this work is the resulting overview of the state of open datasets within the livestock farming domain on the one hand, but also the development of a systematic dataset search strategy, reusable methods and software scripts.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.565",
        "scimago_value": "1,208"
    },
    {
        "issnkey": "01989715",
        "isbn": null,
        "journal": "Computers, Environment and Urban Systems",
        "publisher": null,
        "title": "utilizinggeoreferencedimageryforsystematicsocialobservationofneighborhooddisorder",
        "booktitle": null,
        "doi": "10.1016/j.compenvurbsys.2021.101691",
        "author": [
            "Snaphaan, Thom",
            "Hardyns, Wim"
        ],
        "keywords": [
            "Artificial intelligence, Big data, Convolutional neural networks, Geo-referenced imagery, Neighborhood disorder, Systematic social observation"
        ],
        "abstract": "Research methods in social science take advantage from broader trends such as digitalization and increasing computational power, however, this is an evolving explorative search. The main purpose of this article is to describe the methodological innovations in the collection and processing of geo-referenced imagery for the observation of neighborhood disorder. In this narrative review, attention is paid to advances in both the data sources and the data processing methods used. Neighborhood disorder is traditionally measured by means of survey methods and (systematic) (social) observations, but these methods have specific shortcomings, such as respectively the subjective measurement that does not deliver a valid measure of actual prevalence of disorderly phenomena and the intensive use of resources in terms of time and money. This has repercussions for (the interpretation of) the results based on these data. Today, scholars have innovative data sources and cutting-edge data processing methods at their disposal that can meet (some of) these shortcomings, but which have not yet been fully explored. In this article, the evolutions in the use of geo-referenced imagery for the observation of neighborhood disorder from the last 25 years are described with a focus on the empirical opportunities, and the methodological challenges and prospects. We conclude by outlining the road ahead: promising avenues for future research to exploit the full potential of \u2018big primary data\u2019.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.324",
        "scimago_value": "1,549"
    },
    {
        "issnkey": "10848045",
        "isbn": null,
        "journal": "Journal of Network and Computer Applications",
        "publisher": null,
        "title": "theconvergenceofiotanddistributedledgertechnologiesdltopportunitieschallengesandsolutions",
        "booktitle": null,
        "doi": "10.1016/j.jnca.2020.102936",
        "author": [
            "Farahani, Bahar",
            "Firouzi, Farshad",
            "Luecking, Markus"
        ],
        "keywords": [
            "Internet of Things (IoT), Distributed Ledger Technology (DLT), Blockchain, Directed Acyclic Graph (DAG), Healthcare"
        ],
        "abstract": "Digital revolution is characterized by the convergence of technologies \u2014 from cloud computing to edge/fog computing, Artificial Intelligence (AI), big data, Intelligent Internet of Things (IoT), and Distributed Ledger Technology (DLT) \u2014 that is blurring the lines between physical and digital worlds. In this context, the IoT tsunami, the ubiquitous adoption of intelligent connected devices, and the public embracement of DLT, of which blockchain is a popular example, are increasingly becoming an integral feature of many modern systems, particularly, IoT-based smart and connected healthcare. Although the IoT and DLT/blockchain are two very different technologies and distinct from each other, the fusion of blockchain and IoT technologies is an unprecedented paradigm shift that is expected to disrupt both current and future systems in various fields. Blockchain has exactly what is needed to fix the weaknesses and vulnerabilities of IoT. It solves the security fault line among intelligent IoT where most of the IoT devices are connected to each other through the public trustless environment. Moreover, its distributed peer-to-peer nature can address the shortcomings of client/server models in Cloud-IoT solutions. Although the convergence of IoT and blockchain (Blockchain-IoT) can potentially tackle major shortcomings of today's solutions, its adoption is still in infancy, suffering from various issues and thus there is a necessity to address significant challenges including scalability, consensus algorithms, data privacy, efficiency, availability, storage, interoperability, and standardization, among others. In addition, there is no consensus towards any reference model or best practices that specify how blockchain should be utilized in IoT. This paper aims to present a holistic reference architecture as well as fundamentals, recent advancements, promises, and challenges in order to foster the investigations on cutting-edge research and allowing one to contribute to advancing the convergence of blockchain and IoT.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09242244",
        "isbn": null,
        "journal": "Trends in Food Science & Technology",
        "publisher": null,
        "title": "designingaresearchinfrastructurerionfoodbehaviourandhealthbalancinguserneedsbusinessmodelgovernancemechanismsandtechnology",
        "booktitle": null,
        "doi": "10.1016/j.tifs.2021.07.022",
        "author": [
            "Timotijevic, L.",
            "Astley, S.",
            "Bogaardt, M.J.",
            "Bucher, T.",
            "Carr, I.",
            "Copani, G.",
            "{de la Cueva}, J.",
            "Eftimov, T.",
            "Finglas, P.",
            "Hieke, S.",
            "Hodgkins, C.E.",
            "{Korou\u0161i\u0107 Seljak}, B.",
            "Klepacz, N.",
            "Pasch, K.",
            "Maringer, M.",
            "Mikkelsen, B.E.",
            "Normann, A.",
            "Ofei, K.T.",
            "Poppe, K.",
            "Pourabdollahian, G.",
            "Raats, M.M.",
            "Roe, M.",
            "Sadler, C.",
            "Selnes, T.",
            "{van der Veen}, H.",
            "{van\u2019t Veer}, P.",
            "Zimmermann, K."
        ],
        "keywords": [
            "Food nutrition, e-infrastructure, Data platform, Food consumption, Determinants, Food intake, AI, Big data, Omics, Data governance"
        ],
        "abstract": "Background A better understanding of food-related behaviour and its determinants can be achieved through harmonisation and linking of the various data-sources and knowledge platforms. Scope We describe the key decision-making in the development of a prototype of the Determinants and Intake Platform (DI Platform), a data platform that aims to harmonise and link data on consumer food behaviour. It will be part of the Food Nutrition Health Research Infrastructure (FNH-RI) that will facilitate health, social and food sciences. Approach The decision-making was based on the evidence of user needs and data characteristics that guided the specification of the key building blocks of the DI Platform. Eight studies were carried out, including consumer online survey; interview studies of key DI Platform stakeholders; desk research and workshops. Key findings Consumers were most willing to share data with universities, then industry and government. Trust, risk perception and altruism predicted willingness to share. For most other stakeholders non-proprietary data was most likely to be shared. Lack of data standards, and incentives for sharing were the main barriers for sharing data among the key stakeholders. The value of various data types would hugely increase if linked with other sources. Finding the right balance between optimizing data sharing and minimizing ethical and legal risks was considered a key challenge. Conclusions The development of DI Platform is based on careful balancing of the user, technical, business, legal and ethical requirements, following the FAIR principles and the need for financial sustainability, technical flexibility, transparency and multi-layered organisational governance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,676"
    },
    {
        "issnkey": "26667924",
        "isbn": null,
        "journal": "Advances in Applied Energy",
        "publisher": null,
        "title": "estimatesofdailygroundlevelno2concentrationsinchinabasedonrandomforestmodelintegratedkmeans",
        "booktitle": null,
        "doi": "10.1016/j.adapen.2021.100017",
        "author": [
            "Dou, Xinyu",
            "Liao, Cuijuan",
            "Wang, Hengqi",
            "Huang, Ying",
            "Tu, Ying",
            "Huang, Xiaomeng",
            "Peng, Yiran",
            "Zhu, Biqing",
            "Tan, Jianguang",
            "Deng, Zhu",
            "Wu, Nana",
            "Sun, Taochun",
            "Ke, Piyu",
            "Liu, Zhu"
        ],
        "keywords": [
            "Ground-level NO concentration, China, Random Forest model, Multi-source big data, Socio-economic parameters"
        ],
        "abstract": "Nitrogen dioxide (NO2) is one of the most important atmospheric pollutants and the precursors of acid rain, tropospheric ozone, and atmospheric aerosols. However, due to the poor quality of source data and the computing power of the models, current ground-level NO2 concentration data lack either high-resolution coverage or full nation-wide coverage. This study estimates the ground-level NO2 concentration in China with national coverage at relatively high spatiotemporal resolution (0.25\u00b0; daily intervals) over the newest past 6 years (2013\u20132018). We developed an advanced model, named Random Forest model integrated K-means (RF-K), for the estimates with multi-source parameters. Besides meteorological parameters, satellite retrievals parameters, and anthropogenic emission inventories parameters, we also innovatively introduce socioeconomic parameters to assess the impact of human activities. Our results show that: (1) the RF-K model developed by us shows better prediction performance than others. (2) the annual average NO2 concentration of China showed a weak declining trend (-0.013\u00b10.217 \u03bcgm\u22123yr\u22121) from 2013 to 2018, indicating that pollutant controlling targets had been achieved in China overall. By mapping daily nationwide ground-level NO2 concentrations, this study provides high-quality timely, and detailed data for air quality management and epidemiological analyses for China. The RF-K model can be used easily for other pollutants (e.g. SO2 and O3) considering that their ground-level concentrations can be estimated depending on the similar emitting sources and influence factors, and our model's input data sources also cover information on other pollutants.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15684946",
        "isbn": null,
        "journal": "Applied Soft Computing",
        "publisher": null,
        "title": "classificationutilityawaredatastreamanonymization",
        "booktitle": null,
        "doi": "10.1016/j.asoc.2021.107743",
        "author": [
            "Sopaoglu, Ugur",
            "Abul, Osman"
        ],
        "keywords": [
            "Data streams, Data anonymization, Data privacy, Classification"
        ],
        "abstract": "Data streams are continuous, infinite and ordered sequences of data. In comparison to static dataset anonymization, data stream anonymization confront with a number of constraints and difficulties due to the dynamic nature of data flow. The literature already addressed the k-anonymization of data streams which contain quasi-identifier attributes. However, today most data streams contain sensitive and classification target attributes as well. This work\u2019s main motivation is to develop a k-anonymization method for data streams which additionally protects the sensitivity and enables effective classification models. The k-anonymization, as a result, is formulated as a weighted multi-objective optimization problem. There are three objectives with respective weights as user parameters. A clustering based k-anonymization algorithm is developed as the solution. An extensive experimental evaluation on three real datasets shows the effectiveness of our proposal in various configurations. Moreover, the experimental results also confirm that our proposal attains better classification accuracies in comparison to popular data stream anonymization techniques.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,290"
    },
    {
        "issnkey": "25901982",
        "isbn": null,
        "journal": "Transportation Research Interdisciplinary Perspectives",
        "publisher": null,
        "title": "modellingtrafficduringlilacwildfireevacuationusingcellulardata",
        "booktitle": null,
        "doi": "10.1016/j.trip.2021.100335",
        "author": [
            "Melendez, Benjamin",
            "{Ghanipoor Machiani}, Sahar",
            "Nara, Atsushi"
        ],
        "keywords": [
            "Wildfire, Evacuation, Traffic, Cellular data"
        ],
        "abstract": "Southern California is prone to wildfire events that spark major evacuations of communities in the Wildland-Urban Interface. Highly developed regions such as Southern California have a number of transportation data sources to draw from that can support emergency managers\u2019 decision-making processes. Up to date traffic sensors such as those found on the majority of California\u2019s highways can inform emergency managers on current traffic densities, flows and speeds. Yet, in many wildfire prone regions of the United States, this is not the case. Despite this data shortfall, many regions do have robust cellular networks that inherently produce substantial amounts of location data. The location data produced by cellphone users can be used to predict vehicular densities on evacuation routes. This study examines how cellular data can be used to predict vehicular densities on evacuation routes. A mathematical model was developed to aid in the prediction of vehicular densities on evacuation networks. Correction factors were produced to adjust for the overestimation of users on roadways by cellular networks. Extrapolation factors were also developed for estimation of the number of cellular users based on a single cellphone counts data point. The Lilac Wildfire data in Dec 2017, was used to test and validate the developed model. This methodology may prove useful to transportation planners and emergency managers in planning evacuations in areas not served by a network of traffic sensors.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,383"
    },
    {
        "issnkey": "00457906",
        "isbn": null,
        "journal": "Computers & Electrical Engineering",
        "publisher": null,
        "title": "missingvalueimputationthroughshorterintervalselectiondrivenbyfuzzycmeansclustering",
        "booktitle": null,
        "doi": "10.1016/j.compeleceng.2021.107230",
        "author": [
            "Khan, Hufsa",
            "Wang, Xizhao",
            "Liu, Han"
        ],
        "keywords": [
            "Incomplete data processing, Missing value handling, Missing value imputation, Fuzzy C-Means clustering"
        ],
        "abstract": "The presence of missing data is a common and pivotal issue, which generally leads to a serious decrease of data quality and thus indicates the necessity to effectively handle missing data. In this paper, we propose a missing value imputation approach driven by Fuzzy C-Mean clustering to improve the classification accuracy by referring only to the known feature values of some selected instances. In particular, the missing values for each instance are imputed by selecting a shorter interval based on the cluster membership value within the certain threshold limit of each feature, while using a short interval is considered to improve the imputation effectiveness and get more accurate estimation of the values in comparison with using a long interval. Our method is evaluated through comparing with state-of-the-art imputation methods on UCI datasets. The experimental results demonstrate that the proposed approach performs closely to or better than those state-of-the-art imputation methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,630"
    },
    {
        "issnkey": "18711413",
        "isbn": null,
        "journal": "Livestock Science",
        "publisher": null,
        "title": "improvingfarmdecisionstheapplicationofdataengineeringtechniquestomanagedatastreamsfromcontemporarydairyoperations",
        "booktitle": null,
        "doi": "10.1016/j.livsci.2021.104602",
        "author": [
            "Wangen, Steven",
            "Zhang, Fan",
            "Fadul-Pacheco, Liliana",
            "{da Silva}, Tadeu",
            "Cabrera, Victor"
        ],
        "keywords": [
            "Dairy brain, Data integration, Agricultural data hub, Big data"
        ],
        "abstract": "Modern dairy farms generate vast amounts of data, with different sections of the operation having the ability to produce its own uniquely structured data stream depending on the specific hardware and software used. As a result of this heterogeneity, these streams are difficult to link to each other, thus it is rarely done. This creates an opportunity to add value to the data by integrating and homogenizing data from the different sources, with the end result of enriching analyses and helping to improve farm management decisions. Within a proposed project, a two component modular system is being developed. One component collects, cleans, and integrates data from on-farm systems into a centralized hub (AgDH). This system provides data to a framework designed to deploy and operationalize existing research-derived analytical tools and provide access to these tools and data via a user interface. The AgDH follows five steps to ingest different data streams available at the dairy farm: 1) transporting raw data into a centralized system; 2) decoding and storing data in a database; 3) cleaning data to ensure its validity; 4) homogenization of data by extracting the common features among the different software and farms; and 5) integration of data from the different systems. Each of these steps is crucial to make data available from different sources in a consistent manner, ease algorithmic development and its implementation, and facilitate the deployment of new tools that utilize the integrated data. In order to automate the process and make the data continuously available an open source workflow management platform was used. Both historical and current data can be made available to authenticated users via an application programming interface hosted through a web service. This framework needs to be designated to be flexible and able to adapt quickly to the changes and new technologies that are continuously being developed in the dairy industry. The integration and accessibility of data can facilitate a wide range of descriptive, predictive, and prescriptive analytics that can be developed and deployed directly on farms to increase animal performance, efficiency, health and welfare, profit margins, and decrease the environmental impact of dairy farming.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.943",
        "scimago_value": "0,622"
    },
    {
        "issnkey": "14670895",
        "isbn": null,
        "journal": "International Journal of Accounting Information Systems",
        "publisher": null,
        "title": "explainingthenonadoptionofadvanceddataanalyticsinauditingaprocesstheory",
        "booktitle": null,
        "doi": "10.1016/j.accinf.2021.100511",
        "author": [
            "Krieger, Felix",
            "Drews, Paul",
            "Velte, Patrick"
        ],
        "keywords": [
            "Audit digitization, Audit data analytics, Big data, Machine learning, Advanced data analytics in auditing, Audit innovation"
        ],
        "abstract": "Audit firms are increasingly engaging with advanced data analytics to improve the efficiency and effectiveness of external audits through the automation of audit work and obtaining a better understanding of the client\u2019s business risk and thus their own audit risk. This paper examines the process by which audit firms adopt advanced data analytics, which has been left unaddressed by previous research. We derive a process theory from expert interviews which describes the activities within the process and the organizational units involved. It further describes how the adoption process is affected by technological, organizational and environmental contextual factors. Our work contributes to the extent body of research on technology adoption in auditing by using a previously unused theoretical perspective, and contextualizing known factors of technology adoption. The findings presented in this paper emphasize the importance of technological capabilities of audit firms for the adoption of advanced data analytics; technological capabilities within audit teams can be leveraged to support both the ideation of possible use cases for advanced data analytics, as well as the diffusion of solutions into practice.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.400",
        "scimago_value": "0,897"
    },
    {
        "issnkey": "22131337",
        "isbn": null,
        "journal": "Astronomy and Computing",
        "publisher": null,
        "title": "exploringandinterrogatingastrophysicaldatainvirtualreality",
        "booktitle": null,
        "doi": "10.1016/j.ascom.2021.100502",
        "author": [
            "Jarrett, T.H.",
            "Comrie, A.",
            "Marchetti, L.",
            "Sivitilli, A.",
            "Macfarlane, S.",
            "Vitello, F.",
            "Becciani, U.",
            "Taylor, A.R.",
            "{van der Hulst}, J.M.",
            "Serra, P.",
            "Katz, N.",
            "Cluver, M.E."
        ],
        "keywords": [
            "Virtual reality, Data visualisation, Radio astrophysics, 3D catalogues, Volumetric rendering"
        ],
        "abstract": "Scientists across all disciplines increasingly rely on machine learning algorithms to analyse and sort datasets of ever increasing volume and complexity. Although trends and outliers are easily extracted, careful and close inspection will still be necessary to explore and disentangle detailed behaviour, as well as identify systematics and false positives. We must therefore incorporate new technologies to facilitate scientific analysis and exploration. Astrophysical data is inherently multi-parameter, with the spatial-kinematic dimensions at the core of observations and simulations. The arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as well as the availability of versatile development tools for video games, has enabled scientists to deploy such technology to effectively interrogate and interact with complex data. In this paper we present development and results from custom-built interactive VR tools, called the iDaVIE suite, that are informed and driven by research on galaxy evolution, cosmic large-scale structure, galaxy\u2013galaxy interactions, and gas/kinematics of nearby galaxies in survey and targeted observations. In the new era of Big Data ushered in by major facilities such as the SKA and LSST that render past analysis and refinement methods highly constrained, we believe that a paradigm shift to new software, technology and methods that exploit the power of visual perception, will play an increasingly important role in bridging the gap between statistical metrics and new discovery. We have released a beta version of the iDaVIE software system that is free and open to the community.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.927",
        "scimago_value": "0,692"
    },
    {
        "issnkey": "26667894",
        "isbn": null,
        "journal": "Cleaner Environmental Systems",
        "publisher": null,
        "title": "howcircularareplasticsintheeumfaofplasticsintheeuandpathwaystocircularity",
        "booktitle": null,
        "doi": "10.1016/j.cesys.2020.100004",
        "author": [
            "Hsu, Wan-Ting",
            "Domenech, Teresa",
            "McDowall, Will"
        ],
        "keywords": [
            "Plastic, Plastic waste, Circular economy, Material flow analysis, EU Plastics strategy, Resource management"
        ],
        "abstract": "Plastic is valued for its versatility, but concerns have been raised over the environmental impacts of mismanaged plastic waste. A better understanding of plastic flows can help to identify areas of inefficiency and potential leakage to natural systems. This research provides an overview of plastic flows in the EU and discusses options to increase plastic circularity. The study conducted a comprehensive stationary material flow analysis covering over 400 categories of plastic-containing products with detailed analysis of the final destination of waste. The results show the relevance of the EU plastic sector with production of over 66 MT of plastic polymers/fibres and an estimated consumption for plastic products of 73 MT in 2016. Plastic waste arisings amounted to over 37 MT and, though increasing plastic recycling rates have been reported, the analysis shows that a significant amount of plastic waste was not returned to production in the EU. The uncertainty analysis highlights important data quality issues that need to be addressed, particularly: data on the plastic fraction in plastic-containing products, and data on the final destination of plastic waste. Building on the analysis, the paper discusses a number of strategies for re-directing the plastic system to more circular pathways.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00313203",
        "isbn": null,
        "journal": "Pattern Recognition",
        "publisher": null,
        "title": "tacklingmodecollapseinmultigeneratorganswithorthogonalvectors",
        "booktitle": null,
        "doi": "10.1016/j.patcog.2020.107646",
        "author": [
            "Li, Wei",
            "Fan, Li",
            "Wang, Zhenyu",
            "Ma, Chao",
            "Cui, Xiaohui"
        ],
        "keywords": [
            "GANs, Mode collapse, Multiple generators, Orthogonal vectors, Minimax formula"
        ],
        "abstract": "Generative Adversarial Networks (GANs) have been widely used to generate realistic-looking instances. However, training robust GAN is a non-trivial task due to the problem of mode collapse. Although many GAN variants are proposed to overcome this problem, they have limitations. Those existing studies either generate identical instances or result in negative gradients during training. In this paper, we propose a new approach to training GAN to overcome mode collapse by employing a set of generators, an encoder and a discriminator. A new minimax formula is proposed to simultaneously train all components in a similar spirit to vanilla GAN. The orthogonal vector strategy is employed to guide multiple generators to learn different information in a complementary manner. In this way, we term our approach Multi-Generator Orthogonal GAN (MGO-GAN). Specifically, the synthetic data produced by those generators are fed into the encoder to obtain feature vectors. The orthogonal value is calculated between any two feature vectors, which loyally reflects the correlation between vectors. Such a correlation indicates how different information has been learnt by generators. The lower the orthogonal value is, the more different information the generators learn. We minimize the orthogonal value along with minimizing the generator loss through back-propagation in the training of GAN. The orthogonal value is integrated with the original generator loss to jointly update the corresponding generator\u2019s parameters. We conduct extensive experiments utilizing MNIST, CIFAR10 and CelebA datasets to demonstrate the significant performance improvement of MGO-GAN in terms of generated data quality and diversity at different resolutions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.740",
        "scimago_value": "1,492"
    },
    {
        "issnkey": "01679236",
        "isbn": null,
        "journal": "Decision Support Systems",
        "publisher": null,
        "title": "autoencodersforstrategicdecisionsupport",
        "booktitle": null,
        "doi": "10.1016/j.dss.2020.113422",
        "author": [
            "Verboven, Sam",
            "Berrevoets, Jeroen",
            "Wuytens, Chris",
            "Baesens, Bart",
            "Verbeke, Wouter"
        ],
        "keywords": [
            "Unsupervised learning, Strategic decision support, Outlier detection"
        ],
        "abstract": "In the majority of executive domains, a notion of normality is involved in most strategic decisions. However, few data-driven tools that support strategic decision-making are available. We introduce and extend the use of autoencoders to provide strategically relevant granular feedback. A first experiment indicates that experts are inconsistent in their decision making, highlighting the need for strategic decision support. Furthermore, using two large industry-provided human resources datasets, the proposed solution is evaluated in terms of ranking accuracy, synergy with human experts, and dimension-level feedback. This three-point scheme is validated using (a) synthetic data, (b) the perspective of data quality, (c) blind expert validation, and (d) transparent expert evaluation. Our study confirms several principal weaknesses of human decision-making and stresses the importance of synergy between a model and humans. Moreover, unsupervised learning and in particular the autoencoder are shown to be valuable tools for strategic decision-making.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.795",
        "scimago_value": "1,564"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "impactofdemandinformationsharingonorganicfarmingadoptionanevolutionarygameapproach",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.121001",
        "author": [
            "Yu, Yanan",
            "He, Yong",
            "Zhao, Xuan"
        ],
        "keywords": [
            "Organic food, Organic farming adoption, Sustainable agriculture supply chain, Information sharing, Stackelberg game, Evolutionary game theory"
        ],
        "abstract": "Consumer demand information on healthy food can be readily collected and shared through big data technologies. However, the impact of demand information sharing on the evolution of organic farming is still less understood. Consequently, we use the evolutionary game approach to study the long-term effect of information sharing on the producers\u2019 organic farming adoption based on the profit matrix. Considering that the retailer has some private demand information, the basic models as Stackelberg games in normal forms consisting of one retailer and one producer under information symmetry and information asymmetry are established to form the profit matrix. We also employ the method of cooperation on the forecast to realize information sharing and analyze the advantages and disadvantages of information sharing. Then, we analyze the evolutionarily stable strategy of several producers and retailers playing as a two-player game. The results indicate that (Conversion, Sharing) equilibrium can be easier to achieve with higher customers\u2019 green preference and lower effort cost, whereas (Conversion, Nonsharing) equilibrium can be easier to achieve with more accurate forecast information. More importantly, a higher initial conversion state prompts the retailers to evolve to sharing more quickly and vice versa.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "23521864",
        "isbn": null,
        "journal": "Environmental Technology & Innovation",
        "publisher": null,
        "title": "barrierstothedigitalisationandinnovationofaustraliansmartrealestateamanagerialperspectiveonthetechnologynonadoption",
        "booktitle": null,
        "doi": "10.1016/j.eti.2021.101527",
        "author": [
            "Ullah, Fahim",
            "Sepasgozar, Samad",
            "Thaheem, Muhammad",
            "Al-Turjman, Fadi"
        ],
        "keywords": [
            "Technology adoption barriers, Innovation, Non-adoption, Smart real estate, Disruptive digital technologies (DDTs), Managerial perspective"
        ],
        "abstract": "The real estate sector brings a fortune to the global economy. But, presently, this sector is regressive and uses traditional methods and approaches. Therefore, it needs a technological transformation and innovation in line with the Industry 4.0 requirements to transform into smart real estate. However, it faces the barriers of disruptive digital technology (DDT) adoption and innovation that need effective management to enable such transformation. These barriers present managerial challenges that affect DDT adoption and innovation in smart real estate. The current study assesses these DDTs adoption and innovation barriers facing the Australian real estate sector from a managerial perspective. Based on a comprehensive review of 72 systematically retrieved and shortlisted articles, we identify 21 key barriers to digitalisation and innovation. The barriers are grouped into the technology-organisation-external environment (TOE) categories using a Fault tree. Data is collected from 102 real estate and property managers to rate and rank the identified barriers. The results show that most of the respondents are aware of the DDTs and reported AI (22.5% of respondents), big data (12.75%) and VR (12.75%) as the most critical technologies not adopted so far due to costs, organisation policies, awareness, reluctance, user demand, tech integration, government support and funding. Overall, the highest barrier (risk) scores are observed for high costs of software and hardware (T1), high complexity of the selected technology dissemination system (T2) and lack of government incentives, R&D support, policies, regulations and standards (E1). Among the TOE categories, as evident from the fault tree analysis, the highest percentage of failure to adopt the DDT is attributed to E1 in the environmental group. For the technological group, the highest failure reason is attributed to T2. And for the organisational group, the barrier with the highest failure chances for DDT adoption is the lack of organisational willingness to invest in digital marketing (O4). These barriers must be addressed to pave the way for DDT adoption and innovation in the Australian real estate sector and move towards smart real estate.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,866"
    },
    {
        "issnkey": "15708705",
        "isbn": null,
        "journal": "Ad Hoc Networks",
        "publisher": null,
        "title": "differentiallyprivatedoubleauctionwithreliabilityawareinmobilecrowdsensing",
        "booktitle": null,
        "doi": "10.1016/j.adhoc.2021.102450",
        "author": [
            "Ni, Tianjiao",
            "Chen, Zhili",
            "Xu, Gang",
            "Zhang, Shun",
            "Zhong, Hong"
        ],
        "keywords": [
            "Mobile crowd sensing, Differential privacy, Double auction, Aggregation, Reliability"
        ],
        "abstract": "With the unprecedented proliferation of mobile devices, Mobile Crowd Sensing (MCS) emerges as a promising computing paradigm which utilizes sensor-embedded smart devices to collect sensory data. Recently, a number of privacy-preserving auction-based incentive mechanisms have been proposed. However, none of them guarantees the quality of sensing data in double-side auction scenarios. In this paper, we propose a Differentially Private Double Auction With Reliability-Aware in Mobile Crowd Sensing (DPDR). Specifically, we design the incentive mechanism by employing the exponential mechanism in double-side auction to select the clearing price tuple. Moreover, to collect precise sensory data, we heuristically choose more reliable workers as candidates for each clearing price tuple. We further improve the social welfare of the mechanism by designing the utility function with less sensitivity, or adopting a more practical pricing strategy. Through theoretical analysis, we demonstrate that our mechanisms can guarantee both differential privacy and economic properties, including individual rationality, budget balance, approximate truthfulness and approximate maximal social welfare. Extensive experimental results show that the improved mechanisms can achieve better performance than DPDR in term of social welfare, and all proposed mechanisms can produce high-quality data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.111",
        "scimago_value": "0,781"
    },
    {
        "issnkey": "03608352",
        "isbn": null,
        "journal": "Computers & Industrial Engineering",
        "publisher": null,
        "title": "leadmanagementoptimizationusingdataminingacaseinthetelecommunicationssector",
        "booktitle": null,
        "doi": "10.1016/j.cie.2021.107122",
        "author": [
            "Espadinha-Cruz, P.",
            "Fernandes, A.",
            "Grilo, A."
        ],
        "keywords": [
            "Lead Management, Data Mining, Predictive Models, Machine Learning, Telecommunications"
        ],
        "abstract": "The growing competitiveness of the market has put pressure on companies to improve their customer relationship management strategies. In an era where mass marketing techniques are inadequate, lead management is at the forefront to provide a customized approach to customer acquisition. For this, lead management depends on the correct selection of leads and decision making on what type of approach to take to satisfy the requirements of customers. However, currently, firms are faced with massive quantity of data regarding customers and prospects. Data mining is a solution to cope with this problem, providing a robust approach to massive quantity of data and its complexity. In literature, there is a lack of documented applications of these techniques in lead management. In this sense, we propose a methodology that aims to improve efficiency on the distinct maturity stages of leads management. Also, the methodology aids in support the decision-making regarding the segmentation of leads. This research suggests the application of data mining techniques in the optimization of leads management processes, from capture to conversion, with the objective of improving customer conversion effectiveness. A case study was conducted in a telecommunications company. It was possible to implement the proposed method to estimate the probability of conversion for each lead. With this, was possible to segment the offer to each type of lead.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,315"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "digitalshopfloormanagementenhancedbynaturallanguageprocessing",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.01.046",
        "author": [
            "M\u00fcller, Marvin",
            "Alexandi, Emanuel",
            "Metternich, Joachim"
        ],
        "keywords": [
            "digital shop floor management, natural language processing, document clustering"
        ],
        "abstract": "This paper aims to develop concepts how digital shop floor management (dSFM) can be further enhanced by natural language processing (NLP) to bring a higher value to the shop floor team and decision makers. Based on the literature review on these two fields several valuable application of NLP in dSFM are theorized: recommender engines to improve knowledge management, text clustering to identify frequent problems, voice assistants to ease the interaction with the data base, chat log extraction to fill the database with unstructured written text from chats and spellcheck as well as auto fill to improve data quality. To show the feasibility for NLP in dSFM in industry, a case study for the document clustering is presented: A digital ticket system for shop floor issues used for two years and containing 2,735 entries is analysed with the \u201cGraph\u201d-feature from Elasticsearch to find the most frequent terms and intersections in the described problems. The approach is accurate, quick and detailed and will be established in the company and performed monthly.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "03608352",
        "isbn": null,
        "journal": "Computers & Industrial Engineering",
        "publisher": null,
        "title": "industry35toempowersmartproductionforpoultryfarmingandanempiricalstudyforbroilerliveweightprediction",
        "booktitle": null,
        "doi": "10.1016/j.cie.2020.106931",
        "author": [
            "Wang, Chun-Yao",
            "Chen, Ying-Jen",
            "Chien, Chen-Fu"
        ],
        "keywords": [
            "Industry 3.5, Smart production, Poultry farming, Data-driven approach, Broiler weight estimation, Weight prediction"
        ],
        "abstract": "Emerging countries and traditional industries may not be ready for direct migration of Industry 4.0. In particular, the broiler is a major source for meat, while poultry farming in emerging countries is mainly by small and medium-sized enterprises (SMEs) or family businesses. The live broilers need to maintain the desired specification for food processing while optimizing the feeding conversion rate for revenue management. Conventionally, broiler growth monitoring and prediction rely on farmers\u2019 experience and a small amount of data by unrigorous sampling. Although the automatic weighing system (AWS) has gradually been employed to replace manual weighing to reduce cost and casualties, existing automatic weighing systems have limited capability for weight monitoring and weight prediction of broiler future growth. To fill the gaps, this study aims to employ Industry 3.5 as a hybrid and develop a data-driven framework for weight monitoring and prediction to support smart production for poultry farming for revenue optimization. The proposed framework contains two modules. The weight monitoring integrates Gaussian Mixture Model, bootstrapping resampling, and weighted mean technique to estimate the current weight of live broilers in the farm via big data including electronic signals collected from the farms via multiple sensors and devices. The weight prediction module employs mathematical growth functions as a basis and daily feedback for adjustment to provide real-time weight prediction to support related decisions for smart production. An empirical study was conducted in Taiwan. The results have shown the practical viability of this approach. The developed solution is implemented in the broiler industry.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,315"
    },
    {
        "issnkey": "13538292",
        "isbn": null,
        "journal": "Health & Place",
        "publisher": null,
        "title": "understandingtheinteractionbetweenhumanactivitiesandphysicalhealthunderextremeheatenvironmentinphoenixarizona",
        "booktitle": null,
        "doi": "10.1016/j.healthplace.2021.102691",
        "author": [
            "Zhao, Qunshan",
            "Li, Ziqi",
            "Shah, Dhrumil",
            "Fischer, Heather",
            "Sol\u00eds, Patricia",
            "Wentz, Elizabeth"
        ],
        "keywords": [
            ", Citizen science, Portable sensing, Urban heat, Community resilience"
        ],
        "abstract": "Long-term community resilience, which privileges a long view look at chronic issues influencing communities, has begun to draw more attention from city planners, researchers and policymakers. In Phoenix, resilience to heat is both a necessity and a way of life. In this paper, we attempt to understand how residents living in Phoenix experience and behave in an extreme heat environment. To achieve this goal, we introduced a smartphone application (ActivityLog) to study spatio-temporal dynamics of human interaction with urban environments. Compared with traditional paper activity log results we have in this study, the smartphone-based activity log has higher data quality in terms of total number of logs, response rates, accuracy, and connection with GPS and temperature sensors. The research results show that low-income residents in Phoenix mostly stay home during the summer but experience a relatively high indoor temperature due to the lack/low efficiency of air-conditioning (AC) equipment or lack of funds to run AC frequently. Middle-class residents have a better living experience in Phoenix with better mobility with automobiles and good quality of AC. The research results help us better understand user behaviors for daily log activities and how human activities interact with the urban thermal environment, informing further planning policy development. The ActivityLog smartphone application is also presented as an open-source prototype to design a similar urban climate citizen science program in the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,341"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-819246-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter14edgealgorithmsforwearablesanoverviewofatrulymultidisciplinaryproblem",
        "booktitle": "Wearable Sensors (Second Edition)",
        "doi": "10.1016/B978-0-12-819246-7.00014-0",
        "author": [
            "Beach, Christopher",
            "Balaban, Ertan",
            "Casson, Alexander"
        ],
        "keywords": [
            "Accurate performance testing, Algorithm design, Battery selection, Circuit design, Design trends, Embedded signal processing, Low-power consumption, Real-time signal processing, Sensor node optimization, Wearable devices"
        ],
        "abstract": "This chapter introduces and comprehensively overviews emerging edge algorithms for wearable devices \u2013 signal processing algorithms which are embedded into the wearable hardware itself. We begin by overviewing some of the potential benefits of including low-power real-time signal processing in wearable sensors, with benefits of both increasing the operational lifetime and increasing the device functionality. Results from a practical state-of-the-art sensor platform demonstrate and quantify the design trade-offs present and the potential system optimizations available. We then consider the theory behind wearable algorithms and highlight the key properties of power-lifetime trade-off, Big Data performance testing, and performance-power trade-off that differentiate these new algorithms from conventional signal processing approaches. We conclude by investigating different implementation techniques and the different approaches available to minimize power consumption and also to maintain design flexibility and speed.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15320464",
        "isbn": null,
        "journal": "Journal of Biomedical Informatics",
        "publisher": null,
        "title": "searchandvisualizationofgenedrugdiseaseinteractionsforpharmacogenomicsandprecisionmedicineresearchusinggenedive",
        "booktitle": null,
        "doi": "10.1016/j.jbi.2021.103732",
        "author": [
            "Wong, Mike",
            "Previde, Paul",
            "Cole, Jack",
            "Thomas, Brook",
            "Laxmeshwar, Nayana",
            "Mallory, Emily",
            "Lever, Jake",
            "Petkovic, Dragutin",
            "Altman, Russ",
            "Kulkarni, Anagha"
        ],
        "keywords": [
            "Gene interactions, Retrieval and visualization, Gene sets, Gene-disease and gene-drug relationships, Biomedical information retrieval"
        ],
        "abstract": "Background Understanding the relationships between genes, drugs, and disease states is at the core of pharmacogenomics. Two leading approaches for identifying these relationships in medical literature are: human expert led manual curation efforts, and modern data mining based automated approaches. The former generates small amounts of high-quality data, and the latter offers large volumes of mixed quality data. The algorithmically extracted relationships are often accompanied by supporting evidence, such as, confidence scores, source articles, and surrounding contexts (excerpts) from the articles, that can be used as data quality indicators. Tools that can leverage these quality indicators to help the user gain access to larger and high-quality data are needed. Approach We introduce GeneDive, a web application for pharmacogenomics researchers and precision medicine practitioners that makes gene, disease, and drug interactions data easily accessible and usable. GeneDive is designed to meet three key objectives: (1) provide functionality to manage information-overload problem and facilitate easy assimilation of supporting evidence, (2) support longitudinal and exploratory research investigations, and (3) offer integration of user-provided interactions data without requiring data sharing. Results GeneDive offers multiple search modalities, visualizations, and other features that guide the user efficiently to the information of their interest. To facilitate exploratory research, GeneDive makes the supporting evidence and context for each interaction readily available and allows the data quality threshold to be controlled by the user as per their risk tolerance level. The interactive search-visualization loop enables relationship discoveries between diseases, genes, and drugs that might not be explicitly described in literature but are emergent from the source medical corpus and deductive reasoning. The ability to utilize user\u2019s data either in combination with the GeneDive native datasets or in isolation promotes richer data-driven exploration and discovery. These functionalities along with GeneDive\u2019s applicability for precision medicine, bringing the knowledge contained in biomedical literature to bear on particular clinical situations and improving patient care, are illustrated through detailed use cases. Conclusion GeneDive is a comprehensive, broad-use biological interactions browser. The GeneDive application and information about its underlying system architecture are available at http://www.genedive.net. GeneDive Docker image is also available for download at this URL, allowing users to (1) import their own interaction data securely and privately; and (2) generate and test hypotheses across their own and other datasets.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10462023",
        "isbn": null,
        "journal": "Methods",
        "publisher": null,
        "title": "automationofdataanalysisinmolecularcancerimaginganditspotentialimpactonfutureclinicalpractice",
        "booktitle": null,
        "doi": "10.1016/j.ymeth.2020.06.019",
        "author": [
            "Theek, Benjamin",
            "Magnuska, Zuzanna",
            "Gremse, Felix",
            "Hahn, Horst",
            "Schulz, Volkmar",
            "Kiessling, Fabian"
        ],
        "keywords": [
            "Molecular imaging, Radiomics, Big data, Image analysis, Artificial intelligence, Cancer"
        ],
        "abstract": "Digitalization, especially the use of machine learning and computational intelligence, is considered to dramatically shape medical procedures in the near future. In the field of cancer diagnostics, radiomics, the extraction of multiple quantitative image features and their clustered analysis, is gaining increasing attention to obtain more detailed, reproducible, and meaningful information about the disease entity, its prognosis and the ideal therapeutic option. In this context, automation of diagnostic procedures can improve the entire pipeline, which comprises patient registration, planning and performing an imaging examination at the scanner, image reconstruction, image analysis, and feeding the diagnostic information from various sources into decision support systems. With a focus on cancer diagnostics, this review article reports and discusses how computer-assistance can be integrated into diagnostic procedures and which benefits and challenges arise from it. Besides a strong view on classical imaging modalities like x-ray, CT, MRI, ultrasound, PET, SPECT and hybrid imaging devices thereof, it is outlined how imaging data can be combined with data deriving from patient anamnesis, clinical chemistry, pathology, and different omics. In this context, the article also discusses IT infrastructures that are required to realize this integration in the clinical routine. Although there are still many challenges to comprehensively implement automated and integrated data analysis in molecular cancer imaging, the authors conclude that we are entering a new era of medical diagnostics and precision medicine.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.608",
        "scimago_value": "2,080"
    },
    {
        "issnkey": "11201797",
        "isbn": null,
        "journal": "Physica Medica",
        "publisher": null,
        "title": "enhancingtheimpactofartificialintelligenceinmedicineajointaifminfnitalianinitiativeforadedicatedcloudbasedcomputinginfrastructure",
        "booktitle": null,
        "doi": "10.1016/j.ejmp.2021.10.005",
        "author": [
            "Retico, Alessandra",
            "Avanzo, Michele",
            "Boccali, Tommaso",
            "Bonacorsi, Daniele",
            "Botta, Francesca",
            "Cuttone, Giacomo",
            "Martelli, Barbara",
            "Salomoni, Davide",
            "Spiga, Daniele",
            "Trianni, Annalisa",
            "Stasi, Michele",
            "Iori, Mauro",
            "Talamonti, Cinzia"
        ],
        "keywords": [
            "Artificial intelligence, Decision support systems, Computing infrastructure, Distributed learning"
        ],
        "abstract": "Artificial Intelligence (AI) techniques have been implemented in the field of Medical Imaging for more than forty years. Medical Physicists, Clinicians and Computer Scientists have been collaborating since the beginning to realize software solutions to enhance the informative content of medical images, including AI-based support systems for image interpretation. Despite the recent massive progress in this field due to the current emphasis on Radiomics, Machine Learning and Deep Learning, there are still some barriers to overcome before these tools are fully integrated into the clinical workflows to finally enable a precision medicine approach to patients\u2019 care. Nowadays, as Medical Imaging has entered the Big Data era, innovative solutions to efficiently deal with huge amounts of data and to exploit large and distributed computing resources are urgently needed. In the framework of a collaboration agreement between the Italian Association of Medical Physicists (AIFM) and the National Institute for Nuclear Physics (INFN), we propose a model of an intensive computing infrastructure, especially suited for training AI models, equipped with secure storage systems, compliant with data protection regulation, which will accelerate the development and extensive validation of AI-based solutions in the Medical Imaging field of research. This solution can be developed and made operational by Physicists and Computer Scientists working on complementary fields of research in Physics, such as High Energy Physics and Medical Physics, who have all the necessary skills to tailor the AI-technology to the needs of the Medical Imaging community and to shorten the pathway towards the clinical applicability of AI-based decision support systems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,883"
    },
    {
        "issnkey": "25424351",
        "isbn": null,
        "journal": "Joule",
        "publisher": null,
        "title": "predictingbatteryendoflifefromsolaroffgridsystemfielddatausingmachinelearning",
        "booktitle": null,
        "doi": "10.1016/j.joule.2021.11.006",
        "author": [
            "Aitio, Antti",
            "Howey, David"
        ],
        "keywords": [
            "battery, health, machine learning, rural electrification, Gaussian process, classification, Kalman filter"
        ],
        "abstract": "Summary Hundreds of millions of people lack access to electricity. Decentralized solar-battery systems are key for addressing this while avoiding carbon emissions and air pollution but are hindered by relatively high costs and rural locations that inhibit timely preventive maintenance. Accurate diagnosis of battery health and prediction of end of life from operational data improves user experience and reduces costs. However, lack of controlled validation tests and variable data quality mean existing lab-based techniques fail to work. We apply a scalable probabilistic machine learning approach to diagnose health in 1,027 solar-connected lead-acid batteries, each running for 400\u2013760 days, totaling 620 million data rows. We demonstrate 73% accurate prediction of end of life, 8 weeks in advance, rising to 82% at the point of failure. This work highlights the opportunity to estimate health from existing measurements using \u201cbig data\u201d techniques, without additional equipment, extending lifetime and improving performance in real-world applications.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "41.248",
        "scimago_value": "12,532"
    },
    {
        "issnkey": "",
        "isbn": "978-0-323-89777-8",
        "journal": null,
        "publisher": "Academic Press",
        "title": "5adatascienceperspectiveofrealworldcovid19databases",
        "booktitle": "Leveraging Artificial Intelligence in Global Epidemics",
        "doi": "10.1016/B978-0-323-89777-8.00008-7",
        "author": [
            "Prasanna, Shivika",
            "Rao, Praveen"
        ],
        "keywords": [
            "COVID-19, apache spark, data science, machine learning, data visualization"
        ],
        "abstract": "The COVID-19 pandemic has devastated the lives of millions of people worldwide and damaged the economy of many countries. While the negative impact of the pandemic on mankind is unimaginable, this pandemic has triggered new research and innovation in the use of artificial intelligence for developing solutions to better understand and mitigate the pandemic. Several valuable datasets have been made available by different organizations and research groups. In this chapter, we provide an overview of real-world COVID-19 data sources available for developing novel applications and solutions for the pandemic. We provide a comparison between them from a data science perspective. Next, we delve deep into the Cerner Real-World Data for COVID-19. We discuss the schema of the database, data quality issues, data wrangling using Apache Spark, and data analysis using popular machine learning techniques. Specifically, we provide examples of querying the database, training machine learning models, and visualization. We also discuss the technical challenges that we encountered and how we overcame them to complete multiple clinical studies on COVID-19.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818015-0",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter6othertechniquesandtools",
        "booktitle": "Executing Data Quality Projects (Second Edition)",
        "doi": "10.1016/B978-0-12-818015-0.00003-7",
        "author": [
            "McGilvray, Danette"
        ],
        "keywords": [
            "Tracking issues, tracking action items, data capture, assessment plan, information life cycle, flowchart, IP Map, survey, analyze results, synthesize, documentation, metrics, Six Sigma, ISO, data quality tools, project management."
        ],
        "abstract": "This chapter outlines techniques, with many templates and examples, that can be applied in several places throughout the Ten Steps Process. Techniques include various approaches to visualizing an information life cycle, conducting surveys, developing data capture and assessment plans, and implementing metrics. The technique for tracking issues and action items helps with project management and another shows how the Ten Steps Process can be used with Six Sigma and ISO Standards. Every step, assessment and activity should make use of the technique Analyze, Synthesize, Recommend, Document, and Act on Results. Even though the methodology does not require specific tools, typical tool functionality is described which can be used when considering tools to be used in a data quality project. The techniques should be adjusted and applied to fit what is needed for the particular circumstances and step where they are used.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09265805",
        "isbn": null,
        "journal": "Automation in Construction",
        "publisher": null,
        "title": "iotforpredictiveassetsmonitoringandmaintenanceanimplementationstrategyfortheukrailindustry",
        "booktitle": null,
        "doi": "10.1016/j.autcon.2020.103486",
        "author": [
            "Gbadamosi, Abdul-Quayyum",
            "Oyedele, Lukumon",
            "Delgado, Juan",
            "Kusimo, Habeeb",
            "Akanbi, Lukman",
            "Olawale, Oladimeji",
            "Muhammed-yakubu, Naimah"
        ],
        "keywords": [
            "Internet of things, Predictive maintenance, Remote inspection, Rail assets, Augmented reality"
        ],
        "abstract": "With about 100% increase in rail service usage over the last 20 years, it is pertinent that rail infrastructure continues to function at an optimal level to avoid service disruptions, cancellations or delays due to unforeseen asset breakdown. In an endeavour to propose a strategy for the implementation of Internet of Things (IoT) in rail asset maintenance, a qualitative methodology was adopted through a series of focus-group workshops to identify the priority areas and enabling digital technologies for IoT implementation. The methods of data collection included audio recording, note-taking, and concept mapping. The audio records were transcribed and used for thematic analysis, while the concept maps were integrated for conceptual modelling and analysis. This paper presents an implementation strategy for IoT for rail assets maintenance with focus on priority areas such as real-time condition monitoring using IoT sensors, predictive maintenance, remote inspection, and integrated asset data management platform.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.700",
        "scimago_value": "1,837"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "onlineaccuratestateofhealthestimationforbatterysystemsonrealworldelectricvehicleswithvariabledrivingconditionsconsidered",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.125814",
        "author": [
            "Hong, Jichao",
            "Wang, Zhenpo",
            "Chen, Wen",
            "Wang, Leyi",
            "Lin, Peng",
            "Qu, Changhui"
        ],
        "keywords": [
            "State of health, Battery systems, Variable-length-input, Long short-term memory, Driving behavior"
        ],
        "abstract": "The environmental sustainability stimulates the development of electric vehicles with great energy-saving and emission reduction effects. State of health of the battery system in an electric vehicle is crucial to the safety of vehicle operation, charging station, and the environment. The existing techniques implemented in well-controlled experimental environments fail to learn unpredictable drivers\u2019 driving behaviors and complex road/weather conditions during actual vehicular operation. This paper investigates a novel deep-learning-enabled method to perform accurate state of health estimation for battery systems on real-world electric vehicles. Eight potential evaluation schemes depending on the stable charging stages are recapped and discussed. By fitting the correlation between battery degeneration factors and various vehicle operation parameters such as ambient temperature and mileage, an approximate battery degeneration model oriented for the real application scenarios is obtained. The variable-length-input long short-term memory network is used to learn the variable battery degeneration factors acquired from different driving stages of a yearlong dataset. The test results show that the proposed method has a better performance than other estimation methods. More significantly, based on the acquisition advantages of big-data platforms, it can be used to full-state and full-climate vehicle applications unrestricted by complex actual environments.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "0740624x",
        "isbn": null,
        "journal": "Government Information Quarterly",
        "publisher": null,
        "title": "designprinciplesforcreatingdigitaltransparencyingovernment",
        "booktitle": null,
        "doi": "10.1016/j.giq.2020.101550",
        "author": [
            "Matheus, Ricardo",
            "Janssen, Marijn",
            "Janowski, Tomasz"
        ],
        "keywords": [
            "Transparency, Digital transparency, Transparency-by-design, Open data, Open government, Design principles, Window theory"
        ],
        "abstract": "Under pressure to fight corruption, hold public officials accountable, and build trust with citizens, many governments pursue the quest for greater transparency. They publish data about their internal operations, externalize decision-making processes, establish digital inquiry lines to public officials, and employ other forms of transparency using digital means. Despite the presence of many transparency-enhancing digital tools, putting such tools together to achieve the desired level of digital transparency, to design entire government systems for digital transparency, remains challenging. Design principles and other design guides are lacking in this area. This article aims to fill this gap. We identify a set of barriers to digital transparency in government, define 16 design principles to overcome such barriers, and evaluate these principles using three case studies from different countries. Some principles apply to projects, others to systems, yet others to entire organizations. To achieve digital transparency, before building and deploying digital solutions, government organizations should build technological and institutional foundations and use such foundations to organize themselves for transparency. The proposed design principles can help develop and apply such foundations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03608352",
        "isbn": null,
        "journal": "Computers & Industrial Engineering",
        "publisher": null,
        "title": "multistagemrcartmultiresponseoptimizationinamultistageprocessusingaclassificationandregressiontreemethod",
        "booktitle": null,
        "doi": "10.1016/j.cie.2021.107513",
        "author": [
            "Lee, Dong-Hee",
            "Kim, So-Hee",
            "Kim, Kwang-Jae"
        ],
        "keywords": [
            "Multistage process, Multiresponse optimization, Desirability function, Big data, Data mining, Classification and regression tree"
        ],
        "abstract": "A multistage process consists of sequential consecutive stages. In this process, each stage has multiple responses and is affected by its preceding stage, while at the same time, affecting the following stage. This complex structure makes it difficult to optimize the multistage process. Recently, it became easy to obtain a large amount of operational data from the multistage process due to development of information technologies. The proposed method employs a data mining method called a classification and regression tree for analyzing the data and desirability functions for simultaneously optimizing the multiresponse. To consider the relationship between stages, a backward optimization procedure which treats the multiresponse of the preceding stage as the input variables is proposed. The proposed method is described using a steel manufacturing process example and is compared with existing multiresponse optimization methods. The case study shows that the proposed method works well and outperforms the existing methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,315"
    },
    {
        "issnkey": "00489697",
        "isbn": null,
        "journal": "Science of The Total Environment",
        "publisher": null,
        "title": "gchrmswithcomplementaryionizationtechniquesfortargetandnontargetscreeningforchemicalexposureexpandingtheinsightsoftheairpollutionmarkersinmoscowsnow",
        "booktitle": null,
        "doi": "10.1016/j.scitotenv.2020.144506",
        "author": [
            "Mazur, D.M.",
            "Detenchuk, E.A.",
            "Sosnova, A.A.",
            "Artaev, V.B.",
            "Lebedev, A.T."
        ],
        "keywords": [
            "GC-HRMS, GC-HR-TOFMS, Electron capture negative ionization, Air pollution, Priority pollutants, Persistent organic pollutants, Emerging contaminants, Moscow snow pollution, Big data analysis"
        ],
        "abstract": "Environmental exposure assessment is an important step in establishing a list of local priority pollutants and finding the sources of the threats for proposing appropriate protection measures. Exposome targeted and non-targeted analysis as well as suspect screening may be applied to reveal these pollutants. The non-targeted screening is a challenging task and requires the application of the most powerful analytical tools available, assuring wide analytical coverage, sensitivity, identification reliability, and quantitation. Moscow, Russia, is the largest and most rapidly growing European city. That rapid growth is causing changes in the environment which require periodic clarification of the real environmental situation regarding the presence of the classic pollutants and possible new contaminants. Gas chromatography \u2013 high resolution time-of-flight mass spectrometry (GC-HR-TOFMS) with electron ionization (EI), positive chemical ionization (PCI), and electron capture negative ionization (ECNI) ion sources were used for the analysis of Moscow snow samples collected in the early spring of 2018 in nine different locations. Collection of snow samples represents an efficient approach for the estimation of long-term air pollution, due to accumulation and preservation of environmental contaminants by snow during winter period. The high separation power of GC, complementary ionization methods, high mass accuracy, and wide mass range of TOFMS allowed for the identification of several hundred organic compounds belonging to the various classes of pollutants, exposure to which could represent a danger to the health of the population. Although quantitative analysis was not a primary aim of the study, targeted analysis revealed that some priority pollutants exceeded the established safe levels. Thus, dibutylphthalate concentration was over 10-fold higher than its safe level (0.001 mg/L), while benz[a]pyrene concentration exceeded Russian maximal permissible concentration value of 5 ng/L in three samples. The large amount of information generated during the combination of targeted and non-targeted analysis and screening samples for suspects makes it feasible to apply the big data analysis to observe the trends and tendencies in the pollution exposome across the city.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.963",
        "scimago_value": "1,795"
    },
    {
        "issnkey": "09500618",
        "isbn": null,
        "journal": "Construction and Building Materials",
        "publisher": null,
        "title": "amethodologicalapproachforstructuralhealthmonitoringofmasstimberbuildingsunderconstruction",
        "booktitle": null,
        "doi": "10.1016/j.conbuildmat.2020.121153",
        "author": [
            "Baas, Esther",
            "Riggio, Mariapaola",
            "Barbosa, Andr\u00e9"
        ],
        "keywords": [
            "Big data, Cross-laminated timber, Construction monitoring, Mass plywood panel, Mass-timber, Self-centering rocking wall, Structural health monitoring"
        ],
        "abstract": "Structural health monitoring (SHM) is a method used to evaluate the performance of new structural systems and critical infrastructure. With mass-timber building construction on the rise, SHM programs have emerged to document hygrothermal, static, and dynamic behavior of these structures. To most efficiently document behavior and provide recommendations to industry, it is key that the research community work collaboratively to create consistent data by using standardized approaches. This paper presents a methodological approach for monitoring mass-timber buildings during construction to address this need. The approach was validated over ten months with a mass-timber building under construction at Oregon State University.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.141",
        "scimago_value": "1,662"
    },
    {
        "issnkey": "23524316",
        "isbn": null,
        "journal": "Extreme Mechanics Letters",
        "publisher": null,
        "title": "deeplearningassistedelasticisotropyidentificationforarchitectedmaterials",
        "booktitle": null,
        "doi": "10.1016/j.eml.2021.101173",
        "author": [
            "Wei, Anran",
            "Xiong, Jie",
            "Yang, Weidong",
            "Guo, Fenglin"
        ],
        "keywords": [
            "Deep learning, Convolutional neural network, Architected material, Elastic isotropy, Rapid mechanical characterization, Transfer learning"
        ],
        "abstract": "Architected materials consisting of periodic unit cells are desirable for many engineering applications. Characterizing the elastic isotropy is of great significance for the mechanical design of architected materials. However, prevailing experimental and numerical approaches are normally too costly and time-consuming to screen out isotropic architected materials in the large design space. Here, a deep learning-based approach is developed as a highly efficient and portable tool to identify the elastic isotropy of architected materials directly from images of their unit cells with arbitrary component distributions. The measure of elastic isotropy for heterogeneous architected materials is derived firstly in this paper to construct a database with associated images of unit cells. Then a convolutional neural network is fully trained with the database, performing well on the isotropy identification with about 90% accuracy and milliseconds processing time per sample. Meanwhile, it exhibits enough robustness to maintain its performance under the fluctuating material properties in test sets. Moreover, the transfer learning of the convolutional neural network is successfully implemented among architected materials with different numbers of material components, which further promotes the efficiency of the deep learning-based approach without scarifying its identification performance. This study gives new inspirations on the rapid mechanical characterization of architectured materials, which holds promising applications in the big-data driven topological design and nondestructive testing of architected materials.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.567",
        "scimago_value": "1,524"
    },
    {
        "issnkey": "26665468",
        "isbn": null,
        "journal": "Energy and AI",
        "publisher": null,
        "title": "predictionofvoltagedistributionusingdeeplearningandidentifiedkeysmartmeterlocations",
        "booktitle": null,
        "doi": "10.1016/j.egyai.2021.100103",
        "author": [
            "Mokhtar, Maizura",
            "Robu, Valentin",
            "Flynn, David",
            "Higgins, Ciaran",
            "Whyte, Jim",
            "Loughran, Caroline",
            "Fulton, Fiona"
        ],
        "keywords": [
            "Voltage prediction, Smart meters, Deep neural learning, Distribution network operation, Big Data Analytics, Analytic methods in power networks, Privacy-preserving data analysis"
        ],
        "abstract": "The energy landscape for the Low-Voltage (LV) networks is undergoing rapid changes. These changes are driven by the increased penetration of distributed Low Carbon Technologies, both on the generation side (i.e. adoption of micro-renewables) and demand side (i.e. electric vehicle charging). The previously passive \u2018fit-and-forget\u2019 approach to LV network management is becoming increasing inefficient to ensure its effective operation. A more agile approach to operation and planning is needed, that includes pro-active prediction and mitigation of risks to local sub-networks (such as risk of voltage deviations out of legal limits). The mass rollout of smart meters (SMs) and advances in metering infrastructure holds the promise for smarter network management. However, many of the proposed methods require full observability, yet the expectation of being able to collect complete, error free data from every smart meter is unrealistic in operational reality. Furthermore, the smart meter (SM) roll-out has encountered significant issues, with the current voluntary nature of installation in the UK and in many other countries resulting in low-likelihood of full SM coverage for all LV networks. Even with a comprehensive SM roll-out privacy restrictions, constrain data availability from meters. To address these issues, this paper proposes the use of a Deep Learning Neural Network architecture to predict the voltage distribution with partial SM coverage on actual network operator LV circuits. The results show that SM measurements from key locations are sufficient for effective prediction of the voltage distribution, even without the use of the high granularity personal power demand data from individual customers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820273-9",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter3machinelearningforpredictiveanalytics",
        "booktitle": "Machine Learning in Cardiovascular Medicine",
        "doi": "10.1016/B978-0-12-820273-9.00003-8",
        "author": [
            "Kashyap, Sehj",
            "Corey, Kristin",
            "Kansal, Aman",
            "Sendak, Mark"
        ],
        "keywords": [
            "Artificial intelligence, Cardiovascular medicine, Clinical decision support, Machine learning, Predictive models"
        ],
        "abstract": "Advances in the field of machine learning are enabling the development of new predictive models in the field of cardiovascular medicine. In this chapter, we cover how predictive machine learning models are developed, their recent use-cases in cardiovascular medicine, and limitations for their expanded use in healthcare. Predictive modeling has long been used in medicine, but recently, machine learning methods offer the ability to learn patterns in large and heterogeneous health data for improved predictive modeling. In addition to describing the general methods by which machine learning models are developed, we share their recent applications in electrophysiology, interventional cardiology, heart failure, and preventive cardiovascular care. We conclude with a discussion about issues regarding data quality, generalizability, bias, and interpretability, and how these influence model development and clinical integration.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01659936",
        "isbn": null,
        "journal": "TrAC Trends in Analytical Chemistry",
        "publisher": null,
        "title": "chemometricscomprehensivetwodimensionalgaschromatographyandomicssciencesbasictoolsandrecentapplications",
        "booktitle": null,
        "doi": "10.1016/j.trac.2020.116111",
        "author": [
            "Pollo, Breno",
            "Teixeira, Carlos",
            "Belinato, Joao",
            "Furlan, Mayra",
            "Matos, Isabela",
            "Vaz, Caroline",
            "Volpato, Gustavo",
            "Augusto, Fabio"
        ],
        "keywords": [
            "GC\u00d7GC, Multivariate analysis, Big data, Data mining, Metabolomics, Petroleomics, Foodomics"
        ],
        "abstract": "The advent of Comprehensive Two-dimensional Gas Chromatography (GC \u00d7 GC) as a practical and accessible analytical tool had a considerable impact on analytical procedures associated to the so-called \u201comics\u201d sciences. Specially when GC \u00d7 GC is hyphenated to mass spectrometers or other multichannel detectors, in a single run it is possible to separate, detect and identify up to thousands of metabolites. However, the resulting data sets are exceedingly complex, and retrieving proper biochemical information from them demands powerful statistical tools to deal effectively with the massive amount of information generated by GC \u00d7 GC. Nevertheless, the obtention of results valid on a chemical and biological standpoint depends on a deep understanding by the analyst of the fundamentals both of GC \u00d7 GC and chemometrics. This review focuses on the basics of contemporary, fundamental chemometric tools applied to proccessing of GC \u00d7 GC obtained from metabolomic, petroleomic and foodomic analyses. Here, we described the fundamentals of pattern recognition methods applied to GC \u00d7 GC. Also, we explore how different detectors affect data structure and approaches for better data handling. Limitations regarding data structure and deviations from linearity are stressed for each algorithm, as well as their typical applications and expected output.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "12.296",
        "scimago_value": "2,283"
    },
    {
        "issnkey": "09658564",
        "isbn": null,
        "journal": "Transportation Research Part A: Policy and Practice",
        "publisher": null,
        "title": "supplementingtransportationdatasourceswithtargetedmarketingdataapplicationsintegrationandinternalvalidation",
        "booktitle": null,
        "doi": "10.1016/j.tra.2021.04.021",
        "author": [
            "Shaw, F.",
            "Wang, Xinyi",
            "Mokhtarian, Patricia",
            "Watkins, Kari"
        ],
        "keywords": [
            "Consumer data, Targeted marketing data, Travel behavior, Household travel survey, Big data, Third-party data, Travel demand modeling"
        ],
        "abstract": "Unlike many third-party data sources, targeted marketing (TM) data constitute holistic datasets, with disaggregate variables \u2013 ranging from socioeconomic and demographic characteristics to attitudes, propensities, and behaviors \u2013 available for most individuals in the population. These qualities, along with ease of accessibility and relatively low acquisition costs, make TM data an attractive source for the supplementation of traditional transportation survey data, which are facing growing threats to quality. This paper develops a typology demonstrating ways in which TM data can aid in the design of transport studies, as well as in the augmentation of modeling efforts and policy scenarios, allowing for improved understanding and forecasting of travel-related attributes. However, challenges associated with integrating, validating, and understanding TM variables have resulted in only a few transportation studies that have used these data thus far. In this paper, we provide a transportation discipline-specific resource for TM data, informed by our integration of an extensive TM database with both the National Household Travel Survey (Georgia subset) and a statewide travel behavior survey conducted in Georgia on behalf of the Georgia Department of Transportation. Using the resultant datasets, we validate TM data by means of several approaches, and find that the TM dataset reports gender, age, tenure, race, marital status, and household size with match rates ranging from 70% to 90% relative to both transportation surveys. However, we also identify biases in favor of population segments that may have more longstanding financial/transactional records (e.g., males, homeowners, non-minorities, and older individuals), biases comparable but not identical to those of survey data. While this work suggests wide-ranging implications for the use of TM data in transportation, we caution that flexible and responsible approaches to using these data are critical for staying abreast of evolving privacy regulations that govern third-party data sources such as these.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.594",
        "scimago_value": "2,178"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "productionspecificlanguagecharacteristicstoimprovenlpapplicationsontheshopfloor",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.319",
        "author": [
            "M\u00fcller, Marvin",
            "Metternich, Joachim"
        ],
        "keywords": [
            "digital shop floor management, shop floor language specifics, natural language processing"
        ],
        "abstract": "A variety of assistance functions have been developed based on the rising data availability on the shop floor and increasing capabilities of artificial intelligence applications. An often-mentioned risk is the low data quality, especially in manual text entries for e.g. deviations or defects. This paper aims to evaluate production specific language characteristics to adjust natural language processing applications. To achieve this goal three industry data sets are analyzed, and the findings are used to improve a recommendation engine for previously solved problems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "1044579x",
        "isbn": null,
        "journal": "Seminars in Cancer Biology",
        "publisher": null,
        "title": "anintegratedworkflowforbiomarkerdevelopmentusingmicrornasinextracellularvesiclesforcancerprecisionmedicine",
        "booktitle": null,
        "doi": "10.1016/j.semcancer.2021.03.011",
        "author": [
            "Chen, Yu",
            "Wu, Tan",
            "Zhu, Zhongxu",
            "Huang, Hao",
            "Zhang, Liang",
            "Goel, Ajay",
            "Yang, Mengsu",
            "Wang, Xin"
        ],
        "keywords": [
            "microRNAs, Extracellular vesicles, Biomarker, Network, Data mining, Cancer"
        ],
        "abstract": "EV-miRNAs are microRNA (miRNA) molecules encapsulated in extracellular vesicles (EVs), which play crucial roles in tumor pathogenesis, progression, and metastasis. Recent studies about EV-miRNAs have gained novel insights into cancer biology and have demonstrated a great potential to develop novel liquid biopsy assays for various applications. Notably, compared to conventional liquid biomarkers, EV-miRNAs are more advantageous in representing host-cell molecular architecture and exhibiting higher stability and specificity. Despite various available techniques for EV-miRNA separation, concentration, profiling, and data analysis, a standardized approach for EV-miRNA biomarker development is yet lacking. In this review, we performed a substantial literature review and distilled an integrated workflow encompassing important steps for EV-miRNA biomarker development, including sample collection and EV isolation, EV-miRNA extraction and quantification, high-throughput data preprocessing, biomarker prioritization and model construction, functional analysis, as well as validation. With the rapid growth of \u201cbig data\u201d, we highlight the importance of efficient mining of high-throughput data for the discovery of EV-miRNA biomarkers and integrating multiple independent datasets for in silico and experimental validations to increase the robustness and reproducibility. Furthermore, as an efficient strategy in systems biology, network inference provides insights into the regulatory mechanisms and can be used to select functionally important EV-miRNAs to refine the biomarker candidates. Despite the encouraging development in the field, a number of challenges still hinder the clinical translation. We finally summarize several common challenges in various biomarker studies and discuss potential opportunities emerging in the related fields.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09252312",
        "isbn": null,
        "journal": "Neurocomputing",
        "publisher": null,
        "title": "deeptimeseriesmodelsforscarcedata",
        "booktitle": null,
        "doi": "10.1016/j.neucom.2020.12.132",
        "author": [
            "Wang, Qiyao",
            "Farahat, Ahmed",
            "Gupta, Chetan",
            "Zheng, Shuai"
        ],
        "keywords": [
            "Time series analysis, Scarce data, Deep learning models, Functional data analysis"
        ],
        "abstract": "Time series data have grown at an explosive rate in numerous domains and have stimulated a surge of time series modeling research. A comprehensive comparison of different time series models, for a considered data analytics task, provides useful guidance on model selection for data analytics practitioners. Data scarcity is a universal issue that occurs in a vast range of data analytics problems, due to the high costs associated with collecting, generating, and labeling data as well as some data quality issues such as missing data. In this paper, we focus on the temporal classification/regression problem that attempts to build a mathematical mapping from multivariate time series inputs to a discrete class label or a real-valued response variable. For this specific problem, we identify two types of scarce data: scarce data with small samples and scarce data with sparsely and irregularly observed time series covariates. Observing that all existing works are incapable of utilizing the sparse time series inputs for proper modeling building, we propose a model called sparse functional multilayer perceptron (SFMLP) for handling the sparsity in the time series covariates. The effectiveness of the proposed SFMLP under each of the two types of data scarcity, in comparison with the conventional deep sequential learning models (e.g., Recurrent Neural Network, and Long Short-Term Memory), is investigated through mathematical arguments and numerical experiments.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.719",
        "scimago_value": "1,085"
    },
    {
        "issnkey": "0740624x",
        "isbn": null,
        "journal": "Government Information Quarterly",
        "publisher": null,
        "title": "understandingthekeyfactorsandconfigurationalpathsoftheopengovernmentdataperformancebasedonfuzzysetqualitativecomparativeanalysis",
        "booktitle": null,
        "doi": "10.1016/j.giq.2021.101580",
        "author": [
            "Zhao, Yupan",
            "Fan, Bo"
        ],
        "keywords": [
            "OGD, Implementation performance, fsQCA, Configurational path"
        ],
        "abstract": "The governments worldwide have attached great importance to open government data (OGD), and many OGD projects have emerged in recent years. However, the performance of OGD greatly differs in various districts and governments. Therefore, the influencing factors of OGD performance should be explored. However, the existing research has not yet established a systematic analytical framework for OGD performance, and the explanation degree of performance differences in OGD implementation is limited. Thus, this study takes technical management capacity, financial resource, organization arrangement, rules and regulations, organization culture, public demand, and inter-government competition as antecedent conditions under the perspective of technology\u2013organization\u2013environment framework and resource-based theory. From the cases of 16 provincial OGD practice in China, we employ fuzzy-set qualitative comparative analysis to explore the influencing mechanism of the interaction and coordination of multiple conditions on OGD performance. Results indicate that OGD performance depends on the integration of the total effect of various factors. Moreover, four configurational paths could be utilized to achieve high OGD performance, namely, organization\u2013balanced path, organization\u2013environment path, balanced path, and organization\u2013technology path. Furthermore, a substitution relationship exists among different conditional variables, which points out the direction and focus of the implementation of OGD for governments with different endowment characteristics. This study enriches the existing studies of OGD implementation and provides references for OGD practice.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "conceptualizingtheinternetofthingsdatasupply",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.01.213",
        "author": [
            "Nitschke, Patrick",
            "Williams, Susan"
        ],
        "keywords": [
            "Internet of Things, Data Supply, Value Capturing"
        ],
        "abstract": "By enmeshing the digital and physical worlds, the Internet of Things is envisioned to generate a wealth of real-world data which enables new ways of creating value based on data. Organizations and researchers in the Information Systems community have already begun to transform existing approaches by applying concepts such as Nonownership Business Models that use data as the new main resource to create value. However, as our critical literature review shows, there has been limited attention to the actual characteristics and challenges of data supply in the IoT. Based on the critical review, two distinct themes were identified regarding the conceptualization of data and Things. Data is conceptualized either as Shallow Data or as Provenance Data, whereas things are conceptualized in terms of Things as Sensors or Things as Agents. Based on these themes, two research implications are developed. Firstly, things are more than sensors, they inherit agency and intention from their respective owners. Secondly, the provenance of data is essential. Data is considered a resource, quality control, assessment of legality are essential to securely rely on data to create value.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "01419331",
        "isbn": null,
        "journal": "Microprocessors and Microsystems",
        "publisher": null,
        "title": "developmentofculturaltourismplatformbasedonfpgaandconvolutionalneuralnetwork",
        "booktitle": null,
        "doi": "10.1016/j.micpro.2020.103579",
        "author": [
            "Yin, Xinzhe",
            "Li, Jinghua"
        ],
        "keywords": [
            "Data mining, Field programmable gate array (FPGA) using xilinx, Predictive Modeling, Association Analysis"
        ],
        "abstract": "Data mining can be described as a typical analysis of large datasets to investigate early unknown types, styles, and interpersonal relationships to generate the right decision information. It improves their markets and today to maintain control over whether these companies are forced into the data mining tools and technologies they use to develop and manage tourism products and services in the market. It is falling out of the favorable situation of the travel and tourism industry. Objective work is to provide and display its application in data mining and tourism. Advances in mobile technology provide an opportunity to obtain real-time information of travelers, such as time and space behavior, at the destination they visit. This study analyzed a large-scale mobile phone data set to capture the mobile phone traces of international tourists who visited South Korea. We adopt the trajectory data mining method to understand tourism activities\u2019 spatial structure in three different destinations. The research reveals tourist destinations and multiple \u201chot spots\u201d (or popular areas) that interact spatially in these places through spatial cluster analysis and sequential pattern mining. Therefore, this article provides the planning of spatial model destinations to integrate important tourism influences, which is based on tourism design. The proposed system is modelled in Field Programmable Gate Array (FPGA) using Xilinx software.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.525",
        "scimago_value": "0,323"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822312-3",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter23moleculardockingacontemporarystoryaboutfoodsafety",
        "booktitle": "Molecular Docking for Computer-Aided Drug Design",
        "doi": "10.1016/B978-0-12-822312-3.00025-4",
        "author": [
            "Cavaliere, Francesca",
            "Spaggiari, Giulia",
            "Cozzini, Pietro"
        ],
        "keywords": [
            "Big data, Consensus scoring, Database, Food safety, In silico methods, Molecular docking, Virtual screening"
        ],
        "abstract": "The application of computational methods (repository or database design, screening, and molecular docking) in food safety is a relatively recent challenge. Docking/scoring techniques could be applied to a wide range of food safety problems. An important milestone for screening/docking approaches is the availability of a three-dimensional database to collect the huge amount of food contact chemicals to make possible testing these compounds otherwise unfeasible with traditional in vitro tests. In silico applications could be applied to predict the interaction between food contact chemicals and different receptors/targets involved in human diseases and/or to decipher their mechanism of binding. Another important purpose is the design of chemosensors for mycotoxins detection. The use of docking techniques as an alternative to animal tests is an emerging field and we will illustrate these concepts using recently published cases.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22124268",
        "isbn": null,
        "journal": "Journal of Oral Biology and Craniofacial Research",
        "publisher": null,
        "title": "internetofthingsiotenabledhealthcarehelpstotakethechallengesofcovid19pandemic",
        "booktitle": null,
        "doi": "10.1016/j.jobcr.2021.01.015",
        "author": [
            "Javaid, Mohd",
            "Khan, Ibrahim"
        ],
        "keywords": [
            "Internet of things (IoT), COVID-19, Information technology applications, Healthcare, Smart hospital"
        ],
        "abstract": "Background/objectives The Internet of Things (IoT) can create disruptive innovation in healthcare. Thus, during COVID-19 Pandemic, there is a need to study different applications of IoT enabled healthcare. For this, a brief study is required for research directions. Methods Research papers on IoT in healthcare and COVID-19 Pandemic are studied to identify this technology\u2019s capabilities. This literature-based study may guide professionals in envisaging solutions to related problems and fighting against the COVID-19 type pandemic. Results Briefly studied the significant achievements of IoT with the help of a process chart. Then identifies seven major technologies of IoT that seem helpful for healthcare during COVID-19 Pandemic. Finally, the study identifies sixteen basic IoT applications for the medical field during the COVID-19 Pandemic with a brief description of them. Conclusions In the current scenario, advanced information technologies have opened a new door to innovation in our daily lives. Out of these information technologies, the Internet of Things is an emerging technology that provides enhancement and better solutions in the medical field, like proper medical record-keeping, sampling, integration of devices, and causes of diseases. IoT\u2019s sensor-based technology provides an excellent capability to reduce the risk of surgery during complicated cases and helpful for COVID-19 type pandemic. In the medical field, IoT\u2019s focus is to help perform the treatment of different COVID-19 cases precisely. It makes the surgeon job easier by minimising risks and increasing the overall performance. By using this technology, doctors can easily detect changes in critical parameters of the COVID-19 patient. This information-based service opens up new healthcare opportunities as it moves towards the best way of an information system to adapt world-class results as it enables improvement of treatment systems in the hospital. Medical students can now be better trained for disease detection and well guided for the future course of action. IoT\u2019s proper usage can help correctly resolve different medical challenges like speed, price, and complexity. It can easily be customised to monitor calorific intake and treatment like asthma, diabetes, and arthritis of the COVID-19 patient. This digitally controlled health management system can improve the overall performance of healthcare during COVID-19 pandemic days.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0006291x",
        "isbn": null,
        "journal": "Biochemical and Biophysical Research Communications",
        "publisher": null,
        "title": "transcriptionalregulationandfunctionalanalysisofnicotianatabacumundersaltandabastress",
        "booktitle": null,
        "doi": "10.1016/j.bbrc.2021.07.011",
        "author": [
            "Wu, Hui",
            "Li, Huayang",
            "Zhang, Wenhui",
            "Tang, Heng",
            "Yang, Long"
        ],
        "keywords": [
            "Tobacco, Transcriptome, MAPK, Soil salinization mechanism"
        ],
        "abstract": "Soil salinization is an important factor that restricts crop quality and yield and causes an enormous toll to human beings. Salt stress and abscisic acid (ABA) stress will occur in the process of soil salinization. In this study, transcriptome sequencing of tobacco leaves under salt and ABA stress in order to further study the resistance mechanism of tobacco. Compared with controlled groups, 1654 and 3306 DEGs were obtained in salt and ABA stress, respectively. The genes function enrichment analysis showed that the up-regulated genes in salt stress were mainly concentrated in transcription factor WRKY family and PAR1 resistance gene family, while the up-regulated genes were mainly concentrated on bHLH transcription factor, Kunitz-type protease inhibitor, dehydrin (Xero1) gene and CAT (Catalase) family protein genes in ABA stress. Tobacco MAPK cascade triggered stress response through up-regulation of gene expression in signal transduction. The expression products of these up-regulated genes can improve the abiotic stress resistance of plants. These results have an important implication for further understanding the mechanism of salinity tolerance in plants.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26666839",
        "isbn": null,
        "journal": "Geography and Sustainability",
        "publisher": null,
        "title": "giscienceandremotesensinginnaturalresourceandenvironmentalresearchstatusquoandfutureperspectives",
        "booktitle": null,
        "doi": "10.1016/j.geosus.2021.08.004",
        "author": [
            "Pei, Tao",
            "Xu, Jun",
            "Liu, Yu",
            "Huang, Xin",
            "Zhang, Liqiang",
            "Dong, Weihua",
            "Qin, Chengzhi",
            "Song, Ci",
            "Gong, Jianya",
            "Zhou, Chenghu"
        ],
        "keywords": [
            "Natural resource, Environmental science, GIScience, Remote sensing, Information technology"
        ],
        "abstract": "Geographic information science (GIScience) and remote sensing have long provided essential data and methodological support for natural resource challenges and environmental problems research. With increasing advances in information technology, natural resource and environmental science research faces the dual challenges of data and computational intensiveness. Therefore, the role of remote sensing and GIScience in the fields of natural resources and environmental science in this new information era is a key concern of researchers. This study clarifies the definition and frameworks of these two disciplines and discusses their role in natural resource and environmental research. GIScience is the discipline that studies the abstract and formal expressions of the basic concepts and laws of geography, and its research framework mainly consists of geo-modeling, geo-analysis, and geo-computation. Remote sensing is a comprehensive technology that deals with the mechanisms of human effects on the natural ecological environment system by observing the earth surface system. Its main areas include sensors and platforms, information processing and interpretation, and natural resource and environmental applications. GIScience and remote sensing provide data and methodological support for resource and environmental science research. They play essential roles in promoting the development of resource and environmental science and other related technologies. This paper provides forecasts of ten future directions for GIScience and eight future directions for remote sensing, which aim to solve issues related to natural resources and the environment.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "databasedqualityanalysisinmachiningproductioninfluenceofdatapreprocessingontheresultsofmachinelearningmodels",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.146",
        "author": [
            "Ziegenbein, Amina",
            "Metternich, Joachim"
        ],
        "keywords": [
            "Machine Learning, Part Quality, Drilling"
        ],
        "abstract": "Quality assurance as a non-value-adding process is constantly reviewed for cost optimisation and potential savings. In the pursuit of utilising advanced data analysis and machine learning methods to improve efficiency of quality assurance in machining processes there are several influencing factors severely impacting the performance and hence the value of said methods. Especially data preparation is a time consuming task requiring both domain and data expert knowledge and yielding various options for data preparation. In this paper, the impact of different input data sets for predicting part quality in a drilling process is investigated, using machine control data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "14402440",
        "isbn": null,
        "journal": "Journal of Science and Medicine in Sport",
        "publisher": null,
        "title": "dataanalyticsinmilitaryhumanperformancegettinginthegamesummaryofakeynoteaddress",
        "booktitle": null,
        "doi": "10.1016/j.jsams.2021.04.003",
        "author": [
            "Kaluzny, Bohdan"
        ],
        "keywords": [
            "Data analytics, Operations research, Military human performance"
        ],
        "abstract": "Objectives The rise of data analytics has been not only central to the digital transformation of many industries and governments, but is now ubiquitous in daily life. But what is it? Researchers in military human performance may very well ask themselves: What is new? After all, aren't they already collecting, analysing, interpreting, and presenting data? Do they need to adapt? Discussion Defence and security have often been at the forefront of new technologies, but has lagged other industries with respect to data analytics. Sports science is one of the industries that are on the leading edge and this presents an opportunity that researchers in military human performance must seize. Conclusions Researchers must embrace data analytics and seek opportunities to \u2018operationalize\u2019 their research via data science: responsible analytics respecting scientific development supporting decision making at the necessary speed of relevance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13861425",
        "isbn": null,
        "journal": "Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy",
        "publisher": null,
        "title": "nondestructiveevaluationofsolublesolidscontentintomatowithdifferentstagebyusingvisnirtechnologyandmultivariatealgorithms",
        "booktitle": null,
        "doi": "10.1016/j.saa.2020.119139",
        "author": [
            "Zhang, Dongyan",
            "Yang, Yi",
            "Chen, Gao",
            "Tian, Xi",
            "Wang, Zheli",
            "Fan, Shuxiang",
            "Xin, Zhenghua"
        ],
        "keywords": [
            "Vis/NIR, Soluble solids content, Tomato, PLS, LS-SVM, Effective wavelength"
        ],
        "abstract": "In this study Vis/NIR spectroscopy was applied to evaluate soluble solids content (SSC) of tomato. A total of 168 tomato samples with five different maturity stages, were measured by two developed systems with the wavelength ranges of 500\u2013930 nm and 900\u20131400 nm, respectively. The raw spectral data were pre-processed by first derivative and standard normal variate (SNV), respectively, and then the effective wavelengths were selected using competitive adaptive reweighted sampling (CARS) and random frog (RF). Partial least squares (PLS) and least square-support vector machines (LS-SVM) were employed to build the prediction models to evaluate SSC in tomatoes. The prediction results revealed that the best performance was obtained using the PLS model with the optimal wavelengths selected by CARS in the range of 900\u20131400 nm (Rp = 0.820 and RMSEP = 0.207 \u00b0Brix). Meanwhile, this best model yielded desirable results with Rp and RMSEP of 0.830 and 0.316 \u00b0Brix, respectively, in 60 samples of the independent set. The method proposed from this study can provide an effective and quick way to predict SSC in tomato.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.098",
        "scimago_value": "0,606"
    },
    {
        "issnkey": "15534650",
        "isbn": null,
        "journal": "Journal of Minimally Invasive Gynecology",
        "publisher": null,
        "title": "geneexpressionsignatureofendometrialsamplesfromwomenwithandwithoutendometriosis",
        "booktitle": null,
        "doi": "10.1016/j.jmig.2021.03.011",
        "author": [
            "Adamyan, Leila",
            "Aznaurova, Yana",
            "Stepanian, Assia",
            "Nikitin, Daniil",
            "Garazha, Andrew",
            "Suntsova, Maria",
            "Sorokin, Maxim",
            "Buzdin, Anton"
        ],
        "keywords": [
            "Endometriosis, Molecular diagnostics, RNA sequencing, Gene expression signature, Big data in clinical medicine"
        ],
        "abstract": "ABSTRACT Study Objective To develop a prototype of a complex gene expression biomarker for the diagnosis of endometriosis on the basis of differences between the molecular signatures of the endometrium from women with and without endometriosis. Design Prospective observational cohort study. Evidence obtained from a well-designed, controlled trial without randomization. Setting Department of reproductive medicine and surgery, A.I. Evdokimov Moscow State University of Medicine and Dentistry. Patients A total of 33 women (aged 32\u201338 years) were included in this study. Patients with and without endometriosis were divided into 2 separate groups. The group composed of patients with endometriosis included 19 living patients with endometriosis who underwent laparoscopic excision of endometriosis. The control group included 6 living patients who underwent laparoscopic excision of incompetent uterine scar after cesarean section, with both surgically and histologically confirmed absence of endometriosis and adenomyosis. An additional control/verification group included various previously RNA-sequencing\u2013profiled tissue samples (endocervix, ovarian surface epithelium) of 8 randomly selected healthy female cadaveric donors aged 32 to 38 years. The exclusion criteria for all patients were hormone therapy and any intrauterine device use for more than 1 year preceding surgery, as well as absence of other diseases of the uterus, fallopian tubes, and ovaries. Interventions Laparoscopic excision of endometriotic foci and hysteroscopy with endometrial sampling were performed. The cadaveric tissue samples included endocervix and ovarian surface epithelium. Endometrial sampling was obtained from the women in the control group. RNA sequencing was performed using Illumina HiSeq 3000 equipment (Illumina, Inc., San Diego, CA) for single-end sequencing. Unique bioinformatics algorithms were developed and validated using experimental and public gene expression datasets. Measurements and Main Results We generated a characteristic signature of 5 genes downregulated in the endometrium and endometriotic tissue of the patients with endometriosis, selected after comparison with the endometrium of the women without endometriosis. This gene signature showed a capacity for nearly perfect separation of all 52 analyzed tissue samples of the patients with endometriosis (endometrial as well as endometriotic samples) from the 14 tissue samples of both living and cadaveric donors without endometriosis (area under the curve = 0.982, Matthews correlation coefficient = 0.832). Conclusion The gene signature of the endometrium identified in this study may potentially serve as a nonsurgical diagnostic method for endometriosis detection. Our data also suggest that the statistical method of 5-fold cross-validation of differential gene expression analysis can be used to generate robust gene signatures using real-world clinical data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26663899",
        "isbn": null,
        "journal": "Patterns",
        "publisher": null,
        "title": "howtoberesponsibleinallthestepsofadatasciencepipelinethecaseoftheitalianpublicsector",
        "booktitle": null,
        "doi": "10.1016/j.patter.2021.100393",
        "author": [
            "Scannapieco, Monica",
            "Virgillito, Antonino"
        ],
        "keywords": [
            ""
        ],
        "abstract": "The paper highlights how each step of a data science pipeline can be performed in a \u201cresponsible\u201d way, taking into account privacy, ethics, and quality issues. Several examples from the Italian public sector contribute to clarifying how data collections and data analyses can be carried out under a responsible view.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13837621",
        "isbn": null,
        "journal": "Journal of Systems Architecture",
        "publisher": null,
        "title": "busnetworkassisteddroneschedulingforsustainablechargingofwirelessrechargeablesensornetwork",
        "booktitle": null,
        "doi": "10.1016/j.sysarc.2021.102059",
        "author": [
            "Jin, Yong",
            "Xu, Jia",
            "Wu, Sixu",
            "Xu, Lijie",
            "Yang, Dejun",
            "Xia, Kaijian"
        ],
        "keywords": [
            "Wireless rechargeable sensor network, Bus network, Drone scheduling, Traveling salesman path problem, Submodular orienteering problem"
        ],
        "abstract": "Wireless Rechargeable Sensor Network (WRSN) is largely used in monitoring of environment and traffic, video surveillance and medical care, etc., and helps to improve the quality of urban life. However, it is challenging to provide the sustainable energy for sensors deployed in buildings, soil or other places, where it is hard to harvest the energy from environment. To address this issue, we design a new wireless charging system, which levers the bus network assisted drone in urban areas. We formulate the drone scheduling problem based on this new wireless charging system to minimize the total time cost of drone subject to all sensors can be charged under the energy constraint of drone. Then, we propose an approximation algorithm DSA for the energy tightened drone scheduling problem. To make the tasks of WRSN sustainable, we further formulate the drone scheduling problem with deadlines of sensors, and present the approximation algorithm DDSA to find the drone schedule with the maximal number of sensors charged by the drone before deadlines. Through the extensive simulations, we demonstrate that DSA can reduce the total time cost by 84.83% compared with Greedy Replenished Energy algorithm, and uses at most 5.98 times of the total time cost of optimal solution on average. Then, we also demonstrate that DDSA can increase the survival rate of sensors by 51.95% compared with Deadline Greedy Replenished Energy algorithm, and can obtain 77.54% survival rate of optimal solution on average.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.777",
        "scimago_value": "0,598"
    },
    {
        "issnkey": "13865056",
        "isbn": null,
        "journal": "International Journal of Medical Informatics",
        "publisher": null,
        "title": "completionofelectronicnursingdocumentationofinpatientadmissionassessmentinsightsfromaustralianmetropolitanhospitals",
        "booktitle": null,
        "doi": "10.1016/j.ijmedinf.2021.104603",
        "author": [
            "Shala, Danielle",
            "Jones, Aaron",
            "Fairbrother, Greg",
            "{Thuy Tran}, Duong"
        ],
        "keywords": [
            "Electronic clinical documentation, Nursing admission, eMR data, Health informatics, Data science, Nursing informatics"
        ],
        "abstract": "Introduction Electronic nursing documentation is an essential aspect of inpatient care and multidisciplinary communication. Analysing data in electronic medical record (eMR) systems can assist in understanding clinical workflows, improving care quality, and promoting efficiency in the healthcare system. This study aims to assess timeliness of completion of an electronic nursing admission assessment form and identify patient and facility factors associated with form completion in three metropolitan hospitals. Materials and Methods Records of 37,512 adult inpatient admissions (November 2018-November 2019) were extracted from the hospitals\u2019 eMR system. A dichotomous variable descriptive of completion of the nursing assessment form (Yes/No) was created. Timeliness of form completion was calculated as the interval between date and time of admission and form completion. Univariate and multivariate multilevel logistic regression were used to identify factors associated with form completion. Results An admission assessment form was completed for 78.4% (n = 29,421) of inpatient admissions. Of those, 78% (n = 22,953) were completed within the first 24 h of admission, 13.3% (n = 3,910) between 24 and 72 h from admission, and 8.7% (n = 2,558) beyond 72 h from admission. Patient length of hospital stay, admission time, and admitting unit\u2019s nursing hours per patient day were associated with form completion. Patient gender, age, and admitting unit type were not associated with form completion. Discussion Form completion rate was high, though more emphasis needs to be placed on the importance of timely completion to allow for adequate patient care planning. Staff education, qualitative understanding of delayed form completion, and streamlined guidelines on nursing admission and eMR use are recommended.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15684946",
        "isbn": null,
        "journal": "Applied Soft Computing",
        "publisher": null,
        "title": "datasetqualityinmachinelearningconsistencymeasurebasedongroupdecisionmaking",
        "booktitle": null,
        "doi": "10.1016/j.asoc.2021.107366",
        "author": [
            "Fenza, Giuseppe",
            "Gallo, Mariacristina",
            "Loia, Vincenzo",
            "Orciuoli, Francesco",
            "Herrera-Viedma, Enrique"
        ],
        "keywords": [
            "Consistency, Learning to Rank, Group Decision Making, Training data consistency, Data set quality"
        ],
        "abstract": "Performance of Machine Learning models heavily depends on the quality of the training dataset. Among others, the quality of training data relies on the consistency of the labels assigned to similar items. Indeed, the labels should be coherently assigned (or collected) by avoiding inconsistencies for increasing the performance of the machine learning model. This study focuses on evaluating training data consistency for machine learning algorithms dealing with ranking problems, i.e., the Learning to Rank methods (LTR). This work defines a training data consistency measure based on the consensus value introduced in Group Decision Making. It investigates the statistical relationship between the proposed consistency measure and the performance of a deep neural network implementing an LTR method. This measure could drive data filtering at the training stage and guide model update decisions. Experimentation reveals a strong correlation between the proposed consistency measure and the performance of the model.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,290"
    },
    {
        "issnkey": "09269851",
        "isbn": null,
        "journal": "Journal of Applied Geophysics",
        "publisher": null,
        "title": "robustcsemdataprocessingbyunsupervisedmachinelearning",
        "booktitle": null,
        "doi": "10.1016/j.jappgeo.2021.104262",
        "author": [
            "Li, Guang",
            "He, Zhushi",
            "Deng, Juzhi",
            "Tang, Jingtian",
            "Fu, Youyao",
            "Liu, Xiaoqiong",
            "Shen, Changming"
        ],
        "keywords": [
            "CSEM data processing, Periodic signal de-noising, Signal-noise identification, Fuzzy -means clustering (FCM), Machine Learning, Correlation Analysis"
        ],
        "abstract": "The ambient noise in controlled-source electromagnetic (CSEM) data seriously affects the accuracy and reliability of the exploration result. Traditional correlation-based data selection method requires manually setting the threshold. To overcome the deficiency, we analyze the typical noises in CSEM data and find that normalized cross-correlation (NCC), absolute maximum value of the amplitude (Max), and detrend fluctuation analysis (DFA) can be used to accurately identify high-quality time series. Based on this discovery, we replace traditional manually intervention with unsupervised machine learning and propose a novel CSEM data processing method. We applied the newly proposed method to synthetic and measured CSEM data to verify the feasibility and effectiveness. Experimental results demonstrate that the newly proposed method is superior to the conventional data selection method because it accurately selects the best data fragments from noisy data automatically. The newly proposed method requires no human intervention which makes the results obtained free of subjective distortion caused by the operator.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.121",
        "scimago_value": "0,627"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-819742-4",
        "journal": null,
        "publisher": "Elsevier",
        "title": "5datamanagementfromthedcstothehistorianandhmi",
        "booktitle": "Machine Learning and Data Science in the Power Generation Industry",
        "doi": "10.1016/B978-0-12-819742-4.00005-6",
        "author": [
            "Crompton, Jim"
        ],
        "keywords": [
            "Machine learning, Value chain optimization, Incremental value chain economics, Digital platform, Asset utilization, Grow new markets, Integrated strategies, Innovation"
        ],
        "abstract": "In today's global and fast changing business environment, the urgency with multinational companies to find readily implementable digital solutions has increased significantly as the underlying science yielding operational and business improvements has matured over the past decade. Companies lacking innovative ways to extract incremental value from existing and new business ventures will be left behind. Manufacturing plants, in general, manage billion dollars\u2019 worth of raw and finished products daily across the United States and globally. The quantities of data analyzed in a product lifecycle, capturing cost to produce, logistics, working capital, and other ancillary costs are massive and difficult to both consolidate and integrate. Many production companies still rely heavily on manual processes to evaluate opportunities and economics.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02666138",
        "isbn": null,
        "journal": "Midwifery",
        "publisher": null,
        "title": "exploringvariationintheperformanceofplannedbirthamixedmethodstudy",
        "booktitle": null,
        "doi": "10.1016/j.midw.2021.102988",
        "author": [
            "Dominiek, Coates",
            "Amanda, Henry",
            "Georgina, Chambers",
            "Repon, Paul",
            "Angela, Makris",
            "Teena, Clerke",
            "Donnolley, Natasha"
        ],
        "keywords": [
            "Unwarranted variation, Induction of labour, Planned caesarean section, Clinician perspectives"
        ],
        "abstract": "Objective: Variation in practice in relation to indications and timing for both induction of labour (IOL) and planned caesarean section (CS) clearly exists. However, the extent of this variation, and how this variation is explained by clinicians remains unclear. The aim of this study was to map the variation in IOL and planned CS at eight Australian hospitals, and understand why variation occurs from the perspective of clinicians at these hospitals. Our ultimate aim was to identify opportunities for improvement as evidenced by hospital data, clinician experiences, and feedback. Design: A two-phased mixed method study using sequential explanatory study design. The first phase consisted of an analysis of routinely collected patient data to map variation between hospitals. The second phase consisted of focus groups with clinicians to gain their perspectives on the reasons for variation. Setting and Participants: Patient data consisted of routine data from 19,073 women giving birth at eight Sydney hospitals between November 2017 and October 2018. Focus groups were attended by a total of 61 medical staff and 121 midwives. Results: Hospital data analysis found substantial variation, before and after adjustment for case-mix, in rates of both IOL (adjusted rates 27.6%\u201342%) and planned CS (adjusted rate 15.4%\u201322.6%). Planned CS by gestation also showed variation, although after restricting analysis to term (\u226537 weeks gestation) births, variation was reduced. At focus groups, five main themes explaining variation emerged: local guidelines, policies and procedures (inconsistency and ambiguity); uncertainty of the evidence/what is best practice (contradictory research and different interpretations of evidence); clinician preferences, beliefs and values; the culture of the unit; and organisational influences (access to specialised clinics, theatre time). Key conclusions: Considerable variation in IOL and planned CS, even after case-mix adjustment, was found in this sample of Australian hospitals. Engagement with hospital clinicians identified likely sources of this variation and enabled clinicians at each hospital to consider appropriate local responses to address variation, such as more detailed review of their planned birth cases. Implications for practice: At a macro level, measures to reduce unwarranted variation should initially focus on consistent national guidelines, while supporting equitable access to operating theatres for optimal CS timing, and shared decision-making training to reduce influence of clinician preference.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.372",
        "scimago_value": "0,899"
    },
    {
        "issnkey": "10848045",
        "isbn": null,
        "journal": "Journal of Network and Computer Applications",
        "publisher": null,
        "title": "domainspecificknowledgegraphsasurvey",
        "booktitle": null,
        "doi": "10.1016/j.jnca.2021.103076",
        "author": [
            "Abu-Salih, Bilal"
        ],
        "keywords": [
            "Knowledge graph, Domain-specific knowledge graph, Knowledge graph construction, Knowledge graph embeddings, Knowledge graph evaluation, Domain ontology, Survey"
        ],
        "abstract": "Knowledge Graphs (KGs) have made a qualitative leap and effected a real revolution in knowledge representation. This is leveraged by the underlying structure of the KG which underpins a better comprehension, reasoning and interpretation of knowledge for both human and machine. Therefore, KGs continue to be used as the main means of tackling a plethora of real-life problems in various domains. However, there is no consensus in regard to a plausible and inclusive definition of a domain-specific KG. Further, in conjunction with several limitations and deficiencies, various domain-specific KG construction approaches are far from perfect. This survey is the first to offer a comprehensive definition of a domain-specific KG. Also, the paper presents a thorough review of the state-of-the-art approaches drawn from academic works relevant to seven domains of knowledge. An examination of current approaches reveals a range of limitations and deficiencies. At the same time, uncharted territories on the research map are highlighted to tackle extant issues in the literature and point to directions for future research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "proposalforahealthinformationmanagementmodelbasedonleanthinking",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.01.306",
        "author": [
            "{Caretta Teixeira}, J\u00e9ssica",
            "Bernardi, Filipe",
            "{Lopes Rijo}, Rui",
            "Alves, Domingos"
        ],
        "keywords": [
            "Lean Healthcare, Health Information Systems, Quality Improvement"
        ],
        "abstract": "Although assessing quality in the health field is a prominent challenge, there is unanimity among managers that it is necessary to select appropriate assessment systems and methods to assist the administration of services and provide decision-making with the least degree of uncertainty possible. Lean, also known as lean philosophy, is a management model that has been used in the area of Health. The management of data, knowledge, and health services must be carefully performed, so that quality care can be offered. at all levels of care. In this way, when implementing Lean strategies in Information Technology, it is necessary to evaluate all its processes within the institution to eliminate waste, structure functions within the applied methodology and measure improvement at all levels of the organization. Thus, the general objective of this article is that of a study that leads to a health information management model based on Lean thinking in the municipality of Ituverava. The highly heterogeneous, and sometimes ambiguous, nature of the medical language and its constant evolution, the high amount of data generated constantly by the automation of processes and the emergence of new technologies constitute the foundation for the inevitable computerization of health to promote the production and management of knowledge. Adopting Lean thinking in health may seem a challenge initially for managers and team members, but as the first results begin to appear, profound and concrete changes are visible for positive transformation for improvements in the quality of the service provided, until the culture can be learned completely in order to have the perfect care.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "25889338",
        "isbn": null,
        "journal": "Journal of Biosafety and Biosecurity",
        "publisher": null,
        "title": "laboratoryinformationmanagementsystemforbiosafetylaboratorysafetyandefficiency",
        "booktitle": null,
        "doi": "10.1016/j.jobb.2021.03.001",
        "author": [
            "Sun, Dingzhong",
            "Wu, Linhuan",
            "Fan, Guomei"
        ],
        "keywords": [
            "Laboratory information management system, Biosafety, Biological research laboratory, Laboratory safety, Workflow management"
        ],
        "abstract": "Laboratory information management system (LIMS) has been widely used to facilitate laboratory activities. However, the current LIMSs do not contain functions to improve the safety of laboratory work, which is the major concern of biosafety laboratories (BSLs). With tons of biosafety information that need to be managed and an increasing number of biosafety-related research projects under way, it is worthy of expanding the current framework of LIMS and building a system that is more suitable for BSL usage. Such a system should carefully trade off between the safety and efficiency of regular lab activities, allowing the laboratory staff to conduct their research as free as possible while ensuring their and the environment\u2019s safety. In order to achieve this goal, the information on the research contents, laboratory personnel, experimental materials and experimental equipment need to be well collected and fully utilized by a centralized system and its databases.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "07493797",
        "isbn": null,
        "journal": "American Journal of Preventive Medicine",
        "publisher": null,
        "title": "embracinguncertaintythevalueofpartialidentificationinpublichealthandclinicalresearch",
        "booktitle": null,
        "doi": "10.1016/j.amepre.2021.01.041",
        "author": [
            "Mullahy, John",
            "Venkataramani, Atheendar",
            "Millimet, Daniel",
            "Manski, Charles"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Introduction This paper describes the methodology of partial identification and its applicability to empirical research in preventive medicine and public health. Methods The authors summarize findings from the methodologic literature on partial identification. The analysis was conducted in 2020\u20132021. Results The applicability of partial identification methods is demonstrated using 3 empirical examples drawn from published literature. Conclusions Partial identification methods are likely to be of considerable interest to clinicians and others engaged in preventive medicine and public health research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.043",
        "scimago_value": "2,287"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "requirementstowardsoptimizinganalyticsinindustrialprocesses",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.03.074",
        "author": [
            "Zeiser, Alexander",
            "Stein, Bas",
            "B\u00e4ck, Thomas"
        ],
        "keywords": [
            "Industry 4.0, Predictive Modelling, Optimizing analytics, Information fusion, Machine Learning, Industrial processes"
        ],
        "abstract": "Modern production systems are composed of complex manufacturing processes with highly technology specific cause-effect relationships. Developments in sensor technology and computational science allow for data-driven decision making that facilitate effcient and objective production management. However, process data may only be beneficial if it is enriched with meta information and process expertise, reduced to relevant information and modelling results interpreted correctly. The importance of data integration in the heterogeneous industrial environment rises at the same momentum as new metrology techniques are deployed. In this paper, we focus on optimizing analytics, containing data-driven decision making for predictive quality and maintenance. We summarize key requirements for data analytics and machine learning application in industrial processes. With a use case from automotive component manufacturing we characterize industrial production, categorize process data and put requirements in context to a real-world example.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "digitalisationboostscompanyperformanceanoverviewofitalianlistedcompanies",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.121173",
        "author": [
            "Truant, Elisa",
            "Broccardo, Laura",
            "Dana, L\u00e9o-Paul"
        ],
        "keywords": [
            "Digitalization, Company performance, Listed company, Benefit, Obstacle"
        ],
        "abstract": "Digitalisation has become embedded in products and services, and it increasingly supports corporate business processes. However, few empirical studies have analysed the state of digitalisation and its implementation within companies, and the extant literature has painted an inconsistent picture concerning the effects of digitalisation. This survey-based study explores the diffusion of digitalisation, the advantages and difficulties in the practical transition to digitalisation, and its impact on performance. The sample includes Italian listed companies across diverse industries. The results highlight the still embryonic adoption of digital tools to support daily company operations; however, the impacts of digitalisation on company performance are noticeable. This research contributes to the literature on digitalisation and performance, breaks new ground by focusing on listed companies, and has implications for management investment in digitalisation for value creation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "00457949",
        "isbn": null,
        "journal": "Computers & Structures",
        "publisher": null,
        "title": "machinelearningbaseddigitaltwinfordynamicalsystemswithmultipletimescales",
        "booktitle": null,
        "doi": "10.1016/j.compstruc.2020.106410",
        "author": [
            "Chakraborty, S.",
            "Adhikari, S."
        ],
        "keywords": [
            "Digital twin, Multi-timescale dynamics, Mixture of experts, Gaussian process, Frequency"
        ],
        "abstract": "Digital twin technology has a huge potential for widespread applications in different industrial sectors such as infrastructure, aerospace, and automotive. However, practical adoptions of this technology have been slower, mainly due to a lack of application-specific details. Here we focus on a digital twin framework for linear single-degree-of-freedom structural dynamic systems evolving in two different operational time scales in addition to its intrinsic dynamic time-scale. Our approach strategically separates into two components \u2013 (a) a physics-based nominal model for data processing and response predictions, and (b) a data-driven machine learning model for the time-evolution of the system parameters. The physics-based nominal model is system-specific and selected based on the problem under consideration. On the other hand, the data-driven machine learning model is generic. For tracking the multi-timescale evolution of the system parameters, we propose to exploit a mixture of experts as the data-driven model. Within the mixture of experts model, Gaussian Process (GP) is used as the expert model. The primary idea is to let each expert track the evolution of the system parameters at a single time-scale. For learning the hyperparameters of the \u2018mixture of experts using GP\u2019, an efficient framework that exploits expectation-maximization and sequential Monte Carlo sampler is used. Performance of the digital twin is illustrated on a multi-timescale dynamical system with stiffness and/or mass variations. The digital twin is found to be robust and yields reasonably accurate results. One exciting feature of the proposed digital twin is its capability to provide reasonable predictions at future time-steps. Aspects related to the data quality and data quantity are also investigated.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,450"
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "blogtextqualityassessmentusinga3dcnnbasedstatisticalframework",
        "booktitle": null,
        "doi": "10.1016/j.future.2020.10.025",
        "author": [
            "Ji, Fang",
            "Zhang, Heqing",
            "Zhu, Zijiang",
            "Dai, Weihuang"
        ],
        "keywords": [
            "Blog data, VQA, 3D CNN, Video-related text, Bi-LSTM, Text quality evaluation"
        ],
        "abstract": "Aiming at the problem that blog texts are the streaming data captured by different acquisition modality, each kind of which has its particular quality evaluation mode, this paper proposes a text quality evaluation (TQA) model based on 3D CNN correlated with blog text data. In order to achieve accurate TQA value, the model adopted a Bi-LSTM-based architecture to process video-related blog text as auxiliary part to provide additional information for our TQA architecture. First, the auxiliary part constructs feature vector for each video-related text by the model originating from Bi-LSTM and Seq2Seq. Then, the feature vector was feed to a well-trained decoder to reconstruct the original input data. Then, the feature vector complied with the blog textual data are inputted into end-to-end TQA modal based on the 3D CNN straightly. Comprehensive experimental results on the blog text/video dataset from the well-known truism website \u201chttp://www.mafengwo.cn/\u201d have shown that the proposed model reflects the subjective quality of online texts more accurately, and has better overall blog TQA assessment performance than the other state-of-the-art non-reference methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03062619",
        "isbn": null,
        "journal": "Applied Energy",
        "publisher": null,
        "title": "areviewofmachinelearninginbuildingloadprediction",
        "booktitle": null,
        "doi": "10.1016/j.apenergy.2021.116452",
        "author": [
            "Zhang, Liang",
            "Wen, Jin",
            "Li, Yanfei",
            "Chen, Jianli",
            "Ye, Yunyang",
            "Fu, Yangyang",
            "Livingood, William"
        ],
        "keywords": [
            "Building energy system, Building load prediction, Building energy forecasting, Machine learning, Feature engineering, Data engineering"
        ],
        "abstract": "The surge of machine learning and increasing data accessibility in buildings provide great opportunities for applying machine learning to building energy system modeling and analysis. Building load prediction is one of the most critical components for many building control and analytics activities, as well as grid-interactive and energy efficiency building operation. While a large number of research papers exist on the topic of machine-learning-based building load prediction, a comprehensive review from the perspective of machine learning is missing. In this paper, we review the application of machine learning techniques in building load prediction under the organization and logic of the machine learning, which is to perform tasks T using Performance measure P and based on learning from Experience E. Firstly, we review the applications of building load prediction model (task T). Then, we review the modeling algorithms that improve machine learning performance and accuracy (performance P). Throughout the papers, we also review the literature from the data perspective for modeling (experience E), including data engineering from the sensor level to data level, pre-processing, feature extraction and selection. Finally, we conclude with a discussion of well-studied and relatively unexplored fields for future research reference. We also identify the gaps in current machine learning application and predict for future trends and development.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.746",
        "scimago_value": "3,035"
    },
    {
        "issnkey": "03645916",
        "isbn": null,
        "journal": "Calphad",
        "publisher": null,
        "title": "towardshighthroughputmicrostructuresimulationincompositionallycomplexalloysviamachinelearning",
        "booktitle": null,
        "doi": "10.1016/j.calphad.2020.102231",
        "author": [
            "Li, Yue",
            "Holmedal, Bj\u00f8rn",
            "Liu, Boyu",
            "Li, Hongxiang",
            "Zhuang, Linzhong",
            "Zhang, Jishan",
            "Du, Qiang",
            "Xie, Jianxin"
        ],
        "keywords": [
            "Materials informatics, Machine learning, High-throughput computing, Microstructure simulation, Tabulation"
        ],
        "abstract": "The coupling of computational thermodynamics and kinetics has been the central research theme in Integrated Computational Material Engineering (ICME). Two major bottlenecks in implementing this coupling and performing efficient ICME-guided high-throughput multi-component industrial alloys discovery or process parameters optimization, are slow responses in kinetic calculations to a given set of compositions and processing conditions and the quality of a large amount of calculated thermodynamic data. Here, we employ machine learning techniques to eliminate them, including (1) intelligent corrupt data detection and re-interpolation (i.e. data purge/cleaning) to a big tabulated thermodynamic dataset based on an unsupervised learning algorithm and (2) parameterization via artificial neural networks of the purged big thermodynamic dataset into a non-linear equation consisting of base functions and parameterization coefficients. The two techniques enable the efficient linkage of high-quality data with a previously developed microstructure model. This proposed approach not only improves the model performance by eliminating the interference of the corrupt data and stability due to the boundedness and continuity of the obtained non-linear equation but also dramatically reduces the running time and demand for computer physical memory simultaneously. The high computational robustness, efficiency, and accuracy, which are prerequisites for high-throughput computing, are verified by a series of case studies on multi-component aluminum, steel, and high-entropy alloys. The proposed data purge and parameterization methods are expected to apply to various microstructure simulation approaches or to bridging the multi-scale simulation where handling a large amount of input data is required. It is concluded that machine learning is a valuable tool in fueling the development of ICME and high throughput materials simulations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.017",
        "scimago_value": "0,757"
    },
    {
        "issnkey": "15708268",
        "isbn": null,
        "journal": "Journal of Web Semantics",
        "publisher": null,
        "title": "towardsthenextgenerationofthelinkedgeodataprojectusingvirtualknowledgegraphs",
        "booktitle": null,
        "doi": "10.1016/j.websem.2021.100662",
        "author": [
            "Ding, Linfang",
            "Xiao, Guohui",
            "Pano, Albulen",
            "Stadler, Claus",
            "Calvanese, Diego"
        ],
        "keywords": [
            "LinkedGeoData, Virtual knowledge graph, GeoSPARQL, Ontop, OpenStreetMap"
        ],
        "abstract": "With the advancement of Semantic Technologies, large geospatial data sources have been increasingly published as Linked data on the Web. The LinkedGeoData project is one of the most prominent such projects to create a large knowledge graph from OpenStreetMap (OSM) with global coverage and interlinking of other data sources. In this paper, we report on the ongoing effort of exposing the relational database in LinkedGeoData as a SPARQL endpoint using Virtual Knowledge Graph (VKG) technology. Specifically, we present two realizations of VKGs, using the two systems Sparqlify and Ontop. In order to improve compliance with the OGC GeoSPARQL standard, we have implemented GeoSPARQL support in Ontop v4. Moreover, we have evaluated the VKG-powered LinkedGeoData in the test areas of Italy and Germany. Our experiments demonstrate that such system supports complex GeoSPARQL queries, which confirms that query answering in the VKG approach is efficient.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,502"
    },
    {
        "issnkey": "22145796",
        "isbn": null,
        "journal": "Big Data Research",
        "publisher": null,
        "title": "datapricedeterminantsbasedonahedonicpricingmodel",
        "booktitle": null,
        "doi": "10.1016/j.bdr.2021.100249",
        "author": [
            "Liang, Ji",
            "Yuan, Chunhui"
        ],
        "keywords": [
            "Data market, Pricing management, Hedonic, Data pricing model"
        ],
        "abstract": "Data have become an emerging factor of production; the development of the data trading market is accelerating, but data pricing remains in the initial exploration stage. Based on hedonic price theory, this paper analyzes the factors influencing data prices. From the perspective of data transactions, this paper proposes construction of a data price characteristic index system from three aspects: data objects, data demanders and data suppliers. An empirical analysis is conducted based on 1010 data commodity sample data points from the Jingdong Vientiane platform. Through testing and estimation, it is considered that the hedonic price models can be fitted by a logarithmic function, taken as the function form of data hedonic price analysis. According to the analysis of the sample data sources from the data market, the number of free trials, data specification and quantity, data attention (number of data visits), data demand (sales volume), and preferential policies for businesses are negatively correlated with data price. Data evaluation satisfaction (number of collected users), request parameters (business), return parameters (business), data range, and response speed are positively correlated with data price. In addition, further analysis is made on the reason of the processing complexity variable of the data algorithm which does not conform to the expected estimation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.578",
        "scimago_value": "0,565"
    },
    {
        "issnkey": "0740624x",
        "isbn": null,
        "journal": "Government Information Quarterly",
        "publisher": null,
        "title": "implicationsoftheuseofartificialintelligenceinpublicgovernanceasystematicliteraturereviewandaresearchagenda",
        "booktitle": null,
        "doi": "10.1016/j.giq.2021.101577",
        "author": [
            "Zuiderwijk, Anneke",
            "Chen, Yu-Che",
            "Salem, Fadi"
        ],
        "keywords": [
            "Public governance, Artificial intelligence, Artificial intelligence for government, Public sector, Digital government, Systematic literature review, Research agenda"
        ],
        "abstract": "To lay the foundation for the special issue that this research article introduces, we present 1) a systematic review of existing literature on the implications of the use of Artificial Intelligence (AI) in public governance and 2) develop a research agenda. First, an assessment based on 26 articles on this topic reveals much exploratory, conceptual, qualitative, and practice-driven research in studies reflecting the increasing complexities of using AI in government \u2013 and the resulting implications, opportunities, and risks thereof for public governance. Second, based on both the literature review and the analysis of articles included in this special issue, we propose a research agenda comprising eight process-related recommendations and seven content-related recommendations. Process-wise, future research on the implications of the use of AI for public governance should move towards more public sector-focused, empirical, multidisciplinary, and explanatory research while focusing more on specific forms of AI rather than AI in general. Content-wise, our research agenda calls for the development of solid, multidisciplinary, theoretical foundations for the use of AI for public governance, as well as investigations of effective implementation, engagement, and communication plans for government strategies on AI use in the public sector. Finally, the research agenda calls for research into managing the risks of AI use in the public sector, governance modes possible for AI use in the public sector, performance and impact measurement of AI use in government, and impact evaluation of scaling-up AI usage in the public sector.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03062619",
        "isbn": null,
        "journal": "Applied Energy",
        "publisher": null,
        "title": "datadrivenframeworkforlargescalepredictionofchargingenergyinelectricvehicles",
        "booktitle": null,
        "doi": "10.1016/j.apenergy.2020.116175",
        "author": [
            "Zhao, Yang",
            "Wang, Zhenpo",
            "Shen, Zuo-Jun",
            "Sun, Fengchun"
        ],
        "keywords": [
            "Charging energy, Large-scale prediction, Machine learning, Electric vehicle"
        ],
        "abstract": "Large-scale and high-precision predictions of the charging energy required for electric vehicles (EVs) are essential to ensure the safety of EVs and provide reliable inputs for grid-load calculations. However, the complex and dynamic operating conditions of EVs make it challenging to accurately predict the charging energy under real-world conditions, especially for large-scale EV utilization. In this study, a novel data-driven framework for large-scale charging energy predictions is developed by individually controlling the strongly linear and weakly nonlinear contributions. The proposed framework concurrently addresses the overfitting of nonlinear networks using a low proportion of training data as well as the poorly descriptive ability of linear networks under complex environments. For each charging session, the charging energy predictions appropriately account for important factors such as the variations in the state of charge (SOC) of the battery, ambient temperatures, charging rates, and total driving distances. The results suggest that, compared with existing prediction models (such as the random forest, xgboost, and neural network), the proposed framework persists with evidently higher accuracy and stability over a wide range of the ratio between the number of EVs used for testing and training; its mean absolute percentage error (MAPE) is maintained at 2.5\u20133.8% when the ratio ranges from 0.1 to 1000. The proposed models can be further utilized for cloud-based battery diagnoses and large-scale forecasting of the energy demands of EVs.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.746",
        "scimago_value": "3,035"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "stepstowardsanhealthcareinformationmodelbasedonopenehr",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.04.015",
        "author": [
            "Oliveira, Daniela",
            "Miranda, Rui",
            "Hak, Francini",
            "Abreu, Nuno",
            "Leuschner, Pedro",
            "Abelha, Ant\u00f3nio",
            "Machado, Jos\u00e9"
        ],
        "keywords": [
            "Electronic Health Record, Healthcare Information Systems, OpenEHR, Clinical Information model, Reference Model"
        ],
        "abstract": "During COVID-19 pandemic crisis, healthcare institutions globally were experiencing a VUCA - Volatile, Uncertain, Complex, and Ambiguous - environment. Effcient clinical and administrative management had never been so emergent. To achieve this goal, different components of the Healthcare Information System (HIS) must cooperate and interoperate flawlessly. Data standardization is a necessary step towards normalization and interoperability between existing Legacy Systems (LSs), and provides for longitudinal, highly reliable and persistent Electronic Health Records (EHRs). The openEHR standard was chosen for its overall dual domain architecture, where the more dynamic clinical information model may evolve independently from the relatively stable Reference Model (RM). Its Information Model (IM) comprises demographic, administrative and clinical systems. Critical clinical terms have been aligned to the FHIR HL7 standard, as to further support interoperability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "13675788",
        "isbn": null,
        "journal": "Annual Reviews in Control",
        "publisher": null,
        "title": "robustnessofaibasedprognosticandsystemshealthmanagement",
        "booktitle": null,
        "doi": "10.1016/j.arcontrol.2021.04.001",
        "author": [
            "Khan, Samir",
            "Tsutsumi, Seiji",
            "Yairi, Takehisa",
            "Nakasuka, Shinichi"
        ],
        "keywords": [
            "Prognostics and system health management, Robust AI, Machine learning, PHM, Fault diagnosis"
        ],
        "abstract": "Prognostic and systems Health Management (PHM) is an integral part of a system. It is used for solving reliability problems that often manifest due to complexities in design, manufacturing, operating environment and system maintenance. For safety-critical applications, using a model-based development process for complex systems might not always be ideal but it is equally important to establish the robustness of the solution. The information revolution has allowed data-driven methods to diffuse within this field to construct the requisite process (or system models) to cope with the so-called big data phenomenon. This is supported by large datasets that help machine-learning models achieve impressive accuracy. AI technologies are now being integrated into many PHM related applications including aerospace, automotive, medical robots and even autonomous weapon systems. However, with such rapid growth in complexity and connectivity, a systems\u2019 behaviour is influenced in unforeseen ways by cyberattacks, human errors, working with incorrect or incomplete models and even adversarial phenomena. Many of these models depend on the training data and how well the data represents the test data. These issues require fine-tuning and even retraining the models when there is even a small change in operating conditions or equipment. Yet, there is still ambiguity associated with their implementation, even if the learning algorithms classify accordingly. Uncertainties can lie in any part of the AI-based PHM model, including in the requirements, assumptions, or even in the data used for training and validation. These factors lead to sub-optimal solutions with an open interpretation as to why the requirements have not been met. This warrants the need for achieving a level of robustness in the implemented PHM, which is a challenging task in a machine learning solution. This article aims to present a framework for testing the robustness of AI-based PHM. It reviews some key milestones achieved in the AI research community to deal with three particular issues relevant for AI-based PHM in safety-critical applications: robustness to model errors, robustness to unknown phenomena and empirical evaluation of robustness during deployment. To deal with model errors, many techniques from probabilistic inference and robust optimisation are often used to provide some robustness guarantee metric. In the case of unknown phenomena, techniques include anomaly detection methods, using causal models, the construction of ensembles and reinforcement learning. It elicits from the authors\u2019 work on fault diagnostics and robust optimisation via machine learning techniques to offer guidelines to the PHM research community. Finally, challenges and future directions are also examined; on how to better cope with any uncertainties as they appear during the operating life of an asset.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.091",
        "scimago_value": "1,780"
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "learninghierarchicalfacerepresentationtoenhancehciamongmedicalrobots",
        "booktitle": null,
        "doi": "10.1016/j.future.2020.11.007",
        "author": [
            "Sun, Dianmin",
            "Zhao, Honghua",
            "Song, Tao",
            "Liu, Aiqin",
            "Cheng, Jinling",
            "Liu, Zhi",
            "Zhao, Xin"
        ],
        "keywords": [
            "Face recognition, Deep learning, Hierarchical neural network, Medical robot HCI"
        ],
        "abstract": "In this paper, we propose a hierarchical framework for face recognition by learning deep representation. In order to exploit key patches for face recognition, we separate the entire image into several patches including eyes, nose, and mouth. A binary facegrid is generated to indicate the accurate position of the key patches in face image. The patches are fed into the hierarchical framework to learn the deep representation of the image. We leverage the PCA and SVM method for face recognition. Our face representation can enhance many medical robot applications. Comprehensive experiments have demonstrated that our proposed method can effectively recognize real human faces from fake samples.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "14740346",
        "isbn": null,
        "journal": "Advanced Engineering Informatics",
        "publisher": null,
        "title": "understandingdigitaltransformationinadvancedmanufacturingandengineeringabibliometricanalysistopicmodelingandresearchtrenddiscovery",
        "booktitle": null,
        "doi": "10.1016/j.aei.2021.101428",
        "author": [
            "Lee, Ching-Hung",
            "Liu, Chien-Liang",
            "Trappey, Amy",
            "Mo, John",
            "Desouza, Kevin"
        ],
        "keywords": [
            "Digital transformation, Advanced manufacturing and engineering, Bibliometric analysis, Topic modeling, Systematic review"
        ],
        "abstract": "Digital transformation (DT) is the process of combining digital technologies with sound business models to generate great value for enterprises. DT intertwines with customer requirements, domain knowledge, and theoretical and empirical insights for value propagations. Studies of DT are growing rapidly and heterogeneously, covering the aspects of product design, engineering, production, and life-cycle management due to the fast and market-driven industrial development under Industry 4.0. Our work addresses the challenge of understanding DT trends by presenting a machine learning (ML) approach for topic modeling to review and analyze advanced DT technology research and development. A systematic review process is developed based on the comprehensive DT in manufacturing systems and engineering literature (i.e., 99 articles). Six dominant topics are identified, namely smart factory, sustainability and product-service systems, construction digital transformation, public infrastructure-centric digital transformation, techno-centric digital transformation, and business model-centric digital transformation. The study also contributes to adopting and demonstrating the ML-based topic modeling for intelligent and systematic bibliometric analysis, particularly for unveiling advanced engineering research trends through domain literature.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.603",
        "scimago_value": "1,107"
    },
    {
        "issnkey": "01681699",
        "isbn": null,
        "journal": "Computers and Electronics in Agriculture",
        "publisher": null,
        "title": "anautomatedzizaniaqualitygradingmethodbasedondeepclassificationmodel",
        "booktitle": null,
        "doi": "10.1016/j.compag.2021.106004",
        "author": [
            "Cao, Jingjun",
            "Sun, Tan",
            "Zhang, Wenrong",
            "Zhong, Ming",
            "Huang, Bo",
            "Zhou, Guomin",
            "Chai, Xiujuan"
        ],
        "keywords": [
            "Zizania, Automatic grading, Convolutional neural network, Deep learning, Object classification"
        ],
        "abstract": "Zizania, one of aquatic vegetables, needs to be graded before entering market for assuring the product quality. However, it is time-consuming, tedious, labor-intensive, inaccurate and expensive to assess qualitatively and grade zizanias manually. This paper gives an effective solution to automatically classify fresh zizania into two categories, high quality and defective quality, by using the deep learnt features from the appearances. A new architecture of convolutional neural network, called LightNet, has been proposed and described. Specifically, it is composed of many compressed blocks, which is designed to reduce the computation complexity mainly by converting serial down-sampling and convolution operation to parallel structure. We evaluate the proposed architecture on the zizania image dataset collected by ourselves and integrate the algorithm in the automatic grading device. The experimental results show that the accuracy rate achieves 95.62% and the speed of inference quality is around 47 ms per zizania image. The proposed LightNet for automate classification has less parameters and lower computation complexity than popular networks, while maintaining the comparable accuracy in the task of grading zizanias. It obtains 99.31% accuracy in the task of grading apples. The result proves that it can be extended to other tasks about classification.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.565",
        "scimago_value": "1,208"
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "technoeconomicanalysisforbiomasssupplychainastateoftheartreview",
        "booktitle": null,
        "doi": "10.1016/j.rser.2020.110164",
        "author": [
            "Lo, Shirleen",
            "How, Bing",
            "Leong, Wei",
            "Teng, Sin",
            "Rhamdhani, Muhammad",
            "Sunarso, Jaka"
        ],
        "keywords": [
            "Biomass properties, Optimization, Supply chain risks, Supply chain uncertainties, Bioenergy"
        ],
        "abstract": "Given the increasing risk of climate change and depletion of non-renewable energy sources, countries around the world have been looking into energy profile diversification whereby biomass represents one of the most appealing alternatives for energy production feedstock. To attract interest and more investment from industry players into biomass-based industries, comprehensive techno-economic analysis has to be performed. In addition, various uncertainties related to supply chain such as biomass attainability, demand variation, and material price fluctuation, have to be considered in the evaluation to yield more accurate and reliable feasibility estimation. This review paper aims to: (i) provide an overview on the different types of methods or approaches used in the feasibility evaluation of biomass-based industries from the techno-economic point of view, and (ii) outline the supply chain uncertainties that should be incorporated into the evaluation model using a Malaysian case study to illustrate the impact of these uncertainties. Apart from that, some of the unquantifiable uncertainties and risks are critically reviewed in this paper. It was found that 78 % of the reviewed articles opted for mathematical modelling evaluation method with the majority leaning towards mathematical modelling with optimization (i.e., deterministic and stochastic optimization). Furthermore, only a minority had performed stochastic evaluation that incorporates biomass supply chain uncertainties. This review discussed six quantifiable uncertainties, include: (i) biomass availability, (ii) biomass quality, (iii) transportation cost, (iv) market demand, (v) fluctuation of pricing, and (vi) wages of workers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "avalveclosingbodyasacentralsensoryutilizablecomponent",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.05.018",
        "author": [
            "Kraus, Benjamin",
            "Schmitt, Florian",
            "Steffan, Kay-Eric",
            "Kirchner, Eckhard"
        ],
        "keywords": [
            "Sensing machine elements (SME), Cyber-physical-system (CPS), Sensory-utilizable machine elements (SuME), Product development"
        ],
        "abstract": "Enhancing traditional machine elements by using their embodied physical effects as an additional sensorial function to create so called sensing machine elements (SME) has been gaining momentum as a novel research topic. Utilizing those machine elements has the potential to enable a wide digitization regarding the creation of cyber-physical systems (CPS) in all mechanical engineering fields, since the basic components of every machine are being enhanced. These basic machine elements are often located close to the point of interest and therefore enable the user to collect data directly within the process itself. This design idea for the location of sensors or sensory functions is called in-situ measurement. While the enhancement of machine elements to SMEs seems promising the question arises if not other components and their physical characteristics can be used as sensors themselves. Ideally, to minimize the distance between the desired target variable and the actual measurand, those sensory-utilizable components are integral components for the main function of the machine. One of those central components is the closing body of a valve. During the concept phase of a new valve a concept was chosen while also regarding the future possible integration of sensory functions. With this in mind it was possible to enhance the chosen concept doing a small tweak during the design phase with a sensory function, that detects the current position of the valves closing body, using its physical properties. The basic idea behind the concept and design phase as well as the gathered data from a first prototype is presented in this paper.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "26663899",
        "isbn": null,
        "journal": "Patterns",
        "publisher": null,
        "title": "appytersturningjupyternotebooksintodatadrivenwebapps",
        "booktitle": null,
        "doi": "10.1016/j.patter.2021.100213",
        "author": [
            "Clarke, Daniel",
            "Jeon, Minji",
            "Stein, Daniel",
            "Moiseyev, Nicole",
            "Kropiwnicki, Eryk",
            "Dai, Charles",
            "Xie, Zhuorui",
            "Wojciechowicz, Megan",
            "Litz, Skylar",
            "Hom, Jason",
            "Evangelista, John",
            "Goldman, Lucas",
            "Zhang, Serena",
            "Yoon, Christine",
            "Ahamed, Tahmid",
            "Bhuiyan, Samantha",
            "Cheng, Minxuan",
            "Karam, Julie",
            "Jagodnik, Kathleen",
            "Shu, Ingrid",
            "Lachmann, Alexander",
            "Ayling, Sam",
            "Jenkins, Sherry",
            "Ma'ayan, Avi"
        ],
        "keywords": [
            "workflow, notebooks, big data, data analysis, data visualization, RNA-seq, scRNA-seq, machine learning, TCGA, gene set enrichment analysis"
        ],
        "abstract": "Summary Jupyter Notebooks have transformed the communication of data analysis pipelines by facilitating a modular structure that brings together code, markdown text, and interactive visualizations. Here, we extended Jupyter Notebooks to broaden their accessibility with Appyters. Appyters turn Jupyter Notebooks into fully functional standalone web-based bioinformatics applications. Appyters present to users an entry form enabling them to upload their data and set various parameters for a multitude of data analysis workflows. Once the form is filled, the Appyter executes the corresponding notebook in the cloud, producing the output without requiring the user to interact directly with the code. Appyters were used to create many bioinformatics web-based reusable workflows, including applications to build customized machine learning pipelines, analyze omics data, and produce publishable figures. These Appyters are served in the Appyters Catalog at https://appyters.maayanlab.cloud. In summary, Appyters enable the rapid development of interactive web-based bioinformatics applications.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03062619",
        "isbn": null,
        "journal": "Applied Energy",
        "publisher": null,
        "title": "novelshorttermsolarradiationhybridmodellongshorttermmemorynetworkintegratedwithrobustlocalmeandecomposition",
        "booktitle": null,
        "doi": "10.1016/j.apenergy.2021.117193",
        "author": [
            "{Ngoc-Lan Huynh}, Anh",
            "Deo, Ravinesh",
            "Ali, Mumtaz",
            "Abdulla, Shahab",
            "Raj, Nawin"
        ],
        "keywords": [
            "Short-term solar radiation prediction, Robust Local Mean Decomposition, Deep Learning"
        ],
        "abstract": "Data-intelligent algorithms tailored for short-term energy forecasting can generate meaningful information on the future variability of solar energy developments. Traditional forecasting methods find it relatively difficult to obtain a reliable solar energy monitoring system because of the inherent nonlinearities in solar radiation and the related atmospheric input variables to any forecasting system. This paper proposes a new artificial intelligence-based hybrid model by employing the robust version of local mean decomposition (RLMD) and Long Short-term Memory (LSTM) network denoted as RLMD-LSTM. The objective model (i.e., RLMD-LSTM) is built near real-time, half-hourly ground-based solar radiation dataset for the solar rich, metropolitan study sites in Vietnam with all of the forecasting results being benchmarked through classical modelling approaches (i.e., Support Vector Regression SVR, Long Short-term Memory LSTM, Multivariate Adaptive Regression Spline MARS, Persistence) as well as the other alternative hybrid methods (i.e., RLMD-MARS, RLMD-Persistence and RLMD-SVR). Verified by statistical metrics and visual infographics, the present results demonstrate that the proposed model can generate satisfactory predictions, outperforming several counterpart methods. The predictive performance is stable for all study sites that the root-mean-square error remained profoundly lower for RLMD-LSTM (19\u201320%) compared with RLMD-MARS (20\u201321%), RLMD-SVR (29\u201335%), RLMD- Persistence (29\u201351%), LSTM (25\u201348%), MARS (21\u201351%) and SVR (23\u201385%), Persistence (29\u201351%). The Legates and McCabe\u2019s Index, yielding a value of approximately 0.7988\u20130.9256 for RLMD-LSTM compared with 0.765\u20130.8142, 0.4917\u20130.5711, 0.6900\u20130.7482, 0.6914\u20130.7646, 0.4349\u20130.7170 respectively, for the RLMD-MARS, RLMD-SVR, RLMD-Persistence, LSTM, MARS, SVR, Persistence models, also confirms the outstanding performance of RLMD-LSTM model. Accordingly, the study ascertains that the newly designed approach can be a potential candidate for real-time energy management, renewable energy integration into a power grid and other decisions to optimise the overall system's scheduling and performance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.746",
        "scimago_value": "3,035"
    },
    {
        "issnkey": "26669560",
        "isbn": null,
        "journal": "Neuroimage: Reports",
        "publisher": null,
        "title": "neuroimagereportsanewmemberoftheneuroimagefamilyembracingnegativefindingsreplicationstudiesandregisteredreports",
        "booktitle": null,
        "doi": "10.1016/j.ynirp.2020.100001",
        "author": [
            "Forstmann, Birte",
            "Chambers, Christopher",
            "Cohen, Michael",
            "Hartwigsen, Gesa",
            "Kochunov, Peter",
            "{Van De Ville}, Dimitri",
            "Yan, Chao-Gan"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "24686018",
        "isbn": null,
        "journal": "IFAC Journal of Systems and Control",
        "publisher": null,
        "title": "semisuperviseddatamodelingandanalyticsintheprocessindustrycurrentresearchstatusandchallenges",
        "booktitle": null,
        "doi": "10.1016/j.ifacsc.2021.100150",
        "author": [
            "Ge, Zhiqiang"
        ],
        "keywords": [
            "Semi-supervised data, Data driven modeling, Machine learning, Process data analytics"
        ],
        "abstract": "Semi-supervised data are quite common in the process industry, which has caught much attention in recent years. The semi-supervised feature of process data not only has a great impact on data mining and analytics, but also matters in feature extraction and knowledge discovery in the process. In this paper, the framework of semi-supervised data modeling and applications is formulated for the process industry. First, the semi-supervised data structure is introduced, including the causes of semi-supervised data structure, the main feature of the semi-supervised data, and its effects on data modeling and applications in the process industry. Second, detailed research statuses on semi-supervised data modeling and applications in the process industry are illustrated, with introductions of some representative approaches. Third, several challenges and promising issues on modeling and application of semi-supervised data are discussed and highlighted for future research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "25900617",
        "isbn": null,
        "journal": "Progress in Disaster Science",
        "publisher": null,
        "title": "theroleofdataandinformationqualityduringdisasterresponsedecisionmaking",
        "booktitle": null,
        "doi": "10.1016/j.pdisas.2021.100202",
        "author": [
            "Jayawardene, Vimukthi",
            "Huggins, Thomas",
            "Prasanna, Raj",
            "Fakhruddin, Bapon"
        ],
        "keywords": [
            "Disaster response, Decision making, Data and information quality"
        ],
        "abstract": "Massive amounts of data and information are exchanged during the response phase of disaster management. A large body of contemporary research has indicated that most of these data and information have severe quality related concerns, meaning that they may not be suitable for critical decision-making. The current paper addresses these issues by identifying how certain features of data and information quality function, to support specific, naturalistic decision-making processes during disaster response. These functions are used to revise and consolidate pre-existing definitions of data and information quality, for use in further disaster response research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820714-7",
        "journal": null,
        "publisher": "Gulf Professional Publishing",
        "title": "chapter5datamanagementfromthedcstothehistorian",
        "booktitle": "Machine Learning and Data Science in the Oil and Gas Industry",
        "doi": "10.1016/B978-0-12-820714-7.00005-4",
        "author": [
            "Crompton, Jim"
        ],
        "keywords": [
            "data management, oil and gas industry, sensor data, digital operations, simulation, automation"
        ],
        "abstract": "The oil and gas industry produces great volumes of data on an ongoing basis. This data must be acquired, stored, curated, and made available properly for data science to be able to take place. This process starts at the sensor and continues via systems such as the control system and the historian. In this journey, information and operational technologies converge. Sensor data is acquired and transmitted through various systems to arrive at the control system, which then makes it available through various protocols for consumption by analysis software. Historians save the data and make it available to the human user as diagrams. Documents and simulation data can also be integrated into the picture. This complex landscape is discussed in this chapter.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "conceptforenablingcustomerorienteddataanalyticsviaintegrationofproductionprocessimprovementmethodsanddatasciencemethods",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.091",
        "author": [
            "Morlock, Friedrich",
            "Bo\u00dflau, Mario"
        ],
        "keywords": [
            "Industry 4.0, Data Analytics, Lean Management, Process Improvement"
        ],
        "abstract": "Production companies are facing the major challenge of digitization. New technical developments enable new optimization potentials in production that can be leveraged in a highly competitive market. Data analysis is a good example of this, allowing large amounts of data to be used to support production in optimization projects. For data analysis the CRISP-DM approach has become generally accepted in science and practice. This process model offers a good support for data science projects with useful data analysis methods and tools. In practice, however, it can often be observed that aspects such as customer-oriented project definition and the integration of process knowledge in data science projects are difficult to achieve. This paper will present a solution for that challenge. The combination of process optimization methods from the Lean Management and Six Sigma toolbox as well as project management methods for each phase of the CRISP-DM approach support data science projects to customize the project definition and integrate process knowledge.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "22142096",
        "isbn": null,
        "journal": "Vehicular Communications",
        "publisher": null,
        "title": "anodeoptimizationmodelbasedonthespatiotemporalcharacteristicsoftheroadnetworkforurbantrafficmobilecrowdsensing",
        "booktitle": null,
        "doi": "10.1016/j.vehcom.2021.100383",
        "author": [
            "Yu, Haiyang",
            "Fang, Jing",
            "Liu, Shuai",
            "Ren, Yilong",
            "Lu, Jian"
        ],
        "keywords": [
            "Urban traffic mobile crowd sensing, Spatiotemporal characteristics, Dynamic accessibility, Node selection, Utility maximization"
        ],
        "abstract": "Urban traffic mobile crowd sensing (Urban Traffic MCS) has emerged as a new effective paradigm of sensing and collecting data by means of vehicles equipped with various sensors in urban areas. In an Urban Traffic MCS system, the utility directly reflects the effectiveness of the sensing results, and it is essential to maximize the utility of the collected data. Some studies have shown that utility can be effectively improved by optimizing the selection of sensing nodes. However, most previous research has considered only the coverage and critical links of the road network while neglecting the spatiotemporal characteristics of the traffic flow, although the latter are essential for node selection optimization and significantly impact the utility of Urban Traffic MCS. Therefore, most existing methods are not suitable for Urban Traffic MCS systems. In this paper, a novel node optimization model based on the spatiotemporal characteristics of the road network is proposed. First, we introduce the Urban Traffic MCS system, and dynamic accessibility is introduced to describe the spatiotemporal characteristics of the whole road network. Then, the utility function for Urban Traffic MCS is redefined based on the effective coverage and dynamic accessibility to consider both the topological structure of the road network and the dynamic changes in traffic flow. On this basis, a node selection method with the aim of maximizing the utility of Urban Traffic MCS is proposed. Finally, the results of simulation experiments show that the node selection method in this paper can effectively achieve increased utility for an Urban Traffic MCS system.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.910",
        "scimago_value": "1,298"
    },
    {
        "issnkey": "15517144",
        "isbn": null,
        "journal": "Contemporary Clinical Trials",
        "publisher": null,
        "title": "theuseofrealworlddataevidenceinregulatorysubmissions",
        "booktitle": null,
        "doi": "10.1016/j.cct.2021.106521",
        "author": [
            "Song, Fuyu",
            "Zang, Chenxuan",
            "Ma, Xinyi",
            "Hu, Sifan",
            "Sun, Qiqing",
            "Chow, Shein-Chung",
            "Sun, Hongqiang"
        ],
        "keywords": [
            "Real-world data, Real-world evidence, Substantial evidence, Gap analysis, Fit-for-purpose, Regulatory submission"
        ],
        "abstract": "The 21st Century Cures Act passed by the United States (US) Congress in December 2016 requires the US Food and Drug Administration (FDA) shall establish a program to evaluate the potential use of real-world evidence (RWE) which is generated from real-world data (RWD) to (i) support approval of new indication for a drug approved under section 505 (c) and (ii) satisfy post-approval study requirements. RWE offers the opportunities to develop robust evidence using high-quality data and sophisticated methods for producing causal-effect estimates regardless randomization is feasible. In this article, we have demonstrated that the assessment of treatment effect (RWE) based on RWD could be biased due to the potential selection and information biases of RWD. Although fit-for-purpose RWE may meet regulatory standards under certain assumptions, it is not the same as substantial evidence (current regulatory standard in support of approval of regulatory submission). In practice, it is then suggested that when there are gaps between fit-for-purpose RWE and substantial evidence, we should make efforts to fill these gaps based on a comprehensive evaluation of the treatment effect. We also review two RWE examples to show some potential use of RWE in clinical studies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15740137",
        "isbn": null,
        "journal": "Computer Science Review",
        "publisher": null,
        "title": "asurveyofprivacypreservingmechanismsforheterogeneousdatatypes",
        "booktitle": null,
        "doi": "10.1016/j.cosrev.2021.100403",
        "author": [
            "Cunha, Mariana",
            "Mendes, Ricardo",
            "Vilela, Jo\u00e3o"
        ],
        "keywords": [
            "Privacy, Privacy taxonomy, Privacy-preserving mechanisms, Heterogeneous data types, Privacy tools"
        ],
        "abstract": "Due to the pervasiveness of always connected devices, large amounts of heterogeneous data are continuously being collected. Beyond the benefits that accrue for the users, there are private and sensitive information that is exposed. Therefore, Privacy-Preserving Mechanisms (PPMs) are crucial to protect users\u2019 privacy. In this paper, we perform a thorough study of the state of the art on the following topics: heterogeneous data types, PPMs, and tools for privacy protection. Building from the achieved knowledge, we propose a privacy taxonomy that establishes a relation between different types of data and suitable PPMs for the characteristics of those data types. Moreover, we perform a systematic analysis of solutions for privacy protection, by presenting and comparing privacy tools. From the performed analysis, we identify open challenges and future directions, namely, in the development of novel PPMs.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.872",
        "scimago_value": "1,646"
    },
    {
        "issnkey": "00457906",
        "isbn": null,
        "journal": "Computers & Electrical Engineering",
        "publisher": null,
        "title": "prognosisanalysisofthickdataclusteringheartdiseasesriskgroupscasestudy",
        "booktitle": null,
        "doi": "10.1016/j.compeleceng.2021.107187",
        "author": [
            "Fiaidhi, J.",
            "Mohammed, S."
        ],
        "keywords": [
            "Thick Data, Prognosis Analysis, Fuzzy Clustering, Small Datasets, Healthcare Data, Risk Analysis, Machine Learning"
        ],
        "abstract": "Analyzing clinical data differs from other machine learning data analysis as most of the clinical data are relatively small requiring more qualitative techniques to bring focus to the context and then to predict important indicators like the patient risk in developing heart disease. The strength of qualitative analytics lies in data thickness as they can work on small samples and corpuses (\u201csmall data\u201d). However, working with thick data analytics requires involving patient characteristics (e.g. socioeconomic status, family background, working conditions, social support, psycho-social characteristics, lifestyle risk factors, age group, gender and social capital) and their weights in a particular clinical practice. Therefore, the role of patient characteristics is not only a dominant factor in thick data analytics but it is also linked to predicting the prognosis of patient cases. A Fuzzy C-Means algorithm is presented as technique for prognostic predictions to identify risk groups associated with Cardiovascular Disease (CVD) conditions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,630"
    },
    {
        "issnkey": "0740624x",
        "isbn": null,
        "journal": "Government Information Quarterly",
        "publisher": null,
        "title": "identifyingthevalueofdataanalyticsinthecontextofgovernmentsupervisioninsightsfromthecustomsdomain",
        "booktitle": null,
        "doi": "10.1016/j.giq.2020.101496",
        "author": [
            "Rukanova, Boriana",
            "Tan, Yao-Hua",
            "Slegt, Micha",
            "Molenhuis, Marcel",
            "{van Rijnsoever}, Ben",
            "Migeotte, Jonathan",
            "Labare, Mathieu",
            "Plecko, Krunoslav",
            "Caglayan, Bora",
            "Shorten, Gavin",
            "{van der Meij}, Otis",
            "Post, Suzanne"
        ],
        "keywords": [
            "Framework, Data analytics, Value, Government, Supervision, eCommerce, Capabilities, Collective capability"
        ],
        "abstract": "eCommerce, Brexit, new safety and security concerns are only a few examples of the challenges that government organisations, in particular customs administrations, face today when controlling goods crossing borders. To deal with the enormous volumes of trade customs administrations rely more and more on information technology (IT) and risk assessment, and are starting to explore the possibilities that data analytics (DA) can offer to support their supervision tasks. Driven by customs as our empirical domain, we explore the use of DA to support the supervision role of government. Although data analytics is considered to be a technological breakthrough, there is so far only a limited understanding of how governments can translate this potential into actual value and what are barriers and trade-offs that need to be overcome to lead to value realisation. The main question that we explore in this paper is: How to identify the value of DA in a government supervision context, and what are barriers and trade-offs to be considered and overcome in order to realise this value? Building on leading models from the information system (IS) literature, and by using case studies from the customs domain, we developed the Value of Data Analytics in Government Supervision (VDAGS) framework. The framework can help managers and policy-makers to gain a better understanding of the benefits and trade-offs of using DA when developing DA strategies or when embarking on new DA projects. Future research can examine the applicability of the VDAGS framework in other domains of government supervision.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03787788",
        "isbn": null,
        "journal": "Energy and Buildings",
        "publisher": null,
        "title": "adataminingbasedframeworkfortheidentificationofdailyelectricityusagepatternsandanomalydetectioninbuildingelectricityconsumptiondata",
        "booktitle": null,
        "doi": "10.1016/j.enbuild.2020.110601",
        "author": [
            "Liu, Xue",
            "Ding, Yong",
            "Tang, Hao",
            "Xiao, Feng"
        ],
        "keywords": [
            "Building energy management, Time series clustering, Decision tree, Knowledge discovery, Electricity usage pattern, Data mining"
        ],
        "abstract": "With the development of advanced information techniques, \ufeffsmart energy meters have made a considerable amount of real-time electricity consumption data available. These data provide a promising way to understand energy usage patterns and improve building energy management. However, previous studies have paid more attention to methodologies for the identification of energy usage patterns and are limited in the interpretability and applications of the patterns. In this context, this paper proposes a general data mining-based framework that can extract typical electricity load patterns (TELPs) and discover insightful information hidden in the patterns. The framework integrates multiple data mining techniques and mainly consists of three phases: data preparation, identification of TELPs and knowledge discovery in the patterns. A new clustering method with a two-step clustering analysis is proposed to identify the TELPs at the individual building level. Before clustering, five statistical features that represent the shapes of electricity load profiles are first defined to reduce the dimensions of daily electricity load profiles. The first clustering step aims at detecting outliers of daily electricity load profiles (DELPs) by using the density-based spatial clustering application with noise (DBSCAN) algorithm clustering technique, which addresses the data quality issues for electricity consumption data derived from energy consumption monitoring platforms (ECMPs). The second clustering step aims at grouping similar DELPs by means of the k-means algorithm to extract TELPs. The effectiveness of the proposed clustering method is demonstrated by a comparison with two single-step clustering techniques. Furthermore, a classification and regression tree (CART) algorithm is employed to discover insightful knowledge on TELPs and improve the interpretability of clustering results, namely, to explain the relations between dynamic influencing factors related to electricity consumption and TELPs. The proposed framework is applied to analyze the time-series electricity consumption data of three practical office buildings in Chongqing, and its effectiveness has been confirmed. A potential application of discovered knowledge is presented: early fault detection of anomalous electricity load profiles. The proposed framework can provide building managers with an efficient way to understand the characteristics of building electricity usage patterns and detect anomalies therein.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.879",
        "scimago_value": "1,737"
    },
    {
        "issnkey": "24520748",
        "isbn": null,
        "journal": "NanoImpact",
        "publisher": null,
        "title": "thenanoinformaticsknowledgecommonscapturingspatialandtemporalnanomaterialtransformationsindiversesystems",
        "booktitle": null,
        "doi": "10.1016/j.impact.2021.100331",
        "author": [
            "Amos, Jaleesia",
            "Tian, Yuan",
            "Zhang, Zhao",
            "Lowry, Greg",
            "Wiesner, Mark",
            "Hendren, Christine"
        ],
        "keywords": [
            "Database, Nanoinformatics, Nanomaterials, Environmental nanotechnology, Transformations"
        ],
        "abstract": "The empirical necessity for integrating informatics throughout the experimental process has become a focal point of the nano-community as we work in parallel to converge efforts for making nano-data reproducible and accessible. The NanoInformatics Knowledge Commons (NIKC) Database was designed to capture the complex relationship between nanomaterials and their environments over time in the concept of an \u2018Instance\u2019. Our Instance Organizational Structure (IOS) was built to record metadata on nanomaterial transformations in an organizational structure permitting readily accessible data for broader scientific inquiry. By transforming published and on-going data into the IOS we are able to tell the full transformational journey of a nanomaterial within its experimental life cycle. The IOS structure has prepared curated data to be fully analyzed to uncover relationships between observable phenomenon and medium or nanomaterial characteristics. Essential to building the NIKC database and associated applications was incorporating the researcher's needs into every level of development. We started by centering the research question, the query, and the necessary data needed to support the question and query. The process used to create nanoinformatic tools informs usability and analytical capability. In this paper we present the NIKC database, our developmental process, and its curated contents. We also present the Collaboration Tool which was built to foster building new collaboration teams. Through these efforts we aim to: 1) elucidate the general principles that determine nanomaterial behavior in the environment; 2) identify metadata necessary to predict exposure potential and bio-uptake; and 3) identify key characterization assays that predict outcomes of interest.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.316",
        "scimago_value": "1,045"
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "practicalissuesinimplementingmachinelearningmodelsforbuildingenergyefficiencymovingbeyondobstacles",
        "booktitle": null,
        "doi": "10.1016/j.rser.2021.110929",
        "author": [
            "Wang, Zeyu",
            "Liu, Jian",
            "Zhang, Yuanxin",
            "Yuan, Hongping",
            "Zhang, Ruixue",
            "Srinivasan, Ravi"
        ],
        "keywords": [
            "Machine learning, Building energy modeling, Energy efficiency, Building energy management, Practical issues, Technology diffusion"
        ],
        "abstract": "Implementing machine-learning models in real applications is crucial to achieving intelligent building control and high energy efficiency. Over the past few decades, numerous studies have attempted to explore the application of machine-learning models to building energy efficiency. However, these studies have focused on analyzing the technical feasibility and superiority of machine learning algorithms for fitting building energy-related data and have not considered methods of implementing machine learning technology in building energy efficiency applications. Therefore, this review aims to summarize the current practical issues involved in applying machine-learning models to building energy efficiency by systematically analyzing existing research findings and limitations. The paper first reviews the application status of machine learning-based building energy efficiency research by analyzing the model implementation process and summarizing the main uses of the technology in the overall building energy management life cycle. The paper then elaborates on the causes of, influences on, and potential solutions for practical issues found in the implementation and promotion of machine learning-based building energy efficiency measures. Finally, this paper discusses valuable future machine learning-based building energy efficiency research directions with regard to technology opportunity discovery, data governance, feature engineering, generalizability test, technology diffusion, and knowledge sharing. This paper will provide building researchers and practitioners with a better understanding of the current application statuses of and potential research directions for machine learning models in building energy efficiency.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "16720229",
        "isbn": null,
        "journal": "Genomics, Proteomics & Bioinformatics",
        "publisher": null,
        "title": "gapitversion3boostingpowerandaccuracyforgenomicassociationandprediction",
        "booktitle": null,
        "doi": "10.1016/j.gpb.2021.08.005",
        "author": [
            "Wang, Jiabo",
            "Zhang, Zhiwu"
        ],
        "keywords": [
            "GWAS, Genomic selection, Software, R, GAPIT"
        ],
        "abstract": "Genome-wide association study (GWAS) and genomic prediction/selection (GP/GS) are the two essential enterprises in genomic research. Due to the great magnitude and complexity of genomic and phenotypic data, analytical methods and their associated software packages are frequently advanced. GAPIT is a widely-used genomic association and prediction integrated tool as an R package. The first version was released to the public in 2012 with the implementation of the general linear model (GLM), mixed linear model (MLM), compressed MLM (CMLM), and genomic best linear unbiased prediction (gBLUP). The second version was released in 2016 with several new implementations, including enriched CMLM (ECMLM) and settlement of MLMs under progressively exclusive relationship (SUPER). All the GWAS methods are based on the single-locus test. For the first time, in the current release of GAPIT, version 3 implemented three multi-locus test methods, including multiple loci mixed model (MLMM), fixed and random model circulating probability unification (FarmCPU), and Bayesian-information and linkage-disequilibrium iteratively nested keyway (BLINK). Additionally, two GP/GS methods were implemented based on CMLM (named compressed BLUP; cBLUP) and SUPER (named SUPER BLUP; sBLUP). These new implementations not only boost statistical power for GWAS and prediction accuracy for GP/GS, but also improve computing speed and increase the capacity to analyze big genomic data. Here, we document the current upgrade of GAPIT by describing the selection of the recently developed methods, their implementations, and potential impact. All documents, including source code, user manual, demo data, and tutorials, are freely available at the GAPIT website (http://zzlab.net/GAPIT).",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,114"
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "recentadvancesonindustrialdatadrivenenergysavingsdigitaltwinsandinfrastructures",
        "booktitle": null,
        "doi": "10.1016/j.rser.2020.110208",
        "author": [
            "Teng, Sin",
            "Tou\u0161, Michal",
            "Leong, Wei",
            "How, Bing",
            "Lam, Hon",
            "M\u00e1\u0161a, V\u00edt\u011bzslav"
        ],
        "keywords": [
            "Digital twins, Data-driven energy savings, Artificial intelligence (AI), Blockchain, Internet of things (IoT), Cyber-physical production systems (CPPS)"
        ],
        "abstract": "Data-driven models for industrial energy savings heavily rely on sensor data, experimentation data and knowledge-based data. This work reveals that too much research attention was invested in making data-driven models, as supposed to ensuring the quality of industrial data. Furthermore, the true challenge within the Industry 4.0 is with data communication and infrastructure problems, not so significantly on developing modelling techniques. Current methods and data infrastructures for industrial energy savings were comprehensively reviewed to showcase the potential for a more accurate and effective digital twin-based infrastructure for the industry. With a few more development in enabling technologies such as 5G developments, Internet of Things (IoT) standardization, Artificial Intelligence (AI) and blockchain 3.0 utilization, it is but a matter of time that the industry will transition towards the digital twin-based approach. Global government efforts and policies are already inclining towards leveraging better industrial energy efficiencies and energy savings. This provides a promising future for the development of a digital twin-based energy-saving system in the industry. Foreseeing some potential challenges, this paper also discusses the importance of symbiosis between researchers and industrialists to transition from traditional industry towards a digital twin-based energy-saving industry. The novelty of this work is the current context of industrial energy savings was extended towards cutting-edge technologies for Industry 4.0. Furthermore, this work proposes to standardize and modularize industrial data infrastructure for smart energy savings. This work also serves as a concise guideline for researchers and industrialists who are looking to implement advanced energy-saving systems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "01441647",
        "isbn": null,
        "journal": "Transport Reviews",
        "publisher": null,
        "title": "stravametrodataforbicyclemonitoringaliteraturereview",
        "booktitle": null,
        "doi": "10.1080/01441647.2020.1798558",
        "author": [
            "Lee, Kyuhyun",
            "Sener, Ipek"
        ],
        "keywords": [
            "Strava, bicycle, crowdsourced data, fitness tracking application, emerging travel data"
        ],
        "abstract": "ABSTRACT Monitoring bicycle trips is no longer limited to traditional sources, such as travel surveys and counts. Strava, a popular fitness tracker, continuously collects human movement trajectories, and its commercial data service, Strava Metro, has enriched bicycle research opportunities over the last five years. Accrued knowledge from colleagues who have already utilised Strava Metro data can be valuable for those seeking expanded monitoring options. To convey such knowledge, this paper synthesises a data overview, extensive literature review on how the data have been applied to deal with drivers\u2019 bicycle-related issues, and implications for future work. The review results indicate that Strava Metro data have the potential\u2014although finite\u2014to be used to identify various travel patterns, estimate travel demand, analyse route choice, control for exposure in crash models, and assess air pollution exposure. However, several challenges, such as the under-representativeness of the general population, bias towards and away from certain groups, and lack of demographic and trip details at the individual level, prevent researchers from depending entirely on the new data source. Cross-use with other sources and validation of reliability with official data could enhance the potentiality.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.643",
        "scimago_value": "3,046"
    },
    {
        "issnkey": "13865056",
        "isbn": null,
        "journal": "International Journal of Medical Informatics",
        "publisher": null,
        "title": "technologicalprogressinelectronichealthrecordsystemoptimizationsystematicreviewofsystematicliteraturereviews",
        "booktitle": null,
        "doi": "10.1016/j.ijmedinf.2021.104507",
        "author": [
            "Negro-Calduch, Elsa",
            "Azzopardi-Muscat, Natasha",
            "Krishnamurthy, Ramesh",
            "Novillo-Ortiz, David"
        ],
        "keywords": [
            "Medical informatics, Electronic health records, eHealth, Artificial intelligence, Blockchain, Phenotyping, Deep learning, Natural language processing"
        ],
        "abstract": "Background The recent, rapid development of digital technologies offers new possibilities for more efficient implementation of electronic health record (EHR) and personal health record (PHR) systems. A growing volume of healthcare data has been the hallmark of this digital transformation. The large healthcare datasets' complexity and their dynamic nature pose various challenges related to processing, analysis, storage, security, privacy, data exchange, and usability. Materials and Methods We performed a systematic review of systematic reviews to assess technological progress in EHR and PHR systems. We searched MEDLINE, Cochrane, Web of Science, and Scopus for systematic literature reviews on technological advancements that support EHR and PHR systems published between January 1, 2010, and October 06, 2020. Results The searches resulted in a total of 2,448 hits. Of these, we finally selected 23 systematic reviews. Most of the included papers dealt with information extraction tools and natural language processing technology (n = 10), followed by studies that assessed the use of blockchain technology in healthcare (n = 8). Other areas of digital technology research included EHR and PHR systems in austere settings (n = 1), de-identification methods (n = 1), visualization techniques (n = 1), communication tools within EHR and PHR systems (n = 1), and methodologies for defining Clinical Information Models that promoted EHRs and PHRs interoperability (n = 1). Conclusions Technological advancements can improve the efficiency in the implementation of EHR and PHR systems in numerous ways. Natural language processing techniques, either rule-based, machine-learning, or deep learning-based, can extract information from clinical narratives and other unstructured data locked in EHRs and PHRs, allowing secondary research (i.e., phenotyping). Moreover, EHRs and PHRs are expected to be the primary beneficiaries of the blockchain technology implementation on Health Information Systems. Governance regulations, lack of trust, poor scalability, security, privacy, low performance, and high cost remain the most critical challenges for implementing these technologies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09586946",
        "isbn": null,
        "journal": "International Dairy Journal",
        "publisher": null,
        "title": "futureofdairyfarmingfromthedairybrainperspectivedataintegrationanalyticsandapplications",
        "booktitle": null,
        "doi": "10.1016/j.idairyj.2021.105069",
        "author": [
            "Cabrera, Victor",
            "Fadul-Pacheco, Liliana"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Data integration is one of the biggest challenges the dairy industry faces nowadays due to increased number of technologies and data overflow at the farm. Here we review the current situation of precision dairy farming technologies that use integrated data and its application on the management decision process at the dairy farm. The most common data connections were those from activity monitors, dairy herd improvement records, herd management, and milking recordings. Algorithms used can be defined in general as artificial intelligence and machine learning approaches. Most of the 22 revised papers, research or review, demonstrated that applying different algorithms to integrated data provides additional and complementary insights to improve decision-making tools and therefore enhance economic, management and animal welfare, and hence the sustainability of dairy farms. All revised studies acknowledge the importance of live data integration to develop relevant decision support tools to improve decision making.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.032",
        "scimago_value": "0,903"
    },
    {
        "issnkey": "09234748",
        "isbn": null,
        "journal": "Journal of Engineering and Technology Management",
        "publisher": null,
        "title": "thefourthindustrialrevolutionofsupplychainsatertiarystudy",
        "booktitle": null,
        "doi": "10.1016/j.jengtecman.2021.101624",
        "author": [
            "Barata, Jo\u00e3o"
        ],
        "keywords": [
            "4SC, Supply Chain 4.0, Industry 4.0, Fourth Industrial Revolution, Tertiary study"
        ],
        "abstract": "This paper unfolds the ongoing fourth revolution of supply chains (4SC) and proposes guidelines for future research. The review of sixty-five literature reviews follows three stages: bibliometric analysis of Industry 4.0, its synergies with supply chain transformation, and state-of-the-art assessment. 4SC is a context-bound technological change driven by organizational and cultural priorities, aiming to create more sustainable networks to serve the customers and support responsible decisions in the supply lifecycle. The proposed framework can assist future literature reviews and digital transformation proposals for 4SC that need to frame their context and incorporate functions to endure change.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818914-6",
        "journal": null,
        "publisher": "Academic Press",
        "title": "contents",
        "booktitle": "Digital Health",
        "doi": "10.1016/B978-0-12-818914-6.00031-4",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00104825",
        "isbn": null,
        "journal": "Computers in Biology and Medicine",
        "publisher": null,
        "title": "acomparativeanalysisofelevenneuralnetworksarchitecturesforsmalldatasetsoflungimagesofcovid19patientstowardimprovedclinicaldecisions",
        "booktitle": null,
        "doi": "10.1016/j.compbiomed.2021.104887",
        "author": [
            "Yang, Yuan",
            "Zhang, Lin",
            "Du, Mingyu",
            "Bo, Jingyu",
            "Liu, Haolei",
            "Ren, Lei",
            "Li, Xiaohe",
            "Deen, M."
        ],
        "keywords": [
            "Deep learning, Computed tomography, COVID-19, Image classification"
        ],
        "abstract": "The 2019 novel severe acute respiratory syndrome coronavirus 2-SARS-CoV2, commonly known as COVID-19, is a highly infectious disease that has endangered the health of many people around the world. COVID-19, which infects the lungs, is often diagnosed and managed using X-ray or computed tomography (CT) images. For such images, rapid and accurate classification and diagnosis can be performed using deep learning methods that are trained using existing neural network models. However, at present, there is no standardized method or uniform evaluation metric for image classification, which makes it difficult to compare the strengths and weaknesses of different neural network models. This paper used eleven well-known convolutional neural networks, including VGG-16, ResNet-18, ResNet-50, DenseNet-121, DenseNet-169, Inception-v3, Inception-v4, SqueezeNet, MobileNet, ShuffeNet, and EfficientNet-b0, to classify and distinguish COVID-19 and non-COVID-19 lung images. These eleven models were applied to different batch sizes and epoch cases, and their overall performance was compared and discussed. The results of this study can provide decision support in guiding research on processing and analyzing small medical datasets to understand which model choices can yield better outcomes in lung image classification, diagnosis, disease management and patient care.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.589",
        "scimago_value": "0,884"
    },
    {
        "issnkey": "09255273",
        "isbn": null,
        "journal": "International Journal of Production Economics",
        "publisher": null,
        "title": "industry40adoptionand10radvancemanufacturingcapabilitiesforsustainabledevelopment",
        "booktitle": null,
        "doi": "10.1016/j.ijpe.2020.107844",
        "author": [
            "Bag, Surajit",
            "Gupta, Shivam",
            "Kumar, Sameer"
        ],
        "keywords": [
            "Industry 4.0, Circular economy, Sustainable development, Practice based view, Dynamic capability view, Advanced manufacturing, I4.0 delivery system"
        ],
        "abstract": "Industry 4.0 technologies provide digital solutions for the automation of manufacturing. In circular economy-based models, the resources stay in the system as it experiences one of the 10 R (Refuse, Rethink, Reduce, Reuse, Repair, Refurbish, Remanufacture, Repurpose, Recycle, and Recover) processes. These 10 R processes require the development of advanced manufacturing capabilities; however, 10 R processes suffer from various challenges and can be effectively overcome through Industry 4.0 technological applications. Although literature has indicated the use of various Industry 4.0 technologies, little information is available about firms\u2019 views on the degree of Industry 4.0 application in the 10 R based advanced manufacturing area and its ability to achieve sustainable development. The current study aspires to examine how great an effect Industry 4.0 adoption has on 10 R advanced manufacturing capabilities and its outcome on sustainable development under the moderating effect of an Industry 4.0 delivery system. Practice-based view and Dynamic capability view theories are used to conceptualise the theoretical model. The research team statistically validated the theoretical model considering 124 data points that were collected using an online survey with a structured questionnaire. The findings point out that the path degree of Industry 4.0 adoption and 10 R advanced manufacturing capabilities are statistically significant. 10 R advanced manufacturing capabilities are found to have a positive influence on sustainable development outcomes. Industry 4.0 delivery system has a moderating effect on the path degree of I4.0 implementation and 10 R advanced manufacturing capabilities. The study concludes with key take away points for managers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.885",
        "scimago_value": "2,406"
    },
    {
        "issnkey": "25893777",
        "isbn": null,
        "journal": "Digital Chinese Medicine",
        "publisher": null,
        "title": "quality40technologiestoenhancetraditionalchinesemedicineforovercominghealthcarechallengesduringcovid19",
        "booktitle": null,
        "doi": "10.1016/j.dcmed.2021.06.001",
        "author": [
            "Haleem, Abid",
            "Javaid, Mohd",
            "Singh, Ravi",
            "Suman, Rajiv"
        ],
        "keywords": [
            "Quality 4.0, COVID-19, Digital healthcare, Traditional Chinese medicine (TCM), Industry 4.0, Quality revolution"
        ],
        "abstract": "The Quality 4.0 concept is derived from the industrial fourth revolution, i.e., Industry 4.0. Quality 4.0 is the future of quality, where new digital and disruptive technologies are used to maintain quality in organizations. It is also suitable for traditional Chinese medicine (TCM) to maintain quality. This quality revolution aims to improve industrial and service sectors\u2019 quality by incorporating emerging technologies to connect physical systems with the natural world. The proposed digital philosophy can update and enhance the entire TCM treatment methodology to become more effective and attractive in the current competitive structure of the pharmaceutical and clinical industries. Thus, in healthcare, this revolution empowers quality treatment during the COVID-19 pandemic. There is a major requirement in healthcare to maintain the quality of medical tools, equipment, and treatment processes during a pandemic. Digital technologies can widely be used to provide innovative products and services with excellent quality for TCM. In this paper, we discuss the significant role of Quality 4.0 and how it can be used to maintain healthcare quality and fulfill challenges during the pandemic. Additionally, we discuss 10 significant applications of Quality 4.0 in healthcare during the COVID-19 pandemic. These technologies will provide unique benefits to maintain the quality of TCM throughout the treatment process. With Quality 4.0, quality can be maintained using innovative and advanced digital technologies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23528648",
        "isbn": null,
        "journal": "Digital Communications and Networks",
        "publisher": null,
        "title": "entropybasedredundancyanalysisandinformationscreening",
        "booktitle": null,
        "doi": "10.1016/j.dcan.2021.12.001",
        "author": [
            "Li, Yang",
            "Yang, Jiachen",
            "Wen, Jiabao"
        ],
        "keywords": [
            "Sampling, Quality assessment, Data mining, Low-shot, Few-shot"
        ],
        "abstract": "The ongoing data explosion introduced unprecedented challenges to the information security of communication networks. As images are one of the most commonly used information transmission carriers; therefore, their data redundancy analysis and screening are of great significance. However, most of the current research focus on the algorithm improvement of commonly used image datasets. Thus, we should consider an important question: Is there data redundancy in the open datasets? Considering the factors of model structures and data distribution to ensure the generalization, we conducted extensive experiments to compare the average accuracy based on few random data to the baseline accuracy based on all data. The results show serious data redundancy in the open datasets from different domains. For instance, with the aid of deep model, only 20% data can achieve more than 90% of the baseline accuracy. Further, we proposed a novel entropy-based information screening method, which outperforms the random sampling under many experimental conditions. In particular, considering 20% of data, for the shallow model, the improvement is approximately 10%, and for the deep model, the ratio to the baseline accuracy increases to greater than 95%. Moreover, this work can also serve as a new way of learning from a few valuable samples, compressing the size of existing datasets and guiding the construction of high-quality datasets in the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.797",
        "scimago_value": "1,082"
    },
    {
        "issnkey": "00981354",
        "isbn": null,
        "journal": "Computers & Chemical Engineering",
        "publisher": null,
        "title": "datacentricprocesssystemsengineeringapushtowardspse40",
        "booktitle": null,
        "doi": "10.1016/j.compchemeng.2021.107529",
        "author": [
            "Reis, Marco",
            "Saraiva, Pedro"
        ],
        "keywords": [
            "Process systems engineering 4.0, Data-centric PSE, Data science, Industry 4.0, Artificial intelligence, Applied statistics"
        ],
        "abstract": "Process Systems Engineering (PSE) is now a mature field with a well-established body of knowledge, computational-oriented frameworks and methodologies designed and implemented for addressing chemical processes related problems spanning a wide range of scales in time and space. A common feature of many PSE approaches relies in their mostly deductive nature, based on a deep understanding of the underlying Chemical Engineering Science. Given the current data-intensive industrial and societal contexts, new sources of process or product information are now easily made available and should be exploited to complement and expand the classical PSE paradigm with inductive data-driven reasoning and knowledge discovery methodologies. In this article, based upon our over 25 years of research and teaching experience in the field, we discuss the scope and trends of this PSE evolution, refer to several relevant Data-Centric PSE approaches, and identify the main components, applications and future opportunities of this PSE 4.0 perspective.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,017"
    },
    {
        "issnkey": "24058963",
        "isbn": null,
        "journal": "IFAC-PapersOnLine",
        "publisher": null,
        "title": "datastrategyanddatatrustdriversforbusinessdevelopment",
        "booktitle": null,
        "doi": "10.1016/j.ifacol.2021.10.409",
        "author": [
            "Jesse, Norbert"
        ],
        "keywords": [
            "Innovation Management, Intelligent Systems, Applications"
        ],
        "abstract": "Data is the new oil \u2013 which has often been heard since the end of the 90th. However, many companies took too long to exploit the new technological options for their business. It has been proven that the digital disruption led to seemingly stable and well-known brands disappearing from their market. Insufficient data competence is one of the reasons why companies failed in the escalating process of creative destruction. In this document we address three competence dimensions: data architecture, data preparation and the interchange of data. The competence in these fields is a precondition for a company\u2019s decision-making process to lead successfully to profitability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,308"
    },
    {
        "issnkey": "26663287",
        "isbn": null,
        "journal": "JAAD International",
        "publisher": null,
        "title": "epidemiologicevolutionofcommoncutaneousinfestationsandarthropodbitesagoogletrendsanalysis",
        "booktitle": null,
        "doi": "10.1016/j.jdin.2021.08.003",
        "author": [
            "Simonart, Thierry",
            "{Lam Hoai}, Xu\u00e2n-Lan",
            "{De Maertelaer}, Viviane"
        ],
        "keywords": [
            "bed bugs, head lice, infodemiology, pubic lice, scabies, ticks, Google Trends"
        ],
        "abstract": "Background Common cutaneous infestations and arthropod bites are not reportable conditions in most countries. Their worldwide epidemiologic evolution and distribution are mostly unknown. Objective To explore the evolution and geographic distribution of common cutaneous infestations and arthropod bites through an analysis of Google Trends. Methods Search trends from 2004 through March 2021 for common cutaneous infestations and arthropod bites were extracted from Google Trends, quantified, and analyzed. Results Time series decomposition showed that total search term volume for pubic lice decreased worldwide over the study period, while the interest for ticks, pediculosis, insect bites, scabies, lice, and bed bugs increased (in increasing order). The interest for bed bugs was more pronounced in the former Union of Soviet Socialist Republics countries, interest for lice in Near East and Middle East countries, and interest for pubic lice in South American countries. Internet searches for bed bugs, insect bites, and ticks exhibited the highest seasonal patterns. Limitations Retrospective analysis limits interpretation. Conclusion Surveillance systems based on Google Trends may enhance the timeliness of traditional surveillance systems and suggest that, while most cutaneous infestations increase worldwide, pubic lice may be globally declining.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26673258",
        "isbn": null,
        "journal": "Fundamental Research",
        "publisher": null,
        "title": "towardaresearchframeworktoconceptualizedataasafactorofproductionthedatamarketplaceperspective",
        "booktitle": null,
        "doi": "10.1016/j.fmre.2021.08.006",
        "author": [
            "Huang, Lihua",
            "Dou, Yifan",
            "Liu, Yezheng",
            "Wang, Jinzhao",
            "Chen, Gang",
            "Zhang, Xiaoyang",
            "Wang, Runyin"
        ],
        "keywords": [
            "Data marketplace, Factor of production, Data lifecycle, Asset specificity"
        ],
        "abstract": "The widespread use of machine learning techniques and artificial intelligence algorithms has highlighted the strategic role of data. To acquire data for training algorithms and eventually empowering the digital transformation, data marketplaces are often required to support and coordinate cross-organizational data transactions. However, the prior industry practices have suggested that the transaction costs in the data marketplaces are severely high, and the supporting infrastructure is far from mature. This paper proposes a data attributes-affected data exchange (DADE) conceptual model to understand the challenges and directions for developing data marketplaces. Specifically, our model framework is built upon two dimensions, data lifecycle maturity and data asset specificity. Based on the DADE model, we propose four approaches for developing data marketplaces and discuss future research directions with an overview of computational methods as potential technical solutions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15507289",
        "isbn": null,
        "journal": "Surgery for Obesity and Related Diseases",
        "publisher": null,
        "title": "commentonhighacquisitionrateandinternalvalidityinthescandinavianobesitysurgeryregistry",
        "booktitle": null,
        "doi": "10.1016/j.soard.2020.11.008",
        "author": [
            "Ghanem, Omar",
            "Badaoui, Joseph"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.734",
        "scimago_value": "1,733"
    },
    {
        "issnkey": "27724247",
        "isbn": null,
        "journal": "Communications in Transportation Research",
        "publisher": null,
        "title": "stateofdataplatformsforconnectedvehiclesandinfrastructures",
        "booktitle": null,
        "doi": "10.1016/j.commtr.2021.100013",
        "author": [
            "Lim, Kai",
            "Whitehead, Jake",
            "Jia, Dongyao",
            "Zheng, Zuduo"
        ],
        "keywords": [
            "Connected mobility, Vehicular networks, Data platforms, Electric vehicles"
        ],
        "abstract": "The continuing expansion of connected and electro-mobility products and services has led to their ability to rapidly generate very large amounts of data, leading to a demand for effective data management solutions. This is further catalysed through the need for society to make informed policies and decisions that can properly support their emerging growth. While data systems and platforms exist, they are often proprietary, being only compatible to the products that they are designed for. Given the products and services generate energy and spatial-temporal data that can often correlate, a lack of interoperability between these systems would impede decision making, as data from each system must be considered independently. By studying currently available data platforms and frameworks, this paper weighs the problems that these products address, and identifies necessary gaps for a more cohesive platform to exist. This is performed through a top-down approach, whereby broader vehicle-to-everything approaches are first studied, before moving to the components that could comprise a data platform to integrate and ingest these various data feeds. Finally, potential design considerations for a data platform is presented, along with examples of application benefits that would enable users to make more informed and holistic decisions about current mobility options.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22131388",
        "isbn": null,
        "journal": "Sustainable Energy Technologies and Assessments",
        "publisher": null,
        "title": "energyawaresmartcitymanagementsystemusingdataanalyticsandinternetofthings",
        "booktitle": null,
        "doi": "10.1016/j.seta.2021.100992",
        "author": [
            "Babar, Muhammad",
            "Khattak, Akmal",
            "Jan, Mian",
            "Tariq, Muhammad"
        ],
        "keywords": [
            "Smart city, Internet of Things, Energy management, Data analytics"
        ],
        "abstract": "The rise of Internet of Things (IoT) concept founded the realization of a smart city. Energy management has lately turn out to be a vital concern for the services of a smart city as IoT devices consume massive energy constantly. This concern needs to be addressed to devise a practical method. Efficient energy utilization aims to promise smart city sustainability. Moreover, IoT devices produce enormous data that is required to be processed efficiently. In this article, a framework is proposed that assures the energy efficiency of IoT devices along with data analysis for cities. This article proposes a general design for smart city energy management that assures the energy efficiency of IoT devices along with data analysis. The proposed model includes three different components that are energy management, data processing, and service management. The energy management component is dependent on infrastructure optimization. The energy-efficient clustering, peak load shaving, optimized scheduling, and load balancing algorithms are integrated to achieve efficient energy management. The data processing is performed using distributed framework. The service management is performed using rules and thresholds. The experiments are performed using authentic datasets and the result highlights the efficiency of the proposed model.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.353",
        "scimago_value": "1,040"
    },
    {
        "issnkey": "10538119",
        "isbn": null,
        "journal": "NeuroImage",
        "publisher": null,
        "title": "qualitycontrolstrategiesforbrainmrisegmentationandparcellationpracticalapproachesandrecommendationsinsightsfromthemaastrichtstudy",
        "booktitle": null,
        "doi": "10.1016/j.neuroimage.2021.118174",
        "author": [
            "Monereo-S\u00e1nchez, Jennifer",
            "{de Jong}, Joost",
            "Drenthen, Gerhard",
            "Beran, Magdalena",
            "Backes, Walter",
            "Stehouwer, Coen",
            "Schram, Miranda",
            "Linden, David",
            "Jansen, Jacobus"
        ],
        "keywords": [
            "Brain segmentation, Cortical parcellation, FreeSurfer, Quality control, Manual editing, Outlier exclusion"
        ],
        "abstract": "Quality control of brain segmentation is a fundamental step to ensure data quality. Manual quality control strategies are the current gold standard, although these may be unfeasible for large neuroimaging samples. Several options for automated quality control have been proposed, providing potential time efficient and reproducible alternatives. However, those have never been compared side to side, which prevents consensus in the appropriate quality control strategy to use. This study aimed to elucidate the changes manual editing of brain segmentations produce in morphological estimates, and to analyze and compare the effects of different quality control strategies on the reduction of the measurement error. Structural brain MRI from 259 participants of The Maastricht Study were used. Morphological estimates were automatically extracted using FreeSurfer 6.0. Segmentations with inaccuracies were manually edited, and morphological estimates were compared before and after editing. In parallel, 12 quality control strategies were applied to the full sample. Those included: two manual strategies, in which images were visually inspected and either excluded or manually edited; five automated strategies, where outliers were excluded based on the tools \u201cMRIQC\u201d and \u201cQoala-T\u201d, and the metrics \u201cmorphological global measures\u201d, \u201cEuler numbers\u201d and \u201cContrast-to-Noise ratio\u201d; and five semi-automated strategies, where the outliers detected through the mentioned tools and metrics were not excluded, but visually inspected and manually edited. In order to quantify the effects of each quality control strategy, the proportion of unexplained variance relative to the total variance was extracted after the application of each strategy, and the resulting differences compared. Manually editing brain surfaces produced particularly large changes in subcortical brain volumes and moderate changes in cortical surface area, thickness and hippocampal volumes. The performance of the quality control strategies depended on the morphological measure of interest. Overall, manual quality control strategies yielded the largest reduction in relative unexplained variance. The best performing automated alternatives were those based on Euler numbers and MRIQC scores. The exclusion of outliers based on global morphological measures produced an increase of relative unexplained variance. Manual quality control strategies are the most reliable solution for quality control of brain segmentation and parcellation. However, measures must be taken to prevent the subjectivity associated with these strategies. The detection of inaccurate segmentations based on Euler numbers or MRIQC provides a time efficient and reproducible alternative. The exclusion of outliers based on global morphological estimates must be avoided.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "25303120",
        "isbn": null,
        "journal": "Revista Colombiana de Psiquiatr\u00eda (English ed.)",
        "publisher": null,
        "title": "implementingaredcapbasedresearchdatacollectionsystemformentalhealth",
        "booktitle": null,
        "doi": "10.1016/j.rcpeng.2021.06.004",
        "author": [
            "{Marroquin Rivera}, Arturo",
            "Rosas-Romero, Juan",
            "Castro, Sergio",
            "Su\u00e1rez-Obando, Fernando",
            "Aguilera-Cruz, Jeny",
            "Bartels, Sophia",
            "Park, Sena",
            "Torrey, William",
            "G\u00f3mez-Restrepo, Carlos"
        ],
        "keywords": [
            "Data collection, REDCap, Mental health, Recolecci\u00f3n de datos, REDCap, Salud mental"
        ],
        "abstract": "Background The implementation of new technologies in medical research, such as novel big storage systems, has recently gained importance. Electronic data capture is a perfect example as it powerfully facilitates medical research. However, its implementation in resource-limited settings, where basic clinical resources, internet access, and human resources may be reduced might be a problem. Methods In this paper we described our approach for building a network architecture for data collection to achieve our objectives using a REDCap\u00ae tool in Colombia and provide guidance for data collection in similar settings. Conclusions REDCap is a feasible and efficient electronic data capture software to use in similar contexts to Colombia. The software facilitated the whole data management process and is a way to build research capacities in resourced-limited settings. Resumen Contexto La implementaci\u00f3n de nuevas tecnolog\u00edas en la investigaci\u00f3n m\u00e9dica, como los nuevos sistemas de gran almacenamiento de datos, recientemente han ganado importancia. El almacenamiento electr\u00f3nico de datos es un ejemplo perfecto ya que facilita poderosamente la investigaci\u00f3n m\u00e9dica. Sin embargo, su implementaci\u00f3n en ambientes con recursos limitados, donde los recursos b\u00e1sicos cl\u00ednicos, el acceso a internet y el recurso humano podr\u00edan ser reducidos, suponen un problema. M\u00e9todos En este art\u00edculo describimos nuestro acercamiento para construir una red arquitect\u00f3nica para la recolecci\u00f3n de datos, en aras de alcanzar nuestros objetivos mediante la utilizaci\u00f3n de la herramienta REDCap\u00ae en Colombia y proveer una gu\u00eda para la recolecci\u00f3n de datos en condiciones similares. Conclusiones REDCap es un software de almacenamiento electr\u00f3nico de datos eficiente y encontramos que resulta posible su utilizaci\u00f3n en contextos similares a Colombia. Este software facilit\u00f3 el proceso del manejo de los datos y es una manera de construir capacidades investigativas en contextos donde los recursos son limitados.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02786125",
        "isbn": null,
        "journal": "Journal of Manufacturing Systems",
        "publisher": null,
        "title": "datadrivenmanufacturinganassessmentmodelfordatasciencematurity",
        "booktitle": null,
        "doi": "10.1016/j.jmsy.2021.07.011",
        "author": [
            "G\u00f6kalp, Mert",
            "G\u00f6kalp, Ebru",
            "Kayabay, Kerem",
            "Ko\u00e7yi\u011fit, Altan",
            "Eren, P."
        ],
        "keywords": [
            "Smart manufacturing, Industry 4.0, Maturity model, Data science, Maturity assessment, Process improvement"
        ],
        "abstract": "Today, data science presents immense opportunities by turning raw data into manufacturing intelligence in data-driven manufacturing that aims to improve operational efficiency and product quality together with reducing costs and risks. However, manufacturing firms face difficulties in managing their data science endeavors for reaping these potential benefits. Maturity models are developed to guide organizations by providing an extensive roadmap for improvement in certain areas. Therefore, this paper seeks to address this problem by proposing a theoretically grounded Data Science Maturity Model (DSMM) for manufacturing organizations to assess their existing strengths and weaknesses, perform a gap analysis, and draw a roadmap for continuous improvements in their progress towards data-driven manufacturing. DSMM comprises six maturity levels from \u201cNot Performed\u201d to\u201d Innovating\u201d and twenty-eight data science processes categorized under six headings: Organization, Strategy Management, Data Analytics, Data Governance, Technology Management, and Supporting. The applicability and usefulness of DSMM are validated through multiple case studies conducted in manufacturing organizations of various sizes, industries, and countries. The case study results indicate that DSMM is applicable in different settings and is able to reflect the organizations\u2019 current data science maturity levels and provide significant insights to improve their data science capabilities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.633",
        "scimago_value": "2,310"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822132-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "",
        "booktitle": "Data Stewardship (Second Edition)",
        "doi": "10.1016/B978-0-12-822132-7.00016-4",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "industry40andsustainabilitytowardsconceptualizationandtheory",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.127733",
        "author": [
            "Beltrami, Mirjam",
            "Orzes, Guido",
            "Sarkis, Joseph",
            "Sartor, Marco"
        ],
        "keywords": [
            "Industry 4.0, Fourth industrial revolution, Internet of things, Sustainability, Conceptual framework"
        ],
        "abstract": "Both Industry 4.0 and sustainability have gained momentum in the academic, managerial and policy debate. Despite the relevance of the topics, the relation between Industry 4.0 and sustainability \u2013 revealed by many authors \u2013 is still unclear; literature is fragmented. This paper seeks to overcome this limit by developing a systematic literature review of 117 peer-reviewed journal articles. After descriptive and content analyses, the work presents a conceptualization and theoretical framework. The paper contributes to both theory and practice by advancing current understanding of Industry 4.0 and sustainability, especially the impact of Industry 4.0 technologies on sustainability practices and performance.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "advancedbatterymanagementstrategiesforasustainableenergyfuturemultilayerdesignconceptsandresearchtrends",
        "booktitle": null,
        "doi": "10.1016/j.rser.2020.110480",
        "author": [
            "Dai, Haifeng",
            "Jiang, Bo",
            "Hu, Xiaosong",
            "Lin, Xianke",
            "Wei, Xuezhe",
            "Pecht, Michael"
        ],
        "keywords": [
            "Lithium-ion batteries, Battery management technologies, Multilayer design concepts, Safety and aging, Data and intelligence"
        ],
        "abstract": "Lithium-ion batteries are promising energy storage devices for electric vehicles and renewable energy systems. However, due to complex electrochemical processes, potential safety issues, and inherent poor durability of lithium-ion batteries, it is essential to monitor and manage batteries safely and efficiently. This study reviews the development of battery management systems during the past periods and introduces a multilayer design architecture for advanced battery management, which consists of three progressive layers. The foundation layer focuses on the system physical basis and theoretical principle, the algorithm layer aims at providing a comprehensive understanding of battery, and the application layer ensures a safe and efficient battery system through sufficient management. A comprehensive overview of each layer is presented from both academic and engineering perspectives. Future trends in research and development of next-generation battery management are discussed. Based on data and intelligence, the next-generation battery management will achieve better safety, performance, and interconnectivity.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "22106502",
        "isbn": null,
        "journal": "Swarm and Evolutionary Computation",
        "publisher": null,
        "title": "capturingtheswarmintelligenceintruckersthefoundationanalysisforfutureswarmroboticsinroadfreight",
        "booktitle": null,
        "doi": "10.1016/j.swevo.2021.100845",
        "author": [
            "Gan, Mi",
            "Qian, Qiujun",
            "Li, Dandan",
            "Ai, Yi",
            "Liu, Xiaobo"
        ],
        "keywords": [
            "Swarm Intelligence, Prediction, Trucker Trajectory, Machine learning"
        ],
        "abstract": "A group of individual truckers can be regarded as a swarm intelligence system without central management. With the development of autonomous driving technology, trucker groups will be replaced by driverless vehicles. At that point, a swarm of truckers will become a swarm robotics system. Therefore, considering the design and control of an efficient swarm robotics system, it is essential to investigate the properties and model the behaviors of a swarm of truckers in advance. In this study, we probe the characteristics of both individual truckers and a swarm of truckers using trajectory data of truckers. First, the trajectory data were map matched based on the geographic scale of cities and administrative regions. Then, the properties of the division of labor, pattern formation, and swarm synchronization were obtained through an analysis of the spatiotemporal distribution of radius of gyration, travel distance, and the number of visited places. Because predicting the next visit locations of individuals of a swarm is a measure for modeling swarm behaviors, the prediction model can be used to predict future swarm robotics (driverless trucks) behaviors. Thus, we apply several machine learning models to predict the next locations of truckers. The results show that there are common characteristics and routines embodied in the behavior of the truckers; the swarm shows consistency and regularity. Moreover, the peak predictability of the entire group reached 94%, indicating that our model can predict the behavior of groups and individuals. Our findings provide basis supporting to the future efficient swarm robotics system.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.177",
        "scimago_value": "1,460"
    },
    {
        "issnkey": "13899457",
        "isbn": null,
        "journal": "Sleep Medicine",
        "publisher": null,
        "title": "effectsofobstructivesleepapnoeaseverityonneurocognitiveandbrainwhitematteralterationsinchildrenaccordingtosexatractbasedspatialstatisticsstudy",
        "booktitle": null,
        "doi": "10.1016/j.sleep.2020.08.026",
        "author": [
            "Mei, Lin",
            "Li, Xiaodan",
            "Zhou, Guifei",
            "Ji, Tingting",
            "Chen, Jun",
            "Xu, Zhifei",
            "Peng, Yun",
            "Liu, Yue",
            "Li, Hongbin",
            "Zhang, Jie",
            "Wang, Shengcai",
            "Zhang, Yamei",
            "Ge, Wentong",
            "Guo, Yongli",
            "Qiu, Yue",
            "Jia, Xinbei",
            "Tian, Jinghong",
            "Zheng, Li",
            "Liu, Jiangang",
            "Tai, Jun",
            "Ni, Xin"
        ],
        "keywords": [
            "Diffusion tensor imaging, Obstructive sleep apnoea, Children, Tract-based spatial statistics, White matter"
        ],
        "abstract": "Objectives To investigate alterations in neurocognitive, attention, paediatric sleep questionnaire (PSQ) scores and whole brain white matter (WM) integrity between children with mild and severe obstructive sleep apnoea (OSA) according to sex and whether these changes are associated with OSA severity. Methods Fifty-seven children (36 males and 21 females) diagnosed with OSA were recruited for this study. Children of both sexes were divided into mild (male-MG, female-MG) and severe (male-SG, female-SG) groups according to OSA severity. Polysomnography (PSG), neurocognitive, attention and PSQ tests were compared between groups by one-way samples analysis of variance (ANOVA) F test. Diffusion tensor imaging (DTI) was scanned using a 3T GE MRI scanner and analysed by Tract-based Spatial Statistics (TBSS). Spearman correlation was calculated between DTI Eigenvalues and clinical characteristics. Results Compared to mild OSA patients, severe OSA patients presented greater severity of obstructive apnoea hypopnea index (OAHI), neurocognition, PSQ and attention tests in both male and female patients. Brain WM integrity in the male-SG, compared to the male-MG, demonstrated significantly reduced fractional anisotropy (FA) values in the right middle frontal gyrus and the right frontal sub-gyral regions and increased axial diffusivity (AD) values in the right inferior frontal gyrus, left parietal angular gyrus and sub-gyral regions, while no differences were found between the female-MG and female-SG. Alterations in male-SG brain regions were observably correlated with severity in male OSA patients. Conclusions The integrity of WM, which regulates autonomic, cognitive, and attention functions, is impaired in male, but not female, children with severe OSA.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "anoverviewofdatatoolsforrepresentingandmanagingbuildinginformationandperformancedata",
        "booktitle": null,
        "doi": "10.1016/j.rser.2021.111224",
        "author": [
            "Luo, Na",
            "Pritoni, Marco",
            "Hong, Tianzhen"
        ],
        "keywords": [
            "Building information modeling, Ontology, Data schema, Metadata, Building performance data"
        ],
        "abstract": "Building information modeling (BIM) has been widely adopted for representing and exchanging building data across disciplines during building design and construction. However, BIM's use in the building operation phase is limited. With the increasing deployment of low-cost sensors and meters, as well as affordable digital storage and computing technologies, growing volumes of data have been collected from buildings, their energy services systems, and occupants. Such data are crucial to help decision makers understand what, how, and when energy is consumed in buildings\u2014a critical step to improving building performance for energy efficiency, demand flexibility, and resilience. However, practical analyses and use of the collected data are very limited due to various reasons, including poor data quality, ad-hoc representation of data, and lack of data science skills. To unlock value from building data, there is a strong need for a toolchain to curate and represent building information and performance data in common standardized terminologies and schemas, to enable interoperability between tools and applications. This study selected and reviewed 24 data tools based on common use cases of data across the building life cycle, from design to construction, commissioning, operation, and retrofits. The selected data tools are grouped into three categories: (1) data dictionary or terminology, (2) data ontology and schemas, and (3) data platforms. The data are grouped into ten typologies covering most types of data collected in buildings. This study resulted in five main findings: (1) most data representation tools can represent their intended data typologies well, such as Green Button for smart meter data and Brick schema for metadata of sensors in buildings and HVAC systems, but none of the tools cover all ten types of data; (2) there is a need for data schemas to represent the basis of design data and metadata of occupant data; (3) standard terminologies such as those defined in BEDES are only adopted in a few data tools; (4) integrating data across various stages in the building life cycle remains a challenge; and (5) most data tools were developed and maintained by different parties for different purposes, their flexibility and interoperability can be improved to support broader use cases. Finally, recommendations for future research on building data tools are provided for the data and buildings community based on the FAIR principles to make data Findable, Accessible, Interoperable, and Reusable.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "asystematicliteraturereviewonapplyingcrispdmprocessmodel",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.01.199",
        "author": [
            "Schr\u00f6er, Christoph",
            "Kruse, Felix",
            "G\u00f3mez, Jorge"
        ],
        "keywords": [
            "CRISP-DM, Literature Review, Data Mining, Process Methodology, Deployment"
        ],
        "abstract": "CRISP-DM is the de-facto standard and an industry-independent process model for applying data mining projects. Twenty years after its release in 2000, we would like to provide a systematic literature review of recent studies published in IEEE, ScienceDirect and ACM about data mining use cases applying CRISP-DM. We give an overview of the research focus, current methodologies, best practices and possible gaps in conducting the six phases of CRISP-DM. The main findings are that CRISP-DM is still a de-factor standard in data mining, but there are challenges since the most studies do not foresee a deployment phase. The contribution of our paper is to identify best practices and process phases in which data mining analysts can be better supported. Further contribution is a template for structuring and releasing CRISP-DM studies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "03043894",
        "isbn": null,
        "journal": "Journal of Hazardous Materials",
        "publisher": null,
        "title": "currentadvancesandfuturechallengesofaiotapplicationsinparticulatematterspmmonitoringandcontrol",
        "booktitle": null,
        "doi": "10.1016/j.jhazmat.2021.126442",
        "author": [
            "Yang, Chao-Tung",
            "Chen, Ho-Wen",
            "Chang, En-Jui",
            "Kristiani, Endah",
            "Nguyen, Kieu",
            "Chang, Jo-Shu"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Air pollution is at the center of pollution-control discussion due to the significant adverse health effects on individuals and the environment. Research has shown the association between unsafe environments and different sizes of particulate matter (PM), highlighting the importance of pollutant monitoring to mitigate its detrimental effect. By monitoring air quality with low-cost monitoring devices that collect massive observations, such as Air Box, a comprehensive collection of ground-level PM concentration is plausible due to the simplicity and low-cost, propelling applications in agriculture, aquaculture, and air quality, water resources, and disaster prevention. This paper aims to view IoT-based systems with low-cost microsensors at the sensor, network, and application levels, along with machine learning algorithms that improve sensor networks\u2019 precision, providing better resolution. From the analysis at the three levels, we analyze current PM monitoring methods, including the use of sensors when collecting PM concentrations, demonstrate the use of IoT-based systems in PM monitoring and its challenges, and finally present the integration of AI and IoT (AIoT) in PM monitoring, indoor air quality control, and future directions. In addition, the inclusion of Taiwan as a site analysis was illustrated to show an example of AIoT in PM-control policy-making potential directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.588",
        "scimago_value": "2,034"
    },
    {
        "issnkey": "13865056",
        "isbn": null,
        "journal": "International Journal of Medical Informatics",
        "publisher": null,
        "title": "asystematicreviewofemerginginformationtechnologiesforsustainabledatacentrichealthcare",
        "booktitle": null,
        "doi": "10.1016/j.ijmedinf.2021.104420",
        "author": [
            "Zahid, Arnob",
            "Poulsen, Jennifer",
            "Sharma, Ravi",
            "Wingreen, Stephen"
        ],
        "keywords": [
            "Emerging technologies, Data modelling, Data analytics, Data-centric health-care"
        ],
        "abstract": "Background Of the Sustainable Development Goals (SDGs), the third presents the opportunity for a predictive universal digital healthcare ecosystem, capable of informing early warning, assisting in risk reduction and guiding management of national and global health risks. However, in reality, the existing technology infrastructure of digital healthcare systems is insufficient, failing to satisfy current and future data needs. Objective This paper systematically reviews emerging information technologies for data modelling and analytics that have potential to achieve Data-Centric Health-Care (DCHC) for the envisioned objective of sustainable healthcare. The goal of this review is to: 1) identify emerging information technologies with potential for data modelling and analytics, and 2) explore recent research of these technologies in DCHC. Findings A total of 1619 relevant papers have been identified and analysed in this review. Of these, 69 were probed deeply. Our analysis found that the extant research focused on elder care, rehabilitation, chronic diseases, and healthcare service delivery. Use-cases of the emerging information technologies included providing assistance, monitoring, self-care and self-management, diagnosis, risk prediction, well-being awareness, personalized healthcare, and qualitative and/or quantitative service enhancement. Limitations identified in the studies included vendor hardware specificity, issues with user interface and usability, inadequate features, interoperability, scalability, and compatibility, unjustifiable costs and insufficient evaluation in terms of validation. Conclusion Achievement of a predictive universal digital healthcare ecosystem in the current context is a challenge. State-of-the-art technologies demand user centric design, data privacy and protection measures, transparency, interoperability, scalability, and compatibility to achieve the SDG objective of sustainable healthcare by 2030.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15517144",
        "isbn": null,
        "journal": "Contemporary Clinical Trials",
        "publisher": null,
        "title": "usingdigitaltechnologiesinclinicaltrialscurrentandfutureapplications",
        "booktitle": null,
        "doi": "10.1016/j.cct.2020.106219",
        "author": [
            "Rosa, Carmen",
            "Marsch, Lisa",
            "Winstanley, Erin",
            "Brunner, Meg",
            "Campbell, Aimee"
        ],
        "keywords": [
            "Clinical trials, Digital technology, Smartphones, Electronic health records, Artificial intelligence"
        ],
        "abstract": "In 2015, we provided an overview of the use of digital technologies in clinical trials, both as a methodological tool and as a mechanism to deliver interventions. At that time, there was limited guidance and limited use of digital technologies in clinical research. However, since then smartphones have become ubiquitous and digital health technologies have exploded. This paper provides an update to our earlier publication and an overview of how technology has been used in the past five years in clinical trials, providing examples with varying levels of technological integration and across different health conditions. Digital technology integration ranges from the incorporation of artificial intelligence in diagnostic devices to the use of real-world data (e.g., electronic health records) for study recruitment. Clinical trials can now be conducted entirely virtually, eliminating the need for in-person interaction. Much of the published research demonstrates how digital approaches can improve the design and implementation of clinical trials. While challenges remain, progress over the last five years is encouraging, and barriers can be overcome with careful planning.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02615177",
        "isbn": null,
        "journal": "Tourism Management",
        "publisher": null,
        "title": "reviewoftourismforecastingresearchwithinternetdata",
        "booktitle": null,
        "doi": "10.1016/j.tourman.2020.104245",
        "author": [
            "Li, Xin",
            "Law, Rob",
            "Xie, Gang",
            "Wang, Shouyang"
        ],
        "keywords": [
            "Internet data, Tourism forecasting, Search engine, Social media, Systematic review"
        ],
        "abstract": "Internet techniques significantly influence the tourism industry and Internet data have been used widely used in tourism and hospitality research. However, reviews on the recent development of Internet data in tourism forecasting remain limited. This work reviews articles on tourism forecasting research with Internet data published in academic journals from 2012 to 2019. Then, the findings ae synthesized based on the following Internet data classifications: search engine, web traffic, social media, and multiple sources. Results show that among such classifications, search engine data are most widely incorporated into tourism forecasting. Time series and econometric forecasting models remain dominant, whereas artificial intelligence methods are still developing. For unstructured social media and multi-source data, methodological advancements in text mining, sentiment analysis, and social network analysis are required to transform data into time series for forecasting. Combined Internet data and forecasting models will help in improving forecasting accuracy further in future research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.967",
        "scimago_value": "3,328"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822132-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "introduction",
        "booktitle": "Data Stewardship (Second Edition)",
        "doi": "10.1016/B978-0-12-822132-7.00017-6",
        "author": [
            "Plotkin, David"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00104825",
        "isbn": null,
        "journal": "Computers in Biology and Medicine",
        "publisher": null,
        "title": "bidirectionalcrossmodalityunsuperviseddomainadaptationusinggenerativeadversarialnetworksforcardiacimagesegmentation",
        "booktitle": null,
        "doi": "10.1016/j.compbiomed.2021.104726",
        "author": [
            "Cui, Hengfei",
            "Yuwen, Chang",
            "Jiang, Lei",
            "Xia, Yong",
            "Zhang, Yanning"
        ],
        "keywords": [
            "Generative adversarial networks, Unsupervised domain adaptation, Cardiac segmentation, Self-attention mechanism, Knowledge distillation loss"
        ],
        "abstract": "Background A novel Generative Adversarial Networks (GAN) based bidirectional cross-modality unsupervised domain adaptation (GBCUDA) framework is developed for cardiac image segmentation, which can effectively tackle the problem of network's segmentation performance degradation when adapting to the target domain without ground truth labels. Method GBCUDA uses GAN for image alignment, applies adversarial learning to extract image features, and gradually enhances the domain invariance of extracted features. The shared encoder performs an end-to-end learning task in which features that differ between the two domains complement each other. The self-attention mechanism is incorporated to the GAN network, which can generate details based on the prompts of all feature positions. Furthermore, spectrum normalization is implemented to stabilize the training of GAN, and knowledge distillation loss is introduced to process high-level feature-maps in order to better complete the cross-mode segmentation task. Results The effectiveness of our proposed unsupervised domain adaptation framework is tested over the Multi-Modality Whole Heart Segmentation (MM-WHS) Challenge 2017 dataset. The proposed method is able to improve the average Dice from 74.1% to 81.5% for the four cardiac substructures, and reduce the average symmetric surface distance (ASD) from 7.0 to 5.8 over CT images. For MRI images, our proposed framework trained on CT images gives the average Dice of 59.2% and reduces the average ASD from 5.7 to 4.9. Conclusions The evaluation results demonstrate our method's effectiveness on domain adaptation and the superiority to the current state-of-the-art domain adaptation methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.589",
        "scimago_value": "0,884"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "howcanbicyclesharinghaveasustainablefuturearesearchbasedonlifecycleassessment",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2020.125081",
        "author": [
            "Mao, Guozhu",
            "Hou, Tianyi",
            "Liu, Xi",
            "Zuo, Jian",
            "Kiyawa, Abdul-Hakim",
            "Shi, Pingping",
            "Sandhu, Sukhbir"
        ],
        "keywords": [
            "Life cycle assessment, Bicycle-sharing, Environmental impact, Sustainable"
        ],
        "abstract": "Bicycle-sharing is experiencing explosive growth in China, and it is expected to be an environmental friendly practice, however, mass production and insufficient recycling of shared bicycles may bring great negative environmental impact. In this study, we conduct a life cycle assessment (LCA) of bicycle-sharing in China to estimate the negative environmental impacts of the stages of the whole life cycle with nine environmental impact categories. The results show that the production stage contributes to the greatest negative environmental impacts by an average rate of 81.18% among different impact categories, much higher than other stages, i.e. the use stage, daily management and transportation stage and waste treatment and recycling stage. Specifically, the use of aluminum at the production stage contributes to the most in nine environmental impacts categories (55.43% on average). And rubber is another relatively important contributor to these nine environmental impact categories (16.27% on average). In addition, the cumulative production of excessive parts is expected to bring further environmental impacts at the production stage. To promote the sustainable development of bicycle-sharing, we discuss various potential opportunities at the production and maintenance stage, materials selection for better durability and longer service life, the structure design for the ease of maintenance. Besides, at the waste treatment and recycling stage, we promote that the recycling system is necessary for generating environmental benefits considering the potential huge supply and demand of the market. The construction of a bicycle-sharing industry chain may be an effective way for the sustainable development of bicycle-sharing in the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "00330620",
        "isbn": null,
        "journal": "Progress in Cardiovascular Diseases",
        "publisher": null,
        "title": "socialmediaandpredictiveanalysisregardingdietaryapproachestostophypertension",
        "booktitle": null,
        "doi": "10.1016/j.pcad.2021.07.006",
        "author": [
            "Krittanawong, Chayakrit",
            "Kaplin, Scott",
            "Tang, W.H.",
            "Jneid, Hani",
            "Virani, Salim",
            "Messerli, Franz"
        ],
        "keywords": [
            "Social media, DASH, DASH diet, Social media analysis, Facebook, Twitter"
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "25889141",
        "isbn": null,
        "journal": "Clinical eHealth",
        "publisher": null,
        "title": "comparativeanatomizationofdataminingandfuzzylogictechniquesusedindiabetesprognosis",
        "booktitle": null,
        "doi": "10.1016/j.ceh.2020.11.001",
        "author": [
            "Thakkar, Harshil",
            "Shah, Vaishnavi",
            "Yagnik, Hiteshri",
            "Shah, Manan"
        ],
        "keywords": [
            "Data mining, Fuzzy logic, Diabetes, Health care"
        ],
        "abstract": "Diabetes is an ailment in which glucose level increase in at high rates in blood due to body\u2019s inability to metabolize it. This happens when body does not produce sufficient amount of insulin or it does not respond to it properly. Critical and long-term health issues arise if diabetes is not handled or properly treated which includes: heart problems, disorders of the lungs, skin and liver complications, nerve damage, etc. With increasing number of diabetic patients, its early detection becomes essential. In this paper, our major focus areas are data mining and fuzzy logic techniques used in diabetes diagnosis. Data mining is used for locating patterns in huge datasets using a composition of different methods of machine learning, database manipulations and statistics. Data mining offers a lot of methods to inspect large data considering the expected outcome to find the hidden knowledge. Fuzzy logic is similar to human reasoning system and hence it can handle the uncertainties found in the data of medical diagnosis. These systems are called expert systems. The fuzzy expert systems (FES) analyze the knowledge from the available data which might be vague and suggests linguistic concept with huge approximation as its core to medical texts. In this paper, the methodology section delivers the pipeline of various tasks such as selecting the dataset, preprocessing the data by applying numerous methods such as standardization, normalization etc. After that, feature extraction technique is implemented on the dataset for improving the accuracy and finally dataset worked on data mining and fuzzy logic various classification algorithms. While analyzing different data mining methods, the accuracy computed through random forest classifiers as high as 99.7% and in case of numerous fuzzy logic approaches, high precision and low complexity was found to contribute a fairly high accuracy of 96%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "08957967",
        "isbn": null,
        "journal": "Seminars in Vascular Surgery",
        "publisher": null,
        "title": "currentapplicationsofartificialintelligenceinvascularsurgery",
        "booktitle": null,
        "doi": "10.1053/j.semvascsurg.2021.10.008",
        "author": [
            "Fischer, Uwe",
            "Shireman, Paula",
            "Lin, Judith"
        ],
        "keywords": [
            ""
        ],
        "abstract": "ABSTRACT Basic foundations of artificial intelligence (AI) include analyzing large amounts of data, recognizing patterns, and predicting outcomes. At the core of AI are well-defined areas, such as machine learning, natural language processing, artificial neural networks, and computer vision. Although research and development of AI in health care is being conducted in many medical subspecialties, only a few applications have been implemented in clinical practice. This is true in vascular surgery, where applications are mostly in the translational research stage. These AI applications are being evaluated in the realms of vascular diagnostics, perioperative medicine, risk stratification, and outcome prediction, among others. Apart from the technical challenges of AI and research outcomes on safe and beneficial use in patient care, ethical issues and policy surrounding AI will present future challenges for its successful implementation. This review will give a brief overview and a basic understanding of AI and summarize the currently available and used clinical AI applications in vascular surgery.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01692070",
        "isbn": null,
        "journal": "International Journal of Forecasting",
        "publisher": null,
        "title": "theeffectofspatiotemporalresolutiononpredictivepolicingmodelperformance",
        "booktitle": null,
        "doi": "10.1016/j.ijforecast.2020.03.006",
        "author": [
            "Rummens, Anneleen",
            "Hardyns, Wim"
        ],
        "keywords": [
            "Predictive policing, Crime forecasting, Spatiotemporal forecasting, Decision making, Predictive modeling"
        ],
        "abstract": "Being able to anticipate crime such that new crime events can be dealt with effectively or prevented entirely, leads police forces worldwide to look at applying predictive policing, which provides predictions of times and places at risk for crime, such that proactive preventative measures can be taken. Ideally, predictive policing models predict crime at a high spatio-temporal level, while also providing optimal prediction performance. The main objective of this paper is therefore to evaluate the impact of varying grid resolution, temporal resolution and historical time frame on prediction performance. To investigate this, we analyse home burglary data from a large city in Belgium and predict new crime events using a range of parameter values, comparing the resulting prediction performances. Given the potential prediction performance costs associated with prediction at a high spatio-temporal resolution, consideration should be given to balance practical requirements with performance requirements.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.779",
        "scimago_value": "1,268"
    },
    {
        "issnkey": "02673649",
        "isbn": null,
        "journal": "Computer Law & Security Review",
        "publisher": null,
        "title": "trilemmaandtripartitiontheregulatoryparadigmsofcrossborderpersonaldatatransferintheeutheusandchina",
        "booktitle": null,
        "doi": "10.1016/j.clsr.2021.105610",
        "author": [
            "Zheng, Guan"
        ],
        "keywords": [
            "Trilemma, Tripartition, Personal data protection, Free cross-border flow, National jurisdiction"
        ],
        "abstract": "The regulation of the cross-border transfer of personal data is a major issue of globalization in the digital era. The key point for lawmakers is how to choose two of the following three elements in the trilemma: personal data protection, free transborder flow of information and the expansion of national jurisdiction. The EU, the U.S. and China adopt their own decisions, resulting in three inherently incompatible legislative paradigms, which has led to the restricted flow of personal data around the world as well as the free flow in three different regions, with the EU, the U.S. and China as the center of each region. In this way, the regulating paradigms of cross-border personal data transfer presents a pattern of tripartition.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,815"
    },
    {
        "issnkey": "15698432",
        "isbn": null,
        "journal": "International Journal of Applied Earth Observation and Geoinformation",
        "publisher": null,
        "title": "applicationoftrainingdataaffectssuccessinbroadscalelocalclimatezonemapping",
        "booktitle": null,
        "doi": "10.1016/j.jag.2021.102482",
        "author": [
            "Xu, Chunxue",
            "Hystad, Perry",
            "Chen, Rui",
            "{Van Den Hoek}, Jamon",
            "Hutchinson, Rebecca",
            "Hankey, Steve",
            "Kennedy, Robert"
        ],
        "keywords": [
            "Local climate zone, Machine learning, Training areas, Crowdsourced data, Spatial autocorrelation"
        ],
        "abstract": "Satellite imagery has been widely used to map urbanization processes. To address the urgent need for urban landscape mapping that goes beyond urban footprint analysis, the local climate zone (LCZ) scheme has been increasingly used to reveal the urban forms and functions important to urban heat islands and micro-climates across the globe. As with most supervised classification strategies, proper application of training data is critical for the success of LCZ classification models. However, the collection and application of LCZ training areas brings with it two challenges that may affect mapping success. First, because digitizing training areas is a time-consuming task, there is a broad effort in the LCZ mapping community to create a crowdsourced data collection among different experts. However, this strategy likely leads to inconsistencies in labels that could weaken models. Second, the LCZ labeling process typically involves the delineation of large zones from which multiple training samples are drawn, but those samples are likely spatially autocorrelated and lead to overly optimistic estimates of model accuracy. Although both effects -- inconsistent labeling and spatial autocorrelation -- are theoretically possible, it is unknown whether they substantially affect accuracy. We investigated both issues, specifically asking: (i) how do the discrepancies of LCZ labeling by different experts impact broad-scale LCZ mapping? (ii) to what extent does spatial correlation affect model prediction power? We used two classifiers (Random Forests and ResNets) to map eight metropolitan areas in the US into LCZs, comparing training areas drawn by different or consistent interpreters, and data splitting strategy using rules that allow or reduce spatial autocorrelation. We found large discrepancies among results built from crowdsourced training areas digitized by different experts; improving the consistency of labels can lead to substantial improvements in LCZ classification accuracy. Second, we found that spatial autocorrelation can boost the apparent accuracy of the classifier by 16% to 21%, leading to erroneous interpretation of mapping results. The two effects interplay as well: spatial autocorrelation in the raw data can lead to an underestimation of the model\u2019s predictive error when modeling with crowdsourced training areas of high inconsistency. Due to the uncertainty in the labeling process and spatial autocorrelation in derived training data, broad-scale LCZ mapping results should be interpreted with caution.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.933",
        "scimago_value": "1,623"
    },
    {
        "issnkey": "23525568",
        "isbn": null,
        "journal": "Anaesthesia Critical Care & Pain Medicine",
        "publisher": null,
        "title": "ismultisourcefeedbackthefutureofperioperativemedicine",
        "booktitle": null,
        "doi": "10.1016/j.accpm.2021.100886",
        "author": [
            "Forget, Patrice",
            "Dahlberg, Karuna"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,942"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "impactofcyberphysicallyenhancedmanufacturingontheproductrequirementdocumentationinhightechapplications",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.09.036",
        "author": [
            "H\u00e4nel, Albrecht",
            "Seidel, Andr\u00e9",
            "Mehling, Carl",
            "Dementyev, Alexander",
            "Kozak, Karol",
            "Seidel, Rudi",
            "Teicher, Uwe",
            "Hellmich, Arvid",
            "Drossel, Welf-Guntram",
            "Ihlenfeldt, Steffen"
        ],
        "keywords": [
            "Digital Manufacturing System, Modelling, Process control, Cyber-physical production"
        ],
        "abstract": "Conventional machining of high-tech parts and components is often associated with complex processes, long lead times and small to medium batch sizes. Obviously, there is great interest to shorten process development periods and increase process reliability. The transformation of conventional machines into cyber-physical production systems (CPPS) promises significant improvements here. In fact, CPPS allows real-time data acquisition and processing, enabling to target integrated process improvement and quality assurance. In the present work, a methodical procedure for structured data aggregation is introduced for a Cyber-physical enhanced machining process while being explained from a high-tech application point of view. This includes data generation, extraction, transfer and storage, data-consolidation, linkage, visualization and interpretation. Finally, the paper illustrates these aspects in terms of the effects on the \u201cdigital product requirements\u201d in order to achieve the presented \u201canalytics-ready\u201d data model using the example of a high-tech application.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "01692607",
        "isbn": null,
        "journal": "Computer Methods and Programs in Biomedicine",
        "publisher": null,
        "title": "theroleoftheinternetofthingsinhealthcarefuturetrendsandchallenges",
        "booktitle": null,
        "doi": "10.1016/j.cmpb.2020.105903",
        "author": [
            "Aghdam, Zahra",
            "Rahmani, Amir",
            "Hosseinzadeh, Mehdi"
        ],
        "keywords": [
            "Internet of Things, Healthcare, Future Trends, Systematic Review"
        ],
        "abstract": "Background and Objective With the recent advances in the Internet of Things (IoT), the field has become more and more developed in healthcare. The Internet of things will help physicians and hospital staff perform their duties comfortably and intelligently. With the latest advanced technologies, most of the challenges of using IoT have been resolved, and this technology can be a great revolution and has many benefits in the future of digital. Healthcare is one of the most useful areas for IoT use. The most important application of IoT is to monitor and make quick decisions in critical situations. Thanks to this technology-based treatment approach, there is an unprecedented opportunity to better the quality and productivity of treatments and better the patient's well-being and better government funding. Methods In this paper, we provide a comprehensive overview of the primary uses of IoT in healthcare. We used the Systematic Literature Review (SLR) method to analyze and comparison articles published in this field between 2015 and March 2020. Results A comprehensive taxonomy is presented based on the contents of the articles under study. In this article, a brief overview of selected articles based on research questions is given and highlights the most critical challenges and case studies for the future use of IoT in healthcare. Conclusions According to a detailed study of the 89 articles and a glimpse into about 208 articles, challenges and future trends in healthcare have been identified.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.428",
        "scimago_value": "0,924"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818015-0",
        "journal": null,
        "publisher": "Academic Press",
        "title": "glossary",
        "booktitle": "Executing Data Quality Projects (Second Edition)",
        "doi": "10.1016/B978-0-12-818015-0.09986-2",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822132-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter5trainingthebusinessdatastewards",
        "booktitle": "Data Stewardship (Second Edition)",
        "doi": "10.1016/B978-0-12-822132-7.00005-X",
        "author": [
            "Plotkin, David"
        ],
        "keywords": [
            "Training, Data Quality, Data Stewards, skills, roles, responsibilities"
        ],
        "abstract": "The outline of a training guide for new data stewards is presented here, along with explanations of items that are not otherwise included in this book.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "surfingblockchainwaveordrowningshapingthefutureofdistributedledgersanddecentralizedtechnologies",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2020.120463",
        "author": [
            "Centobelli, Piera",
            "Cerchione, Roberto",
            "Esposito, Emilio",
            "Oropallo, Eugenio"
        ],
        "keywords": [
            "Bibliometric analysis, Block-chain, Decentralized technology, Distributed ledger, Literature review, Network analysis, Performance analysis, Traceability, Tracking, Transparency, trust"
        ],
        "abstract": "Blockchain is a promising technology whose four TRs (TRaceability, TRacking, TRansparency, TRrust) features are bound to revolutionize material, information, financial flows and transactions inside and outside organisations. Many studies have been published showing the potential of this disruptive technology in many fields and this number is growing exponentially in recent years. This enormous amount of papers calls for a more systematic approach to analyse the overall trend in this research field. A bibliometric approach based on performance analysis and network analysis techniques is used to examine the evolution of blockchain technology research. Firstly, this paper contributes to the body of literature by discussing the most influential countries, authors, subject areas and journals of the current blockchain research. Secondly, this paper identifies six main clusters of blockchain-related research contributions and, based on the analysis on centrality and density measures, it classifies research themes in motor themes, basic themes, emerging or disappearing themes, and specialised themes. Despite the majority of contributions belong to the computer science subject area, many papers belonging to the technology management subject area provide pivotal insights for practitioners and policy makers. Specifically, they may exploit the results of this research to rethink many traditional processes in the light of blockchain technology implementation, exploit the benefits of the four TRs to manage processes, automate common tasks, generate actionable results, and improve daily operations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "13596446",
        "isbn": null,
        "journal": "Drug Discovery Today",
        "publisher": null,
        "title": "comingofageofallotropeproceedingsfromthefall2020allotropeconnect",
        "booktitle": null,
        "doi": "10.1016/j.drudis.2021.03.028",
        "author": [
            "Millecam, Todd",
            "Jarrett, Austin",
            "Young, Naomi",
            "Vanderwall, Dana",
            "{Della Corte}, Dennis"
        ],
        "keywords": [
            "Precompetitive consortium, Semantics, Metadata, Standard, Harmonization, Laboratory IT, Allotrope, Digital Lab"
        ],
        "abstract": "The Allotrope Foundation (AF) is a group of pharmaceutical, device vendor, and software companies that develops and releases technologies [the Allotrope Data Format (ADF), the Allotrope Foundation Ontology (AFO), and the Allotrope Data Models (ADM)] to simplify the exchange of electronic data. We present here the first comprehensive history of the AF, its structure, a list of members and partners, and an introduction to the technologies. Finally, we provide current insights into the adoption and development of the technologies by summarizing the Fall 2020 Allotrope Connect virtual conference. This overview provides an easy access to the AF and highlights opportunities for collaboration.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "comparisonbetweendatamaturityandmaintenancestrategyacasesutdy",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.324",
        "author": [
            "{H\u00f8j Brasen}, Lucas",
            "Groos, Oliver",
            "Tambo, Torben"
        ],
        "keywords": [
            "manufacturing, asset management, data maturity, sensor networks, predictive maintenance, Internet-of-Things, SME"
        ],
        "abstract": "With the rise of Industry 4.0, there has been a substantial drive towards sensor networks for enabling predictive maintenance as an essential component of asset management. This study analyses sensor data maturity and asset management strategy. A model is proposed for establishing a best-fit correlation between data maturity and maintenance strategy, both for the current situation and as a guide for future development. The findings are based on the literature and case studies for small and medium-sized enterprises. The research implication is to view enterprise strategy as a balance between the chosen maturity and operational needs. The practical implication is the possibility to sustain or improve and qualify investment planning.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "10848045",
        "isbn": null,
        "journal": "Journal of Network and Computer Applications",
        "publisher": null,
        "title": "fogdatamanagementavisionchallengesandfuturedirections",
        "booktitle": null,
        "doi": "10.1016/j.jnca.2020.102882",
        "author": [
            "Sadri, Ali",
            "Rahmani, Amir",
            "Saberikamarposhti, Morteza",
            "Hosseinzadeh, Mehdi"
        ],
        "keywords": [
            "Fog computing, Internet of things, Data management, Data processing, Data analytics, Data storage, Data security, Systematic literature review"
        ],
        "abstract": "Cloud computing with its key facets and its inherent advantages still faces several challenges in the Internet of Things (IoT) ecosystem. The distance among the IoT end devices and cloud computing might be a problem for latency-sensitive applications such as catastrophe management and content transference applications. Fog computing is a novel paradigm to address such issues that playacts a significant role in massive and real-time data management systems in an IoT environment. Particularly IoT data management by fog computing is one important phase for latency reduction in latency-sensitive applications and necessary to generate more skilled knowledge and intelligent decisions. In this study, we used the SLR (systematic literature review) method to survey fog data management to understand the various topics and main contexts in this domain that have been newly offered. The target of this article is classifying and analyzing the researches about the fog data management domain which has been released from 2014 to 2019. A context-based taxonomy is offered for fog data management including data processing, data storage and data security based on the context of papers that are elected with the SLR method in our study. Based on presented technical taxonomy, the grouped papers in any context are compared with each other pursuant to some metrics of fog data management reference model. Then, for any selected research, the new findings, advantages, and weaknesses are debated. Finally, based on studies the open issues in fog data management and their related challenges for future researches are highlighted.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03787206",
        "isbn": null,
        "journal": "Information & Management",
        "publisher": null,
        "title": "construingonlineconsumersinformationprivacydecisionstheimpactofpsychologicaldistance",
        "booktitle": null,
        "doi": "10.1016/j.im.2021.103497",
        "author": [
            "Bandara, Ruwan",
            "Fernando, Mario",
            "Akter, Shahriar"
        ],
        "keywords": [
            "Construal level theory, Psychological distance, Privacy concerns, Privacy behavior, Privacy empowerment"
        ],
        "abstract": "The role of subjective distance and mental representations in understanding consumers\u2019 information privacy decisions is underexplored in the literature. This study draws on construal level theory and power-responsibility equilibrium framework of privacy to explain consumer privacy behavior based on the interplay between three psychological constructs, namely, privacy concerns, privacy empowerment, and the psychological distance of privacy. This study empirically validates the psychological distance of privacy construct and the results indicate the capability of psychological distance to explain privacy behavior and to moderate the relationship between privacy concerns and privacy behavior. The findings also suggest that empowered consumers\u2019 privacy behavior does not vary despite the degree of psychological distance. Our findings have implications for the privacy scholarship, consumers and e-commerce system developers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,147"
    },
    {
        "issnkey": "01420615",
        "isbn": null,
        "journal": "International Journal of Electrical Power & Energy Systems",
        "publisher": null,
        "title": "digitalizationanddecentralizationdrivingtransactiveenergyinternetkeytechnologiesandinfrastructures",
        "booktitle": null,
        "doi": "10.1016/j.ijepes.2020.106593",
        "author": [
            "Wu, Ying",
            "Wu, Yanpeng",
            "Guerrero, Josep",
            "Vasquez, Juan"
        ],
        "keywords": [
            "Digitalization, Decentralization, Energy Internet, IoT, Blockchain, Microgrids, Energy router, Energy transaction"
        ],
        "abstract": "With the increasing access of renewable energy resources and fast ubiquitous connection of everything, the traditional one-way power flow from centralized generation to end consumers will give way to bidirectional-way power flow with multidirectional energy network among central grids and distributed prosumers. To empower the prosumer-centric Energy Internet (EI) and enhance the integration of energy-aware services, digitalization and decentralization are the key enablers to achieve transactive EI. This article presents a systematic overview on how Internet of Things (IoT) drives the digitalization of transactive EI and how blockchain empowers the decentralization of transactive EI. A comprehensive discussion on the key infrastructures is provided for presenting how to implement digitalization and decentralization of transactive EI, including the last mile \u201cAdvanced metering infrastructure\u201d (AMI), renewables integrator \u201csmart inverter\u201d, energy flow adjuster \u201cenergy router\u201d, and coordinator \u201cMicrogrid\u201d. Challenges and future trends are discussed from an extensive point of view, including energy physical space, data cyber space and human social space.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,050"
    },
    {
        "issnkey": "24726303",
        "isbn": null,
        "journal": "SLAS Technology",
        "publisher": null,
        "title": "artificialintelligenceeffectingaparadigmshiftindrugdevelopment",
        "booktitle": null,
        "doi": "10.1177/2472630320956931",
        "author": [
            "Rashid, Masturah"
        ],
        "keywords": [
            "artificial intelligence, drug development, drug discovery, industry"
        ],
        "abstract": "The inverse relationship between the cost of drug development and the successful integration of drugs into the market has resulted in the need for innovative solutions to overcome this burgeoning problem. This problem could be attributed to several factors, including the premature termination of clinical trials, regulatory factors, or decisions made in the earlier drug development processes. The introduction of artificial intelligence (AI) to accelerate and assist drug development has resulted in cheaper and more efficient processes, ultimately improving the success rates of clinical trials. This review aims to showcase and compare the different applications of AI technology that aid automation and improve success in drug development, particularly in novel drug target identification and design, drug repositioning, biomarker identification, and effective patient stratification, through exploration of different disease landscapes. In addition, it will also highlight how these technologies are translated into the clinic. This paradigm shift will lead to even greater advancements in the integration of AI in automating processes within drug development and discovery, enabling the probability and reality of attaining future precision and personalized medicine. \u6458\u8981 \u85ac\u5264\u958b\u767a\u306b\u304b\u304b\u308b\u30b3\u30b9\u30c8\u3068\u3001\u85ac\u5264\u306e\u6210\u679c\u3092\u5e02\u5834\u306b\u7d44\u307f\u5165\u308c\u308b\u3053\u3068\u3068\u306e\u9593\u306b\u306f\u9006\u76f8\u95a2\u304c\u3042\u308a\u3001\u6025\u901f\u306b\u62e1\u5927\u3057\u3066\u3044\u308b\u3053\u306e\u554f\u984c\u3092\u514b\u670d\u3059\u308b\u305f\u3081\u306e\u9769\u65b0\u7684\u306a\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u304c\u6c42\u3081\u3089\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u304d\u305f\u3002\u3053\u306e\u554f\u984c\u306f\u3001\u81e8\u5e8a\u8a66\u9a13\u306e\u65e9\u671f\u4e2d\u6b62\u3001\u898f\u5236\u8981\u56e0\u3001\u307e\u305f\u306f\u85ac\u5264\u958b\u767a\u30d7\u30ed\u30bb\u30b9\u306e\u521d\u671f\u6bb5\u968e\u3067\u4e0b\u3055\u308c\u308b\u5224\u65ad\u306a\u3069\u3001\u8907\u6570\u306e\u8981\u56e0\u306b\u8d77\u56e0\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002\u85ac\u5264\u958b\u767a\u3092\u3088\u308a\u8fc5\u901f\u5316\u3057\u88dc\u52a9\u3059\u308b\u305f\u3081\u306e\u4eba\u5de5\u77e5\u80fd\uff08artificial intelligence\uff1aAI\uff09\u306e\u5c0e\u5165\u306f\u3001\u6bd4\u8f03\u7684\u5b89\u4fa1\u306a\u3046\u3048\u52b9\u7387\u7684\u306a\u30d7\u30ed\u30bb\u30b9\u3092\u3082\u305f\u3089\u3057\u3001\u6700\u7d42\u7684\u306b\u81e8\u5e8a\u8a66\u9a13\u306e\u6210\u529f\u7387\u3092\u5411\u4e0a\u3055\u305b\u3066\u3044\u308b\u3002\u672c\u30ec\u30d3\u30e5\u30fc\u306e\u306d\u3089\u3044\u306f\u3001\u3055\u307e\u3056\u307e\u306a\u75be\u60a3\u3092\u53d6\u308a\u5dfb\u304f\u72b6\u6cc1\u3092\u63a2\u7a76\u3057\u306a\u304c\u3089\u3001\u85ac\u5264\u958b\u767a\u306e\u81ea\u52d5\u5316\u3092\u652f\u63f4\u3059\u308b\u3068\u3068\u3082\u306b\u3001\u85ac\u5264\u958b\u767a\u306e\u3001\u7279\u306b\u65b0\u85ac\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u7279\u5b9a\u3084\u8a2d\u8a08\u3001\u30c9\u30e9\u30c3\u30b0\u30ea\u30dd\u30b8\u30b7\u30e7\u30cb\u30f3\u30b0\u3001\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u7279\u5b9a\u3001\u6709\u52b9\u306a\u60a3\u8005\u5c64\u5225\u5316\u306a\u3069\u3092\u9996\u5c3e\u3088\u304f\u904b\u3076AI\u6280\u8853\u306e\u3055\u307e\u3056\u307e\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u793a\u3057\u3066\u6bd4\u8f03\u3059\u308b\u3053\u3068\u306b\u3042\u308b\u3002\u307e\u305f\u3001\u3053\u308c\u3089\u306e\u6280\u8853\u304c\u81e8\u5e8a\u5834\u9762\u3067\u6d3b\u7528\u3055\u308c\u308b\u65b9\u6cd5\u306b\u3082\u6ce8\u76ee\u3059\u308b\u3002\u3053\u306e\u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8\u306f\u3001\u85ac\u5264\u958b\u767a\u3084\u5275\u85ac\u306e\u7bc4\u7587\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u81ea\u52d5\u5316\u3059\u308b\u969b\u306bAI\u3092\u7d44\u307f\u8fbc\u3080\u3053\u3068\u306e\u5229\u70b9\u3092\u3055\u3089\u306b\u62e1\u5927\u3059\u308b\u3053\u3068\u306b\u3064\u306a\u304c\u308a\u3001\u3072\u3044\u3066\u306f\u5c06\u6765\u306e\u9ad8\u7cbe\u5ea6\u533b\u7642\u3084\u30aa\u30fc\u30c0\u30e1\u30a4\u30c9\u533b\u7642\u306e\u5b9f\u73fe\u306e\u53ef\u80fd\u6027\u3092\u9ad8\u3081\u308b\u3002 \ucd08\ub85d \uc57d\ubb3c \uac1c\ubc1c \ube44\uc6a9\uacfc \uc57d\ubb3c\uc758 \uc131\uacf5\uc801\uc778 \uc2dc\uc7a5 \ud1b5\ud569 \uac04\uc758 \uc5ed\uc0c1\uad00\uad00\uacc4\ub294 \uc774\ub7ec\ud55c \uae09\uc99d\ud558\ub294 \ubb38\uc81c\ub4e4\uc744 \uadf9\ubcf5\ud558\uae30 \uc704\ud55c \ud601\uc2e0\uc801\uc778 \ud574\ubc95\uc774 \ud544\uc694\ud558\ub2e4\ub294 \uc778\uc2dd\uc744 \uc81c\uae30\ud588\ub2e4. \uc774\ub7ec\ud55c \ubb38\uc81c\ub294 \uc784\uc0c1\uc2dc\ud5d8\uc758 \uc870\uae30 \uc885\ub8cc, \uaddc\uc81c \uc694\uc778 \ub610\ub294 \ucd08\uae30 \uc57d\ubb3c \uac1c\ubc1c \uacfc\uc815\uc5d0\uc11c \uc774\ub8e8\uc5b4\uc9c4 \uacb0\uc815\uc744 \ud3ec\ud568\ud55c \uc5ec\ub7ec \uc694\uc778\ub4e4\uc5d0\uc11c \uae30\uc778\ud560 \uc218 \uc788\ub2e4. \uc57d\ubb3c \uac1c\ubc1c\uc744 \uac00\uc18d\ud654\ud558\uace0 \uc9c0\uc6d0\ud558\uae30 \uc704\ud55c \uc778\uacf5\uc9c0\ub2a5(artificial intelligence, AI)\uc758 \ub3c4\uc785\uc73c\ub85c \uc57d\ubb3c \uac1c\ubc1c \uacfc\uc815\uc774 \ub354\uc6b1 \uc800\ub834\ud558\uace0 \ub354\uc6b1 \ud6a8\uc728\uc801\uc774 \ub418\uc5c8\uc73c\uba70 \uad81\uadf9\uc801\uc73c\ub85c \uc784\uc0c1\uc2dc\ud5d8\uc758 \uc131\uacf5\ub960\uc774 \ud5a5\uc0c1\ub418\uc5c8\ub2e4. \ubcf8 \uc885\uc124\uc758 \ubaa9\uc801\uc740 \ub2e4\uc591\ud55c \uc9c8\ubcd1 \ud658\uacbd\uc758 \ud0d0\uc0c9\uc744 \ud1b5\ud574 \uc790\ub3d9\ud654\ub97c \uc9c0\uc6d0\ud558\uace0 \ud2b9\ud788 \uc2e0\uc57d\uc758 \ud45c\uc801 \ud655\uc778 \ubc0f \uc124\uacc4, \uc57d\ubb3c\uc758 \uc7ac\ud3ec\uc9c0\uc154\ub2dd, \uc0dd\uccb4\ud45c\uc9c0\uc790 \ud655\uc778 \ubc0f \ud6a8\uacfc\uc801\uc778 \ud658\uc790 \uce35\ud654 \ubd80\ubd84\uc5d0\uc11c \uc57d\ubb3c \uac1c\ubc1c\uc758 \uc131\uacf5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 AI \uae30\uc220\uc758 \ub2e4\uc591\ud55c \uc801\uc6a9\uc744 \ubcf4\uc5ec\uc8fc\uace0 \ube44\uad50\ud558\ub294 \uac83\uc774\ub2e4. \ub610\ud55c \uc774\ub7ec\ud55c \uae30\uc220\ub4e4\uc774 \uc784\uc0c1\uc73c\ub85c \uc804\ud658\ub418\ub294 \ubc29\uc2dd\uc5d0 \ub300\ud574\uc11c\ub3c4 \uac15\uc870\ud560 \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c \ud328\ub7ec\ub2e4\uc784 \uc804\ud658\uc740 \uc2e0\uc57d \uac1c\ubc1c \ubc0f \ubc1c\uacac\uc758 \uc790\ub3d9\ud654 \uacfc\uc815\uc5d0\uc11c AI\uc758 \ud1b5\ud569\uc744 \ub354\uc6b1 \ud06c\uac8c \ubc1c\uc804\uc2dc\ucf1c \ud5a5\ud6c4 \uc815\ubc00\uc758\ud559 \ubc0f \ub9de\ucda4 \uc758\ud559 \ub2ec\uc131 \uac00\ub2a5\uc131\uc744 \ub192\uc774\uace0 \uadf8 \uc2e4\ud604\uc744 \uac00\ub2a5\ud558\uac8c \ud560 \uac83\uc774\ub2e4. \u6284\u9332 \u85ac\u5264\u958b\u767a\u306b\u304b\u304b\u308b\u30b3\u30b9\u30c8\u3068\u3001\u85ac\u5264\u306e\u6210\u679c\u3092\u5e02\u5834\u306b\u7d44\u307f\u5165\u308c\u308b\u3053\u3068\u3068\u306e\u9593\u306b\u306f\u9006\u76f8\u95a2\u304c\u3042\u308a\u3001\u6025\u901f\u306b\u62e1\u5927\u3057\u3066\u3044\u308b\u3053\u306e\u554f\u984c\u3092\u514b\u670d\u3059\u308b\u305f\u3081\u306e\u9769\u65b0\u7684\u306a\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u304c\u6c42\u3081\u3089\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u304d\u305f\u3002\u3053\u306e\u554f\u984c\u306f\u3001\u81e8\u5e8a\u8a66\u9a13\u306e\u65e9\u671f\u4e2d\u6b62\u3001\u898f\u5236\u8981\u56e0\u3001\u307e\u305f\u306f\u85ac\u5264\u958b\u767a\u30d7\u30ed\u30bb\u30b9\u306e\u521d\u671f\u6bb5\u968e\u3067\u4e0b\u3055\u308c\u308b\u5224\u65ad\u306a\u3069\u3001\u8907\u6570\u306e\u8981\u56e0\u306b\u8d77\u56e0\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002\u85ac\u5264\u958b\u767a\u3092\u3088\u308a\u8fc5\u901f\u5316\u3057\u88dc\u52a9\u3059\u308b\u305f\u3081\u306e\u4eba\u5de5\u77e5\u80fd\uff08artificial intelligence\uff1aAI\uff09\u306e\u5c0e\u5165\u306f\u3001\u6bd4\u8f03\u7684\u5b89\u4fa1\u306a\u3046\u3048\u52b9\u7387\u7684\u306a\u30d7\u30ed\u30bb\u30b9\u3092\u3082\u305f\u3089\u3057\u3001\u6700\u7d42\u7684\u306b\u81e8\u5e8a\u8a66\u9a13\u306e\u6210\u529f\u7387\u3092\u5411\u4e0a\u3055\u305b\u3066\u3044\u308b\u3002\u672c\u30ec\u30d3\u30e5\u30fc\u306e\u306d\u3089\u3044\u306f\u3001\u3055\u307e\u3056\u307e\u306a\u75be\u60a3\u3092\u53d6\u308a\u5dfb\u304f\u72b6\u6cc1\u3092\u63a2\u7a76\u3057\u306a\u304c\u3089\u3001\u85ac\u5264\u958b\u767a\u306e\u81ea\u52d5\u5316\u3092\u652f\u63f4\u3059\u308b\u3068\u3068\u3082\u306b\u3001\u85ac\u5264\u958b\u767a\u306e\u3001\u7279\u306b\u65b0\u85ac\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u7279\u5b9a\u3084\u8a2d\u8a08\u3001\u30c9\u30e9\u30c3\u30b0\u30ea\u30dd\u30b8\u30b7\u30e7\u30cb\u30f3\u30b0\u3001\u30d0\u30a4\u30aa\u30de\u30fc\u30ab\u30fc\u306e\u7279\u5b9a\u3001\u6709\u52b9\u306a\u60a3\u8005\u5c64\u5225\u5316\u306a\u3069\u3092\u9996\u5c3e\u3088\u304f\u904b\u3076AI\u6280\u8853\u306e\u3055\u307e\u3056\u307e\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u793a\u3057\u3066\u6bd4\u8f03\u3059\u308b\u3053\u3068\u306b\u3042\u308b\u3002\u307e\u305f\u3001\u3053\u308c\u3089\u306e\u6280\u8853\u304c\u81e8\u5e8a\u5834\u9762\u3067\u6d3b\u7528\u3055\u308c\u308b\u65b9\u6cd5\u306b\u3082\u6ce8\u76ee\u3059\u308b\u3002\u3053\u306e\u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8\u306f\u3001\u85ac\u5264\u958b\u767a\u3084\u5275\u85ac\u306e\u7bc4\u7587\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u81ea\u52d5\u5316\u3059\u308b\u969b\u306bAI\u3092\u7d44\u307f\u8fbc\u3080\u3053\u3068\u306e\u5229\u70b9\u3092\u3055\u3089\u306b\u62e1\u5927\u3059\u308b\u3053\u3068\u306b\u3064\u306a\u304c\u308a\u3001\u3072\u3044\u3066\u306f\u5c06\u6765\u306e\u9ad8\u7cbe\u5ea6\u533b\u7642\u3084\u30aa\u30fc\u30c0\u30e1\u30a4\u30c9\u533b\u7642\u306e\u5b9f\u73fe\u306e\u53ef\u80fd\u6027\u3092\u9ad8\u3081\u308b\u3002",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "usingsemanticenrichmentmethodsinexpertsearchsystemforrecruitmentprocessinitcorporation",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.09.011",
        "author": [
            "Wosiak, Agnieszka"
        ],
        "keywords": [
            "expert search system, semantic enrichment, text data analysis, preprocessing and outlier detection",
            ""
        ],
        "abstract": "The problem of intelligent information retrieval and semantic enrichment becomes more and more popular due to the difficulty of searching and analyzing large text datasets. The common approach assumes user manual queries in natural language. Various semantic enrichment methods and intelligent text searching allow obtaining more accurate results leading to broader knowledge and user satisfaction. This research presents state-of-the-art methods of searching with enrichment and building rankings of results for the expert recruitment process in IT industry. The proposed model implements full-text search, semantic enrichment, and machine learning to match experts with job offers. Different data sources on expert competencies were used, including curricula vitae, historical data, and Internet resources. The testing results confirm an improvement in the search quality compared to the existing systems in the recruitment company.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "08936080",
        "isbn": null,
        "journal": "Neural Networks",
        "publisher": null,
        "title": "semisuperviseddisentangledframeworkfortransferablenamedentityrecognition",
        "booktitle": null,
        "doi": "10.1016/j.neunet.2020.11.017",
        "author": [
            "Hao, Zhifeng",
            "Lv, Di",
            "Li, Zijian",
            "Cai, Ruichu",
            "Wen, Wen",
            "Xu, Boyan"
        ],
        "keywords": [
            "Named entity recognition, Semi-supervised learning, Transfer learning, Disentanglement"
        ],
        "abstract": "Named entity recognition (NER) for identifying proper nouns in unstructured text is one of the most important and fundamental tasks in natural language processing. However, despite the widespread use of NER models, they still require a large-scale labeled data set, which incurs a heavy burden due to manual annotation. Domain adaptation is one of the most promising solutions to this problem, where rich labeled data from the relevant source domain are utilized to strengthen the generalizability of a model based on the target domain. However, the mainstream cross-domain NER models are still affected by the following two challenges (1) Extracting domain-invariant information such as syntactic information for cross-domain transfer. (2) Integrating domain-specific information such as semantic information into the model to improve the performance of NER. In this study, we present a semi-supervised framework for transferable NER, which disentangles the domain-invariant latent variables and domain-specific latent variables. In the proposed framework, the domain-specific information is integrated with the domain-specific latent variables by using a domain predictor. The domain-specific and domain-invariant latent variables are disentangled using three mutual information regularization terms, i.e., maximizing the mutual information between the domain-specific latent variables and the original embedding, maximizing the mutual information between the domain-invariant latent variables and the original embedding, and minimizing the mutual information between the domain-specific and domain-invariant latent variables. Extensive experiments demonstrated that our model can obtain state-of-the-art performance with cross-domain and cross-lingual NER benchmark data sets.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.050",
        "scimago_value": "1,396"
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "internetofthingsemergingimpactsondigitalreporting",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2021.01.056",
        "author": [
            "Valentinetti, Diego",
            "{Flores Mu\u00f1oz}, Francisco"
        ],
        "keywords": [
            "Digital reporting, Internet of Things (IoT), Corporate communication, Media richness"
        ],
        "abstract": "This paper develops a future research agenda for fostering a resurged interest in digital reporting through the emergence of the Internet of Things (IoT). Drawing upon the media richness and corporate communication frameworks, we enquire the evolving stages of digital reporting and review the contemporary academic literature on IoT to discuss the opportunities and practical concerns for developing future advances in the digitisation of accounting information. Our analysis explores how the media richness-related features of IoT fit, challenge and enhance the dynamics that constitute corporate communication, i.e.: communicator, message, medium/channel, audience, relationship and conversation. This paper opens new directions of research on advances in digital reporting and sheds light on the innovative ways accounting information is co-produced and shared through the IoT.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "14715953",
        "isbn": null,
        "journal": "Nurse Education in Practice",
        "publisher": null,
        "title": "integratinginformaticsintoundergraduatenursingeducationacasestudyusingaspirallearningapproach",
        "booktitle": null,
        "doi": "10.1016/j.nepr.2020.102934",
        "author": [
            "O'Connor, Siobhan",
            "LaRue, Elizabeth"
        ],
        "keywords": [
            "Nursing education, Informatics, Technology, Digital health"
        ],
        "abstract": "A gap in informatics expertise amongst nursing students, practising staff and faculty has been noted globally, which reduces the potential for nurses to utilise technology to enhance patient care. National nursing education strategies and recommendations from professional associations have identified digital health as an area that needs investment. This case study describes how health informatics is being integrated into a Bachelor of Nursing programme in the United Kingdom. An international collaboration with a US-UK Fulbright Specialist Scholar enabled individual learning units corresponding to key health informatics competencies to be designed and incorporated into a pedagogic framework grounded in the spiral learning approach. This approach is proposed as one way to integrate informatics into nursing education, so students can become competent clinicians that are able to deliver technology enabled care in the health service.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22142126",
        "isbn": null,
        "journal": "Journal of Information Security and Applications",
        "publisher": null,
        "title": "animprovedconvolutionmerkletreebasedblockchainelectronicmedicalrecordsecurestoragescheme",
        "booktitle": null,
        "doi": "10.1016/j.jisa.2021.102952",
        "author": [
            "Zhu, Hegui",
            "Guo, Yujia",
            "Zhang, Libo"
        ],
        "keywords": [
            "Electronic medical record, Blockchain, Convolution operation, Improved convolution Merkle tree"
        ],
        "abstract": "Presently, more and more electronic medical records (EMR) are used to replace traditional recording methods. However, there has potential safety hazard in the transmission of EMR because of the personal privacy disclosure. So, how to store, transmit and share EMR effectively and securely has become a research hotspot. In this paper, we propose an improved Merkle tree based-blockchain EMR storage scheme. The hallmark of the proposed scheme is that we employ the convolutional layer structure to replace the original binary tree structure in the proposed convolution Merkle tree, which can improve the efficiency effectively. Experiments show that the number of stored nodes has decreased significantly with the same amount of input data, and the layers number of the improved convolution Merkle tree and hash calculated amount are all reduced dramatically. The security and efficiency analysis also illustrate that the proposed scheme can provide a reliable choice for the further development of data storage security in the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.872",
        "scimago_value": "0,610"
    },
    {
        "issnkey": "",
        "isbn": "978-1-78548-043-0",
        "journal": null,
        "publisher": "ISTE",
        "title": "1informationanddatamanagement",
        "booktitle": "Records Management At the Heart of Business Processes",
        "doi": "10.1016/B978-1-78548-043-0.50001-9",
        "author": [
            "Ott, Florence"
        ],
        "keywords": [
            "Big data, Business processes, Data governance, Digital environment, Information, Open data, Protection of personal data, Records continuum, Three ages"
        ],
        "abstract": "Abstract: While the digital world brings advantages by simplifying many processes, it also makes the context of records production more complex and difficult to understand according to traditional archival principles. The explosion in the volume of information leads to the multiplication of actors, the acceleration of exchanges, and the atomization and fragmentation of information with numerous digital files to replace what was formerly a paper document or the reproduction of several copies.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23519789",
        "isbn": null,
        "journal": "Procedia Manufacturing",
        "publisher": null,
        "title": "developingamaturitybasedworkflowfortheimplementationofmlapplicationsusingtheexampleofademandforecast",
        "booktitle": null,
        "doi": "10.1016/j.promfg.2021.07.006",
        "author": [
            "Schreckenberg, Felix",
            "Moroff, Nikolas"
        ],
        "keywords": [
            "artificial intelligenz, maturity-based workflow, challenges AI"
        ],
        "abstract": "The aim of the article is to present a guideline that has been developed in the form of a workflow to identify the capability of an organisation to implement machine learning (ML) applications on the one hand and, on the other hand, to describe a maturity-dependent procedure for the development of an ML application based on this knowledge. With the help of the guideline, application-specific requirements can be identified based on the phases of the development process of an ML application adapted to the corporate environment. The article begins with the motivation for using machine learning methods and presents the challenges in implementing these methods. Based on a literature review, a maturity-based approach is designed and the developed and adapted development phases from the literature are described in a more detailed way. The individual characteristics of certain phases are specified based on the maturity level. As well, the weighting of certain maturity dimensions of the respective phase is highlighted. The article ends with an outlook on the further development of the created guideline.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,504"
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "scientometricreviewofartificialintelligenceforoperationsmaintenanceofwindturbinesthepastpresentandfuture",
        "booktitle": null,
        "doi": "10.1016/j.rser.2021.111051",
        "author": [
            "Chatterjee, Joyjit",
            "Dethlefs, Nina"
        ],
        "keywords": [
            "Wind turbines, Operations & maintenance, SCADA, Scientometric review, Artificial intelligence, Machine learning, Condition-based monitoring"
        ],
        "abstract": "Wind energy has emerged as a highly promising source of renewable energy in recent times. However, wind turbines regularly suffer from operational inconsistencies, leading to significant costs and challenges in operations and maintenance (O&M). Condition-based monitoring (CBM) and performance assessment/analysis of turbines are vital aspects for ensuring efficient O&M planning and cost minimisation. Data-driven decision making techniques have witnessed rapid evolution in the wind industry for such O&M tasks during the last decade, from applying signal processing methods in early 2010 to artificial intelligence (AI) techniques, especially deep learning in 2020. In this article, we utilise statistical computing to present a scientometric review of the conceptual and thematic evolution of AI in the wind energy sector, providing evidence-based insights into present strengths and limitations of data-driven decision making in the wind industry. We provide a perspective into the future and on current key challenges in data availability and quality, lack of transparency in black box-natured AI models, and prevailing issues in deploying models for real-time decision support, along with possible strategies to overcome these problems. We hope that a systematic analysis of the past, present and future of CBM and performance assessment can encourage more organisations to adopt data-driven decision making techniques in O&M towards making wind energy sources more reliable, contributing to the global efforts of tackling climate change.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "22147853",
        "isbn": null,
        "journal": "Materials Today: Proceedings",
        "publisher": null,
        "title": "internetofthingsarchitectureandenablingtechnologies",
        "booktitle": null,
        "doi": "10.1016/j.matpr.2020.04.678",
        "author": [
            "Goyal, Parul",
            "Sahoo, Ashok",
            "Sharma, Tarun"
        ],
        "keywords": [
            "Enabling technologies, IoT, RFID, Sensors, ZigBee"
        ],
        "abstract": "Internet of Things is transforming real devices to smart intelligent virtual devices. In IoT day today devices of daily use are manufactured along with sensors which are capable for identification and sensing. They can be networked, are capable to process, can interact with other devices through Internet. IoT objective is to connect almost everything under a common infrastructure. This helps to control devices and will keep us informed about the status of devices. The paper aims to give Internet of Things overview, architectures, enabling technologies and their applications. It presents latest trends, current state, recent developments, challenges, security, privacy, applications of IoT and future research directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,341"
    },
    {
        "issnkey": "03756742",
        "isbn": null,
        "journal": "Journal of Geochemical Exploration",
        "publisher": null,
        "title": "knowledgediscoveryofgeochemicalpatternsfromadatadrivenperspective",
        "booktitle": null,
        "doi": "10.1016/j.gexplo.2021.106872",
        "author": [
            "Yin, Bojun",
            "Zuo, Renguang",
            "Xiong, Yihui",
            "Li, Yongsheng",
            "Yang, Weigang"
        ],
        "keywords": [
            "Data-driven, Knowledge discovery, Data science, Geochemical exploration"
        ],
        "abstract": "We have entered the fourth research paradigm with the overwhelming availability of vast amounts of data. The processing and mining these data for a better understanding of earth systems and predicting mineral resources is challenging. This study discusses a data-driven knowledge discovery of geochemical patterns and presents a case study of geochemical data processing from a data-driven perspective. We employed local indicators of spatial association (LISA), principal component analysis (PCA), and deep autoencoder network (DAN) procedures to explore spatial association of geochemical patterns, extract elemental associations, and detect geochemical anomalies related to AuSb mineralization in the Daqiao district, Gansu Province, China. The results indicate the following: (1) both Au and Sb, and Pb and Zn have a close spatial correlation, indicating genetic connections among them; (2) the elemental association of Au, Sb, As, Hg and Ag can be adopted as a geochemical signature for the discovery of AuSb polymetallic mineralization in the study area; and (3) the geochemical anomalies identified by DAN exhibit a strong spatial relationship with locations of known mineral deposits and can provide a significant clue for further mineral exploration in this district. These findings indicate that data-driven procedures can help in the knowledge discovery of geochemical patterns in mineral exploration. Additional efforts are required for data-driven knowledge discovery in both geochemical prospecting and mineral exploration.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.746",
        "scimago_value": "0,994"
    },
    {
        "issnkey": "01452134",
        "isbn": null,
        "journal": "Child Abuse & Neglect",
        "publisher": null,
        "title": "childmaltreatmentdataasummaryofprogressprospectsandchallenges",
        "booktitle": null,
        "doi": "10.1016/j.chiabu.2020.104650",
        "author": [
            "Fluke, John",
            "Tonmyr, Lil",
            "Gray, Jenny",
            "{Bettencourt Rodrigues}, Leonor",
            "Bolter, Flora",
            "Cash, Scottye",
            "Jud, Andreas",
            "Meinck, Franziska",
            "{Casas Mu\u00f1oz}, Abigail",
            "O\u2019Donnell, Melissa",
            "Pilkington, Rhiannon",
            "Weaver, Leemoy"
        ],
        "keywords": [
            "Child maltreatment data, Linked data, Data collection, Data analysis, Self-report data, Administrative data, Sentinel data, Data collection ethics, International comparison, Child maltreatment data utilization, Evaluation, Decision-making"
        ],
        "abstract": "Background In 1996, the ISPCAN Working Group on Child Maltreatment Data (ISPCAN-WGCMD) was established to provide an international forum in which individuals, who deal with child maltreatment data in their respective professional roles, can share concerns and solutions. Objective This commentary describes some of the key features and the status of child maltreatment related data collection addressed by the ISPCAN-WGCMD. Methods Different types of data collection methods including self-report, sentinel, and administrative data designs are described as well as how they address different needs for information to help understand child maltreatment and systems of prevention and intervention. Results While still lacking in many parts of the world, access to child maltreatment data has become much more widespread, and in many places a very sophisticated undertaking. Conclusion The ISPCAN-WGCMD has been an important forum for supporting the continued development and improvement in the global effort to understand and combat child maltreatment thus contributing to the long term goals of the UN Convention on the Rights of the Child. Nevertheless, based on what has been learned, even greater efforts are required to improve data in order to effectively combat child maltreatment.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,552"
    },
    {
        "issnkey": "20953119",
        "isbn": null,
        "journal": "Journal of Integrative Agriculture",
        "publisher": null,
        "title": "consumerswillingnesstopayforethicalconsumptioninitiativesonecommerceplatforms",
        "booktitle": null,
        "doi": "10.1016/S2095-3119(20)63584-5",
        "author": [
            "WANG, Er-peng",
            "AN, Ning",
            "GENG, Xian-hui",
            "GAO, Zhifeng",
            "KIPROP, Emmanuel"
        ],
        "keywords": [
            "ethical consumption, apples from poverty-stricken areas, WTP, interval regression"
        ],
        "abstract": "Despite China\u2019s fast-growing e-commerce and its great achievement in promoting poverty alleviation through consumption, little is known about Chinese consumers\u2019 online ethical consumption. Using the payment card elicitation method, this paper designs a within-subject survey and a between-subject survey to investigate Chinese consumers\u2019 quality perception and preference for apples from poverty-stricken areas. The results show that before \u201cinformation shock\u201d, emphasizing that taste and safety attributes of apples from poverty-stricken areas are the same as the conventional ones, Chinese consumers on average are willing to pay a 31% premium for apples from poverty-stricken areas. After \u201cinformation shock\u201d, both the within-subject and between-subject designs show a minimal drop of the premium, implying that the ethical attribute is the main motivation for buying apples from poverty-stricken areas. The regression results show that quality perception of private attributes has significant effect on consumers\u2019 willingness to pay (WTP) for apples from poverty-stricken areas, and trust in government supervision of e-commerce plays an essential role in motivating online ethical consumption.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.848",
        "scimago_value": "0,784"
    },
    {
        "issnkey": "15741192",
        "isbn": null,
        "journal": "Pervasive and Mobile Computing",
        "publisher": null,
        "title": "anintelligentchargingschememaximizingtheutilityforrechargeablenetworkinsmartcity",
        "booktitle": null,
        "doi": "10.1016/j.pmcj.2021.101457",
        "author": [
            "Ren, Yingying",
            "Liu, Anfeng",
            "Mao, Xingliang",
            "Li, Fangfang"
        ],
        "keywords": [
            "Mobile charging, Wireless energy transfer, Mobile chargers, Quality utility, Workload balance"
        ],
        "abstract": "The mobile charging scheme is a promising solution to extending the lifetime of the network by replenishing the energy for the sensing nodes, which has attracted more and more attention from the researchers. However, due to the limitation of energy storage both for sensing nodes and mobile chargers, not all the sensing nodes can be recharged in time by mobile chargers. Therefore, how to select appropriate sensing nodes and design the path for the mobile charger are the key to improve the system utility. This paper proposes an Intelligent Charging scheme Maximizing the Quality Utility (ICMQU) to design the charging path for the mobile charger. Comparing to the previous studies, we consider not only the utility of the data collected from the environment, but also the impact of sensing nodes with different quality. Quality Utility is proposed to optimize the charging path design. Besides, ICMQU designs the charging scheme for a single mobile charger and multiple mobile chargers simultaneously. For the charging scheme with multiple mobile chargers, the workload balance among different mobile chargers is also considered as well as the utility of the system. Extensive simulation results are provided, which demonstrates the proposed ICMQU scheme can significantly improve the utility of the system.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.453",
        "scimago_value": "0,687"
    },
    {
        "issnkey": "00099260",
        "isbn": null,
        "journal": "Clinical Radiology",
        "publisher": null,
        "title": "artificialintelligenceinradiologyrelevanceofcollaborativeworkbetweenradiologistsandengineersforbuildingamultidisciplinaryteam",
        "booktitle": null,
        "doi": "10.1016/j.crad.2020.11.113",
        "author": [
            "Mart\u00edn-Noguerol, T.",
            "Paulano-Godino, F.",
            "L\u00f3pez-Ortega, R.",
            "G\u00f3rriz, J.M.",
            "Riascos, R.F.",
            "Luna, A."
        ],
        "keywords": [
            ""
        ],
        "abstract": "The use of artificial intelligence (AI) algorithms in the field of radiology is becoming more common. Several studies have demonstrated the potential utility of machine learning (ML) and deep learning (DL) techniques as aids for radiologists to solve specific radiological challenges. The decision-making process, the establishment of specific clinical or radiological targets, the profile of the different professionals involved in the development of AI solutions, and the relation with partnerships and stakeholders are only some of the main issues that have to be faced and solved prior to starting the development of radiological AI solutions. Among all the players in this multidisciplinary team, the communication between radiologists and data scientists is essential for a successful collaborative work. There are specific skills that are inherent to radiological and medical training that are critical for identifying anatomical or clinical targets as well as for segmenting or labelling lesions. These skills would then have to be transferred, explained, and taught to the data science experts to facilitate their comprehension and integration into ML or DL algorithms. On the other hand, there is a wide range of complex software packages, deep neural-network architectures, and data transfer processes for which radiologists need the expertise of software engineers and data scientists in order to select the optimal manner to analyse and post-process this amount of data. This paper offers a summary of the top five challenges faced by radiologists and data scientists including tips and tricks to build a successful AI team.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "asystematicliteraturereviewontheuseofartificialintelligenceinenergyselfmanagementinsmartbuildings",
        "booktitle": null,
        "doi": "10.1016/j.rser.2021.111530",
        "author": [
            "Aguilar, J.",
            "Garces-Jimenez, A.",
            "R-Moreno, M.D.",
            "Garc\u00eda, Rodrigo"
        ],
        "keywords": [
            "Energy management system, Autonomous management architecture, Smart building, Artificial intelligence, Systematic literature review, Smart grid"
        ],
        "abstract": "Buildings are one of the main consumers of energy in cities, which is why a lot of research has been generated around this problem. Especially, the buildings energy management systems must improve in the next years. Artificial intelligence techniques are playing and will play a fundamental role in these improvements. This work presents a systematic review of the literature on researches that have been done in recent years to improve energy management systems for smart building using artificial intelligence techniques. An originality of the work is that they are grouped according to the concept of \u201cAutonomous Cycles of Data Analysis Tasks\u201d, which defines that an autonomous management system requires specialized tasks, such as monitoring, analysis, and decision-making tasks for reaching objectives in the environment, like improve the energy efficiency. This organization of the work allows us to establish not only the positioning of the researches, but also, the visualization of the current challenges and opportunities in each domain. We have identified that many types of researches are in the domain of decision-making (a large majority on optimization and control tasks), and defined potential projects related to the development of autonomous cycles of data analysis tasks, feature engineering, or multi-agent systems, among others.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "07485751",
        "isbn": null,
        "journal": "Journal of Accounting Education",
        "publisher": null,
        "title": "detectingdirtydatausingsqlrigoroushouseinsurancecase",
        "booktitle": null,
        "doi": "10.1016/j.jaccedu.2021.100714",
        "author": [
            "Lawson, James",
            "Street, Daniel"
        ],
        "keywords": [
            "Data analytics, Accounting education, Dirty data, Structured query language (\u201cSQL\u201d), Data integrity"
        ],
        "abstract": "Proficiency with data analytics is an increasingly important skill within in the accounting profession. However, successful data analysis requires clean source data (i.e., source data without errors) in order to draw reliable conclusions. Although users often assume clean source data, this assumption is frequently incorrect. Therefore, identifying and remediating \u201cdirty data\u201d is a prerequisite to effective data analysis. You, an accountant working at a firm that specializes in data analytics, have been hired by Rigorous House Insurance to analyze the company\u2019s claim insurance data. In addition to investigating specific issues mentioned by the company\u2019s controller, you are tasked with identifying any other data integrity issues that you encounter and providing preventative information system internal control suggestions to the client to mitigate these issues in the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,931"
    },
    {
        "issnkey": "15684946",
        "isbn": null,
        "journal": "Applied Soft Computing",
        "publisher": null,
        "title": "sentimentclassificationusingattentionmechanismandbidirectionallongshorttermmemorynetwork",
        "booktitle": null,
        "doi": "10.1016/j.asoc.2021.107792",
        "author": [
            "Wu, Peng",
            "Li, Xiaotong",
            "Ling, Chen",
            "Ding, Shengchun",
            "Shen, Si"
        ],
        "keywords": [
            "Attention mechanism, Bidirectional long short-term memory network, Sentiment classification, Social media, Word embedding"
        ],
        "abstract": "We propose a sentiment classification method for large scale microblog text based on the attention mechanism and the bidirectional long short-term memory network (SC-ABiLSTM). We use an experimental study to compare our proposed method with baseline methods using real world large-scale microblog data. Comparing the accuracy of the baseline methods to the accuracy of our model, we demonstrate the efficacy of our proposed method. While sentiment classification of social media data has been extensively studied, the main novelty of our study is the implementation of the attention mechanism in a deep learning network for analyzing large scale social media data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,290"
    },
    {
        "issnkey": "25891014",
        "isbn": null,
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter13theroleofiotinsmartcitieschallengesofairqualitymasssensortechnologyforsustainablesolutions",
        "booktitle": "Security and Privacy Issues in IoT Devices and Sensor Networks",
        "doi": "10.1016/B978-0-12-821255-4.00013-4",
        "author": [
            "Pradhan, Alok",
            "Unhelkar, Bhuvan"
        ],
        "keywords": [
            "Air quality monitoring, Smart and sustainable cities, Air quality sensors, Citizen science, Air quality management"
        ],
        "abstract": "Holistic management of air quality in cities is crucial to set a reasonable standard of living for its citizens. With the growing number of large cities across the world, the Internet of Things (IoT) is a technology that holds the promise to make cities smart and sustainable. A primary objective of a smart and sustainable city is to ensure urban air quality is clean and sustainably managed for future generations. Current regulation-based ambient air quality monitoring technology in urban environments involves sparsely distributed bulky and expensive equipment, which requires continuous calibration and maintenance. IoT devices can help measure, monitor, and abet the impact of poor air quality. This is so because of the ubiquitous nature of the devices, available connectivity in smart cities, and opportunities to analyze data within reasonable time to take corrective actions. This chapter discusses various aspects of air quality monitoring for smart cities capitalizing on IoT devices. Starting with a discussion on the background of air quality monitoring in the context of smart cities, this chapter outlines the challenges and the various ways in which IoT and Cloud-based applications can work together to overcome those challenges. Overall, this chapter proposes a three-tiered approach for effectively applying data to achieve better urban air quality. The first tier is a practical approach, the second tier is a strategic approach, and the third and final tier is a legislative approach.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26659174",
        "isbn": null,
        "journal": "Measurement: Sensors",
        "publisher": null,
        "title": "briefoverviewofthefutureofmetrology",
        "booktitle": null,
        "doi": "10.1016/j.measen.2021.100306",
        "author": [
            "{Grasso Toro}, Federico",
            "Lehmann, Hugo"
        ],
        "keywords": [
            "Digital metrology, Digitalization of metrology, Metrology of digitalization, Digitalization strategies, Ontology of knowledge"
        ],
        "abstract": "This position paper opens the discussion about the term digital metrology, the underlying digital trends and the activities related to the digitalization of metrology. Firstly, we present a clarification of terms between digitization and digitalization and the current digitalization strategies of two representative national metrology institutes, members of EURAMET. Subsequently, we describe the current METAS strategy towards digital metrology by its three main pillars and ongoing research and development efforts. Finally, we discuss the state of the digitalization of metrology, as well as presenting our suggestions for the next steps towards the digital transformation of metrology.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "predictivemodelfortheidentificationofactivitiesofdailylivingadlinindoorenvironmentsusingclassificationtechniquesbasedonmachinelearning",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.07.069",
        "author": [
            "Johanna, Garc\u00eda-Restrepo",
            "{Paola Patricia}, Ariza-Colpas",
            "{Alvaro Agust\u00edn}, O\u00f1ate-Bowen",
            "{Eydy del Carmen}, Suarez-Brieva",
            "Miguel, Urina-Triana",
            "Emiro, De-la-Hoz-Franco",
            "{Jorge Luis}, D\u00edaz-Mart\u00ednez",
            "{Shariq Aziz}, Butt",
            "Diego, Molina_Estren"
        ],
        "keywords": [
            "HAR, Human Activity Recognition, Machine Learning, ADL, Activity Daily Living"
        ],
        "abstract": "AI-based techniques have included countless applications within the engineering field. These range from the automation of important procedures in Industry and companies, to the field of Process Control. Smart Home (SH) technology is designed to help house residents improve their daily activities and therefore enrich the quality of life while preserving their privacy. An SH system is usually equipped with a collection of software interrelated with hardware components to monitor the living space by capturing the behavior of the resident and their occupations. By doing so, the system can report risks, situations, and act on behalf of the resident to their satisfaction. This research article shows the experimentation carried out with the human activity recognition dataset, CASAS Kyoto, through preprocessing and cleaning processes of the data, showing the V\u00eda Regression classifier as an excellent option to process this type of data with an accuracy 99.7% effective",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "towardsdatadrivenreliabilitymodelingforcyberphysicalproductionsystems",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.03.073",
        "author": [
            "Friederich, Jonas",
            "Lazarova-Molnar, Sanja"
        ],
        "keywords": [
            "Cyber-Physical Production Systems, Reliability Analysis, Data-Driven Reliability Modeling"
        ],
        "abstract": "Reliability is one of the most important performance indicators in contemporary production facilities. Increasing reliability of manufacturing systems results in their prolonged lifetimes, and reduced maintenance and repair costs. Reliability modeling is a common technique for deriving reliability measurements and illustrating relevant fault-dependencies. There is a significant body of research focusing on hardware- and software reliability models, such as Fault Trees, Petri Nets and Markov Chains. Up until now, development of reliability models has been a labor-intensive and expert-knowledge-driven process. To remedy that, through the prevalence of data stemming from the new and technologically advanced manufacturing systems, we propose that data generated in modern manufacturing lines could be used to either automate or at least to support development of reliability models. In this paper, we elaborate on the details of our proposed framework for data-driven reliability assessment of cyber-physical production systems. We, furthermore, introduce a case study that will aid the development and testing of the proposed novel data-driven approach.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "02684012",
        "isbn": null,
        "journal": "International Journal of Information Management",
        "publisher": null,
        "title": "artificialintelligenceaimultidisciplinaryperspectivesonemergingchallengesopportunitiesandagendaforresearchpracticeandpolicy",
        "booktitle": null,
        "doi": "10.1016/j.ijinfomgt.2019.08.002",
        "author": [
            "Dwivedi, Yogesh",
            "Hughes, Laurie",
            "Ismagilova, Elvira",
            "Aarts, Gert",
            "Coombs, Crispin",
            "Crick, Tom",
            "Duan, Yanqing",
            "Dwivedi, Rohita",
            "Edwards, John",
            "Eirug, Aled",
            "Galanos, Vassilis",
            "Ilavarasan, P.",
            "Janssen, Marijn",
            "Jones, Paul",
            "Kar, Arpan",
            "Kizgin, Hatice",
            "Kronemann, Bianca",
            "Lal, Banita",
            "Lucini, Biagio",
            "Medaglia, Rony",
            "{Le Meunier-FitzHugh}, Kenneth",
            "{Le Meunier-FitzHugh}, Leslie",
            "Misra, Santosh",
            "Mogaji, Emmanuel",
            "Sharma, Sujeet",
            "Singh, Jang",
            "Raghavan, Vishnupriya",
            "Raman, Ramakrishnan",
            "Rana, Nripendra",
            "Samothrakis, Spyridon",
            "Spencer, Jak",
            "Tamilmani, Kuttimani",
            "Tubadji, Annie",
            "Walton, Paul",
            "Williams, Michael"
        ],
        "keywords": [
            "Artificial intelligence, AI, Cognitive computing, Expert systems, Machine learning, Research agenda"
        ],
        "abstract": "As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportunities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "14.098",
        "scimago_value": "2,770"
    },
    {
        "issnkey": "13648152",
        "isbn": null,
        "journal": "Environmental Modelling & Software",
        "publisher": null,
        "title": "aheterogeneouskeyperformanceindicatormetadatamodelforairqualitymonitoringinsustainablecities",
        "booktitle": null,
        "doi": "10.1016/j.envsoft.2020.104955",
        "author": [
            "Zhou, Lianjie",
            "Li, Qingquan",
            "Tu, Wei",
            "Wang, Chisheng"
        ],
        "keywords": [
            "KPI, Meta-model, Formulization framework, Heterogeneity, Geospatial sensor web, SDGs"
        ],
        "abstract": "Due to heterogeneous and inconsistent key performance indicators (KPIs) for the quantitative evaluation of a sustainable city's operational status, it is a great challenge to share multidimensional, multi-source and heterogeneous indicators. We propose a heterogeneous KPI capability representation model (HKPM) in our study. Based on the Meta Object Facility architecture, a nine-tuple multi-hierarchical meta-model is formulated to define the metadata components. Nine specific representation element datasets for specific KPIs are proposed to represent the meta-model. Besides, the KPI classification based on Sustainable Development Goals (SDGs) has been accomplished to support HKPM instantiated in concrete application. Experiments are conducted with the multi-type KPIs to validate the feasibility of HKPM, as shown in public service and Air Quality Index KPI instantiation example. Furthermore, the KPIs can be characterized in different dimensions, which can be modelled in a stereoscopic manner, promoting a comprehensive perception of sustainable cities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,828"
    },
    {
        "issnkey": "02673649",
        "isbn": null,
        "journal": "Computer Law & Security Review",
        "publisher": null,
        "title": "internetcourtschallengesandfutureinchina",
        "booktitle": null,
        "doi": "10.1016/j.clsr.2020.105522",
        "author": [
            "Guo, Meirong"
        ],
        "keywords": [
            "Internet court, Adjudication model, Civil jurisdiction, Internet technology"
        ],
        "abstract": "China established the world's first Internet court in Hangzhou in August 2017. Subsequently in 2018 Internet courts in Beijing and Guangzhou were established respectively. With the official establishment of these three Internet courts, China's electronic litigation advanced to a new stage.. Internet courts offer many advantages, and this innovative adjudication model has earned widespread approval for both its speedy acceptance of cases and speedy hearing of cases. This article analyzes the questions and challenges faced by Internet courts, proposes solutions such as compliance with three basic legal ethical principles, re-establishing the sense of presence and ritual of litigation, establishment of risk mitigation mechanisms between the legal system and technological systems to develop the ability for the construction of Internet courts in China.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,815"
    },
    {
        "issnkey": "0967070x",
        "isbn": null,
        "journal": "Transport Policy",
        "publisher": null,
        "title": "evolvingtermaccessibilityinspatialsystemscontextualevaluationofindicators",
        "booktitle": null,
        "doi": "10.1016/j.tranpol.2021.03.006",
        "author": [
            "Ahuja, Richa",
            "Tiwari, Geetam"
        ],
        "keywords": [
            "Accessibility, Evolution, Review, Measures, Indicator, Barriers"
        ],
        "abstract": "Access terminology is evolving since its inception by Hansen in 1959. Accounting accessibility is central to multiple disciplines such as geography, transportation, health, economics, social sciences, etc. Developing indicators to measure access is a common practice and usually favor specific dimensions of access based on application. Although measuring accessibility and developing related indicators is a common practice, there are missing links in the indicator development, planning process, its implementation and related policy-making. Due to many available indicators, each differing in context, the practicality of implementation and their transferability is generally lost. Current work focuses on extracting commonalities between indicators and understanding how the contextual focus of indicators\u2019 have changed over time to measure access. Requirements for improved access related policies, developing realistic measures and future research directions based on gaps in the identified access measures are suggested.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15749541",
        "isbn": null,
        "journal": "Ecological Informatics",
        "publisher": null,
        "title": "collectandanalysisofagrobiodiversitydatainaparticipativecontextabusinessintelligenceframework",
        "booktitle": null,
        "doi": "10.1016/j.ecoinf.2021.101231",
        "author": [
            "Bimonte, Sandro",
            "Billaud, Olivier",
            "Fontaine, Beno\u00eet",
            "Martin, Thomy",
            "Flouvat, Fr\u00e9d\u00e9ric",
            "Hassan, Ali",
            "Rouillier, Nora",
            "Sautot, Lucile"
        ],
        "keywords": [
            "Data warehouse, Data science, Biodiversity, Agriculture"
        ],
        "abstract": "In France and Europe, farmland represents a large fraction of land cover. The study and assessment of biodiversity in farmland is therefore a major challenge. To monitor biodiversity across wide areas, citizen science programs have demonstrated their effectiveness and relevance. The involvement of citizens in data collection offers a great opportunity to deploy extensive networks for biodiversity monitoring. But citizen science programs come with two issues: large amounts of data to manage and large numbers of participants with heterogeneous skills, needs and expectations about these data. In this article, we offer a solution to these issues, concretized by an information system. The study is based on a real life citizen science program tailored for farmers. This information system provides data and tools at several levels of complexity, to fit the needs and the skills of several users, from citizens with basic IT knowledge to scientists with strong statistical background. The proposed system is designed as follows. First, a data warehouse stores the data collected by citizens. This data warehouse is modelled depending on future data analysis. Secondly, associated with the data warehouse, a standard OLAP tool enables citizens and scientists to explore data. To complete the OLAP tool, we implement and compare four feature selection methods, in order to rank explanatory factors according to their relevance. Finally, for users with extended statistical skills, we use Generalized Linear Mixed Models to explore the temporal dynamics of invertebrate diversity in farmland ecosystems. The proposed system, a combination of business intelligence tools, data mining methods and advanced statistics, offers an example of complete exploitation of data by several user profiles. The proposition is supported by a real life citizen science program, and can be used as a guideline to design information systems in the same field.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.142",
        "scimago_value": "0,774"
    },
    {
        "issnkey": "00991767",
        "isbn": null,
        "journal": "Journal of Emergency Nursing",
        "publisher": null,
        "title": "theaccuracyofmedicationadministrationdataintheemergencydepartmentwhydoesitmatter",
        "booktitle": null,
        "doi": "10.1016/j.jen.2021.08.008",
        "author": [
            "Cato, Kenrick"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26663511",
        "isbn": null,
        "journal": "Sensors International",
        "publisher": null,
        "title": "significanceofquality40towardscomprehensiveenhancementinmanufacturingsector",
        "booktitle": null,
        "doi": "10.1016/j.sintl.2021.100109",
        "author": [
            "Javaid, Mohd",
            "Haleem, Abid",
            "{Pratap Singh}, Ravi",
            "Suman, Rajiv"
        ],
        "keywords": [
            "Quality 4.0, Industry 4.0, Quality revolution, Quality control, Sensors, Technologies"
        ],
        "abstract": "Quality 4.0 corresponds to the growing digitisation of industry, which uses advanced technologies to enhance the quality of manufacturing and services. This fourth quality revolution is envisaged to digitise the entire quality systems and subsequently improve the existing quality approaches. Innovative industries adopt cloud-based quality 4.0 innovations in the controlled production process. It is used to resolve quality problems satisfactorily when they emerge and carry out real-time quality analyses to improve competitiveness and use them. Various ongoing challenges are take-over by Quality 4.0 technologies, such as automated root cause analysis, machine-to-machine connectivity to parameter auto adjustment, simulation of real-time processes and more. Quality 4.0 is a modern form of quality management. Digital technologies paired with more sophisticated methods and smarter processes will allow high-performance teams to provide consumers with high-performance and quality goods reliably. Sensors play an essential role in improving the quality of manufacturing and services. These can improve protection, increased internal productivity and sustainable operations. This paper provides how quality 4.0 will have a significant impact in the field of manufacturing. Various Key Aspects and enablers of Quality 4.0 for Manufacturing are discussed, finally, Identified and discussed eighteen significant applications of Quality 4.0 in the field of manufacturing. Quality 4.0 not only concerns the things happening inside a factory; it also includes the complete supply chain from Research and Development (R&D), manufacturing, development, distribution, sales, and service after-sales.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00012998",
        "isbn": null,
        "journal": "Seminars in Nuclear Medicine",
        "publisher": null,
        "title": "howtodesignaidrivenclinicaltrialsinnuclearmedicine",
        "booktitle": null,
        "doi": "10.1053/j.semnuclmed.2020.09.003",
        "author": [
            "Delso, Gaspar",
            "Cirillo, Davide",
            "Kaggie, Joshua",
            "Valencia, Alfonso",
            "Metser, Ur",
            "Veit-Haibach, Patrick"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Artificial intelligence (AI) is an overarching term for a multitude of technologies which are currently being discussed and introduced in several areas of medicine and in medical imaging specifically. There is, however, limited literature and information about how AI techniques can be integrated into the design of clinical imaging trials. This article will present several aspects of AI being used in trials today and how imaging departments and especially nuclear medicine departments can prepare themselves to be at the forefront of AI-driven clinical trials. Beginning with some basic explanation on AI techniques currently being used and existing challenges of its implementation, it will also cover the logistical prerequisites which have to be in place in nuclear medicine departments to participate successfully in AI-driven clinical trials.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22106707",
        "isbn": null,
        "journal": "Sustainable Cities and Society",
        "publisher": null,
        "title": "fromdoityourselfdiytodoittogetherditreflectionsondesigningacitizendrivenairqualitymonitoringframeworkintaiwan",
        "booktitle": null,
        "doi": "10.1016/j.scs.2020.102628",
        "author": [
            "Mahajan, Sachit",
            "Luo, Cyuan-Heng",
            "Wu, Dong-Yi",
            "Chen, Ling-Jyh"
        ],
        "keywords": [
            "Air pollution monitoring, Particulate matter exposure, Citizen science, Resilient cities, Low-cost sensors"
        ],
        "abstract": "Air pollution is a serious problem and has caused public health concerns all over the world. Despite the evidence, the preparedness and response of citizens has been limited. This underlines the importance of having sustainable air quality monitoring solutions that foster inclusion and multi-stakeholder partnerships for social-scientific interventions. This study illustrates how AirBox project has emerged in Taiwan, where makers and citizens use the sensors to sense air quality and provide the public with actionable data about their environments. The AirBox project includes elements of technology-innovation and citizen science: (1) Participatory Sensing \u2013 Static and mobile air quality sensing, (2) Open Data \u2013 Open hardware, software and access to data, (3) Co-creation Citizen Science \u2013 Citizen-led campaigns and forums, and (4) Outreach \u2013 Knowledge sharing, trust building and multi-stakeholder collaboration. The project uses a wide range of sensors to provide extendable solutions and data at fine spatio-temporal resolution. The results are highlighted using five cases studies that show how integrating social dimensions in an air quality monitoring framework can lead to public awareness, data-driven applications and environmentally sustainable cities. The multi-faceted approach highlights the effects of a bottom-up citizen science approach that considers local culture, practices and problems at grassroots.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.587",
        "scimago_value": "1,645"
    },
    {
        "issnkey": "17486815",
        "isbn": null,
        "journal": "Journal of Plastic, Reconstructive & Aesthetic Surgery",
        "publisher": null,
        "title": "ispoorqualitynonmelanomaskincancerdataaffectinghighqualityresearchandpatientcare",
        "booktitle": null,
        "doi": "10.1016/j.bjps.2020.12.036",
        "author": [
            "Ibrahim, Nader",
            "Gibson, John",
            "Ali, Stephen",
            "Dobbs, Thomas",
            "Whitaker, Iain"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.740",
        "scimago_value": "0,855"
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "abroadoverviewofinteractivedigitalmarketingabibliometricnetworkanalysis",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2021.03.061",
        "author": [
            "Krishen, Anjala",
            "Dwivedi, Yogesh",
            "Bindu, N.",
            "Kumar, K."
        ],
        "keywords": [
            "Digital marketing, Interactive marketing, Mobile marketing, e-marketing, e-advertising, e-Word-of-mouth"
        ],
        "abstract": "The widespread adoption of digital technologies and online social networks has revolutionized the way marketers engage with consumers. By deploying various digital platforms and information and communication technology (ICT) tools (e.g., smartphones, social media, mobile apps, electronic billboards, etc.), organizations can compete with more objective, relational, and interactive marketing techniques. The adoption of innovative devices and data-driven marketing, specifically in digital advertising, provides both a wide and efficient reach. Consequently, digital marketing (DM) triggered the creation of more informed, empowered, and connected groups of customers in both the real and virtual worlds. This paper tracks research dynamics in interactive digital marketing by identifying the stages of evolution of major topics, articles, citation and co-citation networks, using various computational techniques, including growth curve analysis and citation network analysis of bibliometric information. Finally, the study offers contributions to the field of interactive digital marketing as an international and interdisciplinary field of research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "01441647",
        "isbn": null,
        "journal": "Transport Reviews",
        "publisher": null,
        "title": "ontherelevanceofdatascienceforflightdelayresearchasystematicreview",
        "booktitle": null,
        "doi": "10.1080/01441647.2020.1861123",
        "author": [
            "Carvalho, Leonardo",
            "Sternberg, Alice",
            "{Maia Gon\u00e7alves}, Leandro",
            "{Beatriz Cruz}, Ana",
            "Soares, Jorge",
            "Brand\u00e3o, Diego",
            "Carvalho, Diego",
            "Ogasawara, Eduardo"
        ],
        "keywords": [
            "Flight delay, data science, data management, data analytics, systematic review"
        ],
        "abstract": "ABSTRACT Flight delays are a significant problem for society as they evenly impair airlines, transport companies, air traffic controllers, facility managers, and passengers. Studying prior flight data is an essential activity for every player involved in the air transportation system. Besides, developing accurate prediction models for flight delays is a crucial component of the decision-making process. Prescribing actions to solve on-going delays is an even challenging task due to the air transportation system complexity. In this regard, this paper presents a thorough literature review of data science techniques used for investigating flight delays. This work proposes a taxonomy and compiles the initiatives used to address the flight delay studies. It also offers a systematic literature review that describes the trends of the field and methods to analyse the applicability of newly proposed methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.643",
        "scimago_value": "3,046"
    },
    {
        "issnkey": "02786915",
        "isbn": null,
        "journal": "Food and Chemical Toxicology",
        "publisher": null,
        "title": "preface",
        "booktitle": null,
        "doi": "10.1016/j.fct.2021.112372",
        "author": [
            "Heath, David",
            "Horvat, Milena",
            "Ogrinc, Nives"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01697439",
        "isbn": null,
        "journal": "Chemometrics and Intelligent Laboratory Systems",
        "publisher": null,
        "title": "riskbasedarsenicrationalsamplingdesignforpublicandenvironmentalhealthmanagement",
        "booktitle": null,
        "doi": "10.1016/j.chemolab.2021.104274",
        "author": [
            "Yin, Lihao",
            "Sang, Huiyan",
            "Schnoebelen, Douglas",
            "Wels, Brian",
            "Simmons, Don",
            "Mattson, Alyssa",
            "Schueller, Michael",
            "Pentella, Michael",
            "Dai, Susie"
        ],
        "keywords": [
            "Private well, Spatially clustered function model, Resource management"
        ],
        "abstract": "Groundwater contaminated with arsenic has been recognized as a global threat, which negatively impacts human health. Populations that rely on private wells for their drinking water are vulnerable to the potential arsenic-related health risks such as cancer and birth defects. Arsenic exposure through drinking water is among one of the primary arsenic exposure routes that can be effectively managed by active testing and water treatment. From the public and environmental health management perspective, it is critical to allocate the limited resources to establish an effective arsenic sampling and testing plan for health risk mitigation. We present a spatially adaptive sampling design approach based on an estimation of the spatially varying underlying contamination distribution. The method is different from traditional sampling design methods that often rely on a spatially constant or smoothly varying contamination distribution. In contrast, we propose a statistical regularization method to automatically detect spatial clusters of the underlying contamination risk from the currently available private well arsenic testing data in the USA, Iowa. This approach allows us to develop a sampling design method that is adaptive to the changes in the contamination risk across the identified clusters. We provide the spatially adaptive sample size calculation and sampling location determination at different acceptance precision and confidence levels for each cluster. The spatially adaptive sampling approach may effectively mitigate the arsenic risk from the resource management perspectives. The model presents a framework that can be widely used for other environmental contaminant monitoring and sampling for public and environmental health.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.491",
        "scimago_value": "0,600"
    },
    {
        "issnkey": "0160791x",
        "isbn": null,
        "journal": "Technology in Society",
        "publisher": null,
        "title": "industry40currentpracticeandchallengesinmalaysianmanufacturingfirms",
        "booktitle": null,
        "doi": "10.1016/j.techsoc.2021.101749",
        "author": [
            "Tay, S.I.",
            "Alipal, J.",
            "Lee, T.C."
        ],
        "keywords": [
            "Industry 4.0, Manufacturing firms, Industry 4.0 exploration, Internet of things, Policy, Data management"
        ],
        "abstract": "This research employed a qualitative approach to discuss the current practice and challenges of Malaysian manufacturing firms in the implementation of Industry 4.0. The study examined data from seven manufacturing companies pursuing Industry 4.0 initiatives to identify various options for their strategies. The study found that the implementation of Industry 4.0 in the manufacturing firms is still in the exploratory stage. The companies involved in this study were discovered to conduct exploration using an adaptive-like framework. That is, throughout the process, the majority of the subjects are 'trying and adding' Industry 4.0 to their operations. Their trial-and-error approach is based on what is feasible and effective in their manufacturing environment. Overall, the investigation determined that data management and integration, as well as personnel re-education, were the respondents' primary operational challenges.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0012821x",
        "isbn": null,
        "journal": "Earth and Planetary Science Letters",
        "publisher": null,
        "title": "developmentoftheleeuwincurrentonthenorthwestshelfofaustraliathroughthepliocenepleistoceneperiod",
        "booktitle": null,
        "doi": "10.1016/j.epsl.2021.116767",
        "author": [
            "He, Yuxin",
            "Wang, Huanye",
            "Liu, Zhonghui"
        ],
        "keywords": [
            "Leeuwin Current, Indonesian Throughflow, northwest shelf of Australia, Pliocene-Pleistocene, temperatures, primary productivity"
        ],
        "abstract": "Although the Leeuwin Current (LC) is thought to play a pivotal role in climatic and oceanic systems of the western Australian region, how the LC developed through the Pliocene-Pleistocene period remains elusive. Here we used biomarker records to reconstruct variations of temperatures and primary productivity on the northwest shelf of Australia over the last 6 million years. Since \u223c1.2 million years ago (Ma), our sea surface temperature record indicates progressive warming, with temperature values comparable to those in the Indo-Pacific Warm Pool, in contrast with the long-term global cooling trend. The regional surface warming was accompanied by suppressed primary productivity, together indicating prevailing warm, low-salinity, nutrient-deficient surface water, and thus a stronger LC since the Mid-Pleistocene Transition. During 4\u20131.2 Ma, greater surface temperature gradient between the Indo-Pacific Warm Pool and the northwest shelf of Australia and higher primary productivity seem to suggest a generally weaker LC. Warmer temperatures and lower productivity suggest a plausible existence of the LC during 6\u20134 Ma, but more work is required to confirm this. Impact of sea level and the Indonesian Throughflow on the LC strength may exist, but did not dominate through the Pliocene-Pleistocene period, considering different variation patterns among them. We propose the stronger LC after \u223c1.2 Ma was more likely triggered by enhanced atmospheric circulation. Although the increased LC after \u223c1.2 Ma may have potentially brought additional moisture to the Australian continent during the interglacial periods, it has not overturned the long-term drying trend through the Pliocene-Pleistocene period.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15708705",
        "isbn": null,
        "journal": "Ad Hoc Networks",
        "publisher": null,
        "title": "multilabelactivelearningfromcrowdsforsecureiiot",
        "booktitle": null,
        "doi": "10.1016/j.adhoc.2021.102594",
        "author": [
            "Wu, Ming",
            "Li, Qianmu",
            "Bilal, Muhammad",
            "Xu, Xiaolong",
            "Zhang, Jing",
            "Hou, Jun"
        ],
        "keywords": [
            "Crowdsourcing, Secure IIoT, Annotation consensus, Multi-label learning, Active learning"
        ],
        "abstract": "With the development of IIoT (Industrial Internet of Things), Artificial Intelligence technology is widely used in many research areas, such as image classification, speech recognition, and information retrieval. Traditional supervised machine learning obtains labels from high-quality oracles, which is high cost and time-consuming and does not consider security. Since multi-label active learning becomes a hot topic, it is more challenging to train efficient and secure classification models, and reduce the label cost in the field of IIoT. To address this issue, this research focuses on the secure multi-label active learning for IIoT using an economical and efficient strategy called crowdsourcing, which involves querying labels from multiple low-cost annotators with various expertise on crowdsourcing platforms rather than relying on a high-quality oracle. To eliminate the effects of annotation noise caused by imperfect annotators, we propose the Multi-label Active Learning from Crowds (MALC) method, which uses a probabilistic model to simultaneously compute the annotation consensus and estimate the classifier\u2019s parameters while also taking instance similarity into account. Then, to actively choose the most informative instances and labels, as well as the most reliable annotators, an instance-label-annotator triplets selection technique is proposed. Experimental results on two real-world data sets show that the performance of MALC is superior to existing methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.111",
        "scimago_value": "0,781"
    },
    {
        "issnkey": "00489697",
        "isbn": null,
        "journal": "Science of The Total Environment",
        "publisher": null,
        "title": "regionalcharacteristicsofchildrensbloodleadlevelsinchinaasystematicsynthesisofnationalandsubnationalpopulationdata",
        "booktitle": null,
        "doi": "10.1016/j.scitotenv.2020.144649",
        "author": [
            "Liu, Yang",
            "Liu, Feiyan",
            "Dong, Kylie",
            "Wu, Yongning",
            "Yang, Xingfen",
            "Yang, Jintao",
            "Tan, Hong",
            "Niu, Xiaojun",
            "Zhao, Xinyuan",
            "Xiao, Gexin",
            "Zhou, Shaoqi"
        ],
        "keywords": [
            "Blood lead, Child health, Regional characteristics, China, Prefectures"
        ],
        "abstract": "The blood lead levels (BLLs) of children in China remain notably high in many areas. We aimed to summarise the relevant regional characteristics, identifying problematic areas and the causes of lead pollution. We searched the databases of PubMed, China National Knowledge Infrastructure (CNKI), and Wanfang Data, systematically reviewing 219 articles published from January 2010 to September 2020. In doing so, we assessed the BLLs noted in 220 prefectures across China. Data were organised using Geographic Information Systems (GIS) mapping. Out of a total of 629,627 children sampled, we found that the average blood lead level (BLL) of children included in our study is 50.61 \u00b1 13.63 \u03bcg/L, which slightly exceeds the 50.00 \u03bcg/L US standard. Within the sample, 8.75% had BLLs higher than 100.00 \u03bcg/L. Children living in Liaoning, Hebei, Shanxi, Jiangxi, Anhui, Fujian, Guizhou, Yunnan, and Guangxi had notably high BLLs, at more than 60.00 \u03bcg/L. A total of 112 municipalities had an average children's BLL above 50.00 \u03bcg/L. Furthermore, Chenzhou, Linfen, Yuncheng, and Hechi had the highest children's BLLs, with average values above 100.00 \u03bcg/L. The leading contributors to lead pollution are lead mining, lead recovery and the smelting industry. Nonetheless, the lead-acid battery industry needs more attention. Although data suggest that BLLs are decreasing in China, many areas still have high BLLs that need to be monitored. Moreover, national standards must improve to decrease acceptable BLL thresholds for children.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.963",
        "scimago_value": "1,795"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-817976-5",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter9industrysustainablesupplychainmanagementwithdatascience",
        "booktitle": "Data Science Applied to Sustainability Analysis",
        "doi": "10.1016/B978-0-12-817976-5.00010-3",
        "author": [
            "Chakraborty, Deboleena",
            "Helling, Richard"
        ],
        "keywords": [
            "Sustainability, Supply chain, Carbon footprints, Life cycle assessment (LCA), Big data analytics, Data quality, GHG reduction, Multi-objective optimization, Geospatial information"
        ],
        "abstract": "Life cycle assessment (LCA) is a quantitative tool to bring environmental insights into decisions, supplementing consideration of cost, performance and social impact. Calculation of \u201ccarbon footprints\u201d or other metrics derived from LCA typically requires data and information drawn from many sources, both within an organization and externally. Knowing LCA metrics for all products and companies could help society understand the most significant opportunities and trade-offs in the quest for sustainability. A barrier to this state of knowledge is the ability to create, access, manage and verify the extensive data required. New information technology and analytics can allow LCA to get closer to the ideal state, reducing the time required to do an assessment and increasing the quality of the results.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01689525",
        "isbn": null,
        "journal": "Trends in Genetics",
        "publisher": null,
        "title": "opportunitiesandchallengesforartificialintelligenceinclinicalcardiovasculargenetics",
        "booktitle": null,
        "doi": "10.1016/j.tig.2021.04.004",
        "author": [
            "Krittanawong, Chayakrit",
            "Johnson, Kipp",
            "Glicksberg, Benjamin"
        ],
        "keywords": [
            ""
        ],
        "abstract": "A combination of emerging genomic and artificial intelligence (AI) techniques may ultimately unlock a deeper understanding of heterogeneity and biological complexities in cardiovascular diseases (CVDs), leading to advances in prognostic guidance and personalized therapies. We discuss the state of AI in cardiovascular genetics, current applications, limitations, and future directions of the field.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "11.639",
        "scimago_value": "5,713"
    },
    {
        "issnkey": "00128252",
        "isbn": null,
        "journal": "Earth-Science Reviews",
        "publisher": null,
        "title": "spatiotemporalforecastinginearthsystemsciencemethodsuncertaintiespredictabilityandfuturedirections",
        "booktitle": null,
        "doi": "10.1016/j.earscirev.2021.103828",
        "author": [
            "Xu, Lei",
            "Chen, Nengcheng",
            "Chen, Zeqiang",
            "Zhang, Chong",
            "Yu, Hongchu"
        ],
        "keywords": [
            "Spatiotemporal forecasting, Artificial intelligence, Physical model, Uncertainty modeling, Predictability"
        ],
        "abstract": "Spatiotemporal forecasting (STF) extends traditional time series forecasting or spatial interpolation problem to space and time dimensions. Here, we review the statistical, physical and artificial intelligence (AI) methods, data and model uncertainties, predictability and future directions for STF problems. Statistical STF methods have limitations in high-level feature extractions and long-term memory modeling. Physical models are computationally intensive and are imperfect in model structure and parameterization. AI models lack the interpretability and require elaborate training but can model complex nonlinear and non-Gaussian problems. Integrating data-driven and physical model-driven methods could facilitate the improvement of interpretability and forecasting accuracy. The predictive uncertainty comes from data and models, which could be measured by probability distribution and Bayesian inference, respectively. The predictive uncertainty is generally missing in AI models and could be resolved by incorporating Bayesian frameworks. The predictability of dynamic earth systems is spatiotemporally heterogeneous and is generally examined by diagnostic and prognostic approaches. Diagnostic methods analyze the predictability empirically from a theoretical perspective, while prognostic methods investigate the predictability through real experiments. Unraveling the predictability in space and time and the predictability sources will greatly improve earth system understanding and operational forecasting development. Current STF systems are largely not user-friendly to provide probabilistic and understandable forecasting services in near real-time. Intelligent STF systems should automatically prepare various data sources, train the models in a self-adaptative way and provide timely predictive information services for users to make decisions. This review provides state-of-the-art advances in forecasting sciences and highlights new directions for new-generation STF systems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "12.413",
        "scimago_value": "3,893"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "equipmentdesignoptimizationbasedondigitaltwinundertheframeworkofzerodefectmanufacturing",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.01.271",
        "author": [
            "Mourtzis, Dimitris",
            "Angelopoulos, John",
            "Panopoulos, Nikos"
        ],
        "keywords": [
            "Digital Twin, Machine Design, Zero-Defect Manufacturing"
        ],
        "abstract": "A digitalized Smart Factory can be considered as a data island. Moreover, engineers have focused on the development of new technologies and techniques not only for transforming information to data but also to achieve efficient data utilization to further optimize manufacturing processes. However, the Zero-Defect Manufacturing concept has emerged, where the main goal is production optimization. The cornerstone in achieving the factories of the future is to further optimize the design of new assets so as they comply with the unique requirements of the customers. Therefore, this paper proposes the conceptualization, design, and initial development of a platform for the utilization of data derived from industrial environments for the optimization of the equipment design. The main aspects of the proposed framework are the data acquisition, data processing and the simulation. The applicability of the proposed framework has been tested in a laboratory-based machine shop utilizing data from a real-life industrial scenario.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "02732300",
        "isbn": null,
        "journal": "Regulatory Toxicology and Pharmacology",
        "publisher": null,
        "title": "determinationoffitnessforpurposeofquantitativestructureactivityrelationshipqsarmodelstopredictecotoxicologicalendpointsforregulatoryuse",
        "booktitle": null,
        "doi": "10.1016/j.yrtph.2021.104956",
        "author": [
            "Belfield, Samuel",
            "Enoch, Steven",
            "Firman, James",
            "Madden, Judith",
            "Schultz, Terry",
            "Cronin, Mark"
        ],
        "keywords": [
            "models, QSAR, Toxicity prediction, Uncertainty, Regulatory use"
        ],
        "abstract": "In silico models are used to predict toxicity and molecular properties in chemical safety assessment, gaining widespread regulatory use under a number of legislations globally. This study has rationalised previously published criteria to evaluate quantitative structure-activity relationships (QSARs) in terms of their uncertainty, variability and potential areas of bias, into ten assessment components, or higher level groupings. The components have been mapped onto specific regulatory uses (i.e. data gap filling for risk assessment, classification and labelling, and screening and prioritisation) identifying different levels of uncertainty that may be acceptable for each. Twelve published QSARs were evaluated using the components, such that their potential use could be identified. High uncertainty was commonly observed with the presentation of data, mechanistic interpretability, incorporation of toxicokinetics and the relevance of the data for regulatory purposes. The assessment components help to guide strategies that can be implemented to improve acceptability of QSARs through the reduction of uncertainties. It is anticipated that model developers could apply the assessment components from the model design phase (e.g. through problem formulation) through to their documentation and use. The application of the components provides the possibility to assess QSARs in a meaningful manner and demonstrate their fitness-for-purpose against pre-defined criteria.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.271",
        "scimago_value": "0,890"
    },
    {
        "issnkey": "03605442",
        "isbn": null,
        "journal": "Energy",
        "publisher": null,
        "title": "generalizedmodelstopredictthelowerheatingvaluelhvofmunicipalsolidwastemsw",
        "booktitle": null,
        "doi": "10.1016/j.energy.2020.119279",
        "author": [
            "Wang, Dan",
            "Tang, Yu-Ting",
            "He, Jun",
            "Yang, Fei",
            "Robinson, Darren"
        ],
        "keywords": [
            "LHV prediction, Physical composition of municipal solid waste, Multiple regression, Artificial neural network"
        ],
        "abstract": "Accurately and efficiently predicting the LHV of MSW is vital for designing and operating a waste-to-energy plant. However, previous prediction models possess limited geographical applicability. In this paper, we employ multiple linear regression and artificial neural network (ANN) techniques to predict LHV. These data-driven models utilize 151 globally distributed datasets identified during a systematic literature review, describing the wet physical composition of MSW and measured LHV. The results show that models built via both methods exhibited acceptable and compatible levels of performance in predicting LHV, based on the multiple statistical indicators. However, the ANN model proved to be more robust in handling of datasets of diverse quality. Models developed from both methods demonstrate clearly that the wet proportion of food waste has a negative impact on LHV. Supported by the strong and significant correlation between food waste and moisture content, we concluded that the negative impact of high moisture content in food waste on LHV outweighed its calorific value. Separating food waste or any other waste with high moisture content from the MSW for incineration can significantly improve energy recovery efficiency. Contrary to expectation, the models also reveal a higher contribution of paper waste to the LHV of MSW than plastic waste.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15749541",
        "isbn": null,
        "journal": "Ecological Informatics",
        "publisher": null,
        "title": "applyingobjectdetectiontomarinedataandexploringexplainabilityofafullyconvolutionalneuralnetworkusingprincipalcomponentanalysis",
        "booktitle": null,
        "doi": "10.1016/j.ecoinf.2021.101269",
        "author": [
            "Stavelin, Herman",
            "Rasheed, Adil",
            "San, Omer",
            "Hestnes, Arne"
        ],
        "keywords": [
            "Neural networks, PCA, Object detection, XAI, Machine learning, YOLO"
        ],
        "abstract": "With the rise of focus on man made changes to our planet and wildlife therein, more and more emphasis is put on sustainable and responsible gathering of resources. In an effort to preserve maritime wildlife the Norwegian government decided to create an overview of the presence and abundance of various species of marine lives in the Norwegian fjords and oceans. The current work evaluates the possibility of utilizing machine learning methods in particular the You Only Look Once version 3 algorithm to detect fish in challenging conditions characterized by low light, undesirable algae growth and high noise. It was found that the algorithm trained on images collected during the day time under natural light could detect fish successfully in images collected during night under artificial lighting. The overall average precision score of 88% was achieved. Later principal component analysis was used to analyze the features learned in different layers of the network. It is concluded that for the purpose of object detection in specific application areas, the network can be considerably simplified since many of the feature detector turns our to be redundant.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.142",
        "scimago_value": "0,774"
    },
    {
        "issnkey": "22146296",
        "isbn": null,
        "journal": "Energy Research & Social Science",
        "publisher": null,
        "title": "tweetsandtransitionsexploringtwitterbasedpoliticaldiscourseregardingenergyandelectricityinontariocanada",
        "booktitle": null,
        "doi": "10.1016/j.erss.2020.101870",
        "author": [
            "Labonte, D.",
            "Rowlands, I.H."
        ],
        "keywords": [
            "Energy politics, Social media analysis, Twitter, Sustainability transitions, Ontario, Canada"
        ],
        "abstract": "The article explores how Twitter data can inform the study of the socio-political dimensions of sustainability transitions. Twitter is a widely used microblogging platform that allows users to share short comments, media, and links, and that offers researchers significant data collection opportunities. Twitter-based research has been growing in application in many disciplines but has not been prominently used in relation to sustainability transitions or sustainable energy research. This study aims to characterize the Twitter-based conversations regarding energy issues and politics in Ontario, Canada. The analysis in this article is based on 6946 tweets, from 2841 unique users, which were collected between September 2, 2017 and January 12, 2018. The Twitter-based discourse regarding energy issues in Ontario is described by a minority of very engaged users contributing disproportionately to the conversation, the most engaged users contributing different types of tweets to the conversation, and overall engagement that varies based on news events. Coding based on manual interpretation of the tweets by the most engaged users and those tweets that were highly retweeted identified a discourse that was highly partisan and often highlighted economic issues associated with electricity costs. Topics commonly associated with sustainable energy transitions were not prominent in the Twitter discourse. Additionally, the analysis suggests that users lacking traditional political empowerment can influence the political discourse on Twitter through high levels of retweets; however, savvy and strategic use of Twitter communication, rather than simply engagement with an issue, is important in generating consistent amplification from other users.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,313"
    },
    {
        "issnkey": "25425196",
        "isbn": null,
        "journal": "The Lancet Planetary Health",
        "publisher": null,
        "title": "digitalandtechnologicalinnovationinvectorbornediseasesurveillancetopredictdetectandcontrolclimatedrivenoutbreaks",
        "booktitle": null,
        "doi": "10.1016/S2542-5196(21)00141-8",
        "author": [
            "Pley, Caitlin",
            "Evans, Megan",
            "Lowe, Rachel",
            "Montgomery, Hugh",
            "Yacoub, Sophie"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Summary Vector-borne diseases are particularly sensitive to changes in weather and climate. Timely warnings from surveillance systems can help to detect and control outbreaks of infectious disease, facilitate effective management of finite resources, and contribute to knowledge generation, response planning, and resource prioritisation in the long term, which can mitigate future outbreaks. Technological and digital innovations have enabled the incorporation of climatic data into surveillance systems, enhancing their capacity to predict trends in outbreak prevalence and location. Advance notice of the risk of an outbreak empowers decision makers and communities to scale up prevention and preparedness interventions and redirect resources for outbreak responses. In this Viewpoint, we outline important considerations in the advent of new technologies in disease surveillance, including the sustainability of innovation in the long term and the fundamental obligation to ensure that the communities that are affected by the disease are involved in the design of the technology and directly benefit from its application.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,535"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818015-0",
        "journal": null,
        "publisher": "Academic Press",
        "title": "index",
        "booktitle": "Executing Data Quality Projects (Second Edition)",
        "doi": "10.1016/B978-0-12-818015-0.09992-8",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "datadrivendecisionsupportforprocessqualityimprovements",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.03.047",
        "author": [
            "Buschmann, Daniel",
            "Enslin, Chrismarie",
            "Elser, Hannes",
            "L\u00fctticke, Daniel",
            "Schmitt, Robert"
        ],
        "keywords": [
            "Data-Driven Decisions, Machine Learning, Defect Detection, Mass Production Manufacturing, Statistical Process Control, Random Forests"
        ],
        "abstract": "This paper presents a data-driven approach for improving the process quality of production systems. Therefore, the product quality is detected during the production process. The worker is provided with reasonable parameter recommendations about the production process as decision support to improve the process quality. To achieve this, a cross-process data analysis of the process and quality data is carried out using decision trees. The results are visualized in a comprehensible form for the worker. Based on a case study from mass production, the approach is evaluated and its performance is demonstrated in comparison to classical statistical methods.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "20962495",
        "isbn": null,
        "journal": "Petroleum Research",
        "publisher": null,
        "title": "applicationofmachinelearningandartificialintelligenceinoilandgasindustry",
        "booktitle": null,
        "doi": "10.1016/j.ptlrs.2021.05.009",
        "author": [
            "Sircar, Anirbid",
            "Yadav, Kriti",
            "Rayavarapu, Kamakshi",
            "Bist, Namrata",
            "Oza, Hemangi"
        ],
        "keywords": [
            "Artificial intelligence, Machine learning, Upstream, Oil and gas industry, Petroleum exploration"
        ],
        "abstract": "Oil and gas industries are facing several challenges and issues in data processing and handling. Large amount of data bank is generated with various techniques and processes. The proper technical analysis of this database is to be carried out to improve performance of oil and gas industries. This paper provides a comprehensive state-of-art review in the field of machine learning and artificial intelligence to solve oil and gas industry problems. It also narrates the various types of machine learning and artificial intelligence techniques which can be used for data processing and interpretation in different sectors of upstream oil and gas industries. The achievements and developments promise the benefits of machine learning and artificial intelligence techniques towards large data storage capabilities and high efficiency of numerical calculations. In this paper a summary of various researchers work on machine learning and artificial intelligence applications and limitations is showcased for upstream and sectors of oil and gas industry. The existence of this extensive intelligent system could really eliminate the risk factor and cost of maintenance. The development and progress using this emerging technologies have become smart and makes the judgement procedure easy and straightforward. The study is useful to access intelligence of different machine learning methods to declare its application for distinct task in oil and gas sector.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,478"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-809324-5",
        "journal": null,
        "publisher": "Elsevier",
        "title": "useoftechnologyforrealworldsleepandcircadianresearch",
        "booktitle": "Reference Module in Neuroscience and Biobehavioral Psychology",
        "doi": "10.1016/B978-0-12-822963-7.00200-0",
        "author": [
            "Chinoy, Evan",
            "Markwald, Rachel"
        ],
        "keywords": [
            "Actigraphy, Consumer sleep technology, Mobile EEG, Naturalistic settings, Nearables, Non-contact sensors, Polysomnography, Sleep monitoring, Wearables, Validation"
        ],
        "abstract": "Recent advances in technology and demand for biometric data have led to the creation of personal devices that track sleep and other physiological and behavioral patterns with increasing accuracy. Although such technologies have widespread use among the general population, applications for sleep and circadian research show much promise, but their current adoption has been slow in part due to the need for validation versus standard research methodologies. This article outlines the current state of sleep and circadian technologies for real-world research, their strengths and limitations, recommended standards for use, current operational use cases, and future directions for real-world applications.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "20460430",
        "isbn": null,
        "journal": "International Journal of Transportation Science and Technology",
        "publisher": null,
        "title": "smartphonesensingforunderstandingdrivingbehaviorcurrentpracticeandchallenges",
        "booktitle": null,
        "doi": "10.1016/j.ijtst.2020.07.001",
        "author": [
            "Mantouka, Eleni",
            "Barmpounakis, Emmanouil",
            "Vlahogianni, Eleni",
            "Golias, John"
        ],
        "keywords": [
            "Driving, Behavior, Analytics, Smartphones, Maxinum likelihood, Profiling"
        ],
        "abstract": "Understanding driving behavior \u2013 even in the rapid emergence of automation - remains in the spotlight, for decomposing complex driving dynamics, enabling the development of user-friendly and acceptable autonomous vehicles and ensuring the safe co-existence of autonomous and conventional vehicles on the road. Mobile crowdsensing has emerged as a means to understand and model driving behavior. Although the advantages of collecting data through smartphones are many (speed, accuracy, low cost etc.), the challenges including, but do not limited to, the preparation rate, the processing needs, as well as the methodological, legislative and security issues, are significant. The present paper aims to review the research dedicated to analyzing driving behavior based on smartphone sensors\u2019 data streams. We first establish an inclusive stepwise framework to describe the path from data collection to informed decision making. Next, the existing literature is thoroughly analyzed and challenges in relation to data collection and data mining practices are critically discussed placing particular emphasis on the limitations and concerns regarding the use of mobile phones for driving data collection, as well as using crowd sensed data for feature extraction. Subsequently, modeling driving behavior practices and end-to-end solutions for driver assistance and recommendation systems are also reviewed. The paper ends with a discussion on the most critical challenges arising from the literature and future research steps.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,133"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "theproductivityimpactofthedigitallyconnected5layerstackinmanufacturingenterprises",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.058",
        "author": [
            "Castillo, Adolfo",
            "Patsavellas, John",
            "Salonitis, Konstantinos",
            "Emmanouilidis, Christos"
        ],
        "keywords": [
            "Industry 4.0, Industrial Internet of Things (IIoT), MQTT, Manufacturing Execution System (MES), SCADA, Enterprise Scalable Data Architecture (ESDA), Overall Equipment Efficiency (OEE), Self-Cleaning-Data"
        ],
        "abstract": "This paper investigates the application of modern industrial internet protocols and network architecture in manufacturing companies, from the perspectives of productivity and sustainability, framed in the fourth industrial revolution paradigm. This is achieved by delving into the existing information systems and devices, their inter-operability and interconnections using industrial internet of things protocols. The paper details a study generating a standard model of data architecture to better unify the different layers of the information systems that typify most manufacturing companies, leveraging their existing digital infrastructure to establish a solid base for further digitalization.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "adatadrivenreversibleframeworkforachievingsustainablesmartproductservicesystems",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2020.123618",
        "author": [
            "Li, Xinyu",
            "Wang, Zuoxu",
            "Chen, Chun-Hsien",
            "Zheng, Pai"
        ],
        "keywords": [
            "Smart product-service system, Sustainability, Knowledge management, Reversible design, Context-awareness"
        ],
        "abstract": "Higher sustainability with extended product lifecycle is a tireless pursuit in companies\u2019 product design/development endeavours. In this regard, two prevailing concepts, namely the smart circular system and smart product-service system (Smart PSS), have been introduced, respectively. However, most existing studies only focus on the sustainability of physical materials and components, without considering the cyber-physical resources as a whole, let alone an integrated strategy towards the so-called Sustainable Smart PSS. To fill the gap, this paper discusses the key features in Sustainable Smart PSS development from a broadened scope of cyber-physical resources management. A data-driven reversible framework is hereby proposed to sustainably exploit high-value and context-dependent information/knowledge in the development of Sustainable Smart PSS. A four-step context-aware process in the framework, including requirement elicitation, solution recommendation, solution evaluation, and knowledge evolvement, is further introduced to support the decision-making and optimization along the extended or circular lifecycle. An illustrative example is depicted in the sustainable development of a smart 3D printer, which validates the feasibility and advantages of the proposed framework. As an explorative study, it is hoped that this work provides useful insights for Smart PSS development with sustainability concerns in a cyber-physical environment.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "02673649",
        "isbn": null,
        "journal": "Computer Law & Security Review",
        "publisher": null,
        "title": "decisionmakingbymachinesisthelawofeverythingenough",
        "booktitle": null,
        "doi": "10.1016/j.clsr.2021.105541",
        "author": [
            "Tam\u00f2-Larrieux, Aurelia"
        ],
        "keywords": [
            "Automated decision-making, Artificial intelligence, Data protection, Transparency, Fairness, Due process"
        ],
        "abstract": "Machines have moved from supporting decision-making processes of humans to making decisions for humans. This shift has been accompanied by concerns regarding the impact of decisions made by algorithms on individuals and society. Unsurprisingly, the delegation of important decisions to machines has therefore triggered a debate on how to regulate the automated decision-making practices. In Europe, policymakers have attempted to address these concerns through a combination of individual rights and due processes established in data protection law, which relies on other statutes, e.g., anti-discrimination law and restricting trade secret laws, to achieve certain goals. This article adds to the literature by disentangling the challenges arising from automated decision-making systems and focusing on ones arising without malevolence but merely as unwanted side-effects of increased automation. Such side-effects include ones arising from the internal processes leading to a decision, the impacts of decisions, as well as the responsibility for decisions and have consequences on an individual and societal level. Upon this basis the article discusses the redress mechanisms provided in data protection law. It shows that the approaches within data protection law complement one another, but do not fully remedy the identified side-effects. This is particularly true for side-effects that lead to systemic societal shifts. To that end, new paradigms to guide future policymaking discourse are being explored.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,815"
    },
    {
        "issnkey": "18763804",
        "isbn": null,
        "journal": "Petroleum Exploration and Development",
        "publisher": null,
        "title": "applicationanddevelopmenttrendofartificialintelligenceinpetroleumexplorationanddevelopment",
        "booktitle": null,
        "doi": "10.1016/S1876-3804(21)60001-0",
        "author": [
            "KUANG, Lichun",
            "LIU, He",
            "REN, Yili",
            "LUO, Kai",
            "SHI, Mingyu",
            "SU, Jian",
            "LI, Xin"
        ],
        "keywords": [
            "artificial intelligence, logging interpretation, seismic exploration, reservoir engineering, drilling and completion, surface facility engineering"
        ],
        "abstract": "Aiming at the actual demands of petroleum exploration and development, this paper describes the research progress and application of artificial intelligence (AI) in petroleum exploration and development, and discusses the applications and development directions of AI in the future. Machine learning has been preliminarily applied in lithology identification, logging curve reconstruction, reservoir parameter estimation, and other logging processing and interpretation, exhibiting great potential. Computer vision is effective in picking of seismic first breaks, fault identification, and other seismic processing and interpretation. Deep learning and optimization technology have been applied to reservoir engineering, and realized the real-time optimization of waterflooding development and prediction of oil and gas production. The application of data mining in drilling, completion, and surface facility engineering etc. has resulted in intelligent equipment and integrated software. The potential development directions of artificial intelligence in petroleum exploration and development are intelligent production equipment, automatic processing and interpretation, and professional software platform. The highlights of development will be digital basins, fast intelligent imaging logging tools, intelligent seismic nodal acquisition systems, intelligent rotary-steering drilling, intelligent fracturing technology and equipment, real-time monitoring and control of zonal injection and production.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.803",
        "scimago_value": "0,759"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "aframeworkfordigitaltwinsforproductionnetworkmanagement",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.213",
        "author": [
            "Benfer, Martin",
            "Peukert, Sina",
            "Lanza, Gisela"
        ],
        "keywords": [
            "Digital Twin, Production Network, Modeling"
        ],
        "abstract": "The dynamic and highly complex task of production network management requires decision support through quantitative models. In the industrial praxis, these models are specifically designed and implemented for particular management decisions, requiring significant one-time effort for model creation. This contribution utilizes the digital twin concept to facilitate production network models that are continuously synchronized with the examined production network to support several different management decisions. The approach structures data from existing information systems as a synchronized generic base model, which is used to create problem-specific executable models, thereby saving costs through repeated model use and quicker decision making.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "0169023x",
        "isbn": null,
        "journal": "Data & Knowledge Engineering",
        "publisher": null,
        "title": "pairingconceptualmodelingwithmachinelearning",
        "booktitle": null,
        "doi": "10.1016/j.datak.2021.101909",
        "author": [
            "Maass, Wolfgang",
            "Storey, Veda"
        ],
        "keywords": [
            "Conceptual modeling, Machine learning, Methodologies and tools, Models, Database management, Framework for incorporating conceptual modeling into data science projects, Artificial intelligence"
        ],
        "abstract": "Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13837621",
        "isbn": null,
        "journal": "Journal of Systems Architecture",
        "publisher": null,
        "title": "archnetadatahidingdesignfordistributedmachinelearningsystems",
        "booktitle": null,
        "doi": "10.1016/j.sysarc.2020.101912",
        "author": [
            "Chang, Kaiyan",
            "Jiang, Wei",
            "Zhan, Jinyu",
            "Gong, Zicheng",
            "Pan, Weijia"
        ],
        "keywords": [
            "Distributed machine learning, Data hiding, Neural networks, Embedded systems, Encryption"
        ],
        "abstract": "Integrating idle embedded devices into cloud computing is a promising approach to support Distributed Machine Learning (DML). In this paper, we approach to address the data hiding problem in such DML systems. For the purpose of the data encryption in DML systems, we introduce the tripartite asymmetric encryption theorem to provide theoretical support. Based on the theorem, we design a general image encryption scheme (called ArchNet), which can encrypt original images via the encoder to resist against illegal users. ArchNet encrypts the dataset by a specific neural network, which is especially trained for encryption. The encrypted data can be easily recognized by deep learning model. However, the encrypted data cannot be recognized by human, which makes the illegal attacker difficult to steal the encrypted data. We use MNIST, Fashion-MNIST and Cifar-10 datasets to evaluate efficiency of our design. We deploy certain base models on the encrypted datasets and compare them with the RC4 algorithm and differential privacy policy. Our design can improve the accuracy on MNIST up to 97.26% compared with RC4. The accuracies on these three datasets encrypted by ArchNet are similar to the base model. ArchNet can be deployed on DML systems with embedded devices.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.777",
        "scimago_value": "0,598"
    },
    {
        "issnkey": "00014575",
        "isbn": null,
        "journal": "Accident Analysis & Prevention",
        "publisher": null,
        "title": "statedacceptanceandbehavioralresponsesofdriverstowardsinnovativeconnectedvehicleapplications",
        "booktitle": null,
        "doi": "10.1016/j.aap.2021.106095",
        "author": [
            "Li, Weixia",
            "Wu, Guoyuan",
            "Yao, Danya",
            "Zhang, Yi",
            "Barth, Matthew",
            "Boriboonsomsin, Kanok"
        ],
        "keywords": [
            "Connected vehicle technology, Lane speed monitoring, High speed differential warning, Public acceptance, Behavioral response"
        ],
        "abstract": "This research is aimed at investigating drivers\u2019 attitudes towards connected vehicle technology in general and two connected vehicle applications in particular\u2014Lane Speed Monitoring and High Speed Differential Warning\u2014which have been demonstrated via simulation to be effective in enhancing traffic mobility and safety, respectively. An online survey was sent to customers of an automobile manufacturer in the United States. Out of the 1453 survey responses that were received, 650 complete and valid responses were used to analyze the respondents\u2019 stated acceptance of and expected behavioral responses to the two connected vehicle applications under a variety of scenarios. Statistical analyses were conducted to examine the influence of demographic and socioeconomic factors. The results reveal that the respondents express high willingness to use connected vehicle technology and the two applications under various circumstances, and the willingness is strongly associated with age, gender, education level, and income. Higher levels of acceptance are observed in older, male, higher-educated, or higher-income respondents, while the patterns of conditional acceptance and expected behavioral responses vary with specific scenarios. These results provide useful information for application developers, traffic operators, and policy makers to steer connected vehicle technology development and deployment in the direction that will benefit both the users and the society.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "applicationofdatadrivenmodelstopredictivemaintenancebearingwearpredictionattatasteel",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2021.115699",
        "author": [
            "Chen, X.",
            "{Van Hillegersberg}, Jos",
            "Topan, E.",
            "Smith, S.",
            "Roberts, M."
        ],
        "keywords": [
            "Predictive maintenance, Industry 4.0, Data-driven, Machine learning"
        ],
        "abstract": "Industries that are in transition to Industry 4.0 often face challenges in applying data-driven methods to improve performance. While ample methods are available in literature, knowledge on how to select and apply them is scarce. This study aims to address this gap reported on the design and implementation of data-driven models for predictive maintenance at TATA Steel, Shotton. The objective of the project is to predict the wearing behaviour of the components in the steel production line for maintenance activity decision support. To achieve the predictive maintenance goal, the approach applied can be summarized as follows: 1. business understanding and data collection, 2. literature review, 3. data preparation and exploration, 4. modelling and result analysis and 5. conclusion and recommendation. The data-driven methods that were analysed and compared are: Partial Least Squares Regression (PLSR), Artificial Neu- ral Network (ANN) and Random Forest(RF). After cleaning and analysing the production line data, predictive maintenance with the current available data in TATA Steel, Shotton is best feasible with PLSR. The study further concludes that, predictive maintenance is likely to be feasible in similar industries that are in transition to industry 4.0 and have growing volumes of production data with varying quality and detail. However, as illustrated in this case study, careful understanding of the industrial process, thorough modeling and cleaning of the data as well as careful method selection and tuning are required. Moreover, the resulting model needs to be packaged in a user friendly way to find its way to the job floor.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "10406190",
        "isbn": null,
        "journal": "The Electricity Journal",
        "publisher": null,
        "title": "deeplearningassistedpowerlossmanagementforactivedistributionnetworks",
        "booktitle": null,
        "doi": "10.1016/j.tej.2020.106888",
        "author": [
            "Li, Dairui",
            "Tan, Jia",
            "Yu, Qun",
            "Li, Zhiyi"
        ],
        "keywords": [
            "Active distribution network, Deep learning, Data preprocessing, Power loss management, Cybersecurity"
        ],
        "abstract": "With widespread deployment of advanced information and communication techniques, power distribution networks have been undergoing a transition towards active distribution networks (ADNs). As ADNs feature multi-way interactions between power supply and demand as well as tight coupling between cyber and physical elements, conventional physical-driven energy management schemes are insufficient to tackle the challenges of increasingly complicated operating conditions. In this paper, a novel three-stage decision-making framework is proposed to manage the power losses of ADNs by taking advantage of state-of-the-art deep learning methods. The paper first sheds light on the opportunities and challenges brought by the immense amount of heterogeneous data in the context of ADNs. In particular, generative modeling methods such as generative adversarial networks are introduced to develop a data preprocessing scheme for fixing and enhancing data collected from heterogeneous field devices. The paper then employs recurrent neural networks such as long-short-term-memory networks to infer and formulate the intricate relations between preprocessed data with the objective of enhancing the observability of network topology and line losses. More specifically, an optimization-assisted deep learning method is proposed to facilitate the optimal decision making on power loss reduction under fast-changing operating conditions. Last, limitations of deep learning models such as cybersecurity challenges are discussed in depth in the context of ADNs.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,750"
    },
    {
        "issnkey": "07351097",
        "isbn": null,
        "journal": "Journal of the American College of Cardiology",
        "publisher": null,
        "title": "navigatingthepathtodigitaltransformation",
        "booktitle": null,
        "doi": "10.1016/j.jacc.2021.06.018",
        "author": [
            "Itchhaporia, Dipti"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "knowledgemanagementprocessforairqualitysystemsbasedondatawarehousespecification",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.08.004",
        "author": [
            "{Hadj Sassi}, Mohamed",
            "{Chaari Fourati}, Lamia",
            "Zekri, Manel",
            "{Ben Yahia}, Sadok"
        ],
        "keywords": [
            "Knowledge Management, Air Quality, Data Warehouse, Conceptual Data Model, Multidimensional Design, Ontology."
        ],
        "abstract": "Even though several systems for Air Quality (AQ) monitoring have been in existence for over a decade, a research model for Knowledge Management (KM) of AQ data has to be created in order to enhance the decision-making and organize the air quality data collected from the Internet of Things (IoT) consumer devices. This model should be made more performant by ensuring greater flexibility and interoperability between devices and emerging technologies. In this context, we propose an approach for representing Data WareHouse (DWH) schema based on an ontology that captures the multidimensional knowledge of tools, techniques, and technologies used for novel AQ systems. This enhances decision-making by coping with potential problems such as data sources heterogeneity and covering the various phases of the decision-making life cycle.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "",
        "isbn": "978-0-323-88493-8",
        "journal": null,
        "publisher": "Chandos Publishing",
        "title": "18trustworthyornotresearchdataoncovid19indatarepositories",
        "booktitle": "Libraries, Digital Information, and COVID",
        "doi": "10.1016/B978-0-323-88493-8.00027-6",
        "author": [
            "Azeroual, Otmane",
            "Sch\u00f6pfel, Joachim"
        ],
        "keywords": [
            "Research data, Data repository, Data quality, Trustworthiness, Open access, Open science, COVID-19"
        ],
        "abstract": "The outburst of the COVID-19 pandemic has boosted the need for seamless, unrestricted, fast, and free access to the latest research results on the virus, on its treatment, prevention, protocols, and so on. Open access to publications and research data, suddenly, became self-evident, not only for researchers in life and medical sciences but also for politicians, journalists, and society as a whole. At the same time, this sudden awareness triggered another debate on the quality and, moreover, the trustworthiness of this mass of information made available most often without any form of quality control (peer review). Thousands of datasets from research on COVID-19 and related topics have already been deposited on data repositories. Our chapter discusses the issue of the quality and trustworthiness of research data in data repositories using examples from the ongoing pandemic. It offers insights into some fundamental concepts and summarizes recommendations for quality assurance and evaluation of research data.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00063223",
        "isbn": null,
        "journal": "Biological Psychiatry",
        "publisher": null,
        "title": "sharedandanxietyspecificpediatricpsychopathologydimensionsmanifestdistributedneuralcorrelates",
        "booktitle": null,
        "doi": "10.1016/j.biopsych.2020.10.018",
        "author": [
            "Linke, Julia",
            "Abend, Rany",
            "Kircanski, Katharina",
            "Clayton, Michal",
            "Stavish, Caitlin",
            "Benson, Brenda",
            "Brotman, Melissa",
            "Renaud, Olivier",
            "Smith, Stephen",
            "Nichols, Thomas",
            "Leibenluft, Ellen",
            "Winkler, Anderson",
            "Pine, Daniel"
        ],
        "keywords": [
            "Anxiety, Disruptive behavior, Intrinsic brain connectivity, Irritability, Joint canonical correlation and independent component analysis, Youth"
        ],
        "abstract": "Background Imaging research has not yet delivered reliable psychiatric biomarkers. One challenge, particularly among youth, is high comorbidity. This challenge might be met through canonical correlation analysis designed to model mutual dependencies between symptom dimensions and neural measures. We mapped the multivariate associations that intrinsic functional connectivity manifests with pediatric symptoms of anxiety, irritability, and attention-deficit/hyperactivity disorder (ADHD) as common, impactful, co-occurring problems. We evaluate the replicability of such latent dimensions in an independent sample. Methods We obtained ratings of anxiety, irritability, and ADHD, and 10 minutes of resting-state functional magnetic resonance imaging data, from two independent cohorts. Both cohorts (discovery: n = 182; replication: n = 326) included treatment-seeking youth with anxiety disorders, with disruptive mood dysregulation disorder, with ADHD, or without psychopathology. Functional connectivity was modeled as partial correlations among 216 brain areas. Using canonical correlation analysis and independent component analysis jointly we sought maximally correlated, maximally interpretable latent dimensions of brain connectivity and clinical symptoms. Results We identified seven canonical variates in the discovery and five in the replication cohort. Of these canonical variates, three exhibited similarities across datasets: two variates consistently captured shared aspects of irritability, ADHD, and anxiety, while the third was specific to anxiety. Across cohorts, canonical variates did not relate to specific resting-state networks but comprised edges interconnecting established networks within and across both hemispheres. Conclusions Findings revealed two replicable types of clinical variates, one related to multiple symptom dimensions and a second relatively specific to anxiety. Both types involved a multitude of broadly distributed, weak brain connections as opposed to strong connections encompassing known resting-state networks.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "13.382",
        "scimago_value": "5,335"
    },
    {
        "issnkey": "1556407x",
        "isbn": null,
        "journal": "Sleep Medicine Clinics",
        "publisher": null,
        "title": "newpathsinrespiratorysleepmedicineconsumerdevicesehealthanddigitalhealthmeasurements",
        "booktitle": null,
        "doi": "10.1016/j.jsmc.2021.08.006",
        "author": [
            "Penzel, Thomas",
            "Dietz-Terjung, Sarah",
            "Woehrle, Holger",
            "Sch\u00f6bel, Christoph"
        ],
        "keywords": [
            "E-health, Out-of-center testing, Health apps, Longtime monitoring, Diagnostics, Sleep-disordered breathing"
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01641212",
        "isbn": null,
        "journal": "Journal of Systems and Software",
        "publisher": null,
        "title": "anempiricalstudyofcovid19relatedpostsonstackoverflowtopicsandtechnologies",
        "booktitle": null,
        "doi": "10.1016/j.jss.2021.111089",
        "author": [
            "Georgiou, Konstantinos",
            "Mittas, Nikolaos",
            "Chatzigeorgiou, Alexandros",
            "Angelis, Lefteris"
        ],
        "keywords": [
            "COVID-19, Pandemic, StackOverflow, Knowledge-sharing"
        ],
        "abstract": "The COVID-19 outbreak, also known as the coronavirus pandemic, has left its mark on every aspect of our lives and at the time of this writing is still an ongoing battle. Beyond the immediate global-wide health response, the pandemic has triggered a significant number of IT initiatives to track, visualize, analyze and potentially mitigate the phenomenon. For individuals or organizations interested in developing COVID-19 related software, knowledge-sharing communities such as Stack Overflow proved to be an effective source of information for tackling commonly encountered problems. As an additional contribution to the investigation of this unprecedented health crisis and to assess how fast and how well the community of developers has responded, we performed a study on COVID-19 related posts in Stack Overflow. In particular, we profiled relevant questions based on key post features and their evolution, identified the most prominent technologies adopted for developing COVID-19 software and their interrelations and focused on the most persevering problems faced by developers. For the analysis of posts we employed descriptive statistics, Association Rule Graphs, Survival Analysis and Latent Dirichlet Allocation. The results reveal that the response of the developers\u2019 community to the pandemic was immediate and that the interest of developers on COVID-19 related challenges was sustained after its initial peak. In terms of the problems addressed, the results show a clear focus on COVID-19 data collection, analysis and visualization from/to the web, in line with the general needs for monitoring the pandemic.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.829",
        "scimago_value": "0,642"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "facadedeteriorationpredictionwiththeuseofmachinelearningmethodsbasedonobjectiveparametersandeparticipationdata",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.10.005",
        "author": [
            "Antonov, Aleksandr",
            "Khodnenko, Ivan",
            "Kudinov, Sergei"
        ],
        "keywords": [
            "machine learning, logistic regression, predictive data, e-participation data, quality of urban environment, infrastructure deterioration"
        ],
        "abstract": "Condition monitoring and timely repair of residential buildings is an important task when ensuring a comfortable life in cities. In the case of large metropolitan areas, it is a difficult task to perform continuous objective condition monitoring for tens of thousands of residential buildings by efforts of experts. However, residential infrastructure health can be predicted on the basis of indirect data. These can be objective building parameters or subjective data on citizens\u2019 complaints about deterioration. In cities today, it is possible to collect such data in machine-readable form from various information systems. This article proposes a method to predict external deterioration of buildings on the basis of indirect data, using machine learning and SMILE Low-coding platform. Based on the results of method approbation, which used data of a metropolis, the significance of electronic participation data and objective parameters of objects for fa\u00e7ade deterioration forecast was assessed. Options for further research are proposed to improve the quality of deterioration predicting by using data on citizens\u2019 complaints about infrastructure damage.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "02684012",
        "isbn": null,
        "journal": "International Journal of Information Management",
        "publisher": null,
        "title": "fortyyearsoftheinternationaljournalofinformationmanagementabibliometricanalysis",
        "booktitle": null,
        "doi": "10.1016/j.ijinfomgt.2020.102307",
        "author": [
            "Donthu, Naveen",
            "Kumar, Satish",
            "Pandey, Nitesh",
            "Gupta, Prashant"
        ],
        "keywords": [
            "Bibliometric analysis, International Journal of Information Management, Performance analysis, Science mapping, Negative binomial regression, Citation analysis"
        ],
        "abstract": "In 2019, the International Journal of Information Management (IJIM) celebrated its 40th year of publication. This study commemorates this event by presenting a retrospect of the journal. Using a range of bibliometric tools, we find that the journal has grown impressively in terms of publication and citation. The contributions come from all over the world, but the majority are from Europe and the United States. The journal has mostly published empirical articles, with its authors dominantly using quantitative methodology. Further, the culture of collaboration has increased among authors over the years. The journal publishes on a number of including managing information systems, information technologies and their application in business, technology acceptance among consumers, using information systems for decision making, social perspectives on knowledge management, and information research from the social science perspective. Regression analysis reveals that article attributes such as article order, methodology, presence of authors from Europe, number of references, number of keywords, and abstract length have a significant association with the citations. Finally, we find that conceptual and review articles have a positive association with citations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "14.098",
        "scimago_value": "2,770"
    },
    {
        "issnkey": "00991333",
        "isbn": null,
        "journal": "The Journal of Academic Librarianship",
        "publisher": null,
        "title": "exploringpotentialrolesofacademiclibrariesinundergraduatedatascienceeducationcurriculumdevelopment",
        "booktitle": null,
        "doi": "10.1016/j.acalib.2021.102320",
        "author": [
            "Shao, Gang",
            "Quintana, Jenny",
            "Zakharov, Wei",
            "Purzer, Senay",
            "Kim, Eunhye"
        ],
        "keywords": [
            "Data science, Libraries, Curriculum, Education, Data management, College, Data ethics"
        ],
        "abstract": "Undergraduate data science education is receiving increasing interest in many higher education institutions in the U.S., with the proliferation of data and data related work and research. As an emerging interdisciplinary study field, data science curriculum is typically a collection of individual data science related courses from different schools and departments, most of which are teaching data science in a siloed fashion. Therefore, it is necessary to map the landscape of existing curricula and explore how academic libraries can collaborate and contribute to undergraduate data science education. In this study, we analyzed teaching content and topics of over 100 data science related courses at Purdue University to map the landscape and explore roles of academic libraries to support data science education curriculum. Our results indicate most existing courses focused on \u2018hard-core\u2019 scientific analytic principles, such as computer science, statistics, and domain-specific skills. Courses of data-oriented skills, such as data management, data ethics, and data communications were limited across disciplines. In addition, data science courses were more likely targeting STEM students at upper levels (3rd and 4th year students). Academic libraries can enrich data science education efforts, by supporting credit courses, certificate programs, and other co-curricular activities to provide learning opportunities to all students, particularly 1st and 2nd year students and non-STEM majors.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.533",
        "scimago_value": "0,889"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "riskmanagementinsustainablesmartcitiesgovernanceatoeframework",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.120743",
        "author": [
            "Ullah, Fahim",
            "Qayyum, Siddra",
            "Thaheem, Muhammad",
            "Al-Turjman, Fadi",
            "Sepasgozar, Samad"
        ],
        "keywords": [
            "Smart city, Sustainability, Risk management, Smart city governance, Technology-organisation-environment (TOE) framework"
        ],
        "abstract": "Sustainable smart cities are confronted by technological, organisational and external risks, making their governance difficult and susceptible to manipulation. Based on a comprehensive literature review of 796 systematically retrieved articles, the current study proposes a multilayered technology-organisation-environment (TOE-based) risk management framework for sustainable smart city governance. A total of 56 risks are identified and grouped into TOE categories. There are 17 technological risks, including IoT networks, public internet management and user safety concerns, with a 38.7% contribution to smart city governance risks. With a 15.6% share, there are 11 organisational risks, including user data security and cloud management. There are 28 external risks with a contribution of 46.7% to the smart city governance and consist of smart city's environment, governance, integration and security risks. A multilayered TOE-based risk management framework is proposed to identify and manage the risks associated with smart city governance in the current study. The framework links smart citizens to each other through the smart city governance team and the integrated TOE layers. The iterative risk management process of identification, analysis, evaluation, monitoring and response planning is carried out in the TOE layers, both at the external layer levels and internal management levels. The proposed framework operationalises the risk management process for smart city governance by presenting the collection of pertinent risks and their thematic TOE categorisation. The criticality of the identified risks in line with the study's rankings can help researchers and practitioners understand the top risks of smart city governance. These risks present investment opportunities for city governance bodies to develop critical and effective responses as well as provide safety, security and enhanced privacy for citizens.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "acrossdisciplinarycomparisonofmultimodaldatafusionapproachesandapplicationsacceleratinglearningthroughtransdisciplinaryinformationsharing",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2020.113885",
        "author": [
            "Bokade, Rohit",
            "Navato, Alfred",
            "Ouyang, Ruilin",
            "Jin, Xiaoning",
            "Chou, Chun-An",
            "Ostadabbas, Sarah",
            "Mueller, Amy"
        ],
        "keywords": [
            "Multimodal data fusion, Machine learning, Complex systems, Big data, Trans-disciplinary"
        ],
        "abstract": "Multimodal data fusion (MMDF) is the process of combining disparate data streams (of different dimensionality, resolution, type, etc.) to generate information in a form that is more understandable or usable. Despite the explosion of data availability in recent decades, as yet there is no well-developed theoretical basis for multimodal data fusion, i.e., no way to determine a priori which approach is best suited to combine an arbitrary set of available data to achieve a stated goal for a given application. This has resulted in exploration of a wide variety of approaches across numerous domains but as yet very little integration of conclusions at a meta (cross-disciplinary) level. In response, this manuscript poses the following questions: (1) How convergent (or divergent) are approaches within single disciplines? (2) How similar are the challenges posed across different disciplines, i.e., might there be opportunity for successes in MMDF achieved in one field to inform progress in other areas as well? and (3) Where are the outstanding gaps in MMDF research, and what does this imply as targets for high impact research in the coming years? To begin to answer these questions, an apples-to-apples comparison of the literature of nine stakeholder-centric engineering domains (civil engineering, transportation, energy, environmental engineering, food engineering, critical care (healthcare), neuroscience, manufacturing/automation, and robotics) was created by quantifying the numbers and dimensionalities of modalities and sensors in each published project and classifying the algorithms used and purposes for which they are used. Within disciplines, it is shown there is often a tendency for use of similar methodologies, both in choice of level of fusion and data algorithm class. Yet this analysis also reveals that many problem types (defined by data dimensionality, modality number and type, and fusion purpose) are shared across different domains and are approached differently in those domains, e.g., transportation problems have similar characteristics to critical care, food science, robotics, and civil engineering. Of the disciplines studied, most (>75%) share problem characteristics with 3\u20135 others; to support leveraging these resources, lookup tables indexed by data dimensions, number of modalities, etc. are provided as a starting point for cross-disciplinary MMDF literature searches for new applications. Critical gaps identified are (1) a drop off of the number of published studies with increasing number of distinct modalities and (2) a dearth of publications tackling challenges with high dimensionality inputs, especially time-series 2D and 3D data. These gaps may point to topics where algorithm development will be fruitful to enable future solutions as video and other high-dimensionality sensors decrease in price. Finally, the lack of a shared vocabulary across disciplines makes analyses like the one conducted here challenging, as does the often implicit incorporation of expert knowledge into design; therefore progress toward a better leveraging of the current state of knowledge and toward a theoretical MMDF framework depends critically on improved cross-disciplinary communication and coordination on this topic.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "",
        "isbn": "978-0-323-90137-6",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter2aiandsoftwareenablersforhighlyautomatedandautonomousvehicles",
        "booktitle": "Autonomous Vehicles",
        "doi": "10.1016/B978-0-323-90137-6.00014-X",
        "author": [
            "Dimitrakopoulos, George",
            "Tsakanikas, Aggelos",
            "Panagiotopoulos, Elias"
        ],
        "keywords": [
            "Algorithms, Artificial Intelligence, Cognitive, Decision-making, Deep learning, Machine learning"
        ],
        "abstract": "The goal of this chapter is to present the latest advances and challenges on management algorithms and software for enabling highly automated and autonomous driving, enhanced by Artificial Intelligence (AI) tools and methods. Indicative examples include cognitive computing, knowledge-based systems, and noncausal reasoning algorithms, which are used for active and passive safety of autonomous vehicles, as well as emergency management systems for highly automated/autonomous vehicles.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23522143",
        "isbn": null,
        "journal": "Computational Condensed Matter",
        "publisher": null,
        "title": "recentmachinelearningguidedmaterialresearchareview",
        "booktitle": null,
        "doi": "10.1016/j.cocom.2021.e00597",
        "author": [
            "Chowdhury, Mohammad",
            "Hossain, Nayem",
            "{Ahmed Shuvho}, Md",
            "Fotouhi, Mohammad",
            "Islam, Md",
            "Ali, Md",
            "Kashem, Mohammod"
        ],
        "keywords": [
            "ML, Material science, Design, Characterization, Advancements, Challenges"
        ],
        "abstract": "Sustainable development of modern society demands discovering new materials with superior properties in different applications such as aerospace, wind, civil, automotive, etc. Characterizing and predicting material properties using traditional methods are time consuming and expensive. Therefore, advanced methods have been developed to meet the need for quick and reliable design and characterizing of materials properties. ML methods have made it possible to optimize and automate design performance and discover new materials. This review paper gives an overview of the implementation of ML in i) discovery of new materials, and ii) characterization of materials ML. Various ML models for materials manufacturing as well as how ML is applied to model materials are discussed. Recent advances, ML applications, as well as upcoming challenges and perspectives are discussed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,394"
    },
    {
        "issnkey": "22131582",
        "isbn": null,
        "journal": "NeuroImage: Clinical",
        "publisher": null,
        "title": "hybridmodellingforstrokecarereviewandsuggestionsofnewapproachesforriskassessmentandsimulationofscenarios",
        "booktitle": null,
        "doi": "10.1016/j.nicl.2021.102694",
        "author": [
            "Herrg\u00e5rdh, Tilda",
            "Madai, Vince",
            "Kelleher, John",
            "Magnusson, Rasmus",
            "Gustafsson, Mika",
            "Milani, Lili",
            "Gennemark, Peter",
            "Cedersund, Gunnar"
        ],
        "keywords": [
            "Stroke, Mechanistic modelling, Machine learning, Bioinformatics, Precision medicine"
        ],
        "abstract": "Stroke is an example of a complex and multi-factorial disease involving multiple organs, timescales, and disease mechanisms. To deal with this complexity, and to realize Precision Medicine of stroke, mathematical models are needed. Such approaches include: 1) machine learning, 2) bioinformatic network models, and 3) mechanistic models. Since these three approaches have complementary strengths and weaknesses, a hybrid modelling approach combining them would be the most beneficial. However, no concrete approach ready to be implemented for a specific disease has been presented to date. In this paper, we both review the strengths and weaknesses of the three approaches, and propose a roadmap for hybrid modelling in the case of stroke care. We focus on two main tasks needed for the clinical setting: a) For stroke risk calculation, we propose a new two-step approach, where non-linear mixed effects models and bioinformatic network models yield biomarkers which are used as input to a machine learning model and b) For simulation of care scenarios, we propose a new four-step approach, which revolves around iterations between simulations of the mechanistic models and imputations of non-modelled or non-measured variables. We illustrate and discuss the different approaches in the context of Precision Medicine for stroke.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.881",
        "scimago_value": "1,772"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-816078-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "insilicoadmemodeling",
        "booktitle": "Systems Medicine",
        "doi": "10.1016/B978-0-12-801238-3.11532-6",
        "author": [
            "Oprisiu, Ioana",
            "Winiwarter, Susanne"
        ],
        "keywords": [
            "Absorption, AI, Distribution, DMPK, Drug discovery, Excretion, In silico ADME, IVIVe, Machine learning, Metabolism, QSAR, QSPR"
        ],
        "abstract": "Modeling of absorption, distribution, metabolism and excretion properties is well established in drug discovery. Here we present principles, considerations when and how to build models and how to apply them in real life in the industrial setting. The availability and quality of experimental data is important. However, for in silico models to be utilized availability is a major factor. Additionally, model quality measures need to be presented in an easily understandable manner for chemists and DMPK scientists.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "25895974",
        "isbn": null,
        "journal": "Trends in Chemistry",
        "publisher": null,
        "title": "towardmachinelearningenhancedhighthroughputexperimentation",
        "booktitle": null,
        "doi": "10.1016/j.trechm.2020.12.001",
        "author": [
            "Eyke, Natalie",
            "Koscher, Brent",
            "Jensen, Klavs"
        ],
        "keywords": [
            "high-throughput experimentation, machine learning, active learning"
        ],
        "abstract": "Recent literature suggests that the fields of machine learning (ML) and high-throughput experimentation (HTE) have separately received considerable attention from chemists and engineers, leading to the development of powerful reactivity models and platforms capable of rapidly performing thousands of reactions. The merger of ML with HTE presents a wealth of opportunities for the exploration of chemical space, but the integration of the two has yet to be fully realized. We highlight examples of recent developments in ML and HTE that collectively suggest the utility of their integration. Our analysis highlights the complementarity of the two fields, while exposing a number of obstacles that can and should be overcome to take full advantage of this merger and thereby accelerate chemical research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "24.081",
        "scimago_value": "8,037"
    },
    {
        "issnkey": "26666839",
        "isbn": null,
        "journal": "Geography and Sustainability",
        "publisher": null,
        "title": "thecodeoftargetedpovertyalleviationinchinaageographyperspective",
        "booktitle": null,
        "doi": "10.1016/j.geosus.2021.09.004",
        "author": [
            "Yang, Yuanyuan",
            "Liu, Yansui"
        ],
        "keywords": [
            "Targeted poverty alleviation, Sustainable development, China, Geography, Experience"
        ],
        "abstract": "Geography is suitable for the study of sustainability from a transdisciplinary perspective, which takes the human-land relationship as the core research. As a key obstacle to rural sustainability, poverty is an external manifestation of the coupling maladjustment of elements in human-land territorial systems. As the world's largest developing country, China eradicated extreme poverty in 2020 and made significant contributions to global poverty reduction. Especially over the last eight years, China has implemented a targeted poverty alleviation (TPA) strategy and has continuously promoted theoretical, organizational and institutional innovations for poverty reduction. From the perspective of geography, this paper extracts the experiences of China's TPA strategy, represented by the \"5W2H\" mode. The research concludes that: (1) Precise identification, as the foundation of TPA, aims to introduce a registration system to obtain records of all poor households and then answer the \"5W\" (what, where, why, who, when) issues of the geography of poverty. (2) Precise assistance is the key of TPA, which aims to solve the issue of \"how to offer help and support\". The barriers to escaping poverty can be accomplished through policies and measures that focus on the diverse causes of poverty and considering different situations. (3) Accurate assessments are an essential means of TPA, relevant to solve \"how to measure the end of poverty alleviation\", and third-party evaluations play an important role in improving the accuracy of poverty alleviation. (4) The TPA mechanism lies in reconstructing the human-land-industry structures in the impoverished areal system. It is urgent to introduce China's successful experience and typical modes of TPA for global human-earth system coordination and sustainable development and contribute to building a community of human destiny.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22147853",
        "isbn": null,
        "journal": "Materials Today: Proceedings",
        "publisher": null,
        "title": "investigationoffactorsaffectingstudentperformanceevaluationusingeducationmaterialsdataminingtechnique",
        "booktitle": null,
        "doi": "10.1016/j.matpr.2021.05.026",
        "author": [
            "Malini, J.",
            "Kalpana, Y."
        ],
        "keywords": [
            "Educational datamining, Dataset, Students performance, Attributes, Features, Machine learning, Materials and methods, Analysis"
        ],
        "abstract": "Every year students success rate was analysed by the Educational Institutions to develop their Academic standard. To identify the success rate many kinds of techniques are used such as statistics, physical examination and currently ongoing data mining techniques. Data mining Techniques was widely used in many fields, it is also used in the Educational environment known as Educational Data Mining (EDM). Educational data mining generate prototype in solving the research problems in students data and used to locate the unseen patterns in the students detailed dateset. This paper uses the EDM to characterize the distinct factors affecting the students performance by making predictions with efficient algorithms. Educational professionals have to identify the causes for the student failure in academic performance and the students not succeed in completing their education which becomes a social problem these days. The machine learning techniques help the researchers to analyse the student\u2019s learning habits and their performance in academic. This paper would discuss different kinds of algorithms to analyse the economic background of the students which mainly affects the students performance. The dataset was utilized from the UCI Repository of secondary school students performance and analysed using the Weka tool for the data mining process.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,341"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "evaluatingtheperformanceoflbsmdatatoestimatethegrossdomesticproductofchinaatmultiplescalesacomparisonwithnppviirsnighttimelightdata",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.129558",
        "author": [
            "Huang, Ziwei",
            "Li, Shaoying",
            "Gao, Feng",
            "Wang, Fang",
            "Lin, Jinyao",
            "Tan, Ziling"
        ],
        "keywords": [
            "Location based social media data, NPP/VIIRS, Economic, OLS, GWR, China"
        ],
        "abstract": "Regional economic development evaluation is essential for understanding social and environmental issues. Although the nighttime light (NTL) data have been proved to be effective in economic estimation, it cannot reflect the human activities that occur during the daytime. Recently, with the widespread use of smart mobile devices, the location based social media (LBSM) data are increasingly being used as a proxy for real-time human activities. However, little work was carried out to explore the potential of LBSM data in estimating economic development at different scales in China. This study filled this gap by evaluating the effectiveness of Tencent user density (TUD) data, a typical type of LBSM data in China, in Gross Domestic Product (GDP) modeling at the provincial, municipal, and county scales. In this study, we employed holiday and non-holiday TUD sample data to simulate the annual TUD data, and compared it with the new generation NTL data, NPP/VIIRS images. The results showed that although the simulated annual TUD data does not perform better than NPP/VIIRS-NTL data in provincial and municipal GDP estimation, it outperforms NPP/VIIRS-NTL data at the county scale. More importantly, the simulated annual TUD data are much more powerful and reliable than NPP/VIIRS-NTL data in underdeveloped areas with complex terrain, such as the Northwest and Southwest China, as well as in more developed areas with separation of work and housing, such as the North China and South China. This is mainly because TUD data can reduce the impact of natural factors such as terrain on data collection as well as reflect both daytime and nighttime human activities. This study confirmed that the LBSM-TUD data is a potential and promising data source for economic modeling in small scale areas of China, which will help to support China's regional economic evaluation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "03014797",
        "isbn": null,
        "journal": "Journal of Environmental Management",
        "publisher": null,
        "title": "developingmachinelearningmodelsforrelativehumiditypredictioninairbasedenergysystemsandenvironmentalmanagementapplications",
        "booktitle": null,
        "doi": "10.1016/j.jenvman.2021.112736",
        "author": [
            "Qadeer, Kinza",
            "Ahmad, Ashfaq",
            "Qyyum, Muhammad",
            "Nizami, Abdul-Sattar",
            "Lee, Moonyong"
        ],
        "keywords": [
            "Air quality parameters, Random forest, Machine learning-based estimation, Aspen hysys, Support vector machine, Environmental management operations"
        ],
        "abstract": "The prediction of relative humidity is a challenging task because of its nonlinear nature. The machine learning-based prediction strategies have attained significant attention in tackling a broad class of challenging nonlinear and complex problems. The random forest algorithm is a well-proven machine learning algorithm due to its ease of training and implementation, as it requires minimal preprocessing. The random forest algorithm has hitherto not been employed for estimating air quality parameters, such as relative humidity. In this study, the random forest approach is implemented to estimate the relative humidity as a function of dry- and wet-bulb temperatures. A well-known commercial process simulator called Aspen HYSYS\u00ae V10 is linked with MATLAB\u00ae version 2019a to establish a data mining environment. The robustness of the prediction model is evaluated against varying wet-bulb depressions. There is high absolute deviation that indicates a lower prediction performance of the model against the higher wet-bulb depression i.e., ~20.0 \u00b0C. The random forest model can predict relative humidity with a 1.1% mean absolute deviation compared to the values obtained through Aspen HYSYS. The performance of the RF estimation model is also compared with a well-known support vector regression model. The random forest model demonstrates 74.4% better performance than the support vector machine model for the problem of interest, i.e., relative humidity estimation. This study will significantly help the practitioners in efficient designing of air-dependent energy systems as well as in better environmental management through rigorous prediction of relative humidity.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02638762",
        "isbn": null,
        "journal": "Chemical Engineering Research and Design",
        "publisher": null,
        "title": "realisticinterplaysbetweendatascienceandchemicalengineeringinthefirstquarterofthe21stcenturypart2dosanddonts",
        "booktitle": null,
        "doi": "10.1016/j.cherd.2021.03.012",
        "author": [
            "Piccione, Patrick"
        ],
        "keywords": [
            "Data science, Chemical engineering, Digital transformation, Industry 4.0, Smart manufacturing"
        ],
        "abstract": "Under various names, such as, data science, Industry 4.0, or smart manufacturing, digital technologies are transforming our world. Although value statements and promises are published in a steady stream, uptake in the chemical and process industries has been moderate. Successful transformations are not confined to tasks, the \u201cwhats\u201d. They also require great care in how they are carried out. This overview, aimed at all participants in the digital transformation of the chemical industry, presents \u201cdos and don\u2019ts\u201d method recommendations for three successive steps: strategy development to define goals, (organisational) mobilisation for implementation, and project delivery. Successful strategy development requires assembling an empowered and skilled team; truly understanding the data science and digital transformation topics; accepting emergence and iteration; and focusing on real needs. Mobilising an organisation is essential so that it can translate strategy to tactics and value. Within organisations, one must therefore: enable project identification; set up a supportive organisational structure and skilful people within it. Looking outside, participation in partnerships is essential to access external resources. Delivery of valuable projects is the end goal. A diverse portfolio is needed, as well as effective collaborations between subject matter experts and data scientists. Technically, the use of software best practice is beneficial, and care must be taken of the data themselves. In the longer term, data science opportunities will extend beyond merely improving traditional analytics to make them faster, better, and more user-friendly. The early identification of beneficial future trends requires encouraging those individuals who have an interest in disruptive currents, and the perceptiveness to sense their areas of application.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13619209",
        "isbn": null,
        "journal": "Transportation Research Part D: Transport and Environment",
        "publisher": null,
        "title": "modificationofnewellscarfollowingmodelincorporatingmultidimensionalstochasticparametersforemissionestimation",
        "booktitle": null,
        "doi": "10.1016/j.trd.2020.102692",
        "author": [
            "Meng, Dongli",
            "Song, Guohua",
            "Wu, Yizheng",
            "Zhai, Zhiqiang",
            "Yu, Lei",
            "Zhang, Jianbo"
        ],
        "keywords": [
            "Newell\u2019s car-following model, Stochasticity, Emission estimation, Driving behavior heterogeneity, Vehicle trajectory data"
        ],
        "abstract": "Existing studies have indicated that the vehicle trajectories derived from Newell's car-following model (NCM) fail to capture driving behavior heterogeneity, resulting in considerable emission estimation errors. This study investigated the situation-dependent heterogeneity of car-following behavior, based on field vehicle trajectories in Beijing, and proposed a multidimensional stochastic Newell car-following model (MSNCM) incorporating three stochastic parameters: random response time, speed-dependent critical jam spacing, and speed difference- and spacing-dependent acceleration. The comparison between the field data and numerical simulations of the NCM and MSNCM shown that the MSNCM performed well in generating realistic vehicle trajectories for emission estimation. The relative errors of the emission factors derived from the field and the MSNCM simulated trajectories were 0.26%, 0.91%, 1.37%, and 0.25% for CO2, CO, HC, and NOx, respectively, which represented reductions of approximately 15%-46% compared with the traditional NCM.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.495",
        "scimago_value": "1,600"
    },
    {
        "issnkey": "25897500",
        "isbn": null,
        "journal": "The Lancet Digital Health",
        "publisher": null,
        "title": "wirelessskinsensorsforphysiologicalmonitoringofinfantsinlowincomeandmiddleincomecountries",
        "booktitle": null,
        "doi": "10.1016/S2589-7500(21)00001-7",
        "author": [
            "Xu, Shuai",
            "Rwei, Alina",
            "Vwalika, Bellington",
            "Chisembele, Maureen",
            "Stringer, Jeffrey",
            "Ginsburg, Amy",
            "Rogers, John"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Summary Globally, neonatal mortality remains unacceptability high. Physiological monitoring is foundational to the care of these vulnerable patients to assess neonatal cardiopulmonary status, guide medical intervention, and determine readiness for safe discharge. However, most existing physiological monitoring systems require multiple electrodes and sensors, which are linked to wires tethered to wall-mounted display units, to adhere to the skin. For neonates, these systems can cause skin injury, prevent kangaroo mother care, and complicate basic clinical care. Novel, wireless, and biointegrated sensors provide opportunities to enhance monitoring capabilities, reduce iatrogenic injuries, and promote family-centric care. Early validation data have shown performance equivalent to (and sometimes exceeding) standard-of-care monitoring systems in premature neonates cared for in high-income countries. The reusable nature of these sensors and compatibility with low-cost mobile phones have the future potential to enable substantially lower monitoring costs compared with existing systems. Deployment at scale, in low-income countries, holds the promise of substantial improvements in neonatal outcomes.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,346"
    },
    {
        "issnkey": "01681699",
        "isbn": null,
        "journal": "Computers and Electronics in Agriculture",
        "publisher": null,
        "title": "highresolution3dterrestriallidarforcottonplantmainstalkandnodedetection",
        "booktitle": null,
        "doi": "10.1016/j.compag.2021.106276",
        "author": [
            "Sun, Shangpeng",
            "Li, Changying",
            "Chee, Peng",
            "Paterson, Andrew",
            "Meng, Cheng",
            "Zhang, Jingyi",
            "Ma, Ping",
            "Robertson, Jon",
            "Adhikari, Jeevan"
        ],
        "keywords": [
            "High throughput phenotyping, Terrestrial LiDAR, Three-dimensional skeleton, Plant node detection, Minimum spanning tree"
        ],
        "abstract": "Dense three-dimensional point clouds provide opportunities to retrieve detailed characteristics of plant organ-level phenotypic traits, which are helpful to better understand plant architecture leading to its improvements via new plant breeding approaches. In this study, a high-resolution terrestrial LiDAR was used to acquire point clouds of plants under field conditions, and a data processing pipeline was developed to detect plant main stalks and nodes, and then to extract two phenotypic traits including node number and main stalk length. The proposed method mainly consisted of three steps: first, extract skeletons from original point clouds using a Laplacian-based contraction algorithm; second, identify the main stalk by converting a plant skeleton point cloud to a graph; and third, detect nodes by finding the intersection between the main stalk and branches. Main stalk length was calculated by accumulating the distance between two adjacent points from the lowest to the highest point of the main stalk. Experimental results based on 26 plants showed that the proposed method could accurately measure plant main stalk length and detect nodes; the average R2 and mean absolute percentage error were 0.94 and 4.3% for the main stalk length measurements and 0.7 and 5.1% for node counting, respectively, for point numbers between 80,000 and 150,000 for each plant. Three-dimensional point cloud-based high throughput phenotyping may expedite breeding technologies to improve crop production.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.565",
        "scimago_value": "1,208"
    },
    {
        "issnkey": "00014575",
        "isbn": null,
        "journal": "Accident Analysis & Prevention",
        "publisher": null,
        "title": "ensemblebasedmodelselectionforimbalanceddatatoinvestigatethecontributingfactorstomultiplefatalityroadcrashesinghana",
        "booktitle": null,
        "doi": "10.1016/j.aap.2020.105851",
        "author": [
            "Yahaya, Mahama",
            "Guo, Runhua",
            "Jiang, Xinguo",
            "Bashir, Kamal",
            "Matara, Caroline",
            "Xu, Shiwei"
        ],
        "keywords": [
            "Multiple fatal injury crash, Classification, Model selection, Class imbalance, Oversampling, Ensemble classifiers"
        ],
        "abstract": "The study aims to identify relevant variables to improve the prediction performance of the crash injury severity (CIS) classification model. Unfortunately, the CIS database is invariably characterized by the class imbalance. For instance, the samples of multiple fatal injury (MFI) severity class are typically rare as opposed to other classes. The imbalance phenomenon may introduce a prediction bias in favour of the majority class and affect the quality of the learning algorithm. The paper proposes an ensemble-based variable ranking scheme that incorporates the data resampling. At the data pre-processing level, majority weighted minority oversampling (MWMOTE) is employed to treat the imbalanced training data. Ensemble of classifiers induced from the balanced data is used to evaluate and rank the individual variables according to their importance to the injury severity prediction. The relevant variables selected are then applied to the balanced data to form a training set for the CIS classification modelling. An empirical comparison is conducted through considering the variable ranking by: 1) the learning of single inductive algorithm with imbalanced data where the relevant variables are applied to the imbalanced data to form the training data; 2) the learning of single inductive algorithm with MWMOTE data and the relevant variables identified are applied to the balanced data to form the training data; and 3) the learning of ensembles with imbalanced data where the relevant variables identified are applied to the imbalanced data to form the training data. Bayesian Networks (BNs) classifiers are then developed for each ranking method, where nested subsets of the top ranked variables are adopted. The model predictions are captured in four performance indicators in the comparative study. Based on three-year (2014\u20132016) crash data in Ghana, the empirical results show that the proposed method is effective to identify the most prolific predictors of the CIS level. Finally, based on the inference results of BNs developed on the best subset, the study offers the most probable explanations to the occurrence of MFI crashes in Ghana.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22145796",
        "isbn": null,
        "journal": "Big Data Research",
        "publisher": null,
        "title": "multitemperatelogicaldatawarehousedesignforlargescalehealthcaredata",
        "booktitle": null,
        "doi": "10.1016/j.bdr.2021.100255",
        "author": [
            "Martin, Bryan",
            "Davis, Karen"
        ],
        "keywords": [
            "Data warehouse design, OLAP workloads, Healthcare data management, Data partitioning algorithms, Logical data warehouses, Columnar databases"
        ],
        "abstract": "Modern hardware architectures and advances in database technology are driving increased adoption of logical data warehouses (LDWs) that complement traditional physical data warehousing (PDW) approaches. In contrast to PDW design methodologies that emphasize physical consolidation of all data of interest on a single (perhaps distributed) computing platform, along with early-binding approaches that pre-materialize transformations and changes to the source data, LDW techniques allow for the integration and transformation of data at run-time and typically physically move or modify much less data in advance. In an environment with premium hardware such as multi-temperate storage, the successful design of LDWs depends on replication of high value data to their physical core to maximize spatial locality. Identifying and collocating high value data is a non-trivial task that has not been adequately explored in the context of LDWs in multi-temperate storage systems. In this paper, we gather queries to construct an OLAP workload for use in supporting and evaluating LDW design algorithms for a large healthcare organization. We introduce new algorithms to address the preprocessing of the workload, identification of data clusters to support OLAP queries, and assignment of clusters to appropriate (hot, warm, and cold) storage tiers, allowing the LDW to deliver results more efficiently by covering a higher percentage of its query workload using the fastest storage devices. Any use case involving copying data from sources to tiered storage targets for analytic querying could benefit from the techniques and solutions presented here.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.578",
        "scimago_value": "0,565"
    },
    {
        "issnkey": "16749871",
        "isbn": null,
        "journal": "Geoscience Frontiers",
        "publisher": null,
        "title": "evaluationofdeeplearningalgorithmsfornationalscalelandslidesusceptibilitymappingofiran",
        "booktitle": null,
        "doi": "10.1016/j.gsf.2020.06.013",
        "author": [
            "{Thi Ngo}, Phuong",
            "Panahi, Mahdi",
            "Khosravi, Khabat",
            "Ghorbanzadeh, Omid",
            "Kariminejad, Narges",
            "Cerda, Artemi",
            "Lee, Saro"
        ],
        "keywords": [
            "CNN, RNN, Deep learning, Landslide, Iran"
        ],
        "abstract": "The identification of landslide-prone areas is an essential step in landslide hazard assessment and mitigation of landslide-related losses. In this study, we applied two novel deep learning algorithms, the recurrent neural network (RNN) and convolutional neural network (CNN), for national-scale landslide susceptibility mapping of Iran. We prepared a dataset comprising 4069 historical landslide locations and 11 conditioning factors (altitude, slope degree, profile curvature, distance to river, aspect, plan curvature, distance to road, distance to fault, rainfall, geology and land-sue) to construct a geospatial database and divided the data into the training and the testing dataset. We then developed RNN and CNN algorithms to generate landslide susceptibility maps of Iran using the training dataset. We calculated the receiver operating characteristic (ROC) curve and used the area under the curve (AUC) for the quantitative evaluation of the landslide susceptibility maps using the testing dataset. Better performance in both the training and testing phases was provided by the RNN algorithm (AUC \u200b= \u200b0.88) than by the CNN algorithm (AUC \u200b= \u200b0.85). Finally, we calculated areas of susceptibility for each province and found that 6% and 14% of the land area of Iran is very highly and highly susceptible to future landslide events, respectively, with the highest susceptibility in Chaharmahal and Bakhtiari Province (33.8%). About 31% of cities of Iran are located in areas with high and very high landslide susceptibility. The results of the present study will be useful for the development of landslide hazard mitigation strategies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.853",
        "scimago_value": "1,842"
    },
    {
        "issnkey": "09638687",
        "isbn": null,
        "journal": "The Journal of Strategic Information Systems",
        "publisher": null,
        "title": "thelifecycleofalgorithmicdecisionmakingsystemsorganizationalchoicesandethicalchallenges",
        "booktitle": null,
        "doi": "10.1016/j.jsis.2021.101683",
        "author": [
            "Marabelli, Marco",
            "Newell, Sue",
            "Handunge, Valerie"
        ],
        "keywords": [
            "Algorithmic decision-making systems (ADMS), Strategic choices, Ethical implications, IS strategizing, Automatic systems"
        ],
        "abstract": "In this viewpoint article we discuss algorithmic decision-making systems (ADMS), which we view as organizational sociotechnical systems with their use in practice having consequences within and beyond organizational boundaries. We build a framework that revolves around the ADMS lifecycle and propose that each phase challenges organizations with \u201cchoices\u201d related to technical and processual characteristics \u2013 ways to design, implement and use these systems in practice. We argue that it is important that organizations make these strategic choices with awareness and responsibly, as ADMS\u2019 consequences affect a broad array of stakeholders (the workforce, suppliers, customers and society at-large) and involve ethical considerations. With this article we make two main contributions. First, we identify key choices associated with the design, implementation and use in practice of ADMS in organizations, that build on past literature and are tied to timely industry-related examples. Second, we provide IS scholars with a broad research agenda that will promote the generation of new knowledge and original theorizing within the domain of the strategic applications of ADMS in organizations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "11.022",
        "scimago_value": "3,133"
    },
    {
        "issnkey": "10836160",
        "isbn": null,
        "journal": "Organic Process Research & Development",
        "publisher": null,
        "title": "aperspectiveontheanalyticalchallengesencounteredinhighthroughputexperimentation",
        "booktitle": null,
        "doi": "10.1021/acs.oprd.0c00463",
        "author": [
            "Grainger, Rachel",
            "Whibley, Stuart"
        ],
        "keywords": [
            "analytical, HTE, mass spectrometry, nanoscale, optimization"
        ],
        "abstract": "ABSTRACT High-throughput experimentation (HTE) is a well-established technique used in the pharmaceutical industry to accelerate compound synthesis and route optimization through automated chemical processes. A key part of any HTE workflow is the analytical component, through which the reaction outcome can be determined. The development of new analytical techniques capable of high-throughput data generation from nanoscale chemical reactions has been required to streamline the HTE process and address challenges generated through the recent move to miniaturize synthesis. In this Perspective, we review the currently available state-of-the-art analytical methods, discuss the challenges encountered in high-throughput analysis\u2014with a particular focus on the analysis of nanoscale reactions, and provide a future outlook on potential developments in the field.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "20958099",
        "isbn": null,
        "journal": "Engineering",
        "publisher": null,
        "title": "intelligentironmakingoptimizationserviceonacloudcomputingplatformbydigitaltwin",
        "booktitle": null,
        "doi": "10.1016/j.eng.2021.04.022",
        "author": [
            "Zhou, Heng",
            "Yang, Chunjie",
            "Sun, Youxian"
        ],
        "keywords": [
            "Cloud factory, Blast furnace, Multi-objective optimization, Distributed computation"
        ],
        "abstract": "The shortage of computation methods and storage devices has largely limited the development of multi-objective optimization in industrial processes. To improve the operational levels of the process industries, we propose a multi-objective optimization framework based on cloud services and a cloud distribution system. Real-time data from manufacturing procedures are first temporarily stored in a local database, and then transferred to the relational database in the cloud. Next, a distribution system with elastic compute power is set up for the optimization framework. Finally, a multi-objective optimization model based on deep learning and an evolutionary algorithm is proposed to optimize several conflicting goals of the blast furnace ironmaking process. With the application of this optimization service in a cloud factory, iron production was found to increase by 83.91 t\u2219d\u22121, the coke ratio decreased 13.50 kg\u2219t\u22121, and the silicon content decreased by an average of 0.047%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.553",
        "scimago_value": "1,376"
    },
    {
        "issnkey": "24681253",
        "isbn": null,
        "journal": "The Lancet Gastroenterology & Hepatology",
        "publisher": null,
        "title": "impactofthecovid19pandemiconthedetectionandmanagementofcolorectalcancerinenglandapopulationbasedstudy",
        "booktitle": null,
        "doi": "10.1016/S2468-1253(21)00005-4",
        "author": [
            "Morris, Eva",
            "Goldacre, Raphael",
            "Spata, Enti",
            "Mafham, Marion",
            "Finan, Paul",
            "Shelton, Jon",
            "Richards, Mike",
            "Spencer, Katie",
            "Emberson, Jonathan",
            "Hollings, Sam",
            "Curnow, Paula",
            "Gair, Dominic",
            "Sebag-Montefiore, David",
            "Cunningham, Chris",
            "Rutter, Matthew",
            "Nicholson, Brian",
            "Rashbass, Jem",
            "Landray, Martin",
            "Collins, Rory",
            "Casadei, Barbara",
            "Baigent, Colin"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Summary Background There are concerns that the COVID-19 pandemic has had a negative effect on cancer care but there is little direct evidence to quantify any effect. This study aims to investigate the impact of the COVID-19 pandemic on the detection and management of colorectal cancer in England. Methods Data were extracted from four population-based datasets spanning NHS England (the National Cancer Cancer Waiting Time Monitoring, Monthly Diagnostic, Secondary Uses Service Admitted Patient Care and the National Radiotherapy datasets) for all referrals, colonoscopies, surgical procedures, and courses of rectal radiotherapy from Jan 1, 2019, to Oct 31, 2020, related to colorectal cancer in England. Differences in patterns of care were investigated between 2019 and 2020. Percentage reductions in monthly numbers and proportions were calculated. Findings As compared to the monthly average in 2019, in April, 2020, there was a 63% (95% CI 53\u201371) reduction (from 36 274 to 13 440) in the monthly number of 2-week referrals for suspected cancer and a 92% (95% CI 89\u201395) reduction in the number of colonoscopies (from 46 441 to 3484). Numbers had just recovered by October, 2020. This resulted in a 22% (95% CI 8\u201334) relative reduction in the number of cases referred for treatment (from a monthly average of 2781 in 2019 to 2158 referrals in April, 2020). By October, 2020, the monthly rate had returned to 2019 levels but did not exceed it, suggesting that, from April to October, 2020, over 3500 fewer people had been diagnosed and treated for colorectal cancer in England than would have been expected. There was also a 31% (95% CI 19\u201342) relative reduction in the numbers receiving surgery in April, 2020, and a lower proportion of laparoscopic and a greater proportion of stoma-forming procedures, relative to the monthly average in 2019. By October, 2020, laparoscopic surgery and stoma rates were similar to 2019 levels. For rectal cancer, there was a 44% (95% CI 17\u201376) relative increase in the use of neoadjuvant radiotherapy in April, 2020, relative to the monthly average in 2019, due to greater use of short-course regimens. Although in June, 2020, there was a drop in the use of short-course regimens, rates remained above 2019 levels until October, 2020. Interpretation The COVID-19 pandemic has led to a sustained reduction in the number of people referred, diagnosed, and treated for colorectal cancer. By October, 2020, achievement of care pathway targets had returned to 2019 levels, albeit with smaller volumes of patients and with modifications to usual practice. As pressure grows in the NHS due to the second wave of COVID-19, urgent action is needed to address the growing burden of undetected and untreated colorectal cancer in England. Funding Cancer Research UK, the Medical Research Council, Public Health England, Health Data Research UK, NHS Digital, and the National Institute for Health Research Oxford Biomedical Research Centre.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "4,897"
    },
    {
        "issnkey": "",
        "isbn": "978-0-323-85510-5",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter1advancesinmachinelearninganddataanalytics",
        "booktitle": "Intelligent Data-Analytics for Condition Monitoring",
        "doi": "10.1016/B978-0-323-85510-5.00001-6",
        "author": [
            "Malik, Hasmat",
            "Fatema, Nuzhat",
            "Iqbal, Atif"
        ],
        "keywords": [
            "feature extraction, feature selection, data preprocessing, visualization, condition monitoring, open access, software, dataset sources"
        ],
        "abstract": "Artificial intelligence (AI) is the intelligence demonstrated by the machines. AI is also the representation of a machine (like computer), which imitates cognitive behavior/functions associated to human mind for the purpose of learning and problem solving. Also, the subset of AI is machine learning (ML), which is improved automatically through experience. ML algorithms are developed based on the data without explicit information of the system behavior. Data statistics and computational analysis are the subset of ML. The data analytics is a process of analyzing, cleaning, transforming, and modeling the data with respect to the useful information. In this chapter, detailed information of data analytics of smart grid application, data analytics for business, condition monitoring, data and its relation, data preprocessing, feature extraction, feature selection, and different application areas are studied. A wide list of software and dataset\u2019s digital library are also included.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01419331",
        "isbn": null,
        "journal": "Microprocessors and Microsystems",
        "publisher": null,
        "title": "humanresourcemanagementstructureofcommunicationenterprisebasedonmicroprocessorsystemandembeddednetwork",
        "booktitle": null,
        "doi": "10.1016/j.micpro.2020.103749",
        "author": [
            "He, Ying",
            "Li, Ming"
        ],
        "keywords": [
            "Information systems, Human resource management, HRISHR professionals"
        ],
        "abstract": "In the past few years, human resource management (HRM) has undergone significant changes. The focus from administrative tasks to become strategic partners into the organization's overall strategy, mainly in the field of development of information technology, has given strong support. The technology support using Microprocessor System and Embedded Network to help handle the HRM knowledge process. Long-term use of information systems today have a significant impact on how to manage HRM. The Human Resources (HR) processes and practices within the organization; in other words, the collection of information, storage, use, and sharing method have changed completely. The Microprocessor System using to store the condition base data and then communicate with internet support. Part of the HRM process becomes more efficient; due to these improved service levels, it can now be more participation. Human resources of this new business strategy in business strategy have a significant impact on the human resources function and its experts. This chapter reviews the existing literature on this topic and considers the advantages and benefits of HRM information systems. It also outlines some of the technical applications in the functional areas of HRM in the organization.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.525",
        "scimago_value": "0,323"
    },
    {
        "issnkey": "23524847",
        "isbn": null,
        "journal": "Energy Reports",
        "publisher": null,
        "title": "backwardsimulationoftemperaturechangesofdistrictheatingnetworksforenablingloadinghistoryinpredictivemaintenance",
        "booktitle": null,
        "doi": "10.1016/j.egyr.2021.09.031",
        "author": [
            "{Pourbozorgi Langroudi}, Pakdad",
            "Weidlich, Ingo",
            "Hay, Stefan"
        ],
        "keywords": [
            "Backward simulation, Loading history, Predictive maintenance, Machine learning, Asset management, Artificial neural networks, System reliability, District Heating"
        ],
        "abstract": "District Heating (DH) networks, like most of industries, are in transition to the fourth\u200b industry age and they are retrofitting themselves with different sensing and inspection technologies to enable cyber connectivity for different purposes, such as system optimization, failure detection, maintenance, etc. Since DH pipes show different ageing behaviour under different conditions and initially the pre-insulated bounded pipes had been designed for a minimum of 30 years life span, a long-term loading history is required for predictive maintenance (PdM) purposes and it is necessary to understand the ageing of the DH pipes. These historical temperature changes of the networks are not available for such a long period and they are usually limited to the past few years. To exploit the available implemented technologies for PdM , the missing data must become available to understand the ageing patterns and expand the ageing model to the pipes in use. In this research, various Machine Learning (ML) techniques such as Support Vector Machine (SVM), Random Forest algorithm (RF), Artificial Neural Networks (ANN) have been tested to train a model and backward simulate the temperature changes of the system based on recorded weather data. Various none-temperature variables have been used to enhance the prediction qualities to the real-world data. The historical temperature changes of the system shall be used for different ageing estimation such as fatigue cycles or remaining useful life of the polyurethane (PUR) foam.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.870",
        "scimago_value": "1,199"
    },
    {
        "issnkey": "24058963",
        "isbn": null,
        "journal": "IFAC-PapersOnLine",
        "publisher": null,
        "title": "improveindustrialperformancebasedonsystematicanalysesofmanufacturingdata",
        "booktitle": null,
        "doi": "10.1016/j.ifacol.2021.08.083",
        "author": [
            "Lisboa, M.",
            "Jesus, E.",
            "Seixas, R.",
            "Valle, P.",
            "Deschamps, F.",
            "Strobel, C."
        ],
        "keywords": [
            "Industry 4.0, Industrial Internet of things (IIOT), Data Analytics, Predictive Maintenance"
        ],
        "abstract": "This Article was written based on a systematic review of the literature considering three reference axes, Industry 4.0, Data Analytics and Predictive Maintenance, including specific combination of search terms to ensure a reasonable quantity of articles keeping adherence to the topic that is decision making based on collection and analysis of relevant data on B2B (Business to Business). The study focuses on the area of predictive maintenance, which has been strongly highlighted by the fourth industrial revolution, due to the mechanical and mainly the electronic embedded systems complexity increases in the last decades, as well as network connectivity possibilities for equipment\u2019s data acquisition, enabling technologies for predictive maintenance as a key factor for competitiveness, reducing costs, increasing equipment\u2019s availability, or as a servitization strategy. The article is divided into five parts, starting with the research model and selection of articles, including the proposal for a data analysis framework and the application of systematic analysis of these data, concluding with the opening of a discussion and the indication of future directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,308"
    },
    {
        "issnkey": "01607383",
        "isbn": null,
        "journal": "Annals of Tourism Research",
        "publisher": null,
        "title": "tourismflowsinlargescaledestinationsystems",
        "booktitle": null,
        "doi": "10.1016/j.annals.2020.103113",
        "author": [
            "K\u00e1d\u00e1r, B\u00e1lint",
            "Gede, M\u00e1ty\u00e1s"
        ],
        "keywords": [
            "Tourism networks, Network analysis, Tourist flows, Large-scale destinations, Multi-destination trips, Danube, Flickr analysis"
        ],
        "abstract": "Large-scale destination systems, especially cross-border regions are less studied in literature as their size and transnational nature makes these hard to analyse with traditional methods. Tourism systems like the Danube Region are composed of several local and regional destinations, and even when these are branded together for tourists the integration of these into one system is often compromised by national boundaries and socio-economic differences. This study shows how the Danube region is composed of different clusters of destinations, and how national boundaries have a strong shielding effect in the interregional movements of tourists. A methodology based on network analysis with efficient clustering algorithms applied on large geotagged datasets from User Generated Content is proposed. Flickr data was used to map short time-interval visitor flows along the linear system of the river Danube. 18 regional clusters integrated into 3 strong, but separated destination systems were identified by modularity analysis. The central integrating effect of the large capital cities and the boundary-shielding effect impeding the total integration of this large-scale system were made measurable.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.011",
        "scimago_value": "2,159"
    },
    {
        "issnkey": "09507051",
        "isbn": null,
        "journal": "Knowledge-Based Systems",
        "publisher": null,
        "title": "miningtruckplatooningpatternsthroughmassivetrajectorydata",
        "booktitle": null,
        "doi": "10.1016/j.knosys.2021.106972",
        "author": [
            "Ma, Xiaolei",
            "Huo, Enze",
            "Yu, Haiyang",
            "Li, Honghai"
        ],
        "keywords": [
            "Energy consumption, Trajectory mining, Truck platooning, Spatial clustering, Association rule learning"
        ],
        "abstract": "Truck platooning refers to a series of trucks driving in close proximity via communication technologies, and it is considered one of the most implementable systems of connected and automated vehicles, bringing huge energy savings and safety improvements. Properly planning platoons and evaluating the potential of truck platooning are crucial to trucking companies and transportation authorities. This study proposes a series of data mining approaches to learn spontaneous truck platooning patterns from massive trajectories. An enhanced map matching algorithm is developed to identify truck headings by using digital map data, followed by an adaptive spatial clustering algorithm to detect trucks\u2019 instantaneous co-moving sets. These sets are then aggregated to find the network-wide maximum platoon duration and size through frequent itemset mining for computational efficiency. The GPS data were collected from truck fleeting systems in Liaoning Province, China for platooning performance measures and spatiotemporal platooning distribution visualization. Results show that approximately 36% spontaneous truck platoons can be coordinated by speed adjustment without changing routes and schedules. The average platooning distance and duration ratios for these platooned trucks are 9.6% and 9.9%, respectively, leading to a 2.8% reduction in total fuel consumption. This study also distinguishes the optimal platooning periods and space headways for national freeways and trunk roads, and prioritize the road segments with high possibilities of truck platooning. The derived results are reproducible, providing useful policy implications and operational strategies for large-scale truck platoon planning and roadside infrastructure construction.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.038",
        "scimago_value": "1,587"
    },
    {
        "issnkey": "01663615",
        "isbn": null,
        "journal": "Computers in Industry",
        "publisher": null,
        "title": "analyticsenabledescalationmanagementsystemdevelopmentandbusinessvalueassessment",
        "booktitle": null,
        "doi": "10.1016/j.compind.2021.103481",
        "author": [
            "Oberdorf, Felix",
            "Stein, Nikolai",
            "Flath, Christoph"
        ],
        "keywords": [
            "Escalation management, Industry 4.0, Machine learning, Business analytics"
        ],
        "abstract": "Industry 4.0 initiatives can help traditional manufacturing industry cope with increasing global competition. Such solutions facilitate transparency, automation as well as business process transformation. This paper elaborates on a collaboration with a medium-sized manufacturing company. We highlight the design, evaluation and roll-out of an escalation management system with integrated data-driven decision support. We do so by applying an action design research process. Thereby, our study focuses on the system design concerning the creation of business value. The system leverages state-of-the-art machine learning algorithms for disruption type classification and escalation handling duration prediction. These predictions can be embedded in an integrated planning procedure leveraging diverse organizational data sources (e.g., personnel availability, production plans) to instantiate a prescriptive analytics solution. Combined with informative analytics insights, this allows the proposed system to generate significant business value by reducing escalation durations. In the long run, the transformational business value enabled by the system is likely to exceed the automational business value. This highlights the special importance of tight integration of industrial analytics applications within business processes.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.635",
        "scimago_value": "1,432"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "towardsadatacentricarchitectureintheautomotiveindustry",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.01.215",
        "author": [
            "Alvarez-Coello, Daniel",
            "Wilms, Daniel",
            "Bekan, Adnan",
            "{Marx G\u00f3mez}, Jorge"
        ],
        "keywords": [
            "Connected vehicles, data-centric architecture, standardized data, semantic AI, modern data architecture"
        ],
        "abstract": "Vehicle software architectures have been evolving over the last twenty years to support data-driven functionalities. Several enterprises from different domains are currently focusing on improving their data architectures by re-defining the underlying data models to enable core support for analytics and artificial intelligence. Moreover, a common desire to add clear data provenance and explicit context impulses the field of semantics and knowledge graphs. Nevertheless, in the automotive industry, the scenario of connected vehicles implies extra complexity. Vehicle data has an enormous variety, making it essential to develop and adopt standards. This paper presents aspects of ongoing research at the BMW Research Department regarding a conceptual design for vehicle software architectures in the automotive industry. We discuss the principles of a modern data architecture with particular emphasis on the data-centric mindset. We also explore the current challenges and possible working points as the foundation to move from siloed data towards a so-called AI factory.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "05848547",
        "isbn": null,
        "journal": "Spectrochimica Acta Part B: Atomic Spectroscopy",
        "publisher": null,
        "title": "areviewofartificialneuralnetworkbasedchemometricsappliedinlaserinducedbreakdownspectroscopyanalysis",
        "booktitle": null,
        "doi": "10.1016/j.sab.2021.106183",
        "author": [
            "Li, Lu-Ning",
            "Liu, Xiang-Feng",
            "Yang, Fan",
            "Xu, Wei-Ming",
            "Wang, Jian-Yu",
            "Shu, Rong"
        ],
        "keywords": [
            "Laser-induced breakdown spectroscopy, Artificial neural network, Machine learning, Chemometrics, Classification"
        ],
        "abstract": "In the past decades various categories of chemometrics for laser-induced breakdown spectroscopy (LIBS) analysis have been developed, among which an important category is that based on artificial neural network (ANN). The most common ANN scheme employed in LIBS researches so far is back-propagation neural network (BPNN), while there are also several other kinds of neural networks appreciated by the LIBS community, including radial basis function neural network (RBFNN), convolutional neural network (CNN), self-organizing map (SOM), etc. In this paper, we introduce the principles of some representative ANN methods, and offer criticism on their features along with comparison between them. Then we afford an overview of ANN-based chemometrics applied in LIBS analysis, involving material identification/classification, component concentration quantification, and some unconventional applications as well. Furthermore, a comprehensive discussion on ANN-LIBS methodologies is provided from four aspects. First, a few general progressing trends are displayed. Next we expound some specific implementation techniques, including variable selection, network construction, data set utilization, network training, model evaluation, and chemometrics selection. In addition, the limitations of ANN approaches are remarked, mainly concerning overfitting and interpretability. Finally a prospect of future development of ANN-LIBS chemometrics is presented. Throughout the discussion quite a few good practices have been highlighted. This review is expected to shed light on the further upgrade of ANN-based LIBS chemometrics in the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.752",
        "scimago_value": "0,793"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820239-5",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter1historycurrentstatusandfuturedirectionsofartificialintelligence",
        "booktitle": "Precision Medicine and Artificial Intelligence",
        "doi": "10.1016/B978-0-12-820239-5.00002-4",
        "author": [
            "Kubassova, Olga",
            "Shaikh, Faiq",
            "Melus, Carlos",
            "Mahler, Michael"
        ],
        "keywords": [
            "Artificial intelligence, Neural networks, Rheumatoid arthritis, Fatty liver disease, Electrocardiography, Blockchain technology"
        ],
        "abstract": "Artificial intelligence (AI) as a technology concept is making a major impact on a wide range of industries and sectors. This is largely attributed to technical advancements in machine and especially deep learning methodologies fueled by improved computational capabilities which have led to sophisticated approaches in applying AI to various scenarios. These AI applications aim to improve productivity, decrease cost, and comprehend the ever-increasing volumes of data available to ultimately provide actionable insights. Included in this paradigm shift is medicine, where AI is beginning to enable clinical assistance, decision support, improved management and accelerated scientific discovery and development.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "14740346",
        "isbn": null,
        "journal": "Advanced Engineering Informatics",
        "publisher": null,
        "title": "transdisciplinarysystemsapproachtorealizationofdigitaltransformation",
        "booktitle": null,
        "doi": "10.1016/j.aei.2021.101316",
        "author": [
            "Hashmi, Muhammad",
            "Mo, John",
            "Beckett, Ronald"
        ],
        "keywords": [
            "Transdisciplinary engineering, System of systems, Cyber physical systems, Data analytics, Work 4.0, Data integrity"
        ],
        "abstract": "With advancement in technology and emergence of fast networks, operation of businesses and global companies increasingly depend on the Internet and digital transformation of their infrastructures. Adaptation to Industry 4.0 paradigm gives rise to societal, technological and communication issues due to challenges in product, services, social and inter-disciplinary interactions. A more fundamental approach that can mitigate complexity of the new business is necessary. This paper adopts a system of systems model embedded into a transdisciplinary system design to describe a typical X4.0 system where X can be any industry sector migrating to Industry 4.0 paradigm. Two industry sectors: Education 4.0 and Retail 4.0 are studied under the amalgamated transdisciplinary system of systems model. Results show that the four artifacts in X4.0 can form the foundation of new sectorial 4.0 development with focus on specific elements in Cyber Physical Systems and Work4.0 artifacts. The transdisciplinary system approach has the advantage of a self-improving model that drives realization of digital transformation in evolutionary cycles.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.603",
        "scimago_value": "1,107"
    },
    {
        "issnkey": "00917435",
        "isbn": null,
        "journal": "Preventive Medicine",
        "publisher": null,
        "title": "personalizedmobiletechnologiesforlifestylebehaviorchangeasystematicreviewmetaanalysisandmetaregression",
        "booktitle": null,
        "doi": "10.1016/j.ypmed.2021.106532",
        "author": [
            "Tong, Huong",
            "Quiroz, Juan",
            "Kocaballi, A.",
            "Fat, Sandrine",
            "Dao, Kim",
            "Gehringer, Holly",
            "Chow, Clara",
            "Laranjo, Liliana"
        ],
        "keywords": [
            "Mobile applications[MeSH], Fitness trackers[MeSH], Personalization, Tailoring, Health behavior[MeSH]"
        ],
        "abstract": "Given that the one-size-fits-all approach to mobile health interventions have limited effects, a personalized approach might be necessary to promote healthy behaviors and prevent chronic conditions. Our systematic review aims to evaluate the effectiveness of personalized mobile interventions on lifestyle behaviors (i.e., physical activity, diet, smoking and alcohol consumption), and identify the effective key features of such interventions. We included any experimental trials that tested a personalized mobile app or fitness tracker and reported any lifestyle behavior measures. We conducted a narrative synthesis for all studies, and a meta-analysis of randomized controlled trials. Thirty-nine articles describing 31 interventions were included (n = 77,243, 64% women). All interventions personalized content and rarely personalized other features. Source of data included system-captured (12 interventions), user-reported (11 interventions) or both (8 interventions). The meta-analysis showed a moderate positive effect on lifestyle behavior outcomes (standardized difference in means [SDM] 0.663, 95% CI 0.228 to 1.10). A meta-regression model including source of data found that interventions that used system-captured data for personalization were associated with higher effectiveness than those that used user-reported data (SDM 1.48, 95% CI 0.76 to 2.19). In summary, the field is in its infancy, with preliminary evidence of the potential efficacy of personalization in improving lifestyle behaviors. Source of data for personalization might be important in determining intervention effectiveness. To fully exploit the potential of personalization, future high-quality studies should investigate the integration of multiple data from different sources and include personalized features other than content.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01679236",
        "isbn": null,
        "journal": "Decision Support Systems",
        "publisher": null,
        "title": "neighborawarereviewhelpfulnessprediction",
        "booktitle": null,
        "doi": "10.1016/j.dss.2021.113581",
        "author": [
            "Du, Jiahua",
            "Rong, Jia",
            "Wang, Hua",
            "Zhang, Yanchun"
        ],
        "keywords": [
            "Review helpfulness, Sequential bias, Review neighbors, Context clues, Deep learning"
        ],
        "abstract": "Helpfulness prediction techniques have been widely incorporated into online decision support systems to identify high-quality reviews. Most current studies on helpfulness prediction assume that a review's helpfulness only relies on information from itself. In practice, however, consumers hardly process reviews independently because reviews are displayed in sequence; a review is more likely to be affected by its adjacent neighbors in the sequence, which is largely understudied. In this paper, we proposed the first end-to-end neural architecture to capture the missing interaction between reviews and their neighbors. Our model allows for a total of 12 (three selection \u00d7 four aggregation) schemes that contextualize a review into the context clues learned from its neighbors. We evaluated our model on six domains of real-world online reviews against a series of state-of-the-art baselines. Experimental results confirm the influence of sequential neighbors on reviews and show that our model significantly outperforms the baselines by 1% to 5%. We further revealed how reviews are influenced by their neighbors during helpfulness perception via extensive analysis. The results and findings of our work provide theoretical contributions to the field of review helpfulness prediction and offer insights into practical decision support system design.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.795",
        "scimago_value": "1,564"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "astackbasedsetinversionmodelforsmartwatercarbonandecologicalassessmentinurbanagglomerations",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.128665",
        "author": [
            "Yan, Pengdong",
            "Lu, Hongwei",
            "Chen, Yizhong",
            "Li, Ziheng",
            "Li, Hao"
        ],
        "keywords": [
            "Water, Carbon and ecological footprints, Smart evaluation and prediction, Ensemble inversion model, Urban agglomeration, Yangtze river"
        ],
        "abstract": "Footprint evaluation is an important tool for assessing the appropriation of ecological assets, GHG emissions, freshwater consumption parameters, etc., within a specified region. However, traditional evaluation of footprints for mega cities or urban agglomerations requires overmuch different types of high-quality data. There is a great need of seeking a smart model/approach with declined data requirements for evaluation of footprints where part of data can hardly be accessed. Here we propose a new ensemble inversion model (EIM) based on integrated multitask machine learning (MML) and multi-modeling stacking (MMS) algorithms for smart evaluation and prediction of water, carbon and ecological footprints. The accuracy and generalization capability of the model are illustrated through three largest urban agglomerations in the middle reaches of the Yangtze River (MRYR). The testing results show that the EIM achieves similar prediction performance compared to traditional footprints calculation methods (R2 = 0.91, RMSE = 0.18, MAE = 0.11), yet greatly reduces the amount of required data by approximately 80%. Moreover, the accuracy of the EIM is improved by more than 20%, compared with other models using a single inversion algorithm. The modeling results also show that 1) water, carbon and ecological footprints are significantly positively correlated, and 2) an annual increase of 4.8% can be found in terms of the urban environmental pressure index (UEPI), and its projection is even less optimistic for the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "1556407x",
        "isbn": null,
        "journal": "Sleep Medicine Clinics",
        "publisher": null,
        "title": "thefutureofsleepmeasurementsareviewandperspective",
        "booktitle": null,
        "doi": "10.1016/j.jsmc.2021.05.004",
        "author": [
            "Arnardottir, Erna",
            "Islind, Anna",
            "\u00d3skarsd\u00f3ttir, Mar\u00eda"
        ],
        "keywords": [
            "Sleep measurement, Subjective data, Objective data, Sleep diary, Codesign, Machine learning, Data management platform, Data science"
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10534822",
        "isbn": null,
        "journal": "Human Resource Management Review",
        "publisher": null,
        "title": "aiaugmentedhrmantecedentsassimilationandmultilevelconsequences",
        "booktitle": null,
        "doi": "10.1016/j.hrmr.2021.100860",
        "author": [
            "Prikshat, Verma",
            "Malik, Ashish",
            "Budhwar, Pawan"
        ],
        "keywords": [
            "Technology-driven HRM, AI-adoption in HRM, AI-augmented HRM, Processual factors"
        ],
        "abstract": "The current literature on the use of disruptive innovative technologies, such as artificial intelligence (AI) for human resource management (HRM) function, lacks a theoretical basis for understanding. Further, the adoption and implementation of AI-augmented HRM, which holds promise for delivering several operational, relational and transformational benefits, is at best patchy and incomplete. Integrating the technology, organisation and people (TOP) framework with core elements of the theory of innovation assimilation and its impact on a range of AI-Augmented HRM outcomes, or what we refer to as (HRM(AI)), this paper develops a coherent and integrated theoretical framework of HRM(AI) assimilation. Such a framework is timely as several post-adoption challenges, such as the dark side of processual factors in innovation assimilation and system-level factors, which, if unattended, can lead to the opacity of AI applications, thereby affecting the success of any HRM(AI). Our model proposes several testable future research propositions for advancing scholarship in this area. We conclude with implications for theory and practice.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.444",
        "scimago_value": "2,549"
    },
    {
        "issnkey": "22120963",
        "isbn": null,
        "journal": "Climate Risk Management",
        "publisher": null,
        "title": "mobilephonetechnologiesfordisasterriskreduction",
        "booktitle": null,
        "doi": "10.1016/j.crm.2021.100296",
        "author": [
            "Paul, Jonathan",
            "Bee, Emma",
            "Budimir, Mirianna"
        ],
        "keywords": [
            "Citizen science, Disaster risk management (DRM), Disaster risk reduction (DRR), Mobile phone technologies, Natural hazards, User-centered design (UCD)"
        ],
        "abstract": "The explosion of increasingly sophisticated mobile phone technologies can usefully be harnessed by disaster risk reduction (DRR) as a means of enhancing inclusivity and local relevance of knowledge production and resilience building. However, much new technology is designed on an ad hoc basis without considering user needs \u2013 especially mobile applications (apps), which often terminate at the proof-of-concept stage. Here, we examine best practice by marshalling learnings from 45 workers representing 20 organisations working globally across the disaster risk management (DRM) lifecycle, including physical and social science, NGOs, technological developers, and (inter)governmental regulatory bodies. We present a series of generalisable and scalable guidelines that are novel in being independent of any specific natural hazard or development setting, designed to maximise the positive societal impact of exploiting mobile technologies. Specifically, the local context, dynamics, and needs must be carefully interrogated a priori, while any product should ideally be co-developed with local stakeholders through a user-centered design approach.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.090",
        "scimago_value": "1,846"
    },
    {
        "issnkey": "02648377",
        "isbn": null,
        "journal": "Land Use Policy",
        "publisher": null,
        "title": "digitaltransformationoftheagrifoodsystemquantifyingtheconditioningfactorstoinformpolicyplanningintheolivesector",
        "booktitle": null,
        "doi": "10.1016/j.landusepol.2021.105537",
        "author": [
            "Parra-L\u00f3pez, Carlos",
            "Reina-Usuga, Liliana",
            "Carmona-Torres, Carmen",
            "Sayadi, Samir",
            "Klerkx, Laurens"
        ],
        "keywords": [
            "Digitalisation, Olive, SWOT, PESTLE, AHP, TOWS"
        ],
        "abstract": "Despite the growing importance of the digital transformation (DT) of the agrifood sector on the political agenda, traditional policies are not enough to provide proactive responses to rapid technological changes and new approaches for policy planning are necessary especially at regional level. This manuscript proposes and illustrates the implementation of a new methodological framework for DT policy planning in the case of Andalusia, the olive world leader region, but applicable to other regions and sectors, with two objectives: 1) to quantitatively determine the importance of the conditioning factors of DT in the olive sector in the short/medium term, by developing an AHP/SWOT/PESTLE model, and 2) to design public policies to strengthen the DT, taking advantage of the potentialities and alleviating the deficiencies, by carrying out a quantitative TOWS analysis. The knowledge of diverse groups of experts, i.e. stakeholders in the sector, has been used in all analyses due to the lack of reliable data and the complex nature of the issues analysed. The results show that the opportunities and strengths are more prominent than weaknesses and threats for DT. Environmental issues stand out as an opportunity to boost DT. There is also a growing interest in developing an interoperability strategy which is an opportunity to overcome the low technological integration of the value chain. DT can also enable a more transparent value chain and improved traceability. Some negative factors are the lack of evidence on the economic viability of investment in digital technologies, shortage of labour and young farmers, and potential unintended and unanticipated effects of DT. Important policies strategies to foster DT are: improving environmental efficiency though DT; promoting youth employment in the sector; enhancing coordination among innovation actors; developing a common interoperability strategy; and fostering technological integration in the sector.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.398",
        "scimago_value": "1,668"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "acombinationofqueryexpansionrankingandgasvmforimprovingindonesiansentimentclassificationperformance",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.05.074",
        "author": [
            "Prastyo, Pulung",
            "Ardiyanto, Igi",
            "Hidayat, Risanuri"
        ],
        "keywords": [
            "Query Expansion Ranking, Genetic Algorithm, Feature Selection, Machine Learning, Sentiment Classification"
        ],
        "abstract": "The sentiment classification method is a research field that is proliferating in Indonesia since it is fast in extracting public opinion and provides essential and valuable information for stakeholders. Of the best-performing sentiment classification approaches, machine learning is one of them that has excellent performance. However, the method has several problems, such as noisy features and high dimensionality of features that significantly affect the sentiment classification performance. Therefore, to overcome the problems, this paper presents a novel feature selection using a combination of Query Expansion Ranking (QER) and Genetic Algorithm-Support Vector Machine (GA-SVM) for improving sentiment classification performance. Based on the experimental results, the proposed method could significantly improve sentiment classification performance, outperform all state-of-the-art algorithms, and decrease computational time. The method achieved the best performance in average precision, recall, and f-measure with the value of 96.78%, 96.76%, and 96.75%, respectively.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "dataanalyticsinaprivacyconcernedworld",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2019.05.005",
        "author": [
            "Wieringa, Jaap",
            "Kannan, P.K.",
            "Ma, Xiao",
            "Reutterer, Thomas",
            "Risselada, Hans",
            "Skiera, Bernd"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Data is considered the new oil of the economy, but privacy concerns limit their use, leading to a widespread sense that data analytics and privacy are contradictory. Yet such a view is too narrow, because firms can implement a wide range of methods that satisfy different degrees of privacy and still enable them to leverage varied data analytics methods. Therefore, the current study specifies different functions related to data analytics and privacy (i.e., data collection, storage, verification, analytics, and dissemination of insights), compares how these functions might be performed at different levels (consumer, intermediary, and firm), outlines how well different analytics methods address consumer privacy, and draws several conclusions, along with future research directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820273-9",
        "journal": null,
        "publisher": "Academic Press",
        "title": "index",
        "booktitle": "Machine Learning in Cardiovascular Medicine",
        "doi": "10.1016/B978-0-12-820273-9.20001-8",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22120955",
        "isbn": null,
        "journal": "Urban Climate",
        "publisher": null,
        "title": "multioutputtcnautoencoderforlongtermpollutionforecastingformultiplesites",
        "booktitle": null,
        "doi": "10.1016/j.uclim.2021.100943",
        "author": [
            "Samal, K.",
            "Panda, Ankit",
            "Babu, Korra",
            "Das, Santos"
        ],
        "keywords": [
            "Temporal convolutional network, Spatio-temporal prediction, Autoencoder, Deep learning, Pollution"
        ],
        "abstract": "Air pollution is one of the major environmental issues attracting massive attention from researchers and policymakers. Both the developed and developing countries are undergoing a high concentration of pollution levels. Fine particulate matter PM2.5 (particles having a diameter less than 2.5\u03bcm) and PM10 (particles having a diameter less than 10\u03bcm) can easily penetrate the lungs and respiratory system and causes adverse health issues like heart attacks, cardiovascular diseases, lung function reduction. Real-time pollutant information is of great importance to providing prompt and complete information on air quality. Air pollution forecasting is another significant step of air pollution management, which can help policymakers and citizens make proper decisions to prevent air pollution-related diseases. This research study explores a novel pollutant forecasting model named as Multi-output temporal convolutional network autoencoder (MO-TCNA). The model accumulates each step's predicted values to perform multi-step ahead long-term forecasting for multiple pollutants and multiple sites in a single training model. The MO-TCNA network serves both the PM2.5 and PM10 pollutants forecasting for various locations instead of performing single output and site-specific pollutant forecasting. Consequently, the experimental results show that the MO-TCNA network is time-saving and has better performance than the traditional site-specific forecasting models.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.731",
        "scimago_value": "1,151"
    },
    {
        "issnkey": "00104825",
        "isbn": null,
        "journal": "Computers in Biology and Medicine",
        "publisher": null,
        "title": "precisionhealthdatarequirementschallengesandexistingtechniquesfordatasecurityandprivacy",
        "booktitle": null,
        "doi": "10.1016/j.compbiomed.2020.104130",
        "author": [
            "Thapa, Chandra",
            "Camtepe, Seyit"
        ],
        "keywords": [
            "Precision health, Legal requirements, Ethical guidelines, Security, Privacy, Artificial intelligence"
        ],
        "abstract": "Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical conditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Besides, the public, who is the data source, always expects the security, privacy, and trust of their data. Otherwise, they can avoid contributing their data to the precision health system. Consequently, as the public is the targeted beneficiary of the system, the effectiveness of precision health diminishes. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it illustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.589",
        "scimago_value": "0,884"
    },
    {
        "issnkey": "03787788",
        "isbn": null,
        "journal": "Energy and Buildings",
        "publisher": null,
        "title": "reviewofurbanbuildingenergymodelingubemapproachesmethodsandtoolsusingqualitativeandquantitativeanalysis",
        "booktitle": null,
        "doi": "10.1016/j.enbuild.2021.111073",
        "author": [
            "Ali, Usman",
            "Shamsi, Mohammad",
            "Hoare, Cathal",
            "Mangina, Eleni",
            "O\u2019Donnell, James"
        ],
        "keywords": [
            "Urban building energy modeling, Top-down, Bottom-up, Data-driven, Energy modeling, UBEM, Energy efficiency, SWOT"
        ],
        "abstract": "The world has witnessed a significant population shift to urban areas over the past few decades. Urban areas account for about two-thirds of the world\u2019s total primary energy consumption, of which the urban building sector constitutes a significant proportion approximately 40%. Stakeholders such as urban planners and policy makers face substantial challenges when targeting sustainable energy and climate goals related to the buildings\u2019 sector, i.e. to reduce energy use and associated emissions. Urban energy modeling is one possible solution that leverages limited resources to estimate building energy use and support appropriate policy formation. Over the past few years, there have been only a few review studies on urban building energy modeling approaches. These studies lack an in-depth discussion of the challenges and future research opportunities related to data-driven, reduced-order, and simulation-based modeling methods. This paper proposes Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis of approaches, methods and tools used for urban building energy modeling. Furthermore, this paper proposes a generalized framework based on existing literature for different urban energy modeling methods. The aim of this study is to assist urban planners and energy policymakers when choosing appropriate methods to develop and implement in-depth sustainable building energy planning and analysis projects based on limited available resources.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.879",
        "scimago_value": "1,737"
    },
    {
        "issnkey": "00032697",
        "isbn": null,
        "journal": "Analytical Biochemistry",
        "publisher": null,
        "title": "establishmentofbioinformaticspipelinefordecipheringthebiologicalcomplexitiesoffragmentedspermtranscriptome",
        "booktitle": null,
        "doi": "10.1016/j.ab.2021.114141",
        "author": [
            "Ramya, Laxman",
            "Swathi, Divakar",
            "Archana, Santhanahalli",
            "Lavanya, Maharajan",
            "Parthipan, Sivashanmugam",
            "Selvaraju, Sellappan"
        ],
        "keywords": [
            "Bioinformatics pipeline, Fragmented transcripts, Transcriptomics, Differential gene expression, Bovine spermatozoa"
        ],
        "abstract": "Despite the development of several tools for the analysis of the transcriptome data, non-availability of a standard pipeline for analyzing the low quality and fragmented mRNA samples pose a major challenge to the computational molecular biologist for effective interpretation of the data. Hence the present study aimed to establish a bioinformatics pipeline for analyzing the biologically fragmented sperm RNA. Sperm transcriptome data (2 x 75 PE sequencing) generated from bulls (n = 8) of high-fertile (n = 4) and low-fertile (n = 4) classified based on the fertility rate (41.52 \u00b1 1.07 vs 36.04 \u00b1 1.04%) were analyzed with different bioinformatics tools for alignment, quantitation, and differential gene expression studies. TopHat2 was effectual compared to HISAT2 and STAR for sperm mRNA due to the higher exonic (6% vs 2%) mapping percentage and quantitating the low expressed genes. TopHat2 also had significantly strong correlation with STAR (0.871, p = 0.05) and HISAT2 (0.933, p = 0.01). TopHat2 and Cufflinks combo quantitated the number of genes higher than the other combinations. Among the tools (Cuffdiff, DESeq, DESeq2, edgeR, and limma) used for the differential gene expression analysis, edgeR and limma identified the largest number of significantly differentially expressed genes (p < 0.05) with biological relevance. The concordance analysis concurred that edgeR had an edge over the other tools. It also identified a higher number (9.5%) of fertility-related genes to be differentially expressed between the two groups. The present study established that TopHat2, Cufflinks, and edgeR as a suitable pipeline for the analysis of fragmented mRNA from bovine spermatozoa.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "07430167",
        "isbn": null,
        "journal": "Journal of Rural Studies",
        "publisher": null,
        "title": "howdigitalisationinteractswithecologisationperspectivesfromactorsofthefrenchagriculturalinnovationsystem",
        "booktitle": null,
        "doi": "10.1016/j.jrurstud.2021.07.023",
        "author": [
            "Schnebelin, \u00c9l\u00e9onore",
            "Labarthe, Pierre",
            "Touzard, Jean-Marc"
        ],
        "keywords": [
            "Digitalisation, Agriculture, Digital technology, Agricultural innovation system, Organic farming, Institutional economics, Ecological transition"
        ],
        "abstract": "Two major agricultural transformations are currently being promoted worldwide: digitalisation and ecologisation, that include different practices such as organic farming and sustainable intensification. In literature and in societal debates, these two transformations are sometimes described as antagonistic and sometimes as convergent but are rarely studied together. Using an innovation system approach, this paper discusses how diverse ecologisation pathways grasp digitalisation in the French agricultural sector; and do not discriminate against organic farming. Based on interviews with key representatives of conventional agriculture, organic agriculture and organisations that promote or develop digital agriculture, we explore how these actors perceive and participate in digital development in agriculture. We show that although all the actors are interested and involved in digital development, behind this apparent convergence, organic and conventional actors perceive neither the same benefits nor the same risks and consequently do not implement the same innovation processes. We conclude that digitalisation has different meanings depending on the actors\u2019 paradigm, but that digital actors fail to perceive these differences. This difference in perception should be taken into account if digital development is to benefit all kinds of agriculture and not discriminate against organic farming and more widely, against agroecology.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.849",
        "scimago_value": "1,497"
    },
    {
        "issnkey": "13601385",
        "isbn": null,
        "journal": "Trends in Plant Science",
        "publisher": null,
        "title": "challengesandopportunitiesinmachineaugmentedplantstressphenotyping",
        "booktitle": null,
        "doi": "10.1016/j.tplants.2020.07.010",
        "author": [
            "Singh, Arti",
            "Jones, Sarah",
            "Ganapathysubramanian, Baskar",
            "Sarkar, Soumik",
            "Mueller, Daren",
            "Sandhu, Kulbir",
            "Nagasubramanian, Koushik"
        ],
        "keywords": [
            "image-based phenotyping, machine learning, deep learning, biotic stress, abiotic stress, standard area diagram"
        ],
        "abstract": "Plant stress phenotyping is essential to select stress-resistant varieties and develop better stress-management strategies. Standardization of visual assessments and deployment of imaging techniques have improved the accuracy and reliability of stress assessment in comparison with unaided visual measurement. The growing capabilities of machine learning (ML) methods in conjunction with image-based phenotyping can extract new insights from curated, annotated, and high-dimensional datasets across varied crops and stresses. We propose an overarching strategy for utilizing ML techniques that methodically enables the application of plant stress phenotyping at multiple scales across different types of stresses, program goals, and environments.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "18.313",
        "scimago_value": "4,587"
    },
    {
        "issnkey": "23519894",
        "isbn": null,
        "journal": "Global Ecology and Conservation",
        "publisher": null,
        "title": "differentresponseofalpinemeadowandalpinesteppetoclimaticandanthropogenicdisturbanceontheqinghaitibetanplateau",
        "booktitle": null,
        "doi": "10.1016/j.gecco.2021.e01512",
        "author": [
            "Hao, Aihua",
            "Duan, Hanchen",
            "Wang, Xufeng",
            "Zhao, Guohui",
            "You, Quangang",
            "Peng, Fei",
            "Du, Heqiang",
            "Liu, Feiyao",
            "Li, Chengyang",
            "Lai, Chimin",
            "Xue, Xian"
        ],
        "keywords": [
            "NDVI, Vegetation variation, Climate change, Anthropogenic disturbance, Qinghai-Tibetan Plateau"
        ],
        "abstract": "Climate change and anthropogenic disturbance are two main drivers for vegetation dynamics on the Qinghai-Tibetan Plateau (QTP). Alpine meadow and alpine steppe are the primary rangeland ecosystem types on the QTP. However, the vegetation trends of the two land cover types and the underlying mechanisms behind their variation remain under debate. In this study, we used Global Inventory Modeling and Mapping Studies (GIMMS) 3g Normalized Difference Vegetation Index (NDVI) (i.e., GIMMS NDVI3g) by coupling the Breaks for Additive Season and Trend (BFAST) model and the Boosted Regression Tree (BRT) model to analyze alpine meadow and alpine steppe vegetation trends on the QTP between 1982 and 2015. We also assessed vegetation variation response to climatic and anthropogenic indicators in conjunction with climatic and human footprint datasets. Results show that growing season NDVI (GSNDVI) values increased overall for both alpine meadow (0.0001 year\u22121, p = 0.33) and alpine steppe (0.0002 year\u22121, p < 0.05) throughout 1982\u20132015. Significant greening trends in both alpine meadow (0.0007 year\u22121; p < 0.05) and alpine steppe (0.0005 year\u22121; p < 0.05) ecosystems were obtained before 1998 and 2001, respectively. However, browning trends ascertained by GSNDVI (\u22120.0006 year\u22121; p = 0.12) in alpine meadows were observed throughout 1998\u20132015, while greening trends ascertained by GSNDVI (0.0002 year\u22121; p = 0.12) in alpine steppes were observed throughout during 2001\u20132015. Opposing trends in precipitation, solar radiation, and the Standardized Precipitation Evapotranspiration Index (SPEI) occurred before and after breakpoints in both ecosystems. For the alpine meadow ecosystem, adverse precipitation trends caused browning before 1998 followed by greening after 1998 in the Three-River-Source National Park (TNP). Conversely, opposing changes in precipitation, solar radiation, and SPEI resulted in greening before 1998 followed by browning after 1998 in southern Tibet and the southeastern QTP. Alpine meadow vegetation trends were generally dominated by solar radiation before 1998 and jointly by precipitation and solar radiation after 1998. Prior to 2001 variation in alpine steppe greenness was controlled by precipitation, while after 2001 solar radiation dominated. Along with an increase in human footprint pressure (HFP) gradients, greenness trends gradually increased before 1998 but reversed after 1998 in the alpine meadow ecosystem. Additionally, greenness trends gradually decreased before 2001 but remained unchanged after 2001 for the alpine steppe ecosystem. These results highlight the different effects that climate change and anthropogenic disturbances have had on alpine meadow and alpine steppe ecosystems on the QTP over different time frames.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.380",
        "scimago_value": "1,133"
    },
    {
        "issnkey": "10848045",
        "isbn": null,
        "journal": "Journal of Network and Computer Applications",
        "publisher": null,
        "title": "iotbasedtelemedicinefordiseasepreventionandhealthpromotionstateoftheart",
        "booktitle": null,
        "doi": "10.1016/j.jnca.2020.102873",
        "author": [
            "Albahri, A.S.",
            "Alwan, Jwan",
            "Taha, Zahraa",
            "Ismail, Sura",
            "Hamid, Rula",
            "Zaidan, A.A.",
            "Albahri, O.S.",
            "Zaidan, B.B.",
            "Alamoodi, A.H.",
            "Alsalem, M.A."
        ],
        "keywords": [
            "Telemedicine, Remote monitoring, Healthcare services, Diseases, Internet of things, Network"
        ],
        "abstract": "Numerous studies have focused on making telemedicine smart through the Internet of Things (IoT) technology. These works span a wide range of research areas to enhance telemedicine architecture such as network communications, artificial intelligence methods and techniques, IoT wearable sensors and hardware devices, smartphones and cloud computing. Accordingly, several telemedicine applications covering various human diseases have presented their works from a specific perspective and resulted in confusion regarding the IoT characteristics. Although such applications are useful and necessary for improving telemedicine contexts related to monitoring, detection and diagnostics, deriving an overall picture of how IoT characteristics are currently integrated with the telemedicine architecture is difficult. Accordingly, this study complements the academic literature with a systematic review covering all main aspects of advances in IoT-based telemedicine architecture. This study also provides a state-of-the-art telemedicine classification taxonomy under IoT and reviews works in different fields in relation to that classification. To this end, this study checked the ScienceDirect, Institute of Electrical and Electronics Engineers (IEEE) Xplore, and Web of Science databases. A total of 2121 papers were collected from 2014 to July 2020. The retrieved articles were filtered according to the defined inclusion criteria. A final set of 141 articles were selected and classified into two categories, each followed by subcategories and sections. The first category includes an IoT-based telemedicine network that accounts for 24.11% (n = 34/141). The second category includes IoT-based telemedicine healthcare services and applications that account for 75.89% (n = 107/141). This multi-field systematic review has exposed new research opportunities, motivations, recommendations and challenges that need attention for the synergistic integration of interdisciplinary works. This extensive study also lists a set of open issues and provides innovative key solutions along with a systematic review. The classification of diseases under IoT-based telemedicine is divided into 14 groups. Furthermore, the crossover in our taxonomy is demonstrated. The lifecycle of the context of IoT-based telemedicine healthcare applications is mapped for the first time, including the procedure sequencing and definition for each context. We believe that this study is a useful guide for researchers and practitioners in providing direction and valuable information for future research. This study can also address the ambiguity in the trends in IoT-based telemedicine.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00014575",
        "isbn": null,
        "journal": "Accident Analysis & Prevention",
        "publisher": null,
        "title": "acomprehensivestudyofmacrofactorsrelatedtotrafficfatalityratesbyxgboostbasedmodelandgistechniques",
        "booktitle": null,
        "doi": "10.1016/j.aap.2021.106431",
        "author": [
            "Jiang, Feifeng",
            "Ma, Jun"
        ],
        "keywords": [
            "Traffic fatality rates, Macro factors, National scale, XGBoost, GIS, Feature importance"
        ],
        "abstract": "With the fast development of economics, road safety is becoming a serious problem. Exploring macro factors is effective to improve road safety. However, the existing studies have some limitations: (1) The existing studies only considered one aspect of macro factors and constructed models based on a few data samples. (2) The methods commonly used cannot address the non-linear relationship or calculate the feature importance. The findings obtained from such models may be limited and biased. To address the limitations, this study proposes a BO-CV-XGBoost framework to explore the macro factors related to traffic fatality rate classes based on a high-dimensional dataset that fully considers the impact of multi-factor interaction with adequate data samples. The proposed framework is applied to a dataset in the US. 453 county-level macro factors are collected from various data sources, covering ten macro aspects, including topography, transportation, etc. The optimized BO-CV-XGBoost model obtains the best classification performance with an AUC of 0.8977 and an accuracy of 85.02%. Compared with other methods, the proposed model has superiority on fatality rate classification. Ten macro factors are identified, including \u2018Current-dollar GDP\u2019, \u2018highway miles per person\u2019, etc. The ten factors contain four aspects of information, including economics, transportation, education, and medical condition. Geographic information system (GIS) techniques are further used for spatial analysis of the identified macro factors. Therefore, targeted and effective measures are accordingly proposed to prevent traffic fatalities and improve road safety",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "14740346",
        "isbn": null,
        "journal": "Advanced Engineering Informatics",
        "publisher": null,
        "title": "areviewofdigitaltwininproductdesignanddevelopment",
        "booktitle": null,
        "doi": "10.1016/j.aei.2021.101297",
        "author": [
            "Lo, C.K.",
            "Chen, C.H.",
            "Zhong, Ray"
        ],
        "keywords": [
            "Digital twin, Product design, New product development, Product lifecycle, Review"
        ],
        "abstract": "In the era of digitalization, there are many emerging technologies, such as the Internet of Things (IoT), Digital Twin (DT), Cloud Computing and Artificial Intelligence (AI), which are quickly developped and used in product design and development. Among those technologies, DT is one promising technology which has been widely used in different industries, especially manufacturing, to monitor the performance, optimize the progresses, simulate the results and predict the potential errors. DT also plays various roles within the whole product lifecycle from design, manufacturing, delivery, use and end-of-life. With the growing demands of individualized products and implementation of Industry 4.0, DT can provide an effective solution for future product design, development and innovation. This paper aims to figure out the current states of DT research focusing on product design and development through summarizing typical industrial cases. Challenges and potential applications of DT in product design and development are also discussed to inspire future studies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.603",
        "scimago_value": "1,107"
    },
    {
        "issnkey": "03784371",
        "isbn": null,
        "journal": "Physica A: Statistical Mechanics and its Applications",
        "publisher": null,
        "title": "ahybriddeeplearningmodelwith1dcnnlstmattentionnetworksforshorttermtrafficflowprediction",
        "booktitle": null,
        "doi": "10.1016/j.physa.2021.126293",
        "author": [
            "Wang, Ke",
            "Ma, Changxi",
            "Qiao, Yihuan",
            "Lu, Xijin",
            "Hao, Weining",
            "Dong, Sheng"
        ],
        "keywords": [
            "Traffic flow prediction, Deep learning, One-dimensional convolutional neural network, Long short-term memory network, Attentional mechanism"
        ],
        "abstract": "With the rapid development of social economy, the traffic volume of urban roads has raised significantly, which has led to increasingly serious urban traffic congestion problems, and has caused much inconvenience to people\u2019s travel. By focusing on the complexity and long-term dependence of traffic flow sequences on urban road, this paper considered the traffic flow data and weather conditions of the road section comprehensively, and proposed a short-term traffic flow prediction model based on the attention mechanism and the 1DCNN-LSTM network. The model combined the time expansion of the CNN and the advantages of the long-term memory of the LSTM. First, the model employs 1DCNN network to extract the spatial features in the road traffic flow data. Second, the output spatial features are considered as the input of LSTM neural network to extract the time features in road traffic flow data, and the long-term dependence characteristics of LSTM neural network are adopted to improve the prediction accuracy of traffic flow. Next, the spatio-temporal characteristics of road traffic flow were regarded as the input of the regression prediction layer, and the prediction results corresponding to the current input were calculated. Finally, the attention mechanism was introduced on the LSTM side to give enough attention to the key information, so that the model can focus on learning more important data features, and further improve the prediction performance. The experimental results showed that the prediction effect of the 1DCNN-LSTM-Attention model under the weather factor was better than that without considering the weather factor. At the same time, compared with traditional neural network models, the prediction effect of the proposed model revealed faster convergence speed and higher prediction accuracy. It can be found that for short-term traffic flow prediction on urban roads, the 1DCNN-LSTM network structure considering the attention mechanism provides superior features.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.263",
        "scimago_value": "0,640"
    },
    {
        "issnkey": "08835403",
        "isbn": null,
        "journal": "The Journal of Arthroplasty",
        "publisher": null,
        "title": "automateddetectionofperiprostheticjointinfectionsanddataelementsusingnaturallanguageprocessing",
        "booktitle": null,
        "doi": "10.1016/j.arth.2020.07.076",
        "author": [
            "Fu, Sunyang",
            "Wyles, Cody",
            "Osmon, Douglas",
            "Carvour, Martha",
            "Sagheb, Elham",
            "Ramazanian, Taghi",
            "Kremers, Walter",
            "Lewallen, David",
            "Berry, Daniel",
            "Sohn, Sunghwan",
            "Kremers, Hilal"
        ],
        "keywords": [
            "total joint arthroplasty, periprosthetic joint infection, natural language processing, electronic health records, artificial intelligence"
        ],
        "abstract": "Background Periprosthetic joint infection (PJI) data elements are contained in both structured and unstructured documents in electronic health records and require manual data collection. The goal of this study is to develop a natural language processing (NLP) algorithm to replicate manual chart review for PJI data elements. Methods PJI was identified among all total joint arthroplasty (TJA) procedures performed at a single academic institution between 2000 and 2017. Data elements that comprise the Musculoskeletal Infection Society (MSIS) criteria were manually extracted and used as the gold standard for validation. A training sample of 1208 TJA surgeries (170 PJI cases) was randomly selected to develop the prototype NLP algorithms and an additional 1179 surgeries (150 PJI cases) were randomly selected as the test sample. The algorithms were applied to all consultation notes, operative notes, pathology reports, and microbiology reports to predict the correct status of PJI based on MSIS criteria. Results The algorithm, which identified patients with PJI based on MSIS criteria, achieved an f1-score (harmonic mean of precision and recall) of 0.911. Algorithm performance in extracting the presence of sinus tract, purulence, pathologic documentation of inflammation, and growth of cultured organisms from the involved TJA achieved f1-scores that ranged from 0.771 to 0.982, sensitivity that ranged from 0.730 to 1.000, and specificity that ranged from 0.947 to 1.000. Conclusion NLP-enabled algorithms have the potential to automate data collection for PJI diagnostic elements, which could directly improve patient care and augment cohort surveillance and research efforts. Further validation is needed in other hospital settings. Level of Evidence Level III, Diagnostic.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.757",
        "scimago_value": "2,766"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "overviewonhybridapproachestofaultdetectionanddiagnosiscombiningdatadrivenphysicsbasedandknowledgebasedmodels",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.03.041",
        "author": [
            "Wilhelm, Yannick",
            "Reimann, Peter",
            "Gauchel, Wolfgang",
            "Mitschang, Bernhard"
        ],
        "keywords": [
            "Fault detection, Fault diagnosis, Hybrid methods, Diagnostics, maintenance, Knowledge-driven methods, Machine learning"
        ],
        "abstract": "In this paper, we review hybrid approaches for fault detection and fault diagnosis (FDD) that combine data-driven analysis with physics-based and knowledge-based models to overcome a lack of data and to increase the FDD accuracy. We categorize these hybrid approaches according to the steps of an extended common workflow for FDD. This gives practitioners indications of which kind of hybrid FDD approach they can use in their application.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "biologicalcharacteristicsofenergyconversionincarbonfixationbymicroalgae",
        "booktitle": null,
        "doi": "10.1016/j.rser.2021.111661",
        "author": [
            "Zeng, Jing",
            "Wang, Zhenjun",
            "Chen, Guobin"
        ],
        "keywords": [
            "Microalgae, Carbon fixation, Photosynthetic reaction, Carbon pump, Energy conversion"
        ],
        "abstract": "CO2-fixation by microalgae can be regarded as a biological process of energy conversion with CO2 and H2O in microalgae cells in the sunlight. The study of the biological intrinsic characteristics of energy conversion is helpful to reveal the high-efficiency carbon fixation mechanism of microalgae. Firstly, the CO2 emission control technology and the external influencing factors are summarized in this paper, which have laid the foundation for researching the internal biological intrinsic characteristics of carbon fixation by microalgae. Based on photosynthetic reactions, in-situ reaction experiments, hydrodynamic simulations and metabolic networks have been integrated to analyze the biological intrinsic characteristics of carbon fixation by microalgae. The collation of representative studies on theory and quantitative calculation methods reveals that free energy dissipation seriously affects the carbon fixation efficiency of microalgae; Secondly, thermodynamics and metabolic networks are discussed. The role of thermodynamics in addressing the constraints is explored mainly from the perspective of energy conversion mechanisms, free energy dissipation mechanisms, framework and methods. Metabolic networks are studied using sampling methods based on thermodynamic systems and metabolic engineering based on a systems perspective; Thirdly, the key supporting technologies and biological intrinsic characteristics are reviewed from an interdisciplinary perspective, and the researches on metabolic networks based on thermodynamic constraints are given; Finally, challenges are summarized to provide a basis and direction for future research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "00298018",
        "isbn": null,
        "journal": "Ocean Engineering",
        "publisher": null,
        "title": "collisionavoidancenavigationsystemsformaritimeautonomoussurfaceshipsastateoftheartsurvey",
        "booktitle": null,
        "doi": "10.1016/j.oceaneng.2021.109380",
        "author": [
            "Zhang, Xinyu",
            "Wang, Chengbo",
            "Jiang, Lingling",
            "An, Lanxuan",
            "Yang, Rui"
        ],
        "keywords": [
            "Collision avoidance, Autonomous navigation systems, Cognitive navigation, e-navigation, Maritime autonomous surface ships"
        ],
        "abstract": "The rapid development of artificial intelligence significantly promotes collision-avoidance navigation of maritime autonomous surface ships (MASS), which in turn provides prominent services in maritime environments and enlarges the opportunity for coordinated and interconnected operations. Clearly, full autonomy of the collision-avoidance navigation for the MASS in complex environments still faces huge challenges and highly requires persistent innovations. First, we survey relevant guidance of the International Maritime Organization (IMO) and industry code of each country on MASS. Then, major advances in MASS industry R&D, and collision-avoidance navigation technologies, are thoroughly overviewed, from academic to industrial sides. Moreover, compositions of collision-avoidance navigation, brain-inspired cognitive navigation, and e-navigation technologies are analyzed to clarify the mechanism and principles efficiently systematically in typical maritime environments, whereby trends in maritime collision-avoidance navigation systems are highlighted. Finally, considering a general study of existing collision avoidance and action planning technologies, it is pointed out that collision-free navigation would significantly benefit the integration of MASS autonomy in various maritime scenarios.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.795",
        "scimago_value": "1,321"
    },
    {
        "issnkey": "00298018",
        "isbn": null,
        "journal": "Ocean Engineering",
        "publisher": null,
        "title": "dataminingapproachforautomaticshiproutedesignforcoastalseasusingaistrajectoryclusteringanalysis",
        "booktitle": null,
        "doi": "10.1016/j.oceaneng.2021.109535",
        "author": [
            "Zhang, Daheng",
            "Zhang, Yingjun",
            "Zhang, Chuang"
        ],
        "keywords": [
            "RU, FA-DBSCAN, Ship-route design, Data mining, AIS data"
        ],
        "abstract": "In this paper, we propose an automatic route design method based on simple recurrent unit (SRU) and automatic identification system (AIS) data. Laplacian eigen maps and Gaussian kernel functions are used to compress the AIS data and extract the turning points of all ships. Fuzzy adaptive density-based spatial clustering of applications with noise (FA-DBSCAN) technique is used to cluster the turning points obtained at the preprocessing stage to obtain the turning region. Optimal turn region matching is used to connect the turning regions of similar routes, and the SRU neural network algorithm is used to learn the relationship between different types, sizes, and drafts of ships in each turning region; extract the feature-turning points; and obtain the recommended coastal routes, speed, and course of each type of ship. In the experimental stage, a large variety of AIS data from two sea areas are used to compare and analyze the designed route and real-ship data through LSTM and SRU experiments. The results show that the SRU algorithm improves the training speed and accuracy in comparison to LSTM, while the generated automatic route meets the requirements of navigation practice.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.795",
        "scimago_value": "1,321"
    },
    {
        "issnkey": "0968090x",
        "isbn": null,
        "journal": "Transportation Research Part C: Emerging Technologies",
        "publisher": null,
        "title": "freewayaccidentdetectionandclassificationbasedonthemultivehicletrajectorydataanddeeplearningmodel",
        "booktitle": null,
        "doi": "10.1016/j.trc.2021.103303",
        "author": [
            "Yang, Da",
            "Wu, Yuezhu",
            "Sun, Feng",
            "Chen, Jing",
            "Zhai, Donghai",
            "Fu, Chuanyun"
        ],
        "keywords": [
            "Freeway traffic accident, Vehicle trajectory, Deep Convolutional Neural Network, Accident detection and classification"
        ],
        "abstract": "The freeway accident detection and classification have attracted much attention of researchers in the past decades. With the popularity of Global Navigation Satellite System (GNSS) on mobile phones and onboard equipment, increasing amounts of real-time vehicle trajectory data can be obtained more and more easily, which provides a potential way to use the multi-vehicle trajectory data to detect and classify an accident on freeways. The data has the advantages of low cost, high penetration, high real-time performance, and being robust to the outdoor environment. Therefore, this paper proposes a new method for accident detection and classification based on the multi-vehicle trajectory data. Different from the existing methods using GNSS positioning data, the proposed method not only uses the position information of the related vehicles but also tries to capture the development tendencies of the trajectories of accident vehicles over a period of time. A Deep Convolutional Neural Network model is developed to recognize an accident from the normal driving of vehicles and also identify the type of the accident, and the six types of traffic accidents are considered in this study. To train and test the proposed model, the simulated trajectory data is generated based on PC-Crash, including the normal driving trajectories and the trajectories before, in, and after an accident. The results indicate that the detection accuracy of the proposed method can reach up to 100%, and the classification accuracy can reach up to 95%, which both outperform the existing methods using other data. In addition, to ensure the robustness of the detection accuracy, at least 1 s of duration and 5 Hz of frequency for the trajectory data should be adopted in practice. The study will help to accurately and fastly detect an accident, recognize the accident type, and future judge who is liable for the accident.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15320464",
        "isbn": null,
        "journal": "Journal of Biomedical Informatics",
        "publisher": null,
        "title": "datamanagementplansinthegenomicsresearchrevolutionofafricachallengesandrecommendations",
        "booktitle": null,
        "doi": "10.1016/j.jbi.2021.103900",
        "author": [
            "Fadlelmola, Faisal",
            "Zass, Lyndon",
            "Chaouch, Melek",
            "Samtal, Chaimae",
            "Ras, Verena",
            "Kumuthini, Judit",
            "Panji, Sumir",
            "Mulder, Nicola"
        ],
        "keywords": [
            "Data Management Plan, Africa, FAIR, Funders, Data sharing, Genomics data management"
        ],
        "abstract": "Drafting and writing a data management plan (DMP) is increasingly seen as a key part of the academic research process. A DMP is a document that describes how a researcher will collect, document, describe, share, and preserve the data that will be generated as part of a research project. The DMP illustrates the importance of utilizing best practices through all stages of working with data while ensuring accessibility, quality, and longevity of the data. The benefits of writing a DMP include compliance with funder and institutional mandates; making research more transparent (for reproduction and validation purposes); and FAIR (findable, accessible, interoperable, reusable); protecting data subjects and compliance with the General Data Protection Regulation (GDPR) and/or local data protection policies. In this review, we highlight the importance of a DMP in modern biomedical research, explaining both the rationale and current best practices associated with DMPs. In addition, we outline various funders\u2019 requirements concerning DMPs and discuss open-source tools that facilitate the development and implementation of a DMP. Finally, we discuss DMPs in the context of African research, and the considerations that need to be made in this regard.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15258610",
        "isbn": null,
        "journal": "Journal of the American Medical Directors Association",
        "publisher": null,
        "title": "bringingqualityhealthcarehomeviatechnologyinnovations",
        "booktitle": null,
        "doi": "10.1016/j.jamda.2021.03.028",
        "author": [
            "Koru, G\u00fcne\u015f"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.669",
        "scimago_value": "1,840"
    },
    {
        "issnkey": "02786125",
        "isbn": null,
        "journal": "Journal of Manufacturing Systems",
        "publisher": null,
        "title": "artificialintelligenceforthroughputbottleneckanalysisstateoftheartandfuturedirections",
        "booktitle": null,
        "doi": "10.1016/j.jmsy.2021.07.021",
        "author": [
            "Subramaniyan, Mukund",
            "Skoogh, Anders",
            "Bokrantz, Jon",
            "Sheikh, Muhammad",
            "Th\u00fcrer, Matthias",
            "Chang, Qing"
        ],
        "keywords": [
            "Throughput bottlenecks, Artificial intelligence, Production system, Data-driven, Manufacturing"
        ],
        "abstract": "Identifying, and eventually eliminating throughput bottlenecks, is a key means to increase throughput and productivity in production systems. In the real world, however, eliminating throughput bottlenecks is a challenge. This is due to the landscape of complex factory dynamics, with several hundred machines operating at any given time. Academic researchers have tried to develop tools to help identify and eliminate throughput bottlenecks. Historically, research efforts have focused on developing analytical and discrete event simulation modelling approaches to identify throughput bottlenecks in production systems. However, with the rise of industrial digitalisation and artificial intelligence (AI), academic researchers explored different ways in which AI might be used to eliminate throughput bottlenecks, based on the vast amounts of digital shop floor data. By conducting a systematic literature review, this paper aims to present state-of-the-art research efforts into the use of AI for throughput bottleneck analysis. To make the work of the academic AI solutions more accessible to practitioners, the research efforts are classified into four categories: (1) identify, (2) diagnose, (3) predict and (4) prescribe. This was inspired by real-world throughput bottleneck management practice. The categories, identify and diagnose focus on analysing historical throughput bottlenecks, whereas predict and prescribe focus on analysing future throughput bottlenecks. This paper also provides future research topics and practical recommendations which may help to further push the boundaries of the theoretical and practical use of AI in throughput bottleneck analysis.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.633",
        "scimago_value": "2,310"
    },
    {
        "issnkey": "02684012",
        "isbn": null,
        "journal": "International Journal of Information Management",
        "publisher": null,
        "title": "fiftyyearsofinformationmanagementresearchaconceptualstructureanalysisusingstructuraltopicmodeling",
        "booktitle": null,
        "doi": "10.1016/j.ijinfomgt.2021.102316",
        "author": [
            "Sharma, Anuj",
            "Rana, Nripendra",
            "Nunkoo, Robin"
        ],
        "keywords": [
            "Information management, Structural topic models, Topic modeling, Generative models, Text analytics"
        ],
        "abstract": "Information management is the management of organizational processes, technologies, and people which collectively create, acquire, integrate, organize, process, store, disseminate, access, and dispose of the information. Information management is a vast, multi-disciplinary domain that syndicates various subdomains and perfectly intermingles with other domains. This study aims to provide a comprehensive overview of the information management domain from 1970 to 2019. Drawing upon the methodology from statistical text analysis research, this study summarizes the evolution of knowledge in this domain by examining the publication trends as per authors, institutions, countries, etc. Further, this study proposes a probabilistic generative model based on structural topic modeling to understand and extract the latent themes from the research articles related to information management. Furthermore, this study graphically visualizes the variations in the topic prevalences over the period of 1970 to 2019. The results highlight that the most common themes are data management, knowledge management, environmental management, project management, service management, and mobile and web management. The findings also identify themes such as knowledge management, environmental management, project management, and social communication as academic hotspots for future research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "14.098",
        "scimago_value": "2,770"
    },
    {
        "issnkey": "17554365",
        "isbn": null,
        "journal": "Epidemics",
        "publisher": null,
        "title": "challengesinevaluatingrisksandpolicyoptionsaroundendemicestablishmentoreliminationofnovelpathogens",
        "booktitle": null,
        "doi": "10.1016/j.epidem.2021.100507",
        "author": [
            "Metcalf, C.",
            "Andriamandimby, Soa",
            "Baker, Rachel",
            "Glennon, Emma",
            "Hampson, Katie",
            "Hollingsworth, T.",
            "Klepac, Petra",
            "Wesolowski, Amy"
        ],
        "keywords": [
            "Endemic, Epidemic, Mathematical model, Elimination, Emergence"
        ],
        "abstract": "When a novel pathogen emerges there may be opportunities to eliminate transmission - locally or globally - whilst case numbers are low. However, the effort required to push a disease to elimination may come at a vast cost at a time when uncertainty is high. Models currently inform policy discussions on this question, but there are a number of open challenges, particularly given unknown aspects of the pathogen biology, the effectiveness and feasibility of interventions, and the intersecting political, economic, sociological and behavioural complexities for a novel pathogen. In this overview, we detail how models might identify directions for better leveraging or expanding the scope of data available on the pathogen trajectory, for bounding the theoretical context of emergence relative to prospects for elimination, and for framing the larger economic, behavioural and social context that will influence policy decisions and the pathogen\u2019s outcome.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.396",
        "scimago_value": "2,023"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "spatializingenvironmentalfootprintbyintegratinggeographicinformationsystemintolifecycleassessmentareviewandpracticerecommendations",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.129113",
        "author": [
            "Li, Junjie",
            "Tian, Yajun",
            "Zhang, Yueling",
            "Xie, Kechang"
        ],
        "keywords": [
            "Life cycle assessment, Geographic information system, Environmental footprint, GIS-LCA"
        ],
        "abstract": "Life cycle assessment (LCA) is a methodological tool that estimates the environmental footprint from a cradle-to-grave perspective. With the increased need for the geographically explicit assessment, the geographic information system (GIS) is integrating into LCA as a frontier methodology to spatialize the environmental footprint. This paper reviews a total of 105 publications about GIS-LCA, including 50 methodological studies that are analyzed following the four phases of LCA and 55 applied studies that are classified into different domains. The review shows that although GIS-LCA methodology has certain explorations and practices and a large number of cases are carried out in the energy industry, agricultural sector, urban facility, and waste management, the current knowledge system faces several challenges in spatializing environmental footprint. In this case, a universal methodology framework of GIS-LCA and specific schemes are proposed to address the following issues: (1) how to set up a geographically referenced system in the goal and scope definition phase; (2) how to spatialize life-cycle data and integrate and compute foreground and background data in the inventory analysis phase; (3) how to develop spatialized characterization factors with different requirements on resolution and data availability in the impact assessment phase; and (4) how to uniform the contribution analysis of different zones, unit processes, and elementary flows to visualize spatialized environmental footprint in the interpretation phase. The framework we developed provides preliminary practices and recommendations for spatializing environmental footprint, which lays a foundation to support future work.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "13891286",
        "isbn": null,
        "journal": "Computer Networks",
        "publisher": null,
        "title": "federateddeepreinforcementlearningbasedsecuredatasharingforinternetofthings",
        "booktitle": null,
        "doi": "10.1016/j.comnet.2021.108327",
        "author": [
            "Miao, Qinyang",
            "Lin, Hui",
            "Wang, Xiaoding",
            "Hassan, Mohammad"
        ],
        "keywords": [
            "Secure data sharing, Federated learning, Deep reinforcement learning, IoT"
        ],
        "abstract": "The increasing number of Internet of Things (IoT) devices motivate the data sharing that improves the quality of IoT services. However, data providers usually suffer from the privacy leakage caused by direct data sharing. To solve this problem, in this paper, we propose a Federated Learning based Secure data Sharing mechanism for IoT, named FL2S. Specifically, to accomplish efficient and secure data sharing, a hierarchical asynchronous federated learning (FL) framework is developed based on the sensitive task decomposition. In addition, to improve data sharing quality, the deep reinforcement learning (DRL) technology is utilized to select participants of sufficient computational capabilities and high quality datasets. By integrating task decomposition and participant selection, reliable data sharing is realized by sharing local data models instead of the source data with data privacy preserved. Experiment results show that the proposed FL2S achieves high accuracy in secure data sharing for various IoT applications.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.474",
        "scimago_value": "0,798"
    },
    {
        "issnkey": "18755364",
        "isbn": null,
        "journal": "Chinese Journal of Natural Medicines",
        "publisher": null,
        "title": "tcmnetworkpharmacologyanewtrendtowardscombiningcomputationalexperimentalandclinicalapproaches",
        "booktitle": null,
        "doi": "10.1016/S1875-5364(21)60001-8",
        "author": [
            "WANG, Xin",
            "WANG, Zi-Yi",
            "ZHENG, Jia-Hui",
            "LI, Shao"
        ],
        "keywords": [
            "Network pharmacology, Traditional Chinese medicine, Network target, Computation, Experiment, Clinical approach"
        ],
        "abstract": "Traditional Chinese medicine (TCM) is a precious treasure of the Chinese nation and has unique advantages in the prevention and treatment of diseases. The holistic view of TCM coincides with the new generation of medical research paradigm characterized by network and system. TCM gave birth to a new method featuring holistic and systematic \u201cnetwork target\u201d, a core theory and method of network pharmacology. TCM is also an important research object of network pharmacology. TCM network pharmacology, which aims to understand the network-based biological basis of complex diseases, TCM syndromes and herb treatments, plays a critical role in the origin and development process of network pharmacology. This review introduces new progresses of TCM network pharmacology in recent years, including predicting herb targets, understanding biological foundation of diseases and syndromes, network regulation mechanisms of herbal formulae, and identifying disease and syndrome biomarkers based on biological network. These studies show a trend of combining computational, experimental and clinical approaches, which is a promising direction of TCM network pharmacology research in the future. Considering that TCM network pharmacology is still a young research field, it is necessary to further standardize the research process and evaluation indicators to promote its healthy development.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-819671-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "12dataperspectiveonenvironmentalmobilecrowdsensing",
        "booktitle": "Intelligent Environmental Data Monitoring for Pollution Management",
        "doi": "10.1016/B978-0-12-819671-7.00012-9",
        "author": [
            "Brahem, Mariem",
            "Hafyani, Hafsa",
            "Mehanna, Souheir",
            "Zeitouni, Karine",
            "Yeh, Laurent",
            "Taher, Yehia",
            "Kedad, Zoubida",
            "Ktaish, Ahmad",
            "Chachoua, Mohamed",
            "Ray, Cyril"
        ],
        "keywords": [
            "Mobile crowd sensing, Environmental sensing, Data management, Data mining, Exposure analysis, Big data framework, Scalability"
        ],
        "abstract": "The advent of the new generation of low-cost lightweight and connected sensors made a paradigm shift in environmental studies. In particular, nomadic sensors allow for a very precise personalized measurement, by continuously quantifying the individual exposure to air pollution components. Moreover, a broad dissemination among volunteers of these devices, or their deployment on vehicle fleets, is becoming a credible solution. Another major interest of such sensor deployment is to densify the air quality monitoring network, indoor, as in the outdoor, which is today restricted to sparse nodes. However, this high spatiotemporal resolution raises several issues related to their analysis. After an overview of the projects relying on this technology, this chapter points out the remaining challenges to be addressed. Part of these challenges constitutes the research program of the ongoing project Polluscope in France.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-323-85064-3",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter6impetustomachinelearningincardiacdiseasediagnosis",
        "booktitle": "Image Processing for Automated Diagnosis of Cardiac Diseases",
        "doi": "10.1016/B978-0-323-85064-3.00009-1",
        "author": [
            "Vani, T."
        ],
        "keywords": [
            "Machine learning, disease diagnosis, healthcare, cardiac disease, disease detection, medical imaging, automated diagnosis, cardiac diagnosis"
        ],
        "abstract": "Machine learning is a branch of computer science, and it is a subset of artificial intelligence. It comprises many algorithms based on statistical methods to build automated systems for solving a particular problem. Due to its versatility, it is popular in many fields in real life, including scientific researches, healthcare field, industries, pharmaceutical field for drug discovery, social anomalies such as epidemic, and pandemic diseases spread. This chapter aims to identify the impact of machine learning techniques in the diagnosis of cardiac diseases. This chapter starts with the justification of the need for machine learning technology in the healthcare field. The basics of machine learning technology and its various algorithms are explained in the next section. The applications of these algorithms, which includes the diagnosis of various diseases such as diabetics, coronary artery disease (CAD), coronary heart disease (CHD), liver ailments, cancer detection and prevention, radiology, pathology, clinical trials, robotic surgery, drug discovery, and personalized treatments, are described from the contemporary researches. The challenges it faces in the healthcare field are also listed. In the end, the constraints of machine learning techniques in the healthcare field are explained with the suggestions to make accurate and efficient diagnoses in the future.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22131388",
        "isbn": null,
        "journal": "Sustainable Energy Technologies and Assessments",
        "publisher": null,
        "title": "makingenergytransitionheadwayadatadrivenassessmentofgermanenergystartups",
        "booktitle": null,
        "doi": "10.1016/j.seta.2021.101322",
        "author": [
            "Singh, Mahendra",
            "Jiao, Jiao",
            "Klobasa, Marian",
            "Frietsch, Rainer"
        ],
        "keywords": [
            "Energy startups, Emerging technologies, Energy-transition, Funding, Innovation, Digitalisation, Data analysis, Business model, X-as-a-service, Digital platform"
        ],
        "abstract": "This paper explores the linkage between ongoing clean energy-transition, technology and business model emergence in the German energy sector. The speed of energy-transition is often led by innovative startups. Startups with innovative products, services, or value propositions are a key indicator, supporting successful energy-transition. Though, commercial databases cover comprehensive details to understand startup\u2019s financial activity and stakeholder relation, but without considering their innovation and business activity. Measuring the actual activities of energy startups is pivotal to capture the impact of energy-transition. To put this into perspective, a hybrid approach of data collection combining structured and unstructured data has been proposed in the following work. A list of 240 innovative startups belonging to different categories and technology focus are examined. Furthermore, data-driven analysis is performed over the data collected from multiple sources. Renewable technologies are yet the most preferred technology focus among German entrepreneurs and stakeholders. 24.6% startups are identified in this category followed by 17.5% in energy management and 16.2% in energy storage. The evidence from this study suggests a clear shift in technology and the value proposition of successful innovative startups in Germany. Digitalisation of the energy sector is fostering the development of multi-sided digital platform driven business models. The result suggested that 8.0% of startups have implemented purely platform based services while 15.7% are experimenting with platform business models along with traditional business to business (B2B) and business to customer (B2C) business models. Findings could guide policymakers and federal agencies to provide a vision for future technology and business model adaptation in the German energy sector.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.353",
        "scimago_value": "1,040"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "industrialapplicationsofartificialintelligencefromgrandstoriesofdigitaldisruptiontoactualprogress",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.115",
        "author": [
            "Fritzsche, Albrecht",
            "G\u00f6lzer, Philipp"
        ],
        "keywords": [
            "Artificial Intelligence, Data-Driven Operations Management, Digital Transformation Narratives, Industry 4.0"
        ],
        "abstract": "Data-driven operations management goes along with narratives of disruptive change and new potential for innovation. We study how these narratives are reflected in the outcomes of 82 implementation projects that took place during the last ten years. The analysis of the projects identifies varying focal points in different industrial sectors. Radical steps towards new forms of data-driven operations management have only been achieved in exceptional cases. For the most part, new technical solutions follow given organizational structures and preserve extant business processes. We describe typical implementation patterns, compare them across industries and discuss different interpretations of the findings.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "dataenrichmentintheinformationgraphsenvironmentbasedonaspecializedarchitectureofinformationchannels",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.07.001",
        "author": [
            "Kosikov, Sergey",
            "Ismailova, Larisa",
            "Wolfengagen, Viacheslav"
        ],
        "keywords": [
            "data enrichment, information channels, conceptual constructions, informational graph, applicative computations, semantics"
        ],
        "abstract": "The paper considers the possibility of constructing a specialized computing system oriented at the transmission of data through information channels, that are determined taking into account the semantics of the selected data. In the process of computations the data is connected with semantic characteristics that describe the channel of computations, which can be considered as a method of semantic data enrichment. The system of information channels as a whole can be considered as an information graph describing the structuring of the processed data. The information graph supports the data model in the form of a network, the framework of which are objects and the relationships between them. The paper proposes language tools for determining the information graph and interpretation tools that provide practical computations. The set of information channels that make up the information graph can be considered as a low-level tool for data enrichment. The paper studies the possibility of determining tools of higher level. An applicative type language is proposed for defining information graphs, the syntax and semantics of the language are specified. The proposed language can be considered as an intermediate level tool for defining semantics. A procedure is proposed for compiling the language into a low-level construct, preserving the semantics of the language. The supporting system for the proposed computing system includes a low-level language interpreter, as well as an intermediate-level language compiler into a low-level language. The supporting system is implemented in an applicative programming environment. Some elements of the supporting system were tested when developing applied information systems in the field of jurisprudence.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "datascienceasknowledgecreationaframeworkforsynergiesbetweendataanalystsanddomainprofessionals",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.121160",
        "author": [
            "{van der Voort}, Haiko",
            "{van Bulderen}, Sabine",
            "Cunningham, Scott",
            "Janssen, Marijn"
        ],
        "keywords": [
            "Data science, Knowledge, Predictive model, Value creation, Risk-based inspection, Professionalism"
        ],
        "abstract": "The road from data generation to data use is commonly approached as a data-driven, functional process in which domain expertise is integrated as an afterthought. In this contribution we complement this functional view with an institutional view, that takes data analysis and domain professionalism as complementary (yet fallible) knowledge sources. We developed a framework that identifies and amplifies synergies between data analysts and domain professionals instead of taking one of them (i.e. data analytics) at the centre of the analytical process. The framework combines the often-cited CRISP-DM framework with a knowledge creation framework. The resulting framework is used in a data science project at a Dutch inspectorate that seeks to use data for risk-based inspection. The findings show first support of our framework. They also show that whereas more complex models have a higher predictive power, simpler models are sometimes preferred as they have the potential to create more synergies between inspectors and data analyst. Another issue driven by the integrated framework is about who of the involved actors should own the predictive model: data analysts or inspectors.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "exploringdistancebasedapproachesforreducingsensordataindefectrelatedprognosis",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.03.076",
        "author": [
            "Mathias, Selvine",
            "Grossmann, Daniel",
            "Bhanja, Tapanta"
        ],
        "keywords": [
            "sensors, data, reduced distance, machine learning, accuracy scores"
        ],
        "abstract": "Vibration data consists of batches of time series which if accumulated over a period of time is a huge collection of numeric data. Reducing such data for use in deep learning models for computational effciency is a challenge. Combinatorial and discrete approaches, on the other hand, is not an extensively explored area when it comes to datasets. This paper aims to identify feature reduction techniques based on discrete approaches such as euclidean distance using dot products on vibration data samples from accelerometers fitted on bearings. In this limited experimentation, the procured dataset by this approach is considerably smaller in size as compared to the actual complete data, and with comparable results in prediction models, it can be used as a smaller representation of a sensor timeline. The results based on different models show that such reductions can be considered in building IoT applications in industries based on sensors.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822879-1",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter5thedesignofblendedlearningexperiencesforcleandatatoallowproperobservationofstudentparticipation",
        "booktitle": "Technology-Enabled Blended Learning Experiences for Chemistry Education and Outreach",
        "doi": "10.1016/B978-0-12-822879-1.00004-4",
        "author": [
            "Quigley, Cormac",
            "Leavy, Elaine",
            "Kiely, Etain",
            "Jordan, Garrett"
        ],
        "keywords": [
            "Learning analytics, Clean data, Multidisciplinary team, Motivations, VLE, Moodle"
        ],
        "abstract": "This chapter shares the results and insights from a collaborative project to use learning analytics to capture and transform learning in the first year of undergraduate science programs. The multidisciplinary team is composed of academics and technical staff with a shared goal and numerous motivations. The shared goal was to use analytics to describe and optimize learning. This is an ongoing project first instigated in 2016, which has evolved from using descriptive analytics to create personalized feedback forms, to creating dashboards and is working toward using historical data to train models to monitor and predict engagement and disengagement (identify at-risk students). Data are collected through a blended learning model that has enabled students to take greater ownership of their learning and staff to enhance curriculum and learning strategies.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02697491",
        "isbn": null,
        "journal": "Environmental Pollution",
        "publisher": null,
        "title": "mappingleadconcentrationsinurbantopsoilusingproximalandremotesensingdataandhybridstatisticalapproaches",
        "booktitle": null,
        "doi": "10.1016/j.envpol.2020.116041",
        "author": [
            "Shi, Tiezhu",
            "Yang, Chao",
            "Liu, Huizeng",
            "Wu, Chao",
            "Wang, Zhihua",
            "Li, He",
            "Zhang, Huifang",
            "Guo, Long",
            "Wu, Guofeng",
            "Su, Fenzhen"
        ],
        "keywords": [
            "Jenny\u2019s state factor model, Visible and near-infrared reflectance spectroscopy, Landsat image, Geographically weighted regression, Regression kriging"
        ],
        "abstract": "Due to rapid urbanization in China, lead (Pb) continues to accumulate in urban topsoil, resulting in soil degradation and increased public exposure. Mapping Pb concentrations in urban topsoil is therefore vital for the evaluation and control of this exposure risk. This study developed spatial models to map Pb concentrations in urban topsoil using proximal and remote sensing data. Proximal sensing reflectance spectra (350\u20132500 nm) of soils were pre-processed and used to calculate the principal components as landscape factors to represent the soil properties. Other landscape factors, including vegetation and land-use factors, were extracted from time-sequential Landsat images. Two hybrid statistical approaches, regression kriging (RK) and geographically weighted regression (GWR), were adopted to establish prediction models using the landscape factors. The results indicated that the use of landscape factors derived from combined remote and proximal sensing data improved the prediction of Pb concentrations compared with useing these data individually. GWR obtained better results than RK for predicting soil Pb concentration. Thus, joint proximal and remote sensing provides timely, easily accessible, and suitable data for extracting landscape factors.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.071",
        "scimago_value": "2,136"
    },
    {
        "issnkey": "24058963",
        "isbn": null,
        "journal": "IFAC-PapersOnLine",
        "publisher": null,
        "title": "towardsaproactivevisionofthetrainingforthe40industryfromtherequiredskillsdiagnostictothetrainingofemployees",
        "booktitle": null,
        "doi": "10.1016/j.ifacol.2021.08.135",
        "author": [
            "Marmier, Fran\u00e7ois",
            "Deniaud, Ioana",
            "Rasovska, Ivana",
            "Michalak, Jean-Louis"
        ],
        "keywords": [
            "Active Learning, leaning path to 4.0, Engineering education, Human factor, Industry 4.0, Learning Factory, Skills"
        ],
        "abstract": "The digitalisation increase in industrial processes is perceived, by companies, as an opportunity to grow up their competitivity. Data are more and more accessible, potentially allowing making better decisions at all the level of the company. Then, job profiles and their required skills are changing. However, if competencies focused on software tools, programming, data analysis, simulation, virtual design, automatics and electronics becomes necessary, the initial trainings and continuous trainings are not changing as fast. Moreover, if new technologies are more available in companies, the workforce suffers of a lack of preparation. It generates risks of mistakes, improper use of tools and information, under performed activities, insufficiently informed decision. A global vision of how to train the whole industrial network is necessary to generate a progress of the whole industry. Workers must get the right skills for their activities in order to become a factor of efficiency for their workshop and consequently for the whole logistic chain. In that way, the role of the universities is to develop trainings for up-to-date needs as the industry 4.0. For this purpose, this paper introduced an overview of how to propose actual trainings on the topic of the Industry 4.0 both customized for the companies and for the learners. We detail more specifically in this paper 3 tools we develop at the University of Strasbourg: (1) a diagnostic tool to get the maturity level of companies and propose adapted learning paths. (2) a set of grids to design adapted learning path to the different work. (3) a Learning Factory to allow a learning by doing way.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,308"
    },
    {
        "issnkey": "22106707",
        "isbn": null,
        "journal": "Sustainable Cities and Society",
        "publisher": null,
        "title": "comparativeassessmentsandinsightsofdataopennessof50smartcitiesinairqualityaspects",
        "booktitle": null,
        "doi": "10.1016/j.scs.2021.102868",
        "author": [
            "Mak, Hugo",
            "Lam, Yun"
        ],
        "keywords": [
            "Smart cities, Air quality, Open data policy, Environmental monitoring, Data sharing and privacy, Future development of air quality network"
        ],
        "abstract": "Data Openness is considered as an indispensable component for scientific innovation, community engagement and smart city development. In this study, a Data Openness in Air Quality (DOAQ) framework that consists of 3 tiers with a total of 23 open data principles was established to assess and monitor the status and development of data sharing, release and centralization of air quality information in the top 50 smart cities (Top50SC) around the world. The DOAQ utilizes additive formulas with predefined coefficients to obtain scores in each tier, thus reflecting the relative importance on data availability and visibility of different air quality data. The scores of DOAQ were compared with the smart cities scorings from Eden Strategy Institute and ONG&ONG Pte Ltd. (2018), and other socioeconomic attributes (i.e., social, political and humane) within the current study. Strong correlations (i.e., 0.4\u22120.6) among these indices implicate that the status of air quality reporting could be a good proxy to gauge the environmental data openness in a city. Lastly, good practices (e.g., apps and air quality forecasts), essential criteria and directions for future smart city development on air quality reporting were summarized, with the aim of laying down practical and efficient guidelines for individual smart city that desires to seek for improvements in air quality data openness.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.587",
        "scimago_value": "1,645"
    },
    {
        "issnkey": "01441647",
        "isbn": null,
        "journal": "Transport Reviews",
        "publisher": null,
        "title": "crowdsourceddataforbicyclingresearchandpractice",
        "booktitle": null,
        "doi": "10.1080/01441647.2020.1806943",
        "author": [
            "Nelson, Trisalyn",
            "Ferster, Colin",
            "Laberee, Karen",
            "Fuller, Daniel",
            "Winters, Meghan"
        ],
        "keywords": [
            "Crowdsourced, bicycling, exposure, safety, infrastructure, attitudes"
        ],
        "abstract": "ABSTRACT Cities are promoting bicycling for transportation as an antidote to increased traffic congestion, obesity and related health issues, and air pollution. However, both research and practice have been stalled by lack of data on bicycling volumes, safety, infrastructure, and public attitudes. New technologies such as GPS-enabled smartphones, crowdsourcing tools, and social media are changing the potential sources for bicycling data. However, many of the developments are coming from data science and it can be difficult evaluate the strengths and limitations of crowdsourced data. In this narrative review we provide an overview and critique of crowdsourced data that are being used to fill gaps and advance bicycling behaviour and safety knowledge. We assess crowdsourced data used to map ridership (fitness, bike share, and GPS/accelerometer data), assess safety (web-map tools), map infrastructure (OpenStreetMap), and track attitudes (social media). For each category of data, we discuss the challenges and opportunities they offer for researchers and practitioners. Fitness app data can be used to model spatial variation in bicycling ridership volumes, and GPS/accelerometer data offer new potential to characterise route choice and origin-destination of bicycling trips; however, working with these data requires a high level of training in data science. New sources of safety and near miss data can be used to address underreporting and increase predictive capacity but require grassroots promotion and are often best used when combined with official reports. Crowdsourced bicycling infrastructure data can be timely and facilitate comparisons across multiple cities; however, such data must be assessed for consistency in route type labels. Using social media, it is possible to track reactions to bicycle policy and infrastructure changes, yet linking attitudes expressed on social media platforms with broader populations is a challenge. New data present opportunities for improving our understanding of bicycling and supporting decision making towards transportation options that are healthy and safe for all. However, there are challenges, such as who has data access and how data crowdsourced tools are funded, protection of individual privacy, representativeness of data and impact of biased data on equity in decision making, and stakeholder capacity to use data given the requirement for advanced data science skills. If cities are to benefit from these new data, methodological developments and tools and training for end-users will need to track with the momentum of crowdsourced data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.643",
        "scimago_value": "3,046"
    },
    {
        "issnkey": "13091042",
        "isbn": null,
        "journal": "Atmospheric Pollution Research",
        "publisher": null,
        "title": "thespringfestivaleffectthechangeinno2columnconcentrationinchinacausedbythemigrationofhumanactivities",
        "booktitle": null,
        "doi": "10.1016/j.apr.2021.101232",
        "author": [
            "Li, Dongqing",
            "Wu, Qizhong",
            "Wang, Hui",
            "Xiao, Han",
            "Xu, Qi",
            "Wang, Lizhi",
            "Feng, Jinming",
            "Yang, Xiaochun",
            "Cheng, Huaqiong",
            "Wang, Lanning",
            "Sun, Yiming"
        ],
        "keywords": [
            "Spring festival effect, Tropospheric NO column concentration, Megacities, Human activity"
        ],
        "abstract": "The Spring Festival is the most important holiday in China, and human activity and population mobility may contribute greatly to air quality. According to the satellite-based tropospheric nitrogen dioxide (NO2) column and ground-based observational concentration of NO2 in megacities from 2013 to 2018 around the Spring Festival, we found that NO2 concentration obviously decreases, presenting a \u201ctide phenomenon\u201d, particularly in the megacities, with the tropospheric NO2 column density decreasing by 31.8%\u201344.5%. The tropospheric NO2 column density in Beijing decreased by 41.6% and rebounded by 22.3% after the festival. Vehicle sources were among the important causes of NOx emissions in the megacities, and traffic intensity decreased significantly during the festival. As the coronavirus disease 2019 (COVID-19) pandemic progresses, the traffic intensity in urban areas is decreasing significantly, with the tropospheric NO2 column density decreasing by 56.2% and rebounding by only 6.8% in 2020, without the \u201ctide phenomenon\u201d.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.352",
        "scimago_value": "0,984"
    },
    {
        "issnkey": "24058440",
        "isbn": null,
        "journal": "Heliyon",
        "publisher": null,
        "title": "applicationsofartificialintelligencetoimprovepatientflowonmentalhealthinpatientunitsnarrativeliteraturereview",
        "booktitle": null,
        "doi": "10.1016/j.heliyon.2021.e06626",
        "author": [
            "Cecula, Paulina",
            "Yu, Jiakun",
            "Dawoodbhoy, Fatema",
            "Delaney, Jack",
            "Tan, Joseph",
            "Peacock, Iain",
            "Cox, Benita"
        ],
        "keywords": [
            "Mental health, Patient flow, Artificial intelligence, National health service, Inpatient units"
        ],
        "abstract": "Background Despite a growing body of research into both Artificial intelligence and mental health inpatient flow issues, few studies adequately combine the two. This review summarises findings in the fields of AI in psychiatry and patient flow from the past 5 years, finds links and identifies gaps for future research. Methods The OVID database was used to access Embase and Medline. Top journals such as JAMA, Nature and The Lancet were screened for other relevant studies. Selection bias was limited by strict inclusion and exclusion criteria. Research 3,675 papers were identified in March 2020, of which a limited number focused on AI for mental health unit patient flow. After initial screening, 323 were selected and 83 were subsequently analysed. The literature review revealed a wide range of applications with three main themes: diagnosis (33%), prognosis (39%) and treatment (28%). The main themes that emerged from AI in patient flow studies were: readmissions (41%), resource allocation (44%) and limitations (91%). The review extrapolates those solutions and suggests how they could potentially improve patient flow on mental health units, along with challenges and limitations they could face. Conclusion Research widely addresses potential uses of AI in mental health, with some focused on its applicability in psychiatric inpatients units, however research rarely discusses improvements in patient flow. Studies investigated various uses of AI to improve patient flow across specialities. This review highlights a gap in research and the unique research opportunity it presents.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,455"
    },
    {
        "issnkey": "01403664",
        "isbn": null,
        "journal": "Computer Communications",
        "publisher": null,
        "title": "edgeandfogcomputingforiotasurveyoncurrentresearchactivitiesfuturedirections",
        "booktitle": null,
        "doi": "10.1016/j.comcom.2021.09.003",
        "author": [
            "Laroui, Mohammed",
            "Nour, Boubakr",
            "Moungla, Hassine",
            "Cherif, Moussa",
            "Afifi, Hossam",
            "Guizani, Mohsen"
        ],
        "keywords": [
            "Internet of Things (IoT), Edge computing, Cloud computing"
        ],
        "abstract": "The Internet of Things (IoT) allows communication between devices, things, and any digital assets that send and receive data over a network without requiring interaction with a human. The main characteristic of IoT is the enormous quantity of data created by end-user\u2019s devices that needs to be processed in a short time in the cloud. The current cloud-computing concept is not efficient to analyze very large data in a very short time and satisfy the users\u2019 requirements. Analyzing the enormous quantity of data by the cloud will take a lot of time, which affects the quality of service (QoS) and negatively influences the IoT applications and the overall network performance. To overcome such challenges, a new architecture called edge computing \u2014 that allows to decentralize the process of data from the cloud to the network edge has been proposed to solve the problems occurred by using the cloud computing approach. Furthermore, edge computing supports IoT applications that require a short response time and consequently enhances the consumption of energy, resource utilization, etc. Motivated by the extensive research efforts in the edge computing and IoT applications, in this paper, we present a comprehensive review of edge and fog computing research in the IoT. We investigate the role of cloud, fog, and edge computing in the IoT environment. Subsequently, we cover in detail, different IoT use cases with edge and fog computing, the task scheduling in edge computing, the merger of software-defined networks (SDN) and network function virtualization (NFV) with edge computing, security and privacy efforts. Furthermore, the Blockchain in edge computing. Finally, we identify open research challenges and highlight future research directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.167",
        "scimago_value": "0,627"
    },
    {
        "issnkey": "2468502x",
        "isbn": null,
        "journal": "Visual Informatics",
        "publisher": null,
        "title": "visualizationandvisualanalysisofvesseltrajectorydataasurvey",
        "booktitle": null,
        "doi": "10.1016/j.visinf.2021.10.002",
        "author": [
            "Liu, Haiyan",
            "Chen, Xiaohui",
            "Wang, Yidi",
            "Zhang, Bing",
            "Chen, Yunpeng",
            "Zhao, Ying",
            "Zhou, Fangfang"
        ],
        "keywords": [
            "Maritime traffic, Vessel trajectory data, Automatic identification system, Visualization and visual analysis"
        ],
        "abstract": "Maritime transports play a critical role in international trade and commerce. Massive vessels sailing around the world continuously generate vessel trajectory data that contain rich spatial\u2013temporal patterns of vessel navigations. Analyzing and understanding these patterns are valuable for maritime traffic surveillance and management. As essential techniques in complex data analysis and understanding, visualization and visual analysis have been widely used in vessel trajectory data analysis. This paper presents a literature review on the visualization and visual analysis of vessel trajectory data. First, we introduce commonly used vessel trajectory data sets and summarize main operations in vessel trajectory data preprocessing. Then, we provide a taxonomy of visualization and visual analysis of vessel trajectory data based on existing approaches and introduce representative works in details. Finally, we expound on the prospects of the remaining challenges and directions for future research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-819354-9",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter1baselinedataforspillassessmentsambientconditionssocioeconomicdatasensitivitymaps",
        "booktitle": "Marine Hydrocarbon Spill Assessments",
        "doi": "10.1016/B978-0-12-819354-9.00007-7",
        "author": [
            "Romeo, Lucy",
            "Wingo, Patrick",
            "Sabbatino, Michael",
            "Bauer, Jennifer"
        ],
        "keywords": [
            "Baseline, data, spill preparedness, spill response, ambient, socioeconomic, sensitivity mapping, machine learning"
        ],
        "abstract": "Effective oil spill preparedness and response relies heavily on the availability of baseline data. Baselines comprise measurements and information collected prior to natural or anthropogenic disasters and can be applied to predict the transport and fate of pollutants, plan for socioeconomic stressors, and overall mitigate impacts. Spatial and temporal in nature, these datasets represent the current state of a specific area. Baselines representing offshore areas comprise ambient conditions, socioeconomic statuses, and environmental sensitivities. This chapter will highlight the value of baselines and identify means to collect and build representative databases for marine and coastal ecosystems to aid in spill assessments.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00200255",
        "isbn": null,
        "journal": "Information Sciences",
        "publisher": null,
        "title": "dimondriandistributedimprovedmondrianforsatisfactionoftheldiversityprivacymodelusingapachespark",
        "booktitle": null,
        "doi": "10.1016/j.ins.2020.07.066",
        "author": [
            "Ashkouti, Farough",
            "khamforoosh, Keyhan",
            "Sheikhahmadi, Amir"
        ],
        "keywords": [
            "Anonymization, PPDP, -anonymity, L-diversity, Information loss, Apache Spark, RDD"
        ],
        "abstract": "For the extraction of useful patterns, the collected data should be distributed to and shared with analyzers. This, however, creates problems and challenges for the individual with respect to their privacy and identity. In this paper, the Mondrian multidimensional anonymization method was developed and improved for satisfaction of the l-diversity privacy model, and it has been presented in a distributed fashion within the Apache Spark framework. Since one of the major challenges in data privacy is the tradeoff between privacy and data utility, the presented method focuses on information loss and classifier evaluation criteria. Therefore, the cut dimension was selected using the coefficient of variation and information gain criteria, and the cut points were chosen dynamically, which led to a decrease in the information loss parameter and an improvement in the classifier performance evaluation criteria such as accuracy and FMeasure compared to the previous algorithms in the literature. The processing speed is 100 times higher in Spark than in the Hadoop framework. Consequently, the proposed method was presented in a distributed fashion based on RDDs programming within Apache Spark framework. This will resolve the problem of speed in large-scale data anonymization as it exists in the previous Hadoop-based algorithms. The results of the experiments performed on the numerical datasets demonstrate the improvements made by the proposed method.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.795",
        "scimago_value": "1,524"
    },
    {
        "issnkey": "13596446",
        "isbn": null,
        "journal": "Drug Discovery Today",
        "publisher": null,
        "title": "thepresentandfutureofprojectmanagementinpharmaceuticalrd",
        "booktitle": null,
        "doi": "10.1016/j.drudis.2020.07.020",
        "author": [
            "Schuhmacher, Alexander",
            "Gassmann, Oliver",
            "Hinder, Markus",
            "Kuss, Michael"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02673649",
        "isbn": null,
        "journal": "Computer Law & Security Review",
        "publisher": null,
        "title": "whyfairnesscannotbeautomatedbridgingthegapbetweeneunondiscriminationlawandai",
        "booktitle": null,
        "doi": "10.1016/j.clsr.2021.105567",
        "author": [
            "Wachter, Sandra",
            "Mittelstadt, Brent",
            "Russell, Chris"
        ],
        "keywords": [
            "European union, Non-discrimination, Fairness, Discrimination, Bias, Algorithm, Law, Demographic parity, Machine learning, Artificial intelligence"
        ],
        "abstract": "In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in artificial intelligence (AI) and machine learning. Connecting this work to existing legal non-discrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automated fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ; we refer to this approach as \u201ccontextual equality.\u201d This Article makes three contributions. First, we review the evidential requirements to bring a claim under EU non-discrimination law. Due to the disparate nature of algorithmic and human discrimination, the EU's current requirements are too contextual, reliant on intuition, and open to judicial interpretation to be automated. Many of the concepts fundamental to bringing a claim, such as the composition of the disadvantaged and advantaged group, the severity and type of harm suffered, and requirements for the relevance and admissibility of evidence, require normative or political choices to be made by the judiciary on a case-by-case basis. We show that automating fairness or non-discrimination in Europe may be impossible because the law, by design, does not provide a static or homogenous framework suited to testing for discrimination in AI systems. Second, we show how the legal protection offered by non-discrimination law is challenged when AI, not humans, discriminate. Humans discriminate due to negative attitudes (e.g. stereotypes, prejudice) and unintentional biases (e.g. organisational practices or internalised stereotypes) which can act as a signal to victims that discrimination has occurred. Equivalent signalling mechanisms and agency do not exist in algorithmic systems. Compared to traditional forms of discrimination, automated discrimination is more abstract and unintuitive, subtle, intangible, and difficult to detect. The increasing use of algorithms disrupts traditional legal remedies and procedures for detection, investigation, prevention, and correction of discrimination which have predominantly relied upon intuition. Consistent assessment procedures that define a common standard for statistical evidence to detect and assess prima facie automated discrimination are urgently needed to support judges, regulators, system controllers and developers, and claimants. Finally, we examine how existing work on fairness in machine learning lines up with procedures for assessing cases under EU non-discrimination law. A \u2018gold standard\u2019 for assessment of prima facie discrimination has been advanced by the European Court of Justice but not yet translated into standard assessment procedures for automated discrimination. We propose \u2018conditional demographic disparity\u2019 (CDD) as a standard baseline statistical measurement that aligns with the Court's \u2018gold standard\u2019. Establishing a standard set of statistical evidence for automated discrimination cases can help ensure consistent procedures for assessment, but not judicial interpretation, of cases involving AI and automated systems. Through this proposal for procedural regularity in the identification and assessment of automated discrimination, we clarify how to build considerations of fairness into automated systems as far as possible while still respecting and enabling the contextual approach to judicial interpretation practiced under EU non-discrimination law.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,815"
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "hidingsensitiveinformationinehealthdatasets",
        "booktitle": null,
        "doi": "10.1016/j.future.2020.11.026",
        "author": [
            "Wu, Jimmy",
            "Srivastava, Gautam",
            "Jolfaei, Alireza",
            "Fournier-Viger, Philippe",
            "Lin, Jerry"
        ],
        "keywords": [
            "Privacy, Preserving, Data mining, eHealth, Dynamic threshold, Sensitive, Evolutionary computation"
        ],
        "abstract": "Privacy in the realm of data mining known as PPDM has become a hot topic in both academic research and industry due to the fact it can discover implicit rules as well as hide sensitive information for data sanitization. Many different algorithms and heuristics have been investigated to hide sensitive information using the act of transaction deletion based on evolutionary computation techniques, but to date, these algorithms only consider a uniform threshold value for sanitization progress. This technique is not applicable in real-world situations, especially for eHealth based medical datasets. For example, a patient can still be identified if he/she has more confidential information (i.e., symptoms) that cause privacy threats and security leakage in medical applications. In this work, we investigate a unique novel methodology to set varied threshold values that lead to varied lengths of sensitive patterns within a Genetic Algorithm (GA)-based framework. As the pattern length increases, a tighter threshold manifests to provide better protection of sensitive information that can avoid individual patients to be identified in eHealth datasets. Two GA-based models are developed for data sanitization using record deletion techniques. The experimental results are conducted and compared with the traditional Evolutionary Computation (EC)-based PPDM approaches and the results showed that the designed methods offer greater protection than previous methods in terms of side effects. Therefore, the designed models are effective to hide sensitive information in medical situations that can be used in real-world scenarios.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-08-102672-4",
        "journal": null,
        "publisher": "Elsevier",
        "title": "sensorsanddatadrivenapproachesintransport",
        "booktitle": "International Encyclopedia of Transportation",
        "doi": "10.1016/B978-0-08-102671-7.10790-0",
        "author": [
            "Sadrani, Mohammad",
            "Antoniou, Constantinos"
        ],
        "keywords": [
            "Driver behavior monitoring, Machine learning methods, Origin\u2013destination matrices estimation, Safety analysis, Sensors, Sensor fusion approaches, Smartphone-based sensors, Traffic data, Traffic network surveillance, Transportation mode inference, Travel time estimation"
        ],
        "abstract": "Sensors play an important role in collecting real-time information for transportation systems. Nowadays, several different sensor technologies, ranging from traditional ones to mobile sensors in smartphones, are being used to collect a massive data volume on the real-time location and dynamics of users. For example, most of the modern smartphones are equipped with multiple motion sensors, such as accelerometer and magnetometer sensors, which can provide an unprecedented opportunity for the monitoring of the motion status of mobile phone users. On the other hand, there are a wide variety of data mining and prediction techniques, which can support transportation researchers in analyzing raw travel data collected from sensor technologies. This article provides a review of various applications of sensor technologies in transport networks, including travel time estimation, origin\u2013destination matrices estimation, safety analysis, driver behavior monitoring, and transportation mode inference.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02632241",
        "isbn": null,
        "journal": "Measurement",
        "publisher": null,
        "title": "measurementofmultimodalphysiologicalsignalsforstimulationdetectionbywearabledevices",
        "booktitle": null,
        "doi": "10.1016/j.measurement.2021.109966",
        "author": [
            "Cosoli, Gloria",
            "Poli, Angelica",
            "Scalise, Lorenzo",
            "Spinsante, Susanna"
        ],
        "keywords": [
            "Acoustic stimulation detection, Wearable devices, Measurement systems, Multimodal physiological signals, Features selection, Machine learning"
        ],
        "abstract": "The presence of stimuli and the consequent reactions undoubtedly reflect in experience-related changes of physiological parameters, which can be monitored by wearable devices. Generally, reactions related to the sympathetic nervous system activity are assessed through heart rate variability analysis. However, the exploitation of multimodal physiological signals provides a broader fingerprint. This study aims to identify the elicitation of acoustic stimulation through a wearable device; physiological signals, including electrodermal activity and skin temperature, were measured on a test population wearing a wrist-worn medical device. Eight machine learning algorithms were evaluated in a binary classification (presence/absence of stimuli), using 22 meaningful metrics from the collected data. The experimental results showed that Linear Regression (LR) algorithm, followed by Support Vector Machine (SVM), performed satisfactorily across all the evaluation metrics, achieving 75.00% and 72.62% of accuracy rate, respectively. Finally, the trained LR and SVM algorithms have been validated on a publicly available dataset (WESAD).",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,772"
    },
    {
        "issnkey": "18755100",
        "isbn": null,
        "journal": "Journal of Natural Gas Science and Engineering",
        "publisher": null,
        "title": "acriticalreviewofdistributedfiberopticsensingforrealtimemonitoringgeologicco2sequestration",
        "booktitle": null,
        "doi": "10.1016/j.jngse.2020.103751",
        "author": [
            "Sun, Yankun",
            "Liu, Jinquan",
            "Xue, Ziqiu",
            "Li, Qi",
            "Fan, Chengkai",
            "Zhang, Xu"
        ],
        "keywords": [
            "Distributed fiber-optic sensing, Geologic CO sequestration, Brillouin- Rayleigh backscattering, Strain response, Temperature profile, Microseismicity detection"
        ],
        "abstract": "Geologic CO2 sequestration (GCS) has been identified as the most viable option for effectively reducing greenhouse gases emissions to mitigate global warming and worldwide climate change. However, CO2 injection into subsurface can induce reservoir expansion and fault reactivation, which ultimately result in near-surface infrastructure damage and personnel insecurity. Distributed fiber optic sensing (DFOS) technologies function one single fiber as an array of sensors to in-situ monitor multi-parameters, such as geomechanical deformation (i.e., strain), temperature, acoustics and pressure along the entire fiber or cable length. Due to its superiority over conventional geophone and detector, DFOS tool possesses great potential to sense geofluid injection-induced small disturbances in deep subsurface. Here we begin by highlighting recent research efforts in available monitoring tools employed in GCS sites. Given the increasing attentions of optical sensing, we present a first-hand review of DFOS categories, sensing principles, and advantages for GCS related investigations from both laboratory and field scales. We discuss in detail three typical DFOS-deployed GCS projects and explore the implicit findings to guide subsequent GCS field applications. Finally, we summarize the major challenges and going forward in developing, utilizing, and extending DFOS systems to widely apply for the future large-scale all-optical GCS monitoring sites.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.965",
        "scimago_value": "1,079"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-819664-9",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter16designanddevelopmentofiotbaseddecisionsupportsystemfordengueanalysisandpredictioncasestudyonsrilankancontext",
        "booktitle": "Healthcare Paradigms in the Internet of Things Ecosystem",
        "doi": "10.1016/B978-0-12-819664-9.00016-8",
        "author": [
            "Ilmudeen, Aboobucker"
        ],
        "keywords": [
            "Decision support system, Dengue, Disease analysis and prediction, Fuzzy Rule Neural Classification, Internet of Things"
        ],
        "abstract": "Dengue fever is an epidemic viral disease that is spread by various types of dengue viruses of the genus Aedes, primarily Aedes aegypti. Dengue epidemics are common in humid and subhumid areas of the world, mostly in cities and suburban regions. The old methods were delay in diagnosing and restricting the growth of dengue eruption. This chapter proposes a fresh approach in Fuzzy Rule\u2013based Neural Classification with Internet of Things (IoT), cloud computing, and fog computing to analyze and predict dengue outbreak. The proposed fog-driven IoT architecture in which each component is seamlessly connected with each other to execute activities such as disease management, preventative care, clinical monitoring, early warning systems, e-medicine, and drug and food recommender system. This IoT-based decision support system aims to stop, control, and enable forecasting of eruptions of dengue, facilitating medical officers the information and insights to handle the outbreak, well in advance.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "adigitaltwinframeworkforthesimulationandoptimizationofproductionsystems",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.128",
        "author": [
            "Ricondo, Itziar",
            "Porto, Alain",
            "Ugarte, Miriam"
        ],
        "keywords": [
            "digital twin, discrete-event simulation, monitoring, optimization, servitization"
        ],
        "abstract": "Industry 4.0 has raised the expectations on productivity, automation, and resource efficiency of manufacturing systems. This paper proposes a digital twin framework for the simulation and optimization of production lines and cells that can be used in the design and operation stages. The framework is supported by an architecture that connects manufacturing and machine tool data (digital shadow), the discrete event simulation model and the optimization engine, allowing for a variety of functionalities to plan and manage the production system. A use case is provided to demonstrate this framework, implemented in an automated line for the manufacturing of railway axles.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "25425196",
        "isbn": null,
        "journal": "The Lancet Planetary Health",
        "publisher": null,
        "title": "globalantibioticconsumptionandusageinhumans200018aspatialmodellingstudy",
        "booktitle": null,
        "doi": "10.1016/S2542-5196(21)00280-1",
        "author": [
            "Browne, Annie",
            "Chipeta, Michael",
            "Haines-Woodhouse, Georgina",
            "Kumaran, Emmanuelle",
            "Hamadani, Bahar",
            "Zaraa, Sabra",
            "Henry, Nathaniel",
            "Deshpande, Aniruddha",
            "Reiner, Robert",
            "Day, Nicholas",
            "Lopez, Alan",
            "Dunachie, Susanna",
            "Moore, Catrin",
            "Stergachis, Andy",
            "Hay, Simon",
            "Dolecek, Christiane"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Summary Background Antimicrobial resistance (AMR) is a serious threat to global public health. WHO emphasises the need for countries to monitor antibiotic consumption to combat AMR. Many low-income and middle-income countries (LMICs) lack surveillance capacity; we aimed to use multiple data sources and statistical models to estimate global antibiotic consumption. Methods In this spatial modelling study, we used individual-level data from household surveys to inform a Bayesian geostatistical model of antibiotic usage in children (aged <5 years) with lower respiratory tract infections in LMICs. Antibiotic consumption data were obtained from multiple sources, including IQVIA, WHO, and the European Surveillance of Antimicrobial Consumption Network (ESAC-Net). The estimates of the antibiotic usage model were used alongside sociodemographic and health covariates to inform a model of total antibiotic consumption in LMICs. This was combined with a single model of antibiotic consumption in high-income countries to produce estimates of antibiotic consumption covering 204 countries and 19 years. Findings We analysed 209 surveys done between 2000 and 2018, covering 284 045 children with lower respiratory tract infections. We identified large national and subnational variations of antibiotic usage in LMICs, with the lowest levels estimated in sub-Saharan Africa and the highest in eastern Europe and central Asia. We estimated a global antibiotic consumption rate of 14\u00b73 (95% uncertainty interval 13\u00b72\u201315\u00b76) defined daily doses (DDD) per 1000 population per day in 2018 (40\u00b72 [37\u00b72\u201343\u00b77] billion DDD), an increase of 46% from 9\u00b78 (9\u00b72\u201310\u00b75) DDD per 1000 per day in 2000. We identified large spatial disparities, with antibiotic consumption rates varying from 5\u00b70 (4\u00b78\u20135\u00b73) DDD per 1000 per day in the Philippines to 45\u00b79 DDD per 1000 per day in Greece in 2018. Additionally, we present trends in consumption of different classes of antibiotics for selected Global Burden of Disease study regions using the IQVIA, WHO, and ESAC-net input data. We identified large increases in the consumption of fluoroquinolones and third-generation cephalosporins in North Africa and Middle East, and south Asia. Interpretation To our knowledge, this is the first study that incorporates antibiotic usage and consumption data and uses geostatistical modelling techniques to estimate antibiotic consumption for 204 countries from 2000 to 2018. Our analysis identifies both high rates of antibiotic consumption and a lack of access to antibiotics, providing a benchmark for future interventions. Funding Fleming Fund, UK Department of Health and Social Care; Wellcome Trust; and Bill & Melinda Gates Foundation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,535"
    },
    {
        "issnkey": "15749541",
        "isbn": null,
        "journal": "Ecological Informatics",
        "publisher": null,
        "title": "anopenscienceapproachtoinferfishingactivitypressureonstocksandbiodiversityfromvesseltrackingdata",
        "booktitle": null,
        "doi": "10.1016/j.ecoinf.2021.101384",
        "author": [
            "Coro, Gianpaolo",
            "Ellenbroek, Anton",
            "Pagano, Pasquale"
        ],
        "keywords": [
            "Vessel transmitted information, Vessel tracking data, Automatic Identification System, Statistical analysis, e-Infrastructures, Open Science, Biodiversity, Integrated Environmental Assessment"
        ],
        "abstract": "Vessel tracking data help study the potential impact of fisheries on biodiversity and produce risk assessments. Existing workflows process vessel tracks to identify fishing activity and integrate information on species vulnerability. However, there are significant data integration challenges across the data sources needed for an integrated impact assessment due to heterogeneous nomenclatures, data accessibility issues, geographical and computational scalability of the processes, and confidentiality and transparency towards decision making authorities. This paper presents an Open Science data integration approach to use vessel tracking data in integrated impact assessments. Our approach combines heterogeneous knowledge sources from fisheries, biodiversity, and environmental observations to infer fishing activity and risks to potentially impacted species. An Open Science e-Infrastructure facilitates access to data sources and maximises the reproducibility of the results and the method's reusability across several application domains. Our method's quality is assessed through three case studies: The first demonstrates cross-dataset consistency by comparing the results obtained from two different vessel data sources. The second performs a temporal pattern analysis of fishing activity and potentially impacted species over time. The third assesses the potential impact of reduced fishing pressure on marine biodiversity and threatened species due to the 2020 COVID-19 lockdown in Italy. The method is meant to be integrated with other systems through its Open Science-oriented features and can rapidly use new sources of findable, accessible, interoperable, and reusable (FAIR) data. Other systems can use it to (i) classify vessel activity in data-limited scenarios, (ii) identify bycatch species (when catchability data are available), and (iii) study the effects of fisheries on habitats and populations\u2019 growth.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.142",
        "scimago_value": "0,774"
    },
    {
        "issnkey": "24680141",
        "isbn": null,
        "journal": "Horticultural Plant Journal",
        "publisher": null,
        "title": "highthroughputphysiologybasedstressresponsephenotypingadvantagesapplicationsandprospectiveinhorticulturalplants",
        "booktitle": null,
        "doi": "10.1016/j.hpj.2020.09.004",
        "author": [
            "Li, Yanwei",
            "Wu, Xinyi",
            "Xu, Wenzhao",
            "Sun, Yudong",
            "Wang, Ying",
            "Li, Guojing",
            "Xu, Pei"
        ],
        "keywords": [
            "Phenomics, Physiolomics, Isohydric/anisohydric, Abiotic stress"
        ],
        "abstract": "Phenomics is a new branch of science that provides high-throughput quantification of plant and animal traits at systems level. The last decade has witnessed great successes in high-throughput phenotyping of numerous morphological traits, yet major challenges still exist in precise phenotyping of physiological traits such as transpiration and photosynthesis. Due to the highly dynamic nature of physiological traits in responses to the environment, appropriate selection criteria and efficient screening systems at the physiological level for abiotic stress tolerance have been largely absent in plants. In this review, the current status of phenomics techniques was briefly summarized in horticultural plants. Specifically, the emerging field of high-throughput physiology-based phenotyping, which is referred to as \u201cphysiolomics\u201d, for drought stress responses was highlighted. In addition to analyzing the advantages of physiology-based phenotyping over morphology-based approaches, recent examples that applied high-throughput physiological phenotyping to model and non-model horticultural plants were revisited and discussed. Based on the collective findings, we propose that high-throughput, non-destructive, and automatic physiological assays can and should be used as routine methods for phenotyping stress response traits in horticultural plants.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01406736",
        "isbn": null,
        "journal": "The Lancet",
        "publisher": null,
        "title": "thelancetandfinancialtimescommissionongoverninghealthfutures2030growingupinadigitalworld",
        "booktitle": null,
        "doi": "10.1016/S0140-6736(21)01824-9",
        "author": [
            "Kickbusch, Ilona",
            "Piselli, Dario",
            "Agrawal, Anurag",
            "Balicer, Ran",
            "Banner, Olivia",
            "Adelhardt, Michael",
            "Capobianco, Emanuele",
            "Fabian, Christopher",
            "{Singh Gill}, Amandeep",
            "Lupton, Deborah",
            "Medhora, Rohinton",
            "Ndili, Njide",
            "Ry\u015b, Andrzej",
            "Sambuli, Nanjira",
            "Settle, Dykki",
            "Swaminathan, Soumya",
            "Morales, Jeanette",
            "Wolpert, Miranda",
            "Wyckoff, Andrew",
            "Xue, Lan",
            "Bytyqi, Aferdita",
            "Franz, Christian",
            "Gray, Whitney",
            "Holly, Louise",
            "Neumann, Micaela",
            "Panda, Lipsa",
            "Smith, Robert",
            "{Georges Stevens}, Enow",
            "Wong, Brian"
        ],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "13,103"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820595-2",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter17decipheringtheanimalgenomicsusingbioinformaticsapproaches",
        "booktitle": "Advances in Animal Genomics",
        "doi": "10.1016/B978-0-12-820595-2.00017-5",
        "author": [
            "Usha, Talambedu",
            "Panda, Prachurjya",
            "Goyal, Arvind",
            "Sukhralia, Shivani",
            "Afreen, Sarah",
            "{Prashanth Kumar}, H.P.",
            "Shanmugarajan, Dhivya",
            "Middha, Sushil"
        ],
        "keywords": [
            "Animal genomes, Food, India, Sequencing techniques"
        ],
        "abstract": "Animal genomics is gaining popularity among researchers due to its utility-driven approaches. The major advantage lies in the understanding of how genes function and get expressed within various animal populations. Genomic understanding can propionate the thriving yield in farm animals to bioengineer innovative materials, enhanced productivity of livestock, xenotransplantation, and several other animal-based populous items for consumptions and even nonconsumption-based products like fabrics, silk. In this chapter, we present an introductory commentary on techniques and databases available to deduce animal genomes, a rapidly developing genome project resource, completed genomes summary of various domestic animals such as buffalo, sheep, and goat and the latest progress in the field. We have also flagged a concern with regard to resources and updates concerning farm livestock genome projects, especially in India, as compared to growing population and food demands across the globe within subsequent decades.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "18789293",
        "isbn": null,
        "journal": "Developmental Cognitive Neuroscience",
        "publisher": null,
        "title": "opportunitiesforincreasedreproducibilityandreplicabilityofdevelopmentalneuroimaging",
        "booktitle": null,
        "doi": "10.1016/j.dcn.2020.100902",
        "author": [
            "Klapwijk, Eduard",
            "{van den Bos}, Wouter",
            "Tamnes, Christian",
            "Raschle, Nora",
            "Mills, Kathryn"
        ],
        "keywords": [
            "Development, Open science, Sample size, Cognitive neuroscience, Transparency, Preregistration"
        ],
        "abstract": "Many workflows and tools that aim to increase the reproducibility and replicability of research findings have been suggested. In this review, we discuss the opportunities that these efforts offer for the field of developmental cognitive neuroscience, in particular developmental neuroimaging. We focus on issues broadly related to statistical power and to flexibility and transparency in data analyses. Critical considerations relating to statistical power include challenges in recruitment and testing of young populations, how to increase the value of studies with small samples, and the opportunities and challenges related to working with large-scale datasets. Developmental studies involve challenges such as choices about age groupings, lifespan modelling, analyses of longitudinal changes, and data that can be processed and analyzed in a multitude of ways. Flexibility in data acquisition, analyses and description may thereby greatly impact results. We discuss methods for improving transparency in developmental neuroimaging, and how preregistration can improve methodological rigor. While outlining challenges and issues that may arise before, during, and after data collection, solutions and resources are highlighted aiding to overcome some of these. Since the number of useful tools and techniques is ever-growing, we highlight the fact that many practices can be implemented stepwise.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.464",
        "scimago_value": "2,662"
    },
    {
        "issnkey": "10050302",
        "isbn": null,
        "journal": "Journal of Materials Science & Technology",
        "publisher": null,
        "title": "discoveryofmarageingsteelsmachinelearningvsphysicalmetallurgicalmodelling",
        "booktitle": null,
        "doi": "10.1016/j.jmst.2021.02.017",
        "author": [
            "Shen, Chunguang",
            "Wang, Chenchong",
            "Rivera-D\u00edaz-del-Castillo, Pedro",
            "Xu, Dake",
            "Zhang, Qian",
            "Zhang, Chi",
            "Xu, Wei"
        ],
        "keywords": [
            "Machine learning, Physical metallurgy, Small sample problem, Marageing steel"
        ],
        "abstract": "Physical metallurgical (PM) and data-driven approaches can be independently applied to alloy design. Steel technology is a field of physical metallurgy around which some of the most comprehensive understanding has been developed, with vast models on the relationship between composition, processing, microstructure and properties. They have been applied to the design of new steel alloys in the pursuit of grades of improved properties. With the advent of rapid computing and low-cost data storage, a wealth of data has become available to a suite of modelling techniques referred to as machine learning (ML). ML is being emergingly applied in materials discovery while it requires data mining with its adoption being limited by insufficient high-quality datasets, often leading to unrealistic materials design predictions outside the boundaries of the intended properties. It is therefore required to appraise the strength and weaknesses of PM and ML approach, to assess the real design power of each towards designing novel steel grades. This work incorporates models and datasets from well-established literature on marageing steels. Combining genetic algorithm (GA) with PM models to optimise the parameters adopted for each dataset to maximise the prediction accuracy of PM models, and the results were compared with ML models. The results indicate that PM approaches provide a clearer picture of the overall composition-microstructure-properties relationship but are highly sensitive to the alloy system and hence lack on exploration ability of new domains. ML conversely provides little explicit physical insight whilst yielding a stronger prediction accuracy for large-scale data. Hybrid PM/ML approaches provide solutions maximising accuracy, while leading to a clearer physical picture and the desired properties.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,743"
    },
    {
        "issnkey": "03787788",
        "isbn": null,
        "journal": "Energy and Buildings",
        "publisher": null,
        "title": "anovelshorttermloadforecastingframeworkbasedontimeseriesclusteringandearlyclassificationalgorithm",
        "booktitle": null,
        "doi": "10.1016/j.enbuild.2021.111375",
        "author": [
            "Chen, Zhe",
            "Chen, Yongbao",
            "Xiao, Tong",
            "Wang, Huilong",
            "Hou, Pengwei"
        ],
        "keywords": [
            "Short-term load forecasting, Light gradient boosting machine (LightGBM), Time series clustering, Early classification, Feature engineering"
        ],
        "abstract": "With the development of data-driven models, extracting information from historical data for better energy forecasting is critically important for energy planning and optimization in buildings. Feature engineering is a key factor in improving the performance of forecasting models. Adding load pattern labels for different daily energy consumption patterns resulting from different time schedules and weather conditions can help improve forecasting accuracy. Traditionally, pattern labeling focuses mainly on finding a day similar to the forecasting day based on calendar or other information, such as weather conditions. The most intuitive approach for dividing historical time-series load into patterns is clustering; however, the pattern cannot be determined before the load is known. To address this problem, this study proposes a novel short-term load forecasting framework integrating an early classification algorithm that uses a stochastic algorithm to predetermine the load pattern of a forecasting day. In addition, a hybrid multistep method combining the strengths of single-step forecasting and recursive multistep forecasting is integrated into the framework. The proposed framework was validated through a case study using actual metered data. The results demonstrate that the early classification and proposed labeling strategy produce satisfactory forecasting accuracy and significantly improve the forecasting performance of the LightGBM model.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.879",
        "scimago_value": "1,737"
    },
    {
        "issnkey": "00150282",
        "isbn": null,
        "journal": "Fertility and Sterility",
        "publisher": null,
        "title": "internationalcommitteeformonitoringassistedreproductivetechnologiesicmartworldreportonassistedreproductivetechnologies2013",
        "booktitle": null,
        "doi": "10.1016/j.fertnstert.2021.03.039",
        "author": [
            "Banker, Manish",
            "Dyer, Silke",
            "Chambers, Georgina",
            "Ishihara, Osamu",
            "Kupka, Markus",
            "{de Mouzon}, Jacques",
            "Zegers-Hochschild, Fernando",
            "Adamson, G."
        ],
        "keywords": [
            "Assisted reproductive technology, IVF/ICSI outcome, frozen embryo transfer, ICMART, cumulative live birth rate, registry"
        ],
        "abstract": "Objective To report the utilization, effectiveness, and safety of practices in assisted reproductive technology (ART) globally in 2013 and assess global trends over time. Design Retrospective, cross-sectional survey on the utilization, effectiveness, and safety of ART procedures performed globally during 2013. Setting Seventy-five countries and 2,639 ART clinics. Patient(s) Women and men undergoing ART procedures. Intervention(s) All ART. Main Outcome Measure(s) The ART cycles and outcomes on country-by-country, regional, and global levels. Aggregate country data were processed and analyzed based on methods developed by the International Committee for Monitoring Assisted Reproductive Technology (ICMART). Result(s) A total of 1,858,500 ART cycles were conducted for the treatment year 2013 across 2,639 clinics in 75 participating countries with a global participation rate of 73.6%. Reported and estimated data suggest 1,160,474 embryo transfers (ETs) were performed resulting in >344,317 babies. From 2012 to 2013, the number of reported aspiration and frozen ET cycles increased by 3% and 16.4%, respectively. The proportion of women aged >40 years undergoing nondonor ART increased from 25.2% in 2012 to 26.3% in 2013. As a percentage of nondonor aspiration cycles, intracytoplasmic sperm injection (ICSI) was similar to results for 2012. The in vitro fertilization (IVF)/ICSI combined delivery rates per fresh aspiration and frozen ET cycles were 24.2% and 22.8%, respectively. In fresh nondonor cycles, single ET increased from 33.7% in 2012 to 36.5% in 2013, whereas the average number of transferred embryos was 1.81\u2014again with wide country variation. The rate of twin deliveries after fresh nondonor transfers was 17.9%; the triplet rate was 0.7%. In frozen ET cycles performed in 2013, single ET was used in 57.6%, with an average of 1.49 embryos transferred and twin and triplet rates of 10.8% and 0.4%, respectively. The cumulative delivery rate per aspiration was 30.4%, similar to that in 2012. Perinatal mortality rate per 1,000 births was 22.2% after fresh IVF/ICSI and 16.8% after frozen ET. The data presented depended on the quality and completeness of the data submitted by individual countries. This report covers approximately two-thirds of world ART activity. Continued efforts to improve the quality and consistency of reporting ART data by registries are still needed. Conclusion(s) Reported ART cycles, effectiveness, and safety increased between 2012 and 2013 with adoption of a better method for estimating unreported cycles. Comit\u00e9 Internacional para la monitorizaci\u00f3n de las Tecnolog\u00edas de Reproducci\u00f3n Asistida (ICMART): informe mundial. Objetivo Informar sobre la utilizaci\u00f3n, la eficacia y la seguridad de las pr\u00e1cticas de la tecnolog\u00eda de reproducci\u00f3n asistida (TRA) a nivel mundial en 2013 y evaluar las tendencias mundiales a lo largo del tiempo. Dise\u00f1o Encuesta retrospectiva y transversal sobre la utilizaci\u00f3n, la eficacia y la seguridad de los procedimientos de TRA realizados a nivel mundial durante el a\u00f1o 2013. Entorno Setenta y cinco pa\u00edses y 2639 cl\u00ednicas de TRA. Pacientes Mujeres y hombres sometidos a procedimientos de TRA. Intervenci\u00f3n(es) Todas las TRA. Medida(s) principal(es) del resultado Los ciclos de TRA y los resultados a nivel de pa\u00eds, regional y mundial. Los datos agregados de los pa\u00edses se procesaron y analizaron seg\u00fan los m\u00e9todos desarrollados por el Comit\u00e9 Internacional para la Vigilancia de las Tecnolog\u00edas de Reproducci\u00f3n Asistida (ICMART). Tecnolog\u00eda de Reproducci\u00f3n Asistida (ICMART). Resultados En el a\u00f1o de tratamiento 2013 se realizaron un total de 1.858.500 ciclos de TRA en 2.639 cl\u00ednicas de 75 pa\u00edses participantes con una tasa de participaci\u00f3n global del 73,6%. Los datos informados y estimados sugieren que se realizaron 1.160.474 transferencias de embriones (TE) que dieron lugar a >344.317 beb\u00e9s. De 2012 a 2013, el n\u00famero de ciclos de aspiraci\u00f3n y de TE congelados notificados aument\u00f3 un 3% y un 16,4%, respectivamente. La proporci\u00f3n de mujeres de >40 a\u00f1os que se sometieron a TRA aut\u00f3loga aument\u00f3 del 25,2% en 2012 al 26,3% en 2013. Como porcentaje de ciclos de aspiraci\u00f3n aut\u00f3loga, la inyecci\u00f3n intracitoplasm\u00e1tica de espermatozoides (ICSI) fue similar a los resultados de 2012. Las tasas de parto combinadas de fecundaci\u00f3n in vitro (FIV)/ICSI por ciclos de aspiraci\u00f3n en fresco y TE congelada fueron del 24,2% y el 22,8%, respectivamente. En los ciclos aut\u00f3logos en fresco,la TE \u00fanica aument\u00f3 del 33,7% en 2012 al 36,5% en 2013, mientras que el n\u00famero medio de embriones transferidos fue de 1,81 -de nuevo, con una amplia variaci\u00f3n entre pa\u00edses. La tasa de partos gemelares tras transferencias aut\u00f3logas en fresco fue del 17,9%; la tasa de trillizos fue del 0,7%. En los ciclos de TE de congelados realizados en 2013, se utiliz\u00f3 la TE simple en el 57,6%, con una media de 1,49 embriones transferidos y unas tasas de gemelos y trillizos del 10,8% y 0,4%, respectivamente. La tasa acumulada de partos por aspiraci\u00f3n fue del 30,4%, similar a la de 2012. La tasa de mortalidad perinatal por cada 1.000 nacimientos fue del 22,2% tras la FIV/ICSI en fresco y del 16,8% tras la TE de congelados. Los datos presentados depend\u00edan de la calidad y la exhaustividad de los datos presentados por cada pa\u00eds. Este informe abarca aproximadamente dos tercios de la actividad mundial de TRA. Los esfuerzos continuos para mejorar la calidad y la coherencia de los datos presentados por los registros sobre la TRA deben seguir siendo objeto de esfuerzos continuos. Conclusi\u00f3n(es) Los ciclos de TRA notificados, la eficacia y la seguridad aumentaron entre 2012 y 2013 con la adopci\u00f3n de un mejor m\u00e9todo para estimar los ciclos no reportados.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13591789",
        "isbn": null,
        "journal": "Aggression and Violent Behavior",
        "publisher": null,
        "title": "evaluationofoccupationalstressmanagementforimprovingperformanceandproductivityatworkplacesbymonitoringthehealthwellbeingofworkers",
        "booktitle": null,
        "doi": "10.1016/j.avb.2021.101713",
        "author": [
            "Chen, Ming",
            "Ran, Bin",
            "Gao, Xiaoying",
            "Yu, Guilan",
            "Wang, Jing",
            "Jagannathan, J."
        ],
        "keywords": [
            "Occupational stress, Improving performance, Productivity, Stress, Health, Information technology"
        ],
        "abstract": "Competence lack, inadequate social support at work leads to the inability of workers since they are suffering from occupational stress. This will cause distress, burnout or psychosomatic difficulties, decreases in quality of life and service provision. Some of them may connect to work in an individual's personal life, both as managers, recognize stressors in their department, and respond on a departmental basis or individually. Many workers say that their employee utilization monitoring is not sufficient until computer counting involves. In addition, the systems are associated with higher stress, health hazards, and work unhappiness among supervised personnel. Monitoring these problems can increase employee awareness of personal productivity, providing performance information more promptly and frequently. Interventions are based on an examination of the variables that impact the performance of health workers. The article for employee stress management and health monitoring using information technology (SMHM-IT) gives better working conditions, motivation, retention, etc. Evaluation of occupational risks is a framework introduced to manage health and safety implications associated with preventative measures for improving and protecting the highest physical, social, or emotional working skills. Statistical data analysis is introduced to compare a medical specialty which includes analysis of employee's details. Results are compared with assessments shows that architecture offers successful in-time accessibility of performance 98.12% is achieved.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.382",
        "scimago_value": "1,586"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-823410-5",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter6aliteraturereviewonartificialintelligenceandethicsinonlinelearning",
        "booktitle": "Intelligent Systems and Learning Data Analytics in Online Education",
        "doi": "10.1016/B978-0-12-823410-5.00006-1",
        "author": [
            "Casas-Roma, Joan",
            "Conesa, Jordi"
        ],
        "keywords": [
            "artificial intelligence in education, artificial intelligence, data science, learning analytics, ethics, artificial morality, online learning"
        ],
        "abstract": "In recent years, artificial intelligence (AI) has been used in online learning to improve teaching and learning, with the aim of providing a more efficient, purposeful, adaptive, ubiquitous, and fair learning experiences. However, and as it has been seen in other contexts, the integration of AI can have unforeseen consequences with detrimental effects which can result in unfair and discriminatory decisions. Therefore it is worth thinking about potential risks that learning environments integrating AI systems might pose. This work explores the intersections between AI, online learning, and ethics in order to understand the ethical concerns surrounding this crossroads. We review the main ethical challenges identified in the literature and distill a set of guidelines to support the ethical design and integration of AI systems in online learning environments. This should help ensure that online learning is how is meant to be: accessible, inclusive, fair, and beneficial to society.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13865056",
        "isbn": null,
        "journal": "International Journal of Medical Informatics",
        "publisher": null,
        "title": "theneedtoseparatethewheatfromthechaffinmedicalinformaticsintroducingacomprehensivechecklistfortheselfassessmentofmedicalaistudies",
        "booktitle": null,
        "doi": "10.1016/j.ijmedinf.2021.104510",
        "author": [
            "Cabitza, Federico",
            "Campagner, Andrea"
        ],
        "keywords": [
            "Medical artificial intelligence, Machine learning, Checklist, Quality auditing"
        ],
        "abstract": "This editorial aims to contribute to the current debate about the quality of studies that apply machine learning (ML) methodologies to medical data to extract value from them and provide clinicians with viable and useful tools supporting everyday care practices. We propose a practical checklist to help authors to self assess the quality of their contribution and to help reviewers to recognize and appreciate high-quality medical ML studies by distinguishing them from the mere application of ML techniques to medical data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "20958099",
        "isbn": null,
        "journal": "Engineering",
        "publisher": null,
        "title": "machinelearninginchemicalengineeringstrengthsweaknessesopportunitiesandthreats",
        "booktitle": null,
        "doi": "10.1016/j.eng.2021.03.019",
        "author": [
            "Dobbelaere, Maarten",
            "Plehiers, Pieter",
            "{Van de Vijver}, Ruben",
            "Stevens, Christian",
            "{Van Geem}, Kevin"
        ],
        "keywords": [
            "Artificial intelligence, Machine learning, Reaction engineering, Process engineering"
        ],
        "abstract": "Chemical engineers rely on models for design, research, and daily decision-making, often with potentially large financial and safety implications. Previous efforts a few decades ago to combine artificial intelligence and chemical engineering for modeling were unable to fulfill the expectations. In the last five years, the increasing availability of data and computational resources has led to a resurgence in machine learning-based research. Many recent efforts have facilitated the roll-out of machine learning techniques in the research field by developing large databases, benchmarks, and representations for chemical applications and new machine learning frameworks. Machine learning has significant advantages over traditional modeling techniques, including flexibility, accuracy, and execution speed. These strengths also come with weaknesses, such as the lack of interpretability of these black-box models. The greatest opportunities involve using machine learning in time-limited applications such as real-time optimization and planning that require high accuracy and that can build on models with a self-learning ability to recognize patterns, learn from data, and become more intelligent over time. The greatest threat in artificial intelligence research today is inappropriate use because most chemical engineers have had limited training in computer science and data analysis. Nevertheless, machine learning will definitely become a trustworthy element in the modeling toolbox of chemical engineers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.553",
        "scimago_value": "1,376"
    },
    {
        "issnkey": "00457906",
        "isbn": null,
        "journal": "Computers and Electrical Engineering",
        "publisher": null,
        "title": "improvedadaptivegraywolfgeneticalgorithmforphotovoltaicintelligentedgeterminaloptimalconfiguration",
        "booktitle": null,
        "doi": "10.1016/j.compeleceng.2021.107394",
        "author": [
            "Ge, Leijiao",
            "Liu, Jiaheng",
            "Wang, Bo",
            "Zhou, Yue",
            "Yan, Jun",
            "Wang, Ming"
        ],
        "keywords": [
            "Distributed photovoltaic, Photovoltaic intelligent edge terminal, Optimal configuration, Improved adaptive genetic algorithm"
        ],
        "abstract": "Photovoltaic (PV) intelligent edge terminals (IETs) integrate data acquisition, processing, storage and upload functions for intelligent operations of PV power stations. However, the cost of installing a PV IET at one PV station is relatively high. In order to achieve the goal of multiple distributed PV stations sharing one PV IET on the premise of ensuring reliability, the paper proposes a method for the optimal configuration of PV IETs. First of all, considering the economy and reliability of optimizing configuration of PV IET, a two-layer optimization model is established. After that, to solve the nonlinearity of the proposed model, an improved adaptive genetic algorithm and gray wolf optimization (IAGA-GWO) is proposed. Finally, through two application cases of PV IETs, it is proved that the optimized configuration method in this paper can reduce the cost under the premise of ensuring the reliability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,630"
    },
    {
        "issnkey": "00092797",
        "isbn": null,
        "journal": "Chemico-Biological Interactions",
        "publisher": null,
        "title": "applicationofartificialintelligencefordetectionofchemicobiologicalinteractionsassociatedwithoxidativestressanddnadamage",
        "booktitle": null,
        "doi": "10.1016/j.cbi.2021.109533",
        "author": [
            "Davidovic, Lazar",
            "Laketic, Darko",
            "Cumic, Jelena",
            "Jordanova, Elena",
            "Pantic, Igor"
        ],
        "keywords": [
            "Reactive oxygen species, Radiation, Machine learning, Non-coding DNA, Aging"
        ],
        "abstract": "In recent years, various AI-based methods have been developed in order to uncover chemico-biological interactions associated with DNA damage and oxidative stress. Various decision trees, bayesian networks, random forests, logistic regression models, support vector machines as well as deep learning tools, have great potential in the area of molecular biology and toxicology, and it is estimated that in the future, they will greatly contribute to our understanding of molecular and cellular mechanisms associated with DNA damage and repair. In this concise review, we discuss recent attempts to build machine learning tools for assessment of radiation \u2013 induced DNA damage as well as algorithms that can analyze the data from the most frequently used DNA damage assays in molecular biology. We also review recent works on the detection of antioxidant proteins with machine learning, and the use of AI-related methods for prediction and evaluation of noncoding DNA sequences. Finally, we discuss previously published research on the potential application of machine learning tools in aging research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.192",
        "scimago_value": "0,943"
    },
    {
        "issnkey": "22120955",
        "isbn": null,
        "journal": "Urban Climate",
        "publisher": null,
        "title": "urbanclimateandresiliencyasynthesisreportofstateoftheartandfutureresearchdirections",
        "booktitle": null,
        "doi": "10.1016/j.uclim.2021.100858",
        "author": [
            "Gonz\u00e1lez, Jorge",
            "Ramamurthy, Prathap",
            "Bornstein, Robert",
            "Chen, Fei",
            "Bou-Zeid, Elie",
            "Ghandehari, Masoud",
            "Luvall, Jeffrey",
            "Mitra, Chandana",
            "Niyogi, Dev"
        ],
        "keywords": [
            "Urban climate resiliency, Extreme urban weather, Climate adaptation, Modeling and observations of extreme urban weather, Knowledge transfer of urban climate data, Cyber-systems for urban climate and weather"
        ],
        "abstract": "The Urban Climate and Resiliency-Science Working Group (i.e., The WG) was convened in the summer of 2018 to explore the scientific grand challenges related to climate resiliency of cities. The WG leveraged the presentations at the 10th International Conference on Urban Climate (ICUC10) held in New York City (NYC) on 6\u201310 August 2018 as input forum. ICUC10 was a collaboration between the International Association of Urban Climate, American Meteorological Society, and World Meteorological Organization. It attracted more than 600 participants from more than 50 countries, resulting in close to 700 oral and poster presentations under the common theme of \u201cSustainable & Resilient Urban Environments\u201d. ICUC10 covered topics related to urban climate and weather processes with far-reaching implications to weather forecasting, climate change adaptation, air quality, health, energy, urban planning, and governance. This article provides a synthesis of the analysis of the current state of the art and of the recommendations of the WG for future research along each of the four Grand Challenges in the context of urban climate and weather resiliency; Modeling, Observations, Cyber-Informatics, and Knowledge Transfer & Applications.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.731",
        "scimago_value": "1,151"
    },
    {
        "issnkey": "10848045",
        "isbn": null,
        "journal": "Journal of Network and Computer Applications",
        "publisher": null,
        "title": "privacyprotectionamongthreeantitheticpartiesforcontextawareservices",
        "booktitle": null,
        "doi": "10.1016/j.jnca.2021.103115",
        "author": [
            "Huang, Yan",
            "Li, Wei",
            "Wang, Jinbao",
            "Cai, Zhipeng",
            "Bourgeois, Anu"
        ],
        "keywords": [
            "Privacy protection, Game theory, Context-aware services"
        ],
        "abstract": "The popularity of context-aware services is improving the quality of life, while raising serious privacy issues. In order for users to receive quality service, they are at risk of leaking private information by adversaries that are possibly eavesdropping on the data and/or by the untrusted service platform selling off its data to adversaries. Game theory has been utilized as a powerful tool to achieve privacy preservation by strategically balancing the trade-off between profit (service) and cost (data leakage) for the user. However, most of the existing schemes cannot fully exploit the power of game theory, as they fail to depict the mutual relationship between any two (of the three) parties involved: user, platform, and adversary. Existing schemes are also not always able to provide specific guidance for a user to reduce the impact of the joint threats from the platform and adversary. In this paper, we design a privacy-preserving game to quantify the three parties\u2019 concerns and capture interactions between any two of them. We also identify the best strategy for each party at a fine-grained level, i.e. specific settings, not simply binary choices. We validate the performance of our proposed game model through both a theoretical analysis and experiments.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13646613",
        "isbn": null,
        "journal": "Trends in Cognitive Sciences",
        "publisher": null,
        "title": "isittimetoputresttorest",
        "booktitle": null,
        "doi": "10.1016/j.tics.2021.09.005",
        "author": [
            "Finn, Emily"
        ],
        "keywords": [
            "resting state, task-based, functional connectivity, naturalistic tasks, brain\u2013behavior prediction"
        ],
        "abstract": "The so-called resting state, in which participants lie quietly with no particular inputs or outputs, represented a paradigm shift from conventional task-based studies in human neuroimaging. Our foray into rest was fruitful from both a scientific and methodological perspective, but at this point, how much more can we learn from rest on its own? While rest still dominates in many subfields, data from tasks have empirically demonstrated benefits, as well as the potential to provide insights about the mind in addition to the brain. I argue that we can accelerate progress in human neuroscience by de-emphasizing rest in favor of more grounded experiments, including promising integrated designs that respect the prominence of self-generated activity while offering enhanced control and interpretability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00344257",
        "isbn": null,
        "journal": "Remote Sensing of Environment",
        "publisher": null,
        "title": "amethodforlandsurfacetemperatureretrievalbasedonmodeldataknowledgedrivenanddeeplearning",
        "booktitle": null,
        "doi": "10.1016/j.rse.2021.112665",
        "author": [
            "Wang, Han",
            "Mao, Kebiao",
            "Yuan, Zijin",
            "Shi, Jiancheng",
            "Cao, Mengmeng",
            "Qin, Zhihao",
            "Duan, Sibo",
            "Tang, Bohui"
        ],
        "keywords": [
            "Land surface temperature (LST), Model-data-knowledge-driven, Deep learning, Geophysical logical reasoning, Expert knowledge"
        ],
        "abstract": "Most algorithms for land surface temperature (LST) retrieval depend on acquiring prior knowledge. To overcome this drawback, we propose a novel LST retrieval method based on model-data-knowledge-driven and deep learning, called the MDK-DL method. Based on the expert knowledge and radiation transfer model, we deduce LST retrieval mechanism and determine the best combination of the thermal infrared (TIR) bands of the sensor. Then, we use the radiation transfer model simulation and reliable satellite-ground data to establish a training and test database, and finally use the deep learning neural network for optimal computation. Three typical high-, medium- and low-spatial-resolution TIR remote sensing datasets (from Gaofen, the Moderate Resolution Imaging Spectroradiometer (MODIS), and Fengyun) are used for theoretical simulation and application analysis. The simulation shows that the minimum mean absolute error (MAE) is less than 0.1 K (standard deviation: 0.04 K; correlation coefficient: 1.000) at a small viewing direction (<7.5\u00b0) and less than 0.8 K at a large viewing direction (<65\u00b0). The in situ validation shows that the minimum MAE obtained by the optimal band combination is approximately 1 K (root mean square error (RMSE) = 1.12 K; coefficient of determination (R2) = 0.902). The retrieval accuracy is improved by increasing the number of TIR bands in the atmospheric window, and adding accurate atmospheric water vapor information produces better results. In general, four TIR bands in the atmospheric window bands are sufficient to retrieve the LST with high accuracy. Likewise, three TIR bands plus atmospheric water vapor information are sufficient for the retrieval requirements. All analyses indicate that our method is feasible and reliably accurate and can also be used to help design the instrument band to retrieve the LST with high precision.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.164",
        "scimago_value": "3,611"
    },
    {
        "issnkey": "",
        "isbn": "978-0-08-102672-4",
        "journal": null,
        "publisher": "Elsevier",
        "title": "mobilityasaservice",
        "booktitle": "International Encyclopedia of Transportation",
        "doi": "10.1016/B978-0-08-102671-7.10607-4",
        "author": [
            "Mladenovi\u0107, Milo\u0161"
        ],
        "keywords": [
            "Emerging technology, Mobility governance, Mobility on demand, Mobility system, Responsible innovation, Social justice, Transportation network company, Transport policy"
        ],
        "abstract": "Mobility as a Service (MaaS) is a relatively fast-growing emerging technology based on the vision of integration (incl., policy, operational, informational, and transactional levels) and customization in transport systems. The user is expected to receive information, book, and pay for a choice of different mobility services by accessing a \u201cone-stop-shop\u201d or \u201cmobility platform\u201d via digital interfaces. Although highly uncertain, MaaS has significant potential to exert a considerable impact on the socio-technical domains in and beyond mobility. Such implications are also in terms of the composition of actors, institutions, and patterns of interactions among those, along with the associated innovation processes. Technological transition from niche to regime for MaaS depends on associated rhetoric as well as a wider set of converging socio-technical factors of societal automation, digitalization, and reregulation. Ultimately, transport policy and governance institutions will have to reflect and act on the potential undesired consequences from the depolitization of MaaS technological development.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821472-5",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter8aresearchreviewonsemanticinteroperabilityissuesinelectronichealthrecordsystemsinmedicalhealthcare",
        "booktitle": "IoT-Based Data Analytics for the Healthcare Industry",
        "doi": "10.1016/B978-0-12-821472-5.00009-0",
        "author": [
            "Yadav, Rimmy",
            "Murria, Saniksha",
            "Sharma, Anil"
        ],
        "keywords": [
            "Medical healthcare, Electronic health record, Interoperability, Semantic interoperability"
        ],
        "abstract": "With constantly diminishing costs and prolonged effectiveness of wireless communication and transmission techniques and, importantly, the Internet of Things (IoT) emerging as a powerful technology, some aspects of our lives have changed and broadened. The healthcare sector in particular is a developing and highly demanding application sector. IoT contributions to the medical healthcare domain include remote health and monitoring services, care for the elderly, recognition as well as tactical management of chronic illnesses, and offering of adaptive and self-regulated medical facilities. In medical healthcare, electronic health record (EHR) systems provide efficient management of clinical records in today\u2019s clinical healthcare organizations. However, medical records are generating huge amounts of data, with every medical record having its own standard pattern, schema, and level of abstraction and interoperability. To interact with EHRs, medical stakeholders must use standard and well-structured methods and ontology-based languages to analyze and mine the useful information from huge data records. Much research has been done on interoperability issues, particularly syntactic interoperability and technical interoperability. After reviewing the research articles and chapters from respected medical databases such as IEEE Xplore, Elsevier, and Science Direct, the authors noted that semantic interoperability is, in the EHR framework, one of the critical issues. To achieve full semantic interoperability, researchers and scholars have developed and structured numerous methodologies, tools, and techniques. This research review thus includes methodologies, frameworks, tools, and models, along with their advantages and limitations, developed by researchers to cope with semantic interoperability issues in medical healthcare. Furthermore, in this chapter, the authors have focused on searching the papers related to the semantic web with a model-driven architecture (MDA) approach for semantic interoperability. Applications of the MDA approach and advanced features of the semantic web may be able to resolve the issue of semantic interoperability.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "25299123",
        "isbn": null,
        "journal": "Cl\u00ednica e Investigaci\u00f3n en Arteriosclerosis (English Edition)",
        "publisher": null,
        "title": "massivedatascreeningisasecondopportunitytoimprovethemanagementofpatientswithfamilialhypercholesterolemiaphenotype",
        "booktitle": null,
        "doi": "10.1016/j.artere.2020.11.007",
        "author": [
            "Zamora, Alberto",
            "Paluzie, Guillem",
            "Garc\u00eda-Vilches, Joan",
            "Gisbert, Oriol",
            "{M\u00e9ndez Mart\u00ednez}, Ana",
            "Plana, N\u00faria",
            "Rodr\u00edguez-Borjabad, C\u00e8lia",
            "Ibarretxe, Daiana",
            "Mart\u00edn-Urda, Anabel",
            "Masana, Luis"
        ],
        "keywords": [
            "Familial hypercholesterolemia, Massive data screening, Profiling of patients, Hipercolesterolemia familiar, Rastreo masivo de datos, Perfilado de pacientes"
        ],
        "abstract": "Introduction Familial Hypercholesterolemia (FH) is an autosomal dominant disease with an estimated prevalence between 1/200\u2013250. It is under-treated and underdiagnosed. Massive data screening can increase the detection of patients with FH. Methods Study population: Residents in the health coverage area (N: 195.000 inhabitants) and with at least one determination of cholesterol linked to low-density lipoproteins (LDLC) carried out between January 1, 2010 and December 30, 2019. The highest LDL-C values were selected. Exclusion criteria: nephrotic syndrome, hypothyroidism, Hypothyroid treatment or triglycerides > 400 mg/dL. Seven algorithms suggestive of Familial Hypercholesterolemia Phenotype (HF-P) were analyzed, selecting the most efficient algorithm that could easily be translated into clinical practice. Results Based on 6.264.877 assistances and 288.475 patients, after applying the inclusionexclusion criteria, 504.316 tests were included, corresponding to 106.382 adults and 10.509 < 18 years. The selected algorithm presented a prevalence of 0.62%. 840 patients with HF-P were detected, 55.8% being women and 178 < 18 years old, 9.3% had a history of cardiovascular disease (CVD) and 16.4% had died. 65% of the patients in primary prevention had LDL-C values > 130 mg/dL and 83% in secondary prevention values > 70 mg/dL. A ratio of 7.64 (1\u201318) patients with HF-P per analytical requesting physician was obtained. Conclusions Massive data screening and patient profiling are effective tools and easily applicable in clinical practice for the detection of patients with FH. Resumen Introducci\u00f3n La Hipercolesterolemia Familiar (HF) es una enfermedad aut\u00f3s\u00f3mica dominante con una prevalencia estimada entre 1/200\u2013250. Se encuentra infratratada e infradiagnosticada. El rastreo masivo de datos puede incrementar la detecci\u00f3n de pacientes con HF. M\u00e9todos Poblaci\u00f3n a estudio: Residentes en la zona sanitaria de cobertura (N: 195.000 habitantes) y con al menos una determinaci\u00f3n de colesterol ligado a lipoprote\u00ednas de baja densidad (C-LDL) realizada entre el 1 de Enero de 2010 y el 30 de Diciembre de 2019. Se seleccionaron los valores m\u00e1s altos de C-LDL. Criterios de exclusi\u00f3n: s\u00edndrome nefr\u00f3tico, hipotiroidismo, tratamiento hipotiroideo o triglic\u00e9ridos > 400 mg/dL. Se analizaron 7 algoritmos sugestivos de fenotipo de Hipercolesterolemia Familiar (FHF). Se seleccion\u00f3 el algoritmo m\u00e1s eficaz y de f\u00e1cil traslaci\u00f3n a la pr\u00e1ctica cl\u00ednica. Resultados Partiendo de 6.264.877 asistencias y 288.475 pacientes tras aplicar los criterios de inclusi\u00f3n-exclusi\u00f3n se incluyeron 504.316 anal\u00edticas correspondiendo a 106.382 adultos y 10.509 < 18 a\u00f1os.El algoritmo seleccionado present\u00f3 una prevalencia de 0.62%.Se detectaron 840 pacientes con fenotipo de Hipercolestereolemia Familiar (FHF) siendo el 55.8% mujeres y 178 < 18 a\u00f1os, El 9.3% ten\u00edan antecedentes de enfermedad cardio-vascular (ECV) y 16.4% hab\u00edan fallecido.El 65% de los pacientes en prevenci\u00f3n primaria presentaron valores de C-LDL > 130 mg/dL y el 83% en prevenci\u00f3n secundaria valores > 70 mg/dL.Se obtuvo una ratio de 7.64 (1\u201318) pacientes con HF-P por m\u00e9dico solicitante de anal\u00edtica. Conclusiones El rastreo masivo de datos y el perfilado de pacientes son herramientas eficaces y f\u00e1cilmente aplicables en pr\u00e1ctica cl\u00ednica para la detecci\u00f3n de pacientes con HF.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0308521x",
        "isbn": null,
        "journal": "Agricultural Systems",
        "publisher": null,
        "title": "machinelearningforlargescalecropyieldforecasting",
        "booktitle": null,
        "doi": "10.1016/j.agsy.2020.103016",
        "author": [
            "Paudel, Dilli",
            "Boogaard, Hendrik",
            "{de Wit}, Allard",
            "Janssen, Sander",
            "Osinga, Sjoukje",
            "Pylianidis, Christos",
            "Athanasiadis, Ioannis"
        ],
        "keywords": [
            "Crop yield prediction, Machine learning, Modularity, Reusability, Large-scale crop yield forecasting"
        ],
        "abstract": "Many studies have applied machine learning to crop yield prediction with a focus on specific case studies. The data and methods they used may not be transferable to other crops and locations. On the other hand, operational large-scale systems, such as the European Commission's MARS Crop Yield Forecasting System (MCYFS), do not use machine learning. Machine learning is a promising method especially when large amounts of data are being collected and published. We combined agronomic principles of crop modeling with machine learning to build a machine learning baseline for large-scale crop yield forecasting. The baseline is a workflow emphasizing correctness, modularity and reusability. For correctness, we focused on designing explainable predictors or features (in relation to crop growth and development) and applying machine learning without information leakage. We created features using crop simulation outputs and weather, remote sensing and soil data from the MCYFS database. We emphasized a modular and reusable workflow to support different crops and countries with small configuration changes. The workflow can be used to run repeatable experiments (e.g. early season or end of season predictions) using standard input data to obtain reproducible results. The results serve as a starting point for further optimizations. In our case studies, we predicted yield at regional level for five crops (soft wheat, spring barley, sunflower, sugar beet, potatoes) and three countries (the Netherlands (NL), Germany (DE), France (FR)). We compared the performance with a simple method with no prediction skill, which either predicted a linear yield trend or the average of the training set. We also aggregated the predictions to the national level and compared with past MCYFS forecasts. The normalized RMSE (NRMSE) for early season predictions (30 days after planting) were comparable for NL (all crops), DE (all except soft wheat) and FR (soft wheat, spring barley, sunflower). For example, NRMSE was 7.87 for soft wheat (NL) (6.32 for MCYFS) and 8.21 for sugar beet (DE) (8.79 for MCYFS). In contrast, NRMSEs for soft wheat (DE), sugar beet (FR) and potatoes (FR) were twice as much compared to MCYFS. NRMSEs for end of season were still comparable to MCYFS for NL, but worse for DE and FR. The baseline can be improved by adding new data sources, designing more predictive features and evaluating different machine learning algorithms. The baseline will motivate the use of machine learning in large-scale crop yield forecasting.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15472450",
        "isbn": null,
        "journal": "Journal of Intelligent Transportation Systems",
        "publisher": null,
        "title": "adatadrivenapproachtoassessingthereliabilityofusingtaxicabasprobesforrealtimerouteselections",
        "booktitle": null,
        "doi": "10.1080/15472450.2019.1617142",
        "author": [
            "Wang, Zheng",
            "Lin, Wei-Hua",
            "Xu, Wangtu"
        ],
        "keywords": [
            "Data driven approach, route choice, taxi service, travel time estimation"
        ],
        "abstract": "Taxi service is one of the most important modes for urban transportation. In recent years, many taxi companies have been routinely collecting data to track the movement of each taxi for improving security, coordination, and service performance. This paper is intended to use the GPS vehicle positioning data to assess the route choice behavior of taxi drivers and explore if the routes selected by taxi drivers can be incorporated into a traveler information system. It is often perceived that taxi drivers have the ability to select quality routes assuming that: (1) they tend to be more knowledgeable about alternative routes and time-dependent traffic conditions than general public, including some publicly available route guidance systems due to the nature of their profession; and (2) they are typically more motivated to incorporate their knowledge about traffic conditions into their route choice decisions. An experimental study is conducted to examine the validity of these two assumptions. We have developed a framework that can effectively process the data into information about routes selected by taxi drivers and their associated travel times. The performance of the routes selected by taxi drivers is compared with the performance of those recommended by e-maps. Our results indicate that the routes selected by taxi drivers are generally more efficient than the routes recommended by some major e-maps, suggesting that taxi drivers are more active in selecting routes to avoid congestion.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.277",
        "scimago_value": "1,321"
    },
    {
        "issnkey": "09666923",
        "isbn": null,
        "journal": "Journal of Transport Geography",
        "publisher": null,
        "title": "inferringthetrippurposesanduncoveringspatiotemporalactivitypatternsfromdocklesssharedbikedatasetinshenzhenchina",
        "booktitle": null,
        "doi": "10.1016/j.jtrangeo.2021.102974",
        "author": [
            "Li, Shaoying",
            "Zhuang, Caigang",
            "Tan, Zhangzhi",
            "Gao, Feng",
            "Lai, Zhipeng",
            "Wu, Zhifeng"
        ],
        "keywords": [
            "Activity inference, Gravity model, Bayesian rules, Travel patterns, Dockless shared bikes"
        ],
        "abstract": "Trip purpose is closely related to travel patterns and plays an important role in urban planning and transportation management. Recently, there has been a growing interest in investigating the spatio-temporal patterns of dockless shared-bike usage and its influencing mechanisms. Few, however, have focused on revealing the travel patterns by inferring the purpose of dockless shared-bike trips at the individual level. We present a framework for inferring the purpose of dockless shared-bike users, based on gravity model and Bayesian rules, and conduct it in Shenzhen, China. We consider the comprehensive factors including distance, time, environment, activity type proportion, and service capacity of points of interest (POIs), the last two factors of which were usually neglected in previous transport studies. Especially, we integrated areas of interest (AOIs) and Tencent User density (TUD) social media data characterize the service capacity of POIs, which reflect the area and scale differences of different POI categories. Through the comparison between two improved models and the basic model, it is demonstrated that the introduction of activity type proportion and service capacity of POIs can improve the effectiveness of model for inferring the purposes of dockless shared-bike trips. Based on the obtained trip purposes, we further explore the spatio-temporal patterns of different activities and gain some insights into bike travel demand, which can inform scientific decisions for bicycle infrastructure planning and dockless shared- bike management.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.986",
        "scimago_value": "1,809"
    },
    {
        "issnkey": "",
        "isbn": "978-0-08-102672-4",
        "journal": null,
        "publisher": "Elsevier",
        "title": "technologyenableddataforsustainabletransportpolicy",
        "booktitle": "International Encyclopedia of Transportation",
        "doi": "10.1016/B978-0-08-102671-7.10627-X",
        "author": [
            "Grant-Muller, Susan",
            "Abdelrazek, Mahmoud",
            "Budnitz, Hannah",
            "Cottrill, Caitlin",
            "Crawford, Fiona",
            "Choudhury, Charisma",
            "Cunningham, Teddy",
            "Harrison, Gillian",
            "Hodgson, Frances",
            "Hong, Jinhyun",
            "Martin, Adam",
            "O\u2019Brien, Oliver",
            "Papaix, Claire",
            "Tsoleridis, Panagiotis"
        ],
        "keywords": [
            "Big data, Ethics, Influencing technologies, New and emerging data forms, Policy, Transport modeling"
        ],
        "abstract": "The explosive growth of New and Emerging Data Forms (NEDF) has enabled profound new insights into human behavior, especially related to mobility. NEDF are facilitated by technologies such as smartphones, sensor networks and distributed computing architectures, which are all becoming increasingly advanced and widespread. NEDF, which may offer large sample sizes of high resolution data, offer great potential for informing sustainable transport policy, as well as the development of crosssectoral policies, covering public-health, environment, land-use, and social equity. However, many challenges in exploiting NEDF exist including accessing data in public/private ownership; understanding the representativeness, measuring/accommodating biases/missing data; and the integration of traditional data with new forms to maximize overall utility. Questions remain on whether NEDF can be used to actively influence travel choice/behavior, the new skills and additional resources needed by stakeholders to realize data potential and the ethical challenges for all engaging with the data.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22143173",
        "isbn": null,
        "journal": "Information Processing in Agriculture",
        "publisher": null,
        "title": "servicedesignforclimatesmartagriculture",
        "booktitle": null,
        "doi": "10.1016/j.inpa.2020.07.003",
        "author": [
            "O'Grady, Michael",
            "Langton, David",
            "Salinari, Francesca",
            "Daly, Peter",
            "O'Hare, Gregory"
        ],
        "keywords": [
            "Smart agriculture, Climate services, Agrometeorology, Precision agriculture"
        ],
        "abstract": "Holistic information systems for climate-smart agriculture demands the seamless integration of various categories of climate, meteorological and weather data. Any actor in the agricultural value chain may harness weather forecasts at the short and medium-range, local weather history, and prevailing climatic conditions, to inform decision-making. Weather is fundamental to many day-to-day operations, especially at farm-level, influencing decision-making at various spatial and temporal scales. Many operational decisions ideally require hyper-localized service provision. In practice, integrating weather information into decision-support services demands a comprehensive understanding of various categories of weather-related data, their genesis, as well as the specific standards and data formats used by the meteorological community. This paper considers the weather as a crucial context for the delivery of farm-level operational services in smart agriculture, highlighting critical issues for reflection by system designers during the service design and implementation phases.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,769"
    },
    {
        "issnkey": "24681113",
        "isbn": null,
        "journal": "Computational Toxicology",
        "publisher": null,
        "title": "cosmosnextgenerationapublicknowledgebaseleveragingchemicalandbiologicaldatatosupporttheregulatoryassessmentofchemicals",
        "booktitle": null,
        "doi": "10.1016/j.comtox.2021.100175",
        "author": [
            "Yang, C.",
            "Cronin, M.T.D.",
            "Arvidson, K.B.",
            "Bienfait, B.",
            "Enoch, S.J.",
            "Heldreth, B.",
            "Hobocienski, B.",
            "Muldoon-Jacobs, K.",
            "Lan, Y.",
            "Madden, J.C.",
            "Magdziarz, T.",
            "Marusczyk, J.",
            "Mostrag, A.",
            "Nelms, M.",
            "Neagu, D.",
            "Przybylak, K.",
            "Rathman, J.F.",
            "Park, J.",
            "Richarz, A-N",
            "Richard, A.M.",
            "Ribeiro, J.V.",
            "Sacher, O.",
            "Schwab, C.",
            "Vitcheva, V.",
            "Volarath, P.",
            "Worth, A.P."
        ],
        "keywords": [
            "Toxicity, Database, Public database, Knowledge hub, Study reliability, Analogue selection, Guided workflow"
        ],
        "abstract": "The COSMOS Database (DB) was originally established to provide reliable data for cosmetics-related chemicals within the COSMOS Project funded as part of the SEURAT-1 Research Initiative. The database has subsequently been maintained and developed further into COSMOS Next Generation (NG), a combination of database and in silico tools, essential components of a knowledge base. COSMOS DB provided a cosmetics inventory as well as other regulatory inventories, accompanied by assessment results and in vitro and in vivo toxicity data. In addition to data content curation, much effort was dedicated to data governance \u2013 data authorisation, characterisation of quality, documentation of meta information, and control of data use. Through this effort, COSMOS DB was able to merge and fuse data of various types from different sources. Building on the previous effort, the COSMOS Minimum Inclusion (MINIS) criteria for a toxicity database were further expanded to quantify the reliability of studies. COSMOS NG features multiple fingerprints for analysing structure similarity, and new tools to calculate molecular properties and screen chemicals with endpoint-related public profilers, such as DNA and protein binders, liver alerts and genotoxic alerts. The publicly available COSMOS NG enables users to compile information and execute analyses such as category formation and read-across. This paper provides a step-by-step guided workflow for a simple read-across case, starting from a target structure and culminating in an estimation of a NOAEL confidence interval. Given its strong technical foundation, inclusion of quality-reviewed data, and provision of tools designed to facilitate communication between users, COSMOS NG is a first step towards building a toxicological knowledge hub leveraging many public data systems for chemical safety evaluation. We continue to monitor the feedback from the user community at support@mn-am.com.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,754"
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "supportingdigitalcontentmarketingandmessagingthroughtopicmodellinganddecisiontrees",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2021.115546",
        "author": [
            "Gregoriades, Andreas",
            "Pampaka, Maria",
            "Herodotou, Herodotos",
            "Christodoulou, Evripides"
        ],
        "keywords": [
            "Topic modelling, Cultural and economic distance, Decision trees, Shapley additive explanation, Tourists\u2019 reviews"
        ],
        "abstract": "This paper presents a machine learning approach involving tourists\u2019 electronic word of mouth (eWOM) to support destination marketing campaigns. This approach enhances optimisation of a critical aspect of marketing campaigns, that is, the communication of the right content to the right consumers. The proposed method further considers aggregate cultural and economic-related information of the tourists\u2019 country of origin with topic modelling and Decision Tree (DT) models. Each DT addresses different dimensions of culture and purchasing power and the way these dimensions are associated with the topics discussed in eWOM, thus revealing patterns relating tourists\u2019 experiences with potential explanations for their dissatisfaction/satisfaction. The method is implemented in a case study in the context of tourism in Cyprus focusing on two hotel groups (2/3 and 4/5 stars) to account for their differences. Patterns emerged from the extraction of rules from DTs illuminate combinations of variables associated with tourist experience (negative or positive) for each of the two hotel categories and verify the asymmetric relationship between service performance and satisfaction. The approach can be used by management during marketing campaigns to design messages to better address the desires and needs of tourists from different cultural and economic backgrounds, as these emerge from the data analysis.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "09265805",
        "isbn": null,
        "journal": "Automation in Construction",
        "publisher": null,
        "title": "exploringsmartconstructionobjectsasblockchainoraclesinconstructionsupplychainmanagement",
        "booktitle": null,
        "doi": "10.1016/j.autcon.2021.103816",
        "author": [
            "Lu, Weisheng",
            "Li, Xiao",
            "Xue, Fan",
            "Zhao, Rui",
            "Wu, Liupengfei",
            "Yeh, Anthony"
        ],
        "keywords": [
            "Blockchain, Oracles, Smart contract, Supply chain management, Smart construction objects, Prefabricated construction"
        ],
        "abstract": "Blockchain technology has attracted the interest of the global construction industry for its potential to enhance the transparency, traceability, and immutability of construction data and enables collaboration and trust throughout the supply chain. However, such potential cannot be achieved without blockchain \u201coracles\u201d needed to bridge the on-chain (i.e., blockchain system) and off-chain (i.e., real-life physical project) worlds. This study presents an innovative solution that exploits smart construction objects (SCOs). It develops a SCOs-enabled blockchain oracles (SCOs-BOs) framework. To instantiate this framework, the system architecture of a blockchain-enabled construction supply chain management (BCSCM) system is developed and validated using a case study, whereby four primary smart contracts are examined in the context of off-site logistics and on-site assembly services. The validation results show that accurate data is retrieved against malicious data in each request, and the corresponding reputation scores are successfully recorded. The innovativeness of the research lies in two aspects. In addition to mobilizing SCOs as blockchain oracles to bridge the on-chain and off-chain worlds, it develops a decentralized SCO network to avoid the single point of failure (SPoF) problem widely existing in blockchain systems. This study contributes to existing research and practice to harness the power of blockchain in construction.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.700",
        "scimago_value": "1,837"
    },
    {
        "issnkey": "01656147",
        "isbn": null,
        "journal": "Trends in Pharmacological Sciences",
        "publisher": null,
        "title": "disrupting3dprintingofmedicineswithmachinelearning",
        "booktitle": null,
        "doi": "10.1016/j.tips.2021.06.002",
        "author": [
            "Elbadawi, Moe",
            "McCoubrey, Laura",
            "Gavins, Francesca",
            "Ong, Jun",
            "Goyanes, Alvaro",
            "Gaisford, Simon",
            "Basit, Abdul"
        ],
        "keywords": [
            "additive manufacturing, 3D Printed drug products and formulations, Industry 4.0 and digital health, personalized oral drug delivery systems and medical devices, biomedical engineering and pharmaceutical sciences, translational pharmaceutics"
        ],
        "abstract": "3D printing (3DP) is a progressive technology capable of transforming pharmaceutical development. However, despite its promising advantages, its transition into clinical settings remains slow. To make the vital leap to mainstream clinical practice and improve patient care, 3DP must harness modern technologies. Machine learning (ML), an influential branch of artificial intelligence, may be a key partner for 3DP. Together, 3DP and ML can utilise intelligence based on human learning to accelerate drug product development, ensure stringent quality control (QC), and inspire innovative dosage-form design. With ML\u2019s capabilities, streamlined 3DP drug delivery could mark the next era of personalised medicine. This review details how ML can be applied to elevate the 3DP of pharmaceuticals and importantly, how it can expedite 3DP\u2019s integration into mainstream healthcare.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01410296",
        "isbn": null,
        "journal": "Engineering Structures",
        "publisher": null,
        "title": "multifactorandmultilevelpredictivemodelsofbuildingnaturalperiod",
        "booktitle": null,
        "doi": "10.1016/j.engstruct.2021.112622",
        "author": [
            "Wang, Zetao",
            "Chen, Jun",
            "Shen, Jiaxu"
        ],
        "keywords": [
            "Natural period of building, Database, Maximal information coefficient, Kruskal\u2013Wallis ANOVA, Empirical formula, Confidence interval, Rational scope"
        ],
        "abstract": "A comprehensive database containing data on approximately 2700 buildings and 6000 full-scale measured period samples was constructed through massive literature searching and stringent data filtering. The newly emerged maximal information coefficient method, which is suitable for large data set statistical analysis, was adopted in conjunction with Kruskal\u2013Wallis analysis of variance to identify factors that significantly affect a building\u2019s fundamental period. It was quantitatively verified that height, predominant structural material, and lateral-force resisting system are the three most important influencing factors. Subsequently, height was used as the dominant regression variable, and material and lateral-force resisting system were used as categorical variables, predictive models in combination with confidence intervals of the fundamental period are provided for multi-factors, including four material types and three structural types. In addition, multi-level empirical formulas of the natural period in other five modes (two translational and three torsional) are provided on the basis of the regression results of the fundamental period. All these predictive models can effectively reflect the tendency of the median and the rational scope of variability of the natural period of buildings.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02684012",
        "isbn": null,
        "journal": "International Journal of Information Management",
        "publisher": null,
        "title": "aiapplicationsofdatasharinginagriculture40aframeworkforrolebaseddataaccesscontrol",
        "booktitle": null,
        "doi": "10.1016/j.ijinfomgt.2021.102350",
        "author": [
            "Spanaki, Konstantina",
            "Karafili, Erisa",
            "Despoudi, Stella"
        ],
        "keywords": [
            "Agriculture 4.0, Design science, Artificial intelligence, Data sharing, Role-based access control"
        ],
        "abstract": "Industry 4.0 and the associated IoT and data applications are evolving rapidly and expand in various fields. Industry 4.0 also manifests in the farming sector, where the wave of Agriculture 4.0 provides multiple opportunities for farmers, consumers and the associated stakeholders. Our study presents the concept of Data Sharing Agreements (DSAs) as an essential path and a template for AI applications of data management among various actors. The approach we introduce adopts design science principles and develops role-based access control based on AI techniques. The application is presented through a smart farm scenario while we incrementally explore the data sharing challenges in Agriculture 4.0. Data management and sharing practices should enforce defined contextual policies for access control. The approach could inform policymaking decisions for role-based data management, specifically the data-sharing agreements in the context of Industry 4.0 in broad terms and Agriculture 4.0 in specific.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "14.098",
        "scimago_value": "2,770"
    },
    {
        "issnkey": "01663615",
        "isbn": null,
        "journal": "Computers in Industry",
        "publisher": null,
        "title": "verticaldatacontinuitywithleanedgeanalyticsforindustry40production",
        "booktitle": null,
        "doi": "10.1016/j.compind.2020.103389",
        "author": [
            "K\u00fcfner, Thomas",
            "Sch\u00f6nig, Stefan",
            "Jasinski, Richard",
            "Ermer, Andreas"
        ],
        "keywords": [
            "Edge analytics, Industry 4.0, Smart sensors, Machine learning"
        ],
        "abstract": "Industry 4.0 is characterized by the digitization and networking of machines and systems in production. The amount of data in production is increasing, providing information about processes and thus enables the autonomous monitoring, control and optimization of value creation processes. However, there have been several open challenges and current research questions identified. In particular, new solutions need to be scalable and high-performing to deal with the growing volumes of data close to real-time. The work at hand tackles these research gaps by presenting an approach to realize vertical data continuity by combining signal acquisition and simultaneous data evaluation in a decentralized system without the use of time-consuming external cloud solutions. The approach has been evaluated in laboratory as well as in industrial settings.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.635",
        "scimago_value": "1,432"
    },
    {
        "issnkey": "02673649",
        "isbn": null,
        "journal": "Computer Law & Security Review",
        "publisher": null,
        "title": "anevidencebasedmethodologyforhumanrightsimpactassessmenthriainthedevelopmentofaidataintensivesystems",
        "booktitle": null,
        "doi": "10.1016/j.clsr.2021.105561",
        "author": [
            "Mantelero, Alessandro",
            "Esposito, Maria"
        ],
        "keywords": [
            "Artificial intelligence, Human rights, Human Rights Impact Assessment, Data protection, AI regulation, Data ethics"
        ],
        "abstract": "Different approaches have been adopted in addressing the challenges of Artificial Intelligence (AI), some centred on personal data and others on ethics, respectively narrowing and broadening the scope of AI regulation. This contribution aims to demonstrate that a third way is possible, starting from the acknowledgement of the role that human rights can play in regulating the impact of data-intensive systems. The focus on human rights is neither a paradigm shift nor a mere theoretical exercise. Through the analysis of more than 700 decisions and documents of the data protection authorities of six countries, we show that human rights already underpin the decisions in the field of data use. Based on empirical analysis of this evidence, this work presents a methodology and a model for a Human Rights Impact Assessment (HRIA). The methodology and related assessment model are focused on AI applications, whose nature and scale require a proper contextualisation of HRIA methodology. Moreover, the proposed models provide a more measurable approach to risk assessment which is consistent with the regulatory proposals centred on risk thresholds. The proposed methodology is tested in concrete case-studies to prove its feasibility and effectiveness. The overall goal is to respond to the growing interest in HRIA, moving from a mere theoretical debate to a concrete and context-specific implementation in the field of data-intensive applications based on AI.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,815"
    },
    {
        "issnkey": "18716784",
        "isbn": null,
        "journal": "New Biotechnology",
        "publisher": null,
        "title": "thebioeconomyinitalyandthenewnationalstrategyforamorecompetitiveandsustainablecountry",
        "booktitle": null,
        "doi": "10.1016/j.nbt.2020.11.009",
        "author": [
            "Fava, Fabio",
            "Gardossi, Lucia",
            "Brigidi, Patrizia",
            "Morone, Piergiuseppe",
            "Carosi, Daniela",
            "Lenzi, Andrea"
        ],
        "keywords": [
            "Bioeconomy strategy, Circular bioeconomy, Italian bioeconomy"
        ],
        "abstract": "Italy has the third largest bioeconomy in Europe (\u20ac330 billion annual turnover, 2 million employees), making it a core pillar of the national economy. Its sectors of excellence are food and biobased products, and it is a consistent presence in research and innovation projects funded by the EU Horizon 2020 programme (Societal Challenges 2) and the European Public Private Partnership \u201cBiobased industry\u201d (BBI-JU). The bioeconomy reduces dependence on fossil fuels and finite materials, loss of biodiversity and changing land use. It contributes to environmental regeneration, spurs economic growth and supports jobs in rural, coastal and abandoned industrial areas, leveraging local contexts and traditions. In 2017 the Italian government promoted the development of a national Bioeconomy Strategy (BIT), recently updated (BIT II) to interconnect more efficiently the pillars of the national bioeconomy: production of renewable biological resources, their conversion into valuable food/feed, biobased products and bio-energy, and transformation and valorization of bio-waste streams. BIT II aims to improve coordination between Ministries and Italian regions in alignment of policies, regulations, R&I funding programmes and infrastructures investment. The goal is a 15 % increase in turnover and employment in the Italian bioeconomy by 2030. Based on Italy\u2019s strategic geopolitical position in the Mediterranean basin, BIT II also includes actions to improve sustainable productivity, social cohesion and political stability through the implementation of bioeconomy strategies in this area. This paper provides an insight into these strategies and discusses the strengths and weaknesses of the sectors involved and the measures, regulatory initiatives and monitoring actions undertaken.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.079",
        "scimago_value": "1,163"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "validatingtheimpactofaccountingdisclosuresonstockmarketadeepneuralnetworkapproach",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.120903",
        "author": [
            "Eachempati, Prajwal",
            "Srivastava, Praveen",
            "Kumar, Ajay",
            "Tan, Kim",
            "Gupta, Shivam"
        ],
        "keywords": [
            "Disclosures, Data intelligence, Analytics, Finance, Machine learning, Deep learning, Stock market, Forecasts, Private decision-making"
        ],
        "abstract": "Firms disclose information either voluntarily or due to the regulator's mandatory requirements, and such disclosures form good sources to know the prospects of a firm. Information in the disclosures and analysts' opinions influence investor-trading behavior, and consequently, affects the asset prices. As sentiments factored in disclosures are a source of market action, this study aims to capture the sentiments from disclosure information to assess asset prices' impact. The paper adopts a deep neural network-based prediction model for conducting sentiment analysis on heterogeneous datasets. We construct a sentiment simulation model of voluntary disclosures to know whether the managers can use the market sentiment as a strategic input to boost market performance by suitably drafting the tone and content of disclosures without compromising their quality and veracity. The Deep Neural Networks with LSTM algorithm is found to outperform the Deep Neural Networks with RNN and other baseline machine learning classifiers in terms of predictive accuracy of the NSE NIFTY50. The variable importance computed also validates that market news, combined with historical indicators, predicts the stock market trend closer to the actual trend.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "09265805",
        "isbn": null,
        "journal": "Automation in Construction",
        "publisher": null,
        "title": "applicationofweighinmotiontechnologiesforpavementandbridgeresponsemonitoringstateoftheartreview",
        "booktitle": null,
        "doi": "10.1016/j.autcon.2021.103844",
        "author": [
            "Sujon, Mohhammad",
            "Dai, Fei"
        ],
        "keywords": [
            "Weigh-in-motion, Review, Response monitoring, Structural health monitoring, Weight enforcement"
        ],
        "abstract": "Overweight vehicles may cause damage and premature deterioration of pavement and bridge structures. Weigh-in-Motion (WIM) is efficient in avoiding structural damage and ensuring successful weight enforcement by measuring a vehicle's weight in a dynamic state. WIM additionally provides information such as traffic volume, vehicle's speed, axle spacing, equivalent single axle load (ESAL), individual axle and gross vehicle weight (GVW), which is of value to planning, design, construction, and operations of transportation infrastructures. This paper reviewed the state of practice and research in WIM with focuses on its potential, limitations, cost-effectiveness, and data usage. Discussion was made on identifying needs and challenges for further development. This review provides the research community with a holistic view of available WIM techniques, their limitations, cost-effectiveness, and the need for future research on usage of the WIM data that might lead to wider adoption of WIM in transportation applications.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.700",
        "scimago_value": "1,837"
    },
    {
        "issnkey": "22120955",
        "isbn": null,
        "journal": "Urban Climate",
        "publisher": null,
        "title": "multidirectionaltemporalconvolutionalartificialneuralnetworkforpm25forecastingwithmissingvaluesadeeplearningapproach",
        "booktitle": null,
        "doi": "10.1016/j.uclim.2021.100800",
        "author": [
            "Samal, K.",
            "Babu, Korra",
            "Das, Santos"
        ],
        "keywords": [
            "Deep learning, Time series forecasting, Temporal Convolutional Network, Artificial Neural Network, PM2.5"
        ],
        "abstract": "Data imputation and forecasting are the major research areas in environmental data engineering. Solving those critical issues has an immense impact on air pollution management, consequently improving social, economic growth, and public health. Missing data is a common issue for all the domains, especially for environmental data analysis. Most of the research study tries to solve all these problems of time series data using different models. This research study presents a novel deep learning-based hybrid model architecture to solve these issues in a single training process. We come up with Multi-directional Temporal Convolutional Artificial Neural Network (MTCAN) model to impute and forecast PM2.5 pollutant concentration in a single training process. The main idea of the multi-directional properties of MTCAN is to interpolate the PM2.5 pollutant feature matrix to impute its value. Ultimately, it maintains the temporal correlation within the features' measurement and meteorological and pollutant variables to impute PM2.5 missing values. The MTCAN model performs feature learning and sequential modeling simultaneously with a wide range of past observations for long-term forecasting, minimizing memory size requirement and training cost. Experimental results indicate that the proposed model is superior to baseline pollution forecasting models, which prove its effectiveness in air quality modeling.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.731",
        "scimago_value": "1,151"
    },
    {
        "issnkey": "22119124",
        "isbn": null,
        "journal": "Global Food Security",
        "publisher": null,
        "title": "howmighttechnologyrisetothechallengeofdatasharinginagrifood",
        "booktitle": null,
        "doi": "10.1016/j.gfs.2021.100493",
        "author": [
            "Durrant, Aiden",
            "Markovic, Milan",
            "Matthews, David",
            "May, David",
            "Leontidis, Georgios",
            "Enright, Jessica"
        ],
        "keywords": [
            "Data Trusts, Data sharing, AI Technologies, Agri-food supply chains"
        ],
        "abstract": "Data sharing is often hindered by a number of real word challenges caused by a mixture of technological and social factors. To date, the agri-food sector significantly lags behind other sectors in overcoming these challenges. However, the benefits of data sharing are too great to be ignored as they have a potential to address many historical failings such as issues related to food safety, traceability and transparency, and must be carefully considered as the sector is undergoing a widespread digitalisation. In this article, we explore the potential of different technologies in addressing the challenges presented by data sharing in the agri-food sector, and how the use of these technologies in the narrative of a Data Trust may address many of these obstacles. We argue the importance of utilising semantic web technologies, distributed ledger technologies, machine learning, and privacy preserving technologies to enable future transformative data sharing infrastructures in the agri-food sector. The utilisation of holistic statistical analysis of the shared data is also discussed, vital in supporting many of the sectors optimisation and sustainability goals.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,350"
    },
    {
        "issnkey": "20957564",
        "isbn": null,
        "journal": "Journal of Traffic and Transportation Engineering (English Edition)",
        "publisher": null,
        "title": "researchandapplicationsofartificialneuralnetworkinpavementengineeringastateoftheartreview",
        "booktitle": null,
        "doi": "10.1016/j.jtte.2021.03.005",
        "author": [
            "Yang, Xu",
            "Guan, Jinchao",
            "Ding, Ling",
            "You, Zhanping",
            "Lee, Vincent",
            "{Mohd Hasan}, Mohd",
            "Cheng, Xiaoyun"
        ],
        "keywords": [
            "Pavement engineering, Pavement design, Artificial neural network, Deep learning, Pavement life cycle, Health inspection and monitoring"
        ],
        "abstract": "Given the great advancements in soft computing and data science, artificial neural network (ANN) has been explored and applied to handle complicated problems in the field of pavement engineering. This study conducted a state-of-the-art review for surveying the recent progress of ANN application at different stages of pavement engineering, including pavement design, construction, inspection and monitoring, and maintenance. This study focused on the papers published over the last three decades, especially the studies conducted since 2013. Through literature retrieval, a total of 683 papers in this field were identified, among which 143 papers were selected for an in-depth review. The ANN architectures used in these studies mainly included multi-layer perceptron neural network (MLPNN), convolutional neural network (CNN) and recurrent neural network (RNN) for processing one-dimensional data, two-dimensional data and time-series data. CNN-based pavement health inspection and monitoring attracted the largest research interest due to its potential to replace human labor. While ANN has been proved to be an effective tool for pavement material design, cost analysis, defect detection and maintenance planning, it is facing huge challenges in terms of data collection, parameter optimization, model transferability and low-cost data annotation. More attention should be paid to bring multidisciplinary techniques into pavement engineering to tackle existing challenges and widen future opportunities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,656"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "apitfalloflearningfromusergenerateddataindepthanalysisofsubjectiveclassproblem",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.05.017",
        "author": [
            "Nemoto, Kei",
            "Jain, Shweta"
        ],
        "keywords": [
            "Text classification, Supervised learning, Subjective class, Class noise"
        ],
        "abstract": "Research in the supervised learning algorithms field implicitly assumes that training data is labeled by domain experts or at least semi-professional labelers accessible through crowdsourcing services like Amazon Mechanical Turk. With the advent of the Internet, data has become abundant and a large number of machine learning based systems are being trained with user-generated data, where categorical data is used as labels. However, little work has been done in the area of supervised learning with user-defined labels where users are not necessarily experts and might be unable to provide correct labels to some data or the labels might contain significant human bias. In this article, we propose two types of classes in user-defined labels: subjective class and objective class - showing that the objective classes are as reliable as if they were provided by domain experts, whereas the subjective classes are subject to error and bias. We name this a subjective class problem and propose a Normalized Feature Indicative Score that can be effective in detecting subjective classes in a dataset without querying oracle. This score provides early detection of subjective classes in the data, saving time for data mining practitioners working with data that might contain errors and biases.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "01419331",
        "isbn": null,
        "journal": "Microprocessors and Microsystems",
        "publisher": null,
        "title": "researchonthestrategyofmobileshortvideoinproductsalesbasedon5gnetworkandembeddedsystem",
        "booktitle": null,
        "doi": "10.1016/j.micpro.2021.103831",
        "author": [
            "Zheng, Lu",
            "Liu, Shikun"
        ],
        "keywords": [
            "Recurrent Neural Network (RNN), Mobile short video, Product sales data upload bandwidth size, Embedded systems, 5G technology"
        ],
        "abstract": "Mobile short video-based product sales sharing sites like YouTube and Tudor have many established user content for creating and distributing shares. The increasing number of mobile devices for product sales leads to the upcoming new 5G technology roadmap for embedded systems and 5G network connectivity. As these are the main sources of 5G information and online activities for consumers, mobile phone short films are rapidly being replaced by embedded systems. As the demand for more embedded system devices and applications continues to grow, supported bandwidth is also essential to meet this growing connection demand. The existing system does not allocate the product sales data upload bandwidth size. The system proposed here focuses on user upload bandwidth allocation, one of the basic resources of a short video sharing system with product details. Its allocation upload bandwidth Recurrent Neural Network (RNN) algorithm is proposed in a centralized or decentralized way and evaluated for balancing widely used strategies (equal allocation) and a mobile short video. Embedded systems are responsible for running professional product sales and control applications consistently and predictably. Development while using the microprocessor is also important. It increases the need to process product sales to handle the bandwidth, latency requirements, product sales data and data generated from multiple connected devices. It's a big challenge for the industry to data and data capabilities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.525",
        "scimago_value": "0,323"
    },
    {
        "issnkey": "00298018",
        "isbn": null,
        "journal": "Ocean Engineering",
        "publisher": null,
        "title": "improvedkinematicinterpolationforaistrajectoryreconstruction",
        "booktitle": null,
        "doi": "10.1016/j.oceaneng.2021.109256",
        "author": [
            "Guo, Shaoqing",
            "Mou, Junmin",
            "Chen, Linying",
            "Chen, Pengfei"
        ],
        "keywords": [
            "AIS, Ship trajectory, Trajectory reconstruction, Kinematic interpolation"
        ],
        "abstract": "Ship trajectory information has made a significant contribution to the data-based research in analyzing maritime transportation and has facilitated the improvement of maritime safety. However, the AIS data, which consists of ship trajectory, inevitably contains noises or missing data that can interfere with the conclusion. In this paper, an improved kinematic interpolation is presented for AIS trajectory reconstruction, which integrates data preprocessing and interpolation that considers the ships' kinematic information. The improved kinematic reconstruction method includes four steps: (1) data preprocessing, (2) analysis of time interval distribution, (3) abnormal data detection and removal, (4) kinematic interpolation that takes the kinematic feature of ships (i.e., velocity and acceleration) into account, adding forward and backward track points to help correct the acceleration function of reconstruction points. The proposed method is tested on the AIS dataset of Zhoushan Port and was compared with traditional ship trajectory reconstruction methods. The comparison indicates that the proposed method can effectively reconstruct the ship trajectory with higher performance on a single ship trajectory and a large AIS data set of certain water areas.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.795",
        "scimago_value": "1,321"
    },
    {
        "issnkey": "01403664",
        "isbn": null,
        "journal": "Computer Communications",
        "publisher": null,
        "title": "reinforcementanddeepreinforcementlearningforwirelessinternetofthingsasurvey",
        "booktitle": null,
        "doi": "10.1016/j.comcom.2021.07.014",
        "author": [
            "Frikha, Mohamed",
            "Gammar, Sonia",
            "Lahmadi, Abdelkader",
            "Andrey, Laurent"
        ],
        "keywords": [
            "Internet of Things, Reinforcement learning, Deep reinforcement learning, Wireless Networks"
        ],
        "abstract": "Nowadays, many research studies and industrial investigations have allowed the integration of the Internet of Things (IoT) in current and future networking applications by deploying a diversity of wireless-enabled devices ranging from smartphones, wearables, to sensors, drones, and connected vehicles. The growing number of IoT devices, the increasing complexity of IoT systems, and the large volume of generated data have made the monitoring and management of these networks extremely difficult. Numerous research papers have applied Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) techniques to overcome these difficulties by building IoT systems with effective and dynamic decision-making mechanisms, dealing with incomplete information related to their environments. The paper first reviews pre-existing surveys covering the application of RL and DRL techniques in IoT communication technologies and networking. The paper then analyzes the research papers that apply these techniques in wireless IoT to resolve issues related to routing, scheduling, resource allocation, dynamic spectrum access, energy, mobility, and caching. Finally, a discussion of the proposed approaches and their limits is followed by the identification of open issues to establish grounds for future research directions proposal.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.167",
        "scimago_value": "0,627"
    },
    {
        "issnkey": "22141405",
        "isbn": null,
        "journal": "Journal of Transport & Health",
        "publisher": null,
        "title": "referencefreevideotorealdistanceapproximationbasedurbansocialdistancinganalyticsamidcovid19pandemic",
        "booktitle": null,
        "doi": "10.1016/j.jth.2021.101032",
        "author": [
            "Zuo, Fan",
            "Gao, Jingqin",
            "Kurkcu, Abdullah",
            "Yang, Hong",
            "Ozbay, Kaan",
            "Ma, Qingyu"
        ],
        "keywords": [
            "Social distancing, COVID-19, Close contact, Pedestrian, Deep learning, Computer vision"
        ],
        "abstract": "Introduction The rapidly evolving COVID-19 pandemic has dramatically reshaped urban travel patterns. In this research, we explore the relationship between \u201csocial distancing,\u201d a concept that has gained worldwide familiarity, and urban mobility during the pandemic. Understanding social distancing behavior will allow urban planners and engineers to better understand the new norm of urban mobility amid the pandemic, and what patterns might hold for individual mobility post-pandemic or in the event of a future pandemic. Methods There are still few efforts to obtain precise information on social distancing patterns of pedestrians in urban environments. This is largely attributed to numerous burdens in safely deploying any effective field data collection approaches during the crisis. This paper aims to fill that gap by developing a data-driven analytical framework that leverages existing public video data sources and advanced computer vision techniques to monitor the evolution of social distancing patterns in urban areas. Specifically, the proposed framework develops a deep-learning approach with a pre-trained convolutional neural network to mine the massive amount of public video data captured in urban areas. Real-time traffic camera data collected in New York City (NYC) was used as a case study to demonstrate the feasibility and validity of using the proposed approach to analyze pedestrian social distancing patterns. Results The results show that microscopic pedestrian social distancing patterns can be quantified by using a generalized real-distance approximation method. The estimated distance between individuals can be compared to social distancing guidelines to evaluate policy compliance and effectiveness during a pandemic. Quantifying social distancing adherence will provide decision-makers with a better understanding of prevailing social contact challenges. It also provides insights into the development of response strategies and plans for phased reopening for similar future scenarios.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,898"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820045-2",
        "journal": null,
        "publisher": "Academic Press",
        "title": "index",
        "booktitle": "The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry",
        "doi": "10.1016/B978-0-12-820045-2.09997-9",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00298018",
        "isbn": null,
        "journal": "Ocean Engineering",
        "publisher": null,
        "title": "theapplicationoffullyunmannedroboticsystemsforinspectionofsubseapipelines",
        "booktitle": null,
        "doi": "10.1016/j.oceaneng.2021.109214",
        "author": [
            "Rumson, Alexander"
        ],
        "keywords": [
            "Automation, Inspection, Marine robotics, Underwater structures, Unmanned underwater technology, Underwater vehicles"
        ],
        "abstract": "This paper focuses on recent innovations in the methods used for external remote subsea pipeline inspection. An unmanned method is revealed, in which an Autonomous Underwater Vehicle (AUV), paired with an Unmanned Surface Vessel (USV) was utilised to complete inspections. Results are presented from a recent project, in which existing hardware and software were integrated and combined with automated workflows to form a solution, involving an AUV operated from a USV, with operations remotely controlled from shore. As inspection data was acquired, self-actuating workflows ran onboard the AUV, enabling data processing tasks to commence and QC messages/alerts to be transmitted to the control centre, this allowed execution of in-water mission adjustments. The primary focus of this paper is on the development and implementation of an automated, fully unmanned system for subsea inspection operations. Initially, a brief review is presented of recent developments in this field. Links are drawn between these wider developments and progress made within the project, and areas are highlighted where further work is required for realisation of a comprehensive unmanned pipeline inspection solution.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.795",
        "scimago_value": "1,321"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-817663-4",
        "journal": null,
        "publisher": "Academic Press",
        "title": "index",
        "booktitle": "Pragmatic Randomized Clinical Trials",
        "doi": "10.1016/B978-0-12-817663-4.20001-9",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09204105",
        "isbn": null,
        "journal": "Journal of Petroleum Science and Engineering",
        "publisher": null,
        "title": "improvingfaciespredictionbycombiningsupervisedandunsupervisedlearningmethods",
        "booktitle": null,
        "doi": "10.1016/j.petrol.2020.108300",
        "author": [
            "Ippolito, Marco",
            "Ferguson, John",
            "Jenson, Fred"
        ],
        "keywords": [
            "Facies classification, Machine learning, Supervised learning, Unsupervised learning, Bias, Joint PDF"
        ],
        "abstract": "Facies classification from well logs is an indispensable part of seismic interpretation and is important in the determination of sequence stratigraphy and ultimately reservoir characterization. Although there have been improvements in the tools used to perform this task, it remains laborious, subjective, and error-prone. Achieving a proper classification is complicated by increasing dataset sizes as well as the need for correlated multidisciplinary models. Recent developments in machine learning provide an opportunity to assist interpreters in accomplishing this task while also improving the accuracy of classification results. Applications of machine learning methods for automating facies classification from well logs have previously been explored, however these have largely focused on evaluations or comparisons of individual algorithms or of ensembles of homogeneous agents. The proposed method combines heterogeneous agents to enhance prediction accuracy. Specifically, supervised learning, which provides a direct mapping between the data domain and the solution domain while introducing bias to generalize the mapping, is combined with unsupervised learning, which does not depend on similar generalization bias or training data but also does not provide a direct mapping between the data and solution domains. The combination is accomplished via the joint probability density function (PDF) of the supervised classification, which is used to guide identification of clusters delineated by unsupervised learning. This multi-agent approach can reduce bias introduced during training and provides a basis for generating a probability distribution for each sample rather than a discrete classification. The distribution, in turn, can be used to more accurately model the continuous nature of well log signals, which reflects continuity in lithological regimes.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.346",
        "scimago_value": "0,975"
    },
    {
        "issnkey": "03064549",
        "isbn": null,
        "journal": "Annals of Nuclear Energy",
        "publisher": null,
        "title": "uncertaintyquantificationandsoftwareriskanalysisfordigitaltwinsinthenearlyautonomousmanagementandcontrolsystemsareview",
        "booktitle": null,
        "doi": "10.1016/j.anucene.2021.108362",
        "author": [
            "Lin, Linyu",
            "Bao, Han",
            "Dinh, Nam"
        ],
        "keywords": [
            "Digital twin, Autonomous control, Uncertainty quantification, Software risk analysis"
        ],
        "abstract": "A nearly autonomous management and control (NAMAC) system is designed to furnish recommendations to operators for achieving particular goals based on NAMAC\u2019s knowledge base. As a critical component in a NAMAC system, digital twins (DTs) are used to extract information from the knowledge base to support decision-making in reactor control and management during all modes of plant operations. With the advancement of artificial intelligence and data-driven methods, machine learning algorithms are used to build DTs of various functions in the NAMAC system. To evaluate the uncertainty of DTs and its impacts on the reactor digital instrumentation and control systems, uncertainty quantification (UQ) and software risk analysis is needed. As a comprehensive overview of prior research and a starting point for new investigations, this study selects and reviews relevant UQ techniques and software hazard and software risk analysis methods that may be suitable for DTs in the NAMAC system.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.776",
        "scimago_value": "1,159"
    },
    {
        "issnkey": "09242716",
        "isbn": null,
        "journal": "ISPRS Journal of Photogrammetry and Remote Sensing",
        "publisher": null,
        "title": "characterizingurbanlandchangesof30globalmegacitiesusingnighttimelighttimeseriesstacks",
        "booktitle": null,
        "doi": "10.1016/j.isprsjprs.2021.01.002",
        "author": [
            "Zheng, Qiming",
            "Weng, Qihao",
            "Wang, Ke"
        ],
        "keywords": [
            "Nighttime light imagery, Land use and land cover changes, Urbanization, Time series analysis, Megacities, Urban areas, Sustainability"
        ],
        "abstract": "Worldwide urbanization has brought about diverse types of urban land use and land cover (LULC) changes. The diversity of urban land changes, however, have been greatly under studied, since the major focus of past research has been on urban growth. In this study, we proposed a framework to characterize diverse urban land changes of 30 global megacities using monthly nighttime light time series from VIIRS data. First, we developed a Logistic-Harmonic model to fit VIIRS time series. Second, by leveraging the uniqueness of urban land change and nighttime light data, we incorporated temporal information of VIIRS time series and proposed a new classification scheme to produce monthly maps of built-up areas and to disentangle urban land changes into five categories. Third, we provided an in-depth analysis and comparison of urban land change patterns of the selected megacities. Results demonstrated that the Logistic-Harmonic model yielded a robust performance in fitting VIIRS time series. Temporal features based classification can not only significantly improve the mapping accuracy of built-up areas, especially for regions with heterogeneous built-up and non-built-up landscapes, but also promoted temporal consistency and classification efficiency. Urban land changes occurred in 51% of the built-up pixels of the megacities. Compared with urban growth, other types of urban land change, particularly land use intensification, contributed to an unexpectedly large proportion of the changes (83%). The findings of this study offer an insightful understanding on global urbanization processes in megacities, and evoke further investigation on the environmental and ecological implications of urban land changes.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.979",
        "scimago_value": "2,960"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "proceduremodelforthedevelopmentandlaunchofintelligentassistancesystems",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.01.348",
        "author": [
            "Reichardt, Paul",
            "Lang, Sebastian",
            "Reggelin, Tobias"
        ],
        "keywords": [
            "Production planning, control, Machine learning, Planning assistance system, Practical implementation, Generic procedure model, Digital twin"
        ],
        "abstract": "The paper analyses the current state of knowledge on approaches for the practical implementation of machine learning based assistance systems for production planning and control. A concept of a procedure model for application-oriented projects in the field of industrial series production is proposed. It focusses on order sequencing and machine allocation in a real time production environment. As part of an application-oriented research project, a use case is referenced. In this paper, a first conceptual approach is presented, using the example of an industrial production of printed circuit boards. In the following steps, practical suitability is checked on the basis of the practical reference, conclusions are drawn and the methodology will be developed further. The aim is a generally valid procedure model for industrial series production.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "02648377",
        "isbn": null,
        "journal": "Land Use Policy",
        "publisher": null,
        "title": "citizenscienceforsustainableagricultureasystematicliteraturereview",
        "booktitle": null,
        "doi": "10.1016/j.landusepol.2021.105326",
        "author": [
            "Ebitu, Larmbert",
            "Avery, Helen",
            "Mourad, Khaldoon",
            "Enyetu, Joshua"
        ],
        "keywords": [
            "Sustainable agriculture, Citizen science, Farmers, Methodologies"
        ],
        "abstract": "Farmers as volunteers in research could potentially provide a rich resource for exploring sustainable agricultural research questions. To discern emerging patterns in citizen science-based studies on topics with relevance for sustainable agriculture and reveal salient challenges and opportunities for conducting such studies, we conducted a literature review of 27 articles from the period 2004\u20132019 of 250 publications screened from Google Scholar. These articles were thematically grouped under the topics: Soil health, climate adaptation, pest/pathogen monitoring, invasive species, inputs and outputs and pollination. Participants\u2019 characteristics, motivations, study design and project outcomes in the reviewed articles were summarized and discussed. Both observational and experimental studies were represented in the articles, while emerging trends point towards field experimentation and \u2018Large-N\u2032 trials by lay farmers. Crowdsourcing lends itself to projects where the main role of the public is local visual observations and reporting, such as in pest/pathogen monitoring. Challenges included methodological issues such as validation procedures, but above all motivation, recruitment, and retention of volunteers. Despite the importance of participatory approaches for deeper citizen involvement for sustainability transitions and for the quality of knowledge outcomes, the role of citizens was overall restricted to data collection. Several of the methodologies proposed would be difficult to implement in low-income countries, and relatively few studies pertained to agricultural concerns of the global South. To lend value to farmers' time, we recommend projects relevant to livelihoods, health issues or local farming problems, accompanied by well-structured data feedback protocols, routing study results back to farmers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.398",
        "scimago_value": "1,668"
    },
    {
        "issnkey": "23523018",
        "isbn": null,
        "journal": "The Lancet HIV",
        "publisher": null,
        "title": "associationsbetweenhivinfectionandclinicalspectrumofcovid19apopulationlevelanalysisbasedonusnationalcovidcohortcollaborativen3cdata",
        "booktitle": null,
        "doi": "10.1016/S2352-3018(21)00239-3",
        "author": [
            "Yang, Xueying",
            "Sun, Jing",
            "Patel, Rena",
            "Zhang, Jiajia",
            "Guo, Siyuan",
            "Zheng, Qulu",
            "Olex, Amy",
            "Olatosi, Bankole",
            "Weissman, Sharon",
            "Islam, Jessica",
            "Chute, Christopher",
            "Haendel, Melissa",
            "Kirk, Gregory",
            "Li, Xiaoming",
            "Moffitt, Richard",
            "Akelsrod, Hana",
            "Crandall, Keith",
            "Francheschini, Nora",
            "French, Evan",
            "{Po-Yu Chiang}, Teresa",
            "Caleb-Alexander, G",
            "Andersen, Kathleen",
            "Vinson, Amanda",
            "Brown, Todd",
            "Mannon, Roslyn"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Summary Background Evidence of whether people living with HIV are at elevated risk of adverse COVID-19 outcomes is inconclusive. We aimed to investigate this association using the population-based National COVID Cohort Collaborative (N3C) data in the USA. Methods We included all adult (aged \u226518 years) COVID-19 cases with any health-care encounter from 54 clinical sites in the USA, with data being deposited into the N3C. The outcomes were COVID-19 disease severity, hospitalisation, and mortality. Encounters in the same health-care system beginning on or after January 1, 2018, were also included to provide information about pre-existing health conditions (eg, comorbidities). Logistic regression models were employed to estimate the association of HIV infection and HIV markers (CD4 cell count, viral load) with hospitalisation, mortality, and clinical severity of COVID-19 (multinomial). The models were initially adjusted for demographic characteristics, then subsequently adjusted for smoking, obesity, and a broad range of comorbidities. Interaction terms were added to assess moderation effects by demographic characteristics. Findings In the harmonised N3C data release set from Jan 1, 2020, to May 8, 2021, there were 1 436 622 adult COVID-19 cases, of these, 13 170 individuals had HIV infection. A total of 26 130 COVID-19 related deaths occurred, with 445 among people with HIV. After adjusting for all the covariates, people with HIV had higher odds of COVID-19 death (adjusted odds ratio 1\u00b729, 95% CI 1\u00b716\u20131\u00b744) and hospitalisation (1\u00b720, 1\u00b715\u20131\u00b726), but lower odds of mild or moderate COVID-19 (0\u00b761, 0\u00b759\u20130\u00b764) than people without HIV. Interaction terms revealed that the elevated odds were higher among older age groups, male, Black, African American, Hispanic, or Latinx adults. A lower CD4 cell count (<200 cells per \u03bcL) was associated with all the adverse COVID-19 outcomes, while viral suppression was only associated with reduced hospitalisation. Interpretation Given the COVID-19 pandemic's exacerbating effects on health inequities, public health and clinical communities must strengthen services and support to prevent aggravated COVID-19 outcomes among people with HIV, particularly for those with pronounced immunodeficiency. Funding National Center for Advancing Translational Sciences, National Institute of Allergy and Infectious Diseases, National Institutes of Health, USA.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "5,483"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821226-4",
        "journal": null,
        "publisher": "Butterworth-Heinemann",
        "title": "chapter14datahandlingandmodeling",
        "booktitle": "Engine Testing (Fifth Edition)",
        "doi": "10.1016/B978-0-12-821226-4.00014-0",
        "author": [
            "Martyr, Anthony",
            "Rogers, David"
        ],
        "keywords": [
            "Data-science, machine-learning, artificial-intelligence, clustering, regression, prediction, model, data handling"
        ],
        "abstract": "In this chapter an overview of the industry standard for powertrain test data analytics will be provided, along with the emergence of the application of data science for engineering data. This is in order to provide the facility engineering with information of the value of modern data science in powertrain test and development environments. The material serves to provide those who are new to powertrain test data analysis mechanisms, the basic practices of data acquisition, analysis, and utilization as they currently exist within the industry. In addition, providing information for those seeking deeper information on sophisticated statistical and machine-learning tools within the context of powertrain engineering, along with the types of problems to which they are suited. As a summary, the chapter compares and contrasts conventional data practice and emerging processes, making observations on the possible future directions for the industry in this rapidly developing field.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01663615",
        "isbn": null,
        "journal": "Computers in Industry",
        "publisher": null,
        "title": "anapproachanddecisionsupporttoolforformingindustry40supplychaincollaborations",
        "booktitle": null,
        "doi": "10.1016/j.compind.2020.103391",
        "author": [
            "Cisneros-Cabrera, Sonia",
            "Pishchulov, Grigory",
            "Sampaio, Pedro",
            "Mehandjiev, Nikolay",
            "Liu, Zixu",
            "Kununka, Sophia"
        ],
        "keywords": [
            "Digitalization, Supply chain collaboration, Industry 4.0, Decision support systems, Interoperability, Ontology"
        ],
        "abstract": "Industry 4.0 technologies, process digitalisation and automation can be applied to support the formation of supply chain collaborations in manufacturing. Underpinned by information and communication technologies, collaborations of independent companies can dynamically pool production capacities and capabilities to jointly react to new business opportunities. These collaborations may involve a wide range of enterprises with different sizes and scope that individually would not be able to tender for such new business opportunities. To form these collaborative teams, assistive processes and technologies can underpin the effort towards exploring the tender requirements, unbundling the tender into smaller tasks and finding a suitable supplier for each task. In this paper, we present an approach and a tool to support decision making concerning forming supply chain collaborations in Industry 4.0. The approach proposed is unique in integrating industry domain ontologies, assistive human-computer interaction tools and multi-criteria decision support techniques to form team compositions speeding-up the collaboration process whilst maximising the chances of forming a viable team to fulfil the tender requirements. We also show evaluation results involving stakeholders from the supply chain function pointing to the effectiveness of the proposed solution, available online as a demo11http://130.88.97.225:4200 (username: TDMS@uniman.eu; password: uniman)..",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.635",
        "scimago_value": "1,432"
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "highendequipmentdatadesensitizationmethodbasedonimprovedstackelberggan",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2021.114989",
        "author": [
            "Xiang, Nan",
            "Zhang, Xiongtao",
            "Dou, Yajie",
            "Xu, Xiangqian",
            "Yang, Kewei",
            "Tan, Yuejin"
        ],
        "keywords": [
            "High-end equipment, Data desensitization, Generative adversarial networks"
        ],
        "abstract": "High-end equipment refers to a type of technical equipment with high technical content, large capital investment, and long development cycle. Therefore, high-end equipment data has extraordinary significance and its desensitization is an urgent problem in data analysis. Traditional data desensitization principles are processing original data such as substitution and adding noise. These methods may not only damage data correlation information, but also result in data disclosure and high computing cost. Given the aforementioned reasons, the study proposes a high-end equipment data desensitization method based on improved Stackelberg Generative Adversarial Networks (GAN). When compared with the normal GAN, the structure proposed in the study includes more generators and discriminators. By inputting the original data, the trained GAN can output indistinguishable data from the original data which helps data mining and also ensures the privacy of data. We experimented on two datasets: optimal improvement was determined by Gaussian dataset experiments, i.e. Stackelberg GAN with eight discriminators. The second experiment results on real-world datasets proved that the 8-discriminator Stackelberg GAN better fits the original data and significantly aids data desensitization.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "asurveyonsecurityandprivacyoffederatedlearning",
        "booktitle": null,
        "doi": "10.1016/j.future.2020.10.007",
        "author": [
            "Mothukuri, Viraaji",
            "Parizi, Reza",
            "Pouriyeh, Seyedamin",
            "Huang, Yan",
            "Dehghantanha, Ali",
            "Srivastava, Gautam"
        ],
        "keywords": [
            "Artificial intelligence, Machine learning, Distributed learning, Federated learning, Federated machine learning, Security, Privacy"
        ],
        "abstract": "Federated learning (FL) is a new breed of Artificial Intelligence (AI) that builds upon decentralized data and training that brings learning to the edge or directly on-device. FL is a new research area often referred to as a new dawn in AI, is in its infancy, and has not yet gained much trust in the community, mainly because of its (unknown) security and privacy implications. To advance the state of the research in this area and to realize extensive utilization of the FL approach and its mass adoption, its security and privacy concerns must be first identified, evaluated, and documented. FL is preferred in use-cases where security and privacy are the key concerns and having a clear view and understanding of risk factors enable an implementer/adopter of FL to successfully build a secure environment and gives researchers a clear vision on possible research areas. This paper aims to provide a comprehensive study concerning FL\u2019s security and privacy aspects that can help bridge the gap between the current state of federated AI and a future in which mass adoption is possible. We present an illustrative description of approaches and various implementation styles with an examination of the current challenges in FL and establish a detailed review of security and privacy concerns that need to be considered in a thorough and clear context. Findings from our study suggest that overall there are fewer privacy-specific threats associated with FL compared to security threats. The most specific security threats currently are communication bottlenecks, poisoning, and backdoor attacks while inference-based attacks are the most critical to the privacy of FL. We conclude the paper with much needed future research directions to make FL adaptable in realistic scenarios.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "measuringtheperceivedbenefitsofimplementingblockchaintechnologyinthebankingsector",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2020.120407",
        "author": [
            "Garg, Poonam",
            "Gupta, Bhumika",
            "Chauhan, Ajay",
            "Sivarajah, Uthayasankar",
            "Gupta, Shivam",
            "Modgil, Sachin"
        ],
        "keywords": [
            "Blockchain, Banking sector, Perceived benefits, Instrument development, AMOS"
        ],
        "abstract": "This study aims to measure the perceived business benefits of blockchain technology implementation in the banking sector and establish factors to measure these benefits. Concerns regarding security, values, and standards are essential to banking operations. Data was collected from 291 respondents who are either blockchain consultants, blockchain marketing experts, or CEOs/business heads of banks that are in the process of advising, consulting, or implementing blockchain technology. Confirmatory factor analysis (CFA) was carried out to assess the reliability and validity of the proposed instrument. The results support the proposed instrument and its five constructs. The scale emerging from this study indicates a good degree of reliability, validity and unidimensionality in each of its constructs. Technologies like blockchain are in their initial stages, and recent advances in blockchain technology may impact our findings. The developed instrument could help give decision makers a foundational view to measure the benefits of implementing blockchain technology before they choose to integrate it in their existing system. The scientific and societal significance of the study based on its practical and theoretical applications is presented at the end.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "02673649",
        "isbn": null,
        "journal": "Computer Law & Security Review",
        "publisher": null,
        "title": "councilofeuropeconvention108amodernisedinternationaltreatyfortheprotectionofpersonaldata",
        "booktitle": null,
        "doi": "10.1016/j.clsr.2020.105497",
        "author": [
            "{de Terwangne}, C\u00e9cile"
        ],
        "keywords": [
            "Data protection, Council of Europe Convention 108, Modernised Convention 108, Personal data, Informational autonomy, Data subject\u2019s rights, Data security, Transborder data flows, Supervisory authority, Convention Committee"
        ],
        "abstract": "Summary The Council of Europe has modernized its Convention 108 for the protection of individuals with regard to automatic processing of personal data: in 2018 it adopted Convention 108+. The modernised version of Convention 108 seeks to respond to the challenges posed, in terms of human rights, by the use of new information and communication technologies. This article presents a detailed analysis of this new international text. Convention 108+ contains important innovations: it proclaims the importance of protecting the right to informational autonomy and human dignity in the face of technological developments. It consolidates the proportionality requirement for data processing and strengthens the arsenal of rights of the data subjects. It reinforces the responsibility of those in charge of data processing as well as its transparency. It requires notification of security breaches. It strengthens the independence, powers and means of action of the supervisory authorities. It also strengthens the mechanism to ensure its effective implementation by entrusting the Committee set up by the Convention with the task of verifying compliance with the commitments made by Parties.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,815"
    },
    {
        "issnkey": "07351097",
        "isbn": null,
        "journal": "Journal of the American College of Cardiology",
        "publisher": null,
        "title": "nhlbicmrefworkshopreportonpulmonaryvasculardiseaseclassificationjaccstateoftheartreview",
        "booktitle": null,
        "doi": "10.1016/j.jacc.2021.02.056",
        "author": [
            "Oldham, William",
            "Hemnes, Anna",
            "Aldred, Micheala",
            "Barnard, John",
            "Brittain, Evan",
            "Chan, Stephen",
            "Cheng, Feixiong",
            "Cho, Michael",
            "Desai, Ankit",
            "Garcia, Joe",
            "Geraci, Mark",
            "Ghiassian, Susan",
            "Hall, Kathryn",
            "Horn, Evelyn",
            "Jain, Mohit",
            "Kelly, Rachel",
            "Leopold, Jane",
            "Lindstrom, Sara",
            "Modena, Brian",
            "Nichols, William",
            "Rhodes, Christopher",
            "Sun, Wei",
            "Sweatt, Andrew",
            "Vanderpool, Rebecca",
            "Wilkins, Martin",
            "Wilmot, Beth",
            "Zamanian, Roham",
            "Fessel, Joshua",
            "Aggarwal, Neil",
            "Loscalzo, Joseph",
            "Xiao, Lei"
        ],
        "keywords": [
            "drug repurposing, integrative omics, master clinical trial protocol, precision medicine, pulmonary hypertension, systems biology"
        ],
        "abstract": "The National Heart, Lung, and Blood Institute and the Cardiovascular Medical Research and Education Fund held a workshop on the application of pulmonary vascular disease omics data to the understanding, prevention, and treatment of pulmonary vascular disease. Experts in pulmonary vascular disease, omics, and data analytics met to identify knowledge gaps and formulate ideas for future research priorities in pulmonary vascular disease in line with National Heart, Lung, and Blood Institute Strategic Vision goals. The group identified opportunities to develop analytic approaches to multiomic datasets, to identify molecular pathways in pulmonary vascular disease pathobiology, and to link novel phenotypes to meaningful clinical outcomes. The committee suggested support for interdisciplinary research teams to develop and validate analytic methods, a national effort to coordinate biosamples and data, a consortium of preclinical investigators to expedite target evaluation and drug development, longitudinal assessment of molecular biomarkers in clinical trials, and a task force to develop a master clinical trials protocol for pulmonary vascular disease.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00448486",
        "isbn": null,
        "journal": "Aquaculture",
        "publisher": null,
        "title": "aquacultureefficiencyandproductivityacomprehensivereviewandbibliometricanalysis",
        "booktitle": null,
        "doi": "10.1016/j.aquaculture.2021.736881",
        "author": [
            "See, Kok",
            "Ibrahim, Rabiatul",
            "Goh, Kim"
        ],
        "keywords": [
            "Aquaculture, Bibliometric analysis, Data envelopment analysis, Efficiency, Productivity, Stochastic frontier analysis"
        ],
        "abstract": "The scientific research on aquaculture efficiency and productivity has been increasing over the years. This study aims to identify the publication trends and growth potential of aquaculture efficiency and productivity studies. A bibliometric analysis was employed for a sample of 85 scientific articles published during the 1998\u20132020 period. The findings show that authors and institutions have close groups in collaboration networks. Through the citation analysis, three clusters were obtained that were related to the use of stochastic frontier analysis in an empirical application, Norwegian salmon aquaculture, and efficiency studies associated to freshwater aquaculture. For the temporal evolution of the keywords, earlier existing studies adopted a stochastic translog production function to assess the technical efficiency of aquaculture production, whereas later studies used data envelopment analysis, which focused on more diverse research objectives. The farms or subsectors of aquaculture in Norway, Bangladesh, and Vietnam have been analyzed in-depth for the efficiency and productivity in the existing studies. Education, experience, and age of farmers are often used as determinants to explain the variations in technical efficiency. The present study concludes that aquaculture efficiency and productivity research is not moving toward a mature stage. Several of the discovered issues are only focused on specific countries, and there is still room for methodological improvement in assessing aquaculture efficiency and productivity. Nevertheless, research collaborations are growing, and new research trends that are related to environmental regulation and pollution show great interest in aquaculture efficiency and productivity. This study provides a clear roadmap for researchers and practitioners to understand the publication patterns and hotspots in the research field.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.242",
        "scimago_value": "1,066"
    },
    {
        "issnkey": "27724247",
        "isbn": null,
        "journal": "Communications in Transportation Research",
        "publisher": null,
        "title": "emergingapproachesappliedtomaritimetransportresearchpastandfuture",
        "booktitle": null,
        "doi": "10.1016/j.commtr.2021.100011",
        "author": [
            "Yan, Ran",
            "Wang, Shuaian",
            "Zhen, Lu",
            "Laporte, Gilbert"
        ],
        "keywords": [
            "Maritime transport, Shipping, Port, Data-driven modeling, Digitalization in the maritime industry"
        ],
        "abstract": "Maritime transport is the backbone of international trade and globalization. Maritime transport research can be roughly divided into two categories, namely the shipping side and the port side. Most of the classic approaches adopted to address practical problems in these research topics are based on long-term observations and expert knowledge, while few of them are based on historical data accumulated from practice. In recent years, emerging approaches, which we refer to as machine learning and deep learning techniques in this essay, have been receiving a wider attention to solve practical problems. As a relatively conservative industry, there are some initial trials of applying the emerging approaches to solve practical problems in the maritime sector. The objective of this essay is to review the application of emerging approaches to maritime transport research. The main research topics in maritime transport and classic methods developed to solve them are first presented. The introduction of emerging approaches and their suitability to be applied in maritime transport research is then discussed. Related existing studies are then reviewed according to problem settings, main data sources, and emerging approaches adopted. Challenges and solutions in the process are also discussed from the perspectives of data, model, users, and targets. Finally, promising future research directions are identified. This essay is the first to give a comprehensive review of existing studies on developing machine learning and deep learning models together with popular data sources used to address practical problems in maritime transport.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02684012",
        "isbn": null,
        "journal": "International Journal of Information Management",
        "publisher": null,
        "title": "digitaltransformationsolutionsofentrepreneurialsmesbasedonaninformationerrordriventsphericalfuzzycloudalgorithm",
        "booktitle": null,
        "doi": "10.1016/j.ijinfomgt.2021.102384",
        "author": [
            "Yang, Zaoli",
            "Chang, Jinping",
            "Huang, Lucheng",
            "Mardani, Abbas"
        ],
        "keywords": [
            "Digital transformation, Entrepreneurial SMEs, Evaluation and selection, T-spherical fuzzy cloud, T-spherical fuzzy cloud weighted Heronian mean operator"
        ],
        "abstract": "The digital transformation of enterprises has become an inevitable development trend and one of the key driving forces that promotes the sustainable development of enterprises. However, due to the many obstacles of financial burdens, technical thresholds, and talent shortages, digital transformation has become a challenging task for entrepreneurial Small and Medium-Sized Enterprises (SMEs). Additionally, many competitive digital transformation solutions on the market cause confusion when enterprises must choose one. This study drew a new information error-driven T-spherical fuzzy cloud algorithm to evaluate digital transformation solutions of entrepreneurial SMEs and support its selection. First, an evaluation index system for the digital transformation solution of entrepreneurial SMEs was established from four aspects. Then, a new concept of a T-spherical fuzzy cloud was defined to represent the evaluation information of the indicators. Additionally, a T-spherical Fuzzy Cloud Weighted Heronian Mean (T-SFCWHM) operator was used to aggregate the evaluation information. Afterward, an evaluation and selection decision framework for the digital transformation solution of entrepreneurial SMEs based on the T-SFCWHM operator was developed. Further, a practical example was given to illustrate the effectiveness of the proposed method. Finally, a discussion of the findings in our study was conducted, and the conclusions were summarized.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "14.098",
        "scimago_value": "2,770"
    },
    {
        "issnkey": "01651781",
        "isbn": null,
        "journal": "Psychiatry Research",
        "publisher": null,
        "title": "usingcatboostalgorithmtoidentifymiddleagedandelderlydepressionnationalhealthandnutritionexaminationsurvey20112018",
        "booktitle": null,
        "doi": "10.1016/j.psychres.2021.114261",
        "author": [
            "Zhang, Chenyang",
            "Chen, Xiaofei",
            "Wang, Song",
            "Hu, Junjun",
            "Wang, Chunpeng",
            "Liu, Xin"
        ],
        "keywords": [
            "Depression, Machine learning, Middle-aged and elderly, NHANES"
        ],
        "abstract": "Depression is one of the most common mental health problems in middle-aged and elderly people. The establishment of risk factor-based depression risk assessment model is conducive to early detection and early treatment of high-risk groups of depression. Five machine learning models (logistic regression (LR); back propagation (BP); random forest (RF); support vector machines (SVM); category boosting (CatBoost) were used to evaluate the depression among 8374 middle-aged people and 4636 elderly people in the NHANES database from 2011 to 2018. In the 2011\u20132018 cycle, the estimated prevalence of depression was 8.97% in the middle-aged participants and 8.02% in the elderly participants. Among the middle-aged and elderly participants, CatBoost was the best model to identify depression, and its area under the working characteristic curve (AUC) reaches the highest. The second is LR model and SVM model, while the performance of BP and RF model was slightly worse. The primary influencing factor of depression in middle-aged male is alanine aminotransferase. All five machine learning models can identify the occurrence of depression in the NHANES data set through social demographics, lifestyle, laboratory data and other data of middle-aged and elderly people, and among five models, the CatBoost model performed best.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.222",
        "scimago_value": "1,224"
    },
    {
        "issnkey": "03054403",
        "isbn": null,
        "journal": "Journal of Archaeological Science",
        "publisher": null,
        "title": "oninterdisciplinarityinthehumanitiesacommentonfantaetal2020onthebiasindatingobtainedfromhistoricalsources",
        "booktitle": null,
        "doi": "10.1016/j.jas.2021.105392",
        "author": [
            "Kol\u00e1\u0159, Jan",
            "Szab\u00f3, P\u00e9ter"
        ],
        "keywords": [
            "Settlement history, Written records, Middle ages, Time lag, Archaeological method, Bohemia, Central Europe"
        ],
        "abstract": "Medieval settlement history in Europe is a common topic in several scientific disciplines. Recently, Fanta et al. (2020) examined colonization processes in Bohemia through the comparison of archaeological evidence and historical records. They concluded that the first mentions of settlements in historical documents are not reliable sources for settlement dating and should always be verified and preferably superseded by archaeological data, which are, in contrast, mostly unproblematic. We argue that this conclusion is controversial from several aspects. Firstly, it neglects the disciplinary constraints of archaeological evidence for medieval settlement development, as regards quality and chronology. Secondly, there are several legitimate perspectives from which to analyse the data. Our reanalysis of the original dataset showed that \u2013 in partial contrast to the conclusions of Fanta et al. (2020) \u2013 when viewed from the point of view of historical evidence, the time lag between the historical and archaeological dating increased with time and that the historical dating of most of the settlements between the 10th and 13th centuries was supported by archaeological evidence. Lastly, we demonstrated how research combining different disciplines (archaeology, history, palaeoecology, geography) and types evidence can reveal the manifold processes of human settlement dynamics. In our view each type of evidence has advantages as well as drawbacks, therefore strictly prioritising one at the expense of others hardly furthers the understanding of complex social phenomena.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.216",
        "scimago_value": "1,572"
    },
    {
        "issnkey": "1470160x",
        "isbn": null,
        "journal": "Ecological Indicators",
        "publisher": null,
        "title": "opportunitiesforimprovingrecognitionofcoastalwetlandsinglobalecosystemassessmentframeworks",
        "booktitle": null,
        "doi": "10.1016/j.ecolind.2021.107694",
        "author": [
            "Brown, Christopher",
            "Adame, Maria",
            "Buelow, Christina",
            "Frassl, Marieke",
            "Lee, Shing",
            "Mackey, Brendan",
            "McClure, Eva",
            "Pearson, Ryan",
            "Rajkaran, Anusha",
            "Rayner, Thomas",
            "Sievers, Michael",
            "{Saint Ange}, Chantal",
            "Sousa, Ana",
            "Tulloch, Vivitskaia",
            "Turschwell, Mischa",
            "Connolly, Rod"
        ],
        "keywords": [
            "Seagrass, Saltmarsh, Mangrove, Fish nursery, Ecosystem condition, System of environmental-economic accounting indicators, Biodiversity, Health index"
        ],
        "abstract": "Vegetated coastal wetlands, including seagrass, saltmarsh and mangroves, are threatened globally, yet the need to avert these losses is poorly recognized in international policy, such as in the Convention on Biological Diversity and the United Nations (UN) Sustainable Development Goals. Identifying the impact of overlooking coastal wetlands in ecosystem assessment frameworks could help prioritize research efforts to fill these gaps. Here, we examine gaps in the recognition of coastal wetlands in globally applicable ecosystem assessments. We address both shortfalls in assessment frameworks when it comes to assessing wetlands, and gaps in data that limit widespread application of assessments. We examine five assessment frameworks that track fisheries, greenhouse gas emissions, ecosystem threats, and ecosystem services. We found that these assessments inform management decisions, but that the functions provided by coastal wetlands are incompletely represented. Most frameworks had sufficient complexity to measure wetland status, but limitations in data meant they were incompletely informed about wetland functions and services. Incomplete representation of coastal wetlands may lead to them being overlooked by research and management. Improving the coverage of coastal wetlands in ecosystem assessments requires improving global scale mapping of wetland trends, developing global-scale indicators of wetland function and synthesis to quantitatively link animal population dynamics to wetland trends. Filling these gaps will help ensure coastal wetland conservation is properly informed to manage them for the outstanding benefits they bring humanity.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09575820",
        "isbn": null,
        "journal": "Process Safety and Environmental Protection",
        "publisher": null,
        "title": "spatiotemporalattentionmechanismbaseddeepnetworkforcriticalparameterspredictioninchemicalprocess",
        "booktitle": null,
        "doi": "10.1016/j.psep.2021.09.024",
        "author": [
            "Yuan, Zhuang",
            "Yang, Zhe",
            "Ling, Yiqun",
            "Wu, Chuanpeng",
            "Li, Chuankun"
        ],
        "keywords": [
            "Chemical processes, Parameters prediction, Deep networks, Spatiotemporal attention mechanism, Feature representation"
        ],
        "abstract": "In chemical processes, grasping the changing trend of critical parameters can help field operators take appropriate adjustments to eliminate potential fluctuations. Thus, deep networks, renowned for its revolutionary feature representation capability, have been gradually exploited for building reliable prediction models from massive data embraced tremendously nonlinearities and dynamics. Because of the inherent complexity, the process trajectories over the whole running duration make distinctive contributions to the ultimate targets. Specifically, features extracted from different secondary variables at different previous instants have diverse impacts on the current state of primary variables. However, this spatiotemporal relevance discrepancy is rarely considered, which may lead to deterioration of prediction performance. Therefore, this paper seamlessly integrates the spatiotemporal attention (STA) mechanism with convolutional neural networks (CNN) and bi-directional long short-term memory (BiLSTM), and proposes a novel predictive model, namely STA-ConvBiLSTM. Using the deep framework composed of CNN and BiLSTM, the integrated model can, not only automatically explore the esoteric spatial correlations among high-dimensional variables at each time step, but also adaptively excavate beneficial temporal characteristics across all time steps. Meanwhile, STA is further introduced to assign corresponding weights to information with dissimilar importance, so as to prevent high target-relevant interactions from being discarded due to overlong sequences and excessive features. STA-ConvBiLSTM is applied in the case of furnace tube temperature prediction of a delayed coking unit, which exhibits a significant improvement of the prediction accuracy.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00928674",
        "isbn": null,
        "journal": "Cell",
        "publisher": null,
        "title": "iterativetomographywithdigitaladaptiveopticspermitshourlongintravitalobservationof3dsubcellulardynamicsatmillisecondscale",
        "booktitle": null,
        "doi": "10.1016/j.cell.2021.04.029",
        "author": [
            "Wu, Jiamin",
            "Lu, Zhi",
            "Jiang, Dong",
            "Guo, Yuduo",
            "Qiao, Hui",
            "Zhang, Yi",
            "Zhu, Tianyi",
            "Cai, Yeyi",
            "Zhang, Xu",
            "Zhanghao, Karl",
            "Xie, Hao",
            "Yan, Tao",
            "Zhang, Guoxun",
            "Li, Xiaoxu",
            "Jiang, Zheng",
            "Lin, Xing",
            "Fang, Lu",
            "Zhou, Bing",
            "Xi, Peng",
            "Fan, Jingtao",
            "Yu, Li",
            "Dai, Qionghai"
        ],
        "keywords": [
            "long-term high-speed imaging, adaptive optics, light-field microscopy, phototoxicity, intravital, migrasome, retraction fiber, tumor metastasis, calcium imaging"
        ],
        "abstract": "Summary Long-term subcellular intravital imaging in mammals is vital to study diverse intercellular behaviors and organelle functions during native physiological processes. However, optical heterogeneity, tissue opacity, and phototoxicity pose great challenges. Here, we propose a computational imaging framework, termed digital adaptive optics scanning light-field mutual iterative tomography (DAOSLIMIT), featuring high-speed, high-resolution 3D imaging, tiled wavefront correction, and low phototoxicity with a compact system. By tomographic imaging of the entire volume simultaneously, we obtained volumetric imaging across 225 \u00d7 225 \u00d7 16 \u03bcm3, with a resolution of up to 220 nm laterally and 400 nm axially, at the millisecond scale, over hundreds of thousands of time points. To establish the capabilities, we investigated large-scale cell migration and neural activities in different species and observed various subcellular dynamics in mammals during neutrophil migration and tumor cell circulation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "41.582",
        "scimago_value": "26,304"
    },
    {
        "issnkey": "22131337",
        "isbn": null,
        "journal": "Astronomy and Computing",
        "publisher": null,
        "title": "spartanmaximizingtheuseofspectrophotometricobservationaldataduringtemplatefitting",
        "booktitle": null,
        "doi": "10.1016/j.ascom.2020.100427",
        "author": [
            "Thomas, R."
        ],
        "keywords": [
            "Galaxy, Fitting, Observations, Spectroscopy, Photometry"
        ],
        "abstract": "SPARTAN [Spectroscopic And photometRic fitting Tool for Astronomical aNalysis] is a tool designed to perform the fitting of galaxy observations either using photometry and low resolution spectroscopy separately or simultaneously. Based on a grid search \u03c72 fitting method, SPARTAN was tailored to UV-to-NIR data and designed for well calibrated data. The first version of this tool allows the use of the low resolution models of Bruzual & Charlot (2003) and include the treatment of the intergalactic medium as a free parameter. It has been designed to be an user-friendly environment where people do not need to know how to code to perform the fit. SPARTAN includes everything needed to perform the fit, from the galaxy models creation, to the visualization of the results through the graphical interface. SPARTAN is a fully open source software made with Python 3. It is published under the GNU general public license (v3) and is available in a Github repository. It can be installed directly from the python official repository (pypi) and the documentation is available through a github repository.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.927",
        "scimago_value": "0,692"
    },
    {
        "issnkey": "18777821",
        "isbn": null,
        "journal": "Cancer Epidemiology",
        "publisher": null,
        "title": "heterogeneityinheadandneckcancerincidenceamongblackpopulationsfromafricathecaribbeanandtheusaanalysisofcancerregistrydatabytheac3",
        "booktitle": null,
        "doi": "10.1016/j.canep.2021.102053",
        "author": [
            "Auguste, Aviane",
            "Gathere, Samuel",
            "Pinheiro, Paulo",
            "Adebamowo, Clement",
            "Akintola, Adeola",
            "Alleyne-Mike, Kellie",
            "Anderson, Simon",
            "Ashing, Kimlin",
            "Awittor, Fred",
            "Awuah, Baffour",
            "Bhakkan, Bernard",
            "Deloumeaux, Jacqueline",
            "Plessis, Maira",
            "Ekanem, Ima-Obong",
            "Ekanem, Uwemedimbuk",
            "Ezeome, Emmanuel",
            "Felix, Nkese",
            "Gachii, Andrew",
            "Gaete, Stanie",
            "Gibson, Tracey",
            "Hage, Robert",
            "Harrison, Sharon",
            "Igbinoba, Festus",
            "Iseh, Kufre",
            "Kiptanui, Evans",
            "Korir, Ann",
            "Lawson-Myers, Heather-Dawn",
            "Llanos, Adana",
            "Luce, Daniele",
            "McNaughton, Dawn",
            "Odutola, Michael",
            "Omonisi, Abidemi",
            "Otu, Theresa",
            "Peruvien, Jessica",
            "Raheem, Nasiru",
            "Roach, Veronica",
            "Sobers, Natasha",
            "Uamburu, Nguundja",
            "Ragin, Camille"
        ],
        "keywords": [
            "Head and neck cancer, Incidence, Blacks, Tobacco smoking, Alcohol drinking, HPV, Caribbean, Africa, USA, Population-based cancer registry"
        ],
        "abstract": "Background Africa and the Caribbean are projected to have greater increases in Head and neck cancer (HNC) burden in comparison to North America and Europe. The knowledge needed to reinforce prevention in these populations is limited. We compared for the first time, incidence rates of HNC in black populations from African, the Caribbean and USA. Methods Annual age-standardized incidence rates (IR) and 95% confidence intervals (95%CI) per 100,000 were calculated for 2013\u20132015 using population-based cancer registry data for 14,911 HNC cases from the Caribbean (Barbados, Guadeloupe, Trinidad & Tobago, N = 443), Africa (Kenya, Nigeria, N = 772) and the United States (SEER, Florida, N = 13,696). We compared rates by sub-sites and sex among countries using data from registries with high quality and completeness. Results In 2013\u20132015, compared to other countries, HNC incidence was highest among SEER states (IR: 18.2, 95%CI = 17.6\u201318.8) among men, and highest in Kenya (IR: 7.5, 95%CI = 6.3\u20138.7) among women. Nasopharyngeal cancer IR was higher in Kenya for men (IR: 3.1, 95%CI = 2.5\u20133.7) and women (IR: 1.5, 95%CI = 1.0\u20131.9). Female oral cavity cancer was also notably higher in Kenya (IR = 3.9, 95%CI = 3.0\u20134.9). Blacks from SEER states had higher incidence of laryngeal cancer (IR: 5.5, 95%CI = 5.2\u20135.8) compared to other countries and even Florida blacks (IR: 4.4, 95%CI = 3.9\u20135.0). Conclusion We found heterogeneity in IRs for HNC among these diverse black populations; notably, Kenya which had distinctively higher incidence of nasopharyngeal and female oral cavity cancer. Targeted etiological investigations are warranted considering the low consumption of tobacco and alcohol among Kenyan women. Overall, our findings suggest that behavioral and environmental factors are more important determinants of HNC than race.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.984",
        "scimago_value": "1,156"
    },
    {
        "issnkey": "1936878x",
        "isbn": null,
        "journal": "JACC: Cardiovascular Imaging",
        "publisher": null,
        "title": "3dprintingcomputationalmodelingandartificialintelligenceforstructuralheartdisease",
        "booktitle": null,
        "doi": "10.1016/j.jcmg.2019.12.022",
        "author": [
            "Wang, Dee",
            "Qian, Zhen",
            "Vukicevic, Marija",
            "Engelhardt, Sandy",
            "Kheradvar, Arash",
            "Zhang, Chuck",
            "Little, Stephen",
            "Verjans, Johan",
            "Comaniciu, Dorin",
            "O\u2019Neill, William",
            "Vannan, Mani"
        ],
        "keywords": [
            "3D printing, artificial intelligence, computational modeling, computed tomography, left atrial appendage, structural heart disease, transcatheter aortic valve replacement, transcatheter mitral valve replacement, transesophageal echocardiogram"
        ],
        "abstract": "Structural heart disease (SHD) is a new field within cardiovascular medicine. Traditional imaging modalities fall short in supporting the needs of SHD interventions, as they have been constructed around the concept of disease diagnosis. SHD interventions disrupt traditional concepts of imaging in requiring imaging to plan, simulate, and predict intraprocedural outcomes. In transcatheter SHD interventions, the absence of a gold-standard open cavity surgical field deprives physicians of the opportunity for tactile feedback and visual confirmation of cardiac anatomy. Hence, dependency on imaging in periprocedural guidance has led to evolution of a new generation of procedural skillsets, concept of a visual field, and technologies in the periprocedural planning period to accelerate preclinical device development, physician, and patient education. Adaptation of 3-dimensional (3D) printing in clinical care and procedural planning has demonstrated a reduction in early-operator learning curve for transcatheter interventions. Integration of computation modeling to 3D printing has accelerated research and development understanding of fluid mechanics within device testing. Application of 3D printing, computational modeling, and ultimately incorporation of artificial intelligence is changing the landscape of physician training and delivery of patient-centric care. Transcatheter structural heart interventions are requiring in-depth periprocedural understanding of cardiac pathophysiology and device interactions not afforded by traditional imaging metrics.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "thepromisesandperilsofautomaticidentificationsystemdata",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2021.114975",
        "author": [
            "Emmens, Ties",
            "Amrit, Chintan",
            "Abdi, Asad",
            "Ghosh, Mayukh"
        ],
        "keywords": [
            "AIS data, Data mining, Navigation safety, Ship behavior analysis, Environmental evaluation, Advanced applications of AIS data"
        ],
        "abstract": "Automatic Identification System (AIS) is used to identify vessels in maritime navigation. Currently, it is used for various commercial purposes. However, the abundance and lack of quality of AIS data make it difficult to capitalize on its value. Therefore, an understanding of both the limitations of AIS data and the opportunities is important to maximize its value, but these have not been clearly stated in the existing literature. This study aims to help researchers and practitioners understand AIS data by identifying both the promises and perils of AIS data. We identify the different applications and limitations of AIS data in the literature and build upon them in a sequential mixed-design study. We first identify the promises and perils that exist in the literature. We then analyze AIS data from the port of Amsterdam quantitatively to detect noise and to find the perils researchers and practitioners could encounter. Our results incorporate quantitative findings with qualitative insights obtained from interviewing domain experts. This study extends the literature by considering multiple limitations of AIS data across different domains at the same time. Our results show that the amount of noise in AIS data depends on factors such as the equipment used, external factors, humans, dense traffic etc. The contribution that our paper makes is in combining and making a comprehensive list of both the promises and perils of AIS data. Consequently, this study helps researchers and practitioners to (i) identify the sources of noise, (ii) to reduce the noise in AIS data and (iii) use it for the benefits of their research or the optimization of their operations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "",
        "isbn": "978-0-323-88493-8",
        "journal": null,
        "publisher": "Chandos Publishing",
        "title": "authorbiographies",
        "booktitle": "Libraries, Digital Information, and COVID",
        "doi": "10.1016/B978-0-323-88493-8.00030-6",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09265805",
        "isbn": null,
        "journal": "Automation in Construction",
        "publisher": null,
        "title": "classificationandanalysisofdeeplearningapplicationsinconstructionasystematicliteraturereview",
        "booktitle": null,
        "doi": "10.1016/j.autcon.2021.103760",
        "author": [
            "Khallaf, Rana",
            "Khallaf, Mohamed"
        ],
        "keywords": [
            "Systematic literature review, Deep learning, Construction, Damage detection"
        ],
        "abstract": "In recent years, the construction industry has experienced an expansion in the multitude of projects and emergent information. With the advent of deep learning, new opportunities have emerged for utilizing this vast amount of data to solve construction-related issues. While the use of deep learning has been increasing in construction, there has been no review on these applications to date. Therefore, this paper presents a Systematic Literature Review on the use of deep learning applications in construction. A total of 80 journal papers were identified and analyzed. Among these papers, six application-based topics were identified: equipment tracking, crack detection, construction work management, sewer assessment, 3D point cloud enhancement, and miscellaneous topics. Analysis shows that deep learning has been beneficial in leveraging data in areas such as crack detection and segmentation of infrastructure and sewers; equipment and worker detection and; and analysis and reporting on construction-related operations. Additionally, a discussion of the various deep learning techniques is provided as well as a contrast between deep learning, machine learning, and artificial intelligence.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.700",
        "scimago_value": "1,837"
    },
    {
        "issnkey": "03014207",
        "isbn": null,
        "journal": "Resources Policy",
        "publisher": null,
        "title": "applicationofdatamininginiransartisanalandsmallscalemineschallengesanalysis",
        "booktitle": null,
        "doi": "10.1016/j.resourpol.2021.102337",
        "author": [
            "ShakorShahabi, Reza",
            "Qarahasanlou, Ali",
            "Azimi, Seyed",
            "Mottahedi, Adel"
        ],
        "keywords": [
            "Data mining, Artisanal and small-scale mines, Clustering, Decision tree"
        ],
        "abstract": "Most of the mines operating in Iran are classified into Artisanal and Small-scale mines (ASM). ASM accounts for 98.3% of the country's 10,000 mines, more than 80% of employment, and about 65% of the mining sector production. However, these mines face liquidity, legal and administrative issues, sales market, infrastructure, and investment. Though, their activation and restoration require many limited resources compared to large mines. Therefore, it is undeniable to use this sector's capacity to create sustainable employment and development in deprived areas of the country (due to ASM's geographical extent) and help supply raw materials. Hence, in this paper, in the first step, identifying and troubleshooting in these mines was done based on field information and organ documents such as Ministry of Industry, Mine and Trade, Iranian Mines and Mining Industries Development and Renovation Organization (IMIDRO), Iran Minerals Procurement and Production Company, etc. A database consisting of 313 mines from 29 provinces of the country was formed and evaluated using a data mining approach. In this study, two data mining methods, including clustering and decision tree, were used. As a result, appropriate divisions were presented based on available information without any previous hypotheses or backgrounds. The purpose of these divisions was to provide an appropriate classification of mines by applying different estimators to make strategic decisions. Because at present, in most decisions, mines are divided solely based on an estimator such as geographical distance, mineral genus, annual production.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.634",
        "scimago_value": "1,276"
    },
    {
        "issnkey": "02684012",
        "isbn": null,
        "journal": "International Journal of Information Management",
        "publisher": null,
        "title": "settingthefutureofdigitalandsocialmediamarketingresearchperspectivesandresearchpropositions",
        "booktitle": null,
        "doi": "10.1016/j.ijinfomgt.2020.102168",
        "author": [
            "Dwivedi, Yogesh",
            "Ismagilova, Elvira",
            "Hughes, D.",
            "Carlson, Jamie",
            "Filieri, Raffaele",
            "Jacobson, Jenna",
            "Jain, Varsha",
            "Karjaluoto, Heikki",
            "Kefi, Hajer",
            "Krishen, Anjala",
            "Kumar, Vikram",
            "Rahman, Mohammad",
            "Raman, Ramakrishnan",
            "Rauschnabel, Philipp",
            "Rowley, Jennifer",
            "Salo, Jari",
            "Tran, Gina",
            "Wang, Yichuan"
        ],
        "keywords": [
            "Artificial intelligence, Augmented reality marketing, Digital marketing, Ethical issues, eWOM, Mobile marketing, Social media marketing"
        ],
        "abstract": "The use of the internet and social media have changed consumer behavior and the ways in which companies conduct their business. Social and digital marketing offers significant opportunities to organizations through lower costs, improved brand awareness and increased sales. However, significant challenges exist from negative electronic word-of-mouth as well as intrusive and irritating online brand presence. This article brings together the collective insight from several leading experts on issues relating to digital and social media marketing. The experts\u2019 perspectives offer a detailed narrative on key aspects of this important topic as well as perspectives on more specific issues including artificial intelligence, augmented reality marketing, digital content management, mobile marketing and advertising, B2B marketing, electronic word of mouth and ethical issues therein. This research offers a significant and timely contribution to both researchers and practitioners in the form of challenges and opportunities where we highlight the limitations within the current research, outline the research gaps and develop the questions and propositions that can help advance knowledge within the domain of digital and social marketing.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "14.098",
        "scimago_value": "2,770"
    },
    {
        "issnkey": "03772217",
        "isbn": null,
        "journal": "European Journal of Operational Research",
        "publisher": null,
        "title": "prescriptiveanalyticsinpublicsectordecisionmakingaframeworkandinsightsfromcharginginfrastructureplanning",
        "booktitle": null,
        "doi": "10.1016/j.ejor.2020.09.034",
        "author": [
            "Brandt, Tobias",
            "Wagner, Sebastian",
            "Neumann, Dirk"
        ],
        "keywords": [
            "Decision support systems, Public value, Prescriptive analytics, Smart city, Electric mobility"
        ],
        "abstract": "In this work, we investigate the challenges public-sector organizations face when seeking to leverage prescriptive analytics and provide insights into the public value such data-driven tools and methods can provide. Using the strategic triangle of value, legitimacy, and operational capacity as a starting point, we derive a framework to assess public-sector prescriptive analytics initiatives, along with six guiding questions that structure the assessment process. We present a case study applying prescriptive analytics to the placement of charge points in urban areas, a critical challenge many municipalities are currently facing in the transition towards electric mobility. Reflecting on the analytics application as well as its development and implementation process through the guiding questions, we derive key lessons for public-sector organizations seeking to apply prescriptive analytics.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.334",
        "scimago_value": "2,161"
    },
    {
        "issnkey": "01641212",
        "isbn": null,
        "journal": "Journal of Systems and Software",
        "publisher": null,
        "title": "opendataecosystemsanempiricalinvestigationintoanemergingindustrycollaborationconcept",
        "booktitle": null,
        "doi": "10.1016/j.jss.2021.111088",
        "author": [
            "Runeson, Per",
            "Olsson, Thomas",
            "Lin\u00e5ker, Johan"
        ],
        "keywords": [
            "Open data, Open data ecosystem, Open innovation, Empirical study"
        ],
        "abstract": "Software systems are increasingly depending on data, particularly with the rising use of machine learning, and developers are looking for new sources of data. Open Data Ecosystems (ODE) is an emerging concept for data sharing under public licenses in software ecosystems, similar to Open Source Software (OSS). It has certain similarities to Open Government Data (OGD), where public agencies share data for innovation and transparency. We aimed to explore open data ecosystems involving commercial actors. Thus, we organized five focus groups with 27 practitioners from 22 companies, public organizations, and research institutes. Based on the outcomes, we surveyed three cases of emerging ODE practice to further understand the concepts and to validate the initial findings. The main outcome is an initial conceptual model of ODEs\u2019 value, intrinsics, governance, and evolution, and propositions for practice and further research. We found that ODE must be value driven. Regarding the intrinsics of data, we found their type, meta-data, and legal frameworks influential for their openness. We also found the characteristics of ecosystem initiation, organization, data acquisition and openness be differentiating, which we advise research and practice to take into consideration.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.829",
        "scimago_value": "0,642"
    },
    {
        "issnkey": "18788750",
        "isbn": null,
        "journal": "World Neurosurgery",
        "publisher": null,
        "title": "firstamericancollegeofsurgeonsnationalsurgicalqualityimprovementprogramreportfromalowmiddleincomecountrya1yearoutcomeanalysisofneurosurgicalcases",
        "booktitle": null,
        "doi": "10.1016/j.wneu.2021.08.026",
        "author": [
            "Hussain, Mustafa",
            "Bibi, Farida",
            "Shah, Shafqat",
            "Mitha, Rida",
            "Shamim, Muhammad",
            "Ziauddin, Afsheen",
            "Zafar, Hasnain"
        ],
        "keywords": [
            "30-Day complications, ACS-NSQIP, Developing world, Low-middle-income country, Neurosurgery, Pakistan, Postoperative outcomes"
        ],
        "abstract": "Background Low-middle-income countries (LMICs) share a substantial proportion of global surgical complications. This is compounded by the seemingly deficient documentation of postsurgical complications and the lack of a national average for comparison. In this context, the implementation of the American College of Surgeons (ACS) National Surgical Quality Improvement Program (NSQIP) that compares hospital performance based on postsurgical complication data provided by a wide array of centers, could be a major initiative in a resource-challenged setting. Implementation of the NSQIP has provenly mitigated postoperative morbidity and mortality across many centers all over the world. To our knowledge, this report is the first from an LMIC to report its postoperative neurosurgical complications in comparison with international benchmarks. Methods Our hospital joined the NSQIP in 2019. Through a standardized ACS protocol, ACS-trained surgical clinical reviewers (SCRs) reviewed and extracted data from randomly assigned neurosurgical patients\u2019 medical records from preoperative to postoperative (30-day) data using validated, standardized data definitions. SCRs entered deidentified data in an online Health Insurance Portability and Accountability Act web-based secure platform. The validated data were then consigned to the ACS NSQIP head office in the United States where the data were analyzed and compared with similar data from other centers registered with the NSQIP. In this way, our hospital was rated for each of the variables related to postsurgical complications after both spinal and cranial procedures, and the results were sent back to us in the form of text, tables, and graphs. Results Our initial report suggested a relatively higher odds ratio for sepsis and readmissions after spinal procedures at our hospital, and a similarly higher odds ratio for morbidity, sepsis, urinary tract infection, and surgical site infection for cranial procedures. For these variables, our hospital fell in the needs improvement category of the NSQIP. For the rest of the variables studied for both spinal and cranial procedures, the hospital fell in the as expected category of the NSQIP. Conclusions Implementation of the NSQIP is an important first step in creating a culture of transparency, safety, and quality. This is the first report of NSQIP implementation in an LMIC, and we have shown comparable results to developed countries.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03062619",
        "isbn": null,
        "journal": "Applied Energy",
        "publisher": null,
        "title": "reviewofmetaheuristicalgorithmsforwindpowerpredictionmethodologiesapplicationsandchallenges",
        "booktitle": null,
        "doi": "10.1016/j.apenergy.2021.117446",
        "author": [
            "Lu, Peng",
            "Ye, Lin",
            "Zhao, Yongning",
            "Dai, Binhua",
            "Pei, Ming",
            "Tang, Yong"
        ],
        "keywords": [
            "Meta-heuristic algorithms, Wind power forecasting, Combined approach, Multiple time horizons, Multiple error evaluation metrics"
        ],
        "abstract": "The integration of large-scale wind power introduces issues in modern power systems operations due to its strong randomness and volatility. These issues can be resolved via wind power forecasting that can provide comprehensive future information about wind power uncertainties. This paper presents a timely and comprehensive review of meta-heuristic algorithms in the framework of wind power forecasting. The framework is based on the auxiliary layer, forecasting base layer, and core layer. The auxiliary layer, such as the data-decomposition layer, decomposes the wind power time series into many relatively stationary subseries, and uses prediction models, including artificial neural networks (ANNs) and machine learning (ML). The core layer is based on meta-heuristic algorithms, which include evolutionary-based algorithms, physics-based algorithms, human-based algorithms, swarm-based algorithms, hybrid algorithms, and multi-objective optimization algorithms. These algorithms aim to search for the optimal solutions under constraints, which is highly significant for optimizing the key parameters of the prediction models. Besides, multiple error evaluation metrics, e.g., deterministic, uncertainty, and testing methods used in the field of wind power prediction are described. A quantitative analysis focusing on their advantages, disadvantages, forecasting accuracy, and computational costs are also provided. Finally, a few open research issues and trends related to the topic are discussed, which can contribute to improving the understanding of each wind power forecasting method. In general, this review paper provides valuable insights to wind power engineers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.746",
        "scimago_value": "3,035"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "usingsustainableperformancepredictionindatascarcescenariosastudyofparklevelintegratedmicrogridprojectsintianjinchina",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.127042",
        "author": [
            "Wang, Yuan",
            "Li, Shuquan",
            "Wu, Xiuyu",
            "Zhang, Yan",
            "Li, Baoluo",
            "Gao, Lei"
        ],
        "keywords": [
            "Integrated energy system, Sustainable performance prediction, Conditional generative adversarial networks, Long short-term memory, Park level integrated energy system"
        ],
        "abstract": "Time series data of project performance in park-level integrated energy system projects are non-linear, difficult to collect and store, and scarce. Thus, it is difficult to carry out real-time prediction of project performance. In the context of energy and environmental crises, a real-time prediction method for the sustainable development performance of IES projects based on conditional generative adversarial networks-long short-term memory neural networks was proposed after a full study of the park-level IES projects in Tianjin, China. In this study, an evaluation system for the sustainable development performance of IES projects, such as \u201cintegrated energy efficiency,\u201d was established to collect the monthly performance index values of 638 IES projects in Tianjin in 2017. The monthly performance evaluation index was calculated using the entropy weight method. After sorting, a binary method was applied to form the monthly performance evaluation label values,\"1\u2033 corresponding to the top 50% of the project, \u201c0\u2033 corresponding to the bottom 50% projects, establishing a database of historical project performance. The generator in CGAN game training was initially used to learn the mapping relationship between the noise distribution under the predicted conditions and the historical IES project performance data set, resulting in 6220 project data with similar distribution, with improved generalization ability of online data mining and accuracy of the stabilization algorithm. LSTM was then used to capture the time dependence in IES project performance data characteristics to predict monthly sustainable performance after 12 months of project operation. Compared with other machine learning models, this method is time-adaptive and the model structure is simple. The average response time of performance prediction for the same park was shortened to 2.76 months, and the prediction accuracy increased to 98.75%. Three schemes were designed to verify the effectiveness of the proposed method by comparing the real data of the park-level IES project in Tianjin with the predicted results. These results have practical significance for strengthening the real-time control of integrated energy projects and for effectively promoting the sustainable development of the integrated energy industry.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "00139351",
        "isbn": null,
        "journal": "Environmental Research",
        "publisher": null,
        "title": "interdisciplinarydatasciencetoadvanceenvironmentalhealthresearchandimprovebirthoutcomes",
        "booktitle": null,
        "doi": "10.1016/j.envres.2021.111019",
        "author": [
            "Stingone, Jeanette",
            "Triantafillou, Sofia",
            "Larsen, Alexandra",
            "Kitt, Jay",
            "Shaw, Gary",
            "Marsillach, Judit"
        ],
        "keywords": [
            "Preterm birth, Environmental mixtures, Multiple exposures, Public health data science"
        ],
        "abstract": "Rates of preterm birth and low birthweight continue to rise in the United States and pose a significant public health problem. Although a variety of environmental exposures are known to contribute to these and other adverse birth outcomes, there has been a limited success in developing policies to prevent these outcomes. A better characterization of the complexities between multiple exposures and their biological responses can provide the evidence needed to inform public health policy and strengthen preventative population-level interventions. In order to achieve this, we encourage the establishment of an interdisciplinary data science framework that integrates epidemiology, toxicology and bioinformatics with biomarker-based research to better define how population-level exposures contribute to these adverse birth outcomes. The proposed interdisciplinary research framework would 1) facilitate data-driven analyses using existing data from health registries and environmental monitoring programs; 2) develop novel algorithms with the ability to predict which exposures are driving, in this case, adverse birth outcomes in the context of simultaneous exposures; and 3) refine biomarker-based research, ultimately leading to new policies and interventions to reduce the incidence of adverse birth outcomes.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "anewroleforbusinesspromotingtheunitednationssustainabledevelopmentgoalsthroughtheinternetofthingsandblockchaintechnology",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2020.11.066",
        "author": [
            "{de Villiers}, Charl",
            "Kuruppu, Sanjaya",
            "Dissanayake, Dinithi"
        ],
        "keywords": [
            "Internet-of-things, Blockchain, Sustainable development goals, Innovation"
        ],
        "abstract": "We outline the business opportunity for the provision of measurement technology, linked to the internet, i.e. the internet-of-things (IoT), which feeds information into blockchains, providing reliable and trusted data and an incentive for others to contribute towards progress on the United Nations\u2019 Sustainable Development Goals (SDGs). Both existing businesses and start-ups could exploit these new opportunities, which could inspire the participation of employees, volunteers, donors, and other participants. We provide a conceptual framework for the different ways business can play a role in facilitating measurement of SDGs, and trust in these measurements, by harnessing technology.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822258-4",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter15artificialintelligenceinclinicaldecisionmakingfordiagnosisofcardiovasculardiseaseusingepigeneticsmechanisms",
        "booktitle": "Epigenetics in Cardiovascular Disease",
        "doi": "10.1016/B978-0-12-822258-4.00020-1",
        "author": [
            "Kara\u0111uzovi\u0107-Had\u017eiabdi\u0107, Kanita",
            "Peters, Antje"
        ],
        "keywords": [
            "Artificial intelligence, Machine learning, Computational biology, Data mining, Cardiovascular disease, Epigenetics"
        ],
        "abstract": "This chapter provides an overview of machine learning, a mainstream discipline of artificial intelligence. Machine learning is discussed in the context of medical research in general and epigenetics research in cardiology and cardiovascular research in particular. The chapter begins with an overview of machine learning concepts. Main stages of the machine learning workflow including the description of the most popular machine learning techniques used in cardiovascular medicine are presented. In order to reflect the importance of machine learning in biomedical research, selected machine learning applications for disease prediction and diagnosis are reviewed.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03014797",
        "isbn": null,
        "journal": "Journal of Environmental Management",
        "publisher": null,
        "title": "useofspatiotemporalhabitatsuitabilitymodellingtoprioritiseareasforcommoncarpbiocontrolinaustraliausingtheviruscyhv3",
        "booktitle": null,
        "doi": "10.1016/j.jenvman.2021.113061",
        "author": [
            "Graham, K.",
            "Gilligan, D.",
            "Brown, P.",
            "{van Klinken}, R.D.",
            "McColl, K.A.",
            "Durr, P.A."
        ],
        "keywords": [
            "Bayesian networks, Damage thresholds, Expert opinion, Invasive freshwater fish, Murray-Darling basin, Vertebrate pest species"
        ],
        "abstract": "Common carp (Cyprinus carpio) are an invasive species of the rivers and waterways of south-eastern Australia, implicated in the serious decline of many native fish species. Over the past 50 years a variety of control options have been explored, all of which to date have proved either ineffective or cost prohibitive. Most recently the use of cyprinid herpesvirus 3 (CyHV-3) has been proposed as a biocontrol agent, but to assess the risks and benefits of this, as well as to develop a strategy for the release of the virus, a knowledge of the fundamental processes driving carp distribution and abundance is required. To this end, we developed a novel process-based modelling framework that integrates expert opinion with spatio-temporal datasets via the construction of a Bayesian Network. The resulting weekly networks thus enabled an estimate of the habitat suitability for carp across a range of hydrological habitats in south-eastern Australia, covering five diverse catchment areas encompassing in total a drainage area of 132,129 km2 over a period of 17\u201327 years. This showed that while suitability for adult and subadult carp was medium-high across most habitats throughout the period, nevertheless the majority of habitats were poorly suited for the recruitment of larvae and young-of-year (YOY). Instead, high population abundance was confirmed to depend on a small number of recruitment hotspots which occur in years of favourable inundation. Quantification of the underlying ecological drivers of carp abundance thus makes possible detailed planning by focusing on critical weaknesses in the population biology of carp. More specifically, it permits the rational planning for population reduction using the biocontrol agent, CyHV-3, targeting areas where the total population density is above a \u201cdamage threshold\u201d of approximately 100 kg/ha.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "20957564",
        "isbn": null,
        "journal": "Journal of Traffic and Transportation Engineering (English Edition)",
        "publisher": null,
        "title": "amultitaskdeeplearningmodelforshorttermtaxidemandforecastingconsideringspatiotemporaldependences",
        "booktitle": null,
        "doi": "10.1016/j.jtte.2019.07.002",
        "author": [
            "Luo, Huimin",
            "Cai, Jianming",
            "Zhang, Kunpeng",
            "Xie, Ruihang",
            "Zheng, Liang"
        ],
        "keywords": [
            "Traffic engineering, Short-term traffic prediction, Deep learning, Multi-task model, Spatiotemporal dependences"
        ],
        "abstract": "Short-term taxi demand forecasting is of great importance to incentivize vacant cars moving from over-supply regions to over-demand regions, which can minimize the wait time for passengers and drivers. With the consideration of spatiotemporal dependences, this study proposes a multi-task deep learning (MTDL) model to predict short-term taxi demand in multi-zone level. The nonlinear Granger causality test is applied to explore the causality relationships among various traffic zones, and long short-term memory (LSTM) is used as the core neural unit to construct the framework of the multi-task deep learning model. In addition, several hyperparameter optimization methods (e.g., grid search, random search, Bayesian optimization, hyperopt) are used to tune the model. Using the taxi trip data in New York City for validation, the multi-task deep learning model considering spatiotemporal dependences (MTDL\u2217) is compared with the single-task deep learning model (STDL), the full-connected multi-task deep learning model (MTDL#) and other benchmark algorithms (such as LSTM, support vector machine (SVM) and k-nearest neighbors (k-NN)). The experiment results show that the proposed MTDL model is promising to predict short-term taxi demand in multi-zone level, the nonlinear Granger causality analysis is able to capture the spatiotemporal correlations among various traffic zones, and the Bayesian optimization is superior to the other three methods, which verified the feasibility and adaptability of the proposed method.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,656"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "towardsaflexibleprocessindependentmetamodelforproductiondata",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.03.112",
        "author": [
            "Cramer, Simon",
            "Hoffmann, Max",
            "Schlegel, Peter",
            "Kemmerling, Marco",
            "Schmitt, Robert"
        ],
        "keywords": [
            "Process data, Meta-model, Predictive quality, Smart production, Production data, Digital shadow, Data analytics"
        ],
        "abstract": "Data integration is a considerable challenge when investigating information sources from a multi-step manufacturing process. The interpretation of the process data profoundly depends on the incurred meta-data. However, during most data aggregating processes along the production chain, accompanying meta-information of vital importance is lost. To address this shortcoming, we propose a flexible and process-independent meta-model for efficient data integration for multi-step manufacturing processes. The product-oriented model unites process- and meta-data to reflect their mutual relationships within the manufacturing process. The context provided by the meta-information enables automatic data analysis for Predictive Quality applications in a cross-company setting.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "1569190x",
        "isbn": null,
        "journal": "Simulation Modelling Practice and Theory",
        "publisher": null,
        "title": "qosperformanceenhancementpolicythroughcombiningfogandsdn",
        "booktitle": null,
        "doi": "10.1016/j.simpat.2021.102292",
        "author": [
            "Ahammad, Ishtiaq",
            "Khan, Md.",
            "Salehin, Zayed"
        ],
        "keywords": [
            "Internet of Things, Software-Defined Networking, Fog Computing, Quality of Service, Modeling and Simulation, iFogSim Simulator"
        ],
        "abstract": "The goal of Internet of Things (IoT) is to bring any object online, thereby creating a massive volume of data that can overwhelm the existing computing and networking technologies. Therefore, centered cloud isn't ideal for rapidly expanding IoT environmental requirements. Fog computing (FC) moves some portion of the computing load (related to real-time services) from the cloud into edge fog devices. FC is expected to become the subsequent major computing transition and this one has ability to overcome existing cloud limitations. However the key obstacles facing FC are: wide distribution, isolated coupling, quality-of-service (QoS) regulation, adaptability to conditions, and particularly the standardization and normalization is still in phase of development. Software defined networking (SDN) will help fog to solve these obstacles. SDN means unified network control plane (which is separated from data plane), allowing the introduction for advanced traffic control and the orchestration mechanisms of networks and resources. On the grounds of SDN concept, and then combining it with FC, the network type can be modified to resolve all those cloud drawbacks and improve IoT system's QoS. Within this paper, architecture is developed through the combination of independently researched areas of SDN and FC to enhance the QoS in an IoT system. An algorithm (which is dependent on partition the SDN virtually) is presented to support the architecture whose purpose is to select the optimal access point and optimal place to process the data. The main objective of this algorithm is to provide improved QoS by partitioning the corresponding fog devices through the SDN controller. A use case dependent on the presented architecture and algorithm is then provided and assessed this use case's QoS parameter values (network usage, cost, latency and power consumption) using the iFogSim simulator. In contrast to cloud-only deployment, the result indicates a major enhancement of the mentioned QoS parameter values in the deployment of fog with SDN. In addition, once compared to a relative former identical use case; the findings of this paper show improved results for power consumption, network usage and latency. In fact, when compared to a former identical use case, the outcome of this paper shows around 3 times less latency and 2 times less network usage. Finally the ground (IoMT, Industry 4.0, Green IoT, and 5G) that is influenced by this QoS improvement is broadly illustrated in this paper.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "16188667",
        "isbn": null,
        "journal": "Urban Forestry & Urban Greening",
        "publisher": null,
        "title": "whattodoinandwhattoexpectfromurbangreenspacesindicatorbasedapproachtoassessculturalecosystemservices",
        "booktitle": null,
        "doi": "10.1016/j.ufug.2021.126986",
        "author": [
            "Krellenberg, Kerstin",
            "Artmann, Martina",
            "Stanley, Celina",
            "Hecht, Robert"
        ],
        "keywords": [
            "Demand and supply, Indicator conceptualization and operationalization, Multi-step assessment approach, Open data, Recreational urban ecosystem services, Site-level"
        ],
        "abstract": "Literature on urban ecosystem services (UESS) is vast, particularly on cultural ecosystem services. However, due to a lack of knowledge on individual urban green spaces on the site level, further research on enhanced methods is needed to underpin existing assumptions about the reasons why people are visiting urban green spaces and what kinds of ecosystem services they can expect, with a focus on recreational activities. We argue for enhanced methods to assess supply of and demand on cultural UESS that should include the direct work with urban green space users. With the overall aim of developing a Spatial Decision Support System for visiting urban green spaces, we are applying a set of different quantitative methods to gather information on peoples\u2019 needs and perceptions as well as data on what existing green spaces offer them. We present a two-step approach 1) linking green space criteria with recreational activities (demand-side) based on a linear series of three online surveys and 2) conducting a spatial mapping of urban green space criteria based on activity-driven indicators (supply-side). In the course of exemplified indicators operationalized by using open and local authorities\u2032 geospatial data in an explorative study in Dresden and Heidelberg (Germany), we discuss the strengths and weaknesses of the approach.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,163"
    },
    {
        "issnkey": "08966273",
        "isbn": null,
        "journal": "Neuron",
        "publisher": null,
        "title": "cellexploreraframeworkforvisualizingandcharacterizingsingleneurons",
        "booktitle": null,
        "doi": "10.1016/j.neuron.2021.09.002",
        "author": [
            "Petersen, Peter",
            "Siegle, Joshua",
            "Steinmetz, Nicholas",
            "Mahallati, Sara",
            "Buzs\u00e1ki, Gy\u00f6rgy"
        ],
        "keywords": [
            "electrophysiology, extracellular electrodes, framework, graphical interface, standardized processing and data structure, single cell analysis"
        ],
        "abstract": "Summary The large diversity of neuron types provides the means by which cortical circuits perform complex operations. Neuron can be described by biophysical and molecular characteristics, afferent inputs, and neuron targets. To quantify, visualize, and standardize those features, we developed the open-source, MATLAB-based framework CellExplorer. It consists of three components: a processing module, a flexible data structure, and a powerful graphical interface. The processing module calculates standardized physiological metrics, performs neuron-type classification, finds putative monosynaptic connections, and saves them to a standardized, yet flexible, machine-readable format. The graphical interface makes it possible to explore the computed features at the speed of a mouse click. The framework allows users to process, curate, and relate their data to a growing public collection of neurons. CellExplorer can link genetically identified cell types to physiological properties of neurons collected across laboratories and potentially lead to interlaboratory standards of single-cell metrics.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "16188667",
        "isbn": null,
        "journal": "Urban Forestry & Urban Greening",
        "publisher": null,
        "title": "acomparisonofglobalandregionalopendatasetsforurbangreenspacemapping",
        "booktitle": null,
        "doi": "10.1016/j.ufug.2021.127132",
        "author": [
            "Liao, Yiming",
            "Zhou, Qi",
            "Jing, Xuanqiao"
        ],
        "keywords": [
            "FROM-GLC10, Land-use, Land-cover, OpenStreetMap, Park, Urban atlas, Vegetation"
        ],
        "abstract": "Greenspace has positive influences on urban environment and human health, and thus it is desirable to acquire data for (urban) greenspace mapping. Nowadays, global and regional open land-use/land-cover datasets have become essential sources for greenspace mapping, but few studies have quantitatively compared them. To fill this gap, this study carries out a quantitative comparison of six global and regional open datasets (CGLS-LC100, CLC, GLC30, UA, FROM-GLC10 and OSM) for greenspace mapping. First of all, the most appropriate land-use/land-cover classes selected as greenspace are analyzed for each open dataset; then, different open datasets are evaluated and compared in terms of five measures (accuracy, precision, recall, F1-score and green coverage rate). Five urban areas in UK are chosen as study areas. Two categories of reference datasets are used for evaluation, including an Ordnance Survey (OS) greenspace dataset in UK and a number of sampling points classified by referring to Google Earth. Results show that: the OSM dataset performs the best, while comparing with the OS dataset (characterized by a narrowly interpreted greenspace); and the FROM-GLC10 dataset performs the best, while comparing with the sampling points (characterized by a broadly interpreted greenspace). Moreover, by using these two open datasets, most quantitative results are close to or higher than 80 %, in terms of the accuracy, precision, recall and F1-score; in most cases there also is the smallest difference between using these two open datasets and corresponding reference datasets, in terms of the green coverage rate. These findings have benefits for researchers and planners to choose an appropriate open dataset for greenspace mapping.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,163"
    },
    {
        "issnkey": "14740346",
        "isbn": null,
        "journal": "Advanced Engineering Informatics",
        "publisher": null,
        "title": "crowdsourcingtheperceivedurbanbuiltenvironmentviasocialmediathecaseofunderutilizedland",
        "booktitle": null,
        "doi": "10.1016/j.aei.2021.101371",
        "author": [
            "Wang, Yan",
            "Gao, Shangde",
            "Li, Nan",
            "Yu, Siyu"
        ],
        "keywords": [
            "Built environment, Crowdsourcing, Social media, Urban analytics, Underutilized land"
        ],
        "abstract": "Crowdsourcing the public\u2019s perceptions of the built environment in real time enables more responsive and agile infrastructure and land use planning. Social media has emerged to be an effective platform for citizens, engineers, and planners to communicate opinions and feelings transparently. However, a comprehensive terminological resource of the perceived built environment (BE) for consistent data collection and a specified analytical framework are still lacking, particularly for different underutilized land issues. To fill this knowledge gap, we demonstrate a BE-specific term construction and expansion method specifically for collecting Twitter data and propose a Geo-Topic-Sentiment analytical framework for retrieving and analyzing relevant tweets. We conduct a demonstrative study on un(der)utilized land-related BE terms across ten metropolitan statistical areas in the U.S. Findings reveal spatial variations in contents and sentiments about underutilized land environments, and more localized efforts may be required to address specific land use issues across different urban contexts. The research demonstrates Twitter as a useful platform in crowdsourcing perceived BE and sentiments at fine temporal and spatial scales in a timely manner. It contributes to engineering informatics by investigating the role of social media in environmental planning and proposing integrated domain-specific data analytic approaches for engineering practices.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.603",
        "scimago_value": "1,107"
    },
    {
        "issnkey": "",
        "isbn": "978-0-323-90472-8",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter4batterystateestimationmethods",
        "booktitle": "Battery System Modeling",
        "doi": "10.1016/B978-0-323-90472-8.00001-9",
        "author": [
            "Wang, Shunli",
            "Fan, Yongcun",
            "Stroe, Daniel-Ioan",
            "Fernandez, Carlos",
            "Yu, Chunmei",
            "Cao, Wen",
            "Chen, Zonghai"
        ],
        "keywords": [
            "State of charge, State of energy, State of power, State of health, Remaining useful life, Influencing factors, Extended Kalman filtering, Support vector machine, Neural network, Particle filtering"
        ],
        "abstract": "The battery state estimation is a very important task in its management system. The state of charge represents the battery\u2019s remaining energy ratio after a period of use or a long period of disuse, which can reflect the battery life or the battery remaining use time. As for the battery operation, the state parameter reflects its working conditions. The estimation methods are described for the battery state estimation of different working conditions. Before the battery state estimation, the definition of its state parameters is conducted, including state of charge, state of energy, state of power, state of health, and remaining useful life. After that, the main state influencing factors are analyzed as well as algorithm fusion and comparison. The parameter measurement technology is then introduced into the balancing control theory analysis and temperature adjustment. For the estimation method analysis, the foundational methods are analyzed in advance, including open-circuit voltage and ampere hour integral. The smart algorithms are introduced such as extended Kalman filtering, support vector machine, neural network, and particle filtering.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09242716",
        "isbn": null,
        "journal": "ISPRS Journal of Photogrammetry and Remote Sensing",
        "publisher": null,
        "title": "estimatingdailyfullcoveragenearsurfaceo3coandno2concentrationsatahighspatialresolutionoverchinabasedons5ptropomiandgeosfp",
        "booktitle": null,
        "doi": "10.1016/j.isprsjprs.2021.03.018",
        "author": [
            "Wang, Yuan",
            "Yuan, Qiangqiang",
            "Li, Tongwen",
            "Zhu, Liye",
            "Zhang, Liangpei"
        ],
        "keywords": [
            "Full-coverage, Near surface concentrations, Air quality, S5P-TROPOMI, GEOS-FP, COVID-19"
        ],
        "abstract": "The Near Surface Concentrations (NSC) of O3, CO, and NO2 are crucial worldwide indicators of air quality. However, current frameworks devised for the estimation of the NSC of O3, CO, and NO2 have defects, such as coarse spatial resolution and large missing coverage. To address this issue, this study aims to estimate the daily (~13:30 local time) full-coverage NSC of O3, CO, and NO2 at a high spatial resolution (0.05\u00b0 for O3 and NO2; 0.07\u00b0 for CO) over China by using datasets from S5P-TROPOMI and GEOS-FP. In specific, the light gradient boosting machine is employed to train the estimation models. Validation results show that the NSC of O3, CO, and NO2 are well estimated, with the R2s of 0.91, 0.71, and 0.83 for the sample-based cross validation, respectively. Meanwhile, the proposed framework achieves a satisfactory performance in comparison to the latest related works, as reflected by the estimation accuracy and spatial resolution. As for the mapping, the estimated results show coherent spatial distribution and can accurately grasp the seasonal characteristics of each air pollutant. Finally, the estimated results are utilized to analyze the temporal variations of O3, CO, and NO2 during the COrona VIrus Disease 2019 (COVID-19) lockdown in China, which is an extend application for adopting the proposed framework in air quality monitoring. Results show that the estimated NSC of O3, CO, and NO2 in 2020 present significant variations during different periods of the COVID-19 lockdown in China compared to last year. In addition, the variations in the NSC of O3, CO, and NO2 during the COVID-19 lockdown in China possibly result from restrictions in the anthropogenic activities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.979",
        "scimago_value": "2,960"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "exportsalesforecastingusingartificialintelligence",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2020.120480",
        "author": [
            "Sohrabpour, Vahid",
            "Oghazi, Pejvak",
            "Toorajipour, Reza",
            "Nazarpour, Ali"
        ],
        "keywords": [
            "Causal forecasting, Modeling, Export sales forecast, Genetic programming, Artificial intelligence"
        ],
        "abstract": "Sales forecasting is important in production and supply chain management. It affects firms\u2019 planning, strategy, marketing, logistics, warehousing and resource management. While traditional time series forecasting methods prevail in research and practice, they have several limitations. Causal forecasting methods are capable of predicting future sales behavior based on relationships between variables and not just past behavior and trends. This research proposes a framework for modeling and forecasting export sales using Genetic Programming, which is an artificial intelligence technique derived from the model of biological evolution. Analyzing an empirical case of an export company, an export sales forecasting model is suggested. Moreover, a sales forecast for a period of six weeks is conducted, the output of which is compared with the real sales data. Finally, a variable sensitivity analysis is presented for the causal forecasting model.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "03064379",
        "isbn": null,
        "journal": "Information Systems",
        "publisher": null,
        "title": "onthecompositionofthelongtailofbusinessprocessesimplicationsfromaprocessminingstudy",
        "booktitle": null,
        "doi": "10.1016/j.is.2020.101689",
        "author": [
            "Fischer, Marcus",
            "Hofmann, Adrian",
            "Imgrund, Florian",
            "Janiesch, Christian",
            "Winkelmann, Axel"
        ],
        "keywords": [
            "Business Process Management, Long tail of business processes, Process mining, Process performance indicators"
        ],
        "abstract": "Digital transformation forces companies to rethink their processes to meet current customer needs. Business Process Management (BPM) can provide the means to structure and tackle this change. However, most approaches to BPM face restrictions on the number of processes they can optimize at a time due to complexity and resource restrictions. Investigating this shortcoming, the concept of the long tail of business processes suggests a hybrid approach that entails managing important processes centrally, while incrementally improving the majority of processes at their place of execution. This study scrutinizes this observation as well as corresponding implications. First, we define a system of indicators to automatically prioritize processes based on execution data. Second, we use process mining to analyze processes from multiple companies to investigate the distribution of process value in terms of their process variants. Third, we examine the characteristics of the process variants contained in the short head and the long tail to derive and justify recommendations for their management. Our results suggest that the assumption of a long-tailed distribution holds across companies and indicators and also applies to the overall improvement potential of processes and their variants. Across all cases, process variants in the long tail were characterized by fewer customer contacts, lower execution frequencies, and a larger number of involved stakeholders, making them suitable candidates for distributed improvement",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.309",
        "scimago_value": "0,547"
    },
    {
        "issnkey": "15740072",
        "isbn": null,
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter81agriculturaldatacollectiontominimizemeasurementerrorandmaximizecoverage",
        "booktitle": "Handbook of Agricultural Economics",
        "doi": "10.1016/bs.hesagr.2021.10.008",
        "author": [
            "Carletto, Calogero",
            "Dillon, Andrew",
            "Zezza, Alberto"
        ],
        "keywords": [
            "Agriculture, Measurement error, Sampling error, Survey design, Data collection"
        ],
        "abstract": "Advances in agricultural data production provide ever-increasing opportunities for pushing the research frontier in agricultural economics and designing better agricultural policy. As new technologies present opportunities to create new and integrated data sources, researchers face tradeoffs in survey design that may reduce measurement error or increase coverage. In this chapter, we first review the econometric and survey methodology literatures that focus on the sources of measurement error and coverage bias in agricultural data collection. Second, we provide examples of how agricultural data structure affects testable empirical models. Finally, we review the challenges and opportunities offered by technological innovation to meet old and new data demands and address key empirical questions, focusing on the scalable data innovations of greatest potential impact for empirical methods and research.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01663615",
        "isbn": null,
        "journal": "Computers in Industry",
        "publisher": null,
        "title": "dealingwithmissingusagedataindefectpredictionacasestudyofaweldingsupplier",
        "booktitle": null,
        "doi": "10.1016/j.compind.2021.103505",
        "author": [
            "Gashi, Milot",
            "Ofner, Patrick",
            "Ennsbrunner, Helmut",
            "Thalmann, Stefan"
        ],
        "keywords": [
            "Defect prediction, End-of-line testing, Welding industry, Predictive maintenance, Multi-component systems"
        ],
        "abstract": "End-of-line (EoL) testing is performed to determine product quality by ensuring reliable performance. Even though low-quality products may pass EoL testing, they have a high probability of failure over time. Analyzing product usage data can help to improve EoL testing in this regard. However, current approaches do not consider usage data for this purpose. The major challenge for manufacturers is that they do not have access to comprehensive usage data for their products because customers are unwilling to provide usage data. However, manufacturers obtain some usage data from their sales and service departments i.e., contextual data. In this paper, we introduce an alternative approach to improving EoL testing when usage data from customers are missing. We discuss whether it is possible to predict low-quality products from EoL testing data when only contextual information is available (i.e., historical service data and location data of shipped products). We find that a simple, duration-based product usage threshold is sufficient to separate products affected by the production process (low-quality products) from those affected primarily by usage and environmental factors (long-term influence). Low-quality products could only be predicted by combining EoL data and contextual data. Additionally, we identify frequent patterns of maintained components to tackle the challenge of having limited data and promote user acceptance of our predictive model. Finally, we demonstrate our approach by conducting a case study in the welding industry. Our approach can identify frequent component failures and improve product reliability in countries with varying environmental conditions and rates of product usage. We expect that our findings will improve EoL testing protocols in welding and other industries while improving defect prediction models in general.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.635",
        "scimago_value": "1,432"
    },
    {
        "issnkey": "00983004",
        "isbn": null,
        "journal": "Computers & Geosciences",
        "publisher": null,
        "title": "deepconvolutionalneuralnetworkforautomaticfaultrecognitionfrom3dseismicdatasets",
        "booktitle": null,
        "doi": "10.1016/j.cageo.2021.104776",
        "author": [
            "An, Yu",
            "Guo, Jiulin",
            "Ye, Qing",
            "Childs, Conrad",
            "Walsh, John",
            "Dong, Ruihai"
        ],
        "keywords": [
            "Fault recognition, Seismic interpretation, Deep learning, Computer vision, Image processing"
        ],
        "abstract": "With the explosive growth in seismic data acquisition and the successful application of deep convolutional neural networks (DCNN) to various image processing tasks within multidisciplinary fields, many researchers have begun to research DCNN based automatic seismic interpretation techniques. Due to the vast number of parameters considered in deep neural networks, deep learning methods usually require a large amount of data for training. However, collecting a large number of expert interpretations is very time consuming, so related research usually uses synthetic datasets and ignores the practical problems of field datasets. In this paper, we open-source a multi-gigabyte expert-labelled field dataset in response to the challenge of accessing large-scale expert-labelled field datasets. We show that 2D fault recognition within this dataset is an image segmentation or edge detection problem in the computer vision field, that can be expressed as a pixel-level fault/non-fault binary classification. Both types of DCNNs are compared, and we propose a novel fault recognition workflow, which involves processing and screening of seismic images and labels, training DCNNs and automatic numerical evaluation. We have also demonstrated for three case study datasets that effective image augmentation methods can reduce the required labelled crosslines while maintaining satisfactory performance. Our experimental results show that our workflow not only outperforms two state-of-the-art DCNN solutions but also achieves performance comparable to humans on an expert labelled image dataset, even predicting subtle faults that an expert interpreter did not annotate. We suggest that the proposed workflow could reduce the fault interpretation life cycle from months to hours and improve the quality, and define the confidence, of fault interpretation results.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,936"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822144-0",
        "journal": null,
        "publisher": "Chandos Publishing",
        "title": "authorbiographies",
        "booktitle": "Future Directions in Digital Information",
        "doi": "10.1016/B978-0-12-822144-0.09989-4",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01968904",
        "isbn": null,
        "journal": "Energy Conversion and Management",
        "publisher": null,
        "title": "strategiclevelperformanceenhancementofa660mwesupercriticalpowerplantandemissionsreductionbyaiapproach",
        "booktitle": null,
        "doi": "10.1016/j.enconman.2021.114913",
        "author": [
            "Ashraf, Waqar",
            "Uddin, Ghulam",
            "Arafat, Syed",
            "Krzywanski, Jaroslaw",
            "Xiaonan, Wang"
        ],
        "keywords": [
            "Combustion power plant, Fuel management, GHG emission reduction, Artificial intelligence"
        ],
        "abstract": "Power plant heat rate is a plant level performance parameter that indicates the economy of power production, equipment\u2019s safety, and availability. In this paper, seven operating parameters, including the performance indices of integrated energy devices and the environmental conditions are incorporated for modeling the power plant heat rate by Artificial Neural Network (ANN), Support Vector Machine (SVM), and automated machine learning (AutoML) approach. The parametric significance order is determined by ANN and SVM-based Monte Carlo analytics and other machine learning-driven algorithms. Subsequently, the best-performing model is selected based on the external validation test and deployed for knowledge mining purposes. The improvement in the power plant heat rate by the parametric adjustment is achieved and subsequently, up to 3.12 percentage point (pp) increase in the thermal efficiency of the power plant is confirmed. Moreover, the fuel savings corresponding to the improved power plant heat rate are also calculated at three power generation modes. Their equivalence to an annual reduction in emissions is quantified. It is estimated that the accumulated reduction in CO2, SO2, CH4, N2O, and Hg emissions, i.e., 288.2 kilo tons / year (kt/y), can be achieved under 3.15% improvement in the power plant heat rate, corresponding to 75% power generation mode.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.709",
        "scimago_value": "2,743"
    },
    {
        "issnkey": "22145141",
        "isbn": null,
        "journal": "The Crop Journal",
        "publisher": null,
        "title": "highthroughputphenotypingbreakingthroughthebottleneckinfuturecropbreeding",
        "booktitle": null,
        "doi": "10.1016/j.cj.2021.03.015",
        "author": [
            "Song, Peng",
            "Wang, Jinglu",
            "Guo, Xinyu",
            "Yang, Wanneng",
            "Zhao, Chunjiang"
        ],
        "keywords": [
            "High-throughput phenotyping, Crop breeding, Crop phenomics, Phenotyping platform, Data analysis"
        ],
        "abstract": "With the rapid development of genetic analysis techniques and crop population size, phenotyping has become the bottleneck restricting crop breeding. Breaking through this bottleneck will require phenomics, defined as the accurate, high-throughput acquisition and analysis of multi-dimensional phenotypes during crop growth at organism-wide levels, ranging from cells to organs, individual plants, plots, and fields. Here we offer an overview of crop phenomics research from technological and platform viewpoints at various scales, including microscopic, ground-based, and aerial phenotyping and phenotypic data analysis. We describe recent applications of high-throughput phenotyping platforms for abiotic/biotic stress and yield assessment. Finally, we discuss current challenges and offer perspectives on future phenomics research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03787788",
        "isbn": null,
        "journal": "Energy and Buildings",
        "publisher": null,
        "title": "datascienceforbuildingenergyefficiencyacomprehensivetextminingdrivenreviewofscientificliterature",
        "booktitle": null,
        "doi": "10.1016/j.enbuild.2021.110885",
        "author": [
            "Abdelrahman, Mahmoud",
            "Zhan, Sicheng",
            "Miller, Clayton",
            "Chong, Adrian"
        ],
        "keywords": [
            "Reference mining, Natural language processing, Data science, Built environment, Building energy efficiency, Word embeddings"
        ],
        "abstract": "The ever-changing data science landscape is fueling innovation in the built environment context by providing new and more effective means of converting large raw data sets into value for professionals in the design, construction and operations of buildings. The literature developed due to this convergence has rapidly increased in recent years, making it difficult for traditional review approaches to cover all related papers. Therefore, this paper applies a natural language processing (NLP) method to provide an exhaustive and quantitative review.Approximately 30,000 scientific publications were retrieved from the Elsevier API to extract the relationship between data sources, data science techniques, and building energy efficiency applications across the life cycle of buildings. The text-mining and NLP analysis reveals that data sciences techniques are applied more for operation phase applications such as fault detection and diagnosis (FDD), while being under-explored in design and commissioning phases. In addition, it is pointed out that more data science techniques that are to be investigated for various applications. For example, generative adversarial networks (GANs) has potential in facilitating parametric design; transfer learning is a promising path to promoting the application of optimal building operation;",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.879",
        "scimago_value": "1,737"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-814974-4",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter14machinelearningandinsilicomethods",
        "booktitle": "Inhaled Medicines",
        "doi": "10.1016/B978-0-12-814974-4.00013-4",
        "author": [
            "Lin, Ching-Long",
            "Hoffman, Eric",
            "Kassinos, Stavros"
        ],
        "keywords": [
            "Computed tomography, Image registration, Airways, Lung, Asthma, COPD, Machine learning, Deep learning, Clustering, Clusters, Computational fluid and particle dynamics"
        ],
        "abstract": "This chapter reviews the techniques and strategies for identifying subpopulations (clusters) characterized by distinct lung features via machine learning and using cluster information to guide in silico computational fluid and particle dynamics (CFPD) analysis for the design of future inhaled drug delivery methods. We first review the collaborative efforts of collecting imaging, genetic, clinical and biological data sets for large cohorts of healthy, asthma and chronic obstructive pulmonary disease (COPD) subjects to investigate the heterogeneous nature of lung disease. We then focus on imaging-based phenotyping due to its quantitative nature that sensitively captures lung structural and functional alternations at both local (segmental/parenchymal) and global (lobar/lung) scales. Machine learning is then applied to identify imaging clusters for asthma and COPD patients. We select cluster archetypes to perform CFPD analysis and use CFPD-derived variables to interpret the link between cluster-specific alterations and particle depositions in the human lungs. Finally, we discuss the prospect of employing machine learning, physics-based learning and deep learning complementarily toward precision medicine.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03014797",
        "isbn": null,
        "journal": "Journal of Environmental Management",
        "publisher": null,
        "title": "effectsofthejointpreventionandcontrolofatmosphericpollutionpolicyonairpollutantsaquantitativeanalysisofchinesepolicytexts",
        "booktitle": null,
        "doi": "10.1016/j.jenvman.2021.113721",
        "author": [
            "Du, Huibin",
            "Guo, Yaqian",
            "Lin, Zhongguo",
            "Qiu, Yueming",
            "Xiao, Xiao"
        ],
        "keywords": [
            "Joint prevention and control of atmospheric pollution policy, Policy quantification, Emission reduction, Policy effectiveness"
        ],
        "abstract": "Joint prevention and control of atmospheric pollution (JPCAP) policies play a vital role in alleviating regional pollution. Based on Latent Dirichlet Allocation (LDA) model, we construct two policy strength measures of effectiveness and number, and investigate the effects of policy strength on air pollutant emissions for four types of JPCAP policies. The results show that the effects of economic incentive policy tools and supporting policy tools on emission reduction deviate significantly from policy preferences. Economic incentive policy tools are the most effective in promoting emission reductions in SO2, NOx and dust, but their effectiveness are the lowest in reality. Supporting policy tools, with the highest strength, have little effect on emission reduction. Command-control policies and persuasion policies are both relatively high in quantity and effectiveness. In addition, policy strength plays a more important role in reducing air pollutants in key regions than in non-key regions. JPCAP policies have gradually changed from a single policy tool to multiple policy tools, and the government shifted its attention to improving the legal effectiveness of policies after 2015. Finally, we propose some policy implications to optimize JPCAP policies and address regional air pollution problem.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01403664",
        "isbn": null,
        "journal": "Computer Communications",
        "publisher": null,
        "title": "constructionofsuperresolutionmodelofremotesensingimagebasedondeepconvolutionalneuralnetwork",
        "booktitle": null,
        "doi": "10.1016/j.comcom.2021.06.022",
        "author": [
            "Wei, Zikang",
            "Liu, Yunqing"
        ],
        "keywords": [
            "Remote sensing image, Super resolution, Convolution algorithm, Cloud computing"
        ],
        "abstract": "With the continuous improvement of satellite remote sensing technology, using super-resolution image reconstruction technology to reconstruct remote sensing images has important application significance for social development. In the generator model proposed in this paper, the standard convolution layer in the residual network structure is replaced by empty convolution to improve the overall performance of the model while keeping the number of parameters unchanged and the receptive field of convolution at each stage unchanged. By analyzing the advantages of residual network, dense connection network, and cavity convolution in the field of image super resolution, an optimized super-resolution reconstruction model of GAN image with cavity convolution is constructed with dense connection block of cavity residue as a generator component. The cloud computing-based service model is introduced into the image reconstruction system, and the background management module is built through the cloud service system, which is responsible for model training, image transmission, image processing request and database reading. Through experimental analysis, it is proved that the whole automatic data processing from automatic matching data to processing data can be completed, and the performance is better than the traditional service mode, which can produce great economic benefits.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.167",
        "scimago_value": "0,627"
    },
    {
        "issnkey": "0169023x",
        "isbn": null,
        "journal": "Data & Knowledge Engineering",
        "publisher": null,
        "title": "aprofileawaremethodologicalframeworkforcollaborativemultidimensionalmodeling",
        "booktitle": null,
        "doi": "10.1016/j.datak.2021.101875",
        "author": [
            "Sakka, Amir",
            "Bimonte, Sandro",
            "Rizzi, Stefano",
            "Sautot, Lucile",
            "Pinet, Fran\u00e7ois",
            "Bertolotto, Michela",
            "Besnard, Aur\u00e9lien",
            "Rouillier, Noura"
        ],
        "keywords": [
            "Data warehouse design, Collaborative systems, Quality dimensions"
        ],
        "abstract": "Multidimensional modeling, i.e., the design of cube schemata, has a key role in data warehouse (DW) projects, in self-service business intelligence, and in general to let users analyze data via the OLAP paradigm. Though an effective involvement of users in multidimensional modeling is crucial in these projects, not much has been said about how to establish a fruitful collaboration in projects involving numerous users with different skills, reputations, and degrees of authority. This issue is especially relevant in citizen science projects, where several volunteers can contribute their requirements despite not being formally-trained experts in the application domain. To fill this gap, we propose a framework for collaborative multidimensional modeling that can adapt itself to the profiles and skills of the actors involved. We first classify users depending on their authoritativeness, skills, and engagement in the project. Then, following this classification, we identify four possible methodological scenarios and propose a profile-aware methodology supported by two sets of quality attributes. Finally, we describe a Group Decision Support System that implements our methodological framework and present some experiments carried out on a real case study.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13619209",
        "isbn": null,
        "journal": "Transportation Research Part D: Transport and Environment",
        "publisher": null,
        "title": "theroleofmicromobilityinshapingsustainablecitiesasystematicliteraturereview",
        "booktitle": null,
        "doi": "10.1016/j.trd.2021.102734",
        "author": [
            "Abduljabbar, Rusul",
            "Liyanage, Sohani",
            "Dia, Hussein"
        ],
        "keywords": [
            "Micro-mobility, Smart cities, Systematic literature review, Sustainable transport, Bibliometric networks, Co-citation analysis"
        ],
        "abstract": "Micro-mobility is increasingly recognised as a promising mode of urban transport, particularly for its potential to reduce private vehicle use for short-distance travel. Despite valuable research contributions that represent fundamental knowledge on this topic, today\u2019s body of research appears quite fragmented in relation to the role of micro-mobility as a transformative solution for meeting sustainability outcomes in urban environments. This paper consolidates knowledge on the topic, analyses past and on-going research developments, and provides future research directions by using a rigorous and auditable systematic literature review methodology. To achieve these objectives, the paper analysed 328 journal publications from the Scopus database covering the period between 2000 and 2020. A bibliographic analysis was used to identify relevant publications and explore the changing landscape of micro-mobility research. The study constructed and visualised the literature\u2019s bibliometric networks through citations and co-citations analyses for authors, articles, journals and countries. The findings showed a consistent spike in recent research outputs covering the sustainability aspects of micro-mobility reflecting its importance as a low-carbon and transformative mode of urban transport. The co-citation analysis, in particular, helped to categorise the literature into four main research themes that address benefits, technology, policy and behavioural mode-choice categories where the majority of research has been focused during the analysis period. For each cluster, inductive reasoning is used to discuss the emerging trends, barriers as well as pathways to overcome challenges to wide-scale deployment. This article provides a balanced and objective summary of research evidence on the topic and serves as a reference point for further research on micro-mobility for sustainable cities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.495",
        "scimago_value": "1,600"
    },
    {
        "issnkey": "01678809",
        "isbn": null,
        "journal": "Agriculture, Ecosystems & Environment",
        "publisher": null,
        "title": "tradeoffbetweencarbonsequestrationandwaterlossforvegetationgreeninginchina",
        "booktitle": null,
        "doi": "10.1016/j.agee.2021.107522",
        "author": [
            "Lan, Xin",
            "Liu, Zhiyong",
            "Chen, Xiaohong",
            "Lin, Kairong",
            "Cheng, Linying"
        ],
        "keywords": [
            "Land use, Forest, Cropland, GPP, NPP, Evapotranspiration, PT-JPL model"
        ],
        "abstract": "Land use management of forests and croplands mainly drives the vegetation greening in China. Vegetation greening strongly modulates the trade-off between carbon sequestration via photosynthesis and water loss from evapotranspiration (ET) at the terrestrial ecosystem (representing by ecosystem water use efficiency, WUE). The function of vegetation greening in terrestrial carbon sequestration is well known, but the impacts of water loss from ET caused by vegetation greening on WUE are often neglected. Here, the GIS-based Priestley-Taylor Jet Propulsion Laboratory model was established to evaluate ET in China from 2001 to 2015, incorporating vegetation dynamics as a key component. To quantify the net effect of the ET caused by vegetation greening on WUE, we compared two different simulation scenarios: actual vegetation greening scenario and simulated without vegetation greening scenario. The results show that forests and croplands mainly contribute to the growth in GPP and NPP in China with annual rates of 2.53 gC\u00b7m\u22122 yr\u22122 and 1.59 gC\u00b7m\u22122 yr\u22122 from 2001 to 2015, respectively. With the increase of terrestrial carbon sequestration, the ET under actual vegetation greening scenario was generated 6.78 mm\u00b7yr\u22121 more than that under simulated without vegetation greening scenario. But as a result of the negative impacts of vegetation physiological effect (elevated CO2 concentration and the decreased VPD) on ET, values of ET under two different scenarios all exhibited a decline trend from 2001 to 2015 with rates of \u2212 2.04% and \u2212 3.63%, respectively. Consequently, although the WUE under two different scenarios exhibited increased trends (6.44%, actual vegetation dynamics scenario; 10.74%, simulated without vegetation greening scenario), the ET caused by vegetation greening led to an obvious divergence between the WUE under two different scenarios. For better understanding the impacts of human activities on carbon and water cycles at the terrestrial ecosystem, it is necessary to take the water loss from ET caused by vegetation greening into consideration, which is crucial for enhancing the sustainability of future vegetation-related projects.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,844"
    },
    {
        "issnkey": "23727705",
        "isbn": null,
        "journal": "Molecular Therapy - Oncolytics",
        "publisher": null,
        "title": "apolygenicmethylationpredictionmodelassociatedwithresponsetochemotherapyinepithelialovariancancer",
        "booktitle": null,
        "doi": "10.1016/j.omto.2021.02.012",
        "author": [
            "Zhao, Lanbo",
            "Ma, Sijia",
            "Wang, Linconghua",
            "Wang, Yiran",
            "Feng, Xue",
            "Liang, Dongxin",
            "Han, Lu",
            "Li, Min",
            "Li, Qiling"
        ],
        "keywords": [
            "ovarian cancer, bioinformatics, DNA methylation, chemotherapy response, prediction model, AGR2, HSPA2, ACAT2"
        ],
        "abstract": "To identify potential aberrantly differentially methylated genes (DMGs) correlated with chemotherapy response (CR) and establish a polygenic methylation prediction model of CR in epithelial ovarian cancer (EOC), we accessed 177 (47 chemo-sensitive and 130 chemo-resistant) samples corresponding to three DNA-methylation microarray datasets from the Gene Expression Omnibus and 306 (290 chemo-sensitive and 16 chemo-resistant) samples from The Cancer Genome Atlas (TCGA) database. DMGs associated with chemotherapy sensitivity and chemotherapy resistance were identified by several packages of R software. Pathway enrichment and protein-protein interaction (PPI) network analyses were constructed by Metascape software. The key genes containing mRNA expressions associated with methylation levels were validated from the expression dataset by the GEO2R platform. The determination of the prognostic significance of key genes was performed by the Kaplan-Meier plotter database. The key genes-based polygenic methylation prediction model was established by binary logistic regression. Among accessed 483 samples, 457 (182 hypermethylated and 275 hypomethylated) DMGs correlated with chemo resistance. Twenty-nine hub genes were identified and further validated. Three genes, anterior gradient 2 (AGR2), heat shock-related 70-kDa protein 2 (HSPA2), and acetyltransferase 2 (ACAT2), showed a significantly negative correlation between their methylation levels and mRNA expressions, which also corresponded to prognostic significance. A polygenic methylation prediction model (0.5253 cutoff value) was established and validated with 0.659 sensitivity and 0.911 specificity.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.200",
        "scimago_value": "1,424"
    },
    {
        "issnkey": "00981354",
        "isbn": null,
        "journal": "Computers & Chemical Engineering",
        "publisher": null,
        "title": "ananalysisofprocessfaultdiagnosismethodsfromsafetyperspectives",
        "booktitle": null,
        "doi": "10.1016/j.compchemeng.2020.107197",
        "author": [
            "Arunthavanathan, Rajeevan",
            "Khan, Faisal",
            "Ahmed, Salim",
            "Imtiaz, Syed"
        ],
        "keywords": [
            "Process safety, Process risk management, Process failure analysis, Process fault diagnosis, Process automation"
        ],
        "abstract": "Industry 4.0 provides substantial opportunities to ensure a safer environment through online monitoring, early detection of faults, and preventing the faults to failures transitions. Decision making is an important step in abnormal situation management. Assigning risk based on the consequences may provide additional information for abnormal situation management decisions to prevent the accident before it occurs. This paper analyzes the interconnections between the three essential aspects of process safety: fault detection and diagnosis (FDD), risk assessment (RA), and abnormal situation management (ASM) in the context of the current and next generation of process systems. The authors present their thoughts on research directions in process safety in Industry 4.0. This article aims to serve as a road map for the next generation of process safety research to enable safer and sustainable process operations and development.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,017"
    },
    {
        "issnkey": "15356108",
        "isbn": null,
        "journal": "Cancer Cell",
        "publisher": null,
        "title": "artificialintelligenceforclinicaloncology",
        "booktitle": null,
        "doi": "10.1016/j.ccell.2021.04.002",
        "author": [
            "Kann, Benjamin",
            "Hosny, Ahmed",
            "Aerts, Hugo"
        ],
        "keywords": [
            "artificial intelligence, clinical oncology, precision medicine, care pathway, clinical translation"
        ],
        "abstract": "Summary Clinical oncology is experiencing rapid growth in data that are collected to enhance cancer care. With recent advances in the field of artificial intelligence (AI), there is now a computational basis to integrate and synthesize this growing body of multi-dimensional data, deduce patterns, and predict outcomes to improve shared patient and clinician decision making. While there is high potential, significant challenges remain. In this perspective, we propose a pathway of clinical cancer care touchpoints for narrow-task AI applications and review a selection of applications. We describe the challenges faced in the clinical translation of AI and propose solutions. We also suggest paths forward in weaving AI into individualized patient care, with an emphasis on clinical validity, utility, and usability. By illuminating these issues in the context of current AI applications for clinical oncology, we hope to help advance meaningful investigations that will ultimately translate to real-world clinical use.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "31.743",
        "scimago_value": "13,035"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "aibasedpowerscreeningsolutionforsarscov2infectionasociodemographicsurveyandcovid19coughdetector",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.10.081",
        "author": [
            "Sadhana, S",
            "Pandiarajan, S",
            "Sivaraman, E",
            "Daniel, D"
        ],
        "keywords": [
            "Artificial Intelligence, Coronavirus (SARS \u2013CoV2), Machine Learning, Pathomorphological Variations, Power Screening Solutions"
        ],
        "abstract": "Globally, the confirmed coronavirus (SARS-CoV2) cases are being increasing day by day. Coronavirus (COVID-19) causes an acute infection in the respiratory tract that started spreading in late 2019. Huge datasets of SARS-CoV2 patients can be incorporated and analyzed by machine learning strategies for understanding the pattern of pathological spread and helps to analyze the accuracy and speed of novel therapeutic methodologies, also detect the susceptible people depends on their physiological and genetic aspects. To identify the possible cases faster and rapidly, we propose the Artificial Intelligence (AI) power screening solution for SARS- CoV2 infection that can be deployable through the mobile application. It collects the details of the travel history, symptoms, common signs, gender, age and diagnosis of the cough sound. To examine the sharpness of pathomorphological variations in respiratory tracts induced by SARS-CoV2, that compared to other respiratory illnesses to address this issue. To overcome the shortage of SARS-CoV2 datasets, we apply the transfer learning technique. Multipronged mediator for risk-averse Artificial Intelligence Architecture is induced for minimizing the false diagnosis of risk-stemming from the problem of complex dimensionality. This proposed application provides early detection and prior screening for SARS-CoV2 cases. Huge data points can be processed through AI framework that can examine the users and classify them into \u201cProbably COVID\u201d, \u201cProbably not COVID\u201d and \u201cResult indeterminate\u201d.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821259-2",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter16prospectandadversityofartificialintelligenceinurology",
        "booktitle": "Artificial Intelligence in Medicine",
        "doi": "10.1016/B978-0-12-821259-2.00016-8",
        "author": [
            "Eminaga, Okyaz",
            "Liao, Joseph"
        ],
        "keywords": [
            "Urology, artificial intelligence, MRI, CT, urine analyses, AI-based solution, prostate cancer, bladder cancer, kidney cancer, ultrasound, diagnostic imaging, prediction models"
        ],
        "abstract": "The emergence of artificial intelligence (AI) has opened a new avenue for tackling existing challenges in clinical routine. This chapter will briefly introduce potential applications of AI in urology and focus on its benefits and barriers in solving real clinical problems. First, the introduction section will generally discuss AI and existing data resources. Then, the chapter will explain the potential application of AI in urological endoscopy, urine, stone and andrology, imaging and the robotic surgery. Further, this chapter will briefly discuss some tools of risk predictions for urological cancer. Finally, the author will discuss the potential future direction of AI in urology.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818779-1",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter3specificationofarchitecturelayersandreferencemodelsofinternetofthingssystems",
        "booktitle": "Management of IOT Open Data Projects in Smart Cities",
        "doi": "10.1016/B978-0-12-818779-1.00003-1",
        "author": [
            "Orlowski, Cezary"
        ],
        "keywords": [
            "IoT networks, sensors, communication layers and protocols, data collection"
        ],
        "abstract": "While the first chapter introduced open data and Smart Cities, the second chapter presented the environment for generating open data, in this chapter we discuss the layers of architecture of the Internet of Things systems. Characteristics of microcontrollers and sensors are presented in detail, and communication protocols are discussed. The purpose of this chapter is to familiarize the reader with the description of basic internet devices and methods of their communication.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10848045",
        "isbn": null,
        "journal": "Journal of Network and Computer Applications",
        "publisher": null,
        "title": "contextawareschedulinginfogcomputingasurveytaxonomychallengesandfuturedirections",
        "booktitle": null,
        "doi": "10.1016/j.jnca.2021.103008",
        "author": [
            "Islam, Mir",
            "Kumar, Ashok",
            "Hu, Yu-Chen"
        ],
        "keywords": [
            "Fog computing, Context-awareness, Scheduling, Resource management, Resource estimation, Resource provisioning, Contextual information"
        ],
        "abstract": "Fog computing extends Cloud-based facilities and stays in the vicinity of the end-users to provide an attractive solution to a diverse range of latency-sensitive applications. The applications are becoming more sophisticated, context-aware, and computation-intensive due to varying situational and environmental conditions in order to meet the ever-increasing users\u2019 demands. Further, resource heterogeneity, dynamic nature, resource limitations, and unpredictability of the Fog environment make scheduling of application tasks while satisfying Quality of Service (QoS) requirements a challenging job. To overcome these issues various scheduling strategies have been proposed considering contextual information of different entities involved in Fog computing. This survey represents a comprehensive literature analysis pertaining to context-aware scheduling in Fog computing. It provides detailed comparison of existing scheduling approaches based on important factors such as context-aware parameters, case studies, performance metrics, and evaluation tools along with advantages and limitations. It also presents detailed taxonomy, performance metrics, and context-aware parameter analysis. Further, it list several issues and challenges. This study will aid the research community in exploring future research directions and essential aspects of scheduling approaches using different types of contextual information.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22131337",
        "isbn": null,
        "journal": "Astronomy and Computing",
        "publisher": null,
        "title": "robustgaussianprocessregressionbasedoniterativetrimming",
        "booktitle": null,
        "doi": "10.1016/j.ascom.2021.100483",
        "author": [
            "Li, Zhao-Zhou",
            "Li, Lu",
            "Shao, Zhengyi"
        ],
        "keywords": [
            "Gaussian process, Robust regression, Outlier detection, Ridge line, Star clusters"
        ],
        "abstract": "The Gaussian process (GP) regression can be severely biased when the data are contaminated by outliers. This paper presents a new robust GP regression algorithm that iteratively trims the most extreme data points. While the new algorithm retains the attractive properties of the standard GP as a nonparametric and flexible regression method, it can greatly improve the model accuracy for contaminated data even in the presence of extreme or abundant outliers. It is also easier to implement compared with previous robust GP variants that rely on approximate inference. Applied to a wide range of experiments with different contamination levels, the proposed method significantly outperforms the standard GP and the popular robust GP variant with the Student-t likelihood in most test cases. In addition, as a practical example in the astrophysical study, we show that this method can precisely determine the main-sequence ridge line in the color\u2013magnitude diagram of star clusters.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.927",
        "scimago_value": "0,692"
    },
    {
        "issnkey": "87563282",
        "isbn": null,
        "journal": "Bone",
        "publisher": null,
        "title": "theriskofhipfracturesinindividualsover50yearsoldwithprediabetesandtype2diabetesalongitudinalnationwidepopulationbasedstudy",
        "booktitle": null,
        "doi": "10.1016/j.bone.2020.115691",
        "author": [
            "Park, Ho",
            "Han, Kyoungdo",
            "Kim, Youngwoo",
            "Kim, Yoon",
            "Sur, Yoo"
        ],
        "keywords": [
            "Diabetes mellitus, Prediabetic state, Diabetic complications, Hip fractures, Risk assessment, Cohort studies"
        ],
        "abstract": "Background The present study aimed to investigate the association between type 2 diabetes mellitus (T2DM) and hip fractures using a large-scale nationwide population-based cohort that is representative of the Republic of Korea. We determined the risks of hip fractures in individuals with prediabetes and T2DM with different diabetes durations, and compared them with the risks of hip fractures in individuals without T2DM. Methods A total of 5,761,785 subjects over 50 years old who underwent the National Health Insurance Service medical checkup in 2009\u20132010 were included. Subjects were classified into 5 groups based on the diabetes status; Normal, Prediabetes, Newly-diagnosed T2DM, T2DM less than 5 years, and T2DM more than 5 years. They were followed from the date of the medical checkup to the end of 2016. The endpoint was a new development of hip fracture during follow-up. The hazard ratios (HRs) and 95% confidence intervals (CIs) of hip fractures for each group were analyzed using Cox proportional hazard regression models after adjusting for age, sex, smoking, alcohol drinking, regular exercise, body mass index, hypertension, dyslipidemia, and chronic kidney disease. Results The HRs of hip fractures were 1 in the Normal group, 1.032 (95% CI: 1.009, 1.056) in the Prediabetes group, 1.168 (95% CI: 1.113, 1.225) in the Newly-diagnosed T2DM2, 1.543 (95% CI: 1.495, 1.592) in the T2DM less than 5 years and 2.105 (95% CI: 2.054, 2.157) in the T2DM more than 5 years. The secular trend of the HRs of hip fractures according to the duration of T2DM was statistically significant (P < .001). Subgroup analyses also showed the same increasing pattern of the HRs of hip fractures according to the duration of T2DM in both sexes and all age groups (50\u201364 years, 65\u201374 years, over 75 years). Conclusions In summary, this large-scale, retrospective, longitudinal, nationwide population-based cohort study of 5,761,785 subjects demonstrated that the risks of hip fractures started to increase in prediabetes and was associated linearly with the duration of T2DM. The secular trend of risks of hip fractures according to the duration of T2DM was consistent in both sexes and all age groups.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.398",
        "scimago_value": "1,346"
    },
    {
        "issnkey": "03787788",
        "isbn": null,
        "journal": "Energy and Buildings",
        "publisher": null,
        "title": "developmentofelectricityconsumptionprofilesofresidentialbuildingsbasedonsmartmeterdataclustering",
        "booktitle": null,
        "doi": "10.1016/j.enbuild.2021.111376",
        "author": [
            "Cz\u00e9t\u00e1ny, L\u00e1szl\u00f3",
            "V\u00e1mos, Vikt\u00f3ria",
            "Horv\u00e1th, Mikl\u00f3s",
            "Szalay, Zsuzsa",
            "Mota-Babiloni, Adri\u00e1n",
            "Deme-B\u00e9lafi, Zs\u00f3fia",
            "Csoknyai, Tam\u00e1s"
        ],
        "keywords": [
            "Electricity consumption profile, Smart meter, Data clustering, K-means, Fuzzy k-means, Hierarchical, Residential buildings"
        ],
        "abstract": "In the present research, a high-resolution, detailed electric load dataset was assessed, collected by smart meters from nearly a thousand households in Hungary, many of them single-family houses. The objective was to evaluate this database in detail to determine energy consumption profiles from time series of daily and annual electric load. After representativity check of dataset daily and annual energy consumption profiles were developed, applying three different clustering methods (k-means, fuzzy k-means, agglomerative hierarchical) and three different cluster validity indexes (elbow method, silhouette method, Dunn index) in MATLAB environment. The best clustering method for our examination proved to be the k-means clustering technique. Analyses were carried out to identify different consumer groups, as well as to clarify the impact of specific parameters such as meter type in the housing unit (e.g. peak, off-peak meter), day of the week (e.g. weekend, weekday), seasonality, geographical location, settlement type and housing type (single-family house, flat, age class of the building). Furthermore, four electric user profile types were proposed, which can be used for building energy demand simulation, summer heat load and winter heating demand calculation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.879",
        "scimago_value": "1,737"
    },
    {
        "issnkey": "10848045",
        "isbn": null,
        "journal": "Journal of Network and Computer Applications",
        "publisher": null,
        "title": "asurveyondeeplearningforchallengednetworksapplicationsandtrends",
        "booktitle": null,
        "doi": "10.1016/j.jnca.2021.103213",
        "author": [
            "Bochie, Kaylani",
            "Gilbert, Mateus",
            "Gantert, Luana",
            "Barbosa, Mariana",
            "Medeiros, Dianne",
            "Campista, Miguel"
        ],
        "keywords": [
            "Challenged networks, Internet of Things, Sensor networks, Industrial networks, Wireless mobile networks, Vehicular networks, Deep learning, Machine learning"
        ],
        "abstract": "Computer networks are dealing with growing complexity, given the ever-increasing volume of data produced by all sorts of network nodes. Performance improvements are a non-stop ambition and require tuning fine-grained details of the system operation. Analyzing such data deluge, however, is not straightforward and sometimes not supported by the system. There are often problems regarding scalability and the predisposition of the involved nodes to understand and transfer the data. This issue is at least partially circumvented by knowledge acquisition from past experiences, which is a characteristic of the herein called \u201cchallenged networks\u201d. The addition of intelligence in these scenarios is fundamental to extract linear and non-linear relationships from the data collected by multiple sources. This is undoubtedly an invitation to machine learning and, more particularly, to deep learning. This paper identifies five different challenged networks: IoT and sensor, mobile, industrial, and vehicular networks as typical scenarios that may have multiple and heterogeneous data sources and face obstacles concerning connectivity. As a consequence, deep learning solutions can contribute to system performance by adding intelligence and the ability to interpret data. We start the paper by providing an overview of deep learning, further explaining this approach\u2019s benefits over the cited scenarios. We propose a workflow based on our observations of deep learning applications over challenged networks, and based on it, we strive to survey the literature on deep-learning-based solutions at an application-oriented level using the PRISMA methodology. Afterward, we also discuss new deep learning techniques that show enormous potential for further improvements as well as transversal issues, such as security. Finally, we provide lessons learned raising trends linking all surveyed papers to deep learning approaches. We are confident that the proposed paper contributes to the state of the art and can be a piece of inspiration for beginners and also for enthusiasts on advanced networking research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-816078-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "computationaltoxicology",
        "booktitle": "Systems Medicine",
        "doi": "10.1016/B978-0-12-801238-3.11534-X",
        "author": [
            "Schmidt, Friedemann"
        ],
        "keywords": [
            "Applicability domain, Artificial intelligence, Classification, Computational toxicology, Data mining, Descriptor, Expert system, In silico, In silico profiling, Machine learning, Off target toxicity, QSAR, Rule-based, SAR, Toxicology"
        ],
        "abstract": "Powerful computer systems and growing amounts of curated data have rocketed the implementation of computational disciplines in all fields of drug discovery sciences, including toxicology. Computational toxicology feeds from the needs of researchers, manufacturers, regulators and patients to fully characterize the safety profiles of drugs to avoid the exposure of patients at risk. In this article, we introduce basic concepts of computational toxicology and discuss algorithms and tools that are widely employed to complement experimental laboratory work. Since more than two decades, lab work and particularly animal experimentation have found more and more counterparts by in-silico simulation and computational prediction. These methods are employed to inform scientists early in the drug discovery process, fill data gaps, explain causalities and ultimately reduce experimentation and cycle times. Since computational toxicology is applied throughout the full drug discovery value chain, from research to clinic, it has given rise to using an extraordinarily broad kit of tools employing data mining, curation of structural alerts, molecular fragment methods, quantitative structure-property relationships and machine learning. While the use of data-driven technologies is exploding, and multiple competing applications become available for predictive modeling, their underlying training datasets for individual endpoints may still be small\u2014too small to allow for global applicability. Care must be taken therefore to ensure that predictive methods have been properly validated and are fit-for-purpose. We discuss such practice, as well as model interpretability and how computational methods can contribute to generate hypotheses and support the progression of new molecules.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "theneedforurbanformdatainspatialmodelingofurbancarbonemissionsinchinaacriticalreview",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.128792",
        "author": [
            "Cai, Meng",
            "Shi, Yuan",
            "Ren, Chao",
            "Yoshida, Takahiro",
            "Yamagata, Yoshiki",
            "Ding, Chao",
            "Zhou, Nan"
        ],
        "keywords": [
            "Urban carbon emissions, Spatial modeling, Systematic review, Urban form, China"
        ],
        "abstract": "Cities produce over 70% of global carbon emissions and are thus crucial in driving climate change. Urban carbon emissions may continue to increase especially in those less-developed countries and regions which are still under rapid urban development. Policymakers need to find ways to effectively control and reduce carbon emissions. Thus, spatial modeling methods to map and predict urban carbon emissions have been developed to meet these needs. This paper examines the progress of the spatial modeling of carbon emissions and the relationship between urban form and carbon emissions in China by reviewing more than 100 peer-reviewed journal articles in the Scopus database. The latest prediction methods and techniques are described in the paper. Their advantages and limitations are then discussed. Urban forms have a significant influence on carbon emissions and have been applied in spatial modeling studies in other countries. However, this review has identified the lack of urban form data and high-resolution inventories from existing studies in China. Future developments in the spatial modeling in China should therefore have a fine spatial resolution and incorporate open and high-quality urban form data, including urban morphology and land use/land cover.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "buildingthebiofactoryabibliometricanalysisofcirculareconomiesandlifecyclesustainabilityassessmentinwastewatertreatment",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.129127",
        "author": [
            "Furness, Madeline",
            "Bello-Mendoza, Ricardo",
            "Dassonvalle, Jonatan",
            "Chamy-Maggi, Rolando"
        ],
        "keywords": [
            "Life cycle sustainability assessment, Resource recovery, Wastewater treatment, Circular economies, Biofactory"
        ],
        "abstract": "The \u201cBiofactory\u201d is a circular economy-based concept for wastewater treatment that improves water quality, promotes efficient use of materials and energy, while also recovering resources and decreasing both emissions and costs. Due to socio-economic bottlenecks, such as typical high costs and low public acceptance of novel resource recovery scenarios in wastewater treatment, realizing the Biofactory goals becomes a difficult task. Decision makers are currently unable to appreciate the environmental, social, and economic benefits of the Biofactory, as most decision-making tools focus on mainly technical and economic aspects. This is the first review is to use bibliometric analysis of publication trends in life cycle-based modelling for circular economies in wastewater treatment across the globe, focusing on Life Cycle Assessment (LCA), Life Cycle Costing (LCC) and Social Life Cycle Assessment (SLCA). The integration of LCA, LCC and SLCA for the development of a Life Cycle Sustainability Assessment (LCSA) decision making framework is recommended, while practical implications for the methodological development of this tool are compiled. The goal and scope of LCSA for the Biofactory must explore multi-product functional units for the recovery of different resources from wastewater, where system boundaries must include techno-environmental, social and economic systems together. Internal loops, feedbacks loops, avoided products and co-product allocation methodologies within the integrated system boundaries must be considered. Innovation is required for building LCA, LCC and SLCA inventories, where life cycle data management is important for implementing the Biofactory into the future.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818617-6",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter10environmentalsensingareviewofapproachesusinggpsgnss",
        "booktitle": "GPS and GNSS Technology in Geosciences",
        "doi": "10.1016/B978-0-12-818617-6.00013-5",
        "author": [
            "Faka, Antigoni",
            "Tserpes, Konstantinos",
            "Chalkias, Christos"
        ],
        "keywords": [
            "Air pollution, Crowdsourcing, Environmental monitoring, Noise pollution, Smartphones"
        ],
        "abstract": "The condition of the environment is a critical factor for the quality of life and the well-being of humans. Environmental assessment ensures foreseeing and address of potential complications at an early stage in environmental planning and management. The recent technological advancements in spatiotemporal monitoring of environmental features provide to the scientific community the proper material for efficient and accurate assessment of the environment. This chapter aims to review the role of Global Positioning System (GPS)/Global Navigation Satellite System (GNSS) technologies in environmental sensing. The presentation focuses on the technologies and approaches followed in terms of data collection, analysis, and visualization and demonstrates the major categories of environmental sensing applications. The findings reveal a constantly gaining momentum of mobile monitoring and participatory systems. The adoption of GPS/GNSS enhancement techniques, including crowdsourcing methods, would assist environmental policymaking to establish targets and prioritize planned actions.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03605442",
        "isbn": null,
        "journal": "Energy",
        "publisher": null,
        "title": "taxiridesharinginkuwaiteconenvirostudy",
        "booktitle": null,
        "doi": "10.1016/j.energy.2021.120269",
        "author": [
            "AlKheder, Sharaf"
        ],
        "keywords": [
            "Traffic, Ridesharing, Passengers, Fuel consumption, Gas emissions"
        ],
        "abstract": "Traffic congestion had been the most important and sensitive problem facing Kuwait roads for decades. The objectives of this study were to: optimize the traffic flow on road networks by implementing shared rides, reduce vehicle emissions and minimize the ride cost. The concept concentrated on sharing the journeys starting and ending around the same place and time. The effect of ridesharing was studied according to the data collected for 3 months (July 2018, October 2018 and January 2019) from a taxi company. The data was then filtered so that the start point of all trips was South Surra. Three different scenarios were created: single passenger, two passengers, and more than two passengers. Java NetBeans was used to calculate the total distance for each month and the cost for each trip after applying the concept. \u201cMy driving\u201d was used to obtain the fuel consumption to calculate the gas emissions for the three scenarios;CO, NOx and HC emissions. For the trip distance analysis, the results for July, October, and January decreased by 0.84%, 0.45%, and 1.25%, respectively. For gas emissions and by comparing the first scenario with the second one, the CO, NOx and HC emissions were reduced by 9.072%, 9.069%, and 9.074%, respectively. Finally, by comparing the first scenario with the third one, CO, NOx and HC emissions were lowered by 0.116%, 0.08%, and 0.108%, respectively.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "digitalmanufacturingforsmartsmallsatellitessystems",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.01.138",
        "author": [
            "Krau\u00df, Markus",
            "Leutert, Florian",
            "Scholz, Markus",
            "Fritscher, Michael",
            "He\u00df, Robin",
            "Lilge, Christian",
            "Schilling, Klaus"
        ],
        "keywords": [
            "digital manufacturing, demonstrator factory, new space, augmented reality user interfaces, human-robot collaboration, predictive maintenance, telemaintenance, quality of service, mobile robotics, intelligent material flow"
        ],
        "abstract": "The term Industry 4.0 \u2013 manufacturing with exploitation of digital technologies \u2013 comprises several challenging topics, among others intuitive machine programming, advanced maintenance-enabling technologies as well as flexible logistics. This paper gives an overview on recent research and development of our institute (Zentrum f\u00fcr Telematik, ZfT) in this field, the results of which are combined in an Industry 4.0 demonstration factory for the assembly of small satellites systems. We present a total of six tools to be used in such advanced manufacturing systems and their individual advantages. Based on our experience with industrial project partners, customers and visitors of our demonstration factory as well as on the evaluations of the jurors of several awards, we give a qualitative estimate of the effort required to port the individual tools to new production environments. Finally, utilizing our in-house expertise in the New-Space and Industry-4.0 sectors, we give an insight into the benefits achievable using digital manufacturing for small satellite assembly.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "managingrelationshipsonsocialmediainbusinesstobusinessorganisations",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2020.11.028",
        "author": [
            "Cartwright, Severina",
            "Davies, Iain",
            "Archer-Brown, Chris"
        ],
        "keywords": [
            "Social media, B2B marketing, Relationship marketing, Business networks"
        ],
        "abstract": "Social media (SM) constitutes a valuable source of market intelligence, characterised by great ease and efficiency of interactions between networked partners, and by facilitation of individual expressions of self and brand engagement. Thus, SM can enable interaction, collaboration, and networking, thereby strengthening the relationships between actors within networks. Nonetheless, research into B2B organisations \u0301 usage of SM for relationship management remains limited, fragmented and lacking strategic direction. To expand the current state of theory, we draw upon twelve case studies of SM management concerning tactics for acquiring new and potential relationships, building a reputation online, and engaging with business partners. Results show four distinct engagement strategies that organisations tend to employ when implementing SM marketing strategies. The four strategies provide an insight into current approaches to SM marketing within B2B organisations, and how organisations are structuring themselves and managing their resources to respond to this opportunity.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "23529385",
        "isbn": null,
        "journal": "Remote Sensing Applications: Society and Environment",
        "publisher": null,
        "title": "quantifyingcovid19enforcedglobalchangesinatmosphericpollutantsusingcloudcomputingbasedremotesensing",
        "booktitle": null,
        "doi": "10.1016/j.rsase.2021.100489",
        "author": [
            "Singh, Manmeet",
            "Singh, Bhupendra",
            "Singh, Raunaq",
            "Upendra, Badimela",
            "Kaur, Rupinder",
            "Gill, Sukhpal",
            "Biswas, Mriganka"
        ],
        "keywords": [
            "COVID19, Google earth engine, PM, NO, AOD, Tropospheric ozone, Cloud computing"
        ],
        "abstract": "Global lockdowns in response to the COVID-19 pandemic have led to changes in the anthropogenic activities resulting in perceivable air quality improvements. Although several recent studies have analyzed these changes over different regions of the globe, these analyses have been constrained due to the usage of station based data which is mostly limited up to the metropolitan cities. Also the quantifiable changes have been reported only for the developed and developing regions leaving the poor economies (e.g. Africa) due to the shortage of in-situ data. Using a comprehensive set of high spatiotemporal resolution satellites and merged products of air pollutants, we analyze the air quality across the globe and quantify the improvement resulting from the suppressed anthropogenic activity during the lockdowns. In particular, we focus on megacities, capitals and cities with high standards of living to make the quantitative assessment. Our results offer valuable insights into the spatial distribution of changes in the air pollutants due to COVID-19 enforced lockdowns. Statistically significant reductions are observed over megacities with mean reduction by 19.74%, 7.38% and 49.9% in nitrogen dioxide (NO2), aerosol optical depth (AOD) and PM2.5 concentrations. Google Earth Engine empowered cloud computing based remote sensing is used and the results provide a testbed for climate sensitivity experiments and validation of chemistry-climate models. Additionally, Google Earth Engine based apps have been developed to visualize the changes in a real-time fashion.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,703"
    },
    {
        "issnkey": "13596446",
        "isbn": null,
        "journal": "Drug Discovery Today",
        "publisher": null,
        "title": "webresourcesfacilitatedrugdiscoveryintreatmentofcovid19",
        "booktitle": null,
        "doi": "10.1016/j.drudis.2021.04.018",
        "author": [
            "Mei, Long-Can",
            "Jin, Yin",
            "Wang, Zheng",
            "Hao, Ge-Fei",
            "Yang, Guang-Fu"
        ],
        "keywords": [
            "Bioinformatics, SARS-CoV-2, Sequence and structure, Drug design, Vaccines, Monoclonal antibodies"
        ],
        "abstract": "The infectious disease Coronavirus 2019 (COVID-19) continues to cause a global pandemic and, thus, the need for effective therapeutics remains urgent. Global research targeting COVID-19 treatments has produced numerous therapy-related data and established data repositories. However, these data are disseminated throughout the literature and web resources, which could lead to a reduction in the levels of their use. In this review, we introduce resource repositories for the development of COVID-19 therapeutics, from the genome and proteome to antiviral drugs, vaccines, and monoclonal antibodies. We briefly describe the data and usage, and how they advance research for therapies. Finally, we discuss the opportunities and challenges to preventing the pandemic from developing further.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02786125",
        "isbn": null,
        "journal": "Journal of Manufacturing Systems",
        "publisher": null,
        "title": "earlyeventdetectioninadeeplearningdrivenqualitypredictionmodelforultrasonicwelding",
        "booktitle": null,
        "doi": "10.1016/j.jmsy.2021.06.009",
        "author": [
            "Wang, Baicun",
            "Li, Yang",
            "Luo, Ying",
            "Li, Xingyu",
            "Freiheit, Theodor"
        ],
        "keywords": [
            "Ultrasonic welding, Quality prediction, Deep-learning, Long short-term memory, Event detection"
        ],
        "abstract": "A goal in ultrasonic welding (USW) process monitoring is to accurately predict quality outcomes based on monitored signals. However, in most cases, knowing only that the USW process has failed is insufficient. Modern process automation should assess signal information and intercede to rectify process problems. Identification of when a process signal deviates from an acceptable final quality outcome, i.e., the time at which an abnormal event starts, facilitates control action or root cause analysis to bring it back to compliance. A long short-term memory (LSTM) recurrent neural network is proposed to monitor USW and other time-series signals and identify this point. This deep neural network is trained to classify quality outcomes from continuous signals. The process monitoring signals and their sampling time are divided into finite segments as input to this network. The time segment at which the process signal first converges to the final quality class prediction is identified using cross-entropy of the classification probabilities. This procedure is demonstrated using USW quality monitoring algorithms and robot motion failure detection. The examples show an LSTM network not only provides high accuracy for USW quality prediction, but also that the time of classification convergence is consistent with variance observed in USW weld quality factors. Moreover, classification convergence time was shown to be associated to specific robot motion failures, useful as input to adaptive learning. This work realizes deep-learning driven quality prediction and early event detection for quality classification problems, and provides the information necessary for adaptive control algorithms.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.633",
        "scimago_value": "2,310"
    },
    {
        "issnkey": "03064573",
        "isbn": null,
        "journal": "Information Processing & Management",
        "publisher": null,
        "title": "privacypreservinginblockchainbasedgovernmentdatasharingaserviceonchainsocapproach",
        "booktitle": null,
        "doi": "10.1016/j.ipm.2021.102651",
        "author": [
            "Piao, Chunhui",
            "Hao, Yurong",
            "Yan, Jiaqi",
            "Jiang, Xuehong"
        ],
        "keywords": [
            "Consortium blockchain, Government data sharing, Privacy preserving, Smart contract"
        ],
        "abstract": "Sharing government data is of great significance to social development, but insecure and inappropriate sharing may lead to privacy breaches. Data sharing in consortium blockchain has provided a promising direction for efficient privacy preserving government data sharing. However, since government data is not owned by a single person or any government employers, it is hard to attribute participants\u2019 responsibility for motivating data sharing behavior in blockchain systems. Furthermore, the scope and scale of government data to be shared among departments is unclear, as the purposes for retrieving shared data are dynamically changing. In order to solve these problems, we propose a Service-On-Chain (or simply, SOC) approach. The SOC approach can effectively identify different departments\u2019 data retrieving requirements while efficiently sharing government data with trustworthiness in data content and controllability in data ownership. In particular, we utilize smart contracts to provide an onchain service to define data sharing agreements between government departments, which can identify ambiguous data retrieving requirements and formalize the process logic. We apply the SOC approach to a real world scenario and demonstrate that the SOC approach provides a feasible solution for secure and efficient sharing of data among government departments.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22106707",
        "isbn": null,
        "journal": "Sustainable Cities and Society",
        "publisher": null,
        "title": "improvementofpm25ando3forecastingbyintegrationof3dnumericalsimulationwithdeeplearningtechniques",
        "booktitle": null,
        "doi": "10.1016/j.scs.2021.103372",
        "author": [
            "Sun, Haochen",
            "Fung, Jimmy",
            "Chen, Yiang",
            "Chen, Wanying",
            "Li, Zhenning",
            "Huang, Yeqi",
            "Lin, Changqing",
            "Hu, Mingyun",
            "Lu, Xingcheng"
        ],
        "keywords": [
            "Deep learning, Community Multiscale Air Quality model, Spatial correction, PM, O"
        ],
        "abstract": "Air pollution is a major impediment to the sustainable development of cities and society. Governed by emission characteristics and meteorological conditions, the formation and destruction of fine particulate matter (PM2.5) and ozone (O3) are complicated, and accurate predictions of the concentrations of these two major secondary atmospheric pollutants remain challenging. In this study, by combining meteorological and air pollutant data from ground observations and the Weather Research and Forecasting (WRF)-Community Multiscale Air Quality (CMAQ) model simulations, a deep learning model structure based on long short-term memory layers (LSTM) was developed and applied to predict the PM2.5 and O3 concentrations in the future 48 h period. The forecasting improvement was extended to the whole Greater Bay Area by introducing a spatial correction (SC) method to the CMAQ simulation results. Compared with the original CMAQ forecast, the new method gained a 26% reduction in mean absolute error (MAE) and a 33% reduction in root mean square error (RMSE), respectively, in terms of PM2.5; it also achieved a 40% reduction in MAE and a 34% reduction in RMSE in terms of O3. SC method, applied to the whole GBA region, also reduced the overall MAE and RMSE by 10% and 17% in terms of PM2.5 and by 31% and 25% in terms of O3, respectively. Using an AI approach, our study provides new perspectives for further improving air quality forecasting from both temporal and spatial perspectives, thus increasing the smartness and resilience of the cities and promoting environmentally sustainable development in the area.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.587",
        "scimago_value": "1,645"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821259-2",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter18artificialintelligenceinoncology",
        "booktitle": "Artificial Intelligence in Medicine",
        "doi": "10.1016/B978-0-12-821259-2.00018-1",
        "author": [
            "Bibault, Jean-Emmanuel",
            "Burgun, Anita",
            "Fournier, Laure",
            "Dekker, Andr\u00e9",
            "Lambin, Philippe"
        ],
        "keywords": [
            "Oncology, cancer, artificial intelligence, deep learning, machine learning, prediction"
        ],
        "abstract": "Medical decisions can rely on a very large number of parameters, but it is traditionally considered that our cognitive capacity can only integrate up to five factors in order to take a decision. Oncologists will need to combine vast amount of clinical, biological, and imaging data to achieve state-of-the-art treatments. Data science and artificial intelligence (AI) will have an important role in the generation of models to predict outcome and guide treatments. A new paradigm of data-driven decision-making, reusing routine health-care data to provide decision support is emerging. This chapter explores the studies published in imaging, medical and radiation oncology and explains the technical challenges that need to be addressed before AI can be routinely used to treat cancer patients.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0169409x",
        "isbn": null,
        "journal": "Advanced Drug Delivery Reviews",
        "publisher": null,
        "title": "integrationofpersonalizeddrugdeliverysystemsintodigitalhealth",
        "booktitle": null,
        "doi": "10.1016/j.addr.2021.113857",
        "author": [
            "Raijada, Dhara",
            "Wac, Katarzyna",
            "Greisen, Emanuel",
            "Rantanen, Jukka",
            "Genina, Natalja"
        ],
        "keywords": [
            "Personalized medicine, Pharmaceutical supply chain, Digital therapeutics, Smartphone, Internet of Things (IoT), 2D barcodes, Traceability, Anti-counterfeiting, Track and trace, Unique identifiers"
        ],
        "abstract": "Personalized drug delivery systems (PDDS), implying the patient-tailored dose, dosage form, frequency of administration and drug release kinetics, and digital health platforms for diagnosis and treatment monitoring, patient adherence, and traceability of drug products, are emerging scientific areas. Both fields are advancing at a fast pace. However, despite the strong complementary nature of these disciplines, there are only a few successful examples of merging these areas. Therefore, it is important and timely to combine PDDS with an increasing number of high-end digital health solutions to create an interactive feedback loop between the actual needs of each patient and the drug products. This review provides an overview of advanced design solutions for new products such as interactive personalized treatment that would interconnect the pharmaceutical and digital worlds. Furthermore, we discuss the recent advancements in the pharmaceutical supply chain (PSC) management and related limitations of the current mass production model. We summarize the current state of the art and envision future directions and potential development areas.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03608352",
        "isbn": null,
        "journal": "Computers & Industrial Engineering",
        "publisher": null,
        "title": "developingablockchainframeworkfortheautomotivesupplychainasystematicreview",
        "booktitle": null,
        "doi": "10.1016/j.cie.2021.107334",
        "author": [
            "{Raj Kumar Reddy}, Kotha",
            "Gunasekaran, Angappa",
            "Kalpana, P.",
            "{Raja Sreedharan}, V.",
            "{Arvind Kumar}, S"
        ],
        "keywords": [
            "Automotive supply chain, Blockchain, Systematic literature review, VUCA world"
        ],
        "abstract": "As world is affected by demand volatility; process uncertainty; supply chain complexity and information ambiguity forming a VUCA world. To manage this scenario, industries are adopting emerging technologies for business excellence and one among them is Blockchain. Blockchain technology (BCT) is a distributed ledger technology (DLT) that stores transactional records in a tamper-proof and immutable way; it is a promising solution for incorporating transparency and traceability in traditional ecosystem. As automotive industries are facing a Volatile environment, Uncertain schedules & information; Complex supply chain networks, and Ambiguous decisions that cripples the automotive supply chain (ASC). Therefore, BCT can be used to address issues related to ASC in VUCA world. Keeping this in mind, study reported a systematic literature review (SLR) of BCT applications in ASC. More than seventy research papers were reviewed based on different BCT characteristics and applications. Through content analysis, study explored how to link supply chain visibility, information transparency with BCT for an efficient ASC in VUCA world. Moreover, a BCT implementation framework is proposed for ASC, to provide a decision-making approach for practitioners in VUCA world.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,315"
    },
    {
        "issnkey": "00137952",
        "isbn": null,
        "journal": "Engineering Geology",
        "publisher": null,
        "title": "evolutioncharacteristicsanddisplacementforecastingmodeloflandslideswithstairstepslidingsurfacealongthexiangxiriverthreegorgesreservoirregionchina",
        "booktitle": null,
        "doi": "10.1016/j.enggeo.2020.105961",
        "author": [
            "Li, Changdong",
            "Criss, Robert",
            "Fu, Zhiyong",
            "Long, Jingjing",
            "Tan, Qinwen"
        ],
        "keywords": [
            "Reservoir landslide, Evolution process, Multi-step sliding surface, Displacement forecasting model, Lower reaches of Xiangxi River"
        ],
        "abstract": "Five large and many small landslides are developed in Jurassic strata along the lower reaches of Xiangxi River, where interbedded weak and hard bedrock layers foster the development of landslides with a \u201cstair-step\u201d sliding surface. The paper investigates the evolution characteristics of these landslides and presents a novel forecasting model for their displacements. The distribution characteristics and behavior of landslides developed along Xiangxi River is revealed by the database of landslides in the larger Zigui basin, of which this area is part. Most landslides occur at rather low elevations of <300 m and in areas of moderate rainfall. The geological evolution of landslides in the Xiangxi River valley can be divided into four stages, beginning with anticline formation, followed by valley incision, then by weathering and erosion, and culminating in formation of the colluvial landslides. The accumulative displacement curves of landslides with a stair-step sliding surface in Xiangxi River region also present obvious, step-like characteristics. A novel GA-CEEMD-RF algorithm was developed to predict the displacement of these stair-step landslides, which helps to define the combination of induced factors and weak stableness of prediction results using a single displacement prediction model and the multi-field monitoring data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.755",
        "scimago_value": "2,441"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822844-9",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter7securityandprivacyintheinternetofthingscomputationalintelligenttechniquesbasedapproaches",
        "booktitle": "Recent Trends in Computational Intelligence Enabled Research",
        "doi": "10.1016/B978-0-12-822844-9.00009-8",
        "author": [
            "Chanal, Poornima",
            "Kakkasageri, Mahabaleshwar",
            "Manvi, Sunil"
        ],
        "keywords": [
            "Internet of things, security, privacy, authentication, integrity, confidentiality, availability, computational intelligence"
        ],
        "abstract": "The Internet of Things (IoT) is a network of universally interconnected devices via the Internet. The IoT requires the interconnection between billions or trillions of intelligent objects. IoT devices (nodes) are capable of capturing, preserving, analyzing, and sharing data about themselves and their physical world. Security and privacy are the major challenges in the implementation of IoT technology. Major privacy aspects in the IoT are stealing data, monitoring, and tracking, etc. Authentication, integrity, and confidentiality are major concerns for privacy and security preservation in the IoT. Computational intelligence (CI) focuses on the design and development of intelligent algorithms to solve real-time problems with minimum cost. The main goal of CI is to supplement natural and artificial intelligence to produce human-required competitive results. Computational intelligent mechanisms for providing privacy and security in the IoT include quantum cryptography, artificial intelligence, neural networks, natural computational techniques, bio-inspired computational techniques, fuzzy logic techniques, genetic algorithms, intelligent multiagents, etc.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03601315",
        "isbn": null,
        "journal": "Computers & Education",
        "publisher": null,
        "title": "criteriaforselectingappsdebatingtheperceptionsofyoungchildrenparentsandindustrystakeholders",
        "booktitle": null,
        "doi": "10.1016/j.compedu.2021.104134",
        "author": [
            "Dias, Patr\u00edcia",
            "Brito, Rita"
        ],
        "keywords": [
            "Young children, Mobile media, Apps, Parents, Stakeholders"
        ],
        "abstract": "It is indisputable that young children are exposed to digital media since birth and start using them very early. This fuels debate that engages scholars and researchers, industry and brands, policymakers, and parents. Our study aimed to contrast these different perspectives, adding the view of children, who are frequently left out of this debate. Using an exploratory qualitative approach, we conducted interviews with children under 8 years old and their parents in 81 families, and with 17 expert stakeholders in different fields. We focused on their perceptions and practices regarding digital media, and specifically on how they assess and select apps, concluding that parents value safety and learning, children enjoy entertainment, and stakeholders highlight the importance of a good user experience.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,026"
    },
    {
        "issnkey": "18762018",
        "isbn": null,
        "journal": "Asian Journal of Psychiatry",
        "publisher": null,
        "title": "selfhelpcognitivebehavioraltherapyapplicationforcovid19relatedmentalhealthproblemsalongitudinaltrial",
        "booktitle": null,
        "doi": "10.1016/j.ajp.2021.102656",
        "author": [
            "Song, Jiaqi",
            "Jiang, Ronghuan",
            "Chen, Nan",
            "Qu, Wei",
            "Liu, Dan",
            "Zhang, Meng",
            "Fan, Hongzhen",
            "Zhao, Yanli",
            "Tan, Shuping"
        ],
        "keywords": [
            "Cognitive behavioral therapy, Depression, Anxiety, Insomnia, COVID-19"
        ],
        "abstract": "Background and aim Recently, the availability and usefulness of mobile self-help mental health applications have increased, but few applications deal with COVID-19-related psychological problems. This study explored the intervention efficacy of a mobile application on addressing psychological problems related to COVID-19. Methods A longitudinal control trial involving 129 Chinese participants with depression symptoms was conducted through the mobile application \u201cCare for Your Mental Health and Sleep during COVID-19\u201d (CMSC) based on WeChat. Participants were divided into two groups: mobile internet cognitive behavioral therapy (MiCBT) and wait-list. The primary outcome was improvement in depression symptoms. Secondary outcomes included improvement in anxiety and insomnia. The MiCBT group received three self-help CBT intervention sessions in one week via CMSC. Results The MiCBT group showed significant improvement in depression and insomnia (allP < 0.05) compared with the wait-list group. Although both groups showed significant improvement in anxiety at the intervention\u2019s end, compared with the wait-list group, the MiCBT group had no significant advantage. Correlation analysis showed that improvement in depression and anxiety had a significant positive association with education level. Changes in insomnia were significantly negatively correlated with anxiety of COVID-19 at the baseline. CMSC was considered helpful (n=68, 81.9 %) and enjoyable (n=54, 65.9 %) in relieving depression and insomnia during the COVID-19 outbreak. Conclusions CMSC is verified to be effective and convenient for improving COVID-19-related depression and insomnia symptoms. A large study with sufficient evidence is required to determine its continuous effect on reducing mental health problems during the pandemic.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.543",
        "scimago_value": "0,793"
    },
    {
        "issnkey": "01989715",
        "isbn": null,
        "journal": "Computers, Environment and Urban Systems",
        "publisher": null,
        "title": "ageographicdatascienceframeworkforthefunctionalandcontextualanalysisofhumandynamicswithinglobalcities",
        "booktitle": null,
        "doi": "10.1016/j.compenvurbsys.2020.101539",
        "author": [
            "Calafiore, Alessia",
            "Palmer, Gregory",
            "Comber, Sam",
            "Arribas-Bel, Daniel",
            "Singleton, Alex"
        ],
        "keywords": [
            "Foursquare, Geographic data science, Urban analytics"
        ],
        "abstract": "This study develops a Geographic Data Science framework that transforms the Foursquare check-in locations and user origin-destination flows data into knowledge about the emerging forms and characteristics of cities' neighbourhoods. We employ a longitudinal mobility dataset describing human interactions with Foursquare venues in ten global cities: Chicago, Istanbul, Jakarta, London, Los Angeles, New York, Paris, Seoul, Singapore, Tokyo. This social media data provides spatio-temporally referenced digital traces left by human use of urban environments, giving us access to the intangible aspects of urban life, such as people behaviours and preferences. Our framework capitalizes on these new data sources, bringing about a novel Geographic Data Science and human-centered methodological approach. Combining network science \u2013 a study area with great promise for the analysis of cities and their structure \u2013 with geospatial analysis methods, we model cities as a series of global urban networks. Through a spatially weighted community detection algorithm, we uncover functional neighbourhoods for the ten global cities. Each neighbourhood is linked to hyper-local characterisations of their built environment for the Foursquare venues that compose them, and complemented with a range of measures describing their diversity, morphology and mobility. This information is used in a clustering exercise that uncovers a set of four functional neighbourhood types. Our results enable the profiling and comparison of functional neighbourhoods, based on human dynamics and their contexts, across the sample of global cities. The framework is portable to other geographic contexts where interaction data are available to bind different localities into functional agglomerations, and provide insight into their contextual and human dynamics.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.324",
        "scimago_value": "1,549"
    },
    {
        "issnkey": "01604120",
        "isbn": null,
        "journal": "Environment International",
        "publisher": null,
        "title": "towardsacomprehensivecharacterisationofthehumaninternalchemicalexposomechallengesandperspectives",
        "booktitle": null,
        "doi": "10.1016/j.envint.2021.106630",
        "author": [
            "David, Arthur",
            "Chaker, Jade",
            "Price, Elliott",
            "Bessonneau, Vincent",
            "Chetwynd, Andrew",
            "Vitale, Chiara",
            "Kl\u00e1nov\u00e1, Jana",
            "Walker, Douglas",
            "Antignac, Jean-Philippe",
            "Barouki, Robert",
            "Miller, Gary"
        ],
        "keywords": [
            "Exposome, High-Resolution Mass Spectrometry, Internal chemical exposome, Non-targeted analysis, Suspect screening, EWAS"
        ],
        "abstract": "The holistic characterisation of the human internal chemical exposome using high-resolution mass spectrometry (HRMS) would be a step forward to investigate the environmental \u00e6tiology of chronic diseases with an unprecedented precision. HRMS-based methods are currently operational to reproducibly profile thousands of endogenous metabolites as well as externally-derived chemicals and their biotransformation products in a large number of biological samples from human cohorts. These approaches provide a solid ground for the discovery of unrecognised biomarkers of exposure and metabolic effects associated with many chronic diseases. Nevertheless, some limitations remain and have to be overcome so that chemical exposomics can provide unbiased detection of chemical exposures affecting disease susceptibility in epidemiological studies. Some of these limitations include (i) the lack of versatility of analytical techniques to capture the wide diversity of chemicals; (ii) the lack of analytical sensitivity that prevents the detection of exogenous (and endogenous) chemicals occurring at (ultra) trace levels from restricted sample amounts, and (iii) the lack of automation of the annotation/identification process. In this article, we discuss a number of technological and methodological limitations hindering applications of HRMS-based methods and propose initial steps to push towards a more comprehensive characterisation of the internal chemical exposome. We also discuss other challenges including the need for harmonisation and the difficulty inherent in assessing the dynamic nature of the internal chemical exposome, as well as the need for establishing a strong international collaboration, high level networking, and sustainable research infrastructure. A great amount of research, technological development and innovative bio-informatics tools are still needed to profile and characterise the \u201cinvisible\u201d (not profiled), \u201chidden\u201d (not detected) and \u201cdark\u201d (not annotated) components of the internal chemical exposome and concerted efforts across numerous research fields are paramount.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00298018",
        "isbn": null,
        "journal": "Ocean Engineering",
        "publisher": null,
        "title": "datadrivenmodellingofshippropulsionandtheeffectofdatapreprocessingonthepredictionofshipfuelconsumptionandspeedloss",
        "booktitle": null,
        "doi": "10.1016/j.oceaneng.2021.108616",
        "author": [
            "Karagiannidis, Pavlos",
            "Themelis, Nikos"
        ],
        "keywords": [
            "Performance monitoring, Artificial neural network, Data processing, Propulsion modeling, Predictive analytics"
        ],
        "abstract": "Data-driven models for ship propulsion are presented while the effect of data pre-processing techniques is extensively examined. In this study, a large, automatically collected with high sampling frequency data set is exploited for training models that estimate the required shaft power or main engine fuel consumption of a container ship sailing under arbitrary conditions. Emphasis is given to the statistical evaluation and pre-processing of the data and two algorithms are presented for this scope. Additionally, state-of-the-art techniques for training and optimizing Feed-Forward Neural Networks (FNNs) are applied. The results indicate that with a delicate filtering and preparation stage it is possible to significantly increase the model's accuracy. Therefore, increase the prediction ability and awareness regarding the ship's hull and propeller actual condition. Furthermore, such models could be employed in studies targeting at the improvement of ship's operational energy efficiency.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.795",
        "scimago_value": "1,321"
    },
    {
        "issnkey": "",
        "isbn": "978-0-08-102672-4",
        "journal": null,
        "publisher": "Elsevier",
        "title": "index",
        "booktitle": "International Encyclopedia of Transportation",
        "doi": "10.1016/B978-0-08-102671-7.10828-0",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "1470160x",
        "isbn": null,
        "journal": "Ecological Indicators",
        "publisher": null,
        "title": "characteristicsofvegetationresponsetodroughtintheconusbasedonlongtermremotesensingandmeteorologicaldata",
        "booktitle": null,
        "doi": "10.1016/j.ecolind.2021.107767",
        "author": [
            "Zhong, Shaobo",
            "Sun, Ziheng",
            "Di, Liping"
        ],
        "keywords": [
            "Drought, Standardized precipitation evapotranspiration index (SPEI), Vegetation condition index (VCI), Vegetation response, CONUS"
        ],
        "abstract": "Drought is one of the billion-dollar natural disasters and hard to trace and measure. In recent years drought monitoring becomes much easier with remote sensing. However, it is still difficult to pin vegetation variances on drought because of the delay of the caused vegetation stress. To assess vegetative drought, it is important to first understand the relationship between meteorological condition and vegetation condition, and measure the vegetation responses to meteorological drought. It would be very helpful for effective early warning about agricultural drought. This study uses the CONUS as the study area, and utilizes remote sensing products such as NDVI/VCI (normalized difference vegetation index/vegetation condition index) and SPI/SPEI (standardized precipitation index/standardized precipitation evapotranspiration index) to give a comparative evaluation to the vegetation\u2019s drought response. The used vegetation products and meteorological data are ensured to be consistent. The scale and lag of vegetation response to drought for various vegetation types and aridity levels were thoroughly investigated. The results show that: The AVHRR and MODIS NDVI series pairs and the meteorological drought index series pairs (SPEI and SPI) have fairly good consistencies. Among them, 69.5% and 84% have rho (correlation coefficient) values greater than 0.8 respectively. For the NDVI series pairs, the maximum rho value is 0.98, the minimum rho value is \u22120.47, and the mean rho value is 0.79, which are 0.97, 0.68, and 0.87 respectively for the meteorological index series pairs. Compared to rho values of the meteorological index series pairs, the rho values of the NDVI series pairs have more outliers indicating instability. The correlation between SPEI and VCI significantly relies on time lags and has high spatial inhomogeneity. 1- and 2-month lags of SPEI have more significant positive correlation with VCI in Arid, Semi-Arid and Dry sub-humid areas of central west CONUS with less precipitation and lower temperature. For various time scales (time scale is SPEI reference range), the most significant positive correlation between SPEI and VCI happens in the time scales of 6- to 12-month in summer, the time scales of 3- and 6-month in spring and autumn, and the time scales of 2- and 3-month in winter regardless of time lags. Despite of the different vegetation types and aridity levels, the maximum correlations between SPEI and VCI are observed in Hyper arid regions in January, Arid regions in April, Semi-arid regions in July, and Dry sub-humid regions in October. Shrub has prominent responses in January and April, grass responses appear in July and October, and Evergreen forest shows least responses in all seasons. The results add more insights of the connection between vegetation and climate, and guide the development of new technology leveraging remote sensing data for vegetation drought monitoring and early-warning. The results are also helpful to provide important references for studying large-scale physiological and phenological properties of the vegetation under different climate conditions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00489697",
        "isbn": null,
        "journal": "Science of The Total Environment",
        "publisher": null,
        "title": "robustnessanalysisofstormwaterqualitymodellingwithlidinfrastructuresfromnaturaleventbasedfieldmonitoring",
        "booktitle": null,
        "doi": "10.1016/j.scitotenv.2020.142007",
        "author": [
            "Tang, Sijie",
            "Jiang, Jiping",
            "Zheng, Yi",
            "Hong, Yi",
            "Chung, Eun-Sung",
            "Shamseldin, Asaad",
            "Wei, Yan",
            "Wang, Xiuheng"
        ],
        "keywords": [
            "Robustness analysis, Stormwater quality, SWMM, Sponge city, Nash-Sutcliffe efficiency coefficient (NSE)"
        ],
        "abstract": "Sponge city construction (SCC) in China, as a new concept and a practical application of low-impact development (LID), is gaining wide popularity. Modelling tools are widely used to evaluate the ecological benefits of SCC in stormwater pollution mitigation. However, the understanding of the robustness of water quality modelling with different LID design options is still limited due to the paucity of water quality data as well as the high cost of water quality data collection and model calibration. This study develops a new concept of \u2018robustness\u2019 measured by model calibration performances. It combines an automatic calibration technique with intensive field monitoring data to perform the robustness analysis of storm water quality modelling using the SWMM (Storm Water Management Model). One of the national pilot areas of SCC, Fenghuang Cheng, in Shenzhen, China, is selected as the study area. Five water quality variables (COD, NH3-N, TN, TP, and SS) and 13 types of LID/non-LID infrastructures are simulated using 37 rainfall events. The results show that the model performance is satisfactory for different water quality variables and LID types. Water quality modelling of greenbelts and rain gardens has the best performance, while the models of barrels and green roofs are not as robust as those of the other LID types. In urban runoff, three water quality parameters, namely, SS, TN and COD, are better captured by the SWMM models than NH3-N and TP. The modelling performance tends to be better under heavy rain and significant pollutant concentrations, denoting a potentially more stable and reliable design of infrastructures. This study helps to improve the current understanding of the feasibility and robustness of using the SWMM model in sponge city design.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.963",
        "scimago_value": "1,795"
    },
    {
        "issnkey": "00344257",
        "isbn": null,
        "journal": "Remote Sensing of Environment",
        "publisher": null,
        "title": "eddymorphologyegglikeshapeoverallspinningandoceanographicimplications",
        "booktitle": null,
        "doi": "10.1016/j.rse.2021.112348",
        "author": [
            "Chen, Ge",
            "Yang, Jie",
            "Han, Guiyan"
        ],
        "keywords": [
            "Oceanic eddy, Morphology, Altimeter"
        ],
        "abstract": "Systematic tracking of individual eddies during their entire lifetimes marks a significant milestone in satellite oceanography over the first two decades of this century. Assuming that the geometry and properties of an oceanic eddy are orientation-sensitive, an angularly aligned composite analysis of over 40 million eddy-focused surface topographic \u201csnapshots\u201d obtained by merging tandem altimeter data from January 1993 through January 2019 reveals that oceanic vortices appear to have a characteristic surface shape of \u201cegg\u201d rather than circle or ellipse as previously understood. Consequently, a second-order moment in eddy morphology is revealed which leads to a ~ 10 km departure from a standard ellipse in terms of major axis. Furthermore, the sharp poles of oceanic eddies exhibit two quasi-orthogonal modes of orientation: primarily meridional and secondarily zonal. The additional submesoscale asymmetry in eddy shape is confirmed by a consistent anisotropy in a normalized eddy-centric velocity field derived from over 25 thousand drifters. The high-order moments in terms of geometric asymmetry and dynamic anisotropy associated with mesoscale eddies (which are found to be statistically significant at 99% level) may have profound geophysical and biological impacts on energy transport, substance entrainment, as well as ecosystem dynamics in the ocean. In particular, it is demonstrated that some of the previously identified patterns in eddy properties obtained by non-rotated normalization may be notably biased due to the ignorance of existing eddy orientation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.164",
        "scimago_value": "3,611"
    },
    {
        "issnkey": "21855560",
        "isbn": null,
        "journal": "Asian Transport Studies",
        "publisher": null,
        "title": "travelpatternanalyticsdrivenbycellularsignalingdata",
        "booktitle": null,
        "doi": "10.1016/j.eastsj.2021.100042",
        "author": [
            "Chiou, Yu-Chiun",
            "Hsieh, Chih-Wei"
        ],
        "keywords": [
            "Cellular signaling data, Trip identification, Trip chaining pattern"
        ],
        "abstract": "Abstract Cellular signaling data (CSD) could be one of the primary data sources for transportation planning and demand forecasting as a result of the rapid growth in triangulation and location techniques. Utilizing CSD to analyze travel patterns requires a meticulous process to overcome data oscillation and trajectory discontinuities. This study analyzes CSD and develops analytical models to enhance the applicability of CSD in transportation planning. For this study, we invite 30 volunteers to participate in a 30-day travel diary survey to collect data. In addition, based on CSD, we develop analytical algorithms to generate travel trajectories and analyze travel patterns, including the home and work location, trip chaining, and trip purpose of the user. Comparing the model results against the diary travel survey data indicates 84% accuracy for trip estimation and 89% accuracy for trip purpose identification, suggesting that the applicability of the proposed algorithm is satisfactory.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09265805",
        "isbn": null,
        "journal": "Automation in Construction",
        "publisher": null,
        "title": "applicationsofdistributedledgertechnologydltandblockchainenabledsmartcontractsinconstruction",
        "booktitle": null,
        "doi": "10.1016/j.autcon.2021.103955",
        "author": [
            "Li, Jennifer",
            "Kassem, Mohamad"
        ],
        "keywords": [
            "Blockchain, Building Information Modelling (BIM), Construction sector, Distributed ledger technology (DLT), Smart contracts, Systematic review, Thematic analysis"
        ],
        "abstract": "The contribution of distributed ledger technology (DLT) (e.g. blockchain) and smart contracts to the digitalisation and digital transformation of the construction sector is nascent but rapidly gaining traction. \u2018Systematic reviews\u2019 of DLT and smart contract applications that are specific to the construction sector are missing. This paper performs an extensive systematic review of 153 DLT and smart contract papers specific to the design, construction and operation of built assets. The protocols and processes of a systematic review were adopted to ensure full transparency, accountability, reproducibility, and updateability of the results. Through thematic analysis, we identify eight distinct themes of applications for DLT and smart contracts in construction: information management, payments, procurement, supply chain management, regulations and compliance, construction management and delivery, dispute resolution, and technological systems. Each theme identified was analysed to understand current capabilities, applications, and future developments. A cross-themes discussion revealed that DLT and smart contracts are \u2018supplementary\u2019 technologies that are used in combination with other technologies (e.g. BIM, IoT, cloud computing) as part of \u2018technological systems\u2019 that need to co-evolve in order to enable the themes' applications identified. Research into DLT and smart contracts in construction is rapidly moving from theoretical insights and frameworks into developing proofs-of-concept studies (27 studies) and testing them in case studies (20 studies). The next stage of research involving wider academic communities and industry-wide engagement is expected to begin uncovering the anticipated benefits of DLT and smart contracts through investments into technological systems and testing in real-world pilot studies. The discussion of the themes identified from technology, policy, process, and society perspectives exposed the need for an extended socio-technical approach to the solution in order to deliver the necessary change and impact from the adoption of DLT and smart contracts at speed and scale. The results of this systematic review provide a noteworthy reference point for academics, practitioners and policy makers interested in the future development of DLT and smart contract applications in construction.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.700",
        "scimago_value": "1,837"
    },
    {
        "issnkey": "00344257",
        "isbn": null,
        "journal": "Remote Sensing of Environment",
        "publisher": null,
        "title": "productionofglobaldailyseamlessdatacubesandquantificationofgloballandcoverchangefrom1985to2020imapworld10",
        "booktitle": null,
        "doi": "10.1016/j.rse.2021.112364",
        "author": [
            "Liu, Han",
            "Gong, Peng",
            "Wang, Jie",
            "Wang, Xi",
            "Ning, Grant",
            "Xu, Bing"
        ],
        "keywords": [
            "Global land cover mapping, Seamless data cube, Daily, Analysis ready data, Cloud computing, Intelligent mapping, Amazon web Services"
        ],
        "abstract": "Longer time high-resolution, high-frequency, consistent, and more detailed land cover data are urgently needed in order to achieve sustainable development goals on food security, high-quality habitat construction, biodiversity conservation and planetary health, and for the understanding, simulation and management of the Earth system. However, due to technological constraints, it is difficult to provide simultaneously high spatial resolution, high temporal frequency, and high quality observation data. Existing mapping solutions are limited by traditional remotely sensed data, that have shorter observation periods, poor spatio-temporal consistency and comparability. Therefore, a new mapping paradigm is needed. This paper develops a framework for intelligent mapping (iMap) of land cover based on state-of-the-art technologies such as cloud computing, artificial intelligence, virtual constellations, and spatio-temporal reconstruction and fusion. Under this framework, we built an automated, serverless, end-to-end data production chain and parallel mapping system based on Amazon Web Services (AWS) and produced the first 30 m global daily seamless data cubes (SDC), and annual to seasonal land cover maps for 1985\u20132020. The SDC was produced through a multi-source spatio-temporal data reconstruction and fusion workflow based on Landsat, MODIS, and AVHRR virtual constellations. Independent validation results show that the relative mean error of the SDC is less than 2.14%. As analysis ready data (ARD), it can lay a foundation for high-precision quantitative remote sensing information extraction. From this SDC, we produced 36-year long, 30 m resolution global land cover map data set by combining strategies of sample migration, machine learning, and spatio-temporal adjustment. The average overall accuracy of our annual land cover maps over multiple periods of time is 80% for level 1 classification and over 73% for level 2 classification (29 and 33 classes). Based on an objective validation sample consisting of FLUXNET sites, our map accuracy is 10% higher than that of existing global land cover datasets including Globeland30. Our results show that the average global land cover change rate is 0.36%/yr. Global forest decreased by 1.47 million km2 from 38.44 million km2, cropland increased by 0.84 million km2 from 12.49 million km2 and impervious surface increased by 0.48 million km2 from 0.57 million km2 during 1985\u2013 2020.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.164",
        "scimago_value": "3,611"
    },
    {
        "issnkey": "00253227",
        "isbn": null,
        "journal": "Marine Geology",
        "publisher": null,
        "title": "dnnbasedseabedclassificationusingdifferentlyweightedmbesmultifeatures",
        "booktitle": null,
        "doi": "10.1016/j.margeo.2021.106519",
        "author": [
            "Zhu, Zhengren",
            "Cui, Xiaodong",
            "Zhang, Kai",
            "Ai, Bo",
            "Shi, Bo",
            "Yang, Fanlin"
        ],
        "keywords": [
            "MBES, Backscatter mosaic, Angular response, Seabed sediment classification, Deep neural networks, Weight coefficient"
        ],
        "abstract": "Seabed sediment classification has significance for the utilization of marine resources and marine scientific research. Currently, the multibeam echo sounder (MBES) is increasingly becoming the tool of choice for large-scale seabed sediment classification. To further explore the technology of seabed sediment classification, this paper proposes a new classification method. In addition to backscatter mosaic, the method also integrates three other different types of features, including texture features of backscatter mosaic, MBES bathymetry features, and backscatter angular response (AR) features, which are given different weights in the classification process. First, geographically weighted regression (GWR) analysis is performed between different types of features and seabed sediment types, and the normalized coefficient of determination (R2) is employed as the weight coefficient for the different types of features. Second, the backscatter mosaic is combined with features from different types to predict the seabed sediment types using a deep neural network (DNN) classifier. Third, the classification residuals of the features from these three different types are acquired through the above classification results. Last, the classification residuals of features from different types are added to the classification results of the backscatter mosaic according to the weights, thereby achieving seabed sediment classification based on MBES multifeatures with different weights. The results show that the overall classification accuracy of the seabed sediments can be significantly improved from 88.98%/85.14% to 93.43% when using the DNN classification model based on MBES multifeatures with different weights compared with the other two models (DNN classification model based on MBES multifeatures with equal weights and DNN classification model based on principal component analysis (PCA) dimensionality reduction). The kappa coefficient can also be significantly improved from approximately 0.85/0.80 to 0.91. Via analysis, the proposed method can reasonably assign the weights of the different features and take advantage of integrating MBES multifeatures for seabed sediment classification. This approach also provides an important reference for future research on seabed sediment classification.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.548",
        "scimago_value": "1,236"
    },
    {
        "issnkey": "14629011",
        "isbn": null,
        "journal": "Environmental Science & Policy",
        "publisher": null,
        "title": "abayesianlstmmodeltoevaluatetheeffectsofairpollutioncontrolregulationsinbeijingchina",
        "booktitle": null,
        "doi": "10.1016/j.envsci.2020.10.004",
        "author": [
            "Han, Yang",
            "Lam, Jacqueline",
            "Li, Victor",
            "Reiner, David"
        ],
        "keywords": [
            "Air pollution control regulations, Effects of regulatory interventions, Bayesian LSTM, Propensity score, Counterfactual analysis, Causal inference"
        ],
        "abstract": "Rapid socio-economic development and urbanization have resulted in serious deterioration in air-quality in many world cities, including Beijing, China. This study attempts to examine the effectiveness of air pollution control regulations implemented in Beijing during 2008\u20132019 through a data-driven regulatory intervention analysis. Our proposed Bayesian deep learning model utilizes proxy data including Aerosol Optical Depth (AOD) and meteorology as well as socio-economic data, while accounting for confounding effects via propensity score estimation. Our results show that air pollution control regulatory measures implemented in China and Beijing during 2008\u20132019 reduced PM2.5 pollution in Beijing by 11 % on average. After the introduction of Action Plan for Clean Air in China and Beijing in late 2013, as compared to the hypothetical PM2.5 concentration (without any regulatory interventions), the estimated PM2.5 reduction increased dramatically from 15 % in 2015 to 44 % in 2018. Our results suggest that Beijing\u2019s air quality has improved gradually over the past decade, though the annual PM2.5 pollution still exceeds the WHO threshold. In this regard, the air pollution control regulations introduced in Beijing and China tend to become more effective after 2015, suggesting a 2-year time lag before the stringent air pollution control regulations starting from 2013 takes any strong positive effects. Moreover, as compared to the air pollution control regulations introduced before 2013, newly introduced policy-making governance, which couples the policy-makings of the local jurisdictions with that of the central government, and the new policy measures that tackle the vested interests of the local stakeholders in Beijing and its nearby cities, alongside with the stringent local and national air pollution control regulations and plans, should help reduce air pollution and promote healthy living in Beijing over the longer term.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "predictivemethodsincyberdefensecurrentexperienceandresearchchallenges",
        "booktitle": null,
        "doi": "10.1016/j.future.2020.10.006",
        "author": [
            "Hus\u00e1k, Martin",
            "Barto\u0161, V\u00e1clav",
            "Sokol, Pavol",
            "Gajdo\u0161, Andrej"
        ],
        "keywords": [
            "Cybersecurity, Prediction, Forecasting, Data mining, Machine learning, Time series"
        ],
        "abstract": "Predictive analysis allows next-generation cyber defense that is more proactive than current approaches based on intrusion detection. In this paper, we discuss various aspects of predictive methods in cyber defense and illustrate them on three examples of recent approaches. The first approach uses data mining to extract frequent attack scenarios and uses them to project ongoing cyberattacks. The second approach uses a dynamic network entity reputation score to predict malicious actors. The third approach uses time series analysis to forecast attack rates in the network. This paper presents a unique evaluation of the three distinct methods in a common environment of an intrusion detection alert sharing platform, which allows for a comparison of the approaches and illustrates the capabilities of predictive analysis for current and future research and cybersecurity operations. Our experiments show that all three methods achieved a sufficient technology readiness level for experimental deployment in an operational setting with promising accuracy and usability. Namely prediction and projection methods, despite their differences, are highly usable for predictive blacklisting, the first provides a more detailed output, and the second is more extensible. Network security situation forecasting is lightweight and displays very high accuracy, but does not provide details on predicted events.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13619209",
        "isbn": null,
        "journal": "Transportation Research Part D: Transport and Environment",
        "publisher": null,
        "title": "impactofdataprocessingonderivingmicromobilitypatternsfromvehicleavailabilitydata",
        "booktitle": null,
        "doi": "10.1016/j.trd.2021.102913",
        "author": [
            "Zhao, Pengxiang",
            "Haitao, He",
            "Li, Aoyong",
            "Mansourian, Ali"
        ],
        "keywords": [
            "Micro-mobility, E-scooter sharing, Data processing, Data sampling, Spatio-temporal patterns, Vehicle availability data, GPS, Trip identification"
        ],
        "abstract": "Vehicle availability data is emerging as a potential data source for micro-mobility research and applications. However, there is not yet research that systematically evaluates or validates the processing of this emerging mobility data. To fill this gap, we propose a generally applicable data processing framework and validate its related algorithms. The framework exploits micro-mobility vehicle availability data to identify individual trips and derive aggregate patterns by evaluating a range of temporal, spatial, and statistical mobility descriptors. The impact of data processing is systematically and rigorously investigated by applying the proposed framework with a case study dataset from Zurich, Switzerland. Our results demonstrate that the sampling rate used when collecting vehicle availability data has a significant and intricate impact on the derived micro-mobility patterns. This research calls for more attention to investigate various issues with emerging mobility data processing to ensure its validity for transportation research and practices.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.495",
        "scimago_value": "1,600"
    },
    {
        "issnkey": "10534822",
        "isbn": null,
        "journal": "Human Resource Management Review",
        "publisher": null,
        "title": "disruptedhr",
        "booktitle": null,
        "doi": "10.1016/j.hrmr.2020.100820",
        "author": [
            "Minbaeva, Dana"
        ],
        "keywords": [
            "Strategic HR, Disruption, Flexible workforce, Digitalization, Analytics"
        ],
        "abstract": "In this paper, I discuss possible avenues for future research aimed at bridging the research-practice gap on the topic of disruptions in human resources (HR). I focus on three global mega-trends\u2014the flexible workforce, the digitalization of business models, and artificial intelligence and machine learning\u2014and examine their influence on the field of human resource management (HRM) in general and in the context of the COVID-19 pandemic. I discuss why HRM research has overlooked potential paradigm-shifting possibilities that could ultimately equip HR practitioners with the knowledge needed to respond to disruptions caused by these mega-trends.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.444",
        "scimago_value": "2,549"
    },
    {
        "issnkey": "02650568",
        "isbn": null,
        "journal": "Natural Product Reports",
        "publisher": null,
        "title": "microbialnaturalproductdatabasesmovingforwardinthemultiomicsera",
        "booktitle": null,
        "doi": "10.1039/d0np00053a",
        "author": [
            "{van Santen}, Jeffrey",
            "Kautsar, Satria",
            "Medema, Marnix",
            "Linington, Roger"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Covering: 2010\u20132020 The digital revolution is driving significant changes in how people store, distribute, and use information. With the advent of new technologies around linked data, machine learning and large-scale network inference, the natural products research field is beginning to embrace real-time sharing and large-scale analysis of digitized experimental data. Databases play a key role in this, as they allow systematic annotation and storage of data for both basic and advanced applications. The quality of the content, structure, and accessibility of these databases all contribute to their usefulness for the scientific community in practice. This review covers the development of databases relevant for microbial natural product discovery during the past decade (2010\u20132020), including repositories of chemical structures/properties, metabolomics, and genomic data (biosynthetic gene clusters). It provides an overview of the most important databases and their functionalities, highlights some early meta-analyses using such databases, and discusses basic principles to enable widespread interoperability between databases. Furthermore, it points out conceptual and practical challenges in the curation and usage of natural products databases. Finally, the review closes with a discussion of key action points required for the field moving forward, not only for database developers but for any scientist active in the field.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "13.423",
        "scimago_value": "2,703"
    },
    {
        "issnkey": "09255273",
        "isbn": null,
        "journal": "International Journal of Production Economics",
        "publisher": null,
        "title": "evaluatingblockchainrequirementsforeffectivedigitalsupplychainmanagement",
        "booktitle": null,
        "doi": "10.1016/j.ijpe.2021.108309",
        "author": [
            "B\u00fcy\u00fck\u00f6zkan, G\u00fcl\u00e7in",
            "T\u00fcfek\u00e7i, Gizem",
            "Uzt\u00fcrk, Deniz"
        ],
        "keywords": [
            "Blockchain, Digitalization, Supply chain, House of quality, Intuitionistic fuzzy sets, Incomplete preferences"
        ],
        "abstract": "Collapsing traditional supply chain (SC) nodes into digitally interconnected networks creates several opportunities for efficiency, cost savings, and SC traceability. Blockchain is one of the pioneering transformative 4.0 technologies and is seen as the backbone of digital SCs (DSCs). Several attempts have been made to create explorative designs or models, however, forming a strategic Blockchain vision remains a challenge. This paper specifies those customer needs and design requirements that should be prioritized with a systematic approach for effective Blockchain integration into SCs. The proposed functional methodology provides a practical roadmap for practitioners. It is based on the House of Quality (HoQ) method, with its customer-focused design aspect, utilizing a group decision making (GDM) approach with an incomplete intuitionistic fuzzy relation (IIFR) extension. The GDM approach is employed to overcome the biases of decision making, while IIFRs deal with different or complementary focus centers in qualitative data. The usefulness of this methodology is tested with an application, and the results are validated by experts in the field. The results indicate that a Blockchain-based DSC (BC-DSC) is expected to deliver continuing financial benefit, efficient use of time, and the ability to support. Automation, effective coordination, and conformity are identified as highly critical design requirements. GDM and IIFR are combined for the first time in the literature. The originality of this paper comes from its generation of critical factors geared to obtaining an effective BC-DSC structure and the implementation of HoQ as an efficient tool for design.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.885",
        "scimago_value": "2,406"
    },
    {
        "issnkey": "00344257",
        "isbn": null,
        "journal": "Remote Sensing of Environment",
        "publisher": null,
        "title": "investigatingesasentinel2productssystematiccloudcoveroverestimationinveryhighaltitudeareas",
        "booktitle": null,
        "doi": "10.1016/j.rse.2020.112163",
        "author": [
            "Tiede, Dirk",
            "Sudmanns, Martin",
            "Augustin, Hannah",
            "Baraldi, Andrea"
        ],
        "keywords": [
            "Sentinel-2 cloud mask, Cloud cover overestimation, Big EO data image selection, Geographical bias"
        ],
        "abstract": "Cloud detection in optical remote sensing imagery is crucial because undetected clouds can produce misleading results in analyses. Almost all optical remote sensing data access portals rely to some degree on a cloud cover filter. Here we show that cirrus as well as opaque cloud cover in Sentinel-2 Level-1C (L1C) and Level-2A (L2A) imagery is systematically and significantly overestimated in very high altitude areas (e.g. Himalayas, Andes). We argue that this systematic bias is created by applying simple thresholds to single bands instead of using a multi-band spectral signature in the cloud detection process. This results in a lot of \u201chidden\u201d data for very high altitude areas when each image's estimated cloud cover is used as an automated selection criterion for analysis (e.g. global analyses, cloud-free mosaic production). We show geographic locations exemplifying this overestimation, and compare the L1C and L2A cloud masks produced by ESA to cloud masks generated by an expert system that uses comprehensive spectral signatures, showing that reliable cloud estimations are possible in very high altitudes. Based on this comparison, we argue for changes to L1C and L2A cloud detection algorithms in order to improve initial querying and selection of big EO data, where reliable yet automated quality indicators are necessary to handle an overwhelming data volume and velocity. Our contribution raises awareness of potential bias when pre-selecting images based on reported cloud cover in very high altitude areas for researchers and users of Sentinel-2 imagery in the environmental domain.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.164",
        "scimago_value": "3,611"
    },
    {
        "issnkey": "09593780",
        "isbn": null,
        "journal": "Global Environmental Change",
        "publisher": null,
        "title": "improvingtheevidencebaseamethodologicalreviewofthequantitativeclimatemigrationliterature",
        "booktitle": null,
        "doi": "10.1016/j.gloenvcha.2021.102367",
        "author": [
            "Hoffmann, Roman",
            "\u0160edov\u00e1, Barbora",
            "Vinke, Kira"
        ],
        "keywords": [
            "Climate migration, Climate change, Systematic review, Meta-analysis, Methodology, Methods"
        ],
        "abstract": "The question whether and how climatic factors influence human migration has gained both academic and public interest in the past years. Based on two meta-analyses, this paper systematically reviews the quantitative empirical literature on climate-related migration from a methodological perspective. In total, information from 127 original micro- and macro-level studies is analyzed to assess how different concepts, research designs, and analytical methods shape our understanding of climate migration. We provide an overview of common methodological approaches and present evidence on their potential implications for the estimation of climatic impacts. We identify five key challenges, which relate to the i) measurement of migration and ii) climatic events, iii) the integration and aggregation of data, iv) the identification of causal relationships, and v) the exploration of contextual influences and mechanisms. Advances in research and modelling are discussed together with best practice cases to provide guidance to researchers studying the climate-migration nexus. We recommend for future empirical studies to employ approaches that are of relevance for and reflect local contexts, ensuring high levels of comparability and transparency.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "4,659"
    },
    {
        "issnkey": "12962074",
        "isbn": null,
        "journal": "Journal of Cultural Heritage",
        "publisher": null,
        "title": "multispectralaerialimagerybased3ddigitisationsegmentationandannotationoflargescaleurbanareasofsignificantculturalvalue",
        "booktitle": null,
        "doi": "10.1016/j.culher.2021.04.004",
        "author": [
            "Koutsoudis, Anestis",
            "Ioannakis, George",
            "Pistofidis, Petros",
            "Arnaoutoglou, Fotis",
            "Kazakis, Nikolaos",
            "Pavlidis, George",
            "Chamzas, Chistodoulos",
            "Tsirliganis, Nestor"
        ],
        "keywords": [
            "multispectral, machine learning, 3D digitisation, 3D segmentation, annotation, structure from motion, urban, architecture, disaster management"
        ],
        "abstract": "Disaster risk management of movable and immovable cultural heritage is a highly significant research topic. In this work, we present a pipeline for 3D digitisation, segmentation and annotation of large scale urban areas in order to produce data that can be exploited in disaster management simulators (e.g fire spreading, crowd movement, firefighting training, evacuation planning, etc.). We have selected the old town of Xanthi (Greece) as a challenging case study. We developed a custom multispectral camera to be carried by a commercial drone. Using the structure from motion / multiview stereo (SFM/MVS) approach, we produced a 3D model of the urban area covering 0.5km2 that is followed by a multilayer texture map which carries information from visible and near-infrared regions of the electromagnetic spectrum. We developed a set of machine learning approaches based on logistic regression, support vector machines and artificial neural networks that allow 3D model segmentation by exploiting not only morphological and structural features but also the multispectral behaviour of different material surfaces. We objectively evaluate the performance of the proposed segmentation approaches on six significant material-based classes (cobbled-roads granite kilns, building walls, ceramic roof-tiles, low-vegetation, high-vegetation and metal surfaces) that are used in simulating fire propagation and crowd movement. The experiments revealed that the segmentation accuracy can be enhanced by taking into consideration surface material multispectral properties as well as morphological features. A Web-based multi-user annotation tool complements our proposed pipeline by enabling further 3D model segmentation, fine tuning and semantics annotation (e.g. usage-based building classification and evacuation priorities, escape paths and gathering points).",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.955",
        "scimago_value": "0,663"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "semisupervisedanomalydetectioninbusinessprocesseventdatausingselfattentionbasedclassification",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.08.005",
        "author": [
            "Krajsic, Philippe",
            "Franczyk, Bogdan"
        ],
        "keywords": [
            "anomaly detection, business process, classification, event data, process mining"
        ],
        "abstract": "The analysis of business processes has become increasingly important in recent years, not least due to the emergence of analysis tools that enable data-centric views of processes and thus provide increasingly operational support for process flows. In this work, a semi-supervised classification model is presented that takes into account different developments in deep learning (e.g., deep generative models), time series analysis (e.g., long short-term memory) and sequence processing (e.g., attention mechanism) and combines them in one approach. The results of the experimental implementation of the classification model show that it is able to filter activity-related and time-related anomalies from the event data and outperform existing approaches in its classification accuracy (F1 score). The classification model achieves an F1 score of up to 93%.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "03062619",
        "isbn": null,
        "journal": "Applied Energy",
        "publisher": null,
        "title": "spatiotemporalwindfieldpredictionbasedonphysicsinformeddeeplearningandlidarmeasurements",
        "booktitle": null,
        "doi": "10.1016/j.apenergy.2021.116641",
        "author": [
            "Zhang, Jincheng",
            "Zhao, Xiaowei"
        ],
        "keywords": [
            "Deep learning, LIDAR measurements, Physics-informed neural networks, Wind field prediction"
        ],
        "abstract": "Spatiotemporal wind field information is of great interest in wind industry e.g. for wind resource assessment and wind turbine/farm monitoring & control. However, its measurement is not feasible because only sparse point measurements are available with the current sensor technology such as LIDAR. This work fills the gap by developing a method that can achieve spatiotemporal wind field predictions by combining LIDAR measurements and flow physics. Specifically, a deep neural network is constructed and the Navier\u2013Stokes equations, which provide a good description of atmospheric flows, are incorporated in the deep neural network by employing the physics-informed deep learning technique. The training of this physics-incorporated deep learning model only requires the sparse LIDAR measurement data while the spatiotemporal wind field in the whole domain (which cannot be measured) can be predicted after training. This study, which can discover complex wind patterns that do not present in the training dataset, is totally distinct from previous machine learning based wind prediction studies which treat machine learning models as \u201cblack-box\u201d and require the corresponding input and target values to learn complex relations. The numerical results on the prediction of the wind field in front of a wind turbine show that the proposed method predicts the spatiotemporal flow velocity (including both downwind and crosswind components) in the whole domain very well for a wide range of scenarios (including various measurement noises, resolutions, LIDAR look directions, and turbulence levels), which is promising given that only line-of-sight wind speed measurements at sparse locations are used.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.746",
        "scimago_value": "3,035"
    },
    {
        "issnkey": "01681699",
        "isbn": null,
        "journal": "Computers and Electronics in Agriculture",
        "publisher": null,
        "title": "iotagroasmartfarmingsystemtocolombiancoffeefarms",
        "booktitle": null,
        "doi": "10.1016/j.compag.2021.106442",
        "author": [
            "Rodr\u00edguez, Jhonn",
            "Montoya-Munoz, Ana",
            "Rodriguez-Pabon, Carlos",
            "Hoyos, Javier",
            "Corrales, Juan"
        ],
        "keywords": [
            "Smart Farming, Internet of Things, Data Analytics, Outlier Detection, BPMN, Coffee Farm"
        ],
        "abstract": "Currently, the adoption of smart technologies for sustainable farming systems creates a distinct competitive edge for farmers, extension services, agri-business, and policy-makers. However, selecting the most appropriate technologies from a wide range of options is never an easy job. In this context, several authors consider Smart Farming as the best solution. However, they fall short in providing more information to recommend the most appropriate IoT technology, the options to manage the IoT infrastructure, and the services to crop management plans and crop production estimation. This paper implements a Smart Farming System based on a three-layered architecture (Agriculture Perception, Edge Computing, and Data Analytics). In the Agriculture Perception Layer, we evaluated Omicron, Libelium, and Intel technologies under criteria such as the price, the number of inputs for sensor connection, communication protocols, portability, battery life, and harvesting energy system photovoltaic panel. We evaluated edge-based management mechanisms in the Edge Layer to provide data reliability, focusing on outlier detection and treatment using Machine Learning and Interpolation algorithms. We recommend the Isolation Forest algorithm for classifying outliers in the monthly temperature dataset (99% of precision) and the Cubic Spline technique for effectively replacing the data classified as outliers (RMSE lower than 0.085). In the Data Analytics Layer, we evaluated different machine learning algorithms to estimate coffee production. The results show that the measured error values of the XGBOOST algorithm keep the values lower than the other models (RMSE 0.008, MAE 0.032, and RSE 0.585). The www.iot-agro.com platform offers farmer services such as weather variables monitoring, coffee production estimating, and IoT infrastructure setting. Finally, stakeholders, researchers, and engineers validated our Smart Farming Solution through a Colombian coffee farm case study. The test evaluated the usability, the straightforward interpretation of data, and the look feel of the web application.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.565",
        "scimago_value": "1,208"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "stopcriterioninbuildingdecisiontreeswithbaggingmethodfordisperseddata",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.09.129",
        "author": [
            "Przyby\u0142a-Kasperek, Ma\u0142gorzata",
            "Aning, Samuel"
        ],
        "keywords": [
            "Ensemble of classifiers, Dispersed data, Stop criterion, Bagging method, Classification trees, Independent data sources"
        ],
        "abstract": "This article discusses issues related to decision making based on applying decision trees and bagging methods on dispersed knowledge. In dispersed knowledge, local decision tables possess data independently in fragments. In this study, sub-tables are further generated with bagging method for each local table, based on which the decision trees are built. These decision trees classify the test object, and a probability vector is defined over the decision classes for each local table. For each vector, decision classes with the maximum value of the coordinates are selected and final joint decisions for all local tables are made by majority voting. Quality of decision making has been observed to increase when bagging method as an ensemble method is combined with decision trees on independent dispersed data. An important criterion in building a decision tree is to know when to stop growing the tree (stop splitting). That is, at what minimum number of objects on a working node do we stop building the tree to ensure the best decision results. The contribution of the paper is to observe the influence a stop criterion (expressed in the number of objects in the node) for decision trees used in conjunction with bagging method on independent data sources. It can be concluded that in dispersed data set, the stop split criteria does not influence the classification quality much. The statistical significance of the difference in the mean classification error values was confirmed only for a very high stop criterion (0.1\u00d7 number of objects in training set) and for a very low stop criterion (equal to two). There is no significant statistical difference in the classification quality obtained for the stop criterion values: 4, 6, 8 and 10. An interesting remark is that for some dispersed data sets, in the case of smaller number of local tables and larger number of bootstrap samples, better quality of classification is obtained for a small number of objects in the stop criterion (mostly for two objects). Only, at a significant increase in the minimum number of objects at which growth of trees is stopped is quality of classification affected. However, the gain in reducing the complexity for trees that we get when using the larger values of stop criterion is significant.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "15249042",
        "isbn": null,
        "journal": "Pain Management Nursing",
        "publisher": null,
        "title": "painmanagementinclinicalpracticeresearchusingelectronichealthrecords",
        "booktitle": null,
        "doi": "10.1016/j.pmn.2021.01.016",
        "author": [
            "Nomura, Aline",
            "Pruinelli, Lisiane",
            "Barreto, Luciana",
            "Santos, Murilo",
            "Swanson, Elizabeth",
            "Silveira, Thamiris",
            "Abreu, Miriam"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Background: The use of electronic health record (EHR) systems encourages and facilitates the use of data for the development and surveillance of quality indicators, including pain management. Aim: to conduct an integrative review on pain management research using data extracted from EHR in order to synthesize and analyze the following elements: pain management (assessments, interventions, and outcomes) and study results with potential clinical implications, data source, clinical sample characteristics, and method description. Design: An integrative review of the literature was undertaken to identify exemplars of scientific research studies that explore pain management using data from EHR, using Cooper\u2019s framework. Results: Our search of 1,061 records from PubMed, Scopus, and Cinahl was narrowed down to 28 eligible articles to be analyzed. Conclusion: Results of this integrative review will make a critical contribution, assisting others in developing research proposals and sound research methods, as well as providing an overview of such studies over the past 10 years. Through this review it is therefore possible to guide new research on clinical pain management using EHR.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.929",
        "scimago_value": "0,557"
    },
    {
        "issnkey": "02638762",
        "isbn": null,
        "journal": "Chemical Engineering Research and Design",
        "publisher": null,
        "title": "machinelearningbasedpredictivecontrolusingnoisydataevaluatingperformanceandrobustnessviaalargescaleprocesssimulator",
        "booktitle": null,
        "doi": "10.1016/j.cherd.2021.02.011",
        "author": [
            "Wu, Zhe",
            "Luo, Junwei",
            "Rincon, David",
            "Christofides, Panagiotis"
        ],
        "keywords": [
            "Machine learning, Long short-term memory neural networks, Noisy data, Model predictive control, Nonlinear systems, Chemical processes"
        ],
        "abstract": "Machine learning modeling of chemical processes using noisy data is a practically challenging task due to the occurrence of overfitting during learning. In this work, we propose a dropout method and a co-teaching learning algorithm that develop long short-term memory (LSTM) neural networks to capture the ground truth (i.e., underlying process dynamics) from noisy data. To evaluate the performance and robustness of the proposed modeling approaches, we consider an industrial chemical reactor example and use a large-scale process simulator, Aspen Plus Dynamics that does not employ assumptions on reactor properties typically made in the derivation of first-principles models, to generate process operational data that are corrupted by sensor noise which is determined using industrial data. The dropout method is first utilized to reduce the overfitting of LSTM models to noisy data. Then, another approach termed co-teaching method is used to train LSTM models with additional noise-free data generated from simulations of the reactor first-principles model that employs several standard modeling assumptions not made in the Aspen model. Through open-loop and closed-loop simulations, we demonstrate the improvement of model prediction accuracy and of the open- and closed-loop performances under model predictive controllers using dropout and co-teaching LSTM neural network models compared to the LSTM model developed from the standard training process from the noisy data.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23529148",
        "isbn": null,
        "journal": "Informatics in Medicine Unlocked",
        "publisher": null,
        "title": "missingvalueimputationaffectstheperformanceofmachinelearningareviewandanalysisoftheliterature20102021",
        "booktitle": null,
        "doi": "10.1016/j.imu.2021.100799",
        "author": [
            "Hasan, Md.",
            "Alam, Md.",
            "Roy, Shidhartho",
            "Dutta, Aishwariya",
            "Jawad, Md.",
            "Das, Sunanda"
        ],
        "keywords": [
            "Incomplete datasets, Imputation methods and evaluations, Machine learning classifiers and evaluations, PRISMA technique"
        ],
        "abstract": "Recently, numerous studies have been conducted on Missing Value Imputation (MVI), intending the primary solution scheme for the datasets containing one or more missing attribute\u2019s values. The incorporation of MVI reinforces the Machine Learning (ML) models\u2019 performance and necessitates a systematic review of MVI methodologies employed for different tasks and datasets. It will aid beginners as guidance towards composing an effective ML-based decision-making system in various fields of applications. This article aims to conduct a rigorous review and analysis of the state-of-the-art MVI methods in the literature published in the last decade. Altogether, 191 articles, published from 2010 to August 2021, are selected for review using the well-known Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) technique. We summarize those articles with relevant definitions, theories, and analyses to provide essential information for building a precise decision-making framework. In addition, the evaluation metrics employed for MVI methods and ML-based classification models are also discussed and explored. Remarkably, the trends for the MVI method and its evaluation are also scrutinized from the last twelve years\u2019 data. To come up with the conclusion, several ML-based pipelines, where the MVI schemes are incorporated for performance enhancement, are investigated and reviewed for many different datasets. In the end, informative observations and recommendations are addressed for future research directions and trends in related fields of interest.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,440"
    },
    {
        "issnkey": "03787206",
        "isbn": null,
        "journal": "Information & Management",
        "publisher": null,
        "title": "effectofanagencysresourcesontheimplementationofopengovernmentdata",
        "booktitle": null,
        "doi": "10.1016/j.im.2021.103465",
        "author": [
            "Zhao, Yupan",
            "Fan, Bo"
        ],
        "keywords": [
            "OGD, Resource-based theory, Resource allocation, Influencing factors"
        ],
        "abstract": "Open government data (OGD) exhibits substantial political, economic, cultural, and social values that have gained considerable attention globally. Based on the investigation and analysis of the OGD practice, this study raises the research question, \u201cWhy do considerable differences exist in the degree of OGD implementation among different agencies under the same local government?\u201d Our study takes resource-based theory as theoretical foundation to explore the factors that affect OGD implementation of constituent agencies within the same local government. A questionnaire survey is conducted to analyze the effect of factors including technical capacity, organizational awareness, organizational arrangement, and rules and regulations on OGD implementation. Results show that the technical capacity, organizational arrangement, and rules and regulations of government agencies have a direct positive effect on OGD implementation. Notably, rules and regulations moderate the relationship between technical capacity and OGD implementation. Besides, the matching degree of technical capacity and other organizational factors in a government agency exerts a positive influence on OGD implementation. Finally, our study proposes policy suggestions that emphasize the direction and focus for OGD implementation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,147"
    },
    {
        "issnkey": "09507051",
        "isbn": null,
        "journal": "Knowledge-Based Systems",
        "publisher": null,
        "title": "metadatabasedmeasurementstransmissionverifiedbyamerkletree",
        "booktitle": null,
        "doi": "10.1016/j.knosys.2021.106871",
        "author": [
            "Div\u00e1n, Mario",
            "S\u00e1nchez-Reynoso, Mar\u00eda"
        ],
        "keywords": [
            "Measurement, Metadata-guided transmission, Brief data message, Integrity record, Data streaming"
        ],
        "abstract": "The Data Stream Processing Strategy (DSPS) is focused on the automatization of measurement projects based on a measurement framework. The measurement adapter (MA) is an architecture component located on mobile devices aims to integrate heterogeneous data sources (i.e., sensors). The Gathering Function (GF) is the component responsible for interacting and receiving measures from the MAs, and it resides on the Stream Processing Engine (SPE). MA and GF share the project definition based on a measurement framework to foster data interoperability, while MA regulates the frequency, size, and route related to data transmission. As contributions (i) The brief data message is introduced to optimize the data transmission keeping immutable the hierarchical data organization based on the project definition, and (ii) The integrity record for mobile and SPE environments is described based on a Merkle Tree. This allows optimizing each data transaction, incorporating a historical integrity record both MA and SPE. The proposals and simulations have been implemented on the cincamimis, cincamipd, mair, and pabmmcommons libraries, which are freely available on GitHub under the terms of the Apache 2.0 licence. Four simulations are explained to detail how to measures were obtained. Interesting results show that the brief data message consumes 17.50 KB to transmit 1000 measures (2.4 times smaller than JSON), while a message with 200 measures could be generated and compressed using GZIP in 25.12 ms (2.43 times faster than JSON). 196 KB is required to keep 17 min of the integrity history in a MA, being created in 4.85 ms.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.038",
        "scimago_value": "1,587"
    },
    {
        "issnkey": "10538119",
        "isbn": null,
        "journal": "NeuroImage",
        "publisher": null,
        "title": "whitemattermicrostructureacrosstheadultlifespanamixedlongitudinalandcrosssectionalstudyusingadvanceddiffusionmodelsandbrainageprediction",
        "booktitle": null,
        "doi": "10.1016/j.neuroimage.2020.117441",
        "author": [
            "Beck, Dani",
            "{de Lange}, Ann-Marie",
            "Maximov, Ivan",
            "Richard, Genevi\u00e8ve",
            "Andreassen, Ole",
            "Nordvik, Jan",
            "Westlye, Lars"
        ],
        "keywords": [
            "Ageing, White matter, Multi-shell, Longitudinal,, Diffusion, Brain age"
        ],
        "abstract": "The macro- and microstructural architecture of human brain white matter undergoes substantial alterations throughout development and ageing. Most of our understanding of the spatial and temporal characteristics of these lifespan adaptations come from magnetic resonance imaging (MRI), including diffusion MRI (dMRI), which enables visualisation and quantification of brain white matter with unprecedented sensitivity and detail. However, with some notable exceptions, previous studies have relied on cross-sectional designs, limited age ranges, and diffusion tensor imaging (DTI) based on conventional single-shell dMRI. In this mixed cross-sectional and longitudinal study (mean interval: 15.2 months) including 702 multi-shell dMRI datasets, we combined complementary dMRI models to investigate age trajectories in healthy individuals aged 18 to 94 years (57.12% women). Using linear mixed effect models and machine learning based brain age prediction, we assessed the age-dependence of diffusion metrics, and compared the age prediction accuracy of six different diffusion models, including diffusion tensor (DTI) and kurtosis imaging (DKI), neurite orientation dispersion and density imaging (NODDI), restriction spectrum imaging (RSI), spherical mean technique multi-compartment (SMT-mc), and white matter tract integrity (WMTI). The results showed that the age slopes for conventional DTI metrics (fractional anisotropy [FA], mean diffusivity [MD], axial diffusivity [AD], radial diffusivity [RD]) were largely consistent with previous research, and that the highest performing advanced dMRI models showed comparable age prediction accuracy to conventional DTI. Linear mixed effects models and Wilk's theorem analysis showed that the \u2018FA fine\u2019 metric of the RSI model and \u2018orientation dispersion\u2019 (OD) metric of the NODDI model showed the highest sensitivity to age. The results indicate that advanced diffusion models (DKI, NODDI, RSI, SMT mc, WMTI) provide sensitive measures of age-related microstructural changes of white matter in the brain that complement and extend the contribution of conventional DTI.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "2352152x",
        "isbn": null,
        "journal": "Journal of Energy Storage",
        "publisher": null,
        "title": "datadrivenstateofhealthmodellingareviewofstateoftheartandreflectionsonapplicationsformaritimebatterysystems",
        "booktitle": null,
        "doi": "10.1016/j.est.2021.103158",
        "author": [
            "Vanem, Erik",
            "Salucci, Clara",
            "Bakdi, Azzeddine",
            "Alnes, \u00d8ystein"
        ],
        "keywords": [
            "Battery state of health, Degradation modelling, Capacity, Maritime battery systems, Data-driven modelling"
        ],
        "abstract": "Battery systems are becoming an increasingly attractive alternative for powering ocean going ships, and the number of fully electric or hybrid ships relying on battery power for propulsion and manoeuvring is growing. In order to ensure the safety of such electric ships, it is of paramount importance to monitor the available energy that can be stored in the batteries, and classification societies typically require that the state of health of the batteries can be verified by independent tests \u2014 annual capacity tests. However, this paper discusses data-driven state of health modelling for maritime battery systems based on operational sensor data collected from the batteries as an alternative approach. Thus, this paper presents a comprehensive review of different data-driven approaches to state of health modelling, and aims at giving an overview of current state of the art. More than 300 papers have been reviewed, most of which are referred to in this paper. Moreover, some reflections and discussions on what types of approaches can be suitable for modelling and independent verification of state of health for maritime battery systems are presented.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "23525509",
        "isbn": null,
        "journal": "Sustainable Production and Consumption",
        "publisher": null,
        "title": "quantifyingthedynamicsbetweenenvironmentalinformationdisclosureandfirmsfinancialperformanceusingfunctionaldataanalysis",
        "booktitle": null,
        "doi": "10.1016/j.spc.2021.03.026",
        "author": [
            "Wang, Deqing",
            "Li, Xuemei",
            "Tian, Sihua",
            "He, Lingyun",
            "Xu, Yan",
            "Wang, Xu"
        ],
        "keywords": [
            "Environmental information disclosure, Firm financial performance, Data smoothing, Functional regression analysis, Moderating effect"
        ],
        "abstract": "Environmental information disclosure (EID) is an important way for firms to communicate to the government and the public to fulfill their environmental protection responsibilities. Essentially, the dynamic impacts of firms\u2019 activities on the ecological environment are evolving continuously. We aim to introduce functional data analysis (FDA) for exploring the dynamics in the relationship between environmental information disclosure (EID) and firms\u2019 financial performance. Based on continuous curves smoothed from 75 Chinese listed firms of pollution-intensive industries, this study examined the dynamic effect and its structural break of EID on firms' financial performance. Furthermore, moderating effects of public attention, government subsidy and ratio of profits to total cost were tested within a functional framework. The results revealed that the positive effect of EID on firms' financial performance is constantly significant, whereas there existed a structural break in 2015 due to the implementation of new Environmental Protection Law. Moreover, the positive moderating effect of the ratio of profits to total cost is significant only before 2015, while both the main effect and moderating effect of government subsidy are not significant. Surprisingly, although the main effects of public attention are not significant, its positive moderating effect is statistically significant. We contributed in introducing FDA as a useful toolkit for quantifying the time-dynamics in ecological economics, and our findings could offer guidance for stakeholders seeking to improve EID.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.032",
        "scimago_value": "1,019"
    },
    {
        "issnkey": "13665545",
        "isbn": null,
        "journal": "Transportation Research Part E: Logistics and Transportation Review",
        "publisher": null,
        "title": "incentivealignmentforblockchainadoptioninmedicinesupplychains",
        "booktitle": null,
        "doi": "10.1016/j.tre.2021.102276",
        "author": [
            "Niu, Baozhuang",
            "Dong, Jian",
            "Liu, Yaoqi"
        ],
        "keywords": [
            "Blockchain technology, Social goods, Supply chain competition, Information sharing"
        ],
        "abstract": "In recent years, blockchain technology has been increasingly adopted in OTC medicine supply chains, enabling customers to track the entire process from raw material purchasing to finished medicine distribution. This improves the brand image and hence expands the market. With the use of blockchain, information transparency can be achieved because data are stored immutably and safely in a distributed database that is accessible by all supply chain members. However, will the incentives for supply chain members to participate in blockchain for larger-scale demand come at the cost of information disclosure? In this paper, to investigate the supply chain members\u2019 incentive alignment opportunities towards the adoption of blockchain technology, we consider a two-stage supply chain comprising two medicine manufacturers and a common retailer that has more accurate demand information than the manufacturers have. We find that, interestingly, the retailer has incentives to participate in blockchain when the manufacturers\u2019 competition is mild and the demand variance is low. We further investigate the impact of blockchain on total surplus and customer surplus and find that the adoption of blockchain always benefits customers and society; therefore, blockchain can be particularly useful for social goods such as OTC medicine.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.875",
        "scimago_value": "2,042"
    },
    {
        "issnkey": "00456535",
        "isbn": null,
        "journal": "Chemosphere",
        "publisher": null,
        "title": "satellitebasedgroundpm25estimationusingagradientboostingdecisiontree",
        "booktitle": null,
        "doi": "10.1016/j.chemosphere.2020.128801",
        "author": [
            "Zhang, Tianning",
            "He, Weihuan",
            "Zheng, Hui",
            "Cui, Yaoping",
            "Song, Hongquan",
            "Fu, Shenglei"
        ],
        "keywords": [
            "Aerosol optical depth, Air pollution, Machine learning, MODIS, Particulate matter"
        ],
        "abstract": "Fine particulate matter with an aerodynamic diameter less than 2.5 \u03bcm (PM2.5) is one of the major air pollutants risks to human health worldwide. Satellite-based aerosol optical depth (AOD) products are an effective metric for acquiring PM2.5 information, featuring broad coverage and high resolution, which compensate for the sparse and uneven distribution of existing monitoring stations. In this study, a gradient boosting decision tree (GBDT) model for estimating ground PM2.5 concentration directly from AOD products across China in 2017, integrating human activities and various natural variables was proposed. The GBDT model performed well in estimating temporal variability and spatial contrasts in daily PM2.5 concentrations, with relatively high fitted model (10-fold cross-validation) coefficients of determination of 0.98 (0.81), low root mean square errors of 3.82 (11.57) \u03bcg/m3, and mean absolute error of 1.44 (7.45) \u03bcg/m3. Seasonal examinations revealed that summer had the cleanest air with the highest estimation accuracies, whereas winter had the most polluted air with the lowest estimation accuracies. The model successfully captured the PM2.5 distribution pattern across China in 2017, showing high levels in southwest Xinjiang, the North China Plain, and the Sichuan Basin, especially in winter. Compared with other models, the GBDT model showed the highest performance in the estimation of PM2.5 with a 3-km resolution. This algorithm can be adopted to improve the accuracy of PM2.5 estimation with higher spatial resolution, especially in summer. In general, this study provided a potential method of improving the accuracy of satellite-based ground PM2.5 estimation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.086",
        "scimago_value": "1,632"
    },
    {
        "issnkey": "01663615",
        "isbn": null,
        "journal": "Computers in Industry",
        "publisher": null,
        "title": "meetlmamethodforembeddingsevaluationfortaxonomicdatainthelabourmarket",
        "booktitle": null,
        "doi": "10.1016/j.compind.2020.103341",
        "author": [
            "Malandri, Lorenzo",
            "Mercorio, Fabio",
            "Mezzanzanica, Mario",
            "Nobani, Navid"
        ],
        "keywords": [
            "Embeddings evaluation, Taxonomies, Semantic hierarchies, Labour market, ICT"
        ],
        "abstract": "Taxonomies are the mainstay of the semantic web as they aim at organising knowledge in concepts linked by IS-A relationships. However, keeping such hierarchies updated and able to represent the domain from which they have been drawn is still a time-consuming, costly and error prone activity. Here, word embeddings have proven to be effective in catching lexicon and semantic similarities to enrich taxonomies from text data. This, in turn, would require to evaluate the generated embeddings to estimate the extent to which they encode the semantic similarity derived from the hierarchy itself. In this paper, we propose and implement MEET-LM, a methodology that aims at generating and evaluating embeddings from a text corpus preserving the co-hyponymy relations synthesised from a domain-specific taxonomy. We apply MEET-LM to a real-life dataset of 2M+ vacancies related to ICT-jobs, framed within the research activities of an EU project that collects millions of Online Job Vacancies and classifies them within the European standard hierarchy ESCO. To show MEET-LM is useful in practice, we also trained a neural network to classify co-hyponym relations using the selected embeddings as features. Our experiments reach 99.4% of accuracy and 86.5% of f1-score.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.635",
        "scimago_value": "1,432"
    },
    {
        "issnkey": "03605442",
        "isbn": null,
        "journal": "Energy",
        "publisher": null,
        "title": "applicationofdatadrivenmethodsforenergysystemmodellingdemonstratedonanadaptivecoolingsupplysystem",
        "booktitle": null,
        "doi": "10.1016/j.energy.2021.120894",
        "author": [
            "Schreiber, Thomas",
            "Netsch, Christoph",
            "Eschweiler, S\u00f6ren",
            "Wang, Tianyuan",
            "Storek, Thomas",
            "Baranski, Marc",
            "M\u00fcller, Dirk"
        ],
        "keywords": [
            "Machine-learning, Supervised learning, Building automation and control, Data-driven modelling, Optimal control"
        ],
        "abstract": "The efficient and sustainable operation of building energy systems is playing an increasingly important role in most industrialized countries. At the same time, building energy systems are becoming increasingly complex; fault-free and optimal operation, under dynamic boundary conditions, is becoming more and more challenging. There are many approaches in research to address the optimal control problem of building energy systems, such as Rule-based Control, Model Predictive Control, or Adaptive Control. However, most methods rely on models of the system dynamics with high prediction accuracies. This is especially the case in Model Predictive Control, where the model is part of a continuously executed optimization problem; but models are also required when it comes to the optimal design of Rule-based Controllers, the safe pre-training of Adaptive Controllers, or model-based fault detection. A limiting factor for the manual development of physical models, for building energy systems, are the low monetary incentives for engineering services, due to the low energy prices in most countries. In addition, the creation of such models is time-consuming and error-prone, even for domain experts. Another weakness is that changes in the system dynamics are not automatically adapted within the models. These challenges are contrasted by an increasing availability of monitoring-data and computational power in recent years; with machine-learning algorithms, these resources are used in numerous application areas to achieve very promising results. Machine-learning methods can help to obtain data-driven, self-calibrating models, which can be learned from monitoring-data. In this paper, we apply methods for automated data-driven model generation. We demonstrate how machine-learning algorithms together with structured hyper-parameter tuning can be used to model individual subsystems as well as a complete energy supply system. To represent the dynamics of the supply system, it is first decomposed into simple functional relationships, which are aggregated into the overall system after training of the comparatively simple subsystem models. We evaluate the accuracy of the data-driven subsystem models using established metrics for the evaluation of regression models, namely the R2-score and the RMSE. The considered system is integrated into a district cooling network and consists of two compression chillers and an ice storage unit. Our investigations show that the dynamics of the subsystems can be learned with high accuracies, depending on the operation mode and the selected features. The prediction of the power demand of the compression chillers is learned with R2-scores between 0.94 and 0.99 and RMSE values between 2.02 kW and 3.51 kW. Also, the prediction of the percentage of ice formation within the ice storage is learned accurately with a R2-score of 1 and RMSE values between 0.08 % and 0.72 %. The dynamics of the aggregated system also show plausible behavior and can thus be used in future work. This work is part of an ongoing research project with the aim to optimize the operation of the entire campus cooling energy supply system. Our results show that, if detailed monitoring-data are available, data-driven modelling represents a viable alternative to the labor-intensive physical modelling approach. Furthermore, we emphasize the importance of structured hyper-parameter tuning, discuss the specifics of different machine-learning algorithms, and elaborate on possible future developments in this research area.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "24681113",
        "isbn": null,
        "journal": "Computational Toxicology",
        "publisher": null,
        "title": "fairificationofnanosafetydatatoimproveapplicabilityofqsarapproachesacasestudyoninvitrocometassaygenotoxicitydata",
        "booktitle": null,
        "doi": "10.1016/j.comtox.2021.100190",
        "author": [
            "Bossa, Cecilia",
            "Andreoli, Cristina",
            "Bakker, Martine",
            "Barone, Flavia",
            "{De Angelis}, Isabella",
            "Jeliazkova, Nina",
            "Nymark, Penny",
            "Battistelli, Chiara"
        ],
        "keywords": [
            "Nanomaterials, FAIR principles, (Q)SAR approaches, Nanosafety data, Genotoxicity, Comet assay"
        ],
        "abstract": "(Quantitative) structure-activity relationship ([Q]SAR) methodologies are widely applied to predict the (eco)toxicological effects of chemicals, and their use is envisaged in different regulatory frameworks for filling data gaps of untested substances. However, their application to the risk assessment of nanomaterials is still limited, also due to the scarcity of large and curated experimental datasets. Despite a great amount of nanosafety data having been produced over the last decade in international collaborative initiatives, their interpretation, integration and reuse has been hampered by several obstacles, such as poorly described (meta)data, non-standard terminology, lack of harmonized reporting formats and criteria. Recently, the FAIR (Findable, Accessible, Interoperable, and Reusable) principles have been established to guide the scientific community in good data management and stewardship. The EU H2020 Gov4Nano project, together with other international projects and initiatives, is addressing the challenge of improving nanosafety data FAIRness, for maximizing their availability, understanding, exchange and ultimately their reuse. These efforts are largely supported by the creation of a common Nanosafety Data Interface, which connects a row of project-specific databases applying the eNanoMapper data model. A wide variety of experimental data relating to characterization and effects of nanomaterials are stored in the database; however, the methods, protocols and parameters driving their generation are not fully mature. This article reports the progress of an ongoing case study in the Gov4nano project on the reuse of in vitro Comet genotoxicity data, focusing on the issues and challenges encountered in their FAIRification through the eNanoMapper data model. The case study is part of an iterative process in which the FAIRification of data supports the understanding of the phenomena underlying their generation and, ultimately, improves their reusability.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,754"
    },
    {
        "issnkey": "02085216",
        "isbn": null,
        "journal": "Biocybernetics and Biomedical Engineering",
        "publisher": null,
        "title": "ocularartifacteliminationfromelectroencephalographysignalsasystematicreview",
        "booktitle": null,
        "doi": "10.1016/j.bbe.2021.06.007",
        "author": [
            "Ranjan, Rakesh",
            "{Chandra Sahana}, Bikash",
            "{Kumar Bhandari}, Ashish"
        ],
        "keywords": [
            "EEG signal, Ocular artifact, Artifact removal techniques, Hybrid method, Performance evaluation, Brain-computer interface"
        ],
        "abstract": "Electroencephalography (EEG) is the signal of intrigue that has immense application in the clinical diagnosis of various neurological, psychiatric, psychological, psychophysiological, and neurocognitive disorders. It is significantly crucial in neural communication, brain-computer interface, and other practical tasks. EEG signal is exceptionally susceptible to artifacts, which are external noise signals originated from non-cerebral regions. The interference of artifacts in EEG signals can potentially affect the original recorded EEG signal quality and pattern. Therefore, artifact removal from EEG signal is critically important before applying it to a specific task for accurate outcomes. Researchers have proposed numerous techniques to remove various artifacts present in the contaminated EEG signal. However, neither optimum method nor criterion stands standard for endorsement of clinically recorded EEG signals. Therefore, the research related to artifact elimination from EEG signal is challenging and perplexing task. This paper attempts to give an extensive outline of the advancement in methodologies to eliminate one of the most common artifacts, i.e., ocular artifact. It is anticipated that the study will enlighten the researchers on all the existing ocular artifact elimination techniques with a validated simulation model on the recorded EEG signal. In future advancements, Standard norms in artifact elimination techniques are expected to diminish the neurologist's load by substantiating the clinical diagnosis after gaining correct information from artifact-free EEG signals.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.314",
        "scimago_value": "0,665"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-819412-6",
        "journal": null,
        "publisher": "Elsevier",
        "title": "index",
        "booktitle": "Earth Observation for Flood Applications",
        "doi": "10.1016/B978-0-12-819412-6.00021-3",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01650327",
        "isbn": null,
        "journal": "Journal of Affective Disorders",
        "publisher": null,
        "title": "applesandpearsaresimilarbutstilldifferentthingsbipolardisorderandschizophreniadiscretedisordersorjustdimensions",
        "booktitle": null,
        "doi": "10.1016/j.jad.2021.04.064",
        "author": [
            "Grunze, Heinz",
            "Cetkovich-Bakmas, Marcelo"
        ],
        "keywords": [
            "Bipolar disorder, Schizophrenia, Dichotomy, Phenomenology, Neurobiology, Computational psychiatry"
        ],
        "abstract": "Starting with the dichotomous view of Kraepelin, schizophrenia and bipolar disorder have traditionally been considered as separate entities. More recent, this taxonomic view of illnesses has been challenged and a continuum psychosis has been postulated based on genetic and neurobiological findings suggestive of a large overlap between disorders. In this paper we will review clinical and experimental data from genetics, morphology, phenomenology and illness progression demonstrating what makes schizophrenia and bipolar disorder different conditions, challenging the idea of the obsolescence of the categorical approach. However, perhaps it is also time to move beyond DSM and search for more refined clinical descriptions that could uncover clinical invariants matching better with molecular data. In the future, computational psychiatry employing artificial intelligence and machine learning might provide us a tool to overcome the gap between clinical descriptions (phenomenology) and neurobiology.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.839",
        "scimago_value": "1,892"
    },
    {
        "issnkey": "09257535",
        "isbn": null,
        "journal": "Safety Science",
        "publisher": null,
        "title": "anoverviewofscientometricmappingforthesafetysciencecommunitymethodstoolsandframework",
        "booktitle": null,
        "doi": "10.1016/j.ssci.2020.105093",
        "author": [
            "Li, Jie",
            "Goerlandt, Floris",
            "Reniers, Genserik"
        ],
        "keywords": [
            "Scientometrics, Bibliometrics, Safety science, Mapping knowledge domains, Science mapping"
        ],
        "abstract": "Scientometrics analysis is increasingly applied across scientific domains to gain quantitative insights in the development of research on particular (sub-)domains of scientific inquiry. By visualizing metrics containing quantitative information about such a domain, scientometric mapping allows researchers to gain insights in aspects thereof. Methods have been developed to answer specific research questions, focusing e.g. on collaboration networks, thematic research clusters, historic evolution patterns, and trends in topics addressed. Several articles applying scientometric mapping to safety-related topics have been published. In context of the Special Issue \u2018Mapping Safety Science \u2013 Reviewing Safety Research\u2019, this article first reviews these, and subsequently provides an overview of key concepts, methods, and tools for scientometric mapping. Data sources and freely available tools are introduced, focusing on which research questions these are suited to answer. A brief tutorial-style description of a scientometrics research process is provided, guiding researchers new to this method how to engage with it. Finally, a discussion on best practices in scientometric mapping research is made, focusing on how to obtain reliable and valid results, and how to use the scientometric maps to gain meaningful insights. It is hoped that this work can advance the application of scientometric research within the safety science community.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.877",
        "scimago_value": "1,178"
    },
    {
        "issnkey": "03062619",
        "isbn": null,
        "journal": "Applied Energy",
        "publisher": null,
        "title": "shorttermsolarpowerforecastinginvestigatingtheabilityofdeeplearningmodelstocapturelowlevelutilityscalephotovoltaicsystembehaviour",
        "booktitle": null,
        "doi": "10.1016/j.apenergy.2020.116395",
        "author": [
            "{du Plessis}, A.A.",
            "Strauss, J.M.",
            "Rix, A.J."
        ],
        "keywords": [
            "Short-term power forecasting, Machine learning, Deep learning, Photovoltaic, Long Short-Term Memory, Gated Recurrent Unit"
        ],
        "abstract": "Photovoltaic (PV) system power supply is characteristically intermittent. Therefore, PV forecasting is crucial for decision makers responsible for electrical grid stability. With forecast models traditionally trained as macro-level solutions, where a single model emulates the entire PV system, there is uncertainty regarding the ability of these macro-level models to capture the low-level power output dynamics of large multi-megawatt PV systems. Instead, an aggregated inverter-level forecasting methodology is proposed to obtain an enhanced forecasting accuracy. These macro-level and inverter-level forecasting methodologies are implemented with state-of-the-art deep learning based Feedforward neural network, Long Short-Term Memory and Gated Recurrent Unit recurrent neural network models. Results are generated for a real-world scenario, with multi-step forecasts delivered 1\u20136 h ahead for a 75 MW rated PV system. To ensure the scalability of the proposed methodology, a unique inverter-clustering technique is presented, which reduces the effort of optimising multiple low-level forecast models. A heuristic process of systematic hyperparameter optimisation is also proposed, which serves to guide future forecasting practitioners towards unbiased model development. From the deterministic and probabilistic confidence interval evaluations, overall results demonstrate a marginal increase in forecasting accuracy from the proposed aggregated inverter-level forecasts. The best performing macro-level model obtained Mean Absolute Percentage Error (MAPE) values ranging between 1.42%\u20138.13% for all weather types and forecast horisons. In comparison, the equivalent inverter-level forecasts delivered MAPE values ranging from 1.27%\u20138.29%. Finally, it is concluded that deep learning based macro-level forecast models have a sufficient ability to capture low-level PV system behaviour.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.746",
        "scimago_value": "3,035"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-823991-9",
        "journal": null,
        "publisher": "Elsevier",
        "title": "20metagenomicsacomputationalapproachinemergenceofnovelapplications",
        "booktitle": "Wastewater Treatment Reactors",
        "doi": "10.1016/B978-0-12-823991-9.00004-6",
        "author": [
            "Awasthi, Shruti",
            "Dwivedi, Shubha",
            "Dwivedi, Naveen"
        ],
        "keywords": [
            "Metagenomics, sequencing, computational approach, environment, ecosystem, statistical approaches"
        ],
        "abstract": "Metagenomics is a one of the fastest developing fields of research nowadays. It is a molecular tool used to study the genetic material obtained from environmental samples without culturing. Through culture-independent genomic sequencing, the metagenomics approach offers an exceptional analysis of the various microbial worlds in different environments, such as soil, air, ocean, and other water bodies, human and animal body sites, and many others. In the study of sequencing, metagenomics allows researcher to determine directly the collection of genes which are present in an environmental sample that are totally unknown organisms. It has been difficult to identify them, but now they can be analyzed by conducting biochemical tests and focusing on how they interact in the environment. Metagenomics data are growing at a fast pace so we need new infrastructure, methods, and technology so that this huge data can be analyzed and predicted; overcoming this problem with a computational approach can be a boon. A large volume of data could be managed by using a computational environment. This method collects, processes, and extracts useful biological information from samples and complex datasets, but genomics sequences need the integration of these computational methods. This chapter aims to focus on the detailed study of novel computational approach in metagenomics data analysis and the wide applications of sequencing methods.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "strategytoolsindynamicenvironmentsanexpertpanelstudy",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2020.120560",
        "author": [
            "Rengarajan, Srinath",
            "Moser, Roger",
            "Narayanamurthy, Gopalakrishnan"
        ],
        "keywords": [
            "Strategy tools, Strategy frameworks, Dynamic environments, Information processing, Contextual fit"
        ],
        "abstract": "Strategy tools and frameworks are crucial for managers to navigate their business environment and formulate strategies. Extant research has focused on the characteristics, dimensions, applications, and impact of traditional tools. However, there are questions regarding the suitability of these tools to the increasingly dynamic environments faced by strategy practitioners characterized by blurring industry boundaries, uncertainty, and ambiguity. Using an expert-panel approach, we address this research gap by investigating how strategy experts from practice and academia assess established strategy tools in dynamic environment. We identify the characteristics of strategy tools that experts value in such contexts and which can inform future development of context-specific strategy tools. Additionally, we also investigate why experts select and apply specific tools and how they combine these tools. Our findings further allow us to explore the difference in perspectives of strategy scholars and practitioners, which is necessary to reconcile the gap between strategy theory and practice. Finally, we discuss implications of the study for strategy and management research, education, and practice.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "eyetrackingalgorithmstechniquestoolsandapplicationswithanemphasisonmachinelearningandinternetofthingstechnologies",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2020.114037",
        "author": [
            "Klaib, Ahmad",
            "Alsrehin, Nawaf",
            "Melhem, Wasen",
            "Bashtawi, Haneen",
            "Magableh, Aws"
        ],
        "keywords": [
            "Eye tracking techniques, Eye tracking applications, Electrooculography, Infrared oculography, Internet of Things, Machine learning, Scleral coil, Video oculography, Cloud computing, Fog computing, Choice modeling, Consumer psychology, Marketing"
        ],
        "abstract": "Eye tracking is the process of measuring where one is looking (point of gaze) or the motion of an eye relative to the head. Researchers have developed different algorithms and techniques to automatically track the gaze position and direction, which are helpful in different applications. Research on eye tracking is increasing owing to its ability to facilitate many different tasks, particularly for the elderly or users with special needs. This study aims to explore and review eye tracking concepts, methods, and techniques by further elaborating on efficient and effective modern approaches such as machine learning (ML), Internet of Things (IoT), and cloud computing. These approaches have been in use for more than two decades and are heavily used in the development of recent eye tracking applications. The results of this study indicate that ML and IoT are important aspects in evolving eye tracking applications owing to their ability to learn from existing data, make better decisions, be flexible, and eliminate the need to manually re-calibrate the tracker during the eye tracking process. In addition, they show that eye tracking techniques have more accurate detection results compared with traditional event-detection methods. In addition, various motives and factors in the use of a specific eye tracking technique or application are explored and recommended. Finally, some future directions related to the use of eye tracking in several developed applications are described.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "thedigitaltwininorderprocessing",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.11.145",
        "author": [
            "Wagner, Sarah",
            "Milde, Michael",
            "Reinhart, Gunther"
        ],
        "keywords": [
            "digital twin, order processing, decision support, production, logistic network"
        ],
        "abstract": "Environmental factors, such as the high number of product variants, increase the complexity of order processing (OP). Holistic decision making presents a challenge due to the lack of transparency in OP, an inadequate data basis, and the unknown effects of decision alternatives. Today\u2019s information systems and traditional calculation formulas for managing production only consider subsystems and isolated abstraction levels. As digital twins introduce new opportunities for decision support, their potential in OP is widely recognized, while their designing process still lacks research and methodologies. Consequently, we present a digital twin in OP, which enables efficient decision support.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820045-2",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter3datatypesandresources",
        "booktitle": "The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry",
        "doi": "10.1016/B978-0-12-820045-2.00004-0",
        "author": [
            "Ashenden, Stephanie",
            "Deswal, Sumit",
            "Bulusu, Krishna",
            "Bartosik, Aleksandra",
            "Shameer, Khader"
        ],
        "keywords": [
            "Data, Omics, FAIR, Big data, SMILES, InChI"
        ],
        "abstract": "Recent innovation in the field of machine learning has been enabled by the confluence of three advances: rapid expansion of affordable computing power in the form of cloud computing environments, the accelerating pace of infrastructure associated with large-scale data collection and rapid methodological advancements, particularly neural network architecture improvements. Development and adoption of these advances have lagged in the health care domain largely due to restrictions around public use of data and siloed nature of these datasets with respect to providers, payers and clinical trial sponsors.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13648152",
        "isbn": null,
        "journal": "Environmental Modelling & Software",
        "publisher": null,
        "title": "networkconstructionevaluationanddocumentationaguideline",
        "booktitle": null,
        "doi": "10.1016/j.envsoft.2021.105020",
        "author": [
            "Scharler, U.M.",
            "Borrett, S.R."
        ],
        "keywords": [
            "Weighted networks, Ecosystem, Socio-economic, Best-practice, Plausibility, Sensitivity"
        ],
        "abstract": "Network analysis of complex systems is a rapidly growing field. Both theoretical and empirical network studies have permeated many different ecological, biological, social, and economic fields, investigating the interrelationships between nodes as structural and functional attributes in static, time-dynamic, or spatially explicit formats. We consider the network construction phase as a vital, but neglected component, and therefore provide recommended guidelines, describe how to evaluate the resulting network model quality, and highlight tools to assess their plausibility. Thereby we stress the importance of constructing multiple plausible networks to comply with basic scientific standards, and to pave the way for better informed evaluations. Finally, we provide recommendations for the management and policy arena where we advocate a thorough interrogation of network analyses outcomes (metrics) especially with regard to their sensitivity to the construction process, and a focus on relative changes between and within systems (e.g. as indication of vulnerability), rather than strict benchmarks.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,828"
    },
    {
        "issnkey": "00344257",
        "isbn": null,
        "journal": "Remote Sensing of Environment",
        "publisher": null,
        "title": "anautomatedgeneralizeddeeplearningbasedmethodfordelineatingthecalvingfrontsofgreenlandglaciersfrommultisensorremotesensingimagery",
        "booktitle": null,
        "doi": "10.1016/j.rse.2020.112265",
        "author": [
            "Zhang, Enze",
            "Liu, Lin",
            "Huang, Lingcao",
            "Ng, Ka"
        ],
        "keywords": [
            ""
        ],
        "abstract": "In the past two decades, the data volume of remote sensing imagery in the polar regions has increased dramatically. The calving fronts of many Greenland glaciers have been undergoing substantial variations, and a comprehensive front dataset is necessary for better understanding such frontal dynamics. Therefore, there is a need for an automated approach to identifying glaciological features such as calving fronts. In 2019, three deep-learning-based methods were applied to calving front delineation, but were restricted to a specific area or dataset. Here, we develop a more generalized method that can be applied to a major outlet glacier or remote sensing datasets that are not included in the training. We integrate seven remote sensing datasets into a single deep learning network. The core datasets include optical (Landsat-8 and Sentinel-2) and synthetic aperture radar images (Envisat, ALOS-1 TerraSAR-X, Sentinel-1, and ALOS-2) taken over Jakobshavn Isbr\u00e6, Kangerlussuaq, and Helheim, spanning from 2002 to 2019. We evaluate four neural network architectures (e.g., U-Net, DeepLabv3+ with ResNet, DRN, and MobileNet as the backbones) and three histogram modification strategies (e.g., histogram normalization, linear stretching, and no histogram modification). We find that the combination of histogram normalization and DRN-DeepLabv3+ has the lowest test error, at 86 m. These promising results show that our method has a high generalization ability on various glaciers and data types.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "10.164",
        "scimago_value": "3,611"
    },
    {
        "issnkey": "22119124",
        "isbn": null,
        "journal": "Global Food Security",
        "publisher": null,
        "title": "areviewofsatellitebasedglobalagriculturalmonitoringsystemsavailableforafrica",
        "booktitle": null,
        "doi": "10.1016/j.gfs.2021.100543",
        "author": [
            "Nakalembe, Catherine",
            "Becker-Reshef, Inbal",
            "Bonifacio, Rogerio",
            "Hu, Guangxiao",
            "Humber, Michael",
            "Justice, Christina",
            "Keniston, John",
            "Mwangi, Kenneth",
            "Rembold, Felix",
            "Shukla, Shraddhanand",
            "Urbano, Ferdinando",
            "Whitcraft, Alyssa",
            "Li, Yanyun",
            "Zappacosta, Mario",
            "Jarvis, Ian",
            "Sanchez, Antonio"
        ],
        "keywords": [
            "Satellite Earth Observations, Scalable, Operational agriculture monitoring, Open access, Africa"
        ],
        "abstract": "The increasing frequency and severity of extreme climatic events and their impacts are being realized in many regions of the world, particularly in smallholder crop and livestock production systems in Sub-Saharan Africa (SSA). These events underscore the need for timely early warning. Satellite Earth Observation (EO) availability, rapid developments in methodology to archive and process them through cloud services and advanced computational capabilities, continue to generate new opportunities for providing accurate, reliable, and timely information for decision-makers across multiple cropping systems and for resource-constrained institutions. Today, systems and tools that leverage these developments to provide open access actionable early warning information exist. Some have already been employed by early adopters and are currently operational in selecting national monitoring programs in Angola, Kenya, Rwanda, Tanzania, and Uganda. Despite these capabilities, many governments in SSA still rely on traditional crop monitoring systems, which mainly rely on sparse and long latency in situ reports with little to no integration of EO-derived crop conditions and yield models. This study reviews open-access operational agricultural monitoring systems available for Africa. These systems provide the best-available open-access EO data that countries can readily take advantage of, adapt, adopt, and leverage to augment national systems and make significant leaps (timeliness, spatial coverage and accuracy) of their monitoring programs. Data accessible (vegetation indices, crop masks) in these systems are described showing typical outputs. Examples are provided including crop conditions maps, and damage assessments and how these have integrated into reporting and decision-making. The discussion compares and contrasts the types of data, assessments and products can expect from using these systems. This paper is intended for individuals and organizations seeking to access and use EO to assess crop conditions who might not have the technical skill or computing facilities to process raw data into informational products.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,350"
    },
    {
        "issnkey": "10538119",
        "isbn": null,
        "journal": "NeuroImage",
        "publisher": null,
        "title": "tractlearnageodesiclearningframeworkforquantitativeanalysisofbrainbundles",
        "booktitle": null,
        "doi": "10.1016/j.neuroimage.2021.117927",
        "author": [
            "Atty\u00e9, Arnaud",
            "Renard, F\u00e9lix",
            "Baciu, Monica",
            "Roger, Elise",
            "Lamalle, Laurent",
            "Dehail, Patrick",
            "Cassoudesalle, H\u00e9l\u00e8ne",
            "Calamante, Fernando"
        ],
        "keywords": [
            "Diffusion MRI, Fiber tractography, Precision medicine, Manifold learning"
        ],
        "abstract": "Deep learning-based convolutional neural networks have recently proved their efficiency in providing fast segmentation of major brain fascicles structures, based on diffusion-weighted imaging. The quantitative analysis of brain fascicles then relies on metrics either coming from the tractography process itself or from each voxel along the bundle. Statistical detection of abnormal voxels in the context of disease usually relies on univariate and multivariate statistics models, such as the General Linear Model (GLM). Yet in the case of high-dimensional low sample size data, the GLM often implies high standard deviation range in controls due to anatomical variability, despite the commonly used smoothing process. This can lead to difficulties to detect subtle quantitative alterations from a brain bundle at the voxel scale. Here we introduce TractLearn, a unified framework for brain fascicles quantitative analyses by using geodesic learning as a data-driven learning task. TractLearn allows a mapping between the image high-dimensional domain and the reduced latent space of brain fascicles using a Riemannian approach. We illustrate the robustness of this method on a healthy population with test-retest acquisition of multi-shell diffusion MRI data, demonstrating that it is possible to separately study the global effect due to different MRI sessions from the effect of local bundle alterations. We have then tested the efficiency of our algorithm on a sample of 5 age-matched subjects referred with mild traumatic brain injury. Our contributions are to propose: 1/ A manifold approach to capture controls variability as standard reference instead of an atlas approach based on a Euclidean mean. 2/ A tool to detect global variation of voxels\u2019 quantitative values, which accounts for voxels\u2019 interactions in a structure rather than analyzing voxels independently. 3/ A ready-to-plug algorithm to highlight nonlinear variation of diffusion MRI metrics. With this regard, TractLearn is a ready-to-use algorithm for precision medicine.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0169555x",
        "isbn": null,
        "journal": "Geomorphology",
        "publisher": null,
        "title": "rapidlyassessingearthquakeinducedlandslidesusceptibilityonaglobalscaleusingrandomforest",
        "booktitle": null,
        "doi": "10.1016/j.geomorph.2021.107889",
        "author": [
            "He, Qian",
            "Wang, Ming",
            "Liu, Kai"
        ],
        "keywords": [
            "Landslide susceptibility, Random forest model, Earthquake, Global scale"
        ],
        "abstract": "Earthquake-induced landslides (EQILs) are an incredibly destructive geological disaster. Rapid landslide susceptibility assessments are indispensable and critical for risk analysis and emergency management. Previous studies mainly focus on the regional-scale assessment of EQIL susceptibility, while the global analyses of that are lacking. In this study, we constructed a global model for rapidly assessing earthquake-induced landslide susceptibility based on the random forest (RF) algorithm using globally available data. In total, 288,114 landslides from 16 high-quality EQIL inventories were utilized to develop the global landslide model. We split the data into 70% training dataset for model training and 30% testing data for model evaluation. We also used three blind test events to validate the model performance. The model showed excellent performance on the testing data (accuracy = 0.945, and AUC = 0.985). The RF model exhibited strong spatial generalizability and robustness, with an AUC exceeding 0.8 for each landslide inventory and showing good performance on the blind test events. The resulting landslide susceptibility maps also match relatively well with the actual landslide locations. Among the conditioning factors, modified Mercalli intensity (MMI), elevation and slope are the three most important conditioning factors. The susceptibility maps for each landslide event were produced. The developed RF model would be useful in studies of earthquake-induced landslide susceptibility and emergency response after an earthquake.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02684012",
        "isbn": null,
        "journal": "International Journal of Information Management",
        "publisher": null,
        "title": "smartcityresearchadvancesinsoutheasteurope",
        "booktitle": null,
        "doi": "10.1016/j.ijinfomgt.2020.102127",
        "author": [
            "{Nin\u010devi\u0107 Pa\u0161ali\u0107}, Ivana",
            "\u0106uku\u0161i\u0107, Maja",
            "Jadri\u0107, Mario"
        ],
        "keywords": [
            "Smart cities, Southeast Europe, Descriptive literature review"
        ],
        "abstract": "Smart city (SC) research is an engaging research area as evidenced by a rising number of publications indexed in the most relevant global citation databases. However, research advances are not equally discussed and distributed within Europe. This study puts a focus on the specific geographic location of Southeast Europe (SEE), intending to fill the gap in understanding the research advances in this part of Europe. The aim of this descriptive review was to systematically investigate peer-reviewed publications focused on SC research in SEE in order to present the findings and the state-of-art in this research domain. Seventy-four papers were thoroughly studied, analysed and classified based on their focus on SC themes and common sub-themes. While smart governance had been studied extensively in the SEE region, topics related to the smart economy and smart people received low attention from researchers. Mapping the selected papers to the Plan-Do-Check-Act (PDCA) cycle showed that SC research in SEE is still in the conceptualising and planning stages, with very little evidence from the real implementation and follow-up activities. From the stakeholders\u2019 perspective, the focus is on the institutional point of view as most of the papers present their findings in relation to (national or local) government bodies or policies, without balancing with corresponding businesses\u2019 or individuals\u2019 (users\u2019) point of view. In general, user involvement was found to be very low in regards to current SC research in the SEE region.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "14.098",
        "scimago_value": "2,770"
    },
    {
        "issnkey": "02642751",
        "isbn": null,
        "journal": "Cities",
        "publisher": null,
        "title": "quantifyingspatiotemporalpatternsofshrinkingcitiesinurbanizingchinaanovelapproachbasedontimeseriesnighttimelightdata",
        "booktitle": null,
        "doi": "10.1016/j.cities.2021.103346",
        "author": [
            "Yang, Yang",
            "Wu, Jianguo",
            "Wang, Ying",
            "Huang, Qingxu",
            "He, Chunyang"
        ],
        "keywords": [
            "Shrinking cities, Urban shrinkage, Nighttime light, Urban sustainability, Urbanization, China"
        ],
        "abstract": "The shrinking of cities has become an increasingly global phenomenon, posing challenges for sustainable urban development. However, most focus remains on Europe and North America, and relatively little attention has been paid to the East Asia, especially the urbanizing China. Nighttime light (NL) dataset and its features (long-term time-series free access and large coverage) provide an alternative means to quantify shrinking cities. Here, we developed a new approach to identify shrinking cities and measure urban shrinkage, using corrected-integrated DMSP/OLS and NPP/VIIRS NL data. Based on this approach, we quantified the spatiotemporal patterns of shrinking cities in China from 1992 to 2019. Our study identified 153 shrinking cities in China during the study period, accounting for 23.39% of all 654 cities. These shrinking cities were widely distributed across eight economic regions and most provinces. The number of shrinking cities changed periodically and peaked following the Asian Financial Crisis in 1997 and again after the Global Economic Crisis in 2008. The cities that experienced the greatest shrinkage intensity were mainly distributed in northeast China, with severe urban shrinkage occurring between 2008 and 2013. The new approach proposed in this study can effectively identify shrinking city hotspots and key periods of urban shrinkage. Our findings suggest that sustainable urban development in China must consider shrinking cities, which are faced with challenging and urgent sustainability issues different from those by rapidly growing cities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.835",
        "scimago_value": "1,771"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "howandwhenhigherclimatechangeriskperceptionpromoteslessclimatechangeinaction",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.128952",
        "author": [
            "Wang, Changcheng",
            "Geng, Liuna",
            "Rodr\u00edguez-Casallas, Juli\u00e1n"
        ],
        "keywords": [
            "Climate change risk perception, Sustainable development, Climate change inaction, Mindfulness, Climate change belief, Environmental efficacy"
        ],
        "abstract": "Climate change has been positioned as one of the most severe environmental threats facing us today. To address climate change, enhancing climate change risk perception and reducing climate change inaction are both critical. However, little research has touched on the issue of whether climate change risk perception is linked to climate change inaction in a negative manner. Moreover, there is still much unknown about the complex process behind this relationship, and the boundary conditions of this process awaits clarification. To address these gaps in the literature, two studies were conducted to first confirm the possible negative association between climate change risk perception and climate change inaction and, second, explore through a parallel mediational model whether climate change belief and environmental efficacy mediate simultaneously the relation between climate change risk perception and climate change inaction. Finally, a moderated sequential mediational model was used to investigate whether climate change risk perception is associated with climate change inaction through the sequential mediation of climate change belief and environmental efficacy, and to clarify underlying boundary conditions by analyzing the moderation of mindfulness as well. The results showed that, as expected, higher levels of climate change risk perception were related to less climate change inaction, and this relation was mediated by enhanced climate change belief and heightened environmental efficacy in a sequential manner. Furthermore, the sequential mediating effect of climate change belief and environmental efficacy was stronger among those who had a higher level of mindfulness. These findings advance the emerging research on climate change inaction by elucidating the mechanisms underlying the effect of climate change risk perception. Moreover, they extend the Domain-Context-Behavior (DCB) model and Gateway belief model (GBM). In practice, climate change education and climate change inaction interventions can be designed and implemented to nudge clean production, green supply chain and green consumption, which finally contribute to sustainable development and the \u2018green transformation\u2019 of society.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "09252312",
        "isbn": null,
        "journal": "Neurocomputing",
        "publisher": null,
        "title": "anovelrumordetectionalgorithmbasedonentityrecognitionsentencereconfigurationandordinarydifferentialequationnetwork",
        "booktitle": null,
        "doi": "10.1016/j.neucom.2021.03.055",
        "author": [
            "Ma, Tinghuai",
            "Zhou, Honghao",
            "Tian, Yuan",
            "Al-Nabhan, Najla"
        ],
        "keywords": [
            "Rumor detection, Entity recognition, Sentence reconfiguration, ODE-net"
        ],
        "abstract": "Social media has recently become one of the most used media in the world. This has resulted in a great hotbed for the growth of rumors, as anyone can spread knowledge and opinions without confirmation. Previous works on rumor detection focused on hand-extracted features and spent less effort on text representation. In this research, a novel method for rumor detection on social media is proposed, which integrates entity recognition, sentence reconfiguration and ordinary differential equation network under a unified framework called ESODE. An entity recognition method to enhance the semantic understanding of rumor texts is used. Then, a sentence reconfiguration to improve the frequency of important words is designed. The complete feature map is established by further collecting statistical features from three aspects: linguistic features on the content of rumors, characteristics of users involved in rumor propagating, and propagation network structures. Finally, the ordinary differential equation network (ODEnet) is applied to detect rumors. Experimental results on datasets from Twitter and Weibo show that the proposed method achieves better performance than previous ones.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.719",
        "scimago_value": "1,085"
    },
    {
        "issnkey": "01679236",
        "isbn": null,
        "journal": "Decision Support Systems",
        "publisher": null,
        "title": "tunocmamodeldrivenapproachtosupportdatabasetuningdecisionmaking",
        "booktitle": null,
        "doi": "10.1016/j.dss.2021.113538",
        "author": [
            "Almeida, Ana",
            "Bai\u00e3o, Fernanda",
            "Lifschitz, S\u00e9rgio",
            "Schwabe, Daniel",
            "Campos, Maria"
        ],
        "keywords": [
            "Database systems, Tuning decision, Heuristics, Configuration management, Ontology pattern language"
        ],
        "abstract": "Database tuning is a task executed by Database Administrators (DBAs) based on their practical experience and on tuning systems, which support DBA actions towards improving the performance of a database system. It is notoriously a complex task that requires precise domain knowledge about possible database configurations. Ideally, a DBA should keep track of several Database Management Systems (DBMS) parameters, configure data structures, and must be aware about possible interferences among several database (DB) configurations. We claim that an automatic tuning system is a decision support system and DB tuning may also be seen as a configuration management task. Therefore, we may characterize it by means of a formal domain conceptualization, benefiting from existing control practices and computational support in the configuration management domain. This work presents Tun-OCM, a conceptual model represented as a well-founded ontology, that encompasses a novel characterization of the database tuning domain as a configuration management conceptualization to support decision making. We develop and represent Tun-OCM using the CM-OPL methodology and its underlying language. The benefits of Tun-OCM are discussed by instantiating it in a real scenario.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.795",
        "scimago_value": "1,564"
    },
    {
        "issnkey": "18770509",
        "isbn": null,
        "journal": "Procedia Computer Science",
        "publisher": null,
        "title": "taxonomyofgenerativeadversarialnetworksfordigitalimmunityofindustry40systems",
        "booktitle": null,
        "doi": "10.1016/j.procs.2021.01.290",
        "author": [
            "Terziyan, Vagan",
            "Gryshko, Svitlana",
            "Golovianko, Mariia"
        ],
        "keywords": [
            "Industry 4.0, Generative Adversarial Networks, cybersecurity, artificial digital immunity"
        ],
        "abstract": "Industry 4.0 systems are extensively using artificial intelligence (AI) to enable smartness, automation and flexibility within variety of processes. Due to the importance of the systems, they are potential targets for attackers trying to take control over the critical processes. Attackers use various vulnerabilities of such systems including specific vulnerabilities of AI components. It is important to make sure that inappropriate adversarial content will not break the security walls and will not harm the decision logic of critical systems. We believe that the corresponding security toolset must be organized as a trainable self-protection mechanism similar to immunity. We found certain similarities between digital vs. biological immunity and we study the possibilities of Generative Adversarial Networks (GANs) to provide the basis for the digital immunity training. We suggest the taxonomy of GANs (including new architectures) suitable to simulate various aspects of the immunity for Industry 4.0 applications.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,334"
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "blockchainapplicationsforclimateprotectionaglobalempiricalinvestigation",
        "booktitle": null,
        "doi": "10.1016/j.rser.2021.111378",
        "author": [
            "Dorfleitner, Gregor",
            "Muck, Franziska",
            "Scheckenbach, Isabel"
        ],
        "keywords": [
            "Blockchain, Distributed ledger, Green finance, Consensus mechanisms, Peer-to-peer transactions, Sustainability goals"
        ],
        "abstract": "Our research consolidates the actual environment of blockchain applications that contribute in a certain way to climate protection. In view of the growing interest in climate change and the need to act on a global scale, knowledge about these applications enables investors, politicians, and citizens to drive this development forward through diverse support opportunities. This article provides an extensive overview of existing mitigation and adaptation measures based on blockchain technology. We collect data on 85 such applications and describe the empirical distributions of different attributes of these applications. In a logit regression, we analyze which application-specific and blockchain-specific characteristics determine the success of an application in the sense of an advanced operational status. We find evidence that applications of the type \u201cenergy trading\u201d exhibit reduced chances of success, while green blockchain-based applications implementing a proof-of-stake consensus mechanism are more likely to become operational. Moreover, pursuing an initial coin offering has no significant effect on the success of an application. Our work provides the basis for a better understanding of the success factors of this new technology.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "02697491",
        "isbn": null,
        "journal": "Environmental Pollution",
        "publisher": null,
        "title": "increasedozonepollutionalongsidereducednitrogendioxideconcentrationsduringviennasfirstcovid19lockdownsignificanceforairqualitymanagement",
        "booktitle": null,
        "doi": "10.1016/j.envpol.2021.117153",
        "author": [
            "Brancher, Marlon"
        ],
        "keywords": [
            "COVID-19 lockdown, Air quality data, Atmospheric composition, Meteorology, Machine learning"
        ],
        "abstract": "Background Lockdowns amid the COVID-19 pandemic have offered a real-world opportunity to better understand air quality responses to previously unseen anthropogenic emission reductions. Methods and main objective This work examines the impact of Vienna\u2019s first lockdown on ground-level concentrations of nitrogen dioxide (NO2), ozone (O3) and total oxidant (Ox). The analysis runs over January to September 2020 and considers business as usual scenarios created with machine learning models to provide a baseline for robustly diagnosing lockdown-related air quality changes. Models were also developed to normalise the air pollutant time series, enabling facilitated intervention assessment. Core findings NO2 concentrations were on average \u221220.1% [13.7\u201330.4%] lower during the lockdown. However, this benefit was offset by amplified O3 pollution of +8.5% [3.7\u201311.0%] in the same period. The consistency in the direction of change indicates that the NO2 reductions and O3 increases were ubiquitous over Vienna. Ox concentrations increased slightly by +4.3% [1.8\u20136.4%], suggesting that a significant part of the drops in NO2 was compensated by gains in O3. Accordingly, 82% of lockdown days with lowered NO2 were accompanied by 81% of days with amplified O3. The recovery shapes of the pollutant concentrations were depicted and discussed. The business as usual-related outcomes were broadly consistent with the patterns outlined by the normalised time series. These findings allowed to argue further that the detected changes in air quality were of anthropogenic and not of meteorological reason. Pollutant changes on the machine learning baseline revealed that the impact of the lockdown on urban air quality were lower than the raw measurements show. Besides, measured traffic drops in major Austrian roads were more significant for light-duty than for heavy-duty vehicles. It was also noted that the use of mobility reports based on cell phone movement as activity data can overestimate the reduction of emissions for the road transport sector, particularly for heavy-duty vehicles. As heavy-duty vehicles can make up a large fraction of the fleet emissions of nitrogen oxides, the change in the volume of these vehicles on the roads may be the main driver to explain the change in NO2 concentrations. Interpretation and implications A probable future with emissions of volatile organic compounds (VOCs) dropping slower than emissions of nitrogen oxides could risk worsened urban O3 pollution under a VOC-limited photochemical regime. More holistic policies will be needed to achieve improved air quality levels across different regions and criteria pollutants.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.071",
        "scimago_value": "2,136"
    },
    {
        "issnkey": "09265805",
        "isbn": null,
        "journal": "Automation in Construction",
        "publisher": null,
        "title": "workingstageidentificationofexcavatorsbasedoncontrolsignalsofoperatinghandles",
        "booktitle": null,
        "doi": "10.1016/j.autcon.2021.103873",
        "author": [
            "Shi, Yupeng",
            "Xia, Yimin",
            "Luo, Lianglin",
            "Xiong, Zhihong",
            "Wang, Chengyu",
            "Lin, Laikuang"
        ],
        "keywords": [
            "Excavator, Working stage identification, Control signal, Operating handle, Long short-term memory (LSTM)"
        ],
        "abstract": "To improve the automated management level on construction sites, real-time monitoring of excavators' working stage for production efficiency and economic consumption analysis has been implemented in many projects, revealing great advantages. However, existing vision-based and non-vision-based working stage identification methods ignore the influence of response delay of hydraulic system on the recognition results. To overcome this problem, three machine learning algorithms, which select the control signals of operating handles that can reflect the actuator real-time operating status as segmentation marks, are used to establish the excavator working stage identification model in this study. The results show that the Long Short-Term Memory (LSTM) classifier has an accuracy of 93.21% and effectively reduces the lagging misidentification to 4.68%. This study contributes to automatic measurement of the excavator operational efficiency. For future work, the approach is combined with the adjustment strategy of engine working point to realize the staged energy-saving control of excavators.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.700",
        "scimago_value": "1,837"
    },
    {
        "issnkey": "22106707",
        "isbn": null,
        "journal": "Sustainable Cities and Society",
        "publisher": null,
        "title": "usingdatamonitoringalgorithmstophysiologicalindicatorsinmotionbasedoninternetofthingsinsmartcity",
        "booktitle": null,
        "doi": "10.1016/j.scs.2021.102727",
        "author": [
            "Tian, Jian",
            "Gao, Lulu"
        ],
        "keywords": [
            "Internet of Things, Data fusion algorithm, Physiological indicators, Monitoring, Smart city"
        ],
        "abstract": "This article discusses the monitoring of physiological indicators during exercise, combined with the data fusion algorithm of the smart city Internet of Things health. We use the hash value of the tuple key to the corresponding data block of the node, use the data block record to obtain the response of the target node, and output the data tuple. It is used as a measure of the load balance of health data streams to determine whether load migration is needed and to determine the way and amount of migration tasks to make migration decisions. The simulation experiments show that the method has good computational performance and dynamic load balancing. A series of mean arterial pressure and heart rate of patients and non-stationary health data, and a series of blood pressure and heart rate of health individuals in different postures are selected to perform experiments to analyze the transfer function and power spectra in the model, validating that the model can be used to reveal the changes associated with severe systemic response syndrome (SIRS), providing a hypothesis for the decomposition of autoregulation of physiological control under health and disease conditions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.587",
        "scimago_value": "1,645"
    },
    {
        "issnkey": "00018791",
        "isbn": null,
        "journal": "Journal of Vocational Behavior",
        "publisher": null,
        "title": "predictingtheselfregulatedjobsearchofmatureagedjobseekerstheuseofelectiveselectionlossbasedselectionoptimizationandcompensationstrategies",
        "booktitle": null,
        "doi": "10.1016/j.jvb.2021.103591",
        "author": [
            "Watermann, Henriette",
            "Fasbender, Ulrike",
            "Klehe, Ute-Christine"
        ],
        "keywords": [
            "Aging, Job search, Mature-aged job seekers, Reemployment efficacy, Self-regulation, SOC strategies"
        ],
        "abstract": "Job search is a demanding and often demotivating process, challenging job-seekers' self-regulation. Particularly, mature-aged job seekers face lower reemployment chances \u2013 and may benefit from strategies known from the lifespan literature. The current study examined whether and when the use of aging strategies (elective selection, loss-based selection, optimization, and compensation; SOC strategies) can support mature-aged job seekers in their self-regulated job search process (goal establishment and goal pursuit). We collected data from 659 mature-aged job seekers in three countries (Germany, United Kingdom, and United States) at four different times over two months. Results of multi-level modeling showed no support for gain-oriented strategies, namely elective selection (prioritizing one instead of multiple goals) and optimization (investing every effort to reach one's goal). In contrast, loss-oriented strategies, namely loss-based selection (prioritizing or selecting a new goal after a setback) and compensation (using new or previously unused means in the face of obstacles), supported mature-aged job seekers' goal establishment and goal pursuit. Moreover, with increasing age, mature-aged job seekers reported lower reemployment efficacy (the confidence to find a new job), which moderated the relation between compensation with goal pursuit. Compensation was particularly helpful for mature-aged job seekers' goal pursuit in weeks in which they reported lower (vs. higher) reemployment efficacy. These findings highlight the importance of loss-oriented aging strategies as beneficial coping strategies. With regard to practice, the present study speaks to the benefits of SOC strategies and points to the development of interventions targeted toward mature-aged job seekers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10511377",
        "isbn": null,
        "journal": "Journal of Housing Economics",
        "publisher": null,
        "title": "measuringaggregatehousingwealthnewinsightsfrommachinelearning",
        "booktitle": null,
        "doi": "10.1016/j.jhe.2020.101734",
        "author": [
            "Gallin, Joshua",
            "Molloy, Raven",
            "Nielsen, Eric",
            "Smith, Paul",
            "Sommer, Kamila"
        ],
        "keywords": [
            "Residential real estate, Consumer economics and finance, Data collection and estimation, Flow of funds"
        ],
        "abstract": "We construct a new measure of aggregate housing wealth for the U.S. based on (1) home-value estimates derived from machine learning algorithms applied to detailed information on property characteristics and recent transaction prices, and (2) Census housing unit counts. According to our new measure, the timing and amplitude of the recent house-price cycle differs materially but plausibly from commonly-used measures, which are based on survey data or repeat-sales price indexes. Thus, our methodology generates estimates that should be of considerable value to researchers and policymakers interested in the dynamics of aggregate housing wealth.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "17468094",
        "isbn": null,
        "journal": "Biomedical Signal Processing and Control",
        "publisher": null,
        "title": "areviewofmachinelearninginhypertensiondetectionandbloodpressureestimationbasedonclinicalandphysiologicaldata",
        "booktitle": null,
        "doi": "10.1016/j.bspc.2021.102813",
        "author": [
            "Martinez-R\u00edos, Erick",
            "Montesinos, Luis",
            "Alfaro-Ponce, Mariel",
            "Pecchia, Leandro"
        ],
        "keywords": [
            "Hypertension, Clinical data, Physiological data, Machine learning"
        ],
        "abstract": "The use of machine learning techniques in medicine has increased in recent years due to a rise in publicly available datasets. These techniques have been applied in high blood pressure studies following two approaches: hypertension stage classification based on clinical data and blood pressure estimation based on related physiological signals. This paper presents a literature review on such studies. We aimed to identify the best practices, challenges, and opportunities in developing machine learning models to detect hypertension or estimate blood pressure using clinical data and physiological signals. Hence, we identified and examined the machine learning techniques, publicly available datasets, and predictors used in previous studies. The feature selection techniques used to reduce model complexity are also reviewed. We found a lack of studies combining socio-demographic or clinical data with physiological signals, despite the correlation of blood pressure with photoplethysmography waveforms and variables such as age, gender, body mass index, and heart rate. Therefore, there is an opportunity to increase model performance by using both types of data for hypertension detection or blood pressure monitoring.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.880",
        "scimago_value": "0,767"
    },
    {
        "issnkey": "02673649",
        "isbn": null,
        "journal": "Computer Law & Security Review",
        "publisher": null,
        "title": "howfarcanconvention108globaliseprospectsforasianaccessions",
        "booktitle": null,
        "doi": "10.1016/j.clsr.2020.105414",
        "author": [
            "Greenleaf, Graham"
        ],
        "keywords": [
            ""
        ],
        "abstract": "The \u2018globalisation\u2019 of Council of Europe data protection Convention 108 through non-European accessions has continued steadily, with eight such accessions since the first in 2013. The \u2018modernisation\u2019 of the Convention was completed on 10 October 2018 when the amending protocol for the new \u2018Convention 108+\u2019 became open for signature. Any new countries from outside Europe wishing to accede will have to accede to both Convention 108 and the amending Protocol (ie to 108+). The standards required of the laws of acceding countries by 108+ are higher than those required by 108, and are arguably mid-way between 108 and those of the European Union's General Data Protection Regulation (GDPR). This article examines to what extent each of the 26 \u2018countries\u2019 (separate jurisdictions) in Asia are likely to be able to accede to 108+, if they wish to. As yet, none have acceded to 108. It proposes an efficient way to consider such a question across such a complex set of jurisdictions. Fifteen of the 26 Asian countries already have data privacy laws, and two others have official Bills for such laws. An assessment of the prospects for accession can be done by considering in order the following grounds which may be impediments to accession: Jurisdictions which are not States; States which are not democratic; Laws of inadequate scope; Laws lacking an independent data protection authority; Laws with substantive provisions falling short of 108+ \u2018accession standards\u2019; States with proposed Bills only; and States with no relevant laws or proposed Bills. The most difficult step in this procedure is in deciding which of the substantive provisions of 108+ constitute its \u2018accession standards\u2019, or elements essential for accession to be invited. Neither the Convention, nor the guidelines issued by its Consultative Committee, shed much light on this question. However, previous practice under Convention 108, show there is some flexibility involved. The article concludes with suggestions as to how such flexibility can be made more transparent, and observations on which Asian countries, in light of the seven step assessment carried out in the article, are the most likely candidates to be able to accede to 108+, in both the short and medium terms.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,815"
    },
    {
        "issnkey": "03787788",
        "isbn": null,
        "journal": "Energy and Buildings",
        "publisher": null,
        "title": "buildingoccupancyforecastingasystematicalandcriticalreview",
        "booktitle": null,
        "doi": "10.1016/j.enbuild.2021.111345",
        "author": [
            "Jin, Yuan",
            "Yan, Da",
            "Chong, Adrian",
            "Dong, Bing",
            "An, Jingjing"
        ],
        "keywords": [
            "Occupancy prediction, Forecast, Occupant behavior, Building, Operation, Energy conservation"
        ],
        "abstract": "Indoor environment construction for occupants has high energy consumption; as such, occupancy plays a noteworthy role in the complete life cycle phase of buildings, including design, operation, and retrofitting. In the past few years, building occupancy, which is considered the basis of occupant behavior, has attracted increasing attention from researchers. There are increasing requirements for buildings to be both comfortable and energy efficient; with the development of detection methods and analyzing algorithms, occupancy prediction has become a topic of interest for building automation and energy conservation. Therefore, this article reviews the literature regarding future building occupancy predictions (forecasting). This review is distinguished from occupancy simulation and detection research and focuses on the research purpose, physical routine, and complete methodology of occupancy forecasting. First, the research purposes, including the application field and detailed requirements for occupancy forecasting, are summarized and analyzed. Next, an overall methodology of occupancy forecasting, including data acquisition, modeling techniques, and evaluation, is discussed in terms of issues affecting prediction performance. Finally, the current challenges and perspectives of occupancy forecasting are highlighted, considering the insights of natural characteristics, on-site implementation, valid dataset sharing, and research techniques. Overall, accurate and robust future occupancy predictions will help to improve building system operations and energy conservation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.879",
        "scimago_value": "1,737"
    },
    {
        "issnkey": "00991333",
        "isbn": null,
        "journal": "The Journal of Academic Librarianship",
        "publisher": null,
        "title": "anexaminationofdatareusepracticeswithinhighlycitedarticlesoffacultyataresearchuniversity",
        "booktitle": null,
        "doi": "10.1016/j.acalib.2021.102369",
        "author": [
            "Imker, Heidi",
            "Luong, Hoa",
            "Mischo, William",
            "Schlembach, Mary",
            "Wiley, Chris"
        ],
        "keywords": [
            "Data reuse, Data sharing, Data management, Data services, Scopus API"
        ],
        "abstract": "Data sharing and reuse are regarded as important components of the research workflow and key elements in open science. While reuse is well-documented in some circumstances, the utility of data sharing for all domains is less clear, and limited evidence of wide-spread demand can make it challenging to justify effort and funds required to format, document, share, and preserve data. This paper describes a project that: (1) surveyed authors of highly cited papers published in 2015 at the University of Illinois at Urbana-Champaign in nine STEM disciplines to determine if data were generated for their article and their knowledge of reuse by other researchers, and (2) surveyed authors who cited these 2015 articles to ascertain whether they reused data from the original article and how that data was obtained. The project goal was to better understand data reuse in practice and to explore if research data from an initial publication was reused in subsequent publications. While the results revealed reuse in many situations (and deemed important in these cases), the survey results and researcher supplied comments also indicated that data does not play the same role in all studies or even in studies that build on previous ones.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.533",
        "scimago_value": "0,889"
    },
    {
        "issnkey": "03787206",
        "isbn": null,
        "journal": "Information & Management",
        "publisher": null,
        "title": "aquantitativeandqualitativestudyofthelinkbetweenbusinessprocessmanagementanddigitalinnovation",
        "booktitle": null,
        "doi": "10.1016/j.im.2020.103413",
        "author": [
            "{Van Looy}, Amy"
        ],
        "keywords": [
            "Business process management, Process change management, Life cycle management, Adoption, Process innovation, Digital innovation, Digital transformation, Survey, Expert panel"
        ],
        "abstract": "The information revolution leaves its mark on businesses, resulting in organizations looking for digital innovation (DI) to apply to their business processes and anticipate competitors. Since the interplay between business process management (BPM) and DI has been underdeveloped, this mixed-methods article investigates the strength and nature of the relationship. We supplement the findings of an international survey (stage 1) with explanations from an expert panel (stage 2) to generalize a positive yet moderate link because of manifold contextual factors affecting strategic decision-making. We extend the technology\u2013organization\u2013environment (TOE) framework and profile organizations along their digital process innovation (DPI) mastery in a readiness matrix.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,147"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822144-0",
        "journal": null,
        "publisher": "Chandos Publishing",
        "title": "1futuredirectionsindigitalinformationscenariosandthemes",
        "booktitle": "Future Directions in Digital Information",
        "doi": "10.1016/B978-0-12-822144-0.00001-X",
        "author": [
            "Baker, David",
            "Ellis, Lucy"
        ],
        "keywords": [
            "COVID-19, Public memory, Digital disruption, Delphi study, Digital education and training, Service innovation, Digital-first, Harvester paradigm, Digital literacy, Digital inclusiveness"
        ],
        "abstract": "This chapter introduces the purpose and aims of the book and provides an overview of each of the 19 chapters in terms of methodological approach and what they tell us about the future of digital information. Subheadings indicate the major areas of discussion that emerge from the whole and which reflect the issues of our time that occupy the minds and principles of scholars and practitioners. The chapter takes a look at the source of trends in digital information access and provision and considers the current issues and contexts for service innovation. The discussion of the key themes is reinforced and augmented by the results of the Delphi exercise and by selected thought pieces both of which are presented in text boxes.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "09507051",
        "isbn": null,
        "journal": "Knowledge-Based Systems",
        "publisher": null,
        "title": "imageclassificationwithdeeplearninginthepresenceofnoisylabelsasurvey",
        "booktitle": null,
        "doi": "10.1016/j.knosys.2021.106771",
        "author": [
            "Algan, G\u00f6rkem",
            "Ulusoy, Ilkay"
        ],
        "keywords": [
            "Deep learning, Label noise, Classification with noise, Noise robust, Noise tolerant"
        ],
        "abstract": "Image classification systems recently made a giant leap with the advancement of deep neural networks. However, these systems require an excessive amount of labeled data to be adequately trained. Gathering a correctly annotated dataset is not always feasible due to several factors, such as the expensiveness of the labeling process or difficulty of correctly classifying data, even for the experts. Because of these practical challenges, label noise is a common problem in real-world datasets, and numerous methods to train deep neural networks with label noise are proposed in the literature. Although deep neural networks are known to be relatively robust to label noise, their tendency to overfit data makes them vulnerable to memorizing even random noise. Therefore, it is crucial to consider the existence of label noise and develop counter algorithms to fade away its adverse effects to train deep neural networks efficiently. Even though an extensive survey of machine learning techniques under label noise exists, the literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels. This paper aims to present these algorithms while categorizing them into one of the two subgroups: noise model based and noise model free methods. Algorithms in the first group aim to estimate the noise structure and use this information to avoid the adverse effects of noisy labels. Differently, methods in the second group try to come up with inherently noise robust algorithms by using approaches like robust losses, regularizers or other learning paradigms.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.038",
        "scimago_value": "1,587"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820951-6",
        "journal": null,
        "publisher": "Academic Press",
        "title": "index",
        "booktitle": "Twin and Family Studies of Epigenetics",
        "doi": "10.1016/B978-0-12-820951-6.09992-0",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00928674",
        "isbn": null,
        "journal": "Cell",
        "publisher": null,
        "title": "humanpopulationhistoryatthecrossroadsofeastandsoutheastasiasince11000yearsago",
        "booktitle": null,
        "doi": "10.1016/j.cell.2021.05.018",
        "author": [
            "Wang, Tianyi",
            "Wang, Wei",
            "Xie, Guangmao",
            "Li, Zhen",
            "Fan, Xuechun",
            "Yang, Qingping",
            "Wu, Xichao",
            "Cao, Peng",
            "Liu, Yichen",
            "Yang, Ruowei",
            "Liu, Feng",
            "Dai, Qingyan",
            "Feng, Xiaotian",
            "Wu, Xiaohong",
            "Qin, Ling",
            "Li, Fajun",
            "Ping, Wanjing",
            "Zhang, Lizhao",
            "Zhang, Ming",
            "Liu, Yalin",
            "Chen, Xiaoshan",
            "Zhang, Dongju",
            "Zhou, Zhenyu",
            "Wu, Yun",
            "Shafiey, Hassan",
            "Gao, Xing",
            "Curnoe, Darren",
            "Mao, Xiaowei",
            "Bennett, E.",
            "Ji, Xueping",
            "Yang, Melinda",
            "Fu, Qiaomei"
        ],
        "keywords": [
            "ancient DNA, 12,000-year-old humans, deeply diverged ancestry, pre-farming, cross-interactions, admixture"
        ],
        "abstract": "Summary Past human genetic diversity and migration between southern China and Southeast Asia have not been well characterized, in part due to poor preservation of ancient DNA in hot and humid regions. We sequenced 31 ancient genomes from southern China (Guangxi and Fujian), including two \u223c12,000- to 10,000-year-old individuals representing the oldest humans sequenced from southern China. We discovered a deeply diverged East Asian ancestry in the Guangxi region that persisted until at least 6,000 years ago. We found that \u223c9,000- to 6,000-year-old Guangxi populations were a mixture of local ancestry, southern ancestry previously sampled in Fujian, and deep Asian ancestry related to Southeast Asian H\u00f2ab\u00ecnhian hunter-gatherers, showing broad admixture in the region predating the appearance of farming. Historical Guangxi populations dating to \u223c1,500 to 500 years ago are closely related to Tai-Kadai and Hmong-Mien speakers. Our results show heavy interactions among three distinct ancestries at the crossroads of East and Southeast Asia.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "41.582",
        "scimago_value": "26,304"
    },
    {
        "issnkey": "00928674",
        "isbn": null,
        "journal": "Cell",
        "publisher": null,
        "title": "invivostructuralcharacterizationofthesarscov2rnagenomeidentifieshostproteinsvulnerabletorepurposeddrugs",
        "booktitle": null,
        "doi": "10.1016/j.cell.2021.02.008",
        "author": [
            "Sun, Lei",
            "Li, Pan",
            "Ju, Xiaohui",
            "Rao, Jian",
            "Huang, Wenze",
            "Ren, Lili",
            "Zhang, Shaojun",
            "Xiong, Tuanlin",
            "Xu, Kui",
            "Zhou, Xiaolin",
            "Gong, Mingli",
            "Miska, Eric",
            "Ding, Qiang",
            "Wang, Jianwei",
            "Zhang, Qiangfeng"
        ],
        "keywords": [
            "SARS-CoV-2, RNA secondary structure, host factor, RBP binding prediction, drug reproposing"
        ],
        "abstract": "Summary Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the cause of the ongoing coronavirus disease 2019 (COVID-19) pandemic. Understanding of the RNA virus and its interactions with host proteins could improve therapeutic interventions for COVID-19. By using icSHAPE, we determined the structural landscape of SARS-CoV-2 RNA in infected human cells and from refolded RNAs, as well as the regulatory untranslated regions of SARS-CoV-2 and six other coronaviruses. We validated several structural elements predicted in silico and discovered structural features that affect the translation and abundance of subgenomic viral RNAs in cells. The structural data informed a deep-learning tool to predict 42 host proteins that bind to SARS-CoV-2 RNA. Strikingly, antisense oligonucleotides targeting the structural elements and FDA-approved drugs inhibiting the SARS-CoV-2 RNA binding proteins dramatically reduced SARS-CoV-2 infection in cells derived from human liver and lung tumors. Our findings thus shed light on coronavirus and reveal multiple candidate therapeutics for COVID-19 treatment.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "41.582",
        "scimago_value": "26,304"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-824090-8",
        "journal": null,
        "publisher": "Elsevier",
        "title": "11processmonitoringoflaserpowderbedfusion",
        "booktitle": "Fundamentals of Laser Powder Bed Fusion of Metals",
        "doi": "10.1016/B978-0-12-824090-8.00012-3",
        "author": [
            "Grasso, Marco",
            "Colosimo, Bianca",
            "Slattery, Kevin",
            "MacDonald, Eric"
        ],
        "keywords": [
            "Anomaly detection, In-situ inspection, In-situ monitoring, Process control, Process signatures"
        ],
        "abstract": "The layerwise production paradigm entailed by the Laser Powder Bed Fusion process makes potentially available a large amount of information on a layer-by-layer basis, to determine the stability of the process and anticipate quality inspections of the part while it is being produced. Such information can be gathered through a variety of sensors used in-situ and in-line, ranging from pyrometers to high spatial and/or temporal resolution cameras or acoustic emission sensors. This chapter provides an overview of the quantities that can be acquired as signatures of the process and proxies of the final quality of the part, the methods suitable to make sense of acquired signals to detect anomalies, and process control solutions for defect mitigation, correction, or avoidance. The chapter also provides an up-to-date perspective on the current open issues for a wider industrial adoption of these techniques and most promising future research directions.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01641212",
        "isbn": null,
        "journal": "Journal of Systems and Software",
        "publisher": null,
        "title": "systemqualityandsecuritycertificationinsevenweeksamulticasestudyinspanishsmes",
        "booktitle": null,
        "doi": "10.1016/j.jss.2021.110960",
        "author": [
            "Gaitero, Domingo",
            "Genero, Marcela",
            "Piattini, Mario"
        ],
        "keywords": [
            "SME, Quality management system, Security management system, ISO 9001, ISO/IEC 27001, Multi-case study"
        ],
        "abstract": "Every company wishes to improve its system quality and security, all the more so in these times of digital transformation, since having a good quality and security management system is essential to any company\u2019s commercial survival. Such needs are even more pressing for small and medium-sized enterprises (SMEs), given their limited time and resources. To address these needs, a Spanish company, Proceso Social, has developed an innovative method called \u201cSevenWeeks\u201d to allow SMEs to create or improve their quality and security management systems in just seven weeks, with a view to obtaining one or both of the ISO 9001 and ISO/IEC 27001 certifications. We have evaluated the effectiveness and usefulness of SevenWeeks by carrying out a multi-case study of 26 Spanish companies, based on independent sources of evidence. This allowed us to corroborate that SevenWeeks was indeed effective for and perceived as useful by all the companies, as it enabled them to create their own quality and security management systems in only seven weeks and to obtain the necessary ISO certification. The interviewees found SevenWeeks to be an agile and intuitive method, easy to implement, which reduces costs and effort. We also include some recommendations to improve and further develop the method.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.829",
        "scimago_value": "0,642"
    },
    {
        "issnkey": "15662535",
        "isbn": null,
        "journal": "Information Fusion",
        "publisher": null,
        "title": "asurveyondeeplearninginmedicinewhyhowandwhen",
        "booktitle": null,
        "doi": "10.1016/j.inffus.2020.09.006",
        "author": [
            "Piccialli, Francesco",
            "Somma, Vittorio",
            "Giampaolo, Fabio",
            "Cuomo, Salvatore",
            "Fortino, Giancarlo"
        ],
        "keywords": [
            "Deep learning, Medicine, Artificial intelligence, Data science, Neural networks"
        ],
        "abstract": "New technologies are transforming medicine, and this revolution starts with data. Health data, clinical images, genome sequences, data on prescribed therapies and results obtained, data that each of us has helped to create. Although the first uses of artificial intelligence (AI) in medicine date back to the 1980s, it is only with the beginning of the new millennium that there has been an explosion of interest in this sector worldwide. We are therefore witnessing the exponential growth of health-related information with the result that traditional analysis techniques are not suitable for satisfactorily management of this vast amount of data. AI applications (especially Deep Learning), on the other hand, are naturally predisposed to cope with this explosion of data, as they always work better as the amount of training data increases, a phase necessary to build the optimal neural network for a given clinical problem. This paper proposes a comprehensive and in-depth study of Deep Learning methodologies and applications in medicine. An in-depth analysis of the literature is presented; how, where and why Deep Learning models are applied in medicine are discussed and reviewed. Finally, current challenges and future research directions are outlined and analysed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-819671-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "index",
        "booktitle": "Intelligent Environmental Data Monitoring for Pollution Management",
        "doi": "10.1016/B978-0-12-819671-7.09991-7",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03787788",
        "isbn": null,
        "journal": "Energy and Buildings",
        "publisher": null,
        "title": "changingtheapproachtoenergycomplianceinresidentialbuildingsreimaginingepcs",
        "booktitle": null,
        "doi": "10.1016/j.enbuild.2021.111239",
        "author": [
            "Jenkins, D.P.",
            [],
            "Patidar, S.",
            "McCallum, P."
        ],
        "keywords": [
            "EPC, Dynamic simulation, Urban energy modelling"
        ],
        "abstract": "As our need for energy information of buildings evolves, and the tools and methods at our disposal increase in scale and complexity, it is perhaps reasonable to expect a similar level of change in the way energy in buildings is assessed within national energy compliance frameworks. By comparing the available opportunities for building energy modelling with the current methodologies underlying Energy Performance Certificates, this study proposes future directions for standardised energy assessment of residential buildings and the impact this could have on different facets of energy policy. In carrying out this exercise, a number of criteria are proposed that could be used to appraise methodologies that align with future requirements of energy assessment, with two potential candidates for future energy assessment considered as part of this appraisal. An argument is thus proposed for better aligning future forms of standardised energy assessment with directions and requirements of future low-carbon energy policy.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.879",
        "scimago_value": "1,737"
    },
    {
        "issnkey": "03050483",
        "isbn": null,
        "journal": "Omega",
        "publisher": null,
        "title": "researchmanuscriptthebullwhipeffectinrulebasedsupplychainplanningsystemsacasebasedsimulationatahardgoodsretailer",
        "booktitle": null,
        "doi": "10.1016/j.omega.2019.102121",
        "author": [
            "Nguyen, Duy",
            "Adulyasak, Yossiri",
            "Landry, Sylvain"
        ],
        "keywords": [
            "Flowcasting, Distribution, Logistics, Bullwhip Effect, Simulation, Factor analysis, Regression analysis"
        ],
        "abstract": "The vision of a well-integrated supply chain (SC) was developed as early as 1958 by Forrester, who addressed what would eventually be called the Bullwhip Effect (BWE). The Flowcasting concept, originally called Retail Resource Planning, was proposed to connect all SC upper tiers to the storefront through fulfillment logic based on the Distribution Resource Planning (DRP) system. This method can therefore be understood as fully or SC-wide integrated DRP with a focus on the role of retailers instead of that of vendors or distributors. We studied a Canadian retailer that implemented Flowcasting in order to gain insight into the benefits and operational logic of this system. Based on the data obtained from the company, we simulate Flowcasting operations across a 3-tier SC compared to the Reorder Point (ROP) system, which was previously used at the firm, as well as a combination of ROP and DRP (ROP/DRP or partially integrated DRP), which are some of the most common implementations in use. The simulation is configured based on the company's settings, including historical average demand, demand estimates, lead time, etc. Then, multivariate regression is deployed to statistically compare the efficacy of these methods in SC management using various assessment criteria, including BWE measures. The results show that the requirement calculation logic used in an SC-wide integrated DRP system (Flowcasting) generally outperforms the other two approaches, and its benefits in curtailing the BWE become more noticeable in the upper tiers of the SC. This paper indicates the enormous potential of SC-wide integrated DRP logic in rule-based replenishment planning systems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,500"
    },
    {
        "issnkey": "09507051",
        "isbn": null,
        "journal": "Knowledge-Based Systems",
        "publisher": null,
        "title": "industrialtimeseriesdeterminativeanomalydetectionbasedonconstrainthypergraph",
        "booktitle": null,
        "doi": "10.1016/j.knosys.2021.107548",
        "author": [
            "Liang, Zheng",
            "Wang, Hongzhi",
            "Ding, Xiaoou",
            "Mu, Tianyu"
        ],
        "keywords": [
            "Industrial time series, Anomaly detection, Constraint hypergraph"
        ],
        "abstract": "The explosive growth of time series captured by sensors in industrial pipelines gives rise to the flourish of intelligent industry. Exploiting the value of these time series is conductive to workload balancing and production optimization. Unfortunately, knowledge obtained from the mining process turns out to be insufficient for use due to widespread anomalies, indicating machine breakdown, sensor failure or working status shifts. To tackle this problem, we propose a constraint hypergraph-based method, combining multiple constraints for anomaly detection. We develop strategies for adaptive determinative anomaly detection and anomaly pattern mining. We also investigate the problem of Anomaly Pattern Matching, prove its NP-completeness, and propose algorithms to obtain its global and local optimum. Finally, we demonstrate our approach with three real world datasets from a real powerplant, a chemical production pipeline and a hydraulic system. The experimental results show that our approach can effectively and efficiently work under different circumstances.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.038",
        "scimago_value": "1,587"
    },
    {
        "issnkey": "22119736",
        "isbn": null,
        "journal": "Tourism Management Perspectives",
        "publisher": null,
        "title": "anintelligenttraveltechnologyassessmentmodelfordestinationimpactsoftouristadoption",
        "booktitle": null,
        "doi": "10.1016/j.tmp.2021.100882",
        "author": [
            "Phaosathianphan, Noppadol",
            "Leelasantitham, Adisorn"
        ],
        "keywords": [
            "Technology Acceptance Model, IS Success, IS Continuance, Intelligent travel technology assessment model, Destination Impacts, Human-Computer Interaction, Socio-Technical System"
        ],
        "abstract": "As intelligent travel technology creates a business trend to support the travel and tourism industry, it is necessary to create a complete model to assess the users in terms of continuous user acceptance and destination impacts which are Competitiveness, Loyalty, and Sustainability with entire stakeholders (travellers, service providers, and destinations). The proposed conceptual model is formulated based on four studies: A Plenary Free Individual Traveler Life Cycle, IS Success, IS Continuance, and Destination Impacts of Travel and Tourism. The sample and data collections were done through the online questionnaire and survey via social media platforms such as Facebook and Line with individuals who have used intelligent personal assistants such as Google Assistant, Apple Siri, Microsoft Cortana, Amazon Alexa, and Samsung Bixby for travel and tourism. 400 respondents were analyzed with descriptive statistics and inferential statistics using the measurement model and the structural model by PASW Statistics and SmartPLS.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.586",
        "scimago_value": "1,454"
    },
    {
        "issnkey": "03783774",
        "isbn": null,
        "journal": "Agricultural Water Management",
        "publisher": null,
        "title": "updatedsingleanddualcropcoefficientsfortreeandvinefruitcrops",
        "booktitle": null,
        "doi": "10.1016/j.agwat.2020.106645",
        "author": [
            "Rallo, G.",
            "Pa\u00e7o, T.A.",
            "Paredes, P.",
            "Puig-Sirera, \u00c0.",
            "Massai, R.",
            "Provenzano, G.",
            "Pereira, L.S."
        ],
        "keywords": [
            "K and K values, Vineyards, Evergreen fruit trees, Deciduous fruit trees, Nut trees, Tropical fruit crops, Berries and hop"
        ],
        "abstract": "The present study reviews the research on the FAO56 crop coefficients of fruit trees and vines performed over the past twenty years. The main objective was to update information and extend tabulated single (Kc) and basal (Kcb) standard crop coefficients. The selection and analysis of the literature for this review have been done to consider only studies that adhere to FAO56 method, computing the reference ET with the FAO Penman\u2013Monteith\u200b ETo equation and field measuring crop ET with proved accuracy. The crops considered refer to vine fruit crops, berries and hops, temperate climate evergreen fruit trees, temperate climate deciduous fruit trees and, tropical and subtropical fruit crops. Papers satisfying the conditions expressed above, and that studied the crops under pristine or appropriate eustress conditions, were selected to provide for standard Kc and Kcb data. Preference was given to studies reporting on the fraction of ground cover (fc), crop height (h), planting density, crop age and adopted training systems. The Kc and Kcb values obtained from the selected literature generally show coherence relative to the crop biophysical characteristics and reflect those characteristics, mainly fc, h and training systems. The ranges of reported Kc and Kcb values were grouped according to crop density, particularly fc and h, and were compared with FAO56 (Allen et al., 1998) previously tabulated Kc and Kcb values, as well as by Allen and Pereira (2009) and Jensen and Allen (2016), which lead to define update indicative standard Kc and Kcb values. These values are aimed for use in crop water requirement computations and modeling for irrigation planning and scheduling, thus also aimed at supporting improved water use and saving in orchards and vines.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.516",
        "scimago_value": "1,493"
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "integratedframeworkofprocessminingandsimulationoptimizationforpodstructuredclinicallayoutdesign",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2021.115696",
        "author": [
            "Halawa, Farouq",
            "{Chalil Madathil}, Sreenath",
            "Khasawneh, Mohammad"
        ],
        "keywords": [
            "Process mining, Particle swarm optimization, Simulation-optimization, Facility layout, Healthcare"
        ],
        "abstract": "This paper proposes a three-phase framework to leverage hospital tracking data of patient visits while designing healthcare layouts with pod structures. The first phase proposes a process mining algorithm that modifies the Probabilistic Determining Finite Automata (PDFA) with Particle Swarm Optimization (PDFA-PSO) algorithm to predict the significant patient workflows from hospital historical data. The second phase employs simulation modeling to solve a right-sizing problem to determine the optimal size of the layout pods and the frequency of flows between the different clinical locations. The final phase uses an Unequal Area Facility Layout Problem (UAFLP) to determine the layout typology. The proposed process mining and simulation model are vital steps to measure the frequency between spaces and pod areas, which are needed to solve the UAFLP for outpatient settings. The proposed framework is validated using a case study for a renovation project of a large heart and vascular clinic in the US. The research shows that process mining is an efficient tool to extract a subset of significant patient pathways among 90 pathway variants and build a more realistic simulation that reflects behavioral and operational aspects. The research shows that the PSO algorithm is efficient in estimating the PDFA parameters and improving the prediction accuracy of the extracted patient pathways. In addition, the research shows that Genetic Algorithm with Placement Staretegy is an efficient algorithm for layout automation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "09265805",
        "isbn": null,
        "journal": "Automation in Construction",
        "publisher": null,
        "title": "abimdataminingintegrateddigitaltwinframeworkforadvancedprojectmanagement",
        "booktitle": null,
        "doi": "10.1016/j.autcon.2021.103564",
        "author": [
            "Pan, Yue",
            "Zhang, Limao"
        ],
        "keywords": [
            "Digital twin, Building information modeling (BIM), Process mining, Time series analysis"
        ],
        "abstract": "With the focus of smart construction project management, this paper presents a closed-loop digital twin framework under the integration of Building Information Modeling (BIM), Internet of Things (IoT), and data mining (DM) techniques. To be specific, IoT connects the physical and cyber world to capture real-time data for modeling and analyzing, and data mining methods incorporated in the virtual model aim to discover hidden knowledge in collected data. The proposed digital twin has been verified in a practical BIM-based project. Based on large inspection data from IoT devices, the 4D visualization and task-centered or worker-centered process model are built as the virtual model to simulate both the task execution and worker cooperation. Then, the high-fidelity virtual model is investigated by process mining and time series analysis. Results show that possible bottlenecks in the current process can be foreseen using the fuzzy miner, while the number of finished tasks in the next phase can be predicted by the multivariate autoregressive integrated moving average (ARIMAX) model. Consequently, tactic decision-making can realize to not only prevent possible failure in advance, but also arrange work and staffing reasonably to make the process adapt to changeable conditions. In short, the significance of this paper is to build a data-driven digital twin framework integrating with BIM, IoT, and data mining for advanced project management, which can facilitate data communication and exploration to better understand, predict, and optimize the physical construction operations. In future works, more complex cases with multiple data streams will be used to test the developed framework, and more detailed interpretations with the actual observations of construction activities will be given.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.700",
        "scimago_value": "1,837"
    },
    {
        "issnkey": "02684012",
        "isbn": null,
        "journal": "International Journal of Information Management",
        "publisher": null,
        "title": "predictingsubjectivewellbeingamongmhealthusersareadinessvaluemodel",
        "booktitle": null,
        "doi": "10.1016/j.ijinfomgt.2020.102247",
        "author": [
            "Aboelmaged, Mohamed",
            "Hashem, Gharib",
            "Mouakket, Samar"
        ],
        "keywords": [
            "mHealth, Subjective well-being, Utilitarian value, Hedonic value, Technology readiness, Post-adoption"
        ],
        "abstract": "mHealth applications (MHA) have recently attracted great attention from various stakeholders as they are indeed important means to enhance users\u2019 subjective well-being. While prior research has mainly focused on intention or adoption phase, little work has empirically examined the post-adoption effects of MHA with scarce attention given to the well-being outcome. Actual users are likely to conceive the values of MHA based mainly on their direct experience with it. In this paper, the dimensions of users\u2019 technology readiness are regarded as major impetuses for perceived utilitarian and hedonic values, which in turn influence subjective well-being among MHA users. The proposed readiness-value model is analyzed using survey data collected from 731 users of MHA. The findings show that the model significantly predicts users\u2019 subjective well-being considering that utilitarian value is more important for male users, whereas hedonic value has a more salient effect for female users. It also reveals that enablers of technology readiness (i.e., innovativeness and optimism) exert a stronger influence than that of inhibitors (i.e., discomfort and insecurity) on the perceived values of MHA. These results have essential implications for theory and practice.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "14.098",
        "scimago_value": "2,770"
    },
    {
        "issnkey": "13665545",
        "isbn": null,
        "journal": "Transportation Research Part E: Logistics and Transportation Review",
        "publisher": null,
        "title": "dataanalyticsforfuelconsumptionmanagementinmaritimetransportationstatusandperspectives",
        "booktitle": null,
        "doi": "10.1016/j.tre.2021.102489",
        "author": [
            "Yan, Ran",
            "Wang, Shuaian",
            "Psaraftis, Harilaos"
        ],
        "keywords": [
            "Maritime transportation, Ship fuel consumption prediction, Ship performance prediction, Ship energy efficiency optimization, Ship performance optimization"
        ],
        "abstract": "The shipping industry is associated with approximately three quarters of all world trade. In recent years, the sustainability of shipping has become a public concern, and various emissions control regulations to reduce pollutants and greenhouse gas (GHG) emissions from ships have been proposed and implemented globally. These regulations aim to drive the shipping industry in a low-carbon and low-pollutant direction by motivating it to switch to more efficient fuel types and reduce energy consumption. At the same time, the cyclical downturn of the world economy and high bunker prices make it necessary and urgent for the shipping industry to operate in a more cost-effective way while still satisfying global trade demand. As bunker fuel bunker (e.g., heavy fuel oil [HFO], liquified natural gas [LNG]) consumption is the main source of emissions and bunker fuel costs account for a large proportion of operating costs, shipping companies are making unprecedented efforts to optimize ship energy efficiency. It is widely accepted that the key to improving the energy efficiency of ships is the development of accurate models to predict ship fuel consumption rates under different scenarios. In this study, ship fuel consumption prediction models presented in the literature (including the academic literature and technical reports as a typical type of \u201cgrey literature\u201d) are reviewed and compared, and models that optimize ship operations based on fuel consumption prediction results are also presented and discussed. Current research challenges and promising research questions on ship performance monitoring and operational optimization are identified.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.875",
        "scimago_value": "2,042"
    },
    {
        "issnkey": "09507051",
        "isbn": null,
        "journal": "Knowledge-Based Systems",
        "publisher": null,
        "title": "customizingsvmasabaselearnerwithadaboostensembletolearnfrommulticlassproblemsahybridapproachadaboostmsvm",
        "booktitle": null,
        "doi": "10.1016/j.knosys.2021.106845",
        "author": [
            "Mehmood, Zafar",
            "Asghar, Sohail"
        ],
        "keywords": [
            "Machine learning classifiers, Class overlapping, Imbalanced distribution of data, Imbalanced problem, Decomposition techniques"
        ],
        "abstract": "Learning from a multi-class problem has not been an easy task for most of the classifiers, because of multiple issues. In the complex multi-class scenarios, samples of different classes overlap with each other by sharing attribute, and hence the visibility of least represented samples decrease even more. Learning from imbalanced data studied extensively in the research community, however, the overlapping issues and the co-occurrence impact of overlapping with data imbalance have received comparatively less attention, even though their joint impact is more thoughtful on classifiers\u2019 performance. In this paper, we introduce a modified SVM, MSVM to use as a base classifier with the AdaBoost ensemble classifier (MSVM-AdB) to enhance the learning capability of the ensemble classifier. To implement the proposed technique, we divide the multi-class dataset into overlapping and non-overlapping region. The overlapping region is further filter into the Critical and less Critical region depending upon their sample contribution in the overlapped region. The MSVM is designed to map the overlapped samples in a higher dimension by modifying the kernel mapping function of the standard SVM by using the mean distance of the Critical region samples. To highlight the learning enhancement of the MSVM-AdB, we use 20 real datasets with varying imbalance ratio and the overlapping degree to compare the significance of the AdaBoost-MSVM with the standard SVM, and AdaBoost with standard base classifiers. Experimental results show the superiority of the MSVM-AdB on a collection of benchmark datasets to its standard counterpart classifiers.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.038",
        "scimago_value": "1,587"
    },
    {
        "issnkey": "0740624x",
        "isbn": null,
        "journal": "Government Information Quarterly",
        "publisher": null,
        "title": "platformortechnologyprojectaspectrumofsixstrategicplaysfromukgovernmentitinitiativesandtheirimplicationsforpolicy",
        "booktitle": null,
        "doi": "10.1016/j.giq.2021.101628",
        "author": [
            "Thompson, Mark",
            "Venters, Will"
        ],
        "keywords": [
            "Platform innovation, Platform strategy, Government policy, Digital innovation, UK"
        ],
        "abstract": "There is a markedly broad range of definitions and illustrative examples of the role played by governments themselves within the literature on government platforms. In response we conduct an inductive and deductive qualitative review of the literature to clarify this landscape and so to develop a typology of six definitions of government platforms, organised within three genres along a spectrum from fully centralised, through to fully decentralised. For each platform definition we offer illustrative \u2018mini-cases\u2019 drawn from the UK government experience as well as further insights and implications for each genre, drawn from the broader information systems literature on platforms. A range of benefits, risks, governance challenges, policy recommendations, and suggestions for further research are then identified and discussed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15320464",
        "isbn": null,
        "journal": "Journal of Biomedical Informatics",
        "publisher": null,
        "title": "identificationofpediatricrespiratorydiseasesusingafinegraineddiagnosissystem",
        "booktitle": null,
        "doi": "10.1016/j.jbi.2021.103754",
        "author": [
            "Yu, Gang",
            "Yu, Zhongzhi",
            "Shi, Yemin",
            "Wang, Yingshuo",
            "Liu, Xiaoqing",
            "Li, Zheming",
            "Zhao, Yonggen",
            "Sun, Fenglei",
            "Yu, Yizhou",
            "Shu, Qiang"
        ],
        "keywords": [
            "Respiratory diseases, Fine-grained diagnosis, Pediatric diagnosis, Clinical notes, Multi-modal"
        ],
        "abstract": "Respiratory diseases, including asthma, bronchitis, pneumonia, and upper respiratory tract infection (RTI), are among the most common diseases in clinics. The similarities among the symptoms of these diseases precludes prompt diagnosis upon the patients\u2019 arrival. In pediatrics, the patients\u2019 limited ability in expressing their situation makes precise diagnosis even harder. This becomes worse in primary hospitals, where the lack of medical imaging devices and the doctors\u2019 limited experience further increase the difficulty of distinguishing among similar diseases. In this paper, a pediatric fine-grained diagnosis-assistant system is proposed to provide prompt and precise diagnosis using solely clinical notes upon admission, which would assist clinicians without changing the diagnostic process. The proposed system consists of two stages: a test result structuralization stage and a disease identification stage. The first stage structuralizes test results by extracting relevant numerical values from clinical notes, and the disease identification stage provides a diagnosis based on text-form clinical notes and the structured data obtained from the first stage. A novel deep learning algorithm was developed for the disease identification stage, where techniques including adaptive feature infusion and multi-modal attentive fusion were introduced to fuse structured and text data together. Clinical notes from over 12000 patients with respiratory diseases were used to train a deep learning model, and clinical notes from a non-overlapping set of about 1800 patients were used to evaluate the performance of the trained model. The average precisions (AP) for pneumonia, RTI, bronchitis and asthma are 0.878, 0.857, 0.714, and 0.825, respectively, achieving a mean AP (mAP) of 0.819. These results demonstrate that our proposed fine-grained diagnosis-assistant system provides precise identification of the diseases.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821226-4",
        "journal": null,
        "publisher": "Butterworth-Heinemann",
        "title": "index",
        "booktitle": "Engine Testing (Fifth Edition)",
        "doi": "10.1016/B978-0-12-821226-4.00026-7",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "0305750x",
        "isbn": null,
        "journal": "World Development",
        "publisher": null,
        "title": "mobilisingurbanknowledgeinaninfodemicurbanobservatoriessustainabledevelopmentandthecovid19crisis",
        "booktitle": null,
        "doi": "10.1016/j.worlddev.2020.105295",
        "author": [
            "Acuto, Michele",
            "Dickey, Ariana",
            "Butcher, Stephanie",
            "Washbourne, Carla-Leanne"
        ],
        "keywords": [
            "Urban observatories, Infodemic, Boundary-spanning, Knowledge translation, Science-policy interface"
        ],
        "abstract": "Along with disastrous health and economic implications, COVID-19 has also been an epidemic of misinformation and rumours - an \u2018infodemic\u2019. The desire for robust, evidence-based policymaking in this time of disruption has been at the heart of the multilateral response to the crisis, not least in terms of supporting a continuing agenda for global sustainable development. The role of boundary-spanning knowledge institutions in this context could be pivotal, not least in cities, where much of the pandemic has struck. \u2018Urban observatories\u2019 have emerged as an example of such institutions; harbouring great potential to produce and share knowledge supporting sustainable and equitable processes of recovery. Building on four \u2018live\u2019 case studies during the crisis of institutions based in Johannesburg, Karachi, Freetown and Bangalore, our research note aims to capture the role of these institutions, and what it means to span knowledge boundaries in the current crisis. We do so with an eye towards a better understanding of their knowledge mobilisation practices in contributing towards sustainable urban development. We highlight that the crisis offers a key window for urban observatories to play a progressive and effective role for sustainable and inclusive development. However, we also underline continuing challenges in these boundary knowledge dynamics: including issues of institutional trust, inequality of voices, collective memory, and the balance between normative and advisory roles for observatories.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13891286",
        "isbn": null,
        "journal": "Computer Networks",
        "publisher": null,
        "title": "asurveyonwearabletechnologyhistorystateoftheartandcurrentchallenges",
        "booktitle": null,
        "doi": "10.1016/j.comnet.2021.108074",
        "author": [
            "Ometov, Aleksandr",
            "Shubina, Viktoriia",
            "Klus, Lucie",
            "Skibi\u0144ska, Justyna",
            "Saafi, Salwa",
            "Pascacio, Pavel",
            "Flueratoru, Laura",
            "Gaibor, Darwin",
            "Chukhno, Nadezhda",
            "Chukhno, Olga",
            "Ali, Asad",
            "Channa, Asma",
            "Svertoka, Ekaterina",
            "Qaim, Waleed",
            "Casanova-Marqu\u00e9s, Ra\u00fal",
            "Holcer, Sylvia",
            "Torres-Sospedra, Joaqu\u00edn",
            "Casteleyn, Sven",
            "Ruggeri, Giuseppe",
            "Araniti, Giuseppe",
            "Burget, Radim",
            "Hosek, Jiri",
            "Lohan, Elena"
        ],
        "keywords": [
            "Wearables, Communications, Standardization, Privacy, Security, Data processing, Interoperability, User adoption, Localization, Classification, Future perspective"
        ],
        "abstract": "Technology is continually undergoing a constituent development caused by the appearance of billions new interconnected \u201cthings\u201d and their entrenchment in our daily lives. One of the underlying versatile technologies, namely wearables, is able to capture rich contextual information produced by such devices and use it to deliver a legitimately personalized experience. The main aim of this paper is to shed light on the history of wearable devices and provide a state-of-the-art review on the wearable market. Moreover, the paper provides an extensive and diverse classification of wearables, based on various factors, a discussion on wireless communication technologies, architectures, data processing aspects, and market status, as well as a variety of other actual information on wearable technology. Finally, the survey highlights the critical challenges and existing/future solutions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.474",
        "scimago_value": "0,798"
    },
    {
        "issnkey": "02673649",
        "isbn": null,
        "journal": "Computer Law & Security Review",
        "publisher": null,
        "title": "digitalevidenceunaddressedthreatstofairnessandthepresumptionofinnocence",
        "booktitle": null,
        "doi": "10.1016/j.clsr.2021.105575",
        "author": [
            "Stoykova, Radina"
        ],
        "keywords": [
            "Presumption of innocence, Digital evidence, Reliability, Digital forensics, Fair trial"
        ],
        "abstract": "Contemporary criminal investigation assisted by computing technology imposes challenges to the right to a fair trial and the scientific validity of digital evidence. This paper identifies three categories of unaddressed threats to fairness and the presumption of innocence during investigations \u2013 (i) the inappropriate and inconsistent use of technology; (ii) old procedural guarantees, which are not adapted to contemporary digital evidence processes and services; (iii) and the lack of reliability testing in digital forensics practice. Further, the solutions that have been suggested to overcome these issues are critically reviewed to identify their shortcomings. Ultimately, the paper argues for the need of legislative intervention and enforcement of standards and validation procedures for digital evidence in order to protect innocent suspects and all parties in the criminal proceedings from the negative consequences of technology-assisted investigations.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,815"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821187-8",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapterninepredictionandclassificationofdiabetesmellitususinggenomicdata",
        "booktitle": "Intelligent IoT Systems in Personalized Health Care",
        "doi": "10.1016/B978-0-12-821187-8.00009-5",
        "author": [
            "Awotunde, Joseph",
            "Ayo, Femi",
            "Jimoh, Rasheed",
            "Ogundokun, Roseline",
            "Matiluko, Opeyemi",
            "Oladipo, Idowu",
            "Abdulraheem, Muyideen"
        ],
        "keywords": [
            "Genomic data, Diabetes mellitus, Classification, Genetic algorithm, Deep neural networks"
        ],
        "abstract": "Diabetes mellitus (DM) is one of the chronic and debilitating diseases in modern society, hence the urgent need to prevent epidemic growth in society. This chapter is motivated by the studies of several scholars in the field of microarrays datasets for gene expression. Nonetheless, there are very few available gene signatures across datasets, thereby generating sample selection bias and over selection sets matching. Subjective selection of this gene and sample pairings could be addressed through large average submatrices and a unique method of biclustering using objective statistical assumptions to reconstruct robust signatures of expression. Hence, SWITCH (SupWald Identification of CHanges DNA copy) was created to label CNAs in platforms of aCGH and to connect them with subtypes. Therefore, the process of selecting the most informative gene biomarker was done using the genetic algorithm (GA) and deep neural networks (DNN) for biological sample classification. The simulated genomics datasets were divided into 95% training and 5% test samples and the DNN classifier is modified using these sets of SNPs and fine-tuned to classify type II DM analyses. The datasets are cleaned into four single-nucleotide polymorphism (SNP) function sets: 96 (P-value: 1\u00d710\u22125), 214 (P-value: 1\u00d710\u22124), 399 (P-value: 1\u00d710\u22123), and 678 (P-value: 1\u00d710\u22122) using P-value thresholds. The classifier was built using the training data while testing its efficiency on the test sample. MATLAB was used to implement the GA and DNN. The DNN model showed a significant predictive output with type II DM having AUC=0.9537 in male and AUC=0.9349 in female. An experimental test was carried out to determine the associations of all SNP datasets with the type II diabetes phenotype. The results from the test showed an enhanced model performance with 399 and 678 SNPs, respectively. The test result also showed that the higher the number of SNPs the better the predictive performance of the designed model. Finally, the result showed that DNNs can be used to predict type II diabetes using genomic-based data.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10848045",
        "isbn": null,
        "journal": "Journal of Network and Computer Applications",
        "publisher": null,
        "title": "aselectiveensemblemodelforcognitivecybersecurityanalysis",
        "booktitle": null,
        "doi": "10.1016/j.jnca.2021.103210",
        "author": [
            "Jiang, Yuning",
            "Atif, Yacine"
        ],
        "keywords": [
            "Information security, Vulnerability analysis, Data correlation, Machine learning, Ensemble, Data mining, Database management"
        ],
        "abstract": "Dynamic data-driven vulnerability assessments face massive heterogeneous data contained in, and produced by SOCs (Security Operations Centres). Manual vulnerability assessment practices result in inaccurate data and induce complex analytical reasoning. Contemporary security repositories\u2019 diversity, incompleteness and redundancy contribute to such security concerns. These issues are typical characteristics of public and manufacturer vulnerability reports, which exacerbate direct analysis to root out security deficiencies. Recent advances in machine learning techniques promise novel approaches to overcome these notorious diversity and incompleteness issues across massively increasing vulnerability reports corpora. Yet, these techniques themselves exhibit varying degrees of performance as a result of their diverse methods. We propose a cognitive cybersecurity approach that empowers human cognitive capital along two dimensions. We first resolve conflicting vulnerability reports and preprocess embedded security indicators into reliable data sets. Then, we use these data sets as a base for our proposed ensemble meta-classifier methods that fuse machine learning techniques to improve the predictive accuracy over individual machine learning algorithms. The application and implication of this methodology in the context of vulnerability analysis of computer systems are yet to unfold the full extent of its potential. The proposed cognitive security methodology in this paper is shown to improve performances when addressing the above-mentioned incompleteness and diversity issues across cybersecurity alert repositories. The experimental analysis conducted on actual cybersecurity data sources reveals interesting tradeoffs of our proposed selective ensemble methodology, to infer patterns of computer system vulnerabilities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00063207",
        "isbn": null,
        "journal": "Biological Conservation",
        "publisher": null,
        "title": "survivalandcausespecificmortalityofeuropeanwildcatfelissilvestrisacrosseurope",
        "booktitle": null,
        "doi": "10.1016/j.biocon.2021.109239",
        "author": [
            "Bastianelli, Matteo",
            "Premier, Joseph",
            "Herrmann, Mathias",
            "Anile, Stefano",
            "Monterroso, Pedro",
            "Kuemmerle, Tobias",
            "Dormann, Carsten",
            "Streif, Sabrina",
            "Jerosch, Saskia",
            "G\u00f6tz, Malte",
            "Simon, Olaf",
            "Mole\u00f3n, Marcos",
            "Gil-S\u00e1nchez, Jos\u00e9",
            "Bir\u00f3, Zsolt",
            "Dekker, Jasja",
            "Severon, Analena",
            "Krannich, Axel",
            "Hupe, Karsten",
            "Germain, Estelle",
            "Pontier, Dominique",
            "Janssen, Ren\u00e9",
            "Ferreras, Pablo",
            "D\u00edaz-Ruiz, Francisco",
            "L\u00f3pez-Mart\u00edn, Jos\u00e9",
            "Urra, Ferm\u00edn",
            "Bizzarri, Lolita",
            "Bertos-Mart\u00edn, Elena",
            "Dietz, Markus",
            "Trinzen, Manfred",
            "Ballesteros-Duper\u00f3n, Elena",
            "Barea-Azc\u00f3n, Jos\u00e9",
            "Sforzi, Andrea",
            "Poulle, Marie-Lazarine",
            "Heurich, Marco"
        ],
        "keywords": [
            "Anthropogenic landscapes, European wildcat, Survival, Human-caused mortality, Roadkill, Road density"
        ],
        "abstract": "Humans have transformed most landscapes across the globe, forcing other species to adapt in order to persist in increasingly anthropogenic landscapes. Wide-ranging solitary species, such as wild felids, struggle particularly in such landscapes. Conservation planning and management for their long-term persistence critically depends on understanding what determine survival and what are the main mortality risks. We carried out the first study on annual survival and cause-specific mortality of the European wildcat with a large and unique dataset of 211 tracked individuals from 22 study areas across Europe. Furthermore, we tested the effect of environmental and human disturbance variables on the survival probability. Our results show that mortalities were mainly human-caused, with roadkill and poaching representing 57% and 22% of the total annual mortality, respectively. The annual survival probability of wildcat was 0.92 (95% CI = 0.87\u20130.98) for females and 0.84 (95% CI = 0.75\u20130.94) for males. Road density strongly impacted wildcat annual survival, whereby an increase in the road density of motorways and primary roads by 1 km/km2 in wildcat home-ranges increased mortality risk ninefold. Low-traffic roads, such as secondary and tertiary roads, did not significantly affect wildcat's annual survival. Our results deliver key input parameters for population viability analyses, provide planning-relevant information to maintain subcritical road densities in key wildcat habitats, and identify conditions under which wildcat-proof fences and wildlife crossing structures should be installed to decrease wildcat mortality.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.990",
        "scimago_value": "2,227"
    },
    {
        "issnkey": "25426605",
        "isbn": null,
        "journal": "Internet of Things",
        "publisher": null,
        "title": "datamanagementandinternetofthingsamethodologicalreviewinsmartfarming",
        "booktitle": null,
        "doi": "10.1016/j.iot.2021.100378",
        "author": [
            "Debauche, Olivier",
            "Trani, Jean-Philippe",
            "Mahmoudi, Sa\u00efd",
            "Manneback, Pierre",
            "Bindelle, J\u00e9r\u00f4me",
            "Mahmoudi, Sidi",
            "Guttadauria, Adriano",
            "Lebeau, Fr\u00e9d\u00e9ric"
        ],
        "keywords": [
            "Internet of things, Network protocols, Technological selection methodology, Cloud architecture, Security, Smart farming"
        ],
        "abstract": "Introduction. In the field of research, we are familiar to employ ready-to-use commercial solutions. This bibliographic review highlights the various technological paths that can be used in the context of agriculture digitalization and illuminates the reader on the capacities and limits of each one. Literature. Based on a literature review that we conducted, we describe the main components of the Internet of Things. Also, we analyzed the different technological pathways used by researchers to develop their projects. Finally, these versatile approaches are summarized in the form of tables and a methodological flowchart of communication protocols choices. Conclusions. In this article, we propose a methodology and a reflection on the technological choices and their implication on the durability and valorization of research projects in the field of smart agriculture.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "25895370",
        "isbn": null,
        "journal": "EClinicalMedicine",
        "publisher": null,
        "title": "prevalenceandinterventionofpreoperativeanemiainchineseadultsaretrospectivecrosssectionalstudybasedonnationalpreoperativeanemiadatabase",
        "booktitle": null,
        "doi": "10.1016/j.eclinm.2021.100894",
        "author": [
            "Lin, Jie",
            "Wang, Chao",
            "Liu, Junting",
            "Yu, Yang",
            "Wang, Shufang",
            "Wen, Aiqing",
            "Wu, Jufeng",
            "Zhang, Long",
            "Sun, Futing",
            "Guo, Xiaojun",
            "Liu, Fenghua",
            "Li, Hailan",
            "Li, Na",
            "Wang, Haibao",
            "Lv, Yi",
            "Jia, Zhonghua",
            "Li, Xiaoyan",
            "Zhang, Jun",
            "Li, Zunyan",
            "Liu, Shanshan",
            "Zhong, Shuhuai",
            "Yang, Jun",
            "Ma, Shuxuan",
            "Zhou, Lingling",
            "Guan, Xiaozhen",
            "Ma, Chunya",
            "Cheng, Shijun",
            "Chen, Shengxiong",
            "Xu, Zhenhua",
            "Li, Gang",
            "Wang, Deqing"
        ],
        "keywords": [
            "Preoperative anemia, Transfusion, Iron, Erythropoietin"
        ],
        "abstract": "Background Preoperative anemia is an important pillar of perioperative patient blood management. However, there was no literature comprehensively described the current situation of preoperative anemia in China. Methods We conducted a national retrospective cross-sectional study to assess the prevalence and intervention of preoperative anemia in Chinese adults. Data were from the National Preoperative Anemia Database based on hospital administration data from January 1, 2013 to December 31, 2018. Findings A total of 797,002 patients were included for analysis. Overall, 27.57% (95% CI 27.47\u201327.67) of patients had preoperative anemia, which varied by gender, age, regions, and type of operation. Patients who were female, age over 60 years old, from South China, from provinces with lower per capita GDP, underwent operations on the lymphatic and hematopoietic system, with laboratory abnormalities were more likely to have a high risk of preoperative anemia. Among patients with preoperative anemia, 5.16% (95% CI 5.07\u20135.26) received red blood cell transfusion, 7.79% (95% CI 7.67\u20137.91) received anemia-related medications such as iron, erythropoietin, folic acid or vitamin B12, and 12.25% (95% CI 12.10\u201312.40) received anemia-related therapy (red blood cell transfusion or anemia-related medications) before operation. The probability of preoperative RBC transfusion decreased by 54.92% (OR 0.46, 95% CI 0.46\u20130.47) as each 10-g/L increase in preoperative hemoglobin. Patients with preoperative hemoglobin less than 130 g/L was associated with longer hospital stay and more hospital costs. Patients with severe preoperative anemia given iron preoperatively had lower intra/post-operative RBC transfusion rate, shorter length of stay and less hospitalization costs, but no similar correlation was found in patients with mild and moderate preoperative anemia and patients given erythropoietin preoperatively. Interpretation Our present study shows that preoperative anemia is currently a relatively prevalent problem that has not been fully appreciated in China. More researches will be required to optimize the treatment of preoperative anemia. Funding National Natural Science Foundation of China and the Logistics Support Department of the Central Military Commission.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,915"
    },
    {
        "issnkey": "02632241",
        "isbn": null,
        "journal": "Measurement",
        "publisher": null,
        "title": "aprobabilisticsequenceclassificationapproachforearlyfaultpredictionindistributiongridsusinglongshorttermmemoryneuralnetworks",
        "booktitle": null,
        "doi": "10.1016/j.measurement.2020.108691",
        "author": [
            "Skydt, Mathis",
            "Bang, Mads",
            "Shaker, Hamid"
        ],
        "keywords": [
            "Fault prediction, Predictive maintenance, Grid management, Risk assessment, Neural networks, LSTM"
        ],
        "abstract": "As the global power grid must undergo a profound transformation in the coming decades to ensure reliable and cost-effective operation in a system with large shares of intermittent renewable energy generation, a critical element will be to leverage advanced data-driven predictive tools to optimise grid management activities. As it is expected that existing grids will be operated more to their limits, it is important to obtain better operational insights and estimations of the time to equipment failure to provide useful operational guidance and maintenance prioritisation support for grid operators. In this regard, this paper proposes a novel and real-time applicable method for fault prediction in 10 kV underground oil-insulated power cables using low-resolution data from a real case study from a Danish distribution system operator. The developed method is based on a sequence classification approach using long short-term memory neural networks where three different operational states are defined (Normal, Early warning, and Critical warning) to allow for prediction flexibility and better indication of the presence of systemic faults. Moreover, to enhance the data foundation, this paper investigates a Virtual Sample Generation method based on an adaptive Gaussian distribution. The capability of the proposed method yields satisfying results with prediction accuracy on the test set reaching as high as ~90%, hence proving the usefulness of the proposed approach and paving the way for smarter maintenance protocols.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,772"
    },
    {
        "issnkey": "09575820",
        "isbn": null,
        "journal": "Process Safety and Environmental Protection",
        "publisher": null,
        "title": "fielddataanalysisandriskassessmentofgaskickduringindustrialdeepwaterdrillingprocessbasedonsupervisedlearningalgorithm",
        "booktitle": null,
        "doi": "10.1016/j.psep.2020.08.012",
        "author": [
            "Yin, Qishuai",
            "Yang, Jin",
            "Tyagi, Mayank",
            "Zhou, Xu",
            "Hou, Xinxin",
            "Cao, Bohan"
        ],
        "keywords": [
            "Industrial deep-water drilling, Gas kick, Field data analysis, Risk assessment, Early gas, Kick detection, Supervised learning"
        ],
        "abstract": "During industrial offshore deep-water drilling process, gas kick event occurs frequently due to extremely narrow Mud Weight (MW) window (minimum 0.01sg) and negligible safety margins for the well control purposes. Further, traditional gas kick detection methods in such environments have significant time-lag and can often lead to severe well control issues, and occasionally to well blowouts or borehole abandonment. In this study, firstly, the raw field data is processed through data collection, data cleaning, feature scaling, outlier detection, data labeling and dataset splitting. Additionally, a novel data labeling criterion for gas kick risks is proposed where five kick risks (Indicated by different colors in this study) are defined based on three key indicators: differential flow out (DFO), kick gain volume (Vol), and kick duration time (Time). Kick risk status represents one of the following cases: Case 0 - No indicators are activated (Green), Case 1 - Multi-drilling parameters deviation or DFO is activated (Orange), Case 2 - DFO and Vol are simultaneously activated (Light Red), Case 3 - DFO and Time are simultaneously activated (Light Red), Case 4 - DFO, Vol and Time alarms are simultaneously activated (Dark Red). Then, a novel data mining method using Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) is presented for early detection of gas kick events by analyzing time series data from field drilling process. The network parameters such as number of hidden layers and number of neurons are initialized to build the LSTM network. The learned LSTM model is evaluated using the testing set, and the best LSTM model (six (6)-layers eighty (80)-nodes (6 L*80 N)) is optimally selected and deployed. The accuracy of deployed LSTM model is 87 % in the testing dataset, which is reliable enough to identify the kick fault during the deep-water drilling field operation. Lastly, the LSTM model detected the gas kick events earlier than the \u201cTank Volume\u201d detection method in several representative case studies to conclude that the application of LSTM model can potentially improve well control safety in the deep-water wells with narrow MW windows.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01652370",
        "isbn": null,
        "journal": "Journal of Analytical and Applied Pyrolysis",
        "publisher": null,
        "title": "estimationoflignocellulosicbiomasspyrolysisproductyieldsusingartificialneuralnetworks",
        "booktitle": null,
        "doi": "10.1016/j.jaap.2021.105180",
        "author": [
            "Tsekos, C.",
            "Tandurella, S.",
            "{de Jong}, W."
        ],
        "keywords": [
            "Pyrolysis, Artificial neural networks, Biomass modelling"
        ],
        "abstract": "As the push towards more sustainable ways to produce energy and chemicals intensifies, efforts are needed to refine and optimize the systems that can give an answer to these needs. In the present work, the use of neural networks as modelling tools for lignocellulosic biomass pyrolysis main products yields estimation was evaluated. In order to achieve this, the most relevant compositional and reaction parameters for lignocellulosic biomass pyrolysis were reviewed and their effect over the main products yields was assessed. Based on relevant literature data, a database was set up, containing parameters and experimental results from 32 published studies for a total of 482 samples, including both fast and slow pyrolysis experiments performed on a heterogeneous collection of lignocellulosic biomasses. The parameters that in the database configured as best predictors for the solid, liquid and gaseous products were determined through preliminary tests and were then used to build reduced models, one for each of the main products, which use five parameters instead of the full set for the estimation of yields. The procedures included hyperparameter optimizations steps. The performances of these reduced models were compared to those of the ones obtained using the full set of parameters as inputs by using the root mean squared error (RMSE) as metric. For both the char and gas products, the best results were consistently achieved by the reduced versions of the network (RMSE 5.1 wt% ar and 5.6 wt% ar respectively), while for the liquid product the best result was given by the full network (RMSE 6.9 wt% ar) indicating substantial value in proper selection of the input features. In general, the char models were the best performing ones. Additional models for the liquid and gas product featuring char as additional input to the system were also devised and obtained better performance (RMSE 5.5 wt% ar and 4.9 wt% ar respectively) compared to the original ones. Models based on single studies were also included in order to showcase both the capabilities of the tool and the challenges that arise when trying to build a generalizable model of this kind. Overall, artificial neural networks were shown to be an interesting tool for the construction of setup-unspecific biomass pyrolysis product yield models. The obstacles standing currently in the way of a more accurate modelling of the system were highlighted, along with certain literature discrepancies, which hinder reliable quantitative comparison of experimental conditions and results among separate studies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.541",
        "scimago_value": "1,186"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820045-2",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter9imageanalysisindrugdiscovery",
        "booktitle": "The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry",
        "doi": "10.1016/B978-0-12-820045-2.00010-6",
        "author": [
            "Corrigan, Adam",
            "Sutton, Daniel",
            "Zimmermann, Johannes",
            "Dillon, Laura",
            "Bera, Kaustav",
            "Meier, Armin",
            "Cecchi, Fabiola",
            "Madabhushi, Anant",
            "Schmidt, G\u00fcnter",
            "Hipp, Jason"
        ],
        "keywords": [
            "Artificial intelligence, Computational pathology, Radiomics, Deep learning"
        ],
        "abstract": "Across all stages of drug discovery and development, experimental assays are performed to understand the effect of a drug or drug candidate\u2014at the molecular, cellular, organ, or organism level. Imaging is a key technology in this process, and an imaging assay consisting of sample preparation, image acquisition, and image analysis provides a quantitative readout of a system. Historically, chemical assays have been the workhorse of early discovery, screening millions of compounds for a simple endpoint, whereas imaging was primarily used in lower throughput mechanistic studies. However, with development of high-throughput high-content microscopy platforms, the throughput of imaging assays now rivals chemical screens. Similarly, innovations in image analysis mean that robust quantitative conclusions can be derived from complex and multimodal image data, driving informed decision making later in the drug development process. For these reasons, imaging is widely used throughout the pharmaceutical industry and throughout multiple stages of the drug discovery process.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01698095",
        "isbn": null,
        "journal": "Atmospheric Research",
        "publisher": null,
        "title": "utilityofintegratedimergprecipitationandgleampotentialevapotranspirationproductsfordroughtmonitoringovermainlandchina",
        "booktitle": null,
        "doi": "10.1016/j.atmosres.2020.105141",
        "author": [
            "Jiang, Shanhu",
            "Wei, Linyong",
            "Ren, Liliang",
            "Xu, Chong-Yu",
            "Zhong, Feng",
            "Wang, Menghao",
            "Zhang, Linqi",
            "Yuan, Fei",
            "Liu, Yi"
        ],
        "keywords": [
            "IMERG, GLEAM, Standardized Precipitation Evapotranspiration Index (SPEI), Drought monitoring, Mainland China"
        ],
        "abstract": "In this paper, we comprehensively evaluated the utility of integrated long-term satellite-based precipitation and evapotranspiration products for drought monitoring over mainland China. The latest Integrated Multi-satelliteE Retrievals for Global Precipitation Measurement V06 three Runs precipitation products, i.e., the near real-time Early Run (IMERG-E) and Late Run (IMERG-L) and the post-real time Final Run (IMERG-F), and the Global Land Evaporation Amsterdam Model V3.3a (GLEAM) potential evapotranspiration (PET) products from 2001 to 2017 were considered. The accuracy of IMERG precipitation and GLEAM PET products was first evaluated against observed precipitation and Penman-Monteith method estimated PET, respectively, based on dense meteorological station network. The Standard Precipitation Evapotranspiration Index (SPEI) calculated based on IMERG precipitation and GLEAM PET products (SPEIs, including SPEIE, SPEIL and SPEIF corresponding to IMERG-E, IMERG-L and IMERG-F, respectively) were then validated by using SPEI calculated based on meteorological data (SPEIm) at multiple temporal-spatial scales. Finally, four typical drought events were selected to analyse the ability of SPEIs to characterize the temporal-spatial evolution of drought situations. The results showed that the IMERG-F presents much better performance than IMERG-E and IMERG-L in terms of higher CC and smaller BIAS and RMSE values over mainland China. The GLEAM PET well simulated the change trend of reference PET, but generally underestimated reference PET in Northwest China (NW), Xinjiang (XJ) and Qinghai\u2013Tibet plateau (TP). In general, the performances of SPEIs over eastern China and Southwest China (SW) were significantly superior to their performances in the NW, XJ, and TP regions. Even though the SPEIF performed the best, the SPEIE and SPEIL also performed reasonably well in some specific regions. SPEIs can well capture the temporal process and reasonably reflect the spatial characteristics for four typical drought events. It is thus highlighted that the latest IMERG precipitation (especially for IMERG-F) and GLEAM PET products could be used as alternative data sources for comprehensive drought monitoring, on account of the water balance principle over mainland China, particularly in eastern China and SW China. The outcomes of this study will provide valuable references for drought monitoring by integration of multi-source remote-sensing datasets in the GPM era.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.369",
        "scimago_value": "1,488"
    },
    {
        "issnkey": "01604120",
        "isbn": null,
        "journal": "Environment International",
        "publisher": null,
        "title": "theexposomeinpracticeanexploratorypanelstudyofbiomarkersofairpollutantexposureinchinesepeopleaged6069yearschinabapestudy",
        "booktitle": null,
        "doi": "10.1016/j.envint.2021.106866",
        "author": [
            "Tang, Song",
            "Li, Tiantian",
            "Fang, Jianlong",
            "Chen, Renjie",
            "Cha, Yu'e",
            "Wang, Yanwen",
            "Zhu, Mu",
            "Zhang, Yi",
            "Chen, Yuanyuan",
            "Du, Yanjun",
            "Yu, Tianwei",
            "Thompson, David",
            "{Godri Pollitt}, Krystal",
            "Vasiliou, Vasilis",
            "Ji, John",
            "Kan, Haidong",
            "Zhang, Junfeng",
            "Shi, Xiaoming"
        ],
        "keywords": [
            "PM, Exposomics, Panel Study, Personal Exposure Monitoring, Metabolomics, Exposome-Wide Association Study"
        ],
        "abstract": "The exposome overhauls conventional environmental health impact research paradigms and provides a novel methodological framework that comprehensively addresses the complex, highly dynamic interplays of exogenous exposures, endogenous exposures, and modifiable factors in humans. Holistic assessments of the adverse health effects and systematic elucidation of the mechanisms underlying environmental exposures are major scientific challenges with widespread societal implications. However, to date, few studies have comprehensively and simultaneously measured airborne pollutant exposures and explored the associated biomarkers in susceptible healthy elderly subjects, potentially resulting in the suboptimal assessment and management of health risks. To demonstrate the exposome paradigm, we describe the rationale and design of a comprehensive biomarker and biomonitoring panel study to systematically explore the association between individual airborne exposure and adverse health outcomes. We used a combination of personal monitoring for airborne pollutants, extensive human biomonitoring, advanced omics analysis, confounding information, and statistical methods. We established an exploratory panel study of Biomarkers of Air Pollutant Exposure in Chinese people aged 60\u201369 years (China BAPE), which included 76 healthy residents from a representative community in Jinan City, Shandong Province. During the period between September 2018 and January 2019, we conducted prospective longitudinal monitoring with a 3-day assessment every month. This project: (1) leveraged advanced tools for personal airborne exposure monitoring (external exposures); (2) comprehensively characterized biological samples for exogenous and endogenous compounds (e.g., targeted and untargeted monitoring) and multi-omics scale measurements to explore potential biomarkers and putative toxicity pathways; and (3) systematically evaluated the relationships between personal exposure to air pollutants, and novel biomarkers of exposures and effects using exposome-wide association study approaches. These findings will contribute to our understanding of the mechanisms underlying the adverse health impacts of air pollution exposures and identify potential adverse clinical outcomes that can facilitate the development of effective prevention and targeted intervention techniques.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22120955",
        "isbn": null,
        "journal": "Urban Climate",
        "publisher": null,
        "title": "trendstopicsandlessonslearntfromrealcasestudiesusingmesoscaleatmosphericmodelsforurbanclimateapplicationsin20002019",
        "booktitle": null,
        "doi": "10.1016/j.uclim.2021.100785",
        "author": [
            "Kwok, Yu",
            "Ng, Edward"
        ],
        "keywords": [
            "Urban climate, Urban climate modelling, Urban climate application, Mesoscale atmospheric model, Urban parameterization"
        ],
        "abstract": "Researchers have made immense progress in understanding the urban-induced microclimate by numerical modelling. It has been around two decades since urban canopy models now commonly employed in mesoscale atmospheric models for operational and applied research purposes have emerged. To drive further advancement, it is timely to conduct a review of the state-of-the-art and lessons learnt from the relevant literature. In this paper, 102 urban climate real case modelling studies published in 2000\u20132019 are reviewed. Patterns and preferences in their study locations, periods, model choices, land cover databases, topics discussed, and scenarios investigated are holistically examined. There is an evident improvement in model complexity and urban surface data precision during the period reviewed. Most studies focus on the urban thermal climate and effects of urbanization. Based on the research gaps identified, more work is needed on the currently underrepresented but vulnerable cities in developing countries with tropical, arid, and cold climates. Collaborative field campaigns, initiatives to characterize cities in a consistent manner, and multi-scale modelling approaches have proven to benefit the progress in urban climate studies and should therefore be encouraged. More importantly, efforts should be invested in translating the science into information relevant to human well-being, urban planning, and policymaking.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.731",
        "scimago_value": "1,151"
    },
    {
        "issnkey": "00220302",
        "isbn": null,
        "journal": "Journal of Dairy Science",
        "publisher": null,
        "title": "hepatictranscriptomicadaptationfromprepartumtopostpartumindairycows",
        "booktitle": null,
        "doi": "10.3168/jds.2020-19101",
        "author": [
            "Gao, S.T.",
            "Girma, D.D.",
            "Bionaz, M.",
            "Ma, L.",
            "Bu, D.P."
        ],
        "keywords": [
            "RNA sequencing, peripartum cow, metabolic adaptation, hepatic transcriptome"
        ],
        "abstract": "ABSTRACT The transition from pregnancy to lactation is the most challenging period for high-producing dairy cows. The liver plays a key role in biological adaptation during the peripartum. Prior works have demonstrated that hepatic glucose synthesis, cholesterol metabolism, lipogenesis, and in\ufb02ammatory response are increased or activated during the peripartum in dairy cows; however, those works were limited by a low number of animals used or by the use of microarray technology, or both. To overcome such limitations, an RNA sequencing analysis was performed on liver biopsies from 20 Holstein cows at 7 \u00b1 5d before (Pre-P) and 16 \u00b1 2d after calving (Post-P). We found 1,475 upregulated and 1,199 downregulated differently expressed genes (DEG) with a false discovery rate adjusted P-value < 0.01 between Pre-P and Post-P. Bioinformatic analysis revealed an activation of the metabolism, especially lipid, glucose, and amino acid metabolism, with increased importance of the mitochondria and a key role of several signaling pathways, chiefly peroxisome proliferators-activated receptor (PPAR) and adipocytokines signaling. Fatty acid oxidation and gluconeogenesis, with a likely increase in amino acid utilization to produce glucose, were among the most important functions revealed by the transcriptomic adaptation to lactation in the liver. Although gluconeogenesis was induced, data indicated decrease in expression of glucose transporters. The analysis also revealed high activation of cell proliferation but inhibition of xenobiotic metabolism, likely due to the liver response to inflammatory-like conditions. Co-expression network analysis disclosed a tight connection and coordination among genes driving biological processes associated with protein synthesis, energy and lipid metabolism, and cell proliferation. Our data confirmed the importance of metabolic adaptation to lipid and glucose metabolism in the liver of early Post-P cows, with a pivotal role of PPAR and adipocytokines.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03781127",
        "isbn": null,
        "journal": "Forest Ecology and Management",
        "publisher": null,
        "title": "remotesensingoftemperateandborealforestphenologyareviewofprogresschallengesandopportunitiesintheintercomparisonofinsituandsatellitephenologicalmetrics",
        "booktitle": null,
        "doi": "10.1016/j.foreco.2020.118663",
        "author": [
            "Berra, Elias",
            "Gaulton, Rachel"
        ],
        "keywords": [
            "Satellite data, Land surface phenology, Phenometrics, Ground observations, SOS, EOS, Validation"
        ],
        "abstract": "Vegetation phenology is the study of recurring plant life cycle stages, seasonality which is linked to many ecosystem processes and is an important proxy of climate and environmental change. Remote sensing has been playing an important and increasing role in the monitoring and assessment of vegetation phenology. The aim of this review is to critically examine key studies related to remote sensing of vegetation phenology, with a special focus on temperate and boreal forests. Specifically, we focus on how the latest ground, near-surface and aerial data have been used to assess the satellite-derived Land Surface Phenology (LSP) metrics and the agreements that has been achieved in the last 15 years. Results demonstrated that the timing of satellite-derived LSP events can be detected, in the best-case scenarios, with a certainty of around half-week for spring metrics (e.g. Day of Year -DOY- of start of growing season) and around one week for autumn metrics (e.g. DOY of end of growing season). With expected shifts in plant phenology averaging <1 day per decade, such LSP uncertainties (in terms of absolute phenological dates) could greatly over- or under-estimate these species-level shifts; but the spatial variation in phenology can be consistently monitored. An increasing number of studies have investigated autumn phenology in the last decade, but autumn phenological dates continue to be more challenging to retrieve and interpret than spring dates. Emerging opportunities to further advance remote sensing of forest phenology is presented that includes synergetic use of multiple orbital sensors and its LSP evaluation with data from new sensors at a ground, near-surface and airborne level; yet traditional ground-based observations will continue to be highly useful to accurately record the timing of species-specific phenological events. This review might provide a guide for planning and managing remote sensing of forest phenology.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.558",
        "scimago_value": "1,288"
    },
    {
        "issnkey": "15708268",
        "isbn": null,
        "journal": "Journal of Web Semantics",
        "publisher": null,
        "title": "onrevealingsharedconceptualizationamongopendatasets",
        "booktitle": null,
        "doi": "10.1016/j.websem.2020.100624",
        "author": [
            "Bogdanovi\u0107, Milo\u0161",
            "Veljkovi\u0107, Nata\u0161a",
            "{Frtuni\u0107 Gligorijevi\u0107}, Milena",
            "Puflovi\u0107, Darko",
            "Stoimenov, Leonid"
        ],
        "keywords": [
            "Open data, Formal concept analysis, Semantic similarity, Categorization, Natural language processing"
        ],
        "abstract": "Openness and transparency initiatives are not only milestones of science progress but have also influenced various fields of organization and industry. Under this influence, varieties of government institutions worldwide have published a large number of datasets through open data portals. Government data covers diverse subjects and the scale of available data is growing every year. Published data is expected to be both accessible and discoverable. For these purposes, portals take advantage of metadata accompanying datasets. However, a part of metadata is often missing which decreases users\u2019 ability to obtain the desired information. As the scale of published datasets grows, this problem increases. An approach we describe in this paper is focused towards decreasing this problem by implementing knowledge structures and algorithms capable of proposing the best match for the category where an uncategorized dataset should belong to. By doing so, our aim is twofold: enrich datasets metadata by suggesting an appropriate category and increase its visibility and discoverability. Our approach relies on information regarding open datasets provided by users \u2014 dataset description contained within dataset tags. Since dataset tags express low consistency due to their origin, in this paper we will present a method of optimizing their usage through means of semantic similarity measures based on natural language processing mechanisms. Optimization is performed in terms of reducing the number of distinct tag values used for dataset description. Once optimized, dataset tags are used to reveal shared conceptualization originating from their usage by means of Formal Concept Analysis. We will demonstrate the advantage of our proposal by comparing concept lattices generated using Formal Concept Analysis before and after the optimization process and use generated structure as a knowledge base to categorize uncategorized open datasets. Finally, we will present a categorization mechanism based on the generated knowledge base that takes advantage of semantic similarity measures to propose a category suitable for an uncategorized dataset.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,502"
    },
    {
        "issnkey": "26663864",
        "isbn": null,
        "journal": "Cell Reports Physical Science",
        "publisher": null,
        "title": "thedataintensivescientificrevolutionoccurringwheretwodimensionalmaterialsmeetmachinelearning",
        "booktitle": null,
        "doi": "10.1016/j.xcrp.2021.100482",
        "author": [
            "Yin, Hang",
            "Sun, Zhehao",
            "Wang, Zhuo",
            "Tang, Dawei",
            "Pang, Cheng",
            "Yu, Xuefeng",
            "Barnard, Amanda",
            "Zhao, Haitao",
            "Yin, Zongyou"
        ],
        "keywords": [
            "machine learning, 2D materials, materials preparation, structure analysis, property exploration"
        ],
        "abstract": "Summary Machine learning (ML) has experienced rapid development in recent years and been widely applied to assist studies in various research areas. Two-dimensional (2D) materials, due to their unique chemical and physical properties, have been receiving increasing attention since the isolation of graphene. The combination of ML and 2D materials science has significantly accelerated the development of new functional 2D materials, and a timely review may inspire further ML-assisted 2D materials development. In this review, we provide a horizontal and vertical summary of the recent advances at the intersection of the fields of ML and 2D materials, discussing ML-assisted 2D materials preparation (design, discovery, and synthesis of 2D materials), atomistic structure analysis (structure identification and formation mechanism), and properties prediction (electronic properties, thermodynamic properties, mechanical properties, and other properties) and revealing their connections. Finally, we highlight current research challenges and provide insight into future research opportunities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "10462023",
        "isbn": null,
        "journal": "Methods",
        "publisher": null,
        "title": "dnamethylationmethodsglobaldnamethylationandmethylomicanalyses",
        "booktitle": null,
        "doi": "10.1016/j.ymeth.2020.10.002",
        "author": [
            "Li, Shizhao",
            "Tollefsbol, Trygve"
        ],
        "keywords": [
            "DNA methylation, DNA hydroxymethylation, Next-generation sequencing, Bisulfite conversion, Endonuclease digestion, Affinity enrichment, Microarray"
        ],
        "abstract": "DNA methylation provides a pivotal layer of epigenetic regulation in eukaryotes that has significant involvement for numerous biological processes in health and disease. The function of methylation of cytosine bases in DNA was originally proposed as a \u201csilencing\u201d epigenetic marker and focused on promoter regions of genes for decades. Improved technologies and accumulating studies have been extending our understanding of the roles of DNA methylation to various genomic contexts including gene bodies, repeat sequences and transcriptional start sites. The demand for comprehensively describing DNA methylation patterns spawns a diversity of DNA methylation profiling technologies that target its genomic distribution. These approaches have enabled the measurement of cytosine methylation from specific loci at restricted regions to single-base-pair resolution on a genome-scale level. In this review, we discuss the different DNA methylation analysis technologies primarily based on the initial treatments of DNA samples: bisulfite conversion, endonuclease digestion and affinity enrichment, involving methodology evolution, principles, applications, and their relative merits. This review may offer referable information for the selection of various platforms for genome-wide analysis of DNA methylation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.608",
        "scimago_value": "2,080"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "informationdisclosuredecisionsinanorganicfoodsupplychainundercompetition",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.125976",
        "author": [
            "Yu, Yanan",
            "He, Yong"
        ],
        "keywords": [
            "Asymmetric and private information, Production and pricing, Organic certification, Information disclosure, Competition, Food supply chain"
        ],
        "abstract": "This paper mainly investigates the information disclosure decisions in an organic food supply chain including one farmers\u2019 organization and one small-scale producer (producer 2). We find that when demand competition is fierce, the organization is reluctant to disclose demand information. Producer 2 is willing to disclose product information facing a low portion of revenue sharing and a high safety perception. A \u201cwin-win\u201d situation for two producers can occur by means of group certification with a rational portion of revenue sharing. Interestingly, a more transparent market is not necessarily desirable for customers and social welfare.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820239-5",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter11regulatoryaspectsofartificialintelligenceandmachinelearningenabledsoftwareasmedicaldevicessamd",
        "booktitle": "Precision Medicine and Artificial Intelligence",
        "doi": "10.1016/B978-0-12-820239-5.00010-3",
        "author": [
            "Mahler, Michael",
            "Auza, Carolina",
            "Albesa, Roger",
            "Melus, Carlos",
            "Wu, Jungen"
        ],
        "keywords": [
            "SaMD, FDA, Regulatory, Artificial intelligence, Machine learning, Culture of quality and organizational excellence, Pre-Cert Program, MDR, NMPA, Cybersecurity"
        ],
        "abstract": "With the introduction of artificial intelligence (AI) and machine learning (ML) in healthcare and the development of an increasing number of commercial products based on software as medical device (SaMD), regulatory processes need to evolve in parallel. This book chapter aims to provide a high-level overview of the history of SaMD and reviews some of the various regulatory aspects associated with this group of medical devices. Although we aim to cover the regulations globally, the main focus is on markets dependent on the Food and Drug Administration (FDA) based review process mostly due to the early stages of regulations in other geographies.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03787788",
        "isbn": null,
        "journal": "Energy and Buildings",
        "publisher": null,
        "title": "prioritizingurbanplanningfactorsoncommunityenergyperformancebasedongisinformedbuildingenergymodeling",
        "booktitle": null,
        "doi": "10.1016/j.enbuild.2021.111191",
        "author": [
            "Yu, Hang",
            "Wang, Meng",
            "Lin, Xiaoyu",
            "Guo, Haijin",
            "Liu, He",
            "Zhao, Yingru",
            "Wang, Hongxin",
            "Li, Chaoen",
            "Jing, Rui"
        ],
        "keywords": [
            "Residential community, Energy use, Two-stage clustering process, Urban planning factor, Sensitivity analysis"
        ],
        "abstract": "The residential sector accounts for an increasing amount of global energy use with continued urbanization. Residential energy-informed urban planning offers an economical and easy-to-operate approach to achieve more efficient urban energy utilization. However, quantifying the interactions between residential energy and urban planning remains an open challenge. This study proposes a holistic approach integrating GIS techniques, building energy modeling, and a global sensitivity analysis to prioritize eight key urban planning factors on the community energy performance based on a building energy dataset. The dataset, including urban planning and building information, was first established using GIS techniques and validated using survey data. The residential energy performance model at the community scale was developed using the clustering tree structure of residential building prototypes and building performance simulations. A combined data-driven and global sensitivity analysis approach was further applied to prioritize the impacts of eight vital urban planning factors on energy use intensity and peak load intensity. A case study of 1963 communities in Shanghai revealed that, for the energy performance of residential communities, the floor area ratio and building coverage ratio are the most influential factors, followed by the maximum height and high-rise proportion having a relatively low impact but higher than other factors. Overall, the proposed holistic approach generates robust insights into urban-scale residential energy performance, which can effectively inform urban planners to achieve more energy-efficient regulatory planning.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.879",
        "scimago_value": "1,737"
    },
    {
        "issnkey": "20952635",
        "isbn": null,
        "journal": "Frontiers of Architectural Research",
        "publisher": null,
        "title": "fromsmarttoempathiccities",
        "booktitle": null,
        "doi": "10.1016/j.foar.2020.10.001",
        "author": [
            "Biloria, Nimish"
        ],
        "keywords": [
            "Empathic city, Smart city, Wellbeing, Neoliberalism, Regenerative model"
        ],
        "abstract": "This paper acknowledges the contemporary neoliberal mode of operation of Smart Cities. The pitfalls of Smart Cities concerning its propensity towards techno-centric and efficiency-focused governance are identified, with diminutive emphasis on social equity and human-centric urban growth. Thus, the paper elaborates upon an alternative mode of person-environment-interaction based approach towards placemaking: Empathic Cities. This approach implies embracing a shift from efficiency to sufficiency and wellbeing embedded regenerative perspective for conceiving the built environment. First, the variable dimensions of urban growth and governance, which gave rise to the smart city, are contextualized. The embedded neoliberal operational agenda of smart cities are established. On this basis, the underpinnings of an empathic city are established by acknowledging the shift from techno-centric to human-centric and from product-based to context-based smart city and wellbeing perspectives. Strategies toward urban development are proposed, such as embracing a regenerative perspective wherein the city and its constituents need to be understood as interdependent systemic elements while embracing a human-centric and ethical approach. Additionally, a transition from efficiency to sufficiency-oriented practices and a shift towards inclusive modes of participatory governance are proposed as fundamental principles for an empathic future of the built environment.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,444"
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "opportunitiesandchallengesofusingbiometricsforbusinessdevelopingaresearchagenda",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2021.07.028",
        "author": [
            "{De Keyser}, Arne",
            "Bart, Yakov",
            "Gu, Xian",
            "Liu, Stephanie",
            "Robinson, Stacey",
            "Kannan, P.K."
        ],
        "keywords": [
            "Biometrics, Technology, Ethics, Privacy, Security, AI, Bias"
        ],
        "abstract": "Recently, biometric data generated by fingerprints, hand geometry, heart rate, voice patterns, facial characteristics and expressions, brain activity and body movement has increased in both volume and prominence. Surprisingly, academic business literature has remained relatively silent on the immense potential of biometric data, as well as on the various dangers that come with its collection and usage. This article sets out to (1) detail what biometric data entails and how it may be used, (2) describe opportunities associated with using biometric data in various business applications, (3) discuss challenges related to biometric data collection and usage, privacy and security, storage and safety, and potential for reduced inclusiveness and enhanced biases, and (4) outline related directions for future research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "01641212",
        "isbn": null,
        "journal": "Journal of Systems and Software",
        "publisher": null,
        "title": "asoftwareengineeringperspectiveonengineeringmachinelearningsystemsstateoftheartandchallenges",
        "booktitle": null,
        "doi": "10.1016/j.jss.2021.111031",
        "author": [
            "Giray, G\u00f6rkem"
        ],
        "keywords": [
            "Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review"
        ],
        "abstract": "Context: Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems. Objective: The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems. Method: I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies. Results: The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions. Conclusion: The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.829",
        "scimago_value": "0,642"
    },
    {
        "issnkey": "22124209",
        "isbn": null,
        "journal": "International Journal of Disaster Risk Reduction",
        "publisher": null,
        "title": "acomprehensivefloodeventspecificationandinventory19302020turkeycasestudy",
        "booktitle": null,
        "doi": "10.1016/j.ijdrr.2021.102086",
        "author": [
            "Haltas, Ismail",
            "Yildirim, Enes",
            "Oztas, Fatih",
            "Demir, Ibrahim"
        ],
        "keywords": [
            "Flooding, Flood event inventory, Flood event specification, Data model"
        ],
        "abstract": "Flooding is one of the most frequent natural disasters that have significant impact on communities in terms of loss of life, direct and indirect economic losses, and disruption of daily life. Decision makers often depend on flood data inventories to make more informed decisions on the development of flood mitigation plans to protect flood prone communities. A comprehensive inventory that covers multiple aspects of a flood event is critical to identify vulnerable regions, historical trends, and mitigate possible flood impacts. This study proposes an integrated flood data specification to support multi-stakeholder use cases, community-based sustainable domain specific maintenance, and crowdsourced data collection and management. The specification is designed based on comprehensive review of existing global and national repositories, scientific studies and needs and requirements of stakeholders. The specification is designed to include metadata on environmental, economic, and demographic impact, hydraulic, hydrologic, and meteorological features, and detailed location information of a flood event. As a case study, a flood event inventory was compiled for Turkey between 1930 and 2020 using existing national and global data sources and digitized media archives. A total of 2101 flood events with 64 data attributes have been collected over the period of 90 years. An initial statistical analysis of the inventory is also presented for assessment of the seasonal and regional characteristics of flooding in Turkey.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.320",
        "scimago_value": "1,161"
    },
    {
        "issnkey": "13618415",
        "isbn": null,
        "journal": "Medical Image Analysis",
        "publisher": null,
        "title": "neuropsychiatricdiseaseclassificationusingfunctionalconnectomicsresultsoftheconnectomicsinneuroimagingtransferlearningchallenge",
        "booktitle": null,
        "doi": "10.1016/j.media.2021.101972",
        "author": [
            "Schirmer, Markus",
            "Venkataraman, Archana",
            "Rekik, Islem",
            "Kim, Minjeong",
            "Mostofsky, Stewart",
            "Nebel, Mary",
            "Rosch, Keri",
            "Seymour, Karen",
            "Crocetti, Deana",
            "Irzan, Hassna",
            "H\u00fctel, Michael",
            "Ourselin, Sebastien",
            "Marlow, Neil",
            "Melbourne, Andrew",
            "Levchenko, Egor",
            "Zhou, Shuo",
            "Kunda, Mwiza",
            "Lu, Haiping",
            "Dvornek, Nicha",
            "Zhuang, Juntang",
            "Pinto, Gideon",
            "Samal, Sandip",
            "Zhang, Jennings",
            "Bernal-Rusiel, Jorge",
            "Pienaar, Rudolph",
            "Chung, Ai"
        ],
        "keywords": [
            "Functional connectomics, Disease classification, ADHD, Challenge"
        ],
        "abstract": "Large, open-source datasets, such as the Human Connectome Project and the Autism Brain Imaging Data Exchange, have spurred the development of new and increasingly powerful machine learning approaches for brain connectomics. However, one key question remains: are we capturing biologically relevant and generalizable information about the brain, or are we simply overfitting to the data? To answer this, we organized a scientific challenge, the Connectomics in NeuroImaging Transfer Learning Challenge (CNI-TLC), held in conjunction with MICCAI 2019. CNI-TLC included two classification tasks: (1) diagnosis of Attention-Deficit/Hyperactivity Disorder (ADHD) within a pre-adolescent cohort; and (2) transference of the ADHD model to a related cohort of Autism Spectrum Disorder (ASD) patients with an ADHD comorbidity. In total, 240 resting-state fMRI (rsfMRI) time series averaged according to three standard parcellation atlases, along with clinical diagnosis, were released for training and validation (120 neurotypical controls and 120 ADHD). We also provided Challenge participants with demographic information of age, sex, IQ, and handedness. The second set of 100 subjects (50 neurotypical controls, 25 ADHD, and 25 ASD with ADHD comorbidity) was used for testing. Classification methodologies were submitted in a standardized format as containerized Docker images through ChRIS, an open-source image analysis platform. Utilizing an inclusive approach, we ranked the methods based on 16 metrics: accuracy, area under the curve, F1-score, false discovery rate, false negative rate, false omission rate, false positive rate, geometric mean, informedness, markedness, Matthew\u2019s correlation coefficient, negative predictive value, optimized precision, precision, sensitivity, and specificity. The final rank was calculated using the rank product for each participant across all measures. Furthermore, we assessed the calibration curves of each methodology. Five participants submitted their method for evaluation, with one outperforming all other methods in both ADHD and ASD classification. However, further improvements are still needed to reach the clinical translation of functional connectomics. We have kept the CNI-TLC open as a publicly available resource for developing and validating new classification methodologies in the field of connectomics.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.545",
        "scimago_value": "2,887"
    },
    {
        "issnkey": "25900056",
        "isbn": null,
        "journal": "Array",
        "publisher": null,
        "title": "asurveyontheapplicationofdeeplearningforcodeinjectiondetection",
        "booktitle": null,
        "doi": "10.1016/j.array.2021.100077",
        "author": [
            "Abaimov, Stanislav",
            "Bianchi, Giuseppe"
        ],
        "keywords": [
            "Machine learning, Deep learning, Network intrusion detection, Code injection, Preprocessing"
        ],
        "abstract": "Code injection is one of the top cyber security attack vectors in the modern world. To overcome the limitations of conventional signature-based detection techniques, and to complement them when appropriate, multiple machine learning approaches have been proposed. While analysing these approaches, the surveys focus predominantly on the general intrusion detection, which can be further applied to specific vulnerabilities. In addition, among the machine learning steps, data preprocessing, being highly critical in the data analysis process, appears to be the least researched in the context of Network Intrusion Detection, namely in code injection. The goal of this survey is to fill in the gap through analysing and classifying the existing machine learning techniques applied to the code injection attack detection, with special attention to Deep Learning. Our analysis reveals that the way the input data is preprocessed considerably impacts the performance and attack detection rate. The proposed full preprocessing cycle demonstrates how various machine-learning-based approaches for detection of code injection attacks take advantage of different input data preprocessing techniques. The most used machine learning methods and preprocessing stages have been also identified.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01482963",
        "isbn": null,
        "journal": "Journal of Business Research",
        "publisher": null,
        "title": "augmentingorganizationaldecisionmakingwithdeeplearningalgorithmsprinciplespromisesandchallenges",
        "booktitle": null,
        "doi": "10.1016/j.jbusres.2020.09.068",
        "author": [
            "Shrestha, Yash",
            "Krishna, Vaibhav",
            "{von Krogh}, Georg"
        ],
        "keywords": [
            "Case studies, Decision-making, Deep learning, Artificial intelligence"
        ],
        "abstract": "The current expansion of theory and research on artificial intelligence in management and organization studies has revitalized the theory and research on decision-making in organizations. In particular, recent advances in deep learning (DL) algorithms promise benefits for decision-making within organizations, such as assisting employees with information processing, thereby augment their analytical capabilities and perhaps help their transition to more creative work. We conceptualize the decision-making process in organizations augmented with DL algorithm outcomes (such as predictions or robust patterns from unstructured data) as deep learning\u2013augmented decision-making (DLADM). We contribute to the understanding and application of DL for decision-making in organizations by (a) providing an accessible tutorial on DL algorithms and (b) illustrating DLADM with two case studies drawing on image recognition and sentiment analysis tasks performed on datasets from Zalando, a European e-commerce firm, and Rotten Tomatoes, a review aggregation website for movies, respectively. Finally, promises and challenges of DLADM as well as recommendations for managers in attending to these challenges are also discussed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.550",
        "scimago_value": "2,049"
    },
    {
        "issnkey": "0968090x",
        "isbn": null,
        "journal": "Transportation Research Part C: Emerging Technologies",
        "publisher": null,
        "title": "acustomizeddeeplearningapproachtointegratenetworkscaleonlinetrafficdataimputationandprediction",
        "booktitle": null,
        "doi": "10.1016/j.trc.2021.103372",
        "author": [
            "Zhang, Zhengchao",
            "Lin, Xi",
            "Li, Meng",
            "Wang, Yinhai"
        ],
        "keywords": [
            "Traffic prediction, Online data imputation, Deep learning, Bidirectional recurrent neural network, Graph convolution, 1 \u00d7 1 Convolution"
        ],
        "abstract": "Online data imputation and traffic prediction based on real-time data streams are essential for the intelligent transportation systems, particularly online navigation applications based on the real-time traffic information. However, the inevitable data missing problem caused by various disturbances undermines the information contained in such real-time data, thereby threatening the reliability of data acquisition as well as the prediction results. Such scenarios raise a strong need for integrating the tasks of network-scale online data imputation and traffic prediction, because the existing two-step approaches that separate the above procedures cannot be implemented in an online manner. In this paper, we propose a customized spatiotemporal deep learning architecture, named the graph convolutional bidirectional recurrent neural network (GCBRNN), to combine network-scale online data imputation and traffic prediction into an integrated task. The imputation mechanism and bidirectional framework are developed to cooperatively estimate missing entries and infer future values. We further design a network-scale graph convolutional gated recurrent unit (NGC-GRU) within the GCBRNN, which applies the graph convolution operation and 1\u00d71 convolution module to capture the spatiotemporal dependencies in the traffic data. Experiments are carried out on two real-world traffic networks, including traffic speed and flow datasets. The comparison results demonstrate that our approach significantly outperforms several classical benchmark models with respect to both the imputation and prediction tasks on two datasets under various missing data rates.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "15359476",
        "isbn": null,
        "journal": "Molecular & Cellular Proteomics",
        "publisher": null,
        "title": "proximityextensionassayincombinationwithnextgenerationsequencingforhighthroughputproteomewideanalysis",
        "booktitle": null,
        "doi": "10.1016/j.mcpro.2021.100168",
        "author": [
            "Wik, Lotta",
            "Nordberg, Niklas",
            "Broberg, John",
            "Bj\u00f6rkesten, Johan",
            "Assarsson, Erika",
            "Henriksson, Sara",
            "Grundberg, Ida",
            "Pettersson, Erik",
            "Westerberg, Christina",
            "Liljeroth, Elin",
            "Falck, Adam",
            "Lundberg, Martin"
        ],
        "keywords": [
            "biomarker, proteomics, next-generation sequencing, proximity extension assay, multiplex, immunoassay, plasma, serum, antibody"
        ],
        "abstract": "Understanding the dynamics of the human proteome is crucial for developing biomarkers to be used as measurable indicators for disease severity and progression, patient stratification, and drug development. The Proximity Extension Assay (PEA) is a technology that translates protein information into actionable knowledge by linking protein-specific antibodies to DNA-encoded tags. In this report we demonstrate how we have combined the unique PEA technology with an innovative and automated sample preparation and high-throughput sequencing readout enabling parallel measurement of nearly 1500 proteins in 96 samples generating close to 150,000 data points per run. This advancement will have a major impact on the discovery of new biomarkers for disease prediction and prognosis and contribute to the development of the rapidly evolving fields of wellness monitoring and precision medicine.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,757"
    },
    {
        "issnkey": "17511577",
        "isbn": null,
        "journal": "Journal of Informetrics",
        "publisher": null,
        "title": "tracingthemainpathofinterdisciplinaryresearchconsideringcitationpreferenceacasefromblockchaindomain",
        "booktitle": null,
        "doi": "10.1016/j.joi.2021.101136",
        "author": [
            "Yu, Dejian",
            "Pan, Tianxing"
        ],
        "keywords": [
            "Blockchain, Main path analysis, Discipline difference, Citation preference"
        ],
        "abstract": "Main path analysis has been widely used in various fields to detect their development trajectories. However, the previous methods treat every citation equally. In fact, it leaves a question open to scholars considering that there are different citation preferences in different disciplines and at different publication times. There are different citation preferences in different disciplines and at different periods, which are ignored by scholars. In order to deal with the problem in identifying development paths in interdisciplinary research areas, this paper proposes a new main path analysis method. The improved main path analysis considers two factors involved in citation preference, including discipline bias and time bias. An evidence analysis from blockchain domain is conducted to demonstrate the effectiveness of the proposed method. The research result shows that the proposed main path analysis method in this paper can resolve the problem of discipline bias and time bias in interdisciplinary research. Moreover, the improved method provides a more differentiated ranking for citation linkages in the network. Our research can enhance the objectivity of the resulting main paths and promote broader application of the main path analysis.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.107",
        "scimago_value": "1,605"
    },
    {
        "issnkey": "0968090x",
        "isbn": null,
        "journal": "Transportation Research Part C: Emerging Technologies",
        "publisher": null,
        "title": "scalablelowranktensorlearningforspatiotemporaltrafficdataimputation",
        "booktitle": null,
        "doi": "10.1016/j.trc.2021.103226",
        "author": [
            "Chen, Xinyu",
            "Chen, Yixian",
            "Saunier, Nicolas",
            "Sun, Lijun"
        ],
        "keywords": [
            "Spatiotemporal traffic data, High-dimensional data, Missing data imputation, Low-rank tensor completion, Linear unitary transformation, Quadratic variation"
        ],
        "abstract": "Missing value problem in spatiotemporal traffic data has long been a challenging topic, in particular for large-scale and high-dimensional data with complex missing mechanisms and diverse degrees of missingness. Recent studies based on tensor nuclear norm have demonstrated the superiority of tensor learning in imputation tasks by effectively characterizing the complex correlations/dependencies in spatiotemporal data. However, despite the promising results, these approaches do not scale well to large data tensors. In this paper, we focus on addressing the missing data imputation problem for large-scale spatiotemporal traffic data. To achieve both high accuracy and efficiency, we develop a scalable tensor learning model\u2014Low-Tubal-Rank Smoothing Tensor Completion (LSTC-Tubal)\u2014based on the existing framework of Low-Rank Tensor Completion, which is well-suited for spatiotemporal traffic data that is characterized by multidimensional structure of location \u00d7 time of day \u00d7 day. In particular, the proposed LSTC-Tubal model involves a scalable tensor nuclear norm minimization scheme by integrating linear unitary transformation. Therefore, tensor nuclear norm minimization can be solved by singular value thresholding on the transformed matrix of each day while the day-to-day correlation can be effectively preserved by the unitary transform matrix. Before setting up the experiment, we consider some real-world data sets, including two large-scale 5-min traffic speed data sets collected by the California PeMS system with 11160 sensors: 1) PeMS-4W covers the data over 4 weeks (i.e., 288\u00d728 time points), and 2) PeMS-8W covers the data over 8 weeks (i.e., 288\u00d756 time points). We compare LSTC-Tubal with some state-of-the-art baseline models, and find that LSTC-Tubal can achieve competitively accuracy with a significantly lower computational cost. In addition, the LSTC-Tubal will also benefit other tasks in modeling large-scale spatiotemporal traffic data, such as network-level traffic forecasting.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13640321",
        "isbn": null,
        "journal": "Renewable and Sustainable Energy Reviews",
        "publisher": null,
        "title": "artificialintelligenceandinternetofthingstoimproveefficacyofdiagnosisandremotesensingofsolarphotovoltaicsystemschallengesrecommendationsandfuturedirections",
        "booktitle": null,
        "doi": "10.1016/j.rser.2021.110889",
        "author": [
            "Mellit, Adel",
            "Kalogirou, Soteris"
        ],
        "keywords": [
            "Deep learning, Fault detection and diagnosis, Internet of things, Machine learning, Photovoltaic systems, Remote sensing, Smart monitoring"
        ],
        "abstract": "Currently, a huge number of photovoltaic plants have been installed worldwide and these plants should be carefully protected and supervised continually in order to be safe and reliable during their working lifetime. Photovoltaic plants are subject to different types of faults and failures, while available fault detection equipment are mainly used to protect and isolate the photovoltaic plants from some faults (such as arc fault, line-to-line, line-to-ground and ground faults). Although a good number of international standards (IEC, NEC, and UL) exists, undetectable faults continue to create serious problems in photovoltaic plants. Thus, designing smart equipment, including artificial intelligence and internet of things for remote sensing and fault detection and diagnosis of photovoltaic plants, will considerably solve the shortcomings of existing methods and commercialized equipment. This paper presents an overview of artificial intelligence and internet of things applications in photovoltaic plants. This research presents also the most advanced algorithms such as machine and deep learning, in terms of cost implementation, complexity, accuracy, software suitability, and feasibility of real-time applications. The embedding of artificial intelligence and internet of things techniques for fault detection and diagnosis into simple hardware, such as low-cost chips, may be economical and technically feasible for photovoltaic plants located in remote areas, with costly and challenging accessibility for maintenance. Challenging issues, recommendations, and trends of these techniques will also be presented in this paper.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "3,522"
    },
    {
        "issnkey": "26661438",
        "isbn": null,
        "journal": "Latin American Journal of Central Banking",
        "publisher": null,
        "title": "policyreportonfintechdatagaps",
        "booktitle": null,
        "doi": "10.1016/j.latcb.2021.100037",
        "author": [
            "Marqu\u00e9s, Jos\u00e9",
            "\u00c1vila, Fernando",
            "Rodr\u00edguez-Mart\u00ednez, Anah\u00ed",
            "Morales-Res\u00e9ndiz, Ra\u00fal",
            "Marcos, Antonio",
            "Godoy, Tamara",
            "Villalobos, Pablo",
            "Ocontrillo, Andrea",
            "Lankester, Valerie",
            "Blanco, Clemente",
            "Reyes, Karla",
            "Lopez, Silvia",
            "Fern\u00e1ndez, Ana",
            "Santos, Rom\u00e1n",
            "Maza, Luis",
            "S\u00e1nchez, Manuel",
            "Dom\u00ednguez, Carlos",
            "Haynes, Natalie",
            "Panton, Novelette",
            "Griffiths, Mario",
            "Murray, Kurt",
            "Doyle-Lowe, Michelle",
            "{Des Vignes}, Leslie",
            "Francis-Pantor, Michelle"
        ],
        "keywords": [
            ""
        ],
        "abstract": "This document aims to provide an overview of the main issues related to data gaps to facilitate monitoring of FinTech and overcome the significant challenges towards incorporating FinTech activities in regular statistics. Moreover, the document explains the implications of data gaps on some of the Central Banks\u2019 main areas, in particular, monetary policy, financial stability, payment systems, and economic activity. Additionally, other implications related to the activity of BigTech companies, the impact of COVID-19 and Cybersecurity issues are explained, which represent an important challenge for data gathering at Central Banks. Also, it describes the main findings of the Irving Fisher Committee (IFC) survey \u201cCentral Banks and FinTech data\u201d based on the answers provided by Latin American and Caribbean (LAC) countries, which identify their different positions regarding this topic and the current initiatives that each one is launching. Finally, a number of next steps are proposed based on a policy discussion and how LAC countries could overcome data gaps and improve data collection based on their current experience.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22128271",
        "isbn": null,
        "journal": "Procedia CIRP",
        "publisher": null,
        "title": "frameworkfortheintegrationofprocessminingintolifecycleassessment",
        "booktitle": null,
        "doi": "10.1016/j.procir.2021.01.024",
        "author": [
            "Ortmeier, Christian",
            "Henningsen, Nadja",
            "Langer, Adrian",
            "Reiswich, Alexander",
            "Karl, Alexander",
            "Herrmann, Christoph"
        ],
        "keywords": [
            "process mining, process discovery, life cycle assesment, hotspot analyzes, life cycle inventory"
        ],
        "abstract": "An increasing product variance, shorter life cycles and the integration of new products and technologies into existing factories lead to a high complexity in today\u2019s production systems. This makes it difficult to carry out Life Cycle Assessments (LCA) continuously and effectively. Major challenges of LCA are on the one hand a high expenditure of time and on the other hand a static evaluation of individual products. In order to perform dynamic and continuous process analyses, companies increasingly rely on new technologies and methods. Numerous studies have already been able to tap promising potentials by using Process Mining (PM). In contrast to traditional methods of process modeling, PM uses event log data to model the actual production processes. Based on this data, a real-time model is built to identify waste. In a holistic approach, PM is able to automatically uncover social and organizational networks and map them in a simulation model. According to the current state of research, there is no concept for integrating PM into LCA. Therefore, the present work focuses on the investigation of interfaces between PM and LCA in order to systematically identify and evaluate the potentials of using PM. The results are applied to a use case in the sector of commercial vehicles.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,683"
    },
    {
        "issnkey": "09265805",
        "isbn": null,
        "journal": "Automation in Construction",
        "publisher": null,
        "title": "semanticframeworkforinterdependentinfrastructureresiliencedecisionsupport",
        "booktitle": null,
        "doi": "10.1016/j.autcon.2021.103852",
        "author": [
            "Dao, Jicao",
            "Ng, S.",
            "Yang, Yifan",
            "Zhou, Shenghua",
            "Xu, Frank",
            "Skitmore, Martin"
        ],
        "keywords": [
            "Semantic Web, Ontology, Infrastructure systems, Interdependency, Data integration, Resilient decisions"
        ],
        "abstract": "The increasing need for interdependent infrastructure systems to withstand natural disasters has called for the co-creation of resilience decisions to minimize the impact on society. However, issues related to information integration across different infrastructure systems hamper decision making from a system-to-systems perspective. To resolve this problem, the Semantic Web technologies are presented in this paper to serve four functions: (i) linking cross domains through ontology development to represent different domain knowledge; (ii) integrating multiple-source heterogeneous data by a common data format; (iii) retrieving useful information using semantic query language; and (iv) deriving machine automatic logical reasoning by rule languages and logic engines to provide informed resilience decision making support. The proposed framework is tested by a case scenario involving intertwined drainage-transport-building systems under the influence of urban flooding. The result indicates that the framework effectively facilitates information integration between diverse infrastructure systems and helps decision-makers by providing resilience decision-making support.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.700",
        "scimago_value": "1,837"
    },
    {
        "issnkey": "03014215",
        "isbn": null,
        "journal": "Energy Policy",
        "publisher": null,
        "title": "doesinternetdevelopmentimprovegreentotalfactorenergyefficiencyevidencefromchina",
        "booktitle": null,
        "doi": "10.1016/j.enpol.2021.112247",
        "author": [
            "Wu, Haitao",
            "Hao, Yu",
            "Ren, Siyu",
            "Yang, Xiaodong",
            "Xie, Guo"
        ],
        "keywords": [
            "Internet development, Green total factor energy efficiency, Spatial durbin model, Dynamic threshold model"
        ],
        "abstract": "Information and communication technology supported by the internet has become an important driving force that promotes the intelligent development of environmental governance in China. Using Chinese provincial panel data for the period 2006\u20132017, this study investigates whether the internet has improved China's green total factor energy efficiency (GTFEE) using a dynamic spatial Durbin model, mediation effect model and dynamic threshold panel model. The empirical results indicate that the GTFEE has a significant positive spatial correlation. Internet development can not only directly improve local GTFEE but also improve GTFEE in neighboring regions. After accounting for potential endogeneity, this conclusion is still valid. Meanwhile, internet development can indirectly improve regional GTFEE by reducing the degree of resource mismatch while enhancing GTFEE by improving regional innovation capabilities and promoting industrial structure upgrades. In addition, the regression results of the dynamic threshold model show that there is a nonlinear relationship between the influence of the internet development and GTFEE. Specifically, due to an increase in the degree of labor resource mismatch and capital resource mismatch, the impact of the internet on GTFEE has gradually decreased, and this effect has gradually increased with the improvement of regional innovation capabilities and the industrial structure.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.142",
        "scimago_value": "2,093"
    },
    {
        "issnkey": "07365853",
        "isbn": null,
        "journal": "Telematics and Informatics",
        "publisher": null,
        "title": "beyondthesupplysideuseandimpactofmunicipalopendataintheus",
        "booktitle": null,
        "doi": "10.1016/j.tele.2020.101526",
        "author": [
            "Wilson, Bev",
            "Cong, Cong"
        ],
        "keywords": [
            "Open data, Local government, Civic technology, Digital equity"
        ],
        "abstract": "While the number of open government data initiatives has increased considerably over the past decade, the impact of these initiatives remains uncertain. Recent studies have been critical of the \u201cbias toward the supply side\u201d and lack of \u201csufficient attention to the user perspective\u201d in the way that open government data initiatives are implemented. This article asks: (1) who is using municipal open government data resources and for what purposes? and (2) what impact are municipal open government data having in cities where they have been implemented? We performed a qualitative analysis of 26 semi-structured telephone interviews conducted with government staff, civic technologists, and private sector stakeholders in nine cities around the United States. Each of these 30 to 45-minute telephone interviews were transcribed and analyzed to distill insights regarding the use and impact of municipal open government data in the nine cities considered. We find that the array of actors within open government data ecosystems at the local level is expanding as distinctions between the public and private sectors becomes increasingly blurred and that the demands of managing and sustaining these initiatives has led to changes in the services offered by local government, as well as in the duties of government staff. The impact of these data resources has been primarily felt within local government itself, although the lack of monitoring mechanisms makes it difficult to systematically evaluate their broader effects. We conclude that open government data initiatives should be coordinated and better integrated with digital equity and digital inclusion efforts in order to advance their political and social goals.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.182",
        "scimago_value": "1,567"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821953-9",
        "journal": null,
        "publisher": "Elsevier",
        "title": "index",
        "booktitle": "Environmental Systems Science",
        "doi": "10.1016/B978-0-12-821953-9.09991-8",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "02650568",
        "isbn": null,
        "journal": "Natural Product Reports",
        "publisher": null,
        "title": "metabolomicsandgenomicsinnaturalproductsresearchcomplementarytoolsfortargetingnewchemicalentities",
        "booktitle": null,
        "doi": "10.1039/d1np00036e",
        "author": [
            "Caesar, Lindsay",
            "Montaser, Rana",
            "Keller, Nancy",
            "Kelleher, Neil"
        ],
        "keywords": [
            ""
        ],
        "abstract": "ABSTRACT Covering: 2010 to 2021 Organisms in nature have evolved into proficient synthetic chemists, utilizing specialized enzymatic machinery to biosynthesize an inspiring diversity of secondary metabolites. Often serving to boost competitive advantage for their producers, these secondary metabolites have widespread human impacts as antibiotics, anti-inflammatories, and antifungal drugs. The natural products discovery field has begun a shift away from traditional activity-guided approaches and is beginning to take advantage of increasingly available metabolomics and genomics datasets to explore undiscovered chemical space. Major strides have been made and now enable -omics-informed prioritization of chemical structures for discovery, including the prospect of confidently linking metabolites to their biosynthetic pathways. Over the last decade, more integrated strategies now provide researchers with pipelines for simultaneous identification of expressed secondary metabolites and their biosynthetic machinery. However, continuous collaboration by the natural products community will be required to optimize strategies for effective evaluation of natural product biosynthetic gene clusters to accelerate discovery efforts. Here, we provide an evaluative guide to scientific literature as it relates to studying natural product biosynthesis using genomics, metabolomics, and their integrated datasets. Particular emphasis is placed on the unique insights that can be gained from large-scale integrated strategies, and we provide source organism-specific considerations to evaluate the gaps in our current knowledge.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "13.423",
        "scimago_value": "2,703"
    },
    {
        "issnkey": "09242244",
        "isbn": null,
        "journal": "Trends in Food Science & Technology",
        "publisher": null,
        "title": "digitaltwinsarecomingwillweneedtheminsupplychainsoffreshhorticulturalproduce",
        "booktitle": null,
        "doi": "10.1016/j.tifs.2021.01.025",
        "author": [
            "Defraeye, Thijs",
            "Shrivastava, Chandrima",
            "Berry, Tarl",
            "Verboven, Pieter",
            "Onwude, Daniel",
            "Schudel, Seraina",
            "B\u00fchlmann, Andreas",
            "Cronje, Paul",
            "Rossi, Ren\u00e9"
        ],
        "keywords": [
            "Postharvest, Physics-based, Virtual, Modeling, Simulation, Cyber-physical"
        ],
        "abstract": "Background Digital twins have advanced fast in various industries, but are just emerging in postharvest supply chains. A digital twin is a virtual representation of a certain product, such as fresh horticultural produce. This twin is linked to the real-world product by sensors supplying data of the environmental conditions near the target fruit or vegetable. Statistical and data-driven twins quantify how quality loss of fresh horticultural produce occurs by grasping patterns in the data. Physics-based twins provide an augmented insight into the underlying physical, biochemical, microbiological and physiological processes, enabling to explain also why this quality loss occurs. Scope and approach We identify what the key advantages are of digital twins and how the supply chain of fresh horticultural produce can benefit from them in the future. Key findings and conclusions A digital twin has a huge potential to help horticultural produce to tell its history as it drifts along throughout its postharvest life. The reason is that each shipment is subject to a unique and unpredictable set of temperature and gas atmosphere conditions from farm to consumer. Digital twins help to identify the resulting, largely uncharted, postharvest evolution of food quality. The benefit of digital twins particularly comes forward for perishable species and at low airflow rates. Digital twins provide actionable data for exporters, retailers, and consumers, such as the remaining shelf life for each shipment, on which logistics decisions and marketing strategies can be based. The twins also help diagnose and predict potential problems in supply chains that will reduce food quality and induce food loss. Twins can even suggest preventive shipment-tailored measures to reduce retail and household food losses.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,676"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "buildinginformationmodelingandinternetofthingsintegrationforsmartandsustainableenvironmentsareview",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.127716",
        "author": [
            "Malagnino, Ada",
            "Montanaro, Teodoro",
            "Lazoi, Mariangela",
            "Sergi, Ilaria",
            "Corallo, Angelo",
            "Patrono, Luigi"
        ],
        "keywords": [
            "Environmental sustainability, Smart environment, Internet of Things, BIM (Building Information Modeling), IoT"
        ],
        "abstract": "During the last decades, society has increasingly moved towards the adoption of digital solutions in almost every aspect of people's lives with the aim of enhancing daily activities. At the same time, the environmental impact of the built environment has attracted the attention of public opinion that is gradually perceiving the necessity of limiting its negative effects in order to safeguard the Earth and people's wellbeing. The Internet of Things is one of the biggest ecosystems that is bringing innovations encompassing digital solutions in almost every sector. On the other hand, the Building Information Modeling approach allows for data sharing among stakeholders, traceability, and the integrated management of the building or infrastructure life-cycle through a 3D informative virtual model. Our study reviews existing research works and technological solutions that integrate these two important topics to enhance the sustainability of the built environment, making it smarter. The presented review analyses the existing papers available in literature from January 2015 to December 2020, to present the best practices in this integration and discuss limitations of the identified solutions. Based on the outcomes of the analysis and aiming at the creation of a solid knowledge basis for the community interested in the sector, a comprehensive modular architecture has been proposed. Finally, new directions for future works are presented by discussing how the proposed architecture can actually facilitate the design and development phases.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "13648152",
        "isbn": null,
        "journal": "Environmental Modelling & Software",
        "publisher": null,
        "title": "multiplerotationsofgaussianquadraturesanefficientmethodforuncertaintyanalysesinlargescalesimulationmodels",
        "booktitle": null,
        "doi": "10.1016/j.envsoft.2020.104929",
        "author": [
            "Stepanyan, Davit",
            "Grethe, Harald",
            "Zimmermann, Georg",
            "Siddig, Khalid",
            "Deppermann, Andre",
            "Feuerbacher, Arndt",
            "Luckmann, Jonas",
            "Valin, Hugo",
            "Nishizawa, Takamasa",
            "Ermolieva, Tatiana",
            "Havlik, Petr"
        ],
        "keywords": [
            "Uncertainty analysis, Systematic sensitivity analysis, Stochastic modeling, Multiple rotations of Gaussian quadratures, Monte Carlo sampling, Computable general equilibrium models, Partial equilibrium models"
        ],
        "abstract": "Concerns regarding the impact of climate change, food price volatility, and weather uncertainty have motivated users of simulation models to consider uncertainty in their simulations. One way to do this is to integrate uncertainty components in the model equations, thus turning the model into a problem of numerical integration. Most of these problems do not have analytical solutions, and researchers, therefore, apply numerical approximation methods. This article presents a novel approach to conducting an uncertainty analysis as an alternative to the computationally burdensome Monte Carlo-based (MC) methods. The developed method is based on the degree three Gaussian quadrature (GQ) formulae and is tested using three large-scale simulation models. While the standard single GQ method often produces low-quality approximations, the results of this study demonstrate that the proposed approach reduces the approximation errors by a factor of nine using only 3.4% of the computational effort required by the MC-based methods in the most computationally demanding model.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,828"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "quantificationandmanagementofurbantrafficemissionsbasedonindividualvehicledata",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.129386",
        "author": [
            "Yu, Zhi",
            "Li, Weichi",
            "Liu, Yonghong",
            "Zeng, Xuelan",
            "Zhao, Yongming",
            "Chen, Kaiying",
            "Zou, Bin",
            "He, Jiajun"
        ],
        "keywords": [
            "Individual vehicle emissions, Emission quantification, Licence plate recognition data, Spatiotemporal emission characteristics, Traffic emission reduction policy"
        ],
        "abstract": "Urban traffic pollution poses a serious threat to the environment and human health, especially in urban centres with high population density. Traditional traffic pollution quantification and management methods can be improved based on fine-grained individual vehicle data provided by intelligent transportation systems. Traditional traffic emission quantification and management are often based on simulated or relatively coarse-grained measured data. Such data lack a comprehensive reflection of the actual conditions of all vehicles travelling on roads, which leads to deviations in emission quantification; thus, they cannot support the delicate control policy of traffic pollution. This paper presents a high-resolution individual vehicle emission quantification method based on real-time, real-world individual vehicle data, with a combination of automatic licence plate recognition data and vehicle registration data currently used for traffic management. In this study, we quantified the emissions of each vehicle driving in the urban centre of the case city and analysed regional traffic emission characteristics. We found that there was an apparent uneven distribution of vehicle emissions; that is, the emissions from a small number of high-emission vehicles accounted for a large proportion of the regional traffic emissions. Different pollutants and vehicle types had different emission distribution characteristics. Furthermore, we explored emission reduction policies based on the management of high-emission vehicles identified by individual vehicle data and conduct fine-scale analysis of the link-level hourly emission reduction effects. In addition, a comparison between traditional methods and the method used in this paper for emission quantification was performed. This paper provides a basis for the accurate analysis of regional traffic emission characteristics, individual-based emission reduction policy formulation, and refined policy effect analysis, which has great significance for the control of traffic pollution.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "opennesstoindustry40andperformancetheimpactofbarriersandincentives",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.120756",
        "author": [
            "Cugno, Monica",
            "Castagnoli, Rebecca",
            "B\u00fcchi, Giacomo"
        ],
        "keywords": [
            "Industry 4.0, Openness, Performance, Barriers, Incentives, Mediators"
        ],
        "abstract": "The impact of barriers and incentives on the relationship between openness to Industry 4.0 and performance have so far received little scholarly attention. As a result, this paper explores this relationship by employing a mixed methods approach. A qualitative analysis using in-depth interviews and multiple case studies identifies prominent barriers and incentives, whilst a quantitative analysis on a representative sample of 500 local manufacturing units in Piedmont (a region of Northern Italy) is undertaken via an OLS regression-based path analysis. The results of the parallel-serial multiple mediation model show that: (1) greater openness to Industry 4.0 is related to better performance; (2) greater openness to Industry 4.0 leads to a higher perception of barriers; (3) greater knowledge-related and economic and financial barriers improve performance, abstracting from the adoption of incentives; and (4) greater openness to Industry 4.0 drives the adoption of incentives. However, perceived economic and financial barriers are found not to drive firms to adopt more incentives. The study contributes to the Industry 4.0 literature by identifying previously unidentified strengths and weaknesses to barriers and incentives, and highlights the necessity of policies that reflect real firms\u2019 needs.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "18711413",
        "isbn": null,
        "journal": "Livestock Science",
        "publisher": null,
        "title": "diagnosticpropertiesofmilkdiversionandfarmerreportedmastitistoindicateclinicalmastitisstatusindairycowsusingbayesianlatentclassanalysis",
        "booktitle": null,
        "doi": "10.1016/j.livsci.2021.104698",
        "author": [
            "Bonestroo, John",
            "Fall, Nils",
            "{van der Voort}, Mariska",
            "Klaas, Ilka",
            "Hogeveen, Henk",
            "Emanuelson, Ulf"
        ],
        "keywords": [
            "antibiotic treatment, proxy, Automatic milking system, Milk withdrawal, Latent class analysis"
        ],
        "abstract": "The development of digital farming gives bovine mastitis research and management tools access to large datasets. However, the quality of registered data on clinical mastitis cases or treatments may be inadequate (e.g. due to missing records). In automatic milking systems, the decision to divert milk from the bulk milk tank during milking is registered (i.e. milk diversion indicator) for every milking and could potentially indicate a clinical mastitis case. This study accordingly estimated the diagnostic performance of a milk diversion indicator in relation to farmer-recorded clinical mastitis cases in the absence of a \u201cgold standard\u201d. Data on milk diversion and farmer-reported clinical mastitis from 3,443 lactations in 13 herds were analyzed. Each cow lactation was split into 30-DIM periods in which it was registered whether milk was diverted and whether clinical mastitis was reported. One 30-DIM period was randomly sampled for each lactation and this was the unit of analysis, this procedure was repeated 300 times, resulting in 300 datasets to create autocorrelation-robust results during analysis. We used Bayesian latent class analysis to assess the diagnostic properties of milk diversion and farmer-reported clinical status. We analyzed different episode lengths of milk diversion of 1 or more milk diversion days until 10 or more milk diversion days for two scenarios: farmers with poor-quality (51% sensitivity, 99% specificity) and high-quality (90% sensitivity, 99% specificity) mastitis registrations. The analysis was done for all 300 datasets. The results showed that for the scenario where the quality of clinical mastitis reporting was high, the sensitivity was similar for milk-diversion threshold durations of 1\u20134 days (0.843 to 0.793 versus 0.893). Specificity increased when the number of days of milk diversion increased and was \u226598% at a milk-diversion threshold durations of 8 or more consecutive milk diversion days. In the scenario where the quality of clinical mastitis reporting was low, the sensitivity of milk diversion and reported clinical mastitis cases was similar at milk-diversion threshold durations of 1\u20137 days (0.687 to 0.448 versus 0.503 to 0.504) while specificity exceeded the 98% at milk-diversion threshold durations of 7 or more consecutive milk diversion days. In both scenarios, a milk diversion threshold duration of 4\u20137 days achieved the most desirable combined sensitivity and specificity. This study concluded that milk diversion can be a valid alternative to farmer-reported clinical mastitis as it performs similarly in indicating actual clinical mastitis.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "1.943",
        "scimago_value": "0,622"
    },
    {
        "issnkey": "09265805",
        "isbn": null,
        "journal": "Automation in Construction",
        "publisher": null,
        "title": "automatedprocessdiscoveryfromeventlogsinbimconstructionprojects",
        "booktitle": null,
        "doi": "10.1016/j.autcon.2021.103713",
        "author": [
            "Pan, Yue",
            "Zhang, Limao"
        ],
        "keywords": [
            "Building information modeling, Process mining, Social network analysis, Performance evaluation, Construction management"
        ],
        "abstract": "To fully understand how a construction project actually proceeds, a novel framework for automated process discovery from building information modeling (BIM) event logs is developed. The significance of the work is to manage and optimize the complex construction process towards the ultimate goal of narrowing the gap between BIM and process mining. More specifically, meaningful information is retrieved from prepared event logs to build a participant-specific process model, and then the established model with executable semantics and fitness guarantees provides evidence in process improvement through identifying deviations, inefficiencies, and collaboration features. The proposed method has been validated in a case study, where the input is an as-planned event log from a real BIM construction project. The process model is created automatically by the inductive mining and fuzzy mining algorithms, which is then analyzed deeply under the joint use of conformance checking, frequency and bottleneck analysis, and social network analysis (SNA). The discovered knowledge contributes to revealing potential problems and evaluating the performance of workflows and participants objectively. In the discussion part, as-built data from the internet of things (IoT) deployment in construction site monitoring is automatically compared with the as-planned event log in the BIM platform to detect the actual delays. It turns out that the participant playing a central role in the network tends to overburden with heavier workloads, leading to more undesirable discrepancies and delays. As a result, extensive investigations based on process mining supports data-driven decision making to strategically smooth the construction process and increase collaboration opportunities, which also help in reducing the risk of project failure ahead of time.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.700",
        "scimago_value": "1,837"
    },
    {
        "issnkey": "00320633",
        "isbn": null,
        "journal": "Planetary and Space Science",
        "publisher": null,
        "title": "physicalrealityinplanetarygeomorphologicalinferenceandthepathwaytoacriticalplanetarygeology",
        "booktitle": null,
        "doi": "10.1016/j.pss.2020.105121",
        "author": [
            "Siwabessy, Andrew"
        ],
        "keywords": [
            "Philosophy of geology, Philosophy of geography, Sociology of geology, Epistemology of geology, Critical planetary geology"
        ],
        "abstract": "I engage the philosophy of geology, and the philosophy of planetary geomorphology in particular, offering a perspective from the standpoint of a Mars geographer who is acquainted with both experimental and social research. I discuss the nature of geological reasoning and probe its somewhat paradoxical commitment to both a physical realism and to interpretative narrative construction, engaging particularly with how the interplay of these two factors are addressed in the works of the philosopher-geologist Victor Baker. I also draw attention to specific characteristics of knowledge construction in Martian geomorphology which systemically limit the attachment of our inferences to a physical reality. For instance I highlight a geospatial-geochemical dimorphism in the semantic content of available planetary data and several non-scientific controls on the knowledge construction process, including mission safety constraints and sociopolitical factors. Lastly, I find that physical geography has grown apart from planetary geomorphology in the decades since planetary geomorphology assumed its largely geological modern form. An academic siloing effect has followed suit. An immediate implication is that this contours the typical pathways of knowledge construction in our discipline, and I try to call attention to a few examples of this effect. As an effort to bridge this gap, I highlight the recent developments of the physical geographical subdisciplines of GIScience and critical physical geography. These disciplines offer opportunities for insight which might be fruitful for planetary geomorphologists, but their lines of inquiry have largely passed beneath our community\u2019s radar. Specifically, in offering examples of what discourse in a critical planetary geology might look like, I hope to encourage the informal philosophical discussions already circulating in our community to pass into a formal space of debate.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.030",
        "scimago_value": "0,696"
    },
    {
        "issnkey": "22106707",
        "isbn": null,
        "journal": "Sustainable Cities and Society",
        "publisher": null,
        "title": "naturallanguageprocessingbasedcharacterizationoftopdowncommunicationinsmartcitiesforenhancingcitizenalignment",
        "booktitle": null,
        "doi": "10.1016/j.scs.2020.102674",
        "author": [
            "Nicolas, Cl\u00e9ment",
            "Kim, Jinwoo",
            "Chi, Seokho"
        ],
        "keywords": [
            "Smart city, Top-down communication, Citizen alignment, Natural language processing, Web scraping, Topic modeling, Latent Dirichlet allocation"
        ],
        "abstract": "Many city governments have implemented promising smart initiatives to make cities more efficient, livable, and ecological. To harness the full potential of smart city initiatives, it is vital for policymakers to align citizens with the project objectives. This study comprehensively characterizes and classifies top-down announcements formulated by city developers into six alignment categories (i.e., smart economy, smart people, smart governance, smart mobility, smart environment, and smart living) using natural language processing. The proposed framework consists of five main processes: (1) web scraping-based extraction of announcements of four smart cities \u2013 Boston, Helsinki, Seoul, and Taipei, (2) text data preprocessing, (3) latent Dirichlet allocation-based modeling of strategic topics, (4) quantification of inter-topic similarities using Hellinger distance, and (5) comparison of top-down communication trends with real-world levels of urban performance. Through the comparative analysis of top-down communication trends and actual urban performances, the top-down discourses of smart cities were deconstructed as a reflection of wider political programs developed to enhance citizen alignment and urban performances. Furthermore, inter-topic similarities were also quantified to reflect whether communication strategies are multidisciplinary and city-tailored. In conclusion, the findings of this study can enhance our understanding and provide workable guidance for future smart city development.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.587",
        "scimago_value": "1,645"
    },
    {
        "issnkey": "03787206",
        "isbn": null,
        "journal": "Information & Management",
        "publisher": null,
        "title": "privacyandtheinternetofthingsanexperimentindiscretechoice",
        "booktitle": null,
        "doi": "10.1016/j.im.2020.103292",
        "author": [
            "Goad, David",
            "Collins, Andrew",
            "Gal, Uri"
        ],
        "keywords": [
            "Internet of Things, IoT, Privacy, Discrete choice methods"
        ],
        "abstract": "The Internet of Things (IoT) is the concept that everyday devices are connected to the Internet generating data about us and the world around us. With the number of devices connected directly to the Internet expected to be three times the number of people by 2020, the potential for a reduction in personal privacy is evident. This research fills a gap in the literature by conducting a quantitative analysis of people\u2019s privacy preferences as it relates to the IoT. Our findings provide potential guidance to practitioners in their IoT architectural design and increase our understanding of privacy preference overall.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "2,147"
    },
    {
        "issnkey": "01681699",
        "isbn": null,
        "journal": "Computers and Electronics in Agriculture",
        "publisher": null,
        "title": "acomprehensivereviewonrecentapplicationsofunmannedaerialvehicleremotesensingwithvarioussensorsforhighthroughputplantphenotyping",
        "booktitle": null,
        "doi": "10.1016/j.compag.2021.106033",
        "author": [
            "Feng, Lei",
            "Chen, Shuangshuang",
            "Zhang, Chu",
            "Zhang, Yanchao",
            "He, Yong"
        ],
        "keywords": [
            "Unmanned aerial vehicle, Remote sensing, High-throughput phenotyping, Sensors, Applications review"
        ],
        "abstract": "High-throughput phenotyping has been widely studied in plant science to monitor plant growth and analyze the influence of genotypes and environment on plant growth. To meet the demand of large-scale high-throughput phenotyping, unmanned aerial vehicles (UAVs) have been developed for near-ground remote sensing. UAVs based remote sensing has been used for high-throughput phenotyping of various traits of plants. This review focused on the applications of UAVs based remote sensing of different traits with different phenotyping sensors. In this review, the UAVs platforms and the phenotyping sensors were briefly introduced. The applications of UAVs to obtain and analyze plant phenotype traits were introduced and summarized by the traits in a more comprehensive way. A comparison of different phenotyping sensors was conducted. Furthermore, the challenges and future prospects of phenotype information acquisition and data analysis using UAVs as remote sensing platforms were also discussed. Since the current studies from various countries and researchers were fragmented to just explore the feasibility of UAVs based high-throughput phenotyping, this review aimed to provide the researchers and readers the current applications of UAVs for high-throughput phenotyping and how the studies were conducted, provide guidelines for future studies.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.565",
        "scimago_value": "1,208"
    },
    {
        "issnkey": "02673649",
        "isbn": null,
        "journal": "Computer Law & Security Review",
        "publisher": null,
        "title": "euupdateofthecomputerlawsecurityreview",
        "booktitle": null,
        "doi": "10.1016/j.clsr.2021.105598",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,815"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821259-2",
        "journal": null,
        "publisher": "Academic Press",
        "title": "index",
        "booktitle": "Artificial Intelligence in Medicine",
        "doi": "10.1016/B978-0-12-821259-2.00030-2",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00128252",
        "isbn": null,
        "journal": "Earth-Science Reviews",
        "publisher": null,
        "title": "sensingtheoceanbiologicalcarbonpumpfromspaceareviewofcapabilitiesconceptsresearchgapsandfuturedevelopments",
        "booktitle": null,
        "doi": "10.1016/j.earscirev.2021.103604",
        "author": [
            "Brewin, Robert",
            "Sathyendranath, Shubha",
            "Platt, Trevor",
            "Bouman, Heather",
            "Ciavatta, Stefano",
            "Dall'Olmo, Giorgio",
            "Dingle, James",
            "Groom, Steve",
            "J\u00f6nsson, Bror",
            "Kostadinov, Tihomir",
            "Kulk, Gemma",
            "Laine, Marko",
            "Mart\u00ednez-Vicente, Victor",
            "Psarra, Stella",
            "Raitsos, Dionysios",
            "Richardson, Katherine",
            "Rio, Marie-H\u00e9l\u00e8ne",
            "Rousseaux, C\u00e9cile",
            "Salisbury, Joe",
            "Shutler, Jamie",
            "Walker, Peter"
        ],
        "keywords": [
            "Ocean, Carbon cycle, Satellite, Biology"
        ],
        "abstract": "The element carbon plays a central role in climate and life on Earth. It is capable of moving among the geosphere, cryosphere, atmosphere, biosphere and hydrosphere. This flow of carbon is referred to as the Earth's carbon cycle. It is also intimately linked to the cycling of other elements and compounds. The ocean plays a fundamental role in Earth's carbon cycle, helping to regulate atmospheric CO2 concentration. The ocean biological carbon pump (OBCP), defined as a set of processes that transfer organic carbon from the surface to the deep ocean, is at the heart of the ocean carbon cycle. Monitoring the OBCP is critical to understanding how the Earth's carbon cycle is changing. At present, satellite remote sensing is the only tool available for viewing the entire surface ocean at high temporal and spatial scales. In this paper, we review methods for monitoring the OBCP with a focus on satellites. We begin by providing an overview of the OBCP, defining and describing the pools of carbon in the ocean, and the processes controlling fluxes of carbon between the pools, from the surface to the deep ocean, and among ocean, land and atmosphere. We then examine how field measurements, from ship and autonomous platforms, complement satellite observations, provide validation points for satellite products and lead to a more complete view of the OBCP than would be possible from satellite observations alone. A thorough analysis is then provided on methods used for monitoring the OBCP from satellite platforms, covering current capabilities, concepts and gaps, and the requirement for uncertainties in satellite products. We finish by discussing the potential for producing a satellite-based carbon budget for the oceans, the advantages of integrating satellite-based observations with ecosystem models and field measurements, and future opportunities in space, all with a view towards bringing satellite observations into the limelight of ocean carbon research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "12.413",
        "scimago_value": "3,893"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-823768-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "subjectindex",
        "booktitle": "Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in R and BUGS",
        "doi": "10.1016/B978-0-12-809585-0.20002-1",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821777-1",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter13machinelearningforoptimizinghealthcareresources",
        "booktitle": "Machine Learning, Big Data, and IoT for Medical Informatics",
        "doi": "10.1016/B978-0-12-821777-1.00020-3",
        "author": [
            "Tawhid, Abdalrahman",
            "Teotia, Tanya",
            "Elmiligi, Haytham"
        ],
        "keywords": [
            "Chronic diseases, Public Health Agency of Canada, Diabetic patients, Cross-validation"
        ],
        "abstract": "Optimizing healthcare resources is a major goal for any healthcare administrator. The significance of such a goal becomes very clear during pandemics. Access to such resources at the right time affects the quality of healthcare services and provides alternative treatments that can save patients\u2019 lives. Achieving this goal becomes more challenging when there are large number of patients who have chronic diseases. Machine learning algorithms provide a very promising solution that can help healthcare administrators make the right decision at the right time. Machine learning model can predict the progress of pandemics, classify a patient with well-defined symptoms as contagious or not, and can also predict the number of patients who will be hospitalized in the future. This chapter shows how to utilize machine learning algorithms to create a models that can predict some of the key issues in healthcare systems. The discussion in the chapter relates to COVID-19 pandemic and highlights the solutions offered by machine learning in such scenarios. The chapter also highlights the significance of feature engineering and its impact on the accuracy of machine learning models. The chapter ends with two case studies. The first case study shows how to build a prediction model that can predict the number of diabetic patients who will visit certain hospitals in a specific geographic location in future years. The second case study analyzes health records during the COVID-19 pandemic.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-08-102909-1",
        "journal": null,
        "publisher": "Academic Press",
        "title": "index",
        "booktitle": "Encyclopedia of Geology (Second Edition)",
        "doi": "10.1016/B978-0-08-102908-4.09958-6",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "25892371",
        "isbn": null,
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter17theroleoffuturescienceandtechnologiesinwatermanagement",
        "booktitle": "Murray-Darling Basin, Australia",
        "doi": "10.1016/B978-0-12-818152-2.00017-6",
        "author": [
            "Donnelly, Chantal",
            "Lymburner, Leo",
            "Bende-Michl, Ulrike",
            "Frost, Andrew",
            "Rodriguez, Eva"
        ],
        "keywords": [
            "Future science, Technology, Water resources information, Management"
        ],
        "abstract": "Water decision-making in the Murray\u2013Darling Basin by government agencies, utilities, water holders, and farmers is complex and involves use of technical, environmental, economic, social, and cultural information. Confidence in the decisions is dependent upon the quality of the information, the transparency of the process used to generate the information, the availability of the data and information used, and the quality and transparency of the models used to bring together different data and information sources. This chapter discusses likely scenarios for how current and future science and technology might improve information availability in the Murray\u2013Darling Basin and contribute to improved decision-making. Possible future changes to information availability are discussed in three decision-making timeframes: near-term (now to next fortnight), seasonal (months to seasons ahead), and strategic (climatological timescales).",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "18745482",
        "isbn": null,
        "journal": "International Journal of Critical Infrastructure Protection",
        "publisher": null,
        "title": "securingscadaandcriticalindustrialsystemsfromneedstosecuritymechanisms",
        "booktitle": null,
        "doi": "10.1016/j.ijcip.2020.100394",
        "author": [
            "{Abou el Kalam}, Anas"
        ],
        "keywords": [
            "SCADA, Protection, Security, Access control, Integrity, Intrusion tolerance, Self-healing, Availability, Threat, Risks, Security policies and models"
        ],
        "abstract": "Supervisory control and data acquisition (SCADA) systems are used in critical infrastructure to control vital sectors such as smart grids, oil pipelines, water treatment, chemical manufacturing plants, etc. Any malicious or accidental intrusion could cause dramatic human, material and economic damages. Thus, the security of the SCADA is very important, not only to keep the continuity of services (i.e., availability) against hostile and cyber-terrorist attacks, but also to ensure the resilience and integrity of processes and actions. Dealing with this issue, this paper discusses SCADA vulnerabilities and security threats, with a focus on recent ones. Then, we define a holistic methodology to derive the suitable security mechanisms for this kind of critical systems. Our methodology starts by identifying the security needs and objectives, specifying the security policies and models, deriving the adapted architecture and, finally, implementing the security mechanisms that satisfy the needs and cover the risks. We focus on the modelling step by proposing the new CI-OrBAC model. In this paper, we focused on securing communication and protecting SCADA against both internal and external threats while satisfying the self-healing, intrusion tolerance, integrity, scalability and collaboration needs.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "2.865",
        "scimago_value": "0,650"
    },
    {
        "issnkey": "09596526",
        "isbn": null,
        "journal": "Journal of Cleaner Production",
        "publisher": null,
        "title": "aglobalreviewofconsumerbehaviortowardsewasteandimplicationsforthecirculareconomy",
        "booktitle": null,
        "doi": "10.1016/j.jclepro.2021.128297",
        "author": [
            "Islam, Md",
            "Huda, Nazmul",
            "Baumber, Alex",
            "Shumon, Rezaul",
            "Zaman, Atiq",
            "Ali, Forkan",
            "Hossain, Rumana",
            "Sahajwalla, Veena"
        ],
        "keywords": [
            "Consumer behavior, Waste electrical and electronic equipment (WEEE), Sustainable production and consumption, Disposal, Recycling, Literature review"
        ],
        "abstract": "To tackle the alarming increase in e-waste or end-of-life (EoL) electronic products, consumer behavior towards the end of their useful life needs to be thoroughly studied. End users or consumers are the starting point where e-waste starts its journey into several paths within the circular economy (CE), such as repair, reuse, remanufacturing, and recycling. E-waste often ends up in landfill due to improper disposal of e-waste with household waste by consumers. Studying consumer behavior allows for the identification of appropriate approaches to achieve CE. Numerous academic journal papers have been published concerning consumers' e-waste-related knowledge and awareness, and behavior on consumption, disposal, storage, recycling, and repair. Substantial knowledge gap exists around how understandings of consumer behavior around e-waste may be integrated into the CE model. This article aims to reduce this gap by reviewing 109 research papers published in international peer-reviewed journals identified in the Web of Science (WoS) core collection database, using content analysis methodology to analyze and review the articles. The study aims to provide invaluable input for developing a more consumer-centric CE framework for both policymakers and researchers seeking to advance knowledge and implementation strategies around e-waste. This is one of the earliest systematic reviews of studies on consumer behavior around e-waste. The study results show that consumers' disposal and recycling behaviors are the two main areas of research interest in the studies reviewed. In contrast, reuse and repair behavior were investigated to a lesser extent. In this study, several research gaps and areas for future research are identified, along with suggestions for a CE framework focusing on the e-waste sector that, encompasses policy initiatives and business model innovations. The identified studies presented here offer a valuable starting point for researchers who are starting to work on consumer behavior-related e-waste research.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "9.297",
        "scimago_value": "1,937"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-820193-0",
        "journal": null,
        "publisher": "Gulf Professional Publishing",
        "title": "appendix",
        "booktitle": "Drilling Engineering",
        "doi": "10.1016/B978-0-12-820193-0.15001-4",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "26673258",
        "isbn": null,
        "journal": "Fundamental Research",
        "publisher": null,
        "title": "knowledgegraphqualitycontrolasurvey",
        "booktitle": null,
        "doi": "10.1016/j.fmre.2021.09.003",
        "author": [
            "Wang, Xiangyu",
            "Chen, Lyuzhou",
            "Ban, Taiyu",
            "Usman, Muhammad",
            "Guan, Yifeng",
            "Liu, Shikang",
            "Wu, Tianhao",
            "Chen, Huanhuan"
        ],
        "keywords": [
            "Knowledge graph, Quality control, Quality evaluation, Quality enhancement"
        ],
        "abstract": "A knowledge graph (KG), a special form of semantic network, integrates fragmentary data into a graph to support knowledge processing and reasoning. KG quality control is important to the utility of KGs. It is essential to investigate KG quality and the parameters influencing KG quality to better understand its quality control. Although many works have been conducted to evaluate the dimensions of KG quality, quality control of the construction process, and enhancement methods for quality, a comprehensive literature review has not been presented on this topic. This paper intends to fill this research gap by presenting a comprehensive survey on the quality control of KGs. First, this paper defines six main evaluation dimensions of KG quality and investigates their correlations and differences. Second, quality control treatments during KG construction are introduced from the perspective of these dimensions of KG quality. Third, the quality enhancement of a constructed KG is described from various dimensions. This paper ultimately aims to promote the research and applications of KGs.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13837621",
        "isbn": null,
        "journal": "Journal of Systems Architecture",
        "publisher": null,
        "title": "alightweightverifiabletrustbaseddatacollectionapproachforsensorcloudsystems",
        "booktitle": null,
        "doi": "10.1016/j.sysarc.2021.102219",
        "author": [
            "Guo, Jiawei",
            "Wang, Haoyang",
            "Liu, Wei",
            "Huang, Guosheng",
            "Gui, Jinsong",
            "Zhang, Shaobo"
        ],
        "keywords": [
            "Sensor\u2013cloud systems, Trustworthiness, Data collection, Mobile vehicles, Unmanned aerial vehicles"
        ],
        "abstract": "A Lightweight Verifiable Trust based Data Collection (LVT-DC) approach is proposed to obtain credible data for mobile vehicles network, in which, a large number of IoT devices are deployed in smart city, and Mobile Vehicles (MVs) collect data from IoT devices and report data to cloud to construct various applications. First, some IoT devices are selected as core-IoT device whose data more than others. Then, Unmanned Aerial Vehicles (UAV) delivers a short verification code to each core-IoT devices and collects its data, and it embeds verification code into the data when the MVs collect it in order for cloud to check. Last, a verifiable trust inference method is proposed which includes two types of trust calculations. (a) Direct trust: MVs If the reported data cannot recover the verification code or is inconsistent with the UAV collection result, reduce the trust of the MVs, (b) Trust inference: If the data of a non-core-IoT devices reported by a MVs is inconsistent with the trusted MVs, then reduce its trust. After a large number of experimental results, the LVT-DC approach can quickly and accurately identify the credibility of the data collector and ensure credible data collection.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.777",
        "scimago_value": "0,598"
    },
    {
        "issnkey": "10848045",
        "isbn": null,
        "journal": "Journal of Network and Computer Applications",
        "publisher": null,
        "title": "acomprehensivesurveyandtaxonomyofthesvmbasedintrusiondetectionsystems",
        "booktitle": null,
        "doi": "10.1016/j.jnca.2021.102983",
        "author": [
            "Mohammadi, Mokhtar",
            "Rashid, Tarik",
            "Karim, Sarkhel",
            "Aldalwie, Adil",
            "Tho, Quan",
            "Bidaki, Moazam",
            "Rahmani, Amir",
            "Hosseinzadeh, Mehdi"
        ],
        "keywords": [
            "SVM, Anomaly, Multi-class SVM, Feature selection, Intrusion detection, PCA"
        ],
        "abstract": "The increasing number of security attacks have inspired researchers to employ various classifiers, such as support vector machines (SVMs), to deal with them in Intrusion detection systems (IDSs). This paper presents a comprehensive study and investigation of the SVM-based intrusion detection and feature selection systems proposed in the literature. It first presents the essential concepts and background knowledge about security attacks, IDS, and SVM classifiers. It then provides a taxonomy of the SVM-based IDS schemes and describes how they have adapted numerous types of SVM classifiers in detecting various types of anomalies and intrusions. Moreover, it discusses the main contributions of the investigated schemes and highlights the algorithms and techniques combined with the SVM to enhance its detection rate and accuracy. Finally, different properties and limitations of the SVM-based IDS schemes are discussed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-818779-1",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter1smartcitiesandopendata",
        "booktitle": "Management of IOT Open Data Projects in Smart Cities",
        "doi": "10.1016/B978-0-12-818779-1.00001-8",
        "author": [
            "Orlowski, Cezary"
        ],
        "keywords": [
            "Smart Cities, open data, Internet of Things networks, business models, software project management"
        ],
        "abstract": "This chapter is devoted to Smart Cities and the importance of open data for their development. We, therefore, address the complexity of the Smart Cities environment for generating, collecting, and using open data from Internet of Things devices. The characteristics for the development of both Smart Cities and open data are discussed. We also present sample projects, whose product was embodied via open data. On this basis, we determine the business and legal conditions of Smart Cities for the acquisition, processing, and use of open data. In this way we intend to demonstrate the complexity of open data project management processes and to demonstrate the need to analyze existing managerial methods.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "00401625",
        "isbn": null,
        "journal": "Technological Forecasting and Social Change",
        "publisher": null,
        "title": "delegatingdecisionmakingtoautonomousproductsavaluemodelemphasizingtheroleofwellbeing",
        "booktitle": null,
        "doi": "10.1016/j.techfore.2021.120846",
        "author": [
            "BERTRANDIAS, Laurent",
            "LOWE, Ben",
            "SADIK-ROZSNYAI, Orsolya",
            "CARRICANO, Manu"
        ],
        "keywords": [
            "Perceived value, Autonomous cars, Autonomous products, Well-being, Consumers"
        ],
        "abstract": "Given the rapid growth of autonomous technologies it is important to understand how consumers attribute value to them. Such technologies require consumers to give up control to a machine by delegating decision-making power. To better understand value perceptions, and ultimately adoption, this paper proposes a conceptual model that explains the value attributed to autonomous cars as an archetypal consumer autonomous technology. The model is developed from literature around the theme of autonomy and two qualitative studies, which identify consumers\u2019 perceived individual benefits (Freeing Time, Overcoming Human Weaknesses, Outperforming Human Capacities), risks (Loss of Competencies, Security and Privacy risk, Performance risk) and their proximal antecedents (Perceived Expertise, Attitude toward the Delegated Task, Previous Engagement in Delegation). The model is tested on a national sample of French drivers using a quantitative methodology and Structural Equation Modelling (SEM). This research contributes to literature on technological forecasting of autonomous technologies by developing and testing a conceptual model, which includes salient predictors of perceived value and highlights the mediating role of improvement in subjective well-being that consumers anticipate from adoption. The model can be used by managers to predict how users are likely to react to their products and communications about them.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.593",
        "scimago_value": "2,226"
    },
    {
        "issnkey": "01406736",
        "isbn": null,
        "journal": "The Lancet",
        "publisher": null,
        "title": "measuringroutinechildhoodvaccinationcoveragein204countriesandterritories19802019asystematicanalysisfortheglobalburdenofdiseasestudy2020release1",
        "booktitle": null,
        "doi": "10.1016/S0140-6736(21)00984-3",
        "author": [
            "Galles, Natalie",
            "Liu, Patrick",
            "Updike, Rachel",
            "Fullman, Nancy",
            "Nguyen, Jason",
            "Rolfe, Sam",
            "Sbarra, Alyssa",
            "Schipp, Megan",
            "Marks, Ashley",
            "Abady, Gdiom",
            "Abbas, Kaja",
            "Abbasi, Sumra",
            "Abbastabar, Hedayat",
            "Abd-Allah, Foad",
            "Abdoli, Amir",
            "Abolhassani, Hassan",
            "Abosetugn, Akine",
            "Adabi, Maryam",
            "Adamu, Abdu",
            "Adetokunboh, Olatunji",
            "Adnani, Qorinah",
            "Advani, Shailesh",
            "Afzal, Saira",
            "Aghamir, Seyed",
            "Ahinkorah, Bright",
            "Ahmad, Sohail",
            "Ahmad, Tauseef",
            "Ahmadi, Sepideh",
            "Ahmed, Haroon",
            "Ahmed, Muktar",
            "{Ahmed Rashid}, Tarik",
            "{Ahmed Salih}, Yusra",
            "Akalu, Yonas",
            "Aklilu, Addis",
            "Akunna, Chisom",
            "{Al Hamad}, Hanadi",
            "Alahdab, Fares",
            "Albano, Luciana",
            "Alemayehu, Yosef",
            "Alene, Kefyalew",
            "Al-Eyadhy, Ayman",
            "Alhassan, Robert",
            "Ali, Liaqat",
            "Aljunid, Syed",
            "Almustanyir, Sami",
            "Altirkawi, Khalid",
            "Alvis-Guzman, Nelson",
            "Amu, Hubert",
            "Andrei, Catalina",
            "Andrei, Tudorel",
            "Ansar, Adnan",
            "Ansari-Moghaddam, Alireza",
            "Antonazzo, Ippazio",
            "Antony, Benny",
            "Arabloo, Jalal",
            "Arab-Zozani, Morteza",
            "Artanti, Kurnia",
            "Arulappan, Judie",
            "Awan, Asma",
            "Awoke, Mamaru",
            "Ayza, Muluken",
            "Azarian, Ghasem",
            "Azzam, Ahmed",
            "B, Darshan",
            "Babar, Zaheer-Ud-Din",
            "Balakrishnan, Senthilkumar",
            "Banach, Maciej",
            "Bante, Simachew",
            "B\u00e4rnighausen, Till",
            "Barqawi, Hiba",
            "Barrow, Amadou",
            "Bassat, Quique",
            "Bayarmagnai, Narantuya",
            "{Bejarano Ramirez}, Diana",
            "Bekuma, Tariku",
            "Belay, Habtamu",
            "Belgaumi, Uzma",
            "Bhagavathula, Akshaya",
            "Bhandari, Dinesh",
            "Bhardwaj, Nikha",
            "Bhardwaj, Pankaj",
            "Bhaskar, Sonu",
            "Bhattacharyya, Krittika",
            "Bibi, Sadia",
            "Bijani, Ali",
            "Biondi, Antonio",
            "Boloor, Archith",
            "Braithwaite, Dejana",
            "Buonsenso, Danilo",
            "Butt, Zahid",
            "Camargos, Paulo",
            "Carreras, Giulia",
            "Carvalho, Felix",
            "Casta\u00f1eda-Orjuela, Carlos",
            "Chakinala, Raja",
            "Charan, Jaykaran",
            "Chatterjee, Souranshu",
            "Chattu, Soosanna",
            "Chattu, Vijay",
            "Chowdhury, Fazle",
            "Christopher, Devasahayam",
            "Chu, Dinh-Toi",
            "Chung, Sheng-Chia",
            "Cortesi, Paolo",
            "Costa, Vera",
            "Couto, Rosa",
            "Dadras, Omid",
            "Dagnew, Amare",
            "Dagnew, Baye",
            "Dai, Xiaochen",
            "Dandona, Lalit",
            "Dandona, Rakhi",
            "{De Neve}, Jan-Walter",
            "{Derbew Molla}, Meseret",
            "Derseh, Behailu",
            "Desai, Rupak",
            "Desta, Abebaw",
            "Dhamnetiya, Deepak",
            "Dhimal, Mandira",
            "Dhimal, Meghnath",
            "Dianatinasab, Mostafa",
            "Diaz, Daniel",
            "Djalalinia, Shirin",
            "Dorostkar, Fariba",
            "Edem, Bassey",
            "Edinur, Hisham",
            "Eftekharzadeh, Sahar",
            "{El Sayed}, Iman",
            "{El Sayed Zaki}, Maysaa",
            "Elhadi, Muhammed",
            "El-Jaafary, Shaimaa",
            "Elsharkawy, Aisha",
            "Enany, Shymaa",
            "Erkhembayar, Ryenchindorj",
            "Esezobor, Christopher",
            "Eskandarieh, Sharareh",
            "Ezeonwumelu, Ifeanyi",
            "Ezzikouri, Sayeh",
            "Fares, Jawad",
            "Faris, Pawan",
            "Feleke, Berhanu",
            "Ferede, Tomas",
            "Fernandes, Eduarda",
            "Fernandes, Jo\u00e3o",
            "Ferrara, Pietro",
            "Filip, Irina",
            "Fischer, Florian",
            "Francis, Mark",
            "Fukumoto, Takeshi",
            "Gad, Mohamed",
            "Gaidhane, Shilpa",
            "Gallus, Silvano",
            "Garg, Tushar",
            "Geberemariyam, Biniyam",
            "Gebre, Teshome",
            "Gebregiorgis, Birhan",
            "Gebremedhin, Ketema",
            "Gebremichael, Berhe",
            "Gessner, Bradford",
            "Ghadiri, Keyghobad",
            "Ghafourifard, Mansour",
            "Ghashghaee, Ahmad",
            "Gilani, Syed",
            "Gl\u0103van, Ionela-Roxana",
            "Glushkova, Ekaterina",
            "Golechha, Mahaveer",
            "Gonfa, Kebebe",
            "Gopalani, Sameer",
            "Goudarzi, Houman",
            "Gubari, Mohammed",
            "Guo, Yuming",
            "Gupta, Veer",
            "Gupta, Vivek",
            "Guti\u00e9rrez, Reyna",
            "Haeuser, Emily",
            "Halwani, Rabih",
            "Hamidi, Samer",
            "Hanif, Asif",
            "Haque, Shafiul",
            "Harapan, Harapan",
            "Hargono, Arief",
            "Hashi, Abdiwahab",
            "Hassan, Shoaib",
            "Hassanein, Mohamed",
            "Hassanipour, Soheil",
            "Hassankhani, Hadi",
            "Hay, Simon",
            "Hayat, Khezar",
            "Hegazy, Mohamed",
            "Heidari, Golnaz",
            "Hezam, Kamal",
            "Holla, Ramesh",
            "Hoque, Mohammad",
            "Hosseini, Mostafa",
            "Hosseinzadeh, Mehdi",
            "Hostiuc, Mihaela",
            "Househ, Mowafa",
            "Hsieh, Vivian",
            "Huang, Junjie",
            "Humayun, Ayesha",
            "Hussain, Rabia",
            "Hussein, Nawfal",
            "Ibitoye, Segun",
            "Ilesanmi, Olayinka",
            "Ilic, Irena",
            "Ilic, Milena",
            "Inamdar, Sumant",
            "Iqbal, Usman",
            "Irham, Lalu",
            "Irvani, Seyed",
            "Islam, Sheikh",
            "Ismail, Nahlah",
            "Itumalla, Ramaiah",
            "Jha, Ravi",
            "Joukar, Farahnaz",
            "Kabir, Ali",
            "Kabir, Zubair",
            "Kalhor, Rohollah",
            "Kamal, Zul",
            "Kamande, Stanley",
            "Kandel, Himal",
            "Karch, Andr\u00e9",
            "Kassahun, Getinet",
            "Kassebaum, Nicholas",
            "Katoto, Patrick",
            "Kelkay, Bayew",
            "Kengne, Andre",
            "Khader, Yousef",
            "Khajuria, Himanshu",
            "Khalil, Ibrahim",
            "Khan, Ejaz",
            "Khan, Gulfaraz",
            "Khan, Junaid",
            "Khan, Maseer",
            "Khan, Moien",
            "Khang, Young-Ho",
            "Khoja, Abdullah",
            "Khubchandani, Jagdish",
            "Kim, Gyu",
            "Kim, Min",
            "Kim, Yun",
            "Kimokoti, Ruth",
            "Kisa, Adnan",
            "Kisa, Sezer",
            "Korshunov, Vladimir",
            "Kosen, Soewarta",
            "{Kuate Defo}, Barthelemy",
            "Kulkarni, Vaman",
            "Kumar, Avinash",
            "Kumar, G",
            "Kumar, Nithin",
            "Kwarteng, Alexander",
            "{La Vecchia}, Carlo",
            "Lami, Faris",
            "Landires, Iv\u00e1n",
            "Lasrado, Savita",
            "Lassi, Zohra",
            "Lee, Hankil",
            "Lee, Yeong",
            "Levi, Miriam",
            "Lewycka, Sonia",
            "Li, Shanshan",
            "Liu, Xuefeng",
            "Lobo, Stany",
            "Lopukhov, Platon",
            "Lozano, Rafael",
            "{Lutzky Saute}, Ricardo",
            "{Magdy Abd El Razek}, Muhammed",
            "Makki, Alaa",
            "Malik, Ahmad",
            "Mansour-Ghanaei, Fariborz",
            "Mansournia, Mohammad",
            "Mantovani, Lorenzo",
            "Martins-Melo, Francisco",
            "Matthews, Philippa",
            "Medina, John",
            "Mendoza, Walter",
            "Menezes, Ritesh",
            "Mengesha, Endalkachew",
            "Meretoja, Tuomo",
            "Mersha, Amanual",
            "Mesregah, Mohamed",
            "Mestrovic, Tomislav",
            "Miazgowski, Bartosz",
            "Milne, George",
            "Mirica, Andreea",
            "Mirrakhimov, Erkin",
            "Mirzaei, Hamid",
            "Misra, Sanjeev",
            "Mithra, Prasanna",
            "Moghadaszadeh, Masoud",
            "Mohamed, Teroj",
            "Mohammad, Karzan",
            "Mohammad, Yousef",
            "Mohammadi, Mokhtar",
            "Mohammadian-Hafshejani, Abdollah",
            "Mohammed, Arif",
            "Mohammed, Shafiu",
            "Mohapatra, Archisman",
            "Mokdad, Ali",
            "Molokhia, Mariam",
            "Monasta, Lorenzo",
            "Moni, Mohammad",
            "Montasir, Ahmed",
            "Moore, Catrin",
            "Moradi, Ghobad",
            "Moradzadeh, Rahmatollah",
            "Moraga, Paula",
            "Mueller, Ulrich",
            "Munro, Sandra",
            "Naghavi, Mohsen",
            "Naimzada, Mukhammad",
            "Naveed, Muhammad",
            "Nayak, Biswa",
            "Negoi, Ionut",
            "{Neupane Kandel}, Sandhya",
            "Nguyen, Trang",
            "Nikbakhsh, Rajan",
            "Ningrum, Dina",
            "Nixon, Molly",
            "Nnaji, Chukwudi",
            "Noubiap, Jean",
            "Nu\u00f1ez-Samudio, Virginia",
            "Nwatah, Vincent",
            "Oancea, Bogdan",
            "Ochir, Chimedsuren",
            "Ogbo, Felix",
            "Olagunju, Andrew",
            "Olakunde, Babayemi",
            "Onwujekwe, Obinna",
            "Otstavnov, Nikita",
            "Otstavnov, Stanislav",
            "Owolabi, Mayowa",
            "Padubidri, Jagadish",
            "Pakshir, Keyvan",
            "Park, Eun-Cheol",
            "{Pashazadeh Kan}, Fatemeh",
            "Pathak, Mona",
            "Paudel, Rajan",
            "Pawar, Shrikant",
            "Pereira, Jeevan",
            "Peres, Mario",
            "Perianayagam, Arokiasamy",
            "Pinheiro, Marina",
            "Pirestani, Majid",
            "Podder, Vivek",
            "Polibin, Roman",
            "Pollok, Richard",
            "Postma, Maarten",
            "Pottoo, Faheem",
            "Rabiee, Mohammad",
            "Rabiee, Navid",
            "Radfar, Amir",
            "Rafiei, Alireza",
            "Rahimi-Movaghar, Vafa",
            "Rahman, Mosiur",
            "Rahmani, Amir",
            "Rahmawaty, Setyaningrum",
            "Rajesh, Aashish",
            "Ramshaw, Rebecca",
            "Ranasinghe, Priyanga",
            "Rao, Chythra",
            "Rao, Sowmya",
            "Rathi, Priya",
            "Rawaf, David",
            "Rawaf, Salman",
            "Renzaho, Andre",
            "Rezaei, Negar",
            "Rezai, Mohammad",
            "Rios-Blancas, Maria",
            "Rogowski, Emma",
            "Ronfani, Luca",
            "Rwegerera, Godfrey",
            "Saad, Anas",
            "Sabour, Siamak",
            "Saddik, Basema",
            "Saeb, Mohammad",
            "Saeed, Umar",
            "Sahebkar, Amirhossein",
            "Sahraian, Mohammad",
            "Salam, Nasir",
            "Salimzadeh, Hamideh",
            "Samaei, Mehrnoosh",
            "Samy, Abdallah",
            "Sanabria, Juan",
            "Sanmarchi, Francesco",
            "Santric-Milicevic, Milena",
            "Sartorius, Benn",
            "Sarveazad, Arash",
            "Sathian, Brijesh",
            "Sawhney, Monika",
            "Saxena, Deepak",
            "Saxena, Sonia",
            "Seidu, Abdul-Aziz",
            "Seylani, Allen",
            "Shaikh, Masood",
            "Shamsizadeh, Morteza",
            "Shetty, Pavanchand",
            "Shigematsu, Mika",
            "Shin, Jae",
            "Sidemo, Negussie",
            "Singh, Ambrish",
            "Singh, Jasvinder",
            "Sinha, Smriti",
            "Skryabin, Valentin",
            "Skryabina, Anna",
            "Soheili, Amin",
            "Tadesse, Eyayou",
            "Tamiru, Animut",
            "Tan, Ker-Kan",
            "Tekalegn, Yohannes",
            "Temsah, Mohamad-Hani",
            "Thakur, Bhaskar",
            "Thapar, Rekha",
            "Thavamani, Aravind",
            "Tobe-Gai, Ruoyan",
            "Tohidinik, Hamid",
            "Tovani-Palone, Marcos",
            "Traini, Eugenio",
            "Tran, Bach",
            "Tripathi, Manjari",
            "Tsegaye, Berhan",
            "Tsegaye, Gebiyaw",
            "Ullah, Anayat",
            "Ullah, Saif",
            "Ullah, Sana",
            "Unim, Brigid",
            "Vacante, Marco",
            "Velazquez, Diana",
            "Vo, Bay",
            "Vollmer, Sebastian",
            "Vu, Giang",
            "Vu, Linh",
            "Waheed, Yasir",
            "Winkler, Andrea",
            "Wiysonge, Charles",
            "Yi\u011fit, Vahit",
            "Yirdaw, Birhanu",
            "Yon, Dong",
            "Yonemoto, Naohiro",
            "Yu, Chuanhua",
            "Yuce, Deniz",
            "Yunusa, Ismaeel",
            "Zamani, Mohammad",
            "Zamanian, Maryam",
            "Zewdie, Dejene",
            "Zhang, Zhi-Jiang",
            "Zhong, Chenwen",
            "Zumla, Alimuddin",
            "Murray, Christopher",
            "Lim, Stephen",
            "Mosser, Jonathan"
        ],
        "keywords": [
            ""
        ],
        "abstract": "Summary Background Measuring routine childhood vaccination is crucial to inform global vaccine policies and programme implementation, and to track progress towards targets set by the Global Vaccine Action Plan (GVAP) and Immunization Agenda 2030. Robust estimates of routine vaccine coverage are needed to identify past successes and persistent vulnerabilities. Drawing from the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2020, Release 1, we did a systematic analysis of global, regional, and national vaccine coverage trends using a statistical framework, by vaccine and over time. Methods For this analysis we collated 55 326 country-specific, cohort-specific, year-specific, vaccine-specific, and dose-specific observations of routine childhood vaccination coverage between 1980 and 2019. Using spatiotemporal Gaussian process regression, we produced location-specific and year-specific estimates of 11 routine childhood vaccine coverage indicators for 204 countries and territories from 1980 to 2019, adjusting for biases in country-reported data and reflecting reported stockouts and supply disruptions. We analysed global and regional trends in coverage and numbers of zero-dose children (defined as those who never received a diphtheria-tetanus-pertussis [DTP] vaccine dose), progress towards GVAP targets, and the relationship between vaccine coverage and sociodemographic development. Findings By 2019, global coverage of third-dose DTP (DTP3; 81\u00b76% [95% uncertainty interval 80\u00b74\u201382\u00b77]) more than doubled from levels estimated in 1980 (39\u00b79% [37\u00b75\u201342\u00b71]), as did global coverage of the first-dose measles-containing vaccine (MCV1; from 38\u00b75% [35\u00b74\u201341\u00b73] in 1980 to 83\u00b76% [82\u00b73\u201384\u00b78] in 2019). Third-dose polio vaccine (Pol3) coverage also increased, from 42\u00b76% (41\u00b74\u201344\u00b71) in 1980 to 79\u00b78% (78\u00b74\u201381\u00b71) in 2019, and global coverage of newer vaccines increased rapidly between 2000 and 2019. The global number of zero-dose children fell by nearly 75% between 1980 and 2019, from 56\u00b78 million (52\u00b76\u201360\u00b79) to 14\u00b75 million (13\u00b74\u201315\u00b79). However, over the past decade, global vaccine coverage broadly plateaued; 94 countries and territories recorded decreasing DTP3 coverage since 2010. Only 11 countries and territories were estimated to have reached the national GVAP target of at least 90% coverage for all assessed vaccines in 2019. Interpretation After achieving large gains in childhood vaccine coverage worldwide, in much of the world this progress was stalled or reversed from 2010 to 2019. These findings underscore the importance of revisiting routine immunisation strategies and programmatic approaches, recentring service delivery around equity and underserved populations. Strengthening vaccine data and monitoring systems is crucial to these pursuits, now and through to 2030, to ensure that all children have access to, and can benefit from, lifesaving vaccines. Funding Bill & Melinda Gates Foundation.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "13,103"
    },
    {
        "issnkey": "25890042",
        "isbn": null,
        "journal": "iScience",
        "publisher": null,
        "title": "anintegratedmultiomicanalysisofipscderivedmotorneuronsfromc9orf72alspatients",
        "booktitle": null,
        "doi": "10.1016/j.isci.2021.103221",
        "author": [
            "Phatnani, Hemali",
            "Kwan, Justin",
            "Sareen, Dhruv",
            "Broach, James",
            "Simmons, Zachary",
            "Arcila-Londono, Ximena",
            "Lee, Edward",
            "{Van Deerlin}, Vivianna",
            "Shneider, Neil",
            "Fraenkel, Ernest",
            "Ostrow, Lyle",
            "Baas, Frank",
            "Zaitlen, Noah",
            "Berry, James",
            "Malaspina, Andrea",
            "Fratta, Pietro",
            "Cox, Gregory",
            "Thompson, Leslie",
            "Finkbeiner, Steve",
            "Dardiotis, Efthimios",
            "Miller, Timothy",
            "Chandran, Siddharthan",
            "Pal, Suvankar",
            "Hornstein, Eran",
            "MacGowan, Daniel",
            "Heiman-Patterson, Terry",
            "Hammell, Molly",
            "Patsopoulos, Nikolaos.A.",
            "Butovsky, Oleg",
            "Dubnau, Joshua",
            "Nath, Avindra",
            "Bowser, Robert",
            "Harms, Matt",
            "Poss, Mary",
            "Phillips-Cremins, Jennifer",
            "Crary, John",
            "Atassi, Nazem",
            "Lange, Dale",
            "Adams, Darius",
            "Stefanis, Leonidas",
            "Gotkine, Marc",
            "Baloh, Robert",
            "Babu, Suma",
            "Raj, Towfique",
            "Paganoni, Sabrina",
            "Shalem, Ophir",
            "Smith, Colin",
            "Zhang, Bin",
            "Harris, Brent",
            "Broce, Iris",
            "Drory, Vivian",
            "Ravits, John",
            "McMillan, Corey",
            "Menon, Vilas",
            "Wu, Lani",
            "Altschuler, Steven",
            "Li, Jonathan",
            "Lim, Ryan",
            "Kaye, Julia",
            "Dardov, Victoria",
            "Coyne, Alyssa",
            "Wu, Jie",
            "Milani, Pamela",
            "Cheng, Andrew",
            "Thompson, Terri",
            "Ornelas, Loren",
            "Frank, Aaron",
            "Adam, Miriam",
            "Banuelos, Maria",
            "Casale, Malcolm",
            "Cox, Veerle",
            "Escalante-Chong, Renan",
            "Daigle, J.",
            "Gomez, Emilda",
            "Hayes, Lindsey",
            "Holewenski, Ronald",
            "Lei, Susan",
            "Lenail, Alex",
            "Lima, Leandro",
            "Mandefro, Berhan",
            "Matlock, Andrea",
            "Panther, Lindsay",
            "Patel-Murray, Natasha",
            "Pham, Jacqueline",
            "Ramamoorthy, Divya",
            "Sachs, Karen",
            "Shelley, Brandon",
            "Stocksdale, Jennifer",
            "Trost, Hannah",
            "Wilhelm, Mark",
            "Venkatraman, Vidya",
            "Wassie, Brook",
            "Wyman, Stacia",
            "Yang, Stephanie",
            "{Van Eyk}, Jennifer",
            "Lloyd, Thomas",
            "Finkbeiner, Steven",
            "Fraenkel, Ernest",
            "Rothstein, Jeffrey",
            "Sareen, Dhruv",
            "Svendsen, Clive",
            "Thompson, Leslie"
        ],
        "keywords": [
            "Biological sciences, Neuroscience, Systems neuroscience, Systems biology, Omics"
        ],
        "abstract": "Summary Neurodegenerative diseases are challenging for systems biology because of the lack of reliable animal models or patient samples at early disease stages. Induced pluripotent stem cells (iPSCs) could address these challenges. We investigated DNA, RNA, epigenetics, and proteins in iPSC-derived motor neurons from patients with ALS carrying hexanucleotide expansions in C9ORF72. Using integrative computational methods combining all omics datasets, we identified novel and known dysregulated pathways. We used a C9ORF72 Drosophila model to distinguish pathways contributing to disease phenotypes from compensatory ones and confirmed alterations in some pathways in postmortem spinal cord tissue of patients with ALS. A different differentiation protocol was used to derive a separate set of C9ORF72 and control motor neurons. Many individual -omics differed by protocol, but some core dysregulated pathways were consistent. This strategy of analyzing patient-specific neurons provides disease-related outcomes with small numbers of heterogeneous lines and reduces variation from single-omics to elucidate network-based signatures.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.458",
        "scimago_value": "1,805"
    },
    {
        "issnkey": "09204105",
        "isbn": null,
        "journal": "Journal of Petroleum Science and Engineering",
        "publisher": null,
        "title": "4dseismichistorymatching",
        "booktitle": null,
        "doi": "10.1016/j.petrol.2021.109119",
        "author": [
            "Oliver, Dean",
            "Fossum, Kristian",
            "Bhakta, Tuhin",
            "Sand\u00f8, Ivar",
            "N\u00e6vdal, Geir",
            "Lorentzen, Rolf"
        ],
        "keywords": [
            "Time-lapse seismic, 4D seismic, History matching"
        ],
        "abstract": "Reservoir simulation models are used to forecast future reservoir behavior and to optimally manage reservoir production. These models require specification of hundreds of thousands of parameters, some of which may be determined from measurements along well paths, but the distance between wells can be large and the formations in which oil and gas are found are almost always heterogeneous with many geological complexities so many of the reservoir parameters are poorly constrained by well data. Additional constraints on the values of the parameters are provided by general geologic knowledge, and other constraints are provided by historical measurements of production and injection behavior. This type of information is often not sufficient to identify locations of either currently remaining oil, or to provide accurate forecasts where oil will remain at the end of project life. The repeated use of surface seismic surveys offers the promise of providing observations of locations of changes in physical properties between wells, thus reducing uncertainty in predictions of future reservoir behavior. Unfortunately, while methodologies for assimilation of 4D seismic data have demonstrated substantial value in synthetic model studies, the application to real fields has not been as successful. In this paper, we review the literature on 4D seismic history matching (SHM), focusing discussions on the aspects of the problem that make it more difficult than the more traditional production history matching. In particular, we discuss the possible choices for seismic attributes that can be used for comparison between observed or modeled attribute to determine the properties of the reservoir and the difficulty of estimating the magnitude of the noise or bias in the data. Depending on the level of matching, the bias may result from errors in the forward modeling, or errors in the inversion. Much of the practical literature has focused on methodologies for reducing the effect of bias or modeling error either through choice of attribute, or by appropriate weighting of data. Applications to field cases appear to have been at least partially successful, although quantitative assessment of the history matches and the improvements in forecast is difficult.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "4.346",
        "scimago_value": "0,975"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-823768-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter10integratedmodelsformultipletypesofdata",
        "booktitle": "Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in R and BUGS",
        "doi": "10.1016/B978-0-12-809585-0.00010-7",
        "author": [
            "K\u00e9ry, Marc",
            "Royle, J."
        ],
        "keywords": [
            "Abundance, Citizen science, Data fusion, Distribution, IM, IPM, Population dynamics, SDM"
        ],
        "abstract": "Integrated models (IMs) represent one of the major research avenues in ecological statistics during the past couple of decades. They are hierarchical models for integrating two or more, disparate, data sets simultaneously. We cover a special case, integrated population models (IPMs), and emphasize the general principles of data integration, or fusion, which underlie all other types of IMs. These are powerful and sensible models, because they utilize all available information in an estimation problem, usually yield estimates with greater precision, and sometimes enable estimation of additional parameter that are not estimable with each individual data set alone. We discuss six widely different examples of IMs, which are useful by themselves and furthermore illustrate the very broad scope of IMs.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03608352",
        "isbn": null,
        "journal": "Computers & Industrial Engineering",
        "publisher": null,
        "title": "internetofthingsandoccupationalwellbeinginindustry40asystematicmappingstudyandtaxonomy",
        "booktitle": null,
        "doi": "10.1016/j.cie.2021.107670",
        "author": [
            "Bavaresco, Rodrigo",
            "Arruda, Helder",
            "Rocha, Eduarda",
            "Barbosa, Jorge",
            "Li, Guann-Pyng"
        ],
        "keywords": [
            "Internet of Things, Industry 4.0, Occupational Health and Safety, Occupational Well-being, Human-Centered Industry"
        ],
        "abstract": "A high estimate of not only workplace fatal injuries but also nonfatal injuries and illnesses via overexertion, and contact with objects, equipment, and machinery workplace has been reported over the years. To address this issue, the Occupational Health and Safety (OHS) program has put an emphasis on policy and regulation for prevention, protection, and improvement of the individuals\u2019 health related to the working conditions and industrial environment. In the last 10 years, the Internet of Things (IoT) has matured to seamlessly enable real-time communications and cooperation among machines, environments, and humans using data analytics. Thus, IoT offers potential technical solutions for the prevention and protection of workplace injuries and illnesses. Moreover, IoT invites opportunities for collaboration with OHS in various industries. This article presents a systematic mapping study of the literature to address the impact of IoT on occupational well-being, analyzing the progress of Industry 4.0 during the last decade. This study systematizes the literature providing a taxonomy of the area through the results of four general, four focused, and two statistical research questions. These questions outline industrial environments and aspects of health concerning workers\u2019 well-being, concentrating on a human-centered approach leveraged by physiological measurements and psychological health. In addition, this paper explores questions regarding IoT\u2019s technical components, such as sensors, devices, and communication technologies, investigating methods of data processing supported by the employment of classification algorithms and data fusion strategies. As a result, the systematic mapping process initially found 7515 articles from six academic databases in the period from 2009 to 2019. After the execution of filtering methods, a complete read of 67 articles allowed to answer quantitatively and qualitatively the research questions. The classification of the answers contributed to systematize the literature through the taxonomy and the relationships among the topics covered by the articles. Accordingly, this research produced theoretical benefits, mainly, a broad view of the state-of-the-art, a taxonomy to guide related researches, and guidelines for future works. Furthermore, this research would benefit management and corporations by shedding light on technologies explored in the literature and elucidating their feasibility in support of the workforce\u2019s safety, psychological, and physical health.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,315"
    },
    {
        "issnkey": "02732300",
        "isbn": null,
        "journal": "Regulatory Toxicology and Pharmacology",
        "publisher": null,
        "title": "regulatorylandscapeofnanotechnologyandnanoplasticsfromaglobalperspective",
        "booktitle": null,
        "doi": "10.1016/j.yrtph.2021.104885",
        "author": [
            "Allan, Jacqueline",
            "Belz, Susanne",
            "Hoeveler, Arnd",
            "Hugas, Marta",
            "Okuda, Haruhiro",
            "Patri, Anil",
            "Rauscher, Hubert",
            "Silva, Primal",
            "Slikker, William",
            "Sokull-Kluettgen, Birgit",
            "Tong, Weida",
            "Anklam, Elke"
        ],
        "keywords": [
            "Nanotechnology, Nanomaterials, Nanoplastics, Harmonisation, Regulatory science, Standards, GSRS"
        ],
        "abstract": "Nanotechnology and more particularly nanotechnology-based products and materials have provided a huge potential for novel solutions to many of the current challenges society is facing. However, nanotechnology is also an area of product innovation that is sometimes developing faster than regulatory frameworks. This is due to the high complexity of some nanomaterials, the lack of a globally harmonised regulatory definition and the different scopes of regulation at a global level. Research organisations and regulatory bodies have spent many efforts in the last two decades to cope with these challenges. Although there has been a significant advancement related to analytical approaches for labelling purposes as well as to the development of suitable test guidelines for nanomaterials and their safety assessment, there is a still a need for greater global collaboration and consensus in the regulatory field. Furthermore, with growing societal concerns on plastic litter and tiny debris produced by degradation of littered plastic objects, the impact of micro- and nanoplastics on humans and the environment is an emerging issue. Despite increasing research and initial regulatory discussions on micro- and nanoplastics, there are still knowledge gaps and thus an urgent need for action. As nanoplastics can be classified as a specific type of incidental nanomaterials, current and future scientific investigations should take into account the existing profound knowledge on nanotechnology/nanomaterials when discussing issues around nanoplastics. This review was conceived at the 2019 Global Summit on Regulatory Sciences that took place in Stresa, Italy, on 24\u201326 September 2019 (GSRS 2019) and which was co-organised by the Global Coalition for Regulatory Science Research (GCRSR) and the European Commission's (EC) Joint Research Centre (JRC). The GCRSR consists of regulatory bodies from various countries around the globe including EU bodies. The 2019 Global Summit provided an excellent platform to exchange the latest information on activities carried out by regulatory bodies with a focus on the application of nanotechnology in the agriculture/food sector, on nanoplastics and on nanomedicines, including taking stock and promoting further collaboration. Recently, the topic of micro- and nanoplastics has become a new focus of the GCRSR. Besides discussing the challenges and needs, some future directions on how new tools and methodologies can improve the regulatory science were elaborated by summarising a significant portion of discussions during the summit. It has been revealed that there are still some uncertainties and knowledge gaps with regard to physicochemical properties, environmental behaviour and toxicological effects, especially as testing described in the dossiers is often done early in the product development process, and the material in the final product may behave differently. The harmonisation of methodologies for quantification and risk assessment of nanomaterials and micro/nanoplastics, the documentation of regulatory science studies and the need for sharing databases were highlighted as important aspects to look at.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.271",
        "scimago_value": "0,890"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-822060-3",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter14covid19detectionfromxrayimagesusingartificialintelligence",
        "booktitle": "Artificial Intelligence and Big Data Analytics for Smart Healthcare",
        "doi": "10.1016/B978-0-12-822060-3.00013-9",
        "author": [
            "Subasi, Abdulhamit",
            "Qureshi, Saqib",
            "Brahimi, Tayeb",
            "Serireti, Akila"
        ],
        "keywords": [
            "COVID-19 detection, Image classification, X-ray imaging, Artificial Intelligence, Deep learning"
        ],
        "abstract": "Currently, the most common disease is the new coronavirus disease identified as COVID-19. Various techniques to identifying the COVID-19 disease have been offered. Computer vision techniques are widely used to classify COVID-19 with the use of chest X-ray images. Rapid clinical results may prevent COVID-19 from spreading and help doctors treat patients under high workload conditions. As the normal diagnosis phase of illness with a laboratory test is time-consuming and requires a well-equipped laboratory, the X-ray imaging technique is a fast and cheap diagnostic tool for COVID-19. Machine learning methods can enhance the diagnosis of COVID-19 as a decision support platform for radiologists. This chapter utilizes various convolutional neural network (CNN) models, including pretrained models, to classify X-ray images into three classes: COVID-19, pneumonia, and normal. CNN, a form of deep neural networks that have become dominant in various computer vision tasks, attracts interest across various domains, including radiology. Pretrained models on ImageNet are good at detecting high-level features such as edges and patterns. These models understand certain representations of features, which can be reused. Also, deep classifiers have shown promising results in many kinds of work across various domains. We drew some useful results from these classifiers, which could be used faster when detecting COVID-19. Experimental results showed that the accuracy of the VGG19 classifier is 97.56%.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "1936878x",
        "isbn": null,
        "journal": "JACC: Cardiovascular Imaging",
        "publisher": null,
        "title": "fullissuepdf",
        "booktitle": null,
        "doi": "10.1016/S1936-878X(20)31073-1",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "13618415",
        "isbn": null,
        "journal": "Medical Image Analysis",
        "publisher": null,
        "title": "chaoschallengecombinedctmrhealthyabdominalorgansegmentation",
        "booktitle": null,
        "doi": "10.1016/j.media.2020.101950",
        "author": [
            "Kavur, A.",
            "Gezer, N.",
            "Bar\u0131\u015f, Mustafa",
            "Aslan, Sinem",
            "Conze, Pierre-Henri",
            "Groza, Vladimir",
            "Pham, Duc",
            "Chatterjee, Soumick",
            "Ernst, Philipp",
            "\u00d6zkan, Sava\u015f",
            "Baydar, Bora",
            "Lachinov, Dmitry",
            "Han, Shuo",
            "Pauli, Josef",
            "Isensee, Fabian",
            "Perkonigg, Matthias",
            "Sathish, Rachana",
            "Rajan, Ronnie",
            "Sheet, Debdoot",
            "Dovletov, Gurbandurdy",
            "Speck, Oliver",
            "N\u00fcrnberger, Andreas",
            "Maier-Hein, Klaus",
            "{Bozda\u011f\u0131 Akar}, G\u00f6zde",
            "\u00dcnal, G\u00f6zde",
            "Dicle, O\u011fuz",
            "Selver, M."
        ],
        "keywords": [
            "Segmentation, Challenge, Abdomen, Cross-modality"
        ],
        "abstract": "Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance are hard to interpret. This makes comparative analysis a necessary tool towards interpretable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal semantic segmentation tasks has been rarely discussed. In order to expand the knowledge on these topics, the CHAOS \u2013 Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Abdominal organ segmentation from routine acquisitions plays an important role in several clinical applications, such as pre-surgical planning or morphological and volumetric follow-ups for various diseases. These applications require a certain level of performance on a diverse set of metrics such as maximum symmetric surface distance (MSSD) to determine surgical error-margin or overlap errors for tracking size and shape differences. Previous abdomen related challenges are mainly focused on tumor/lesion detection and/or classification with a single modality. Conversely, CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks were designed to analyze the capabilities of participating approaches from multiple perspectives. The results were investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 \u00b1 0.00 / 0.95 \u00b1 0.01), but the best MSSD performance remains limited (21.89 \u00b1 13.94 / 20.85 \u00b1 10.63 mm). The performances of participating models decrease dramatically for cross-modality tasks both for the liver (DICE: 0.88 \u00b1 0.15 MSSD: 36.33 \u00b1 21.97 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs are observed to perform worse compared to organ-specific ones (performance drop around 5%). Nevertheless, some of the successful models show better performance with their multi-organ versions. We conclude that the exploration of those pros and cons in both single vs multi-organ and cross-modality segmentations is poised to have an impact on further research for developing effective algorithms that would support real-world clinical applications. Finally, having more than 1500 participants and receiving more than 550 submissions, another important contribution of this study is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomenon.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "8.545",
        "scimago_value": "2,887"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-814712-2",
        "journal": null,
        "publisher": "Academic Press",
        "title": "chapter15econometricmodellingandforecastingofwholesaleelectricityprices",
        "booktitle": "Handbook of Energy Economics and Policy",
        "doi": "10.1016/B978-0-12-814712-2.00015-4",
        "author": [
            "Sapio, Alessandro"
        ],
        "keywords": [
            "Econometrics, Forecasting, Reduced-form models, Structural models, Stylised facts, Time series, Wholesale electricity prices"
        ],
        "abstract": "This chapter is an introduction to econometric modelling and forecasting electricity prices determined in liberalised wholesale power exchanges, focusing on a high frequency of observation (hourly, daily). Wholesale electricity prices pose a number of modelling and forecasting challenges because of the peculiar physical and economic features of electricity. After reviewing the stylised facts on wholesale electricity prices, such as seasonality, mean reversion, volatility clustering, spikes, and negative prices, the chapter illustrates the two main econometric approaches to modelling and forecasting: reduced-form models and structural models. The chapter overviews the empirical evidence on wholesale electricity prices; offers insights on how economic features of power generation and demand translate into empirically sound econometric models and provides ideas on comparison and selection of econometric models of wholesale electricity prices.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22145796",
        "isbn": null,
        "journal": "Big Data Research",
        "publisher": null,
        "title": "deeplearningbasedweatherpredictionasurvey",
        "booktitle": null,
        "doi": "10.1016/j.bdr.2020.100178",
        "author": [
            "Ren, Xiaoli",
            "Li, Xiaoyong",
            "Ren, Kaijun",
            "Song, Junqiang",
            "Xu, Zichen",
            "Deng, Kefeng",
            "Wang, Xiang"
        ],
        "keywords": [
            "Deep learning, Big meteorological data, Weather forecasting, Spatio-temporal feature, Time series"
        ],
        "abstract": "Weather forecasting plays a fundamental role in the early warning of weather impacts on various aspects of human livelihood. For instance, weather forecasting provides decision making support for autonomous vehicles to reduce traffic accidents and congestions, which completely depend on the sensing and predicting of external environmental factors such as rainfall, air visibility and so on. Accurate and timely weather prediction has always been the goal of meteorological scientists. However, the conventional theory-driven numerical weather prediction (NWP) methods face many challenges, such as incomplete understanding of physical mechanisms, difficulties in obtaining useful knowledge from the deluge of observation data, and the requirement of powerful computing resources. With the successful application of data-driven deep learning method in various fields, such as computer vision, speech recognition, and time series prediction, it has been proven that deep learning method can effectively mine the temporal and spatial features from the spatio-temporal data. Meteorological data is a typical big geospatial data. Deep learning-based weather prediction (DLWP) is expected to be a strong supplement to the conventional method. At present, many researchers have tried to introduce data-driven deep learning into weather forecasting, and have achieved some preliminary results. In this paper, we survey the state-of-the-art studies of deep learning-based weather forecasting, in the aspects of the design of neural network (NN) architectures, spatial and temporal scales, as well as the datasets and benchmarks. Then we analyze the advantages and disadvantages of DLWP by comparing it with the conventional NWP, and summarize the potential future research topics of DLWP.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.578",
        "scimago_value": "0,565"
    },
    {
        "issnkey": "09574174",
        "isbn": null,
        "journal": "Expert Systems with Applications",
        "publisher": null,
        "title": "artificialintelligenceinophthalmopathyandultrawidefieldimageasurvey",
        "booktitle": null,
        "doi": "10.1016/j.eswa.2021.115068",
        "author": [
            "Yang, Jie",
            "Fong, Simon",
            "Wang, Han",
            "Hu, Quanyi",
            "Lin, Chen",
            "Huang, Shigao",
            "Shi, Jian",
            "Lan, Kun",
            "Tang, Rui",
            "Wu, Yaoyang",
            "Zhao, Qi"
        ],
        "keywords": [
            "Ophthalmopathy, Ultra-wide field (UWF) imaging, Deep learning, Machine learning"
        ],
        "abstract": "Fundus digital photography and optical coherence tomography (OCT) are currently the primary imaging approaches for early diagnosis and treatment of eye diseases. In recent years, the significant development in artificial intelligence (AI), particularly in machine learning (ML) and deep learning (DL) are new and vital technical-driven motivations impacting on the traditional diagnosis and treatment methods. At the same time, the ultra-wide field (UWF) imaging technology is getting widely accepted and prevalent by its obvious advantageous features of non-dilate pupils, express-track result and the vast pool of fundus viewing angles. As a result, numerous research have been done to explore AI in ultra-wide field fundus imaging ophthalmology for joint diagnosis and treatment. However, the current review of this method is still in least ink. We first outlines the application and impact of AI technology in ophthalmic diseases in the past ten years. With the following part exclusively summarizing the technical integration of ultra-wide field fundus images and AI technology in the past four years, which has brought innovations to clinical treatment methods for the diagnosis and treatment of ophthalmic diseases; finally, we analyzed the application and implementation of the novel technology as well as the potential limitations and challenges, to predict the possibility of the technology\u2019s further principles role and values in clinical ophthalmology.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "6.954",
        "scimago_value": "1,368"
    },
    {
        "issnkey": "02648377",
        "isbn": null,
        "journal": "Land Use Policy",
        "publisher": null,
        "title": "areformativeframeworkforprocessesfrombuildingpermitissuingtopropertyownershipinturkey",
        "booktitle": null,
        "doi": "10.1016/j.landusepol.2020.105115",
        "author": [
            "Guler, Dogus",
            "Yomralioglu, Tahsin"
        ],
        "keywords": [
            "Building permit, Digitalization, Property ownership, 3D land administration, 3D city model, Turkey"
        ],
        "abstract": "A smart built environment has become necessary for ensuring social well-being due to uncontrolled population growth and unrestrainable urban sprawl. In this connection, effective land administration is a significant element to actualize sustainable development. Yet existing building permit procedures fail to satisfy the need for current construction demands because of the insufficient transparency and inefficient procedures. Two dimensional (2D) based systems also remain incapable to unambiguously delineate the property ownership related to complex buildings. Keeping up-to-date the three dimensional (3D) urban models is another key for smart cities but this issue has become difficult owing to the rapid changes in the cities. In this sense, this paper first examines thoroughly the current situation in Turkey in terms of the building permit procedures, land administration, and 3D city modeling. Then, the paper detailedly proposes a reformative framework. The framework consists of the use of digital building models for both building permit processes and 3D registration of property ownership, as well as updating the 3D city model databases. The proposed framework is evaluated in terms of its applicability with a discussion of the prospective directions.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "5.398",
        "scimago_value": "1,668"
    },
    {
        "issnkey": "0169409x",
        "isbn": null,
        "journal": "Advanced Drug Delivery Reviews",
        "publisher": null,
        "title": "artificialintelligenceandmachinelearningassisteddrugdeliveryforeffectivetreatmentofinfectiousdiseases",
        "booktitle": null,
        "doi": "10.1016/j.addr.2021.113922",
        "author": [
            "He, Sheng",
            "Leanse, Leon",
            "Feng, Yanfang"
        ],
        "keywords": [
            "Artificial intelligence, Drug delivery, Infectious diseases, Antimicrobial resistance"
        ],
        "abstract": "In the era of antimicrobial resistance, the prevalence of multidrug-resistant microorganisms that resist conventional antibiotic treatment has steadily increased. Thus, it is now unquestionable that infectious diseases are significant global burdens that urgently require innovative treatment strategies. Emerging studies have demonstrated that artificial intelligence (AI) can transform drug delivery to promote effective treatment of infectious diseases. In this review, we propose to evaluate the significance, essential principles, and popular tools of AI in drug delivery for infectious disease treatment. Specifically, we will focus on the achievements and key findings of current research, as well as the applications of AI on drug delivery throughout the whole antimicrobial treatment process, with an emphasis on drug development, treatment regimen optimization, drug delivery system and administration route design, and drug delivery outcome prediction. To that end, the challenges of AI in drug delivery for infectious disease treatments and their current solutions and future perspective will be presented and discussed.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "03009572",
        "isbn": null,
        "journal": "Resuscitation",
        "publisher": null,
        "title": "europeanresuscitationcouncilguidelines2021ethicsofresuscitationandendoflifedecisions",
        "booktitle": null,
        "doi": "10.1016/j.resuscitation.2021.02.017",
        "author": [
            "Mentzelopoulos, Spyros",
            "Couper, Keith",
            "Voorde, Patrick",
            "Druw\u00e9, Patrick",
            "Blom, Marieke",
            "Perkins, Gavin",
            "Lulic, Ileana",
            "Djakow, Jana",
            "Raffay, Violetta",
            "Lilja, Gisela",
            "Bossaert, Leo"
        ],
        "keywords": [
            ""
        ],
        "abstract": "These European Resuscitation Council Ethics guidelines provide evidence-based recommendations for the ethical, routine practice of resuscitation and end-of-life care of adults and children. The guideline primarily focus on major ethical practice interventions (i.e. advance directives, advance care planning, and shared decision making), decision making regarding resuscitation, education, and research. These areas are tightly related to the application of the principles of bioethics in the practice of resuscitation and end-of-life care.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-821953-9",
        "journal": null,
        "publisher": "Elsevier",
        "title": "chapter8decisionsupporttools",
        "booktitle": "Environmental Systems Science",
        "doi": "10.1016/B978-0-12-821953-9.00016-7",
        "author": [
            "Vallero, Daniel"
        ],
        "keywords": [
            "Multicriteria decision analysis (MCDA), Bayesian belief network, Root cause analysis, Ishikawa diagram, Adverse outcome pathway (AOP), Big data, Informatics, Chemistry dashboard, Life cycle analysis (LCA), Integrated environmental modeling (IEM)"
        ],
        "abstract": "There are a growing number of models, modules, dashboards, and other tools that can be used to support sound risk assessments. This chapter discusses those that are most promising and applicable to environmental systems science. The chapter discusses approaches for evaluating complex systems. In addition, methods are recommended for analyzing projects and actions in terms of responsible and ethical content.",
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "18777503",
        "isbn": null,
        "journal": "Journal of Computational Science",
        "publisher": null,
        "title": "anartificialintelligencemodelconsideringdataimbalanceforshipselectioninportstatecontrolbasedondetentionprobabilities",
        "booktitle": null,
        "doi": "10.1016/j.jocs.2020.101257",
        "author": [
            "Yan, Ran",
            "Wang, Shuaian",
            "Peng, Chuansheng"
        ],
        "keywords": [
            "Port state control inspection, ship detention, machine learning in maritime transportation, artificial intelligence in maritime transportation, imbalanced data"
        ],
        "abstract": "Port state control inspection is seen as a safety net to guard marine safety, protect the marine environment, and guarantee decent onboard working and living conditions for seafarers. A substandard ship can be detained in an inspection if serious deficiencies are found onboard. Ship detention is regarded as a severe result in port state control inspection. However, developing accurate prediction models for ship detention based on ship\u2019s generic factors (e.g. ship age, type, and flag), dynamic factors (e.g. times of ship flag changes), and inspection historical factors (e.g. total previous detentions in PSC inspection, last PSC inspection time, and last deficiency number in PSC inspection) before an inspection is conducted is not a trivial task as the low detention rate leads to a highly imbalanced inspection records dataset. To address this issue, this paper develops a classification model called balanced random forest (BRF) to predict ship detention by using 1,600 inspection records at the Hong Kong port for three years. Numerical experiments show that the proposed BRF model can identify 81.25% of all the ships with detention in the test set which contains another 400 inspection records. Compared with the current ship selection method at the Hong Kong port, the BRF model is much more efficient and can achieve an average improvement of 73.72% in detained ship identification.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.976",
        "scimago_value": "0,704"
    },
    {
        "issnkey": "22147160",
        "isbn": null,
        "journal": "Operations Research Perspectives",
        "publisher": null,
        "title": "asystematicliteraturereviewaboutdimensioningsafetystockunderuncertaintiesandrisksintheprocurementprocess",
        "booktitle": null,
        "doi": "10.1016/j.orp.2021.100192",
        "author": [
            "Barros, J\u00falio",
            "Cortez, Paulo",
            "Carvalho, M."
        ],
        "keywords": [
            "Safety stocks, Inventory management, Procurement, Supply chain risk management, Uncertainty factors, Systematic literature review"
        ],
        "abstract": "This paper analyses literature contributions in the search for safety stock problem under uncertainties and risks in the procurement process, focusing on the dimensioning problem (determination of the safety stock level). We perform a systematic literature review (SLR) from 1995 to 2019 in relevant journals, covering 193 selected articles. These selected articles were classified into three safety stock main issues: safety stock dimensioning, safety stock management, and safety stock positioning, allocation or placement. The SLR analysis allowed the identification of literature gaps and research opportunities, thus providing a road map to guide future research on this topic.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "0,697"
    },
    {
        "issnkey": "0167739x",
        "isbn": null,
        "journal": "Future Generation Computer Systems",
        "publisher": null,
        "title": "selfimprovingsystemintegrationmasteringcontinuouschange",
        "booktitle": null,
        "doi": "10.1016/j.future.2020.11.019",
        "author": [
            "Bellman, Kirstie",
            "Botev, Jean",
            "Diaconescu, Ada",
            "Esterle, Lukas",
            "Gruhl, Christian",
            "Landauer, Christopher",
            "Lewis, Peter",
            "Nelson, Phyllis",
            "Pournaras, Evangelos",
            "Stein, Anthony",
            "Tomforde, Sven"
        ],
        "keywords": [
            "Self-integration, Self-improvement, Autonomous systems, Taxonomy, Organic computing, System engineering"
        ],
        "abstract": "The research initiative \u201cself-improving system integration\u201d (SISSY) was established with the goal to master the ever-changing demands of system organisation in the presence of autonomous subsystems, evolving architectures, and highly-dynamic open environments. It aims to move integration-related decisions from design-time to run-time, implying a further shift of expertise and responsibility from human engineers to autonomous systems. This introduces a qualitative shift from existing self-adaptive and self-organising systems, moving from self-adaptation based on predefined variation types, towards more open contexts involving novel autonomous subsystems, collaborative behaviours, and emerging goals. In this article, we revisit existing SISSY research efforts and establish a corresponding terminology focusing on how SISSY relates to the broad field of integration sciences. We then investigate SISSY-related research efforts and derive a taxonomy of SISSY technology. This is concluded by establishing a research road-map for developing operational self-improving self-integrating systems.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "22106707",
        "isbn": null,
        "journal": "Sustainable Cities and Society",
        "publisher": null,
        "title": "discoveringoptimalstrategiesformitigatingcovid19spreadusingmachinelearningexperiencefromasia",
        "booktitle": null,
        "doi": "10.1016/j.scs.2021.103254",
        "author": [
            "Pan, Yue",
            "Zhang, Limao",
            "Yan, Zhenzhen",
            "Lwin, May",
            "Skibniewski, Miroslaw"
        ],
        "keywords": [
            "COVID-19, Random forest regression, Feature importance analysis, Multi-objective optimization, Sustainability"
        ],
        "abstract": "To inform data-driven decisions in fighting the global pandemic caused by COVID-19, this research develops a spatiotemporal analysis framework under the combination of an ensemble model (random forest regression) and a multi-objective optimization algorithm (NSGA-II). It has been verified for four Asian countries, including Japan, South Korea, Pakistan, and Nepal. Accordingly, we can gain some valuable experience to better understand the disease evolution, forecast the prevalence of the disease, which can provide sustainable evidence to guide further intervention and management. Random forest with a proper rolling time-window can learn the combined effects of environmental and social factors to accurately predict the daily growth of confirmed cases and daily death rate on a national scale, which is followed by NSGA-II to find a range of Pareto optimal solutions for ensuring the minimization of the infection rate and mortality at the same time. Experimental results demonstrate that the predictive model can alert the local government in advance, allowing the accused time to put forward relevant measures. The temperature in the category of environment and the stringency index belonging to the social factor are identified as the top 2 important features to exert a greater impact on the virus transmission. Moreover, optimal solutions provide references to design the best control strategies towards pandemic containment and prevention that can accommodate the country-specific circumstance, which are possible to decrease the two objectives by more than 95%. In particular, appropriate adjustment of social-related features needs to take priority over others, since it can bring about at least 1.47% average improvement of two objectives compared to environmental factors.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "7.587",
        "scimago_value": "1,645"
    },
    {
        "issnkey": "",
        "isbn": "978-0-12-816078-7",
        "journal": null,
        "publisher": "Academic Press",
        "title": "",
        "booktitle": "Systems Medicine",
        "doi": "10.1016/B978-0-12-816077-0.09001-4",
        "author": [],
        "keywords": [
            ""
        ],
        "abstract": null,
        "year": "2021",
        "type_publication": "incollection",
        "jcr_value": null,
        "scimago_value": null
    },
    {
        "issnkey": "01672681",
        "isbn": null,
        "journal": "Journal of Economic Behavior & Organization",
        "publisher": null,
        "title": "anticorruptionsafetycomplianceandcoalminedeathsevidencefromchina",
        "booktitle": null,
        "doi": "10.1016/j.jebo.2021.05.013",
        "author": [
            "Xu, Gang",
            "Wang, Xue",
            "Wang, Ruiting",
            "Yano, Go",
            "Zou, Rong"
        ],
        "keywords": [
            "Anti-corruption, Safety compliance, Coal mine deaths, China"
        ],
        "abstract": "This study evaluates the impact of the anti-corruption campaign launched by President Xi since 2013 on coal mine mortality in China. Combining several unique provincial-level datasets on coal mines from 2004 to 2017, we find evidence that provinces with stronger exposure to the anti-corruption campaign have experienced a significantly larger decrease in coalmine death rates. This effect survives a vast array of robustness checks and also displays great heterogeneity. Further evidence suggests the campaign has led to fewer safety violations, more fixed investments and a decrease in profits accompanied by an increase in the costs of principal business in the coal mining industry. The above findings are most consistent with the interpretation that the campaign has made coal mining firms less likely to shirk on safety by curbing collusion between coal mines and local officials. We also rule out other channels such as intensified inspection and the change of employment composition in the industry.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": null,
        "scimago_value": "1,256"
    },
    {
        "issnkey": "01403664",
        "isbn": null,
        "journal": "Computer Communications",
        "publisher": null,
        "title": "mobilecomputingandcommunicationsdrivenfogassisteddisasterevacuationtechniquesforcontextawareguidancesupportasurvey",
        "booktitle": null,
        "doi": "10.1016/j.comcom.2021.07.020",
        "author": [
            "Kurniawan, Ibnu",
            "Asyhari, A.",
            "He, Fei",
            "Liu, Ye"
        ],
        "keywords": [
            "Disaster recovery, Evacuation guidance, Fog, Fog computing, Fog communications, Collaborative analytics"
        ],
        "abstract": "The importance of an optimal solution for disaster evacuation has recently raised attention from researchers across multiple disciplines. This is not only a serious, but also a challenging task due to the complexities of the evacuees\u2019 behaviors, route planning, and demanding coordination services. Although existing studies have addressed these challenges to some extent, mass evacuation in natural disasters tends to be difficult to predict and manage due to the limitation of the underlying models to capture realistic situations. It is therefore desirable to have on-demand mechanisms of locally-driven computing and data exchange services in order to enable near real-time capture of the disaster area during the evacuation. For this purpose, this paper comprehensively surveys recent advances in information and communication technology-enabled disaster evacuations, with the focus on fog computation and communication services to support a massive evacuation process. A numerous variety of tools and techniques are encapsulated within a coordinated on-demand strategy of an evacuation platform, which is aimed to provide a situational awareness and response. Herein fog services appear to be one of the viable options for responsive mass evacuation because they enable low latency data processing and dissemination. They can additionally provide data analytics support for autonomous learning for both the short-term guidance supports and long-term usages. This work extends the existing data-oriented framework by outlining comprehensive functionalities and providing seamless integration. We review the principles, challenges, and future direction of the state-of-the-art strategies proposed to sit within each functionality. Taken together, this survey highlights the importance of adaptive coordination and reconfiguration within the fog services to facilitate responsive mass evacuations as well as open up new research challenges associated with analytics-embedding network and computation, which is critical for any disaster recovery activities.",
        "year": "2021",
        "type_publication": "article",
        "jcr_value": "3.167",
        "scimago_value": "0,627"
    }
]