@incollection{CHOUVARDA2022151,
title = {Chapter 6 - Connected health technologies for knowledge extraction and knowledge-based medicine in cardiac care},
editor = {Anna Maria Bianchi and Jorge Henriques and Vicente {Traver Salcedo}},
booktitle = {Personalized Health Systems for Cardiovascular Disease},
publisher = {Academic Press},
pages = {151-175},
year = {2022},
isbn = {978-0-12-818950-4},
doi = {https://doi.org/10.1016/B978-0-12-818950-4.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818950400001X},
author = {Ioanna Chouvarda},
keywords = {Connected health technologies, personal health systems, enabling technologies, cardiovascular disease, knowledge-based medicine, learning health cycle},
abstract = {Connected health is a new model for health management. It puts the correct information in the correct hands at the correct time. It allows patients and clinicians to make better decisions and relies heavily on advanced technologies to do so. In the connected health era there is an opportunity to collect a multitude of data about a patient. These may include daily life and behavior, including compliance to interventions or psychological aspects, as well as daily fluctuations of physiological parameters and contextual factors that influence them and can be combined with clinical and biological data available for the patient at different time points. This multitude of data can offer the opportunity to move from evidence-based medicine to knowledge-based medicine, that is, medicine that relies on knowledge about the patient. To achieve this goal, it is worth investigating the path from new technological methods for data acquisition to analysis and new knowledge and the path from patient knowledge to better intervention, as aspects that have gradually been discovered within the connected health domain. Both points would contribute to the impact of wide adoption of connected health technologies (CHT) as parts of new healthcare models. This chapter discusses first the connected health technologies in cardiac care. Basic concepts and building blocks are discussed, such as point-of-care diagnostics and new sensors, big data analytics, health behavior informatics and systems medicine, interventions, and knowledge-based medicine. CHT as a part of an iterative learning cycle for knowledge acquisition and application is presented, and the relevant challenges are discussed.}
}
@incollection{DOGUC202295,
title = {Chapter 9 - Recent applications of data mining in medical diagnosis and prediction},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {95-109},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00006-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000066},
author = {Ozge Doguc and Zehra Nur Canbolat and Gokhan Silahtaroglu},
keywords = {Big data, Data mining, Health sector, Intelligent medical diagnosis systems},
abstract = {Big data has been used in the health sector to improve the quality of life, predict epidemics, cure diseases, and avoid preventable deaths, beyond increasing profits or reducing the burden of excess labor. Data sources in healthcare have become quite diversified and accessible to individuals, such as wearable and implantable devices, smartphones, and real-time sensors. When combined with existing health data, daily (even instantaneous) data from these devices can be used to predict future health conditions of individuals and to identify necessary intervention points. This chapter discusses a number of recent studies that introduces methods for using big data to create intelligent systems for patient diagnosis, triage, predicting lab results, and even detecting tumors. These studies open ways for researchers in the healthcare sector to improve the quality of services provided to the patients as well as reducing costs for the healthcare institutions.}
}
@article{DERARDJA2022107136,
title = {A deep learning model for mapping the perturbation in pressurised irrigation systems},
journal = {Computers and Electronics in Agriculture},
volume = {199},
pages = {107136},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107136},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922004537},
author = {Bilal Derardja and Umberto Fratino and Nicola Lamaddalena and R. {González Perea} and J.A. {Rodríguez Díaz}},
keywords = {Pressurized irrigation systems, Unsteady state flow, Perturbation, On-demand irrigation networks, Deep neural networks},
abstract = {Nowadays, the management of pressurised irrigation networks requires plenty of information to provide an efficient and reliable service to farmers. Perturbation is the propagation of pressure waves through the networks pipes which could expose the network to a serious risk that could cause components damaging. Several computational codes were developed to simulate such phenomenon. The most recent ones are efficient enough to provide a good image of the perturbation occurrence through different indicators, but they are time and computationally expensive. For real time decision making and more flexible management, there is a need for faster models to be developed. In this study the directly programmed models were used as big data generators to train a developed deep learning model. This approach was applied on a pressurised on-demand irrigation system located in south of Italy that consists of 19 hydrants (service outlets) and covers 57 ha. Two thousand configurations (operational scenarios) were simulated using a predeveloped directly programmed model and fed to train a deep learning model with the objective of forecasting the maximum pressure occurred due the perturbation at each section. The occurred pressure is represented as classes according to the case sensitivity and the required precision. In the present work, scenarios for 1, 2 and 3 bars steps were simulated. The model proved to be significantly time saving compared to previous approaches as the results are produced instantaneously with a forecasting accuracy of 85 %. Furthermore, from the called confusion matrix, the error committed by the model is of one class lower or higher that may be considered tolerable according to the system sensitivity. Thus, modelling the perturbation in the on-demand pressurised irrigation networks would add a significant contribution to provide practical recommendations for real-time decision-making processes.}
}
@article{CHEN2022102474,
title = {EVFL: An explainable vertical federated learning for data-oriented Artificial Intelligence systems},
journal = {Journal of Systems Architecture},
volume = {126},
pages = {102474},
year = {2022},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102474},
url = {https://www.sciencedirect.com/science/article/pii/S1383762122000583},
author = {Peng Chen and Xin Du and Zhihui Lu and Jie Wu and Patrick C.K. Hung},
keywords = {Data-oriented AI, Federated counterfactual explanation, Feature importance, Vertical federated learning, Data cleansing},
abstract = {Vertical federated learning (VFL), as one of the latest advances of security in the data-oriented Artificial Intelligence (AI) systems, facilitates better keeping the AI systems converge faster with higher performance and security. Since a large amount of data from these systems is often of low quality, the training data needs to be interpreted and evaluated. While there have been some research efforts, they still have significant shortcomings, such as high computational complexity and impracticality. Considering the characteristics of the data, the interpretation of machine learning models allows for data cleansing, which can improve data quality and help regulators understand the decision-making process. In this paper, we propose an explainable vertical federated learning (EVFL) framework, including the credibility assessment strategy, the federated counterfactual explanation and the importance rate (IR) metric. Furthermore, we initialize the knowledge-based counterfactual instance based on prior knowledge and retrain the federated counterfactual method for feasible counterfactual features. We report experimental results obtained on the Lending Club and Zhongyuan datasets for implementing our framework to show that our approach is significantly effective. Notably, on the Lending Club dataset, our method can have a +4.9% improvement over other selections.}
}
@article{KRAJSIC2022235,
title = {Catch Me If You Can: Online Classification for Near Real-Time Anomaly Detection in Business Process Event Streams},
journal = {Procedia Computer Science},
volume = {207},
pages = {235-244},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.056},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009292},
author = {Philippe Krajsic and Bogdan Franczyk},
keywords = {business process, classification, online learning, explainability},
abstract = {Near-real-time monitoring and classification of business process event streams is becoming more and more prominent. This also includes ensuring data quality for the application of downstream online process mining activities and therefore identify and classify incorrect process behavior of incoming event streams in an online setting, what is considered too little in existing approaches. In this paper, we present an online classification approach that supports monitoring and anomaly detection in event streams at the event level. Possible process drifts can be handled by an online learning workflow. By integrating two explanatory components, the results of the online classification are made transparent and comprehensible. Through a technical experiment, the performance of the classification approach is evaluated based on different data sets. Thereby, the classification model achieves an average F1 score of 0.877 with an average processing time of ∼15 ms per event.}
}
@article{CAPPIELLO2022101874,
title = {Assessing and improving measurability of process performance indicators based on quality of logs},
journal = {Information Systems},
volume = {103},
pages = {101874},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101874},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000995},
author = {Cinzia Cappiello and Marco Comuzzi and Pierluigi Plebani and Matheus Fim},
keywords = {Business process, Event log, Data quality assessment, Data quality improvement},
abstract = {The efficiency and effectiveness of business processes are usually evaluated by Process Performance Indicators (PPIs), which are computed using process event logs. PPIs can be insightful only when they are measurable, i.e., reliable. This paper proposes to define PPI measurability on the basis of the quality of the data in the process logs. Then, based on this definition, a framework for PPI measurability assessment and improvement is presented. For the assessment, we propose novel definitions of PPI accuracy, completeness, consistency, timeliness and volume that contextualise the traditional definitions in the data quality literature to the case of process logs. For the improvement, we define a set of guidelines for improving the measurability of a PPI. These guidelines may concern improving existing event logs, for instance through data imputation, implementation or enhancement of the process monitoring systems, or updating the PPI definitions. A case study in a large-sized institution is discussed to show the feasibility and the practical value of the proposed framework.}
}
@article{MOZZONI2022402,
title = {Transfer’s monitoring in bus transit services by Automatic Vehicle Location data},
journal = {Transportation Research Procedia},
volume = {60},
pages = {402-409},
year = {2022},
note = {New scenarios for safe mobility in urban areasProceedings of the XXV International Conference Living and Walking in Cities (LWC 2021), September 9-10, 2021, Brescia, Italy},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2021.12.052},
url = {https://www.sciencedirect.com/science/article/pii/S2352146521009534},
author = {Sara Mozzoni and Massimo Di Francesco and Giulio Maternini and Benedetto Barabino},
keywords = {Big Data, Transfer diagnosis, Automatic Vehicle Location Data},
abstract = {Since transfers increase the connectivity of routes, they improve the characteristics of transit networks. Designing and managing transfers are well-investigated issues arising at the tactical and operational level. Conversely, the monitoring phase was rarely faced to verify the consistency between well planned and/or delivered transfers. In this paper, we tailor an innovative methodology for measuring the rate of transfers between two routes by using archived Automatic Vehicle Location (AVL) data. This measurement is performed spatially, at shared and unshared (but reasonably quite close) bus stops, and temporally at each time period. The results are represented by easy-to-read control dashboards. This methodology is tested by about 240,000 AVL real records provided by the local bus operator of Cagliari (Italy) and provides valuable insights into the characterization of transfers.}
}
@article{PHOON2022967,
title = {Unpacking data-centric geotechnics},
journal = {Underground Space},
volume = {7},
number = {6},
pages = {967-989},
year = {2022},
issn = {2467-9674},
doi = {https://doi.org/10.1016/j.undsp.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2467967422000514},
author = {Kok-Kwang Phoon and Jianye Ching and Zijun Cao},
keywords = {Data-centric geotechnics, Bayesian machine learning, Data-driven site characterization (DDSC), Project DeepGeo, Data-informed decision support index},
abstract = {The purpose of this paper (presented online as a keynote lecture at the 25th Annual Indonesian Geotechnical Conference on 10 Nov 2021) is to broadly conceptualize the agenda for data-centric geotechnics, an emerging field that attempts to prepare geotechnical engineering for digital transformation. The agenda must include (1) development of methods that make sense of all real-world data (not selective input data for a physical model), (2) offering insights of significant value to critical real-world decisions for current or future practice (not decisions for an ideal world or decisions of minor concern to geotechnical engineers), and (3) sensitivity to the physical context of geotechnics (not abstract data-driven analysis connected to geotechnics in a peripheral way, i.e., engagement with the knowledge and experience base should be substantial). These three elements are termed “data centricity”, “fit for (and transform) practice”, and “geotechnical context” in the agenda. Given that a knowledge of the site is central to any geotechnical engineering project, data-driven site characterization (DDSC) must constitute one key application domain in data-centric geotechnics, although other infrastructure lifecycle phases such as project conceptualization, design, construction, operation, and decommission/reuse would benefit from data-informed decision support as well. One part of DDSC that addresses numerical soil data in a site investigation report and soil property databases is pursued under Project DeepGeo. In principle, the source of data can also go beyond site investigation, and the type of data can go beyond numbers, such as categorical data, text, audios, images, videos, and expert opinion. The purpose of Project DeepGeo is to produce a 3D stratigraphic map of the subsurface volume below a full-scale project site and to estimate relevant engineering properties at each spatial point based on actual site investigation data and other relevant Big Indirect Data (BID). Uncertainty quantification is necessary, as current real-world data is insufficient, incomplete, and/or not directly relevant to construct a deterministic map. The value of a deterministic map for decision support is debatable. The computational cost to do this for a 3D true scale subsurface volume must be reasonable. Ultimately, geotechnical structures need to be a part of a completely smart infrastructure that fits the circular economy and need to focus on delivering service to end-users and the community from project conceptualization to decommission/reuse with full integration to smart city and smart society. Although current geotechnical practice has been very successful in taking “calculated risk” informed by limited data, imperfect theories, prototype testing, observations, among others and exercising judicious caution and engineering judgment, there is no clear pathway forward to leverage on big data and digital technologies such as machine learning, BIM, and digital twin to meet more challenging needs such as sustainability and resilience engineering.}
}
@incollection{SEBASTIANCOLEMAN202269,
title = {Chapter 4 - The Data Challenge: The Mechanics of Meaning},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {69-92},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000043},
author = {Laura Sebastian-Coleman},
keywords = {History of data, statistics, scientific data, organizational data, relational data, characteristics of data},
abstract = {This chapter presents an extended definition of the concept of data, through the lens of history. All forms of data encode information about the real world. Using data always involves interpretation, so it is important to understand how data works, to understand “data as data.” But what we mean by data and how we create and use it in science, statistics, and commerce have changed over time. Many assumptions about data quality are rooted in this evolution. A better understanding of the evolution of data helps us define and manage specific expectations related to data quality.}
}
@article{LIU2022102936,
title = {A review of spatially-explicit GeoAI applications in Urban Geography},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102936},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102936},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222001339},
author = {Pengyuan Liu and Filip Biljecki},
keywords = {Urban studies, Deep learning, Socio-economics, Location encoder, Graph neural network},
abstract = {Urban Geography studies forms, social fabrics, and economic structures of cities from a geographic perspective. Catalysed by the increasingly abundant spatial big data, Urban Geography seeks new models and research paradigms to explain urban phenomena and address urban issues. Recent years have witnessed significant advances in spatially-explicit geospatial artificial intelligence (GeoAI), which integrates spatial studies and AI, primarily focusing on incorporating spatial thinking and concept into deep learning models for urban studies. This paper provides an overview of techniques and applications of spatially-explicit GeoAI in Urban Geography based on 581 papers identified using a systematic review approach. We examined and screened papers in three scopes of Urban Geography (Urban Dynamics, Social Differentiation of Urban Areas, and Social Sensing) and found that although GeoAI is a trending topic in geography and the applications of deep neural network-based methods are proliferating, the development of spatially-explicit GeoAI models is still at their early phase. We identified three challenges of existing models and advised future research direction towards developing multi-scale explainable spatially-explicit GeoAI. This review paper acquaints beginners with the basics of GeoAI and state-of-the-art and serve as an inspiration to attract more research in exploring the potential of spatially-explicit GeoAI in studying the socio-economic dimension of the city and urban life.}
}
@article{MUNAPPY2022111359,
title = {Data management for production quality deep learning models: Challenges and solutions},
journal = {Journal of Systems and Software},
volume = {191},
pages = {111359},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111359},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000905},
author = {Aiswarya Raj Munappy and Jan Bosch and Helena Holmström Olsson and Anders Arpteg and Björn Brinne},
keywords = {Deep learning, Data management, Production quality DL models, Challenges, Solutions, Validation},
abstract = {Deep learning (DL) based software systems are difficult to develop and maintain in industrial settings due to several challenges. Data management is one of the most prominent challenges which complicates DL in industrial deployments. DL models are data-hungry and require high-quality data. Therefore, the volume, variety, velocity, and quality of data cannot be compromised. This study aims to explore the data management challenges encountered by practitioners developing systems with DL components, identify the potential solutions from the literature and validate the solutions through a multiple case study. We identified 20 data management challenges experienced by DL practitioners through a multiple interpretive case study. Further, we identified 48 articles through a systematic literature review that discuss the solutions for the data management challenges. With the second round of multiple case study, we show that many of these solutions have limitations and are not used in practice due to a combination of four factors: high cost, lack of skill-set and infrastructure, inability to solve the problem completely, and incompatibility with certain DL use cases. Thus, data management for data-intensive DL models in production is complicated. Although the DL technology has achieved very promising results, there is still a significant need for further research in the field of data management to build high-quality datasets and streams that can be used for building production-ready DL systems. Furthermore, we have classified the data management challenges into four categories based on the availability of the solutions.}
}
@article{WEI2022100675,
title = {How to improve learning experience in MOOCs an analysis of online reviews of business courses on Coursera},
journal = {The International Journal of Management Education},
volume = {20},
number = {3},
pages = {100675},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100675},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722000775},
author = {Xiaoxia Wei and Viriya Taecharungroj},
keywords = {MOOC learning experience, Online reviews, Sentiment analysis, Textual analysis, Coursera},
abstract = {This study enriched the research horizon of the social sciences and contents of MOOCs by providing stakeholders with authentic data-driven recommendations to better conceptualize, design, develop and deliver MOOCs in today's higher education context. Key factors driving positive/negative learning experiences in business MOOCs were identified and explored. A topic modelling algorithm—Latent Dirichlet allocation (LDA)—was used to examine 144,946 online reviews of 729 business courses on Coursera between August 7, 2015 and August 16, 2021. Two major themes and 11 topics emerged as MOOC delivery (professor, information, comprehension, assessment and materials) and subject matter (finance, marketing, people management, computer skills, technology and project management). A textual salience-valence analysis was employed to analyze the factors driving positive/negative learning experiences regarding MOOC delivery. Findings suggested that 1) business MOOCs should value the importance of instructors' professional and celebrity image to appeal to learners, 2) course design and structure should be easy and simple to manage by learners, 3) course contents, information and assessment should be challenging rather than hard, and 4) the application and validation of peer reviews in both learning process and assessment should be more responsive to eliminate potential issues that could negatively impact learning experiences.}
}
@incollection{KATARINA202283,
title = {Chapter 6 - Innovative technologies in precision healthcare},
editor = {Debmalya Barh},
booktitle = {Biotechnology in Healthcare},
publisher = {Academic Press},
pages = {83-102},
year = {2022},
isbn = {978-0-323-89837-9},
doi = {https://doi.org/10.1016/B978-0-323-89837-9.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323898379000164},
author = {Šoltýs Katarína and Kľoc Marek and Rabajdová Miroslava and Mareková Mária},
keywords = {Precision healthcare, biotechnology, emerging technologies, big data},
abstract = {Precision medicine is the intersection of data science, analytics, and biomedicine in creating a healthy learning system that conducts research in the context of clinical care while optimizing the tools and information used to provide better outcomes for patients. Emerging technologies represent a novel, innovative, and fast-evolving trend within a particular field. Among the latest trends as virtual reality, robotics, wearable, and implantable sensors, and removable tattoos, together with nanotechnologies, 3D printing, and others are considered. In addition, new advanced computing technologies including artificial intelligence, machine learning (ML), big data mining, and cloud computing form an integral part of personalized healthcare. Personalized medicine is not necessarily the same as precision medicine. From the point of view of technology development, precision medicine is an intermediate step to personalized medicine, which will be much more complex and will require even more data. There is a big challenge to combine multiomics approaches in analysis, as we can see in bioinformatics that there are a lot of techniques in one area. Analysis of more than one area such as the genome, transcriptome even microbiome, starts exponentially grown on science field. The integration of multiomics data analysis and machine learning can have led to the discovery of new biomarkers, and improve of differential diagnostics of latent diseases. In this chapter, we describe the use of emerging technologies as well as bioengineering and ML for precision healthcare.}
}
@article{PANTHI2022155641,
title = {Saltwater intrusion into coastal aquifers in the contiguous United States — A systematic review of investigation approaches and monitoring networks},
journal = {Science of The Total Environment},
volume = {836},
pages = {155641},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.155641},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722027371},
author = {Jeeban Panthi and Soni M. Pradhanang and Annika Nolte and Thomas B. Boving},
keywords = {Saltwater intrusion, Monitoring networks, Geophysical techniques, Modeling, Review},
abstract = {Saltwater intrusion (SWI) into coastal aquifers is a growing problem for the drinking water supply of coastal communities worldwide, including for the sustainability of coastal ecosystems depending on freshwater inflow. The interface between freshwater and seawater in coastal aquifers is highly dynamic and is sensitive to changes in the hydraulic gradient between the sea- and groundwater levels. Sea level rise, storm surges, and drought are natural drivers changing the hydrostatic equilibrium between fresh- and saltwater. Coastal aquifers are further stressed by groundwater over-pumping because of the increasing needs of coastal populations. A systematic literature review and analysis of the current state of understanding the SWI drivers is presented, focusing on recent (1980 to 2020) investigations in the contiguous United States (CONUS). Results confirm that SWI is an active research area in CONUS. The drivers of SWI are increasingly better understood and quantified; however, the need for increased monitoring is also recognized. Our study shows that the number of monitoring sites have not increased significantly over the review period. Additionally, geophysical, and geochemical investigation techniques and numerical modeling tools are not utilized to their full potential, and data on SWI is not readily available from some sources. We conclude that there is a need for more SWI monitoring networks and closer multi-disciplinary collaboration, particularly between practitioners in the field and emerging modeling technique experts. Though we focus primarily on CONUS, our insights may be of value to the broader SWI research community and coastal water quality managers around the globe.}
}
@article{KHOURY20221178,
title = {A Framework for Augmented Intelligence in Allergy and Immunology Practice and Research—A Work Group Report of the AAAAI Health Informatics, Technology, and Education Committee},
journal = {The Journal of Allergy and Clinical Immunology: In Practice},
volume = {10},
number = {5},
pages = {1178-1188},
year = {2022},
issn = {2213-2198},
doi = {https://doi.org/10.1016/j.jaip.2022.01.047},
url = {https://www.sciencedirect.com/science/article/pii/S221321982200143X},
author = {Paneez Khoury and Renganathan Srinivasan and Sujani Kakumanu and Sebastian Ochoa and Anjeni Keswani and Rachel Sparks and Nicholas L. Rider},
keywords = {Artificial intelligence, Asthma, Primary immunodeficiency, Atopic dermatitis, Augmented intelligence, Clinical decision support, Electronic health records, Equity, Machine learning, Natural language processing, Medical education},
abstract = {Artificial and augmented intelligence (AI) and machine learning (ML) methods are expanding into the health care space. Big data are increasingly used in patient care applications, diagnostics, and treatment decisions in allergy and immunology. How these technologies will be evaluated, approved, and assessed for their impact is an important consideration for researchers and practitioners alike. With the potential of ML, deep learning, natural language processing, and other assistive methods to redefine health care usage, a scaffold for the impact of AI technology on research and patient care in allergy and immunology is needed. An American Academy of Asthma Allergy and Immunology Health Information Technology and Education subcommittee workgroup was convened to perform a scoping review of AI within health care as well as the specialty of allergy and immunology to address impacts on allergy and immunology practice and research as well as potential challenges including education, AI governance, ethical and equity considerations, and potential opportunities for the specialty. There are numerous potential clinical applications of AI in allergy and immunology that range from disease diagnosis to multidimensional data reduction in electronic health records or immunologic datasets. For appropriate application and interpretation of AI, specialists should be involved in the design, validation, and implementation of AI in allergy and immunology. Challenges include incorporation of data science and bioinformatics into training of future allergists-immunologists.}
}
@incollection{PARMAR2022401,
title = {18 - 5G-enabled deep learning-based framework for healthcare mining: State of the art and challenges},
editor = {Sudeep Tanwar},
booktitle = {Blockchain Applications for Healthcare Informatics},
publisher = {Academic Press},
pages = {401-420},
year = {2022},
isbn = {978-0-323-90615-9},
doi = {https://doi.org/10.1016/B978-0-323-90615-9.00016-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323906159000165},
author = {Rahil Parmar and Dhruval Patel and Naitik Panchal and Uttam Chauhan and Jitendra Bhatia},
keywords = {5G healthcare, Deep learning, Big data, Human disease prediction},
abstract = {The importance of healthcare technologies has been made clear in the current pandemic. Healthcare informatics plays an important role in facilitating healthcare and providing healthcare services in real time. Healthcare informatics has developed from Healthcare 1.0 to Healthcare 4.0 in the last few decades. The data generated from the various sources are stored as electronic health record. These data are collected in different forms and formats. The inconsistent data could be handled using various techniques of big data. The information obtained from big data analytics can be used for the prediction of diseases or conditions using artificial intelligence, machine learning, and deep learning techniques. 5G plays an important role in healthcare informatics by enabling real-time remote monitoring and improving augmented reality, virtual reality, and spatial computing. With 5G technologies, a large number of devices can be connected using high-performance computing over large distances to provide healthcare services. Blockchain is applied in healthcare for health record management, insurance claims, drug tracking, authentication, and ensuring the integrity of medical data. Deep learning techniques can be applied to ever-changing data for the detection and prevention of disease. For the classification challenge, deep convolutional neural networks using pictures of diseased regions are often utilized. In many research techniques, AlexNet and GoogLeNet have been utilized to identify plant diseases. This chapter discusses the state of the art for detecting human sickness as well as the associated 5G healthcare framework for improving it.}
}
@article{XIE202272,
title = {Real-World Data for Healthcare Research in China: Call for Actions},
journal = {Value in Health Regional Issues},
volume = {27},
pages = {72-81},
year = {2022},
issn = {2212-1099},
doi = {https://doi.org/10.1016/j.vhri.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212109921000765},
author = {Jipan Xie and Eric Q. Wu and Shan Wang and Tao Cheng and Zhou Zhou and Jia Zhong and Larry Liu},
keywords = {administrative claims, data access, electronic health records, real-world data},
abstract = {Objectives
This study aimed to provide an overview of major data sources in China that can be potentially used for epidemiology, health economics, and outcomes research; compare them with similar data sources in other countries; and discuss future directions of healthcare data development in China.
Methods
The study was conducted in 2 phases. First, various data sources were identified through a targeted literature review and recommendations by experts. Second, an in-depth assessment was conducted to evaluate the strengths and limitations of administrative claims and electronic health record data, which were further compared with similar data sources in developed countries.
Results
Secondary databases, including administrative claims and electronic health records, are the major types of real-world data in China. There are substantial variations in available data elements even within the same type of databases. Compared with similar databases in developed countries, the secondary databases in China have some general limitations such as variations in data quality, unclear data usage mechanism, and lack of longitudinal follow-up data. In contrast, the large sample size and the potential to collect additional data based on research needs present opportunities to further improve real-world data in China.
Conclusions
Although healthcare data have expanded substantially in China, high-quality real-world evidence that can be used to facilitate decision making remains limited in China. To support the generation of real-world evidence, 2 fundamental issues in existing databases need to be addressed—data access/sharing and data quality.}
}
@article{JOSEPH2022115,
title = {A Predictive Maintenance Application for A Robot Cell using LSTM Model},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {115-120},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.193},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014082},
author = {Doyel Joseph and Tilani Gallege and Ebru Turanoglu Bekar and Catarina Dudas and Anders Skoogh},
keywords = {Smart Maintenance, Predictive Maintenance, Machine Learning, Long Short-Term Memory (LSTM), CRISP-DM, Industrial Robots, Manufacturing},
abstract = {Maintaining equipment is critical for increasing production capacity and decreasing production time. With the advent of digitalization, industries are able to access massive amounts of data that can be used to ensure their long-term viability and competitive advantage by implementing predictive maintenance. Therefore, this study aims to demonstrate a predictive maintenance application for a robot cell using real-world manufacturing big data coming from a company in the automotive industry. A hyperparameter tuned Long Short-Term Memory (LSTM) model is developed, and the results show that this model is capable of predicting the day of failure with good accuracy. The difficulties inherent in conducting real-world industrial initiatives are analyzed, and recommendations for improvement are presented.}
}
@incollection{REIMER2022135,
title = {Chapter 6 - Data management in culture collections},
editor = {İpek Kurtböke},
booktitle = {Importance of Microbiology Teaching and Microbial Resource Management for Sustainable Futures},
publisher = {Academic Press},
pages = {135-155},
year = {2022},
isbn = {978-0-12-818272-7},
doi = {https://doi.org/10.1016/B978-0-12-818272-7.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128182727000043},
author = {Lorenz Christian Reimer and Andrey Yurkov},
keywords = {Collection research, Data harmonisation, Data management, Databases, Literature, Microorganisms, Quality control},
abstract = {Culture collections preserve the living material and the associated information alike. The physical culture and its properties are both important for users. Strain characteristics can be inferred from a detailed description of environmental parameters and results of ex situ experiments. Some of these results will be published in the literature but others remain unpublished, such as strain tests performed by collection staff. Culture collections that did not restrict their holdings to a particular taxonomic or functional group of microorganisms consequently employed a large diversity of tests. In order to handle that heterogeneous information on a large scale, data management in culture collections is rapidly gaining importance. Data management includes several mobilisation and harmonisation steps. This chapter provides examples of data types routinely accumulated by culture collections, and how this information is unified, analysed and shared. Databases that accumulate and display records from collections worldwide become a window into modern big data research. In this chapter, we review different strategies for building up strain-related databases, point to important difficulties and name possible solutions.}
}
@article{SOUIFI2022103666,
title = {Uncertainty of key performance indicators for Industry 4.0: A methodology based on the theory of belief functions},
journal = {Computers in Industry},
volume = {140},
pages = {103666},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103666},
url = {https://www.sciencedirect.com/science/article/pii/S016636152200063X},
author = {Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar},
keywords = {Industry 4.0, Performance management, Decision support, Big Data, Uncertainty modeling},
abstract = {For the past few years, we have been hearing about Industry 4.0 (or the fourth industrial revolution), which promises to improve productivity, flexibility, quality, customer satisfaction and employee well-being. To assess whether these goals are achieved, it is necessary to implement a performance management system (PMS). However, a PMS must take into account the various challenges associated with Industry 4.0, including the availability of large amounts of data. While it represents an opportunity for companies to improve performance, big data does not necessarily mean good data. It can be uncertain, imprecise, ambiguous, etc. Uncertainty is one of the major challenges and it is essential to take it into account when computing performance indicators to increase confidence in decision making. To address this issue, we propose a method to model uncertainty in key performance indicators (KPIs). Our work allows associating with each indicator an uncertainty noted m, computed on the basis of the theory of belief functions. The KPI and its associated uncertainty form a pair (KP I, m). The method developed allows calculating this uncertainty m for the input data of the performance management system. We show how these modeled uncertainties should be propagated to the KPIs. For these KPI uncertainties, we have defined rules to support decision-making. The method developed, based on the theory of belief functions, is part of a methodology we propose to define and extract smart data from massive data. To our knowledge, this is the first attempt to use this theory to model uncertain performance indicators. Our work has shown its effectiveness and its applicability to a case of bottle filling line simulation. In addition to these results, this work opens up new perspectives, particularly for taking uncertainty into account in expert opinions and in industrial risk assessment.}
}
@incollection{NASSEHI2022317,
title = {Chapter 11 - Review of machine learning technologies and artificial intelligence in modern manufacturing systems},
editor = {Dimitris Mourtzis},
booktitle = {Design and Operation of Production Networks for Mass Personalization in the Era of Cloud Technology},
publisher = {Elsevier},
pages = {317-348},
year = {2022},
isbn = {978-0-12-823657-4},
doi = {https://doi.org/10.1016/B978-0-12-823657-4.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128236574000026},
author = {Aydin Nassehi and Ray Y. Zhong and Xingyu Li and Bogdan I. Epureanu},
keywords = {Predictive maintenance, Artificial intelligence, Machine learning, Smart manufacturing, Industry 4.0},
abstract = {With the advent of new methods usually identified under the banners of artificial intelligence (AI) and machine learning (ML), statistical analysis methods of complex and uncertain manufacturing systems have been undergoing significant changes. Therefore, various definitions of AI, a brief history, and its differences with traditional statistics are presented. Moreover, ML is introduced to identify its place in data science and differences to topics such as big data analytics and manufacturing problems that use AI and ML are then characterized. Next, a lifecycle-based approach is adopted and the use of various methods in each phase is analyzed, identifying the most useful techniques and the unifying attributes of AI in manufacturing. Finally, the chapter maps out future developments of AI and the emerging trends and identifies a vision based on combining machine and human intelligence in a productive and empowering manner as well. This vision presents humans and increasingly more intelligent machines, not as competitors, but as partners allowing creative and innovative paradigms to emerge.}
}
@article{MAHAJAN2022415,
title = {Data to the people: a review of public and proprietary data for transport models},
journal = {Transport Reviews},
volume = {42},
number = {4},
pages = {415-440},
year = {2022},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2021.1977414},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722007036},
author = {Vishal Mahajan and Nico Kuehnel and Aikaterini Intzevidou and Guido Cantelmo and Rolf Moeckel and Constantinos Antoniou},
keywords = {Open data, public data, transport model, big data, transport modelling},
abstract = {ABSTRACT
Data play an indispensable role in transport modelling. The availability of data from non-conventional sources, such as mobile phones, social media, and public transport smart cards, changes the way we conduct mobility analyses and travel forecasting. Existing studies have demonstrated the multitude and varied applications of these emerging data in transport modelling. The transferability of current research and further endeavours depend mostly on the availability of these data. Therefore, the openness or public availability of the prominent data for transport modelling needs to be adequately investigated. Such a discussion should also encompass these data’s application aspects to provide a holistic overview. This paper defines a typology for the data classification based on a set of availability or openness attributes from the existing literature. Subsequently, we use the developed typology to classify the prominent transport data into four categories: (i) Commercial data, (ii) Inaccessible data, (iii) Gratis and accessible data with restricted use, and (iv) Open data. Using this typology, we conclude that the public data, which refer to the data that are accessible and free of cost, are a superset of open data. Further, we discuss the applications and limitations of the selected data in transport modelling and highlight in which task(s) certain data excel. Lastly, we synthesise our review using a Strengths, Weaknesses, Opportunities and Threats (SWOT) analysis to bring out the aspects relevant to data owners and data consumers. Public availability of data can help in various modelling steps such as trip generation, accessibility, destination choice, route choice, network modelling. Complementary datasets such as General Transit Feed Specification (GTFS) and Volunteered Geographic Information (VGI) increase the usability of other data. Thus, modellers can gain from the positive cascade effect by prioritising these data. There is also a potential for data owners to release proprietary data, such as mobile phone data, with restricted-use licenses after addressing privacy risks. Our study contributes by dealing with two problems at the same time. On the one hand, the paper analyses existing data based on their potential for mobility studies. On the other hand, we classify them based on how open they are. Hence, we identify the most promising public data for developing the next generation of transport models.}
}
@article{TAYLOR2022107492,
title = {An interdisciplinary research perspective on the future of multi-vector energy networks},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {135},
pages = {107492},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107492},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521007316},
author = {P.C. Taylor and M. Abeysekera and Y. Bian and D. Ćetenović and M. Deakin and A. Ehsan and V. Levi and F. Li and R. Oduro and R. Preece and P.G. Taylor and V. Terzija and S.L. Walker and J. Wu},
keywords = {Energy markets, Information and communication technologies, Modelling, Multi-vector energy networks, Policy, Risk},
abstract = {Understanding the future of multi-vector energy networks in the context of the transition to net zero and the energy trilemma (energy security, environmental impact and social cost) requires novel interdisciplinary approaches. A variety of challenges regarding systems, plant, physical infrastructure, sources and nature of uncertainties, technological in general and more specifically Information and Communication Technologies requirements, cyber security, big data analytics, innovative business models and markets, policy and societal changes, are critically important to ensure enhanced flexibility and higher resilience, as well as reduced costs of an integrated energy system. Integration of individual energy networks into multi-vector entities opens a number of opportunities, but also presents a number of challenges requiring interdisciplinary perspectives and solutions. Considering drivers like societal evolution, climate change and technology advances, this paper describes the most important aspects which have to be taken into account when designing, planning and operating future multi-vector energy networks. For this purpose, the issues addressing future architecture, infrastructure, interdependencies and interactions of energy network infrastructures are elaborated through a novel interdisciplinary perspective. Aspects related to optimal operation of multi-vector energy networks, implementation of novel technologies, jointly with new concepts and algorithms, are extensively discussed. The role of policy, markets and regulation in facilitating multi-vector energy networks is also reported. Last but not least, the aspects of risks and uncertainties, relevant for secure and optimal operation of future multi-vector energy networks are discussed.}
}
@incollection{SEBASTIANCOLEMAN202293,
title = {Chapter 5 - The Process Challenge: Managing for Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {93-117},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000055},
author = {Laura Sebastian-Coleman},
keywords = {Product quality management, quality by design, dimensions of quality, product life cycle, Juran Trilogy, data quality management, quality improvement methodology},
abstract = {This chapter presents foundational principles of quality management and applies these principles to data. Organizations do not produce high-quality products by accident. High-quality results depend on planning and commitment. Data is both a product of organizational processes and a resource required to execute those processes. Although data differs from other products and assets, with respect to its life cycle and the ways in which organizations can derive value from it, the product model of data nevertheless provides the foundational components on data quality management. It also allows us to see the connections between the process challenge and the other challenges at different points in the life cycle.}
}
@article{WU202248,
title = {Process modeling by integrating quantitative and qualitative information using a deep embedding network and its application to an extrusion process},
journal = {Journal of Process Control},
volume = {115},
pages = {48-57},
year = {2022},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0959152422000749},
author = {Haibin Wu and Yu-Han Lo and Le Zhou and Yuan Yao},
keywords = {Process modeling, Small data, Deep neural network, Embedding, Autoencoder},
abstract = {In the big data era, small data problems still exist in many industrial sectors. Taking the high-value process industries as an example, a large number of materials and processing methods are often tested at the design stage. However, only a small amount of data can be collected for each material-process combination, which poses a serious challenge to data-driven process modeling. There is a great necessity to integrate the small data measured in different tasks and build the process model by sharing the information. In this work, a deep embedding neural network is proposed to extract the qualitative task information for process modeling. Specifically, an autoencoder is used to learn embeddings which are combined with the quantitative process conditions as the inputs of a feed-forward neural network to produce the final predictions. The feasibility, including interpretability and prediction accuracy, of the developed method is illustrated with an extrusion process.}
}
@article{WANG2022106310,
title = {Are the official national data credible? Empirical evidence from statistics quality evaluation of China's coal and its downstream industries},
journal = {Energy Economics},
volume = {114},
pages = {106310},
year = {2022},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2022.106310},
url = {https://www.sciencedirect.com/science/article/pii/S014098832200439X},
author = {Delu Wang and Fan Chen and Jinqi Mao and Nannan Liu and Fangyu Rong},
keywords = {Industrial statistics, Data quality, Comprehensive evaluation, Coal-related industry},
abstract = {The authenticity and quality of industrial statistical data directly affects all types of systematic research based on it. Considering the limitations of extant data quality evaluation literature on research objects and evaluation methods, we constructed a new data quality comprehensive inspection and evaluation model based on Benford's Law (BL) and the technique for order of preference by similarity to ideal solution (TOPSIS), selected coal-related industries as the research object, and conducted an empirical test along the research path of “Industry→Province→Indicator”. The results showed that, at industry level, the quality of statistical data for China's coal-related industries from 2001 to 2016 was generally poor. Among the eight sample industries selected, the data quality for five industries (including coal, electricity, and steel) was assessed as poor or slightly poor. Furthermore, at the provincial level, there is significant spatial heterogeneity in the quality of statistical data for various industries affected by factors such as economic structure, marketization level, and industrial diversity. Compared with other types of statistical indicators, industry financial indicators are more prone to data quality problems at the indicator level, and the suspicious indicators of different industries show certain common characteristics and some industry differences. To improve the quality of industrial statistical data and reduce the possible adverse impacts of data quality problems, based on the research findings, we propose targeted countermeasures and suggestions on how to prevent data fraud and effectively identify and rationally use suspicious data.}
}
@article{MORADI2022300,
title = {Applications of artificial intelligence in B2B marketing: Challenges and future directions},
journal = {Industrial Marketing Management},
volume = {107},
pages = {300-314},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2022.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0019850122002553},
author = {Masoud Moradi and Mayukh Dass},
keywords = {Artificial intelligence, Machine learning, Propensity model, Deep learning, B2B marketing, Technology acceptance model},
abstract = {With the growing popularity of artificial intelligence (AI) transforming business-to-business (B2B) marketing, there is a growing demand to comprehensively understand the adoption and application of AI to advance B2B marketing. This study examines AI methods and their applications in B2B marketing across the four customer life cycle stages of reach, acquisition, conversion, and retention. The paper also analyzes and synthesizes the findings of five B2B industry surveys conducted to do the following: 1) examine B2B marketers' knowledge and attitudes toward using AI in their businesses, 2) determine the various ways in which AI is used in B2B marketing, and 3) investigate the perceived merits and challenges of using AI in B2B marketing. The findings reconcile various machine learning (ML) techniques suitable for use by B2B marketers. Employing the technology acceptance model (TAM), the paper identifies how B2B marketers perceive the benefits of AI adoption. Furthermore, this study discusses the perceived barriers to AI adoption, including data privacy challenges and the replacement of human workforces. To further highlight the benefits of AI, the study showcases three examples of successful AI adoption in B2B marketing. The paper concludes by summarizing the theoretical and managerial implications of AI adoption in B2B marketing and directions for future studies.}
}
@article{KONGBOON2022130711,
title = {Greenhouse gas emissions inventory data acquisition and analytics for low carbon cities},
journal = {Journal of Cleaner Production},
volume = {343},
pages = {130711},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130711},
url = {https://www.sciencedirect.com/science/article/pii/S095965262200350X},
author = {Ratchayuda Kongboon and Shabbir H. Gheewala and Sate Sampattagul},
keywords = {Sustainable city, Low carbon city, Greenhouse gas inventory, Greenhouse gas emissions, Municipalities},
abstract = {This paper studied greenhouse gas inventory data acquisition and analytics for municipalities in Thailand. A complete and transparent GHG inventory of eight municipalities was developed to document the current situation, and to help decision-makers to clarify their priorities for reducing greenhouse gas emissions. The Global Protocol for Community-Scale Greenhouse Gas Emissions Inventories guidelines was used to investigate and calculate the greenhouse gas emissions and assess data accuracy. The results indicated that the data source, data format, and data collection of each municipality are relatively similar. Moreover, the activity data needed to be obtained from several authorities. The results showed that Nonthaburi Municipality had the highest greenhouse gas emissions at 2,286,838 tCO2e/yr and Buriram Municipality, the lowest at 239,795 tCO2e/yr. On a per-capita basis, Lamphun Municipality was the highest with 10.1 tCO2e/capita and Buriram Municipality the lowest with 3.8 tCO2e/capita. The results suggest that the municipalities should continually develop a GHG database by creating a routine procedure. An information management system should be produced in the shape of big data which can lead to state policies, plans, and actions for city development to ensure the reduction of greenhouse gas emissions. This in turn will lead to a low carbon city.}
}
@article{SUSHA2022101763,
title = {An ecosystem perspective on developing data collaboratives for addressing societal issues: The role of conveners},
journal = {Government Information Quarterly},
pages = {101763},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101763},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22000995},
author = {Iryna Susha and Tijs {van den Broek} and Anne-Fleur {van Veenstra} and Johan Linåker},
keywords = {, , , },
abstract = {With the open and big data movement in full swing, data sharing becomes more ubiquitous and more often crosses sectoral boundaries. The promise of data to help address societal issues and foster innovation requires public organizations to work together with businesses and researchers. Data collaboratives whereby actors collaborate to share and use data for public good gain increasing interest. Most of these collaborations, however, tend to be one-off, small, and limited in impact due to a complex web of legal, technical, ethical, commercial, and organizational challenges. Initiators of data collaboratives, termed as conveners, can potentially alleviate some of these concerns by playing various roles in developing a more sustainable data ecosystem for the data collaboratives. Our study investigates what convener roles are perceived to be critical in developing data collaboratives. By drawing on data ecosystems thinking, we developed a framework of convener roles and sub-roles which we further used to analyze four cases in the Netherlands and Sweden. We conclude that connecting role and learning catalyst role are critical at the initiation stage, while stimulating and mediating roles emerge as future critical roles as the data ecosystem develops. We further identified convener meta-roles that are associated with particular data ecosystem structures (keystone-centric, marketplace-based, intermediary-based, and platform-centric). Our research can be instrumental to actors leading the efforts of creating such data ecosystems as it provides insights on the needs and resources that can be leveraged to stimulate development and innovation.}
}
@incollection{KOUL20223,
title = {Chapter 1 - Influence and implementation of Industry 4.0 in health care},
editor = {Aboul Ella Hassanien and Jyotir Moy Chatterjee and Vishal Jain},
booktitle = {Artificial Intelligence and Industry 4.0},
publisher = {Academic Press},
pages = {3-21},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-88468-6},
doi = {https://doi.org/10.1016/B978-0-323-88468-6.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884686000024},
author = {Sumit Koul},
keywords = {Industrial revolution, Healthcare 4.0, Blockchain technology, Internet of Things (IoT), Big data},
abstract = {With the advancement of technology, the world is changing and automating at a rapid pace. Digitization plays a major role in the automation of technology. In this context, Industry 4.0 shows how industrial production is developing along with the latest technology. In Industry 4.0, much manual work has been replaced by automated machines that can be controlled with developing technologies such as artificial intelligence (AI), big data, cloud computing, and so on. Various technologies have been introduced in the healthcare sector due to Industry 4.0. These include AI, three-dimensional printing, machine learning, cognitive systems, autonomous robots, autonomous vehicles, augmented reality, big data, Internet of Things (IoT), blockchain technology, and more. This chapter discusses the transformation of the healthcare industry in the context of Industry 4.0. It presents a detailed study on big data, IoT, and blockchain technology with different applications that can enhance what is known as Healthcare 4.0. The chapter includes three case studies that illustrate the use of innovation in Healthcare 4.0 to detect and diagnose disease using portable medical devices connected to the IoT.}
}
@article{LI2022101808,
title = {Urban population distribution in China: Evidence from internet population},
journal = {China Economic Review},
volume = {74},
pages = {101808},
year = {2022},
issn = {1043-951X},
doi = {https://doi.org/10.1016/j.chieco.2022.101808},
url = {https://www.sciencedirect.com/science/article/pii/S1043951X22000669},
author = {Huixuan Li and Jing Chen and Zihao Chen and Jianguo Xu},
keywords = {Internet population, Population distribution, Zipf's law, Public resource distortions, Big data},
abstract = {Based on mobile internet user data, we construct an “Internet population” measure and reexamine spatial population distribution in China. The location based service (LBS) data of mobile internet uses is able to capture the accurate location of users' residence and solve the underestimation problem of missing migrants. We have three main findings. First, contrary to previous studies based on traditional population statistics, city size distribution of Internet population fits well into Zipf's law with a R2 of 90.7%. Second, the Internet population indicator is superior to traditional population statistics in explaining inelastic household consumption such as water consumption, electricity consumption, and garbage disposal. It suggests that the “Internet population” is a better proxy of actual city population. Third, the traditional population statistics systematically overestimate population in small cities and underestimate population in large cities. It indicates that the public resource distortions will continue to exist or even worsen off in China if the allocation process relies greatly on traditional population statistics. Although no measures are perfect, our new population measure provides important incremental information for future discussion.}
}
@incollection{KARACA202221,
title = {Chapter 3 - Multi-chaos, fractal and multi-fractional AI in different complex systems},
editor = {Yeliz Karaca and Dumitru Baleanu and Yu-Dong Zhang and Osvaldo Gervasi and Majaz Moonis},
booktitle = {Multi-Chaos, Fractal and Multi-Fractional Artificial Intelligence of Different Complex Systems},
publisher = {Academic Press},
pages = {21-54},
year = {2022},
isbn = {978-0-323-90032-4},
doi = {https://doi.org/10.1016/B978-0-323-90032-4.00016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390032400016X},
author = {Yeliz Karaca},
keywords = {Chaotic systems, Complex order, Complexity, Computational complexity, Data ethics, Different complex systems, Dynamics of complex systems, Evolution, Fractional thinking, Nonlinearity and irregularity},
abstract = {Modern scientific thinking adopts the systemic properties and addresses them through revealing the spontaneous processes related to self-organization in a dynamical system in a state far from the equilibrium point and close to the disequilibrium point with no existence of external force acting upon the system. The modern way of thinking poses a challenge against the dichotomy between the natural world and social world, by taking into account the concepts around complexity, evolution and order. This study provides an overview encompassing multi-chaos, fractal, fractional and Artificial Intelligence (AI) way of thinking for the solution of the complex system problems concerned with natural and social sciences. Furthermore, ethical decision-making frameworks and strategies concerning big data and AI applications to provide assistance for the identification of the related problems in different settings and help thinking in a methodical manner with a deliberative compensating process so that tensions between conflicting aspects can be managed systematically. The values related to ethical issues, which are thorny in nature, point to being practical, flexible and problem-driven rather than purely theory-driven in order that dilemmas can be addressed and critical decision-making guided in a way beyond theoretical positions with a focus on applied aspects. Through the lenses of such transformative thinking along with mathematics-informed frameworks encompassing chaos, fractal and multi-fractional ways, the incorporation of technology, with Artificial Intelligence, as the most viable and far-reaching leg, is essentially required to be able to address and tackle complexity that has chaotic, nonlinear, and dynamic characteristics. Hence, optimized solutions can be conceived and implemented efficiently and in a facilitating way with some required degree of flexibility as well. Considering the impact and ubiquity of data technologies concerning all aspects of modern life, it becomes important to establish a balance between data use and ethical matters. Computational technologies in different complex systems based on mathematical-driven informed frameworks can enable the generation of more realistic and applicable adaptive models under transient, dynamic and ever-evolving conditions of different complex systems.}
}
@article{RODRIGUES2022101625,
title = {Species misidentification affects biodiversity metrics: Dealing with this issue using the new R package naturaList},
journal = {Ecological Informatics},
volume = {69},
pages = {101625},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101625},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122000747},
author = {Arthur Vinicius Rodrigues and Gabriel Nakamura and Vanessa Graziele Staggemeier and Leandro Duarte},
keywords = {Occurrence records, Biodiversity, Species misidentification, Taxonomy, Ecological patterns},
abstract = {Biodiversity databases are increasingly available and have fostered accelerated advances in many disciplines within ecology and evolution. However, the quality of the evidence generated depends critically on the quality of the input data, and species misidentifications are present in virtually any occurrence dataset. Yet, the lack of automatized tools makes the assessment of the quality of species identification in big datasets time-consuming, which often induces researchers to assume that all species are reliably identified. In this study, we address this issue by evaluating how species misidentification can impact our ability to capture ecological patterns, and by presenting an R package, called naturaList, designed to classify species occurrence data according to identification reliability. naturaList allows the classification of species occurrences up to six confidence levels, in which the highest level is assigned to records identified by specialists. We obtained a list of specialists by using the species occurrence dataset itself, based on the identifier names within it, and by entering an independent list, obtained by contacting experts. Further, we evaluate the effects of filtering out occurrence records not identified by specialists on the estimations of species niche and diversity patterns. We used the tribe Myrteae (Myrtaceae) as a study model, which is a species-rich group in Central and South America and with challenging taxonomy. We found a significant change in species niche in 13% of species when using only occurrences identified by specialists. We found changes in patterns of alpha diversity in four genera and changes in beta diversity in all genera analyzed. We show how the uncertainty in species identification in occurrence datasets affects conclusions on macroecological patterns by generating bias or noise in different aspects of macroecological patterns (niche, alpha, and beta diversity). Therefore, to guarantee reliability in species identification in big data sets we recommend the use of automated tools such as the naturaList package, especially when analyzing variation in species composition. This study also represents a step forward to increasing the quality of large-scale studies that rely on species occurrence data.}
}
@article{CHEN202214274,
title = {Short-term load forecasting for multiple buildings: A length sensitivity-based approach},
journal = {Energy Reports},
volume = {8},
pages = {14274-14288},
year = {2022},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.10.425},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722023654},
author = {Yongbao Chen and Zhe Chen},
keywords = {Short-term load forecasting, Big data for buildings, Data-driven models, LightGBM, Length sensitivity analysis},
abstract = {With the rapid development of large-scale building energy monitoring platforms, it is of great significance to develop precise forecasting methods for buildings on a large scale to achieve better energy system design, system operation, energy management, and renewable energy integration in the grid. Traditionally, using all available historical data to train a data-driven model has been widely employed to ensure prediction performance because more historical information can be learned. However, this strategy may introduce more noise, especially for short-term load forecasting. Thus, this study proposes a novel approach for selectively utilizing building historical data to determine the amount of data that should be used to train the data-driven model. First, the CV(RMSE) curve of each building reflecting the relationship between training data length and forecasting accuracy is obtained using LightGBM. Second, clustering techniques such as k-means are used to identify buildings that are sensitive to the training data length based on CV(RMSE) curves. Finally, the optimal training data length for day-ahead forecasting is estimated for each building. The case study shows that approximately 20% of buildings in the Building Data Genome are labeled as length-sensitive buildings, and adopting appropriate training data lengths can reduce the prediction error of these buildings by up to 15%.}
}
@article{DAHIYA2022100066,
title = {Analysis of the single-regime speed-density fundamental relationships for varying spatiotemporal resolution using Zen Traffic Data},
journal = {Asian Transport Studies},
volume = {8},
pages = {100066},
year = {2022},
issn = {2185-5560},
doi = {https://doi.org/10.1016/j.eastsj.2022.100066},
url = {https://www.sciencedirect.com/science/article/pii/S2185556022000128},
author = {Garima Dahiya and Yasuo Asakura and Wataru Nakanishi},
keywords = {Macroscopic traffic flow, Vehicle trajectory data, Big data and naturalistic datasets, Speed-density relationship, Statistical and theoretical analysis, Spatiotemporal resolutions},
abstract = {This study analyzed the single-regime speed-density (v-k) relationships for urban expressways using high resolution Zen Traffic Data (ZTD) containing all vehicles’ trajectory data obtained using image sensing technology. The steady-state traffic data were extracted for varying spatiotemporal resolutions, followed by estimation of traffic flow parameters, namely, jam density, kinematic-wave-speed, and proportionality factor, a behavioral parameter, using empirical data. Functional and shape parameters were estimated using the Levenberg–Marquardt algorithm. Statistical metrics were used to assess the performance and model fitness in all categories of linear, exponential and logarithmic, and complex forms of v-k relationships for different resolutions. The theoretical analysis revealed that certain relationships satisfy all the static properties and that only one satisfies both the dynamic properties of traffic behavior. Highly parameterized forms had the lowest errors. However, the linear form of model developed by May and Keller has high application potential.}
}
@article{YU2022103483,
title = {Towards a privacy-preserving smart contract-based data aggregation and quality-driven incentive mechanism for mobile crowdsensing},
journal = {Journal of Network and Computer Applications},
volume = {207},
pages = {103483},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103483},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001291},
author = {Ruiyun Yu and Ann Move Oguti and Dennis Reagan Ochora and Shuchen Li},
keywords = {Mobile crowdsensing, Smart contracts, Data aggregation, Incentive mechanism, dApp, IPFS},
abstract = {The crowd's power, combined with the sensing capabilities of smart mobile de-vices, has resulted in the emergence of a revolutionary data acquisition paradigm known as Mobile Crowdsensing. In exchange for rewards, mobile users collect and share location-specific data values. However, most existing crowdsensing systems built on traditional centralized architectures are highly prone to attacks, intrusions, single point of failure, manipulations, and low reliability. Recently, decentralized blockchain technologies are being applied in mobile crowdsensing systems to ensure workers' privacy, data privacy, and the quality of sensed data at a low service fee. By leveraging blockchain technology, this paper inherits the advantages of the public blockchain without the need for any trusted third-parties. We propose a smart contract-based privacy-preserving data aggregation and quality assessment protocol to infer reliable aggregated results and estimate data quality, wherein, we design a fair quality-driven incentive mechanism to distribute rewards based on the data quality. The protocol ensures a secure, cost-optimal, and reliable aggregation and estimation of the sensed data quality on the public blockchain without disclosing the sensed data's and participants' privacy. We adopt Interplanetary File Systems to offset the blockchain's expensive storage costs. Experiments were conducted using real-world datasets which were implemented on a full-stack on-chain and off-chain decentralized application on the Ethereum blockchain. The experimental results demonstrate our design is highly efficient in achieving privacy-preserving data aggregation and significantly reduces on-chain computation costs.}
}
@article{POWELL2022100261,
title = {Garbage in garbage out: The precarious link between IoT and blockchain in food supply chains},
journal = {Journal of Industrial Information Integration},
volume = {25},
pages = {100261},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100261},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000595},
author = {Warwick Powell and Marcus Foth and Shoufeng Cao and Valéri Natanelov},
keywords = {Blockchain, Data quality, Distributed ledger technology, Meat industry, Internet of things, Food supply chains},
abstract = {The application of blockchain in food supply chains does not resolve conventional IoT data quality issues. Data on a blockchain may simply be immutable garbage. In response, this paper reports our observations and learnings from an ongoing beef supply chain project that integrates Blockchain and IoT for supply chain event tracking and beef provenance assurance and proposes two solutions for data integrity and trust in the Blockchain and IoT-enabled food supply chain. Rather than aiming for absolute truth, we explain how applying the notion of ‘common knowledge’ fundamentally changes oracle identity and data validity practices. Based on the learnings derived from leading an IoT supply chain project with a focus on beef exports from Australia to China, our findings unshackle IoT and Blockchain from being used merely to collect lag indicators of past states and liberate their potential as lead indicators of desired future states. This contributes: (a) to limit the possibility of capricious claims on IoT data performance, and; (b) to utilise mechanism design as an approach by which supply chain behaviours that increase the probability of desired future states being realised can be encouraged.}
}
@article{SHAO2022102736,
title = {IoT data visualization for business intelligence in corporate finance},
journal = {Information Processing & Management},
volume = {59},
number = {1},
pages = {102736},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102736},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002181},
author = {Cuili Shao and Yonggang Yang and Sapna Juneja and Tamizharasi GSeetharam},
keywords = {IoT, Data visualization, Business intelligence, Corporate finance},
abstract = {Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability.}
}
@article{RUJUAN2022367,
title = {Research on E-learning Behavior Evaluation of Students Based on Three-way Decisions Classification Algorithm},
journal = {Procedia Computer Science},
volume = {208},
pages = {367-373},
year = {2022},
note = {7th International Conference on Intelligent, Interactive Systems and Applications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014934},
author = {Wang Rujuan and Wang Lei},
keywords = {Big data, educational data mining, learning behavior, three-way decisions classification algorithm},
abstract = {With the popularization and application of E-learning platform, it is feasible to collect learning behavior data of students, which provides data basis for analyzing the knowledge and rules contained in learning process of students. At present, the common e-learning platform only realizes the simple statistics and display function for the data, and does not do further in-depth calculation. The data observed by educators are still superficial learning phenomena, and they cannot see the learning rules behind the appearance. It is difficult to effectively guide students, change their learning routes and feedback teaching strategies. In view of this, the main characteristics of learning behavior deeply mined from learning activities and learning evaluation, and the important data feature items affect E-learning effect obtained. Three decision classification algorithms applied to establish prediction and evaluation model for students with learning behavior risk. The overall accuracy of model test is in line with the expectation. This provides a practical application modeling method for further implementation of personalized learning service.}
}
@article{KAYABAY2022121264,
title = {Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121264},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121264},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006983},
author = {Kerem Kayabay and Mert Onuralp Gökalp and Ebru Gökalp and P. {Erhan Eren} and Altan Koçyiğit},
keywords = {Technology roadmapping, Technology management, Data science, Digital transformation, Data-driven organization, Big data},
abstract = {Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.}
}
@article{LI2022341,
title = {Development of a machine learning-based risk prediction model for cerebral infarction and comparison with nomogram model},
journal = {Journal of Affective Disorders},
volume = {314},
pages = {341-348},
year = {2022},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2022.07.045},
url = {https://www.sciencedirect.com/science/article/pii/S0165032722008138},
author = {Xuewen Li and Yiting Wang and Jiancheng Xu},
keywords = {Cerebral infarction, Machine learning, Fibrinogen, Extreme gradient boosting, Risk prediction, Nomogram},
abstract = {Background
Development of a cerebral infarction (CI) risk prediction model by mining routine test big data with machine learning algorithms.
Methods
Cohort 1 included 2017 CI patients and health checkers, and the optimal machine learning algorithms in Extreme gradient Boosting (XgBoost), Logistic Regression (LR), Support Vector Machine (SVM), Random Forest (RF) were selected to mine all routine test data of the enrolled subjects for screening CI model features. Cohort 2 included patients with CI and Non-CI from 2018 to 2020 to develop an early warning model for CI and was analyzed in subgroups with a cutoff of 50 years. Cohort 3 included CI patients versus Non-CI patients in 2021, and a nomogram models was developed for comparison with the machine learning model.
Results
The optimal algorithm XgBoost was used to develop a CI risk prediction model CI-Lab8 containing eight characteristics of fibrinogen, age, glucose, mean erythrocyte hemoglobin concentration, albumin, neutrophil absolute value, activated partial thromboplastin time, and triglycerides. The model had an AUC of 0.823 in cohort 2, significantly higher than the FIB (AUC = 0.737), which ranked first in feature importance. CI-Lab8 also had higher diagnostic accuracy in CI patients <50 years of age (AUC = 0.800), slightly lower than in CI patients ≥50 years of age (AUC = 0.856). Receiver operating characteristic curve, calibration curve, and decision curve analysis in cohort 3 showed CI-Lab8 to be superior to nomogram.
Conclusion
In this study, the CI risk prediction model developed by XgBoost algorithm outperformed the nomogram model and had higher diagnostic accuracy for CI patients in both <50 and ≥50 years old, which may assist clinical assessment for CI.}
}
@article{HUANG2022138,
title = {An improved federated learning approach enhanced internet of health things framework for private decentralized distributed data},
journal = {Information Sciences},
volume = {614},
pages = {138-152},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522011306},
author = {Chenxi Huang and Gengchen Xu and Sirui Chen and Wen Zhou and Eddie Y.K. Ng and Victor Hugo C. de Albuquerque},
keywords = {Federal learning, Machine learning, Internet of things, Multi-center privacy protection},
abstract = {With the privacy protection increasingly being concerned, Data centralization often heavily causes a big risk of privacy protection, gradually, there is a prevailing trend to enhance the security performance by means of data decentralization, above all, for health care internet of things (IoT) data. Meanwhile, Federated learning has obvious privacy advantages compared to data center training on protecting privacy data. For this reason, a novel framework based on federated learning is presented in this paper, which is suitable for private and decentralized data sets, such as big data in healthy Internet of Things. Specifically, the main work of the puts forward framework includes: (1) Multi-center data collection of healthy Internet of Things. (2) healthy data analysis of Internet of Things. (3) privacy protection method for data of healthy Internet of Things. Finally, related experiments show that the proposed method is feasible, and compared with the traditional methods, it has significantly improved the performance in Quality of Service (QoS) and IoUs indicator.}
}
@article{DING2022136,
title = {An Internet of Things based scalable framework for disaster data management},
journal = {Journal of Safety Science and Resilience},
volume = {3},
number = {2},
pages = {136-152},
year = {2022},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S2666449621000542},
author = {Zhiming Ding and Shan Jiang and Xinrun Xu and Yanbo Han},
keywords = {Disaster data management, IoT, Disaster detection, Big data, Artificial intelligence},
abstract = {In recent years, undesirable disasters attacked the cities frequently, leaving heavy casualties and serious economic losses. Meanwhile, disaster detection based on the Internet of Things(IoT) has become a hot spot that benefited from the established development of smart city construction. And the IoT is visibly sensitive to the management and monitoring of disasters, but massive amounts of monitoring data have brought huge challenges to data storage and data analysis. This article develops a new and much more general framework for disaster emergency management under the IoT environment. The framework is a bottom-up integration of highly scalable Raw Data Storages(RD-Stores) technology, hybrid indexing and queries technology, and machine learning technology for emergency disasters. Experimental results show that hybrid index and query technology have better performance under the condition of supporting multi-modal retrieval, and providing a better solution to offer real-time retrieval for the massive sensor sampling data in the IoT. In addition, further works to evaluate the top-level sub-application system in this framework were performed based on the GPS trajectory data of 35,000 Beijing taxis and the volumetric ground truth data of 7,500 images. The results show that the framework has desirable scalability and higher utility.}
}
@article{NKIKABAHIZI2022108908,
title = {Chaining Zscore and feature scaling methods to improve neural networks for classification},
journal = {Applied Soft Computing},
volume = {123},
pages = {108908},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108908},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002733},
author = {Calpephore Nkikabahizi and Wilson Cheruiyot and Ann Kibe},
keywords = {Feature scaling, Outliers, Data quality, Classification, Chaining},
abstract = {Neural networks for classification aim at identifying the class label of new observation based training data containing instances whose category memberships are known. Therefore the data fed into neural networks has to be preprocessed to enhance its quality resulting in promoting the extraction of meaningful insights of data. However, the fact of processing data until you have the required high quality is challenging and time-consuming to manually search for the best method in a sequence of preprocessing independent methods. For feature scaling methods, they consist of scaling the dataset into the same range of data without monitoring data outliers that should eventually occur in the data source. Zscore for outlier’s detection suffers from the issue of predefining the parameters. This paper discussed various approaches that are applied to scale features and detect outliers during data pre-processing. Thereafter, the paper proposed the algorithm that combines Zscore as an outlier’s detection method with every classical feature scaling method in high-dimensional data. The proposed algorithm has benefits in selecting the optimal subset of methods from a sequence of chained methods, detecting outliers, and removing zero variance predictors. The study findings from five sample sizes revealed that the proposed method significantly excels the classical method in terms of accuracy. The outstanding from them was performed at the rate of 99.67% and had a significant difference of 0.20% over classical feature scaling.}
}
@article{MIA2022100238,
title = {A privacy-preserving National Clinical Data Warehouse: Architecture and analysis},
journal = {Smart Health},
volume = {23},
pages = {100238},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100238},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000544},
author = {Md Raihan Mia and Abu Sayed Md Latiful Hoque and Shahidul Islam Khan and Sheikh Iqbal Ahamed},
keywords = {Privacy, Standardization, Clinical data warehousing, Data modeling, Big data analytics, Decision supports},
abstract = {A centralized clinical data repository is essential for inspecting patients’ medical history, disease analysis, population-wide disease research, treatment decision support, and improving existing healthcare policies and services. Bangladesh, a rapidly developing country, poses several unusual challenges for developing such a centralized clinical data repository as the existing Electronic Health Records (EHR) are stored in unconnected, heterogeneous sources with no unique patient identifier and consistency. Data integration with secure record linkage, privacy preservation, quality control, and data standardization are the main challenges for developing a consistent and interoperable centralized clinical data repository. Based on the findings from our previous researches, we have designed an anonymous National Clinical Data Warehouse (NCDW) framework to reinforce research and analysis. The architecture of NCDW is divided into five stages to overcome the challenges: (1) Wrapper-based anonymous data acquisition; (2) Data loading and staging; (3) Transformation, standardization, and uploading to the data warehouse; (4) Management and monitoring; (5) Data Mart design, OLAP server, data mining, and applications. A prototype of NCDW has been developed with a complete pipeline from data collection to analytics by integrating three data sources. The proposed NCDW model facilitates regional and national decision support, intelligent disease analysis, knowledge discovery, and data-driven research. We have inspected the analytical efficacy of the framework by qualitative evaluation of the national decision support from two derived disease data marts. The experimental result based on the analysis is satisfactory to extend the NCDW on a large scale.}
}
@incollection{HOVENGA2022209,
title = {Chapter 9 - Quality data, design, implementation, and governance},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {209-237},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00013-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234136000136},
author = {Evelyn Hovenga and Heather Grain},
keywords = {Data quality, Data design, System implementation, Information governance, Leadership, Data supply chain},
abstract = {Data is the core for digital health systems; that data needs to be accurate, consistent, and available. The health workforce needs to understand how to define and govern data quality throughout the data supply chain, from origin through sharing, aggregated reporting, and eventually to enable the discovery of new knowledge based upon that data. Data quality applies to the data supply chain as a whole. Data needs to be collectable and useful at origin and able to represent and explain things that may not be known at the time of collection. Consistency of concept representation throughout the data supply chain needs to be clearly specified and transparent. Professional bodies need to provide leadership into the specification and governance of data, information, and computable knowledge, augmenting their traditional role of knowledge acquisition and publication based upon evidence. Throughout all levels of healthcare, the necessity of data quality needs to be better understood and managed.}
}
@incollection{SEBASTIANCOLEMAN2022131,
title = {Chapter 7 - The People Challenge: Building Data Literacy},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {131-164},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00007-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000079},
author = {Laura Sebastian-Coleman},
keywords = {Data literacy, data visualization, analytics, metadata management, critical thinking, scientific thinking, data management},
abstract = {This chapter addresses the skills, knowledge, and experience people require to create, use, and interpret data. Data literacy is the ability to read, understand, interpret, and learn from data in different contexts and to communicate about data with other people. The people challenge is both a skills challenge and a knowledge challenge. No single individual can know everything about an organization’s data. But, together, people can solve more problems in better ways if they understand data as a construct, recognize the risks associated with data production and use, cultivate a level of skepticism about data, and develop skill in visualizing and interpreting data. They will solve even more problems if the organization supports these efforts through disciplined metadata management and data quality management.}
}
@incollection{LARKIN2022283,
title = {Chapter Five - Connecting marine data to society},
editor = {Giuseppe Manzella and Antonio Novellino},
booktitle = {Ocean Science Data},
publisher = {Elsevier},
pages = {283-317},
year = {2022},
isbn = {978-0-12-823427-3},
doi = {https://doi.org/10.1016/B978-0-12-823427-3.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234273000037},
author = {Kate E. Larkin and Andrée-Anne Marsan and Nathalie Tonné and Nathalie {Van Isacker} and Tim Collart and Conor Delaney and Mickaël Vasquez and Eleonora Manca and Helen Lillis and Jan-Bart Calewaert},
keywords = {Big data, Climate change, Data visualization, Digital ocean, Ecosystems, FAIR, Hackathon, Knowledge broker, Marine data, Marine map, Ocean, Ocean literacy, Open data, Science communication, Seabed habitats},
abstract = {This chapter looks at connecting marine data to society, with a focus on key developments in Europe, set in a global context. It presents the European Marine Observation and Data Network-EMODnet as an exemplar in marine domain. EMODnet has significantly advanced European capability for Findable, Accessible, Interoperable, and Reusable marine knowledge, offering access to standardized and harmonized in-situ marine data and added value data products across seven marine environmental themes. Open and free data, products and associated metadata, are available for discovery and access through a wide range of web/data services. These ensure that the wealth of existing ocean observations and marine data collected in Europe and beyond can be easily discovered and used by a growing, and diversifying, user community. Interoperability with key services is crucial toward a pan-European and global approach. Key partnerships include the Copernicus Marine Environment Monitoring Service and international initiatives, e.g., the International Oceanographic Data and Information Exchange. Looking at societal tools and applications, the chapter provides a case study of the European Atlas of the Seas, a web-mapping tool that communicates marine and other open-source data and information in an attractive and interactive way. The EU Atlas is a key tool for the European ocean literacy initiative EU4Ocean, contributing to engage citizens and drive the societal change that is required for Europe to meet the ambitious targets to be climate neutral by 2050. The paper introduces examples of emerging tools for data visualization and presents hackathons, a powerful method to cocreate and innovate applications for society. Finally, the chapter looks toward the digital era and addresses the emerging challenges and opportunities of marine data, e.g., big data and plans for a digital twin of the Ocean, as tools to enable a step-change in societal connection, understanding, and action regarding the ocean.}
}
@article{SEN2022100052,
title = {Are data markets a solution to big tech market power? A competitive analysis},
journal = {Journal of Government and Economics},
volume = {7},
pages = {100052},
year = {2022},
issn = {2667-3193},
doi = {https://doi.org/10.1016/j.jge.2022.100052},
url = {https://www.sciencedirect.com/science/article/pii/S2667319322000234},
author = {Anindya Sen},
keywords = {Big tech, Data markets, Data portability, Data governance, Targeted advertising},
abstract = {This paper explores whether the establishment of data markets based on individual data portability can result in better societal outcomes. The results suggest that markets where individuals can sell data generated through their online engagement to third parties, could result in pareto improving outcomes for subscribers to digital platforms and purchasers of targeted advertising services. Data markets would enable third parties to combine their own proprietary data with other individual level data and produce information for targeted advertising, reducing the market power of Big Tech firms. However, successful data markets require strong regulatory measures by governments that ensure privacy and that data collected by Big Tech firms are considered the property of individuals. Such policies have the potential to guarantee that the benefits of Big Data are not confined to a few large firms.}
}
@article{JIANG2022469,
title = {A multi-dimensional cognitive framework for cognitive manufacturing based on OAR model},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {469-485},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001686},
author = {Tengyuan Jiang and Jingtao Zhou and Jianhua Zhao and Mingwei Wang and Shusheng Zhang},
keywords = {Intelligent manufacturing, Cognitive manufacturing, Cognitive framework, Computable digital twin, Multi-dimensional, OAR model},
abstract = {With the production system shifting to a multi-variety and small-batch production mode, the production process faces more user requirements, changes, and uncertainties. To solve the above problems, it is necessary to obtain the status and trend changes information and provide information support for the optimization of decision-making and dynamic adjustment of the production system. However, the production system cognition faces the problems of state coupling, state dynamic transfer and transition, and multi-system interweaving, which makes the production system cognition face huge challenges. Combining technologies such as the Internet of Things, industrial big data, and artificial intelligence, cognitive manufacturing can realize dynamic cognition of the production process, support dynamic adjustment, and become a promising way to solve the dynamic changes and uncertainties of production systems. In addition, as a formal expression of information processing and knowledge learning process in cognitive informatics, the Object-Attribute-Relation (OAR) model can effectively guide the construction of the production process cognitive mechanism. Therefore, this paper proposes a multi-dimensional cognitive framework based on OAR model of the human cognitive world for the dynamic cognitive needs of production system. The framework carries out dynamic cognition from the three dimensions of the manufacturing unit, production situation, and production system, and builds the continuous cognitive abilities from the three dimensions of analysis, decision-making, and learning. By integrating intelligent algorithms in the fields of artificial intelligence, a computable digital twin model is constructed as a carrier to provide the cognitive enabling technologies and capabilities for the production system. Finally, the feasibility of the proposed framework is illustrated by the developed computational digital twin platform. The computable digital twin platform provides the production system with important cognitive capabilities such as states perception, trend prediction, optimization decision-making, and knowledge learning, to support the dynamic cognition and optimization decision-making of the production system, and lay a technical foundation for adaptive production and cognitive manufacturing.}
}
@article{MOHAMAD20222165,
title = {Using a decision tree to compare rural versus highway motorcycle fatalities in Thailand},
journal = {Case Studies on Transport Policy},
volume = {10},
number = {4},
pages = {2165-2174},
year = {2022},
issn = {2213-624X},
doi = {https://doi.org/10.1016/j.cstp.2022.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S2213624X22001857},
author = {Ittirit Mohamad and Sajjakaj Jomnonkwao and Vatanavongs Ratanavaraha},
keywords = {Comparative analysis, Decision tree, Accidents, Big data, Transportation, Machine learning},
abstract = {Thailand ranks first in Asia and ninth in the world in term of road accident. As of 2020, the number of vehicles registered in Thailand was over 41 million, with motorcycles accounting for half of all vehicles. This study aimed to determine the cause of fatalities to reduce motorcycle accidents. The research entailed separating the accidents and fatalities into those occurring on highways (HWs) versus those occurring on rural roadways (RRs) and focused solely on rider at fault accidents to involve any confounding factors related to passengers or others involved. In Thailand, HWs have higher speed limits and allow more vehicle types than some RRs. Thailand’s Department of Public Disaster Prevention and Mitigation recorded 115,154 motorcycle accidents from 2015 to 2020. Decision trees allow for processing large amounts of data to drill down into associations between the individual variables in a large data set; in this study, the tree also separated accidents into whether or not the driver was exceeding the speed limit. The model’s performance for HWs, predicted misclassifications were found to be 28.3% (fatality to nonfatality) and a 44.5% (nonfatality to fatality) while predicted misclassification for RRs were 15.5% (fatality to nonfatality) and 60% (nonfatality to fatality). At all ages, the most fatalities were among male riders on dry straightaways in clear daytime weather; notably, however, on RRs, even when the rider was driving responsibly, fatalities were high at night on roads with no light. Following the presentation of the study findings, suggestions are made for ways the Thai government can improve the motorcycle accident and fatality statistics, including increasing the age limit for a motorcycle license, with engine size limits further divided according to age; proper enforcement of the existing rules will also improve the country’s accident statistics. It will also be highly effective to improve road lighting, particularly on RRs.}
}
@article{JAVAID2022124,
title = {Evolutionary trends in progressive cloud computing based healthcare: Ideas, enablers, and barriers},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {3},
pages = {124-135},
year = {2022},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666307422000134},
author = {Mohd Javaid and Abid Haleem and Ravi Pratap Singh and Shanay Rab and Rajiv Suman and Ibrahim Haleem Khan},
keywords = {Cloud computing, Healthcare, Applications, Data, Information},
abstract = {Cloud computing is one of the significant facilitators of the health information revolution in the healthcare business. The global exchange of records in the health sector through electronic media is facilitated by cloud computing. In healthcare, this technology increases safety and creates innovation. Communication with the health matrix throughout the world makes feasible by the application of this technology. Cloud computing has been utilised in health care for many years and has evolved in conjunction with developments in business. This technology establishes standard accessible hardware for diverse healthcare applications via a network connection. Cloud computing and processing ensure safe communication, and the cloud servers secure all essential data. Doctors can counsel their individuals on their health and broadcast their patient's daily health regimes, typically keeping their minds and bodies healthy. Psychologists and psychiatrists can use videoconferencing that makes patients comfortable with their patients. This paper discusses cloud computing and its need for healthcare. Major key advantages, barriers, and challenges of Cloud computing for the healthcare industry are identified. Finally, it discusses the significant applications of cloud computing for healthcare. Today more and more healthcare suppliers are providing Internet of Things (IoT) enabled gadgets to patients, and patient data are instantly communicated to their doctors by linking such devices to the cloud system of hospitals. As a result, cloud computing, in conjunction with fast-expanding technologies such as Big Data analytics, artificial intelligence, and the internet of medical things, improves efficiencies and expands the number of ways to streamline healthcare delivery. It improves resource availability, improves interoperability, and reduces costs.}
}
@article{YANG2022134948,
title = {The sequential construction research of regional public electric vehicle charging facilities based on data-driven analysis—Empirical analysis of Shanxi Province},
journal = {Journal of Cleaner Production},
volume = {380},
pages = {134948},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134948},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622045218},
author = {Xiaoyu Yang and Xiaopeng Guo and Yun Li and Kun Yang},
keywords = {The charging facilities demand differences analysis, The utilization differences analysis of charging stations, Public charging facilities, Sequential construction},
abstract = {The contradictions of charging facilities supply and demand are intensified in China, the surplus and shortages of charging facilities simultaneously existing, and the utilization of charging facilities significantly polarized. To solve the problems, the sequential construction research method of regional public electric vehicle (EV) charging facilities is proposed based on the demand differences of charging facilities in cities and the utilization differences of charging stations in the same city. According to the actual charging transactions big data of Shanxi, the construction sequence of regional public charging facilities has been studied, which verified the scientificity and effectiveness of the proposed method. The results show that the charging facilities demand of 11 cities in Shanxi can be divided into 4 types, Linfen and Lvliang having the highest demand for charging facilities in the short term and long term; in the same city, the utilization of charging stations in residential areas is the highest, and the utilization of charging stations in working areas is the lowest; so, the charging facilities construction should focus on residential areas in Linfen and Lvliang. The proposed method is contributed to realizing accurate construction of charging facilities, which helps to guide the charging facilities planning of the government and the investment of charging facility investors.}
}
@article{SUN2022,
title = {A blockchain-based audit approach for encrypted data in federated learning},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000979},
author = {Zhe Sun and Junping Wan and Lihua Yin and Zhiqiang Cao and Tianjie Luo and Bin Wang},
keywords = {Audit, Data quality, Blockchain, Secure aggregation, Federated learning},
abstract = {The development of data-driven artificial intelligence technology has given birth to a variety of big data applications. Data has become an essential factor to improve these applications. Federated learning, a privacy-preserving machine learning method, is proposed to leverage data from different data owners. It is typically used in conjunction with cryptographic methods, in which data owners train the global model by sharing encrypted model updates. However, data encryption makes it difficult to identify the quality of these model updates. Malicious data owners may launch attacks such as data poisoning and free-riding. To defend against such attacks, it is necessary to find an approach to audit encrypted model updates. In this paper, we propose a blockchain-based audit approach for encrypted gradients. It uses a behavior chain to record the encrypted gradients from data owners, and an audit chain to evaluate the gradients’ quality. Specifically, we propose a privacy-preserving homomorphic noise mechanism in which the noise of each gradient sums to zero after aggregation, ensuring the availability of aggregated gradient. In addition, we design a joint audit algorithm that can locate malicious data owners without decrypting individual gradients. Through security analysis and experimental evaluation, we demonstrate that our approach can defend against malicious gradient attacks in federated learning.}
}
@article{JAGATHEESAPERUMAL2022107691,
title = {A holistic survey on the use of emerging technologies to provision secure healthcare solutions},
journal = {Computers and Electrical Engineering},
volume = {99},
pages = {107691},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107691},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000131},
author = {Senthil Kumar Jagatheesaperumal and Preeti Mishra and Nour Moustafa and Rahul Chauhan},
keywords = {Healthcare, Security, Internet of Things, Artificial intelligence, Machine learning, Deep learning, 5G networks},
abstract = {Healthcare applications demand systematic approaches to eradicate inevitable human errors to design a framework that systematically eliminates cyber-threats. The key focus of this paper is to provide a comprehensive survey on the use of modern enabling technologies, such as the Internet of Things (IoT), 5G networks, artificial intelligence (AI), and big data analytics, for providing secure and resilient healthcare solutions. A detailed taxonomy of existing technologies has been demonstrated for tackling various healthcare problems, along with their security-related issues in handling healthcare data. The application areas of each of the emerging technologies, along with their security aspects, are explained. Furthermore, an IoT-enabled smart pill bottle prototype is designed and illustrated as a case study for providing better understanding of the subject. Finally, various key research challenges are summarized with future research directions.}
}
@incollection{FARRE2022197,
title = {Chapter 7 - Data-driven policy evaluation},
editor = {Didier Grimaldi and Carlos Carrasco-Farré},
booktitle = {Implementing Data-Driven Strategies in Smart Cities},
publisher = {Elsevier},
pages = {197-225},
year = {2022},
isbn = {978-0-12-821122-9},
doi = {https://doi.org/10.1016/B978-0-12-821122-9.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211229000026},
author = {Marçal Farré and Federico Todeschini and Didier Grimaldi and Carlos Carrasco-Farré},
keywords = {Survey data, Administrative data, Big data, Policy evaluation, Impact evaluation, Process evaluation},
abstract = {Public policies should be designed and implemented, whenever possible, using evidence as rigorous as possible. Urban interventions then should be no exception. In recent times, we have witnessed increasing efforts to transform information into knowledge, and thus help policymakers make better decisions. In this chapter, we will explore how public policy evaluation helps municipal governments tackle social problems and how big data can improve the design and implementation of more effective, efficient, and transparent policies.}
}
@incollection{BAI202261,
title = {Chapter Three - Data-driven approaches: Use of digitized operational data in process safety},
editor = {Faisal Khan and Hans Pasman and Ming Yang},
series = {Methods in Chemical Process Safety},
publisher = {Elsevier},
volume = {6},
pages = {61-99},
year = {2022},
booktitle = {Methods to Assess and Manage Process Safety in Digitalized Process System},
issn = {2468-6514},
doi = {https://doi.org/10.1016/bs.mcps.2022.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468651422000022},
author = {Yiming Bai and Shuaiyu Xiang and Zeheng Zhao and Borui Yang and Jinsong Zhao},
keywords = {Data-driven models, Process safety, Data preparation, Data cleaning, Statistical models, Artificial intelligence models, Process monitoring, Fault detection and diagnosis, Fault prognosis, Video monitoring},
abstract = {Process safety is playing an important role with the rapid development of industry. With the advent of the Big Data era, various and massive data from the Internet of Things can be used for process safety. In this chapter, we aim to provide the reader with a comprehensive understanding of rapidly growing data-driven process safety approaches in the chemical industry. Data-driven approaches primarily use past process data without a complex mechanism model of chemical properties or processes; hence, they have advantages in practical industrial applications. In this chapter, first, we describe the importance of data in process safety. Then, we briefly introduce the ideas and methods of data pre-processing. We follow this with a discussion on statistical-based and artificial intelligence-based data-driven approaches. Then, we elaborate on the application of data-driven methods in the field of chemical process safety. Finally, we provide a summary and outlook for advancing data-driven methods.}
}
@article{MICHAILIDOU2022101953,
title = {EQUALITY: Quality-aware intensive analytics on the edge},
journal = {Information Systems},
volume = {105},
pages = {101953},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101953},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001496},
author = {Anna-Valentini Michailidou and Anastasios Gounaris and Moysis Symeonides and Demetris Trihinas},
keywords = {Fog computing, Optimization, Sensors, Data quality},
abstract = {Our work is motivated by the fact that there is an increasing need to perform complex analytics jobs over streaming data as close to the edge devices as possible and, in parallel, it is important that data quality is considered as an optimization objective along with performance metrics. In this work, we develop a solution that trades latency for an increased fraction of incoming data, for which data quality-related measurements and operations are performed, in jobs running over geo-distributed heterogeneous and constrained resources. Our solution is hybrid: on the one hand, we perform search heuristics over locally optimal partial solutions to yield an enhanced global solution regarding task allocations; on the other hand, we employ a spring relaxation algorithm to avoid unnecessarily increased degree of partitioned parallelism. Through thorough experiments, we show that we can improve upon state-of-the-art solutions in terms of our objective function that combines latency and extent of quality checks by up to 2.56X. Moreover, we implement our solution within Apache Storm, and we perform experiments in an emulated setting. The results show that we can reduce the latency in 86.9% of the cases examined, while latency is up to 8 times lower compared to the built-in Storm scheduler, with the average latency reduction being 52.5%.}
}
@incollection{SPANAKI2022147,
title = {Chapter 9 - Digital architectures: frameworks for supply chain data and information governance},
editor = {Bart L. MacCarthy and Dmitry Ivanov},
booktitle = {The Digital Supply Chain},
publisher = {Elsevier},
pages = {147-161},
year = {2022},
isbn = {978-0-323-91614-1},
doi = {https://doi.org/10.1016/B978-0-323-91614-1.00009-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323916141000095},
author = {Konstantina Spanaki and Erisa Karafili and Stella Despoudi},
keywords = {Data access control, Data flows, Data quality, Data sharing, Information architecture, Information flows, Supply chain},
abstract = {Advances in digitalization present new and emerging Supply Chain (SC) Information Architectures that rely on data and information as vital resources. While the importance of data and information in SCs has long been understood, there is a dearth of research or understanding about the effective governance, control, or management of data ecosystems at the SC level. This chapter examines data architectures through a navigation of the background of database management and data quality research of previous decades. The chapter unfolds the critical architectural elements around data and information sharing in the SC regarding the context, systems, and infrastructure. A review of various frameworks and conceptual models is presented on data and information in SCs, as well as access control policies. The critical importance of data quality and the management of data in the cyber-physical systems are highlighted. Policies for data sharing agreements (DSAs) and access control are discussed and the importance of effective governance in the distributed environments of digitally enabled SCs is emphasized. We extend the concept of data sharing agreements to capture the interplay between the various SC stakeholders around data use. Research gaps and needs relevant to new and emerging SC data and information ecosystems are highlighted.}
}
@article{HE2022100140,
title = {GARD: Gender difference analysis and recognition based on machine learning},
journal = {Array},
volume = {14},
pages = {100140},
year = {2022},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2022.100140},
url = {https://www.sciencedirect.com/science/article/pii/S259000562200011X},
author = {Shiwen He and Jian Song and Yeyu Ou and Yuanhong Yuan and Xiaojie Zhang and Xiaohua Xu},
keywords = {Gender difference analysis, Gender recognition, Medical examination data, Machine learning},
abstract = {In recent years, intelligent diagnosis and intelligent medical treatment based on big data of medical examinations have become the main trend of medical development in the future. In this paper, we propose a method for analyzing the difference between males and females in medical examination items (medical attributes) and find that males and females of different ages have differences in medical attributes. Then, the cluster analysis method is used to further analyze the differences between male and female in medical examination items, such that some common important attributes (CIAs) that can be used for gender recognition are found within a specific age range. Following, we propose two gender recognition models (GRMs) by using the found CIAs to identify the gender. A large number of experimental results are provided to validate the effectiveness of the proposed GRMs. Experimental results show that the medical attributes with a large value of difference really contribute to gender recognition. Within a certain age range, such as 17 to 51 years old, the proposed GRM can reach 92.8% accuracy using only six medical attributes.}
}
@article{LAMAAZI2022151,
title = {Smart-3DM: Data-driven decision making using smart edge computing in hetero-crowdsensing environment},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {151-165},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200022X},
author = {Hanane Lamaazi and Rabeb Mizouni and Hadi Otrok and Shakti Singh and Ernesto Damiani},
keywords = {Smart edge computing, Crowdsensing, Distributed architecture, Data assessment, Data quality},
abstract = {Mobile Edge Computing (MEC) has recently emerged as a promising paradigm for Mobile Crowdsensing (MCS) environments. In a given Area of Interest (AoI), the sensing process is performed based on task requirements, which usually ask for a specific quality of the sensing outcome. In this work, a two-stage Data-Driven Decision-making Mechanism using smart edge computing (Smart-3DM) is proposed. It advocates the use of smart edge to better fulfill the data-related task requirements. Depending on the type of data to be collected, the minimum quality of the data required, and the heuristics to apply for each type of crowdsensing service, the smart edge orchestrates the selection of workers in MEC. Our approach relies on (a) smart-edge deployment: where a cluster-based distributed architecture using smart edge nodes is considered. Here, two entities are defined: the main edge node (MEN) and the local edge nodes (LENs); and (b) data management offloading where a two-layer re-selection strategy that considers data type and context-awareness is adopted, to reduce data computation complexity and to increase data quality while meeting the task target. The proposed Smart-3DM is evaluated using a real-life dataset and is compared to one-stage local and global approaches. The overall results show that by using two-stage re-selection strategies, better performance with lower processing power (CPU), less Storage(RAM), and improved execution time is achieved, when compared to the benchmarks.}
}
@article{SUN2022105034,
title = {A review of Earth Artificial Intelligence},
journal = {Computers & Geosciences},
volume = {159},
pages = {105034},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105034},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422000036},
author = {Ziheng Sun and Laura Sandoval and Robert Crystal-Ornelas and S. Mostafa Mousavi and Jinbo Wang and Cindy Lin and Nicoleta Cristea and Daniel Tong and Wendy Hawley Carande and Xiaogang Ma and Yuhan Rao and James A. Bednar and Amanda Tan and Jianwu Wang and Sanjay Purushotham and Thomas E. Gill and Julien Chastang and Daniel Howard and Benjamin Holt and Chandana Gangodagamage and Peisheng Zhao and Pablo Rivas and Zachary Chester and Javier Orduz and Aji John},
keywords = {Geosphere, Hydrology, Atmosphere, Artificial intelligence/machine learning, Big data, Cyberinfrastructure},
abstract = {In recent years, Earth system sciences are urgently calling for innovation on improving accuracy, enhancing model intelligence level, scaling up operation, and reducing costs in many subdomains amid the exponentially accumulated datasets and the promising artificial intelligence (AI) revolution in computer science. This paper presents work led by the NASA Earth Science Data Systems Working Groups and ESIP machine learning cluster to give a comprehensive overview of AI in Earth sciences. It holistically introduces the current status, technology, use cases, challenges, and opportunities, and provides all the levels of AI practitioners in geosciences with an overall big picture and to “blow away the fog to get a clearer vision” about the future development of Earth AI. The paper covers all the majorspheres in the Earth system and investigates representative AI research in each domain. Widely used AI algorithms and computing cyberinfrastructure are briefly introduced. The mandatory steps in a typical workflow of specializing AI to solve Earth scientific problems are decomposed and analyzed. Eventually, it concludes with the grand challenges and reveals the opportunities to give some guidance and pre-warnings on allocating resources wisely to achieve the ambitious Earth AI goals in the future.}
}
@article{QUEST2022100167,
title = {A 3D indicator for guiding AI applications in the energy sector},
journal = {Energy and AI},
volume = {9},
pages = {100167},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100167},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000234},
author = {Hugo Quest and Marine Cauz and Fabian Heymann and Christian Rod and Lionel Perret and Christophe Ballif and Alessandro Virtuani and Nicolas Wyrsch},
keywords = {Artificial intelligence, Digitalisation, AI application, Big data, AI policy, Energy sector},
abstract = {The utilisation of Artificial Intelligence (AI) applications in the energy sector is gaining momentum, with increasingly intensive search for suitable, high-quality and trustworthy solutions that displayed promising results in research. The growing interest comes from decision makers of both the industry and policy domains, searching for applications to increase companies’ profitability, raise efficiency and facilitate the energy transition. This paper aims to provide a novel three-dimensional (3D) indicator for AI applications in the energy sector, based on their respective maturity level, regulatory risks and potential benefits. Case studies are used to exemplify the application of the 3D indicator, showcasing how the developed framework can be used to filter promising AI applications eligible for governmental funding or business development. In addition, the 3D indicator is used to rank AI applications considering different stakeholder preferences (risk-avoidance, profit-seeking, balanced). These results allow AI applications to be better categorised in the face of rapidly emerging national and intergovernmental AI strategies and regulations that constrain the use of AI applications in critical infrastructures.}
}
@article{HE20227724,
title = {A closed-loop data-fusion framework for air conditioning load prediction based on LBF},
journal = {Energy Reports},
volume = {8},
pages = {7724-7734},
year = {2022},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.05.289},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722011337},
author = {Ning He and Liqiang Liu and Cheng Qian and Lijun Zhang and Ziqi Yang and Shang Li},
keywords = {Air conditioning load prediction, Long short-term memory, Back propagation neural network, Particle filter, Locally weighted scatterplot smoothing},
abstract = {Accurate air conditioning load prediction is a key component of intelligent building management system for ensuring energy saving and safe operation of air conditioning system. In order to improve the prediction accuracy, a particle filter (PF) load prediction fusion estimation method based on long short-term memory (LSTM) and back propagation neural network (BP) is proposed. Firstly, spearman correlation analysis is used to select the influencing factors with high correlation as feature input. Aiming at the problem that the original signal is easy to be disturbed by noise and the data features are not obvious, locally weighted scatterplot smoothing (LOWESS) method is used to denoise the data to improve the data quality for further accurate prediction. Secondly, the data-driven air conditioning load state-space representation is established, which takes air conditioning load as the state variable and takes the load features collected by the sensor in real-time as the input variables. Thirdly, combined with the space representation method of air conditioning load based on LSTM-BP, PF is introduced to estimate the air conditioning load by using the fusion model. Meanwhile, the output load value of BP is fed back to the fusion model as the observation value to update the state-space representation of air conditioning load. Finally, two practical cases are used to verify the effectiveness of the method. The results indicate that the proposed method can provide more accurate and robust air conditioning load prediction.}
}
@article{WANG2022103939,
title = {Zooming into mobility to understand cities: A review of mobility-driven urban studies},
journal = {Cities},
volume = {130},
pages = {103939},
year = {2022},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2022.103939},
url = {https://www.sciencedirect.com/science/article/pii/S026427512200378X},
author = {Ruoxi Wang and Xinyuan Zhang and Nan Li},
keywords = {Mobility, Conceptualization, Big geodata, Urban issue, Urban application, Review},
abstract = {Emerging big datasets about human mobility provide new and powerful ways of studying cities and addressing various urban issues. However, human mobility has usually been defined narrowly in prior research that limits the understanding of its values for urban applications. The aim of this study is to reveal the complexity and multiplicity of human mobility concept for various urban application scenarios, and present a comprehensive review of mobility-driven urban studies through four re-conceptualized urban mobility perspectives. Using a systematic review approach, existing mobility-driven urban studies are classified based on whether they interpret urban mobility as spatial movements, a social phenomenon, an economic indicator or a policy tool. Then, the core values of knowledge about urban mobility for addressing contemporary urban challenges are analyzed, and the current trends and future directions of mobility-driven urban studies are also discussed. Moving forward, the application of urban mobility knowledge can be further advanced by the evolution of mobility concepts, the improvement of mobility data quality and the innovation of mobility analytical methods. This review can contribute to the understanding the state of the art of mobility-driven urban studies, and provide inspiration and guidelines for studies of this area in the future.}
}
@article{CHAN2022112017,
title = {Development and performance evaluation of a chiller plant predictive operational control strategy by artificial intelligence},
journal = {Energy and Buildings},
volume = {262},
pages = {112017},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112017},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822001888},
author = {K.C. Chan and Victor T.T. Wong and Anthony K.F. Yow and P.L. Yuen and Christopher Y.H. Chao},
keywords = {Chiller plant optimization, Artificial intelligence, Artificial neural network, Particle swarm optimization, VSD chiller, Building energy saving},
abstract = {Traditionally, chiller plants are controlled and monitored by a predetermined control strategy to ensure appropriate operation based on the designed system configuration. With the use of new technology of variable speed drive (VSD) for compressors, smart control strategies could be leveraged to enhance the system efficiency in lieu of traditional control strategies. For example, using orderly and straightforward switching procedures without considering various factors in switching the units, including the high-efficiency partial load range benefitted from the VSD, the actual performance of the units as a whole and the variable chilled water flow rate, result in the chiller plant not operating at maximum performance and efficiency. To address these issues, a hybrid predictive operational chiller plant control strategy is proposed to optimize the performance of the chiller plant. Artificial intelligence is employed as the data mining algorithm, with big data analysis based on the actual acquired voluminous operation data by fully considering the characteristics of chiller plants without additional installation of large-sized and high-priced equipment. Artificial neural network (ANN) was employed in the control strategy to predict the future outdoor temperature, building cooling load demand and the corresponding power consumption of the chiller plants. At the same time, particle swarm optimization (PSO) was applied to search for the optimized setpoints, e.g., chilled water supply temperature, operating sequence, chilled water flow rate, for the chiller plants. The developed control strategy has been launched in a chiller plant with a cooling capacity of 7,700 kW installed in a hospital in Hong Kong. The system coefficient of performance (COP) and overall energy consumption of the chiller plants were enhanced by about 8.6% and reduced by about 7.9%, respectively, compared with the traditional control strategy. This real-time, continuous, automatic optimization control strategy can determine the most efficient combination of operating parameters of a chiller plant with different control settings. This ensures that the chiller plant operates in its most efficient mode year-round under various operational conditions.}
}
@article{BECKSCHULTE2022804,
title = {Digital Vehicle Protocol based on Distributed Ledger Technology in Production},
journal = {Procedia CIRP},
volume = {107},
pages = {804-809},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.066},
url = {https://www.sciencedirect.com/science/article/pii/S221282712200350X},
author = {Sebastian Beckschulte and Louis Huebser and Raphael Kiesel and Robert H. Schmitt},
keywords = {Business model, digital twin, distributed ledger technology, production, protocol, value chain, vehicle},
abstract = {This paper describes the utilization of Distributed Ledger Technology (DLT) as means of a digital backbone – the so-called digital vehicle protocol – across the commercial vehicle industry. Enabling a digital vehicle protocol along the value chain resolves common data management problems, which are still the main inhibitor for advanced analytics methods and serves as a basis for new business models. This contribution demonstrates a per product-data centered approach in which low structured data can be written to and read from any point along the value chain on a product unit-based scope by using DLT. By embedding data post-processing pipelines per product-unit, data easily can be retrofitted to the task at hand. Therefore, it decouples data post-processing from data generation as well as overcoming current data silos within organizations. DLT hereby allows tying processing routines to data leading to a temper-proof digital vehicle protocol. Besides obtaining a stronger product-individual focus, our approach enables an easier integration of stakeholders into the entire business process landscape such as suppliers as well as sellers and leads to future business models such as billing depending on quality defects found during production. Our conceptual framework resolves current problems, i.e. data access, data quality and data processing costs. We align conceptual applicability with regards to a Truck Original Equipment Manufacturer (OEM) within the scope of failure management in production in order to specify implementation details, which reveal new business process models that further can transform commercial vehicle manufacturers from producers to service providers.}
}
@article{WANG2022107040,
title = {Toxicology of respiratory system: Profiling chemicals in PM10 for molecular targets and adverse outcomes},
journal = {Environment International},
volume = {159},
pages = {107040},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2021.107040},
url = {https://www.sciencedirect.com/science/article/pii/S0160412021006656},
author = {Junyu Wang and Yixia Zhang and Zhijun Zhang and Wancong Yu and Ang Li and Xin Gao and Danyu Lv and Huaize Zheng and Xiaohong Kou and Zhaohui Xue},
keywords = {PM, Respiratory system injury, Bioinformatics analysis, Molecular targets, Signaling pathway},
abstract = {Numerous studies have shown that the increasing trend of respiratory diseases have been closely associated with the endogenous toxic chemicals (polycyclic aromatic hydrocarbons, heavy metal ions, etc.) in PM10. In the present study, we aim to determine the strong correlations between the chemicals in PM10 and the adverse consequences. We used the ChemView DB, the ToxRef DB and a comprehensive literature analysis to collect, identify, and evaluate the chemicals in PM10 and their adverse effects on respiratory system, and then used the ToxCast DB to analyze their bioactivity and key targets through 1192 molecular targets and cell characteristic endpoints. Meanwhile, the bioinformatics analysis were carried out on the molecular targets to screen out prevention and treatment targets. A total of 310 chemicals related to the respiratory system were identified. An unsupervised two-directional heatmap was constructed based on hierarchical clustering of 227 chemicals by their effect scores. A subset of 253 chemicals with respiratory system toxicity had in vitro bioactivity on 318 molecular targets that could be described, clustered and annotated in the heatmap and bipartite network, which were analyzed based on the protein information in UniProt KB database and the software of GO, STRING, and KEGG. These results showed that the chemicals in PM10 have strong correlation with different types of respiratory system injury. The main pathways of respiratory system injury caused by PM10 are the Calcium signaling pathway, MAPK signaling pathway, and PI3K-AKT signaling pathway, and the core proteins in which are likely to be the molecular targets for the prevention and treatment of damage caused by PM10.}
}
@article{ENCINAS2022109904,
title = {Downhole data correction for data-driven rate of penetration prediction modeling},
journal = {Journal of Petroleum Science and Engineering},
volume = {210},
pages = {109904},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109904},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521015217},
author = {Mauro A. Encinas and Andrzej T. Tunkiel and Dan Sui},
keywords = {Drilling, Machine learning, Rate of penetration, Drilling data quality improvement, Recurrent neural networks},
abstract = {In recent years, machine learning has been adopted in the Oil and Gas industry as a promising technology for solutions to the most demanding problems like downhole parameters estimations and incidents detection. A big amount of available data makes this technology an attractive option for solving a wide variety of drilling problems, as well as a reliable candidate for performing big-data analysis and interpretation. Nevertheless, this approach may cause, in some cases, that petroleum engineering concepts are disregarded in favor of more data-intensive approaches. This study aims to evaluate the impact of drilling data measurement correction on data-driven model performance. In our study, besides using the standard data processing technologies, like gap filling, outlier removal, noise reduction etc., the physics-based drilling models are also implemented for data quality improvement and data correction in consideration of the measurement physics, rarely mentioned in most of publications. In our case study, recurrent neural networks (RNN) that are able to capture temporal natures of a signal are employed for the rate of penetration (ROP) estimation with an adjustable predictive window. The results show that the RNN model produces the best results when using the drilling data recovered through analytical methods. Moreover, the comprehensive data-driven model evaluation and engineering interpretation are conducted to facilitate better understanding of the data-driven models and their applications.}
}
@incollection{ZOHURI202259,
title = {Chapter 3 - Data warehousing, data mining, data modeling, and data analytics},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {59-86},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000015},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Big data, Data analytics and Data predective, Data modeling, Data warehousing},
abstract = {Data warehouses are constantly evolving to support new technologies and business requirements—and remain relevant when it comes to big data and analytics. Regardless of how new or sophisticated your data warehouse is, it likely needs modernization. Data warehousing, along with data modeling, and side by side with data analytic capability gives us the upper hand with our knowledge by collecting the right information at the right time with the right data coming from all directions, whether or not these data are structured or unstructured. We should be able to have proper tools in hand to be able to take this information and knowledge to be in a position of resilience based on predictive analysis driven by data. This chapter will discuss data warehousing, data modeling, and consequently, data analytics where, in combination, they all are variables functioning within the process of predictive analytic modeling. This process allows us to have the knowledge we are looking for. Getting reliable information from data warehouses is resource-intensive; missing one step can result in wasted processing time and/or bad data.}
}
@article{LI2022102760,
title = {An efficient secure data transmission and node authentication scheme for wireless sensing networks},
journal = {Journal of Systems Architecture},
volume = {133},
pages = {102760},
year = {2022},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102760},
url = {https://www.sciencedirect.com/science/article/pii/S1383762122002454},
author = {Lixiang Li and Sirui Li and Haipeng Peng and Jingguo Bi},
keywords = {Compressed sensing, Homomorphic encryption, Lightweight hash algorithm, Wireless sensing network},
abstract = {With the development of big data era, wireless sensor networks (WSNs) are widely used. However, the wireless sensing network has the disadvantages that it is difficult to monitor after deployment and the node energy after deployment cannot be supplemented. Therefore, this paper proposes an efficient and high-performance data acquisition scheme, and realizes the authentication of effective nodes and the confidentiality of data transmission. Firstly, the sensing node uses compressed sensing sampling network (CSSN) to complete high-performance data collection, and applies deep learning to wireless sensing network data collection. Secondly, the lightweight hash algorithm is applied to process the ID and timestamp time of each node to realize the identification and authentication of the sink node, and avoid the replay attack. Finally, the Paillier additive homomorphic encryption is used to encrypt the signed data to realize the confidential transmission of data. At the same time, this scheme evaluates the reputation of each sense node and deletes the damaged nodes invaded by attackers from the network. Taking image information acquisition as an example, a large number of experiments have proved the feasibility and applicability of this scheme.}
}
@incollection{KHAN2022,
title = {Bivariate, cluster, and suitability analysis of NoSQL solutions for big graph applications},
series = {Advances in Computers},
publisher = {Elsevier},
year = {2022},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000590},
author = {Samiya Khan and Xiufeng Liu and Syed Arshad Ali and Mansaf Alam},
keywords = {NoSQL, Big data system, Storage solution, Bivariate analysis, Cluster analysis, Classification},
abstract = {With the explosion of social media, the Web, Internet of Things, and the proliferation of smart devices, large amounts of data are being generated each day. However, traditional data management technologies are increasingly inadequate to cope with this growth in data. NoSQL has become increasingly popular as this technology can provide consistent, scalable and available solutions for the ever-growing heterogeneous data. Recent years have seen growing applications shifting from traditional data management systems to NoSQL solutions. However, there is limited in-depth literature reporting on NoSQL storage technologies for big graph and their applications in various fields. This chapter fills this gap by conducting a comprehensive study of 80 state-of-the-art NoSQL technologies. In this chapter, we first present a feature analysis of the NoSQL solutions and then generate a data set of the investigated solutions for further analysis in order to better understand and select the technologies. We perform a clustering analysis to segment the NoSQL solutions, compare the classified solutions based on their storage data models and Brewer's CAP theorem, and examine big graph applications in six specific domains. To help users select appropriate NoSQL solutions, we have developed a decision tree model and a web-based user interface to facilitate this process. In addition, the significance, challenges, applications and categories of storage technologies are discussed as well.}
}
@incollection{WU202257,
title = {Chapter 3 - CTDA methodology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {57-100},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000101},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics},
abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.}
}
@article{MERINO2022217,
title = {Lessons learned from an IoT deployment for condition monitoring at the Port of Felixstowe},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {217-222},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.210},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014288},
author = {Jorge Merino and Manu Sasidharan and Manuel Herrera and Hang Zhou and Adolfo {Crespo del Castillo} and Ajith K. Parlikad and Richard Brooks and Karen Poulter},
keywords = {IoT, Sensors, Monitoring, Data quality, Data management, 5G, Smart ports},
abstract = {The ports sector is critical to global trade. While digitalisation of infrastructure asset management in other sectors such as manufacturing, healthcare, water supply, railway and road is rapidly growing with the possibilities of the Internet of Things (IoT) solutions, the maritime industry lags significantly behind. IoT solutions and the near real-time data they produce provide new impetus to improve fault diagnosis of assets and prevent disruptions caused due to asset breakdown. Such solutions also require reliable communication systems to support low latency and high bandwidth. To this end, we are building an IoT-based asset management solution at the Port of Felixstowe, the UK's largest container port, using 5G technology. This paper presents the steps taken, challenges faced and the lessons learned with sourcing, installation, calibration and communication of sensors in this deployment.}
}
@article{DIVAIO2022121201,
title = {Data intelligence and analytics: A bibliometric analysis of human–Artificial intelligence in public sector decision-making effectiveness},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121201},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121201},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100634X},
author = {Assunta {Di Vaio} and Rohail Hassan and Claude Alavoine},
keywords = {Ambidexterity, Industry 4.0, Business intelligence, Big data, Intellectual capital, Human intellect, Accountability and performance},
abstract = {This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human–AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human–artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making.}
}
@article{LEE2022101426,
title = {Determining causal relationships in leadership research using Machine Learning: The powerful synergy of experiments and data science},
journal = {The Leadership Quarterly},
volume = {33},
number = {5},
pages = {101426},
year = {2022},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2020.101426},
url = {https://www.sciencedirect.com/science/article/pii/S1048984320300539},
author = {Allan Lee and Ilke Inceoglu and Oliver Hauser and Michael Greene},
keywords = {Leadership effectiveness, Leadership processes, Machine Learning, Artificial intelligence, Causality, Experiments, Big Data, Heterogeneous treatment effects},
abstract = {Machine Learning (ML) techniques offer exciting new avenues for leadership research. In this paper we discuss how ML techniques can be used to inform predictive and causal models of leadership effects and clarify why both types of model are important for leadership research. We propose combining ML and experimental designs to draw causal inferences by introducing a recently developed technique to isolate “heterogeneous treatment effects.” We provide a step-by-step guide on how to design studies that combine field experiments with the application of ML to establish causal relationships with maximal predictive power. Drawing on examples in the leadership literature, we illustrate how the suggested approach can be applied to examine the impact of, for example, leadership behavior on follower outcomes. We also discuss how ML can be used to advance leadership research from theoretical, methodological and practical perspectives and consider limitations.}
}
@article{CHAUHAN2022121508,
title = {Linking circular economy and digitalisation technologies: A systematic literature review of past achievements and future promises},
journal = {Technological Forecasting and Social Change},
volume = {177},
pages = {121508},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.121508},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522000403},
author = {Chetna Chauhan and Vinit Parida and Amandeep Dhir},
keywords = {Circular economy, Sustainability, Product-service system (PSS), Circular business model, Artificial intelligence, Internet of things},
abstract = {The circular economy (CE) has the potential to capitalise upon emerging digital technologies, such as big data, artificial intelligence (AI), blockchain and the Internet of things (IoT), amongst others. These digital technologies combined with business model innovation are deemed to provide solutions to myriad problems in the world, including those related to circular economy transformation. Given the societal and practical importance of CE and digitalisation, last decade has witnessed a significant increase in academic publication on these topics. Therefore, this study aims to capture the essence of the scholarly work at the intersection of the CE and digital technologies. A detailed analysis of the literature based on emerging themes was conducted with a focus on illuminating the path of CE implementation. The results reveal that IoT and AI play a key role in the transition towards the CE. A multitude of studies focus on barriers to digitalisation-led CE transition and highlight policy-related issues, the lack of predictability, psychological issues and information vulnerability as some important barriers. In addition, product-service system (PSS) has been acknowledged as an important business model innovation for achieving the digitalisation enabled CE. Through a detailed assessment of the existing literature, a viable systems-based framework for digitalisation enabled CE has been developed which show the literature linkages amongst the emerging research streams and provide novel insights regarding the realisation of CE benefits.}
}
@article{HARRISON2022103795,
title = {At the limit? Using operational data to estimate train driver human reliability},
journal = {Applied Ergonomics},
volume = {104},
pages = {103795},
year = {2022},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2022.103795},
url = {https://www.sciencedirect.com/science/article/pii/S0003687022001181},
author = {Chris Harrison and Julian Stow and Xiaocheng Ge and Jonathan Gregory and Huw Gibson and Alice Monk},
abstract = {Human reliability analysis plays an important role in the safety assessment and management of rail operations. This paper discusses how the increasing availability of operational data can be used to develop an understanding of train driver reliability. The paper derives human reliability data for two driving tasks, stopping at red signals and controlling speed on approach to buffer stops. In the first of these cases, a tool has been developed that can estimate the number of times a signal is approached at red by trains on the Great Britain (GB) rail network. The tool has been developed using big data techniques and ideas, recording and analysing millions of pieces of data from live operational feeds to update and summarise statistics from thousands of signal locations in GB on a daily basis. The resulting driver reliability data are compared to similar analyses of other train driving tasks. This shows human reliability approaching the currently accepted limits of human performance. It also shows higher error rates amongst freight train drivers than passenger train drivers for these tasks. The paper highlights the importance of understanding the task specific performance limits if further improvements in human reliability are sought. It also provides a practical example of how big data could play an increasingly important role in system error management, whether from the perspective of understanding normal performance and the limits of performance for specific tasks or as the basis for dynamic safety indicators which, if not leading, could at least become closer to real time.}
}
@incollection{DEPAMPHILIS2022123,
title = {Chapter 5 - Implementation: search through closing: phases 3 to 10 of the acquisition process},
editor = {Donald M. DePamphilis},
booktitle = {Mergers, Acquisitions, and Other Restructuring Activities (Eleventh Edition)},
publisher = {Academic Press},
edition = {Eleventh Edition},
pages = {123-152},
year = {2022},
isbn = {978-0-12-819782-0},
doi = {https://doi.org/10.1016/B978-0-12-819782-0.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197820000058},
author = {Donald M. DePamphilis},
keywords = {acquisition process, merger process, search process, search and screening process, deal negotiation, integration planning, M&A evaluation, implementing M&A integration, due diligence, due diligence questions, negotiation, deal structuring, purchase price, total consideration, closing conditions, deal covenants, mergers, acquisitions, mergers and acquisitions, contacting the target, closing, financing plan, employee retention, data analytics, analytics, data protection, data privacy, privacy, artificial intelligence, smart contracting, machine learning, block chain networks, digital tokens, cryptocurrency, AI, augmented artificial intelligence, augmented AI},
abstract = {Big data, data analytics, artificial intelligence, block chains, and smart contracting are tools that are increasingly applied to the mergers and acquisitions (M&A) process. To what extent do they offer promising solutions to challenging problems? And to what extent are they overhyped? These are among the questions addressed in this chapter, which presumes that a firm has developed a viable business plan requiring an acquisition to implement its business strategy. Whereas the preceding chapter addressed the creation of business and acquisition plans (phases 1 and 2), this chapter focuses on phases 3 to 10 of the M&A process, including search, screening, first contact, negotiation, integration planning, closing, integration implementation, and evaluation. Search and screening potential targets focus on developing appropriate selection criteria, while first contact details strategies for discussing price and developing preliminary legal documents. The negotiation phase involves refining valuation, deal structuring, conducting due diligence, and developing a financing plan. Integration planning addresses the challenges of postacquisition integration of the target. Closing is about the transfer of ownership, resolving transition issues, and completing the merger agreement. Postclosing integration deals with developing communication plans, employee retention, resolving cultural issues, satisfying immediate cash flow requirements, and using best practices. And postclosing evaluation encompasses learning from previous mistakes.}
}
@article{CONDE2022101723,
title = {Applying digital twins for the management of information in turnaround event operations in commercial airports},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101723},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101723},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001811},
author = {Javier Conde and Andres Munoz-Arcentales and Mario Romero and Javier Rojo and Joaquín Salvachúa and Gabriel Huecas and Álvaro Alonso},
keywords = {Aviation, Flight turnaround events, Digital twin, Internet of Things, Data modelling, Big data},
abstract = {The aerospace sector is one of the many sectors in which large amounts of data are generated. Thanks to the evolution of technology, these data can be exploited in several ways to improve the operation and management of industrial processes. However, to achieve this goal, it is necessary to define architectures and data models that allow to manage and homogenise the heterogeneous data collected. In this paper, we present an Airport Digital Twin Reference Conceptualisation’s and data model based on FIWARE Generic Enablers and the Next Generation Service Interfaces-Linked Data standard. Concretely, we particularise the Airport Digital Twin to improve the efficiency of flight turnaround events. The architecture proposed is validated in the Aberdeen International Airport with the aim of reducing delays in commercial flights. The implementation includes an application that shows the real state of the airport, combining two-dimensional and three-dimensional virtual reality representations of the stands, and a mobile application that helps ground operators to schedule departure and arrival flights.}
}
@article{SKARPATHIOTAKI2022100274,
title = {Cross-Industry Process Standardization for Text Analytics},
journal = {Big Data Research},
volume = {27},
pages = {100274},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100274},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000915},
author = {Christina G. Skarpathiotaki and Konstantinos E. Psannis},
keywords = {Big data analytics, Advanced analysis, Artificial intelligence, Machine learning, Text analytics, Cross-industry processes},
abstract = {We are living in a world where everything computes, everyone and everything is connected and sharing data. Going beyond just capturing and managing data, enterprises are tapping into IoT and Artificial Intelligence (AI) to create insights and intelligence in a revolutionary way that was not possible before. For instance, by analyzing unstructured data (such as text), call centers can extract entities, concepts, themes which can enable them to get faster insights that only few years back was not feasible. Public safety and law enforcement are only few of the examples that benefit from text analytics used to strengthen crime investigation. Sentiment Analysis, Content Classification, Language Detection and Intent Detection are just some of the Text Classification applications. The overall process model of such applications considering the complexity of the unstructured data, can be definitely challenging. In response to the chaotic emerging science of unstructured data analysis, the main goal of this paper is to first contribute to the gap of no existing methodology approach for Text Analytics projects, by introducing a methodology approach based on one of the most widely accepted and used methodology approach of CRISP-DM.}
}
@article{BOOMGARDZAGRODNIK2022106580,
title = {Machine learning imputation of missing Mesonet temperature observations},
journal = {Computers and Electronics in Agriculture},
volume = {192},
pages = {106580},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106580},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921005974},
author = {Joseph P. Boomgard-Zagrodnik and David J. Brown},
keywords = {Machine learning, Big data, Surface weather observations, Degree day models, Missing data imputation},
abstract = {Uninterrupted and reliable weather data is a necessary foundation for agricultural decision making, required for models based on accumulated growing degree days (GDD), chill units, and evapotranspiration. When a weather station experiences a mechanical or communications failure, a replacement (imputed) value should be substituted for any missing data. This study introduces a machine learning, network-based approach to imputing missing 15-minute and daily maximum/minimum air temperature observations from 8.5 years of air temperature, relative humidity, wind, and solar radiation observations at 134 AgWeatherNet (AWN) stations in Washington State. A random forest imputation model trained on temperature and humidity observations from the full network predicted 15-minute, daily maximum, and daily minimum temperature values with mean absolute errors of 0.43 °C, 0.53 °C, and 0.70 °C, respectively. Sensitivity experiments determined that imputation skill was related a number of external factors including volume and type of training data, proximity of surrounding stations, and regional topography. In particular, nocturnal cold air flows in the upper Yakima Valley of south-central Washington caused temperature to be less correlated with surrounding stations in the overnight hours. In a separate experiment, the imputation model was used to predict base- 10 °C GDD on 2020 July 1 trained entirely on 15-minute station data from previous years. Even with the entire season of observations missing, the model predicted the GDD value within an average error 1.4% with 125 of 134 stations within 5% of observations. Since missing data can typically be resolved within a timeframe of a few days, the network-based imputation model is a sufficient substitute for short periods of missing observational weather data. Other potential applications of an imputation model are briefly discussed.}
}
@article{FU2022128312,
title = {Extracting historical flood locations from news media data by the named entity recognition (NER) model to assess urban flood susceptibility},
journal = {Journal of Hydrology},
volume = {612},
pages = {128312},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2022.128312},
url = {https://www.sciencedirect.com/science/article/pii/S0022169422008848},
author = {Shengnan Fu and Heng Lyu and Ze Wang and Xin Hao and Chi Zhang},
keywords = {Urban flood susceptibility, News media data, Flood locations, Named entity recognition, Data quality control},
abstract = {Flood susceptibility assessment for identifying flood-prone areas plays a significant role in flood hazard mitigation. Machine learning is an optional assessment method because of its high objectivity and computational efficiency, but how to get enough and accurate information of historical flood locations to train the machine learning models has been a key problem. In recent years, news media data from both news websites and social media accounts has emerged as a promising source for natural science studies. However, the application of news media data in urban flood susceptibility assessment is still inadequate. This study proposed an approach to fill this gap. Firstly, flood locations were extracted from news media data based on a named entity recognition (NER) model. Then, a frequency or distance-based data quality control method was employed to improve the representativeness of the extracted flooded locations. Finally, flood conditioning factors with information of historical flood locations were input into a Support Vector Machine (SVM) model for flood susceptibility assessment. We took the central city of Dalian, China as a case study. The T-test results show that there was no significant difference between the distributions of most flood conditioning factors at the flood locations from the news media data and the official planning report. In the obtained flood susceptibility map, the high flood susceptibility areas got a recall of 90% compared with the high flood hazard areas in the planning report. Performing data quality control in the frequency-based method can improve the precision of the flood susceptibility map by up to 5%, while the distance-based method is ineffective. This study provides an example and offers the value of applying new data sources and modern deep learning techniques for urban flood management.}
}
@article{GACUTAN2022150742,
title = {Continental patterns in marine debris revealed by a decade of citizen science},
journal = {Science of The Total Environment},
volume = {807},
pages = {150742},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.150742},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721058204},
author = {Jordan Gacutan and Emma L. Johnston and Heidi Tait and Wally Smith and Graeme F. Clark},
keywords = {Environmental monitoring, Plastic pollution, Bioregional management, Litter, Citizen science, Marine debris},
abstract = {Anthropogenic marine debris is a persistent threat to oceans, imposing risks to ecosystems and the communities they support. Whilst an understanding of marine debris risks is steadily advancing, monitoring at spatial and temporal scales relevant to management remains limited. Citizen science projects address this shortcoming but are often critiqued on data accuracy and potential bias in sampling efforts. Here we present 10-years of Australia's largest marine debris database - the Australian Marine Debris Initiative (AMDI), in which we perform systematic data filtering, test for differences between collecting groups, and report patterns in marine debris. We defined five stages of data filtering to address issues in data quality and to limit inference to ocean-facing sandy beaches. Significant differences were observed in the average accumulation of items between filtered and remaining data. Further, differences in sampling were compared between collecting groups at the same site (e.g., government, NGOs, and schools), where no significant differences were observed. The filtering process removed 21% of events due to data quality issues and a further 42% of events to restrict analyses to ocean-facing sandy beaches. The remaining 7275 events across 852 sites allowed for an assessment of debris patterns at an unprecedented spatial and temporal resolution. Hard plastics were the most common material found on beaches both nationally and regionally, consisting of up to 75% of total debris. Nationally, land and sea-sourced items accounted for 48% and 7% of debris, respectively, with most debris found on the east coast of Australia. This study demonstrates the value of citizen science datasets with broad spatial and temporal coverage, and the importance of data filtering to improve data quality. The citizen science presented provides an understanding of debris patterns on Australia's ocean beaches and can serve as a foundation for future source reduction plans.}
}
@article{WEVER2022102882,
title = {Designing early warning systems for detecting systemic risk: A case study and discussion},
journal = {Futures},
volume = {136},
pages = {102882},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2021.102882},
url = {https://www.sciencedirect.com/science/article/pii/S0016328721001919},
author = {Mark Wever and Munir Shah and Niall O’Leary},
keywords = {Systemic risk, Early warning system, Risk detection, Design process, Artificial intelligence, Complex systems},
abstract = {Systemic risks are potential trigger events or developments that could undermine the viability of entire networks or systems. Growing complexity in systems make such risks both more likely to occur and more difficult to anticipate. The tools for detecting systemic risk have not kept pace with these challenges; traditional methods are too intermittent, too slow, and too narrow in focus for timely systemic risk detection. However, recent developments in big data analysis and artificial intelligence (AI) have the potential to revolutionize Early Warning Systems (EWSs) for detecting systemic risk. EWSs that are supported by these technologies could provide users with earlier warning signals of a wider range of risks and more up-to-date measures of the fragility of the system against these risks. This area of research is nascent and lacks a robust methodology for designing such EWSs. Addressing this issue, the present paper: 1) identifies the characteristics of competent EWSs; 2) outlines an approach for designing such EWSs; and 3) illustrates the value of this approach, by discussing the conceptual design of an EWS for detecting biosecurity incursions in the New Zealand pastoral industries.}
}
@incollection{TONG20221,
title = {Chapter One - Introduction of medical genomics and clinical informatics integration for p-Health care},
editor = {David B. Teplow},
series = {Progress in Molecular Biology and Translational Science},
publisher = {Academic Press},
volume = {190},
number = {1},
pages = {1-37},
year = {2022},
booktitle = {Precision Medicine},
issn = {1877-1173},
doi = {https://doi.org/10.1016/bs.pmbts.2022.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S187711732200062X},
author = {Li Tong and Hang Wu and May D. Wang and Geoffrey Wang},
keywords = {p-Health, Data integration, Clinical informatics, Genomics, Machine learning and artificial intelligence, Biomedical big data analytics},
abstract = {Achieving predictive, precise, participatory, preventive, and personalized health (abbreviated as p-Health) requires comprehensive evaluations of an individual's conditions captured by various measurement technologies. Since the 1950s, analysis of care providers' and physicians' notes and measurement data by computers to improve healthcare delivery has been termed clinical informatics. Since the 2010s, wide adoptions of Electronic Health Records (EHRs) have greatly improved clinical informatics development with fast growing pervasive wearable technologies that continuously capture the human physiological profile in-clinic (EHRs) and out-of-clinic (PHRs or Personal Health Records) to bolster mobile health (mHealth). In addition, after the Human Genome Project in the 1990s, medical genomics has emerged to capture the high-throughput molecular profile of a person. As a result, integrated data analytics is becoming one of the fast-growing areas under Biomedical Big Data to improve human healthcare outcomes. In this chapter, we first introduce the scope of data integration and review applications, data sources, and tools for clinical informatics and medical genomics. We then describe the data integration analytics at the raw data level, feature level, and decision level with case studies, and the opportunity for research and translation using advanced artificial intelligence (AI), such as deep learning. Lastly, we summarize the opportunities in biomedical big data integration that can reshape healthcare toward p-health.}
}
@article{WANG2022111488,
title = {An empirical study on the challenges that developers encounter when developing Apache Spark applications},
journal = {Journal of Systems and Software},
volume = {194},
pages = {111488},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111488},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001674},
author = {Zehao Wang and Tse-Hsun (Peter) Chen and Haoxiang Zhang and Shaowei Wang},
keywords = {Big data system, Empirical study, Stack Overflow},
abstract = {Apache Spark is one of the most popular big data frameworks that abstract the underlying distributed computation details. However, even though Spark provides various abstractions, developers may still encounter challenges related to the peculiarity of distributed computation and environment. To understand the challenges that developers encounter, and provide insight for future studies, in this paper, we conduct an empirical study on the questions that developers encounter. We manually analyze 1,000 randomly selected questions that we collected from Stack Overflow. We find that: 1) questions related to data processing (e.g., transforming data format) are the most common among the 11 types of questions that we uncovered. 2) Even though data processing questions are the most common ones, they require the least amount of time to receive an answer. Questions related to configuration and performance require the most time to receive an answer. 3) Most of the issues are caused by developers’ insufficient knowledge in API usages, data conversation across frameworks, and environment-related configurations. We also discuss the implication of our findings for researchers and practitioners. In summary, our work provides insights for future research directions and highlight the need for more software engineering research in this area.}
}
@article{PLAZAS2022101971,
title = {Sense, Transform & Send for the Internet of Things (STS4IoT): UML profile for data-centric IoT applications},
journal = {Data & Knowledge Engineering},
volume = {139},
pages = {101971},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101971},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000926},
author = {Julian Eduardo Plazas and Sandro Bimonte and Michel Schneider and Christophe {de Vaulx} and Pietro Battistoni and Monica Sebillo and Juan Carlos Corrales},
keywords = {Data-centric conceptual modelling, Model-driven architecture, Automatic code generation, Internet of Things},
abstract = {The Internet of Things is currently one of the most representative sources of Big Data. It can acquire real-time data from multiple spatially distributed points, allowing for the extraction of valuable insights. However, an appropriate integration, processing, and analysis of these data depends on several factors starting from the correct definition of the information systems. This paper introduces STS4IoT, a UML profile and automatic code-generation tool for model-driven IoT, to address this issue. STS4IoT allows designing and implementing an IoT application from the required data only, bridging the gaps between the IoT and database design worlds. The IoT data design includes both different in-network transformations and the join of streams from multiple sources. Besides, it follows the Model-Driven Architecture (MDA) guidelines to provide abstraction levels oriented to the different roles participating in the application design. The STS4IoT validation shows it has an excellent structure and is highly understandable. Its instance models are well-formed, highly abstract and readable. And the automatic implementation tool can generate complete code for complex real-world applications involving multiple IoT devices. Then, STS4IoT simplifies the definition and development of IoT applications and their integration into other information systems, such as stream data warehouses.}
}
@article{GONZALEZVIDAL2022328,
title = {Intrinsic and extrinsic quality of data for open data repositories},
journal = {ICT Express},
volume = {8},
number = {3},
pages = {328-333},
year = {2022},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S240595952200090X},
author = {Aurora González-Vidal and Alfonso P. Ramallo-González and Antonio F. Skarmeta},
keywords = {Data quality, Open data, IoT, Machine learning},
abstract = {This work assesses the quality of Internet of Things data not only as an intrinsic quality on how well it represents the related phenomenon but also, on how much information it contains to educate an artificial entity. The quality metrics here proposed are tested with real datasets. Also, they are implemented on OpenCPU, so the open data repositories can use them off-the-shelf to rate their datasets without computational cost and minimum human intervention, making them more attractive to potential users and gaining visibility and impact.}
}
@article{ESKANDARITORBAGHAN2022106543,
title = {Understanding the potential of emerging digital technologies for improving road safety},
journal = {Accident Analysis & Prevention},
volume = {166},
pages = {106543},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106543},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521005741},
author = {Mehran {Eskandari Torbaghan} and Manu Sasidharan and Louise Reardon and Leila C.W. Muchanga-Hvelplund},
keywords = {Safety, Road, Transport, Digital technology, Information},
abstract = {Each year, 1.35 million people are killed on the world’s roads and another 20–50 million are seriously injured. Morbidity or serious injury from road traffic collisions is estimated to increase to 265 million people between 2015 and 2030. Current road safety management systems rely heavily on manual data collection, visual inspection and subjective expert judgment for their effectiveness, which is costly, time-consuming, and sometimes ineffective due to under-reporting and the poor quality of the data. A range of innovations offers the potential to provide more comprehensive and effective data collection and analysis to improve road safety. However, there has been no systematic analysis of this evidence base. To this end, this paper provides a systematic review of the state of the art. It identifies that digital technologies - Artificial Intelligence (AI), Machine-Learning, Image-Processing, Internet-of-Things (IoT), Smartphone applications, Geographic Information System (GIS), Global Positioning System (GPS), Drones, Social Media, Virtual-reality, Simulator, Radar, Sensor, Big Data – provide useful means for identifying and providing information on road safety factors including road user behaviour, road characteristics and operational environment. Moreover, the results show that digital technologies such as AI, Image processing and IoT have been widely applied to enhance road safety, due to their ability to automatically capture and analyse data while preventing the possibility of human error. However, a key gap in the literature remains their effectiveness in real-world environments. This limits their potential to be utilised by policymakers and practitioners.}
}
@article{ZORRILLA2022103595,
title = {A reference framework for the implementation of data governance systems for industry 4.0},
journal = {Computer Standards & Interfaces},
volume = {81},
pages = {103595},
year = {2022},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103595},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000908},
author = {Marta Zorrilla and Juan Yebenes},
keywords = {Data governance, Data-Centric architecture, Industry 4.0, Big data, IoT},
abstract = {The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This is mainly based on the digitalization of the industrial environment by means of the convergence of Information Technologies (IT) and operational Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT) and the use of data generated in real time for gaining insights and making decisions. Therefore data becomes a critical asset for Industry 4.0 and must be managed and governed like a strategic asset. We rely on Data Governance (DG) as a key instrument for carrying out this transformation. This paper presents the design of a specific governance framework for Industry 4.0. First, this contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, the cloud and edge computing, the artificial intelligence and the current regulations. Next, we formally define a reference framework for the implementation of Data Governance Systems for Industry 4.0 using international standards and providing several examples of architecture building blocks.}
}
@article{MOSAVI2022503,
title = {Intelligent energy management using data mining techniques at Bosch Car Multimedia Portugal facilities},
journal = {Procedia Computer Science},
volume = {201},
pages = {503-510},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.065},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004781},
author = {Nasim Sadat Mosavi and Francisco Freitas and Rogério Pires and César Rodrigues and Isabel Silva and Manuel Santos and Paulo Novais},
keywords = {Energy consumption, Prediction, Optimization, Data Mining, Machine Learning, Forecasting, Industry 4.0},
abstract = {The fusion of emerged technologies such as Artificial Intelligence, cloud computing, big data, and the Internet of Things in manufacturing has pioneered this industry to meet the fourth stage of the industrial revolution (industry 4.0). One major approach to keeping this sector sustainable and productive is intelligent energy demand planning. Monitoring and controlling the consumption of energy under industry 4.0, directly results in minimizing the cost of operation and maximizing efficiency. To advance the research on the adoption of industry 4.0, this study examines CRISP-DM methodology to project data mining approach over data from 2020 to 2021 which was collected from industrial sensors to predict/forecast future electrical consumption at Bosch car multimedia facilities located at Braga, Portugal. Moreover, the influence of indicators such as humidity and temperature on electrical energy consumption was investigated. This study employed five promising regression algorithms and FaceBook prophet (FB prophet) to apply over data belonging to two HVAC (heating, ventilation, and air conditioning) sensors (E333, 3260). Results indicate Random Forest (RF) algorithms as a potential regression approach for prediction and the outcome of FB prophet to forecast the demand of future usage of electrical energy associated with HVAC presented. Based on that, it was concluded that predicting the usage of electrical energy for both data points requires time series techniques. Where “timestamp” was identified as the most effective feature to predict consume of electrical energy by regression technique (RF). The result of this study was integrated with Intelligent Industrial Management System (IIMS) at Bosch Portugal.}
}
@article{KROTOV2022,
title = {Big web data: Challenges related to data, technology, legality, and ethics},
journal = {Business Horizons},
year = {2022},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2022.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681322001252},
author = {Vlad Krotov and Leigh Johnson},
keywords = {Big data, Web data, Web scraping, Law, Ethics},
abstract = {The digital data available on the World Wide Web is currently measured in zettabytes. These vast repositories of Big Web Data are increasingly viewed as a strategic resource comparable in value to land, gold, and oil. This Big Web Data can be extracted and analyzed by organizations for the purposes of gaining a better understanding of their internal and external environment and improving organizational performance. Because of these opportunities, automated retrieval and organization of Web data (or Web Scraping) for research projects is becoming a common practice. This article educates the reader about the data-related, technical, legal, and ethical issues related to Web Scraping. Awareness of these issues can help researchers save time and other resources and, most importantly, mitigate potential risk of ethical controversies or even lawsuits in relation to the retrieval and use of Big Web Data.}
}
@article{MATHERI2022103152,
title = {Sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant},
journal = {Physics and Chemistry of the Earth, Parts A/B/C},
volume = {126},
pages = {103152},
year = {2022},
issn = {1474-7065},
doi = {https://doi.org/10.1016/j.pce.2022.103152},
url = {https://www.sciencedirect.com/science/article/pii/S1474706522000468},
author = {Anthony Njuguna Matheri and Belaid Mohamed and Freeman Ntuli and Esther Nabadda and Jane Catherine Ngila},
keywords = {Circular bioeconomy, Data pipeline, Digital twin, Process design, Sensor, Wastewater treatment},
abstract = {Rapid urbanization, population increase, emerging contaminants and increasing water scarcity have put a major constraint on the wastewater treatment system. Scarcity of water is steering current way of water recycle, and the drive focus towards resource recovery. Zero waste pathway in circular bioeconomy can bring transformation of wastewater commercialization by adding value with resource recovery. The complex biological reactions, unforeseen microbial behaviours, lack of reliable on-line instrumentation, complex modelling, lack of visualize techniques, low-quality industrial measurements and highly time-varying intensive data-driven operations call for the intelligence techniques and operations. The study is a review of sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant. Water surveillance and monitoring, circular economy and sustainability, automation pyramid, digital transformation, artificial intelligence, data pipeline, digital twin, data mining, and data-driven visualization, cyber-physical systems and water-energy-health management were reviewed. The deployment of the digital systems has evidently proven to bridges the gap between the data-driven soft sensor, operation and control systems in WWTP. Accurate prediction of the WWTP variables can support process design and control, reduce operation cost, improve system reliability, predictive maintenance and troubleshooting, increase water quality, increase stakeholder's engagement and endorse optimization of the plant performance. This procures the best compliance with international standards and diversification. The inclusion of life cycle environmental or cost management technologies in optimization models is an interesting pathway towards sustainable water treatment in-line with sustainable development goals, circular bioeconomy and industry 4.0.}
}
@article{NARASIMHULU2022104690,
title = {High performance social data computing with development of intelligent topic models for healthcare},
journal = {Microprocessors and Microsystems},
volume = {95},
pages = {104690},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2022.104690},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122002204},
author = {K Narasimhulu and K.T. {Meena Abarna}},
keywords = {Topic Model, Ailments Aspects, ATAM, Healthcare Topic Model, Social Recommended Healthcare Results},
abstract = {Data mining and big data computing are the emerging domains in the current era of predictions for societal applications. Millions of people are interested in sharing their views through tweets. Healthcare predictions are one of the attractive researches in big data social mining. Healthcare predictions are derived by implementing topic models by the ailments data. An ailment refers to either illness or sign of a particular health problem. Millions of tweets are collected based on conditions and assessed with ailment topic aspect models. The existing topic model, Latent Dirichlet Allocation (LDA), Latent Semantic Indexing, Probabilistic LSI (PLSI), limits the healthcare results assessment concerning any one of the ailments aspects. Recent ailments topic aspect model (ATAM) overcome the problems of these topic models and delivers the healthcare assessment results concerning the fundamental aspects of ailments data except side-effects analysis of treatments. The scalability performance of ATAM is degraded in showing healthcare results over the massive amounts of health data. A high-performance computing model of ATAM has been developed in the distributed environment to address scalability. Its intelligent model is designed in the cloud and multi-node Hadoop environment to deliver high-performance social computing results for healthcare. Experiments are conducted on many comparative studies is demonstrated between the existing and proposed high-performance models using the massive amount of health-related tweets concerning the ailments aspects.}
}
@article{SHARMA2022101624,
title = {Implementing challenges of artificial intelligence: Evidence from public manufacturing sector of an emerging economy},
journal = {Government Information Quarterly},
volume = {39},
number = {4},
pages = {101624},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101624},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000605},
author = {Manu Sharma and Sunil Luthra and Sudhanshu Joshi and Anil Kumar},
keywords = {Artificial intelligence, Implementing challenges, Public manufacturing sector, AI enabled systems, Emerging economies},
abstract = {The growing Artificial Intelligence (AI) age has been flooded with several innovations in algorithmic machine learning that may bring significant impacts to industries such as healthcare, agriculture, education, manufacturing, retail etc. But challenges such as data quality, privacy and lack of a skilled workforce limit the scope of AI implementation in emerging economies, particularly in the Public Manufacturing Sector (PMS). Therefore, to enhance the body of relevant literature, this study examines the existing challenges of AI implementation in PMS of India and explores the inter-relationships among them. The study has utilized the DEMATEL method for identification of the cause-and-effect group factors. The findings reveal that poor data quality, managers' lack of understanding of cognitive technologies, data privacy, problems in integrating cognitive projects and expensive technologies are the main challenges for AI implementation in PMS of India. Moreover, a model is proposed for industrial decision-makers and managers to take appropriate decisions to develop intelligent AI enabled systems for manufacturing organizations in emerging economies.}
}
@article{WEI2022100153,
title = {Perspective: Predicting and optimizing thermal transport properties with machine learning methods},
journal = {Energy and AI},
volume = {8},
pages = {100153},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100153},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000143},
author = {Han Wei and Hua Bao and Xiulin Ruan},
keywords = {Thermal transport properties, Machine learning, Prediction, Optimization},
abstract = {In recent years, (big) data science has emerged as the “fourth paradigm” in physical science research. Data-driven techniques, e.g. machine learning, are advantageous in dealing with problems of high-dimensional features and complex mappings between quantities, which are otherwise of great difficulty or huge cost with other scientific paradigms. In the past five years or so, there has been a rapid growth of machine learning-assisted research on thermal transport. In this perspective, we review the recent progress in the intersection between machine learning and thermal transport, where machine learning methods generally serve as surrogate models for predicting the thermal transport properties, or as tools for designing structures for the desired thermal properties and exploring thermal transport mechanisms. We provide perspectives about the advantages of machine learning methods in comparison to the physics-based methods for studying thermal transport properties. We also discuss how to improve the accuracy of predictive analytics and efficiency of structural optimization, to provide guidance for better utilizing machine learning-based methods to advance thermal transport research. Finally, we identify several outstanding challenges in this active area as well as opportunities for future developments, including developing machine learning methods suitable for small datasets, discovering effective physics-based descriptors, generating dataset from experiments and validating machine learning results with experiments, and making breakthroughs via discovering new physics.}
}
@article{MACIAS2022,
title = {Nowcasting food inflation with a massive amount of online prices},
journal = {International Journal of Forecasting},
year = {2022},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2022.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S016920702200036X},
author = {Paweł Macias and Damian Stelmasiak and Karol Szafranek},
keywords = {Inflation nowcasting, Online prices, Big data, Nowcasting competition, Web scraping},
abstract = {The consensus in the literature on providing accurate inflation forecasts underlines the importance of precise nowcasts. In this paper, we focus on this issue by employing a unique, extensive dataset of online food and non-alcoholic beverages prices gathered automatically from the webpages of major online retailers in Poland since 2009. We perform a real-time nowcasting experiment by using a highly disaggregated framework among popular, simple univariate approaches. We demonstrate that pure estimates of online price changes are already effective in nowcasting food inflation, but accounting for online food prices in a simple, recursively optimized model delivers further gains in the nowcast accuracy. Our framework outperforms various other approaches, including judgmental methods, traditional benchmarks, and model combinations. After the outbreak of the COVID-19 pandemic, its nowcasting quality has improved compared to other approaches and remained comparable with judgmental nowcasts. We also show that nowcast accuracy increases with the volume of online data, but their quality and relevance are essential for providing accurate in-sample fit and out-of-sample nowcasts. We conclude that online prices can markedly aid the decision-making process at central banks.}
}
@article{MUNINGER2022140,
title = {Social media use: A review of innovation management practices},
journal = {Journal of Business Research},
volume = {143},
pages = {140-156},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322000510},
author = {Marie-Isabelle Muninger and Dominik Mahr and Wafa Hammedi},
keywords = {Social media, Innovation, Systematic review, Framework and research agenda},
abstract = {The use of social media for innovation requires firms to manage rapid information transfers, big data, and multiway communication. Yet managers lack clear insights on the way social media should be managed and current literature is dispersed across various research streams. In this article, the authors aim to develop a better understanding of how social media use should be leveraged for innovation. To achieve this objective, they build a systematic review of evidence from 177 scientific articles across four key management disciplines. They analyze research perspectives and conceptualizations of social media use for innovation and provide a framework of the drivers, contingencies and outcomes related to this topic. Next, they attempt to identify what is currently known about social media use for innovation. Last, they suggest critical areas for future inquiry on this important subject.}
}
@article{KUO2022,
title = {Public transport for smart cities: Recent innovations and future challenges},
journal = {European Journal of Operational Research},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2022.06.057},
url = {https://www.sciencedirect.com/science/article/pii/S037722172200546X},
author = {Yong-Hong Kuo and Janny M.Y. Leung and Yimo Yan},
keywords = {Transportation, Public transit, Smart cities, Analytics, Survey},
abstract = {The idea of a smart city is one that utilises Internet-of-Things (IoT) technologies and data analytics to optimise the efficiency of city operations and services, so as to provide a high quality of life for its citizens. Due to reduced public funding, many public transport systems are already facing challenges to maintain their services. For a smart city, the goal of public transport is not simply the movement of people, but the provision and enhancement of mobility for living. This will be particularly challenging due to changes in habitation trends and work patterns. For example, the growth of mega-cities has led to extreme traffic congestion in city centres and urban sprawl on their outskirts. In order to provide sufficient coverage and frequency of service, an integrated co-ordinated multi-modal public transport system is needed, leading to substantial increase in operational complexity. Environmental concerns and the recent pandemic may also change work and commuting patterns in the future, with more people working from home and companies adopting flexible work shifts. For smart cities, public transport must offer ubiquitous access, real-time response to demand, convenience and quality service, and energy-efficient operations. This paper will discuss the challenges in network design, operations planning, scheduling and management of smart public transport systems. A brief survey of recent research and innovations will also be presented.}
}