@ARTICLE{9254094,
author={Sun, Jin and Li, Heng and Zhang, Yi and Xu, Yang and Zhu, Yaoqin and Zang, Qitao and Wu, Zebin and Wei, Zhihui},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Multiobjective Task Scheduling for Energy-Efficient Cloud Implementation of Hyperspectral Image Classification},
year={2021},
volume={14},
number={},
pages={587-600},
abstract={Cloud computing has become a promising solution to efficient processing of remotely sensed big data, due to its high-performance and scalable computing capabilities. However, existing cloud solutions generally involve the problems of low resource utilization and high energy consumption when processing large-scale remote sensing datasets, affecting the quality-of-service of the cloud system. Aiming at hyperspectral image classification applications, this article proposes an energy-efficient cloud implementation by employing a multiobjective task scheduling algorithm. We first present a parallel computing mechanism for a fusion-based classification method based on Apache Spark. With the general classification flow represented by a workflow model, we formulate a multiobjective scheduling framework that jointly minimizes the total execution time as well as energy consumption. We further develop an effective scheduling algorithm to solve the multiobjective optimization problem and produce a set of Pareto-optimal solutions, providing the tradeoff between computational efficiency and energy efficiency. Experimental results demonstrate that the multiobjective scheduling approach proposed in this work can substantially reduce the execution time and energy consumption for performing large-scale hyperspectral image classification on Spark. In addition, our proposed algorithm can generate better tradeoff solutions to the multiobjective scheduling problem as compared to competing scheduling algorithms.},
keywords={Cloud computing;Hyperspectral imaging;Task analysis;Scheduling;Scheduling algorithms;Energy consumption;Computational modeling;Energy consumption;hyperspectral image classification;makespan;multiobjective optimization;task scheduling},
doi={10.1109/JSTARS.2020.3036896},
ISSN={2151-1535},
month={},}
@ARTICLE{9143071,
author={De Santis, Enrico and Martino, Alessio and Rizzi, Antonello},
journal={IEEE Access},
title={An Infoveillance System for Detecting and Tracking Relevant Topics From Italian Tweets During the COVID-19 Event},
year={2020},
volume={8},
number={},
pages={132527-132538},
abstract={The year 2020 opened with a dramatic epidemic caused by a new species of coronavirus that soon has been declared a pandemic by the WHO due to the high number of deaths and the critical mass of worldwide hospitalized patients, of order of millions. The COVID-19 pandemic has forced the governments of hundreds of countries to apply several heavy restrictions in the citizens’ socio-economic life. Italy was one of the most affected countries with long-term restrictions, impacting the socio-economic tissue. During this lockdown period, people got informed mostly on Online Social Media, where a heated debate followed all main ongoing events. In this scenario, the following study presents an in-depth analysis of the main emergent topics discussed during the lockdown phase within the Italian Twitter community. The analysis has been conducted through a general purpose methodological framework, grounded on a biological metaphor and on a chain of NLP and graph analysis techniques, in charge of detecting and tracking emerging topics in Online Social Media, e.g. streams of Twitter data. A term-frequency analysis in subsequent time slots is pipelined with nutrition and energy metrics for computing hot terms by also exploiting the tweets quality information, such as the social influence of the users. Finally, a co-occurrence analysis is adopted for building a topic graph where emerging topics are suitably selected. We demonstrate via a careful parameter setting the effectiveness of the topic tracking system, tailored to the current Twitter standard API restrictions, in capturing the main sociopolitical events that occurred during this dramatic phase.},
keywords={Twitter;Feature extraction;Natural language processing;Monitoring;Windows;COVID-19;Natural language processing;topic tracking;topic detection;social network analysis;text mining;COVID-19;infodemiology;infoveillance},
doi={10.1109/ACCESS.2020.3010033},
ISSN={2169-3536},
month={},}
@ARTICLE{9335934,
author={Chao, Xiaopeng and Cao, Jiangzhong and Lu, Yuqin and Dai, Qingyun and Liang, Shangsong},
journal={IEEE Access},
title={Constrained Generative Adversarial Networks},
year={2021},
volume={9},
number={},
pages={19208-19218},
abstract={Generative Adversarial Networks (GANs) are a powerful subclass of generative models. Yet, how to effectively train them to reach Nash equilibrium is a challenge. A number of experiments have indicated that one possible solution is to bound the function space of the discriminator. In practice, when optimizing the standard loss function without limiting the discriminator's output, the discriminator may suffer from lack of convergence. To be able to reach the Nash equilibrium in a faster way during training and obtain better generative data, we propose constrained generative adversarial networks, GAN-C, where a constraint on the discriminator's output is introduced. We theoretically prove that our proposed loss function shares the same Nash equilibrium as the standard one, and our experiments on mixture of Gaussians, MNIST, CIFAR-10, STL-10, FFHQ, and CAT datasets show that our loss function can better stabilize training and yield even better high-quality images.},
keywords={Training;Nash equilibrium;Generators;Standards;Generative adversarial networks;Games;Gallium nitride;Generative adversarial networks;Nash equilibrium;Lipschitz constraint},
doi={10.1109/ACCESS.2021.3054822},
ISSN={2169-3536},
month={},}
@ARTICLE{8648375,
author={Zhao, Zhongyao and Liu, Chengyu and Li, Yaowei and Li, Yixuan and Wang, Jingyu and Lin, Bor-Shyh and Li, Jianqing},
journal={IEEE Access},
title={Noise Rejection for Wearable ECGs Using Modified Frequency Slice Wavelet Transform and Convolutional Neural Networks},
year={2019},
volume={7},
number={},
pages={34060-34067},
abstract={Progress in wearable techniques makes the long-term daily electrocardiogram (ECG) monitoring possible. However, the long-term wearable ECGs can be significantly contaminated by various noises, which affect the detection and diagnosis of cardiovascular diseases (CVDs). The situation becomes more serious for wearable ECG screening, where the data are huge, and doctors have no way to visually check the signal quality episode-by-episode. Therefore, automatic and accurate noise rejection for the wearable big-data ECGs is craving. This paper addressed this issue and proposed a noise rejection method for wearable ECGs based on the combination of modified frequency slice wavelet transform (MFSWT) and convolutional neural network (CNN). Wearable ECGs were recorded using the newly developed 12-lead Lenovo smart ECG vest with a sample rate of 500 Hz and a resolution of 16 bit. One thousand 10-s ECG segments were picked up and were manually labeled into three quality types: clinically useful segments with good signal quality (type A), clinically useful segments with poor signal quality (type B), and clinically useless segments (pure noises, type C). Each of the 1,000 10-s ECG segments were transformed into a 2-D time-frequency (T-F) image using the MFSWT, with a pixel size of 200×50. Then, the 2-D grayscale images from MFSWT were fed into a 13-layer CNN model for training the classification models. Results from the standard 5-folder cross-validation showed that the proposed combination method of MFSWT and CNN achieved a highest classification accuracy of 86.3%, which was higher than the comparable methods from continuous wavelet transform (CWT) and artificial neural networks (ANN). The combination of MFSWT and CNN also had a good calculation efficiency. This paper indicated that the combination of MFSWT and CNN is a potential method for automatic identification of noisy segments from wearable ECG recordings.},
keywords={Electrocardiography;Image segmentation;Biomedical monitoring;Time-frequency analysis;Continuous wavelet transforms;Wearable ECG;signal quality assessment (SQA);convolutional neural network (CNN);modified frequency slice wavelet transform (MFSWT)},
doi={10.1109/ACCESS.2019.2900719},
ISSN={2169-3536},
month={},}
@ARTICLE{9130052,
author={Ansari, Homa and De Zan, Francesco and Parizzi, Alessandro},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={Study of Systematic Bias in Measuring Surface Deformation With SAR Interferometry},
year={2021},
volume={59},
number={2},
pages={1285-1301},
abstract={This article investigates the presence of a new interferometric signal in multilooked synthetic aperture radar (SAR) interferograms that cannot be attributed to the atmospheric or Earth-surface topography changes. The observed signal is short-lived and decays with the temporal baseline; however, it is distinct from the stochastic noise attributed to temporal decorrelation. The presence of such a fading signal introduces a systematic phase component, particularly in short temporal baseline interferograms. If unattended, it biases the estimation of Earth surface deformation from SAR time series. Here, the contribution of the mentioned phase component is quantitatively assessed. The biasing impact on the deformation-signal retrieval is further evaluated. A quality measure is introduced to allow the prediction of the associated error with the fading signals. Moreover, a practical solution for the mitigation of this physical signal is discussed; special attention is paid to the efficient processing of Big Data from modern SAR missions such as Sentinel-1 and NISAR. Adopting the proposed solution, the deformation bias is shown to decrease significantly. Based on these analyses, we put forward our recommendations for efficient and accurate deformation-signal retrieval from large stacks of multilooked interferograms.},
keywords={Strain;Fading channels;Time series analysis;Synthetic aperture radar;Systematics;Decorrelation;Moisture;Big Data;deformation estimation;differential interferometric synthetic aperture radar (SAR) (DInSAR);distributed scatterers (DSs);error analysis;near real-time (NRT) processing;phase inconsistencies;signal decorrelation;time-series analysis},
doi={10.1109/TGRS.2020.3003421},
ISSN={1558-0644},
month={Feb},}
@ARTICLE{9016229,
author={Zhijun, Wu and Wenjing, Li and Liang, Liu and Meng, Yue},
journal={IEEE Access},
title={Low-Rate DoS Attacks, Detection, Defense, and Challenges: A Survey},
year={2020},
volume={8},
number={},
pages={43920-43943},
abstract={Low-rate Denial of service (LDoS) attacks has become one of the biggest threats to the Internet, cloud computing platforms, and big data centers. As an evolutionary species of DDoS attack, LDoS attack is essentially different from the DDoS attack. DDoS attacks are the behavior of malicious blocking legitimate network traffic by destroying the targets and the infrastructure around it with huge network traffic. While, LDoS attacks are the behavior of intentional degrading the quality of TCP links by throttling TCP flows to a small fraction of its ideal rate with periodic small pulse sequence. Hence, LDoS attack has a very small flow (around 10%–20% of the background traffic), it is easy to eluding the detection of routers and counter-DoS mechanisms. We try to reveal the mechanism of the LDoS attack and attempt to figure out the generation principle of LDoS attack in this paper. We classify the LDoS attacks and existing defense methods according to time domain and frequency domain in which detection and defense are performed. Furthermore, we highlight the filter approach to defense against LDoS attack. The initial purpose of our work is to encourage researchers to study effective ways to detect and defend against LDoS attacks with innovation and aggressiveness.},
keywords={Computer crime;Cloud computing;Big Data;Floods;Telecommunication traffic;Servers;Low-rate Denial of Service attacks (LDoS);detection method;attack prevention system;defense mechanism},
doi={10.1109/ACCESS.2020.2976609},
ISSN={2169-3536},
month={},}
@ARTICLE{8758074,
author={Pingfang, Tian and Qiang, Guang and Xing, Liu},
journal={Journal of Systems Engineering and Electronics},
title={Cloud detection from visual band of satellite image based on variance of fractal dimension},
year={2019},
volume={30},
number={3},
pages={485-491},
abstract={Cover ratio of cloud is a very important factor which affects the quality of a satellite image, therefore cloud detection from satellite images is a necessary step in assessing the image quality. The study on cloud detection from the visual band of a satellite image is developed. Firstly, we consider the differences between the cloud and ground including high grey level, good continuity of grey level, area of cloud region, and the variance of local fractal dimension (VLFD) of the cloud region. A single cloud region detection method is proposed. Secondly, by introducing a reference satellite image and by comparing the variance in the dimensions corresponding to the reference and the tested images, a method that detects multiple cloud regions and determines whether or not the cloud exists in an image is described. By using several Ikonos images, the performance of the proposed method is demonstrated.},
keywords={Wireless fidelity;Buildings;Big Data;Switches;Throughput;Markov processes;cloud detection;visual image;satellite image;variance of local fractal dimension (VLFD)},
doi={10.21629/JSEE.2019.03.06},
ISSN={1004-4132},
month={June},}
@ARTICLE{9676611,
author={Zhang, Jiyong and Liu, Xin and Zhou, Xiaofei},
journal={IEEE Access},
title={Towards Non-Linear Social Recommendation Using Gaussian Process},
year={2022},
volume={10},
number={},
pages={6028-6041},
abstract={Recent research on recommender systems has proved that by leveraging social network information, the quality of recommendations can be evidently improved. Traditional social recommendation models typically linearly combine social network information. For instance, matrix factorization based models linearly combine latent factors of relevant users and items. However, in practice, the multifaceted social relations are so complex that simple linear combination may not be able to reasonably organize such information for accurate social recommendation. On the other hand, existing deep learning based non-linear methods lack systematic modeling of user-item-friend relations. To handle these issues, we propose a novel, non-linear latent factor model for social recommendations leveraging Gaussian process. By introducing a social-aware covariance function, we organize individual users’ past feedback, as well as the associated social information (e.g., friends’ feedback to the same items) into a covariance matrix, which non-linearly and systematically learns the complex interactions among users, their interacted items and their friends’ opinions. A stochastic gradient descent based optimization algorithm is developed to fit the model. Extensive experiments conducted on three real-world datasets demonstrate that the proposed model outperforms the state-of-the-art social recommendation models and Gaussian process based models.},
keywords={Social networking (online);Gaussian processes;Collaboration;Predictive models;Recommender systems;Deep learning;Data models;Social recommendation;recommender systems;Gaussian process;social networks},
doi={10.1109/ACCESS.2022.3141795},
ISSN={2169-3536},
month={},}
@ARTICLE{9133570,
author={Wang, Jianyuan and Leng, Biao and Wu, Junjie and Du, Heng and Xiong, Zhang},
journal={IEEE Access},
title={MetroEye: A Weather-Aware System for Real-Time Metro Passenger Flow Prediction},
year={2020},
volume={8},
number={},
pages={129813-129829},
abstract={Real-time passenger flow prediction plays an important role in subway network design and management. Most of the existing prediction algorithms only consider the sequence of passenger flow volume, however, ignore the influence of other outer factors, for example, the weather conditions, air quality and temperature. In this paper, a systematic framework, MetroEye, is proposed for weather-aware prediction of real-time passenger flow. The framework contains an offline system and an online system. The offline system adopts a conditional random field (CRF) model to establish the relationship between passenger flow volume and weather factors. Experimental results show the superior prediction accuracy of the model, especially in large stations. The online system provides efficient methods to simulate the real-time passenger flow volume. Due to its high practicality, MetroEye has been adopted by Beijing Urban Rail Transit Control Center to monitor the passenger flow status of the Beijing subway system.},
keywords={Prediction algorithms;Public transportation;Real-time systems;Meteorology;Predictive models;Autoregressive processes;Neural networks;Passenger flow prediction;subway network;conditional random field;intelligent transportation},
doi={10.1109/ACCESS.2020.3007538},
ISSN={2169-3536},
month={},}
@ARTICLE{9426905,
author={Zhou, Jin and Guo, Xin and Li, Feng},
journal={IEEE Access},
title={Urban Rail Train Scheduling With Smoothing Energy Consumption Peaks and Synchronization Time Minimization: Using Novel Time-Shift Control Scheme},
year={2021},
volume={9},
number={},
pages={70142-70154},
abstract={Considering operators of the urban rail transit systems are often faced with cost control, passengers required high service quality, which has inter-affection between each other in congestion issues for the peak period. A good cooperative timetable associated with time-based shift radios is proposed to achieve a mutually beneficial win-win situation for operators required low energy consumption (costs), and passengers required short waiting time in high peak level with different time-shift control schemes by shifting passengers' travel times. By seeking the optimal shift radios, we focus on generating a favorable train schedule by taking the optimal decisions in the presence of trade-offs between two conflicting objectives. Subsequently, an improved non-dominated sorting in genetic algorithms (INSGA-II) was presented to solve the multi-objective programming model. Finally, the computational results show that the optimized time-shift control scheme brings a significant effect on reducing congestion during the peak periods.},
keywords={Rails;Synchronization;Energy consumption;Sun;Rail transportation;Programming;Optimization;Energy consumption;synchronization time;time-shift control scheme;congestion},
doi={10.1109/ACCESS.2021.3078569},
ISSN={2169-3536},
month={},}
@ARTICLE{9220134,
author={Yan, Bai and Zhao, Qi and Zhang, J. Andrew and Wang, Zhihai},
journal={IEEE Access},
title={Multi-Objective Sparse Reconstruction With Transfer Learning and Localized Regularization},
year={2020},
volume={8},
number={},
pages={184920-184933},
abstract={Multi-objective sparse reconstruction methods have shown strong potential in sparse reconstruction. However, most methods are computationally expensive due to the requirement of excessive functional evaluations. Most of these methods adopt arbitrary regularization values for iterative thresholding-based local search, which hardly produces high-precision solutions stably. In this article, we propose a multi-objective sparse reconstruction scheme with novel techniques of transfer learning and localized regularization. Firstly, we design a knowledge transfer operator to reuse the search experience from previously solved homogeneous or heterogeneous sparse reconstruction problems, which can significantly accelerate the convergence and improve the reconstruction quality. Secondly, we develop a localized regularization strategy for iterative thresholding-based local search, which uses systematically designed independent regularization values according to decomposed subproblems. The strategy can lead to improved reconstruction accuracy. Therefore, our proposed scheme is more computationally efficient and accurate, compared to existing multi-objective sparse reconstruction methods. This is validated by extensive experiments on simulated signals and benchmark problems.},
keywords={Search problems;Image reconstruction;Knowledge transfer;Iterative methods;Convergence;Evolutionary computation;Optimization;Sparse reconstruction;multi-objective evolutionary algorithm;transfer learning;regularization},
doi={10.1109/ACCESS.2020.3029968},
ISSN={2169-3536},
month={},}
@ARTICLE{9347418,
author={Khan, Rizwan and Akram, Adeel and Mehmood, Atif},
journal={IEEE Access},
title={Multiview Ghost-Free Image Enhancement for In-the-Wild Images With Unknown Exposure and Geometry},
year={2021},
volume={9},
number={},
pages={24205-24220},
abstract={The multiview low dynamic range images captured with sparse camera arrangement under ill-lighting conditions contain highlighted and shadow regions due to over-exposed and under-exposed regions. The processing of these images produces contrast distortion, and it is challenging to maintain relative brightness with color consistency. Moreover, the disparity map estimation faces the challenges of holes and artifacts due to a wide baseline and poor visibility, with a shared view of vision. In this article, we propose a multiview ghost-free image enhancement strategy for in-the-wild images with unknown exposure and geometry. We address the complex geometric alignment problem for a wide variational baseline among multiple sparsely arranged cameras. The features among multiple viewpoints are detected and matched for the image restoration. The restored image contains highlighted and shadow regions with a color imbalance problem. We synthesize virtual images following the intensity mapping function, which compensates for the relative brightness and color distortions. Finally, we fuse all the images to obtain high-quality images. The proposed method is more frequent and feasible for future multiview systems with varying baselines without relying on disparity maps. The experimental results demonstrate that the proposed method outperformed the state-of-the-art approaches.},
keywords={Lighting;Cameras;Image restoration;Dynamic range;Estimation;Noise reduction;Image color analysis;Multi-view images;feature matching;virtual images;exposure fusion},
doi={10.1109/ACCESS.2021.3057167},
ISSN={2169-3536},
month={},}
@ARTICLE{9026964,
author={Song, Liwen and Li, Ruizhi and Chen, Shiqiang},
journal={IEEE Access},
title={Fabric Defect Detection Based on Membership Degree of Regions},
year={2020},
volume={8},
number={},
pages={48752-48760},
abstract={The detection of fabric defects is an important part of fabric quality control, and hence is thus a research hotspot in the textile industry. With the aim of effectively detecting fabric defects, this paper describes an improved fabric defect detection method based on the membership degree of each fabric region (TPA). By analyzing the regional features of fabric surface defects, the saliency of defect regions can be determined using the extreme point density map of the image and the features of the membership function region. A threshold iterative method and morphological processing are used to ensure the precise and accurate detection of fabric defects. Experimental results show that compared with two classical fabric defect detection methods, the proposed detection method can detect fabric defects more effectively while also suppressing the interference of noise and background textures. Additionally, numerical results demonstrate the validity and feasibility of the proposed method to satisfy the requirements of online detection.},
keywords={Fabrics;Image segmentation;Density functional theory;Feature extraction;Iterative methods;Visualization;Interference;Fabric defect detection;membership;saliency mapping;threshold iterative method},
doi={10.1109/ACCESS.2020.2978900},
ISSN={2169-3536},
month={},}
@ARTICLE{8952710,
author={Brenas, Jon Haël and Shaban-Nejad, Arash},
journal={IEEE Access},
title={Health Intervention Evaluation Using Semantic Explainability and Causal Reasoning},
year={2020},
volume={8},
number={},
pages={9942-9952},
abstract={As serious public health problems require complex responses, health interventions often involve multiple components implemented by groups including policy experts, social workers, and health practitioners. The success or failure of an intervention depends on many different factors, ranging from available resources to characteristics of the targeted public health issue and community to the complex mechanics relating cause and effects of the actions performed. In this paper, we present a novel formal methodology to evaluate public health interventions, policies, and programs. Our method uses the theory of change (TOC) approach along with logic models that define the intervention under consideration to generate a causal diagram and an ontology-based inference model for causal description. The resulting causal diagram will then be compared to existing knowledge and data to determine whether the intervention is coherent, internally consistent and its goals are achievable in the allotted time with the resources provided. The contextual knowledge and semantics provided by the ontology will generate a more explainable, understandable, and trustworthy approach to compare and assess different interventions based on their shared goals. Depending upon the quality and quantity of data available we perform a mix of qualitative and quantitative evaluation of the interventions. This study uses smoking cessation interventions to showcase the proposed methodology in action.},
keywords={Ontologies;Public healthcare;Semantics;Cognition;Adaptation models;Biological system modeling;Sociology;Causal graphs;intervention evaluation;logic models;ontologies;explainable AI;public health program evaluation},
doi={10.1109/ACCESS.2020.2964802},
ISSN={2169-3536},
month={},}
@ARTICLE{8853323,
author={Dorado, Hugo and Cobos, Carlos and Torres-Jimenez, Jose and Burra, Dharani Dhar and Mendoza, Martha and Jiménez, Daniel},
journal={IEEE Access},
title={Wrapper for Building Classification Models Using Covering Arrays},
year={2019},
volume={7},
number={},
pages={148297-148312},
abstract={Wrapper methods are a type of feature selection method that finds a subset of variables to improve the performance of a classifier by removing redundant and irrelevant variables. The use of a wrapper implies that each time a candidate solution is explored, the classifier is evaluated on the quality measures selected (e.g. accuracy or precision). Though robust, this iteration across several candidate solutions can become computationally intensive and time-consuming. In this paper we propose a wrapper, that is based on binary Covering Arrays (CAs), and binary Incremental Covering Arrays (ICAs), that have been widely used for experimental design and fault detection in software and hardware testing. The new wrapper was evaluated with six classifiers on seven data sets. The results show that the CAs and ICAs with strength 6 significantly improve the performance and reduces the number of variables required by the classifier. A comparative analysis of the proposed method against wrappers based on other search approaches such as genetic algorithms (GA) and particle swarm optimization (PSO), shows that the proposed method yields results similar to GA, but not to PSO, with differences to PSO, in accuracy, which in the majority of cases is below 0.04. This lack of accuracy, by which the new wrapper fails to match PSO, is offset by the fact that the user does not need to fine tune algorithm parameters, such as velocity ranges, timing, cognitive coefficient, and social coefficient, while it is also much easier to program in parallel.},
keywords={Feature extraction;Genetic algorithms;Arrays;Search problems;Buildings;Testing;Particle swarm optimization;Classification algorithms;covering arrays;random forest;support vector machines;genetic algorithms;particle swarm optimization},
doi={10.1109/ACCESS.2019.2944641},
ISSN={2169-3536},
month={},}
@ARTICLE{7986937,
author={Cai, Zhipeng and Bourgeois, Anu and Tong, Weitian},
journal={Tsinghua Science and Technology},
title={Guest editorial: Special issue on Internet of Things},
year={2017},
volume={22},
number={4},
pages={343-344},
abstract={Internet of Things (IoT) is a new paradigm that the ubiquitous smart objects, such as devices, vehicles, buildings, etc., interact and exchange data through emerging wireless technology with the intention of improving people's quality of lives in variety areas, such as transportation, manufacturing industry, health care industry, etc. Besides benefits, this envisioned paradigm also poses unprecedented challenges in many respects, including identification of things, privacy and security issues, integration and management of big sensory data, sensing and delivering information from dynamic environments, utilization of knowledge-based decision systems, connectivity issues, etc.},
keywords={Wireless sensor networks;Computer science;Special issues and sections;Internet of Things;Security;Delays;Data privacy},
doi={10.23919/TST.2017.7986937},
ISSN={1007-0214},
month={Aug},}
@ARTICLE{8004469,
author={Roberts, Dale and Mueller, Norman and Mcintyre, Alexis},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={High-Dimensional Pixel Composites From Earth Observation Time Series},
year={2017},
volume={55},
number={11},
pages={6254-6264},
abstract={High-quality and large-scale image composites are increasingly important for a variety of applications. Yet a number of challenges still exist in the generation of composites with certain desirable qualities such as maintaining the spectral relationship between bands, reduced spatial noise, and consistency across scene boundaries so that large mosaics can be generated. We present a new method for generating pixel-based composite mosaics that achieves these goals. The method, based on a high-dimensional statistic called the `geometric median,' effectively trades a temporal stack of poor quality observations for a single high-quality pixel composite with reduced spatial noise. The method requires no parameters or expert-defined rules. We quantitatively assess its strengths by benchmarking it against two other pixel-based compositing approaches over Tasmania, which is one of the most challenging locations in Australia for obtaining cloud-free imagery.},
keywords={Earth;Satellites;Time series analysis;Remote sensing;Australia;Electric breakdown;Clouds;Big data applications;image analysis;remote sensing;time series analysis},
doi={10.1109/TGRS.2017.2723896},
ISSN={1558-0644},
month={Nov},}
@ARTICLE{8703749,
author={Cao, Ming-Wei and Li, Lin and Xie, Wen-Jun and Jia, Wei and Lv, Zhi-Han and Zheng, Li-Ping and Liu, Xiao-Ping},
journal={IEEE Access},
title={Parallel K Nearest Neighbor Matching for 3D Reconstruction},
year={2019},
volume={7},
number={},
pages={55248-55260},
abstract={In recent years, a 3D reconstruction based on structure from motion (SFM) has attracted much attention from the communities of computer vision and graphics. It is well known that the speed and quality of SFM systems largely depend on the technique of feature tracking. If a big volume of image data is inputted for SFM, the speed of this SFM system would become very slow. And, this problem becomes severer for large-scale scenes, which typically needs to capture several thousands of images to recover the point-cloud model of the scene. However, none of the existing methods fully addresses the problem of fast feature tracking. Brute force matching is capable of producing correspondences for small-scale scenes but often getting stuck in repeated features. Hashing matching can only deal with middle-scale scenes and is not capable of large-scale scenes. In this paper, we propose a new feature tacking method working in a parallel manner rather than in a single thread scheme. Our method consists of steps of keypoint detection, descriptor computing, descriptor matching by parallel k -nearest neighbor (Parallel-KNN) search, and outlier rejecting. This method is able to rapidly match a big volume of keypoints and avoids to consume high computation time, then yielding a set of correct correspondences. We demonstrate and evaluate the proposed method on several challenging benchmark datasets, including those with highly repeated features, and compare to the state-of-the-art methods. The experimental results indicate that our method outperforms the compared methods in both efficiency and effectiveness.},
keywords={Three-dimensional displays;Cameras;Acceleration;Feature extraction;Graphics processing units;Solid modeling;Computational modeling;3D reconstruction;K nearest neighbor;feature matching;structure from motion;parallel computing},
doi={10.1109/ACCESS.2019.2912647},
ISSN={2169-3536},
month={},}
@ARTICLE{9345696,
author={Njah, Yosra and Cheriet, Mohamed},
journal={IEEE Access},
title={Parallel Route Optimization and Service Assurance in Energy-Efficient Software-Defined Industrial IoT Networks},
year={2021},
volume={9},
number={},
pages={24682-24696},
abstract={In recent years, the Industrial world has been embracing new digital technology, including the internet of things (IoT) paradigm that promises revolutionizing-prospects in numerous industrial applications. However, many deployment challenges related to real-time big data analytics, service assurance, resource optimization, energy consumption, and security awareness are raised. In this work, we focus on service assurance and resource optimization, including energy consumption challenges over Industrial Internet of Things (IIoT)-based environments since the existing network routing algorithms cannot meet the strict heterogeneous quality of service (QoS) requirements of industrial communications while optimizing resources. We take advantage of the flexibility and programmability offered by the promising software-defined networking paradigm, and we propose a centralized route optimization and service assurance scheme, named ROSA, over a multi-layer programmable industrial architecture. The proposed solution supports a wide range of heterogeneous flows, such as ultra-reliable low-latency communications (URLLC) and bandwidth-sensitive services. The routing optimization problems are formulated as multi-constrained shortest path problems. The Lagrangian Relaxation approach is used to solve the . Hence, we deploy a pair of parallel routing algorithms run according to the flow type to ensure QoS requirements, efficiently allocate constrained resources, and enhance the overall network energy consumption. We conduct extensive simulations to validate the proposed ROSA scheme. The experimental results show promising performance in terms of reducing bandwidth utilization by up to 22%, end-to-end delay at least by 21%, packet loss by more than 19%, flow violation by about 16%, and energy consumption up to 14% as compared to well-known benchmarks in QoS provisioning and energy-aware routing problem.},
keywords={Quality of service;Optimization;Routing;Industrial Internet of Things;Energy consumption;Production;Delays;Industrial Internet of Things (IIoT);software-defined networking (SDN);multiprogrammability;traffic engineering;Quality of Service (QoS);energy awareness;resource optimization},
doi={10.1109/ACCESS.2021.3056931},
ISSN={2169-3536},
month={},}
@ARTICLE{8409952,
author={Cui, Lei and Xie, Gang and Qu, Youyang and Gao, Longxiang and Yang, Yunyun},
journal={IEEE Access},
title={Security and Privacy in Smart Cities: Challenges and Opportunities},
year={2018},
volume={6},
number={},
pages={46134-46145},
abstract={Smart cities are expected to improve the quality of daily life, promote sustainable development, and improve the functionality of urban systems. Now that many smart systems have been implemented, security and privacy issues have become a major challenge that requires effective countermeasures. However, traditional cybersecurity protection strategies cannot be applied directly to these intelligent applications because of the heterogeneity, scalability, and dynamic characteristics of smart cities. Furthermore, it is necessary to be aware of security and privacy threats when designing and implementing new mechanisms or systems. Motivated by these factors, we survey the current situations of smart cities with respect to security and privacy to provide an overview of both the academic and industrial fields and to pave the way for further exploration. Specifically, this survey begins with an overview of smart cities to provide an integrated context for readers. Then, we discuss the privacy and security issues in current smart applications along with the corresponding requirements},
keywords={Smart cities;Security;Privacy;Computer architecture;Sensors;Smart city;Internet of Things;security;privacy},
doi={10.1109/ACCESS.2018.2853985},
ISSN={2169-3536},
month={},}
@ARTICLE{8424161,
author={Melin, Magnus and BäCk, Asta and SöDergåRd, Caj and Munezero, Myriam D. and LeppäNen, Leo J. and Toivonen, Hannu},
journal={IEEE Access},
title={No Landslide for the Human Journalist - An Empirical Study of Computer-Generated Election News in Finland},
year={2018},
volume={6},
number={},
pages={43356-43367},
abstract={In an age of struggling news media, automated generation of news via natural language generation (NLG) methods could be of great help, especially in areas where the amount of raw input data is big, and the structure of the data is known in advance. One such news automation system is the Valtteri NLG system, which generates news articles about the Finnish municipal elections of 2017. To evaluate the quality of Valtteri-produced articles and to identify aspects to improve, n = 152 users were asked to evaluate the output of Valtteri. Each evaluator rated six preselected computer-generated articles, four control articles written by journalists, and four computer-generated articles of their own choice. All the articles were evaluated along four dimensions: credibility, liking, quality, and representativeness. As expected, the texts written by Valtteri received lower ratings than those written by journalists, but overall the ratings were satisfactory (average 2.9 versus 4.0 for journalists on a five-point scale). Valtteri's best rating (3.6) was for credibility. The computer-written articles that the evaluators could freely select got slightly better ratings than the preselected computer-written articles. When looking at the results by demographic groups, males aged 55 or more liked the automatic articles best and females aged 34 or less liked them the least. Evaluators mistook 21% of the computer-written articles as written by humans and 10% of the human-written articles as computer-written. The share of users making these mistakes grew with the age. Overall, the male evaluators made less writer-identification mistakes than female evaluators did.},
keywords={Voting;Computer architecture;Automation;Media;Data mining;Industries;Artificial intelligence;automated content generation;automated storytelling;natural language processing;robot journalism},
doi={10.1109/ACCESS.2018.2861987},
ISSN={2169-3536},
month={},}
@ARTICLE{9036949,
author={Vidal, Carlos and Malysz, Pawel and Kollmeyer, Phillip and Emadi, Ali},
journal={IEEE Access},
title={Machine Learning Applied to Electrified Vehicle Battery State of Charge and State of Health Estimation: State-of-the-Art},
year={2020},
volume={8},
number={},
pages={52796-52814},
abstract={The growing interest and recent breakthroughs in artificial intelligence and machine learning (ML) have actively contributed to an increase in research and development of new methods to estimate the states of electrified vehicle batteries. Data-driven approaches, such as ML, are becoming more popular for estimating the state of charge (SOC) and state of health (SOH) due to greater availability of battery data and improved computing power capabilities. This paper provides a survey of battery state estimation methods based on ML approaches such as feedforward neural networks (FNNs), recurrent neural networks (RNNs), support vector machines (SVM), radial basis functions (RBF), and Hamming networks. Comparisons between methods are shown in terms of data quality, inputs and outputs, test conditions, battery types, and stated accuracy to give readers a bigger picture view of the ML landscape for SOC and SOH estimation. Additionally, to provide insight into how to best approach with the comparison of different neural network structures, an FNN and long short-term memory (LSTM) RNN are trained fifty times each for 3000 epochs. The error is somewhat different for each training repetition due to the random initial values of the trainable parameters, demonstrating that it is important to train networks multiple times to achieve the best result. Furthermore, it is recommended that when performing a comparison among estimation techniques such as those presented in this review paper, the compared networks should have a similar number of learnable parameters and be trained and tested with identical data. Otherwise, it is difficult to make a general conclusion regarding the quality of a given estimation technique.},
keywords={Batteries;State of charge;Machine learning;Maximum likelihood estimation;Training;Temperature measurement;Machine learning;artificial intelligence;deep learning;battery management systems (BMS);electric vehicles;state of charge;state of health},
doi={10.1109/ACCESS.2020.2980961},
ISSN={2169-3536},
month={},}
@ARTICLE{9036967,
author={Gao, Keyan and Yang, Xu and Wu, Chenxia and Qiao, Tingting and Chen, Xiaoya and Yang, Min and Chen, Ling},
journal={IEEE Access},
title={Exploiting Location-Based Context for POI Recommendation When Traveling to a New Region},
year={2020},
volume={8},
number={},
pages={52404-52412},
abstract={Traveling to a new region has become a very common thing for people, due to work or life requirement. With the development of recommendation engine and the popularity of social media network, people are more and more used to relying on personalized Points-of-Interest (POI) recommendations. However, traditional approaches can fail if users moves to a region where they had little or no active history or even social network friends information before. Under the requirement of smart city construction, the need to give high quality personalized POI recommendation when a user travels to a new region has arisen. Fortunately, with the widespread of wireless Internet, the booming of Internet-of-Things (IoT) and the common-usage of location sensors in mobile phones, the coupling degree between social media networks and location information is ever increasing, which could leads us to a new way to solve this problem in the ear of Big Data. In this research, we presented New Place Recommendation Algorithm (N-PRA) which is designed based on Latent Factor model. Many different types of social media contexts (time-related and location-related), such as a user's interest fluctuation, different types of POIs' popularity fluctuation, types of POIs, the influence of geographical neighborhood on POIs, and user's social network friendship are taken into consideration in this approach. The algorithm presented is verified on Yelp, an open-source real urban data-set, and compared against several other baseline POI recommendation algorithms. Experimental results show that the algorithm presented in this paper could achieve a better accuracy.},
keywords={Social networking (online);Engines;Smart cities;History;Internet;Wireless sensor networks;Wireless communication;Big data;location based;point-of-interests;smart city;recommendation},
doi={10.1109/ACCESS.2020.2980982},
ISSN={2169-3536},
month={},}
@ARTICLE{8319426,
author={Luo, Juan and Song, Weiqi and Yin, Luxiu},
journal={IEEE Access},
title={Reliable Virtual Machine Placement Based on Multi-Objective Optimization With Traffic-Aware Algorithm in Industrial Cloud},
year={2018},
volume={6},
number={},
pages={23043-23052},
abstract={In a cloud data center, there is usually a large waste of physical resources and link resources, which leads to increased energy consumption. This paper discusses reducing the loss of link resources from the perspective of connectivity between virtual machines. When the virtual machines of a single user request are concentrated to reduce energy consumption, there will be decreased request reliability. This paper considers a single point of failure to ensure the reliability of the requests. Finally, this paper proposes a multi-objective particle swarm optimization algorithm. It takes the physical resource utilization rate and the link loss rate as the optimization targets and uses service reliability and quality of the tenant as constraint conditions. The simulation results show that the method proposed in this paper reliably satisfies the tenant request, effectively controls the link loss in the data center, and significantly reduces the energy consumption of the data center.},
keywords={Virtual machining;Data centers;Energy consumption;Cloud computing;Production facilities;Resource management;Reliability;Industrial cloud;particle swarm optimization;traffic awareness;virtual machine placement},
doi={10.1109/ACCESS.2018.2816983},
ISSN={2169-3536},
month={},}
@ARTICLE{9494369,
author={Yuan, Qingsheng and Sun, Gang and Liang, Jianming and Leng, Biao},
journal={IEEE Access},
title={Efficient Weakly-Supervised Object Detection With Pseudo Annotations},
year={2021},
volume={9},
number={},
pages={104356-104366},
abstract={Weakly-supervised object detection (WSOD) has attracted lots of attention in recent years. However, there is still a big gap between WSOD and generic object detection. The main barriers to the efficiency of WSOD are the ineffective data augmentations and inaccurate bounding box predictions. Given only image-level annotations, it is hard for WSOD to effectively utilize variant data augmentations and accurately regress the bounding boxes. Although a fully-supervised object detector can be trained using annotations generated from the weakly-supervised object detector, the performance is still severely limited due to the low quality of mined pseudo annotations. This paper proposes an efficient WSOD method with pseudo annotations (EWPA) to make better use of imperfect annotations. With the assistance of pseudo annotations, EWPA can effectively regress more accurate bounding boxes while the traditional WSOD can only locate the salient parts of an object. Furthermore, pseudo annotations can help design more complex data augmentations, driving the network to learn more discriminative feature representations. Extensive experiments are conducted on PASCAL VOC 2007 and 2012 datasets and validate the effectiveness of EWPA.},
keywords={Annotations;Proposals;Training;Object detection;Detectors;Feature extraction;Streaming media;Object detection;weakly-supervised learning;data augmentation;mixed-supervision},
doi={10.1109/ACCESS.2021.3099497},
ISSN={2169-3536},
month={},}
@ARTICLE{8194836,
author={Brown, Alexander and Garg, Saurabh and Montgomery, James},
journal={IEEE Access},
title={Automatic and Efficient Denoising of Bioacoustics Recordings Using MMSE STSA},
year={2018},
volume={6},
number={},
pages={5010-5022},
abstract={Automatic recording and analysis of bird calls is becoming an important way to understand changes in bird populations and assess environmental health. An issue currently proving problematic with the automatic analysis of bird recordings is interference from noise that can mask vocalizations of interest. As such, noise reduction can greatly increase the accuracy of automatic analyses and reduce processing work for subsequent steps in bioacoustics analyses. However, only limited work has been done in the context of bird recordings. Most semiautomatic methods either manually apply sound enhancement methods available in audio processing systems such as SoX and Audacity or apply preliminary filters such as lowand highpass filters. These methods are insufficient both in terms of how generically they can be applied and their integration with automatic systems that need to process large amounts of data. Some other work applied more sophisticated denoising methods or combinations of different methods such as minimum mean square error short-time spectral amplitude estimator (MMSE STSA) and spectral subtraction for other species such as anurans. However, their effectiveness is not tested on bird recordings. In this paper, we analyze the applicability of the MMSE STSA algorithm to remove noise from environmental recordings containing bird sounds, particularly focusing on its quality and processing time. The experimental evaluation using real data clearly shows that MMSE STSA can reduce noise with similar effectiveness [using objective metrics such as predicted signal quality (SIG)] to a previously recommended wavelet-transform-based denoising technique while executing between approximately 5-300 times faster depending on the audio files tested.},
keywords={Birds;Noise reduction;Noise measurement;Wavelet transforms;Biomedical acoustics;Signal to noise ratio;Noise removal;bioacoustics;big data},
doi={10.1109/ACCESS.2017.2782778},
ISSN={2169-3536},
month={},}
@ARTICLE{9099840,
author={Arkhipov, Dmitri I. and Wu, Di and Wu, Tao and Regan, Amelia C.},
journal={IEEE Access},
title={A Parallel Genetic Algorithm Framework for Transportation Planning and Logistics Management},
year={2020},
volume={8},
number={},
pages={106506-106515},
abstract={Small to medium sized transportation and logistics companies are usually constrained by limited computing and IT professional resources on implementing an efficient parallel metaheuristic algorithm for planning or management solutions. In this paper we extend the standard meta-description for genetic algorithms (GA) with a simple non-trivial parallel implementation. Our parallel GA framework is chiefly concerned with the development of a straightforward way for engineers to modify existing genetic algorithm implementations for real transportation and logistics problems to make use of commonly available hardware resources without completely reworking complex, useful and usable codes. The framework presented at its parallel base is a modification of the primitive parallelization concept, but if implemented as described it may be gradually extended to fit the qualities of any underlying problem better (via the adaptation of the merging and communications functions).We present our framework and computational results for a classical transportation related combinatorial optimization problem - the traveling salesman problem with a standard sequential genetic algorithm implementation. Our empirical analysis shows that this simple extension can lead to considerable solution improvements. We also tested our assumptions that the framework is easily implemented by an engineer not initially familiar with genetic algorithms to implement the framework for another minimum multiprocessor scheduling problem. These case studies verify that our framework is better than primitive parallelization because it gives empirically better results under equitable conditions. It also outperforms fine grained parallelization as it is easier and faster to implement.},
keywords={Genetic algorithms;Transportation;Logistics;Planning;Sociology;Statistics;Companies;Parallel metaheuristics;genetic algorithm;transportation planning;logistics management},
doi={10.1109/ACCESS.2020.2997812},
ISSN={2169-3536},
month={},}
@ARTICLE{8561284,
author={Cheng, Geyao and Guo, Deke and Shi, Jianmai and Qin, Yudong},
journal={IEEE Access},
title={Planning City-Wide Package Distribution Schemes Using Crowdsourced Public Transportation Systems},
year={2019},
volume={7},
number={},
pages={1234-1246},
abstract={Due to the rapid development of online retailers, there is a great demand for package express shipping services, which causes traffic congestion, resource consumption, and environmental pollution (e.g., carbon emission). However, there is still a large amount of under-utilized capacity in the public transportation systems during off-peak hours. In this paper, we investigate the same-day package distribution using crowdsourced public transportation systems (CPTSs). Specifically, given a number of packages and the timetable of available CPTSs trips, we optimize the schemes of delivering the packages using the under-utilized capacity of the CPTS trips, without impacting the quality of passenger experience. To estimate the amount of under-utilized capacity of each trip across any two adjacent stations, we propose the passenger transit model based on the history data. To assign the under-utilized capacity of each trip to the package deliveries, we develop the minimum limitation delivery (MLD) method, which only utilizes the minimum amount of under-utilized capacity of the whole trip to deliver packages. However, the available capacity is not fully utilized at most stations by MLD. Therefore, we further propose the adaptive limitation delivery (ALD) method, which loads as many packages as possible, until the volume of loaded packages reaches the available capacity in theory. The experimental results and theoretical analysis show that both MLD and ALD could distribute packages efficiently. Moreover, given a set of packages, scheduling of ALD only consumes about 67% time compared to the scheduling of MLD, with a little higher risk of impacting passengers.},
keywords={Public transportation;Logistics;Urban areas;Vehicles;Task analysis;Planning;Package distributions;crowdsourced;public transportation systems;quality of passenger experience},
doi={10.1109/ACCESS.2018.2885081},
ISSN={2169-3536},
month={},}
@ARTICLE{8606969,
author={Gan, Zhenhua and Zou, Fumin and Zeng, Nianyin and Xiong, Baoping and Liao, Lyuchao and Li, Han and Luo, Xin and Du, Min},
journal={IEEE Access},
title={Wavelet Denoising Algorithm Based on NDOA Compressed Sensing for Fluorescence Image of Microarray},
year={2019},
volume={7},
number={},
pages={13338-13346},
abstract={A microarray can be easily used for quantitatively analyzing the expression levels of DNA genes. Yet, the noises introduced during the application will greatly affect the accuracy of DNA sequence detection. How to reduce the noise constitutes a challenging problem in microarray analysis. Especially, due to the weak fluorescence response, the image of microarray contains difficulties of the low peak-signal-to-noise ratio (PSNR) and luminance contrast. To solve the problem that the wavelet threshold denoising method has poor effective on low PSNR image, a wavelet denoising approach based on compression sensing (CS) optimized by the neural dynamics optimization algorithm (NDOA) is proposed, which preferably solves the denoising difficulties of noise pollution in the microarray image. Under the condition of Gaussian random observation matrix, the effectiveness of NDOA-optimized wavelet denoising based on CS gets better work than the orthogonal matching pursuit and its improved algorithms. The experimental results indicate that the expected wavelet coefficients of the noiseless image have been reconstructed with higher quality. When the compression sampling rate for microarray image is 0.875, the PSNR of the NDOA-optimized wavelet denoising algorithm based on CS is increased about 9 dB, and the root mean squared error is reduced obviously too, in comparison with the wavelet soft-threshold denoising method. It shows that the NDOA-optimized method improves the performance of the classical wavelet threshold denoising.},
keywords={Noise reduction;Matching pursuit algorithms;Image reconstruction;Wavelet coefficients;Compressed sensing;Compressed sensing;wavelet denoising;DNA microarray;image filtering;NDOA},
doi={10.1109/ACCESS.2019.2891759},
ISSN={2169-3536},
month={},}
@ARTICLE{8360973,
author={Chen, Zong-Gan and Zhan, Zhi-Hui and Lin, Ying and Gong, Yue-Jiao and Gu, Tian-Long and Zhao, Feng and Yuan, Hua-Qiang and Chen, Xiaofeng and Li, Qing and Zhang, Jun},
journal={IEEE Transactions on Cybernetics},
title={Multiobjective Cloud Workflow Scheduling: A Multiple Populations Ant Colony System Approach},
year={2019},
volume={49},
number={8},
pages={2912-2926},
abstract={Cloud workflow scheduling is significantly challenging due to not only the large scale of workflow but also the elasticity and heterogeneity of cloud resources. Moreover, the pricing model of clouds makes the execution time and execution cost two critical issues in the scheduling. This paper models the cloud workflow scheduling as a multiobjective optimization problem that optimizes both execution time and execution cost. A novel multiobjective ant colony system based on a co-evolutionary multiple populations for multiple objectives framework is proposed, which adopts two colonies to deal with these two objectives, respectively. Moreover, the proposed approach incorporates with the following three novel designs to efficiently deal with the multiobjective challenges: 1) a new pheromone update rule based on a set of nondominated solutions from a global archive to guide each colony to search its optimization objective sufficiently; 2) a complementary heuristic strategy to avoid a colony only focusing on its corresponding single optimization objective, cooperating with the pheromone update rule to balance the search of both objectives; and 3) an elite study strategy to improve the solution quality of the global archive to help further approach the global Pareto front. Experimental simulations are conducted on five types of real-world scientific workflows and consider the properties of Amazon EC2 cloud platform. The experimental results show that the proposed algorithm performs better than both some state-of-the-art multiobjective optimization approaches and the constrained optimization approaches.},
keywords={Cloud computing;Task analysis;Optimization;Processor scheduling;Scheduling;Computational modeling;Search problems;Cloud computing;evolutionary approach;multiobjective optimization;workflow scheduling},
doi={10.1109/TCYB.2018.2832640},
ISSN={2168-2275},
month={Aug},}
@ARTICLE{9136648,
author={Elaziz, Mohamed Abd and Ewees, Ahmed A. and Yousri, Dalia and Alwerfali, Husein S. Naji and Awad, Qamar A. and Lu, Songfeng and Al-Qaness, Mohammed A. A.},
journal={IEEE Access},
title={An Improved Marine Predators Algorithm With Fuzzy Entropy for Multi-Level Thresholding: Real World Example of COVID-19 CT Image Segmentation},
year={2020},
volume={8},
number={},
pages={125306-125330},
abstract={Medical imaging techniques play a critical role in diagnosing diseases and patient healthcare. They help in treatment, diagnosis, and early detection. Image segmentation is one of the most important steps in processing medical images, and it has been widely used in many applications. Multi-level thresholding (MLT) is considered as one of the simplest and most effective image segmentation techniques. Traditional approaches apply histogram methods; however, these methods face some challenges. In recent years, swarm intelligence methods have been leveraged in MLT, which is considered an NP-hard problem. One of the main drawbacks of the SI methods is when searching for optimum solutions, and some may get stuck in local optima. This because during the run of SI methods, they create random sequences among different operators. In this study, we propose a hybrid SI based approach that combines the features of two SI methods, marine predators algorithm (MPA) and moth-?ame optimization (MFO). The proposed approach is called MPAMFO, in which, the MFO is utilized as a local search method for MPA to avoid trapping at local optima. The MPAMFO is proposed as an MLT approach for image segmentation, which showed excellent performance in all experiments. To test the performance of MPAMFO, two experiments were carried out. The first one is to segment ten natural gray-scale images. The second experiment tested the MPAMFO for a real-world application, such as CT images of COVID-19. Therefore, thirteen CT images were used to test the performance of MPAMFO. Furthermore, extensive comparisons with several SI methods have been implemented to examine the quality and the performance of the MPAMFO. Overall experimental results confirm that the MPAMFO is an efficient MLT approach that approved its superiority over other existing methods.},
keywords={Image segmentation;COVID-19;Optimization methods;Computed tomography;Biomedical imaging;Entropy;Image segmentation;multi-level thresholding;moth-?ame optimization (MFO);marine predators algorithm (MPA);COVID-19;swarm intelligence},
doi={10.1109/ACCESS.2020.3007928},
ISSN={2169-3536},
month={},}
@ARTICLE{8332926,
author={An, Ying and Luo, Xi},
journal={IEEE Access},
title={An In-Network Caching Scheme Based on Energy Efficiency for Content-Centric Networks},
year={2018},
volume={6},
number={},
pages={20184-20194},
abstract={Content-centric networking (CCN) has emerged as a promising architecture for future Internet due to its in-network caching capability and the receiver-driven content retrieval paradigm. Recently, the growing energy consumption driven by explosive increase of network traffic has become a key issue in CCN and caused widespread academic concern. In this paper, we construct a model to analyze the energy consumption of content distribution in CCN, and propose an energy efficiency based in-network caching scheme. In this scheme, a judging condition is designed to reduce the total energy consumption of content dissemination, and then in combination with content popularity and node importance, a cache placement strategy is proposed to optimize the selection of caching nodes. Furthermore, a neighbor cooperation-based cache replacement strategy is also proposed, which uses the cache resource of neighbor nodes to increase the chances of content being cached and improve the quality of caching service and resource utilization. Simulation results demonstrate that our scheme can outperform the existing schemes in terms of the high cache hit rate, the low average response hops, and the low whole energy consumption.},
keywords={Energy consumption;Internet;Resource management;Routing;Servers;Optimization;Cooperative caching;Content-centric networking;energy efficiency;in-network caching;neighbor cooperation},
doi={10.1109/ACCESS.2018.2823722},
ISSN={2169-3536},
month={},}
@ARTICLE{8695003,
author={Ma, Fei and Jing, Xiao-Yuan and Yao, Yongfang and Zhu, Xiaoke and Peng, Zhiping},
journal={IEEE Access},
title={High-Resolution and Low-Resolution Video Person Re-Identification: A Benchmark},
year={2019},
volume={7},
number={},
pages={63426-63436},
abstract={Person re-identification has recently attracted increasing interest in the computer vision and safety-critical applications. In practice, due to poor quality of cameras or long distance away from person, the captured pedestrian videos usually suffer from low resolution, which will result in the loss of useful information contained in videos and make person re-identification between low-resolution (LR) and high-resolution (HR) videos (PRLHV) be a challenging task. However, the problem of PRLHV has not been well studied. In this paper, we propose a semi-coupled mapping based set-to-set distance learning (SMDL) approach for PRLHV. Specifically, by regarding each video as a set of features extracted from several walking cycles, we learn a discriminative set-to-set distance metric to enhance the separability between videos from different persons. To decrease the influence of low resolution on the distance learning, we design a clustering-based semi-coupled mapping term for our approach, which can reduce the variation between features of low-resolution and high-resolution videos by a semi-coupled mapping matrix. Since there exists no low-resolution video pedestrian re-identification dataset under real-world scenario up to now, we contribute a benchmark dataset for PRLHV, named high-resolution and low-resolution video person re-identification dataset (HLVID). Although this dataset is challenging and difficult for person re-identification, it is a useful attempt for further studies on low-resolution video-based pedestrian re-identification under the real-world scene. The extensive experiments on the newly collected video dataset demonstrate that our approach performs better than the state-of-the-art person re-identification methods in the PRLHV task.},
keywords={Feature extraction;Cameras;Image resolution;Computer aided instruction;Measurement;Benchmark testing;Task analysis;Distance learning;low-resolution video-based pedestrian dataset;semi-coupled mapping;person re-identification},
doi={10.1109/ACCESS.2019.2912302},
ISSN={2169-3536},
month={},}
@ARTICLE{9745616,
author={Xu, Lina and Wang, Ziyang and Chen, Xudong and Lin, Zhengwei},
journal={IEEE Access},
title={Multi-Parking Lot and Shelter Heterogeneous Vehicle Routing Problem With Split Pickup Under Emergencies},
year={2022},
volume={10},
number={},
pages={36073-36090},
abstract={The vehicle rescue process for individuals in residential areas in disaster scenarios is a typical vehicle routing problem (VRP). However, most studies do not consider the factor of individual mobility. In residential areas, there are two types of individuals: individuals with high mobility and individuals with low mobility, such as the elderly. To improve the evacuation efficiency, besides ordinary vehicles, special vehicles equipped with wheelchairs and volunteers are also in great need. Thus, evacuation vehicles should consist of a heterogeneous fleet. Vehicles depart from parking lots, arrive at residential areas to pick up individuals, and then transport them to shelters. In other words, the origin and destination are different, but they are viewed as the same in classical VRP. Each residential area can be served directly by vehicles departing from parking lots or by vehicles that have already served others, which means demands can be split. All these make the VRP in emergency rescue more complicated than classical VRP. Therefore, we propose an integer liner program model – multi-parking lot and shelter heterogeneous vehicle routing problem with split pickup (MPSHVRPSP) model, which includes matching constraints of individuals and vehicles to satisfy the demands of different types of individuals, and considers the selectivity of parking lots and shelters too. We provide a Tabu Search (TS) algorithm with diversification strategy to solve the model and ensure the high quality of solution. A lot of experiments are carried out on various instances. Our results show that MPSHVRPSP can be applied to efficient evacuation of complicated scenarios that satisfies the demands of all individuals in residential areas. Besides, it is more reasonable compared with classical VRP, and TS can also obtain a satisfactory solution in less time. Furthermore, sensitivity analysis is conducted on factors that may affect the result of objective function.},
keywords={Vehicle routing;Costs;Planning;Load modeling;Wheelchairs;Public transportation;Logistics;Vehicle routing problem;multi-parking lot and shelter;heterogeneous fleet;split pickup;Tabu Search},
doi={10.1109/ACCESS.2022.3163715},
ISSN={2169-3536},
month={},}
@ARTICLE{9475541,
author={Duan, Junwei and Mao, Shuqi and Jin, Junwei and Zhou, Zhiguo and Chen, Long and Chen, C. L. Philip},
journal={IEEE Access},
title={A Novel GA-Based Optimized Approach for Regional Multimodal Medical Image Fusion With Superpixel Segmentation},
year={2021},
volume={9},
number={},
pages={96353-96366},
abstract={For multimodal medical image fusion problems, most of the existing fusion approaches are based on pixel-level. However, the pixel-based fusion method tends to lose local and spatial information as the relationships between pixels are not considered appropriately, which has much influence on the quality of the fusion results. To address this issue, a region-based multimodal medical image fusion framework is proposed based on superpixel segmentation and a post-processing optimization method in this paper. In this framework, the average image of the source medical images is firstly obtained by a weighted averaging method. To effectively obtain homogeneous regions and preserve the complete information of image details, the fast linear spectral clustering(LSC) superpixel algorithm is carried out to segment the average image and get superpixel labels. For each region of the medical images, log-gabor filter(LGF) and sum modified laplacian(SML) are adopted to extract texture feature and contrast feature for the measurement of region importance. The most important regions are selected and the decision map is generated by comparison. Moreover, to get a more accurate decision map, a new post-processing optimized method based on genetic algorithm(GA) is given. A weighted strategy is applied to the extracted features and the weighting factor can be adaptively adjusted by GA. The effectiveness of the proposed fusion method is validated by conducting experiments on eight pairs of medical images from diverse modalities. In addition, seven other mainstream medical image fusion methods are adopted for comparing the performance of fusion. Experimental results in terms of qualitative and quantitative evaluation demonstrate that the proposed method can achieve state-of-the-art performance for multimodal medical image fusion problems.},
keywords={Image fusion;Image segmentation;Feature extraction;Medical diagnostic imaging;Transforms;Clustering algorithms;Genetic algorithms;Multimodal medical image fusion;superpixel segmentation;genetic algorithm;log-gabor filter;sum modified laplacian},
doi={10.1109/ACCESS.2021.3094972},
ISSN={2169-3536},
month={},}
@ARTICLE{8918464,
author={Liu, Pengfei},
journal={IEEE Access},
title={Joint Spectral and Spatial Consistency Priors for Variational Pansharpening},
year={2019},
volume={7},
number={},
pages={174847-174858},
abstract={This paper proposes a new variational pansharpening model with joint spectral and spatial consistency priors, which aims to fuse a low resolution (LR) multispectral (MS) image and a high resolution (HR) panchromatic (Pan) image to produce a pan-sharpened HR MS image. Specifically, the proposed model combines three consistency terms into a unified variational framework, which are (1) Local spectral consistency fidelity term, which enforces the degradation relation-based local spectral consistency constraint between the HR MS and LR MS images; (2) Hessian feature-enforced spatial consistency prior term, which particularly models the Hessian feature consistency constraint between the HR MS and Pan images to enforce spatial consistency; and (3) Wavelet-based spectral-spatial consistency prior term, which models the consistency between the HR MS image and the constructed Wavelet-based matching image to enforce spectral-spatial consistency. Moreover, the proposed model is efficiently solved by designing an optimization algorithm under the forward-backward splitting framework. Finally, experiments on the QuickBird, Pleiades and GeoEye-1 satellite datasets systematically illustrate that the proposed method performs better spectral and spatial qualities than various compared methods.},
keywords={Image edge detection;Spatial resolution;Multiresolution analysis;Degradation;Optimization;Pansharpening;variational model;spectral and spatial consistency priors},
doi={10.1109/ACCESS.2019.2957214},
ISSN={2169-3536},
month={},}
@ARTICLE{9037316,
author={Xu, Shuting and Zhang, Zhe},
journal={IEEE Access},
title={Mechanism Analysis of Smart Cue on Aircraft for Loss of Control Mitigation},
year={2020},
volume={8},
number={},
pages={58522-58532},
abstract={This paper analyzes the mechanism of the smart inceptor on the aircraft, as a means to mitigate human-vehicle system loss-of-control. We divide the smart inceptor cue into three modes: the smart cue on the human pilot, the smart cue on the flight control system and the smart cue on both of them. The control mechanism of these three modes is developed and analyzed in depth. To evaluate the effect of the three modes, we utilize an intelligent human pilot model to establish the human-vehicle system with the smart inceptor and a scalogram-based pilot induced oscillation metric to predict the handling qualities of the three modes. This paper presents details of the cueing modes and the results of the prediction focused on effectiveness of these modes in preventing the pilots from entering a loss-of-control event. The simulation results indicate that the smart cue on both of them was the most effective method to mitigate the impact of the pilot-aircraft system oscillations for the given failure scenarios. It embodies the function of pilot-aircraft cooperation.},
keywords={Aircraft;Aerospace control;Atmospheric modeling;Control systems;Adaptation models;Force;Oscillators;Flight simulation;human???vehicle system;human pilot model;interface;manual control},
doi={10.1109/ACCESS.2020.2981047},
ISSN={2169-3536},
month={},}
@ARTICLE{9693896,
author={Ibrahim, N. K. and Sali, A. and Karim, H. A. and Ramli, A. F. and Ibrahim, N. S. and Grace, D.},
journal={IEEE Access},
title={Multiple Description Coding for Enhancing Outage and Video Performance Over Relay-Assisted Cognitive Radio Networks},
year={2022},
volume={10},
number={},
pages={11750-11762},
abstract={Multimedia content delivery, such as video transmission over wireless networks, imposes significant challenges include spectrum capacity and packet losses. The cognitive radio (CR) technology is developed to solve the spectrum issue, while multiple description coding (MDC) is one of the promising source coding techniques to alleviate packet loss problems and exploit the benefit of path diversity. The source information was split into several descriptions in MDC, then transmitted over a network with multiple paths. The quality of the received data increases with the number of descriptions received at the receiver. In this paper, the proposed system comprises of relay-assisted cognitive radio network using the MDC technique for video transmission. In the simulations, the outage performance of the MDC scheme over two networks, which were relay-assisted network and non-relay network, were compared. Then, the outage probability was used to estimate the video quality, peak signal to noise ratio (PSNR) of the received video. The results obtained show the benefits of the relay assisted networks by 9% improvement on average outage performance over the non-relay network. Furthermore, the video performance improved by an average of 9% in PSNR compared to the non-relay system.},
keywords={Streaming media;Probability;Power system reliability;Packet loss;Relay networks (telecommunication);Source coding;Cognitive radio;Cognitive radio;cognitive radio networks;H264/AVC;multiple description coding (MDC);outage performance;interweave;peak signal to noise ratio (PSNR);relay},
doi={10.1109/ACCESS.2022.3146396},
ISSN={2169-3536},
month={},}
@ARTICLE{9625982,
author={Wang, Chunzhi and Li, Xing and Wang, Zaoning},
journal={IEEE Access},
title={Bilevel Multi-Objective Gray Wolf Algorithm Based on Packet Transport Network Optimization},
year={2021},
volume={9},
number={},
pages={162792-162804},
abstract={Packet transport network (PTN), as an efficient transmission network technology in mobile communications in the big data era, is used by more and more communication operators. The existing PTN resource utilization rate is low, the network security is poor, so the existing PTN needs to be optimized in all aspects. For the optimization of the PTN, it is necessary to consider the decision of both the operator user and the service product supplier. Therefore, this paper proposes a bilevel multi-objective gray wolf algorithm based on PTN optimization problem. The operator user is the upper-level decision maker, and the objective function is to pay the product supplier the lowest cost. The product supplier is the lower-level decision maker, it mainly includes two major objective functions. The first objective function is to maximize the Label switching path overlap rate(LSPOR) evaluation score to solve the abnormal Label Switching Path (LSP) problem in the network, and the second is to maximize the committed bandwidth with utilizing rate(CBWUR) evaluation score to solve the problem of excessive Committed Information Rate(CIR) bandwidth usage in the network. According to the three scale network situation in Hubei, China, the improved multi-objective gray wolf algorithm is used to solve the PTN bilevel programming problem. The experimental results show that compared with the initial network, the optimized network size dropped by 125314 hops on average, the LSPOR increased by 13.64%, and the CBWUR increased by 3.7%. This model not only improves the utilization of network resources, but also reduces the cost to be paid by superior decision makers.},
keywords={Optimization;Linear programming;Optical fibers;Heuristic algorithms;Bandwidth;Task analysis;Packet transport network;bilevel programming;multi-objective gray wolf algorithm;label switching path;committed information rate},
doi={10.1109/ACCESS.2021.3130280},
ISSN={2169-3536},
month={},}
@ARTICLE{9171438,
author={Jiang, Xin and Zhang, Xinchang and Xin, Qinchuan and Xi, Xu and Zhang, Pengcheng},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Arbitrary-Shaped Building Boundary-Aware Detection With Pixel Aggregation Network},
year={2021},
volume={14},
number={},
pages={2699-2710},
abstract={Large-scale building extraction is an essential work in the field of a remote sensing image analysis. The high-resolution image extraction methods based on deep learning have achieved state-of-the-art performance. However, most of the previous work has focused on region accuracy rather than boundary quality. Aiming at the low-accuracy problems and incomplete boundary of the building extraction method, we propose a predictive optimization architecture, BAPANet. Notably, the architecture consists of an encoder–decoder network, and residual refinement modules responsible for prediction, and refinement. The objective function optimizes the network in the form of three levels (pixel, feature map, and patch) by fusing three loss functions: binary cross-entropy, intersection over-union, and structural similarity. The five public datasets’ experimental results show that the extraction method in this article has high region accuracy, and the boundary of buildings is clear and complete.},
keywords={Feature extraction;Buildings;Image segmentation;Semantics;Remote sensing;Optimization;Image edge detection;Boundary quality;building extraction;high resolution;structural similarity (SSIM)},
doi={10.1109/JSTARS.2020.3017934},
ISSN={2151-1535},
month={},}
@ARTICLE{9475964,
author={Al-Andoli, Mohammed Nasser and Tan, Shing Chiang and Cheah, Wooi Ping and Tan, Sin Yin},
journal={IEEE Access},
title={A Review on Community Detection in Large Complex Networks from Conventional to Deep Learning Methods: A Call for the Use of Parallel Meta-Heuristic Algorithms},
year={2021},
volume={9},
number={},
pages={96501-96527},
abstract={Complex networks (CNs) have gained much attention in recent years due to their importance and popularity. The rapid growth in the size of CNs leads to more difficulties in the analysis of CNs tasks. Community Detection (CD) is an important multidisciplinary research area where many machine/deep learning-based methods have been applied to map CNs into a low-dimensional representation for extracting information similarity among members of CNs. Currently, Deep Learning (DL) is one of the promising methods to extract knowledge and learn information from high dimensional space and represent it in low dimensional space. However, designing an accurate and efficient DL-based CD method especially when dealing with large CNs is always an on-going research endeavor to pursue. Meta-Heuristic (MH) algorithms have shown their potentials in improving DL models in terms of solution quality and computational cost. In addition, parallel computing is a feasible solution for building efficient DL models. The algorithmic principle of MH is parallel in nature; however, its computation framework in DL training that is reported in the literature is not really implemented in a parallel computing setup. In this paper, we present a systematic review of CD in CNs from conventional machine learning to DL methods and point out the gap of applying DL-based CD methods in large CNs. In addition, the relevant studies on DL with parallel and MH approaches are reviewed and their implications on DL models are highlighted to prospect effective solutions to overcome the challenges of DL-based CD methods. We also point out research challenges in the field of CD and suggest possible future research directions.},
keywords={Deep learning;Computational modeling;Complex networks;Social networking (online);Optimization;Big Data;Terminology;Community detection;deep learning;complex networks;meta-heuristic algorithms;parallel computing},
doi={10.1109/ACCESS.2021.3095335},
ISSN={2169-3536},
month={},}
@ARTICLE{7349932,
author={Zhang, Yanping and Jing, Zihui and Zhang, Yiwen},
journal={Tsinghua Science and Technology},
title={MR-IDPSO: a novel algorithm for large-scale dynamic service composition},
year={2015},
volume={20},
number={6},
pages={602-612},
abstract={In the era of big data, data intensive applications have posed new challenges to the field of service composition. How to select the optimal composited service from thousands of functionally equivalent services but different Quality of Service (QoS ) attributes has become a hot research in service computing. As a consequence, in this paper, we propose a novel algorithm MR-IDPSO (MapReduce based on Improved Discrete Particle Swarm Optimization), which makes use of the improved discrete Particle Swarm Optimization (PSO) with the MapReduce to solve large-scale dynamic service composition. Experiments show that our algorithm outperforms the parallel genetic algorithm in terms of solution quality and is efficient for large-scale dynamic service composition. In addition, the experimental results also demonstrate that the performance of MR-IDPSO becomes more better with increasing number of candidate services.},
keywords={Quality of service;Heuristic algorithms;Signal processing algorithms;Particle swarm optimization;Cloud computing;Genetic algorithms;Programming;MapReduce; service composition; Quality of Service (QoS); parallel particle swarm optimization},
doi={10.1109/TST.2015.7349932},
ISSN={1007-0214},
month={December},}
@ARTICLE{9316706,
author={Tahir, Bilal and Mehmood, Muhammad Amir},
journal={IEEE Access},
title={Corpulyzer: A Novel Framework for Building Low Resource Language Corpora},
year={2021},
volume={9},
number={},
pages={8546-8563},
abstract={The rapid proliferation of artificial intelligence has led to the development of sophisticated cutting-edge systems in natural language processing and computational linguistics domains. These systems heavily rely on high-quality dataset/corpora for the training of deep-learning algorithms to develop precise models. The preparation of a high-quality gold standard corpus for natural language processing on a large scale is a challenging task due to the need of huge computational resources, accurate language identification models, and precise content parsing tools. This task is further exacerbated in case of regional languages due to the scarcity of web content. In this article, we propose a generic framework of Corpus Analyzer - Corpulyzer - a novel framework for building low resource language corpora. Our framework consists of corpus generation and corpus analyzer module. We demonstrate the efficacy of our framework by creating a high-quality large scale corpus for the Urdu language as a case study. Leveraging dataset from Common Crawl Corpus (CCC), first, we prepare a list of seed URLs by filtering the Urdu language webpages. Next, we use Corpulyzer to crawl the World-Wide-Web (WWW) over a period of four years (2016-2020). We build Urdu web corpus “UrduWeb20” that consists of 8.0 million Urdu webpages crawled from 6,590 websites. In addition, we propose Low-Resource Language (LRL) website scoring algorithm and content-size filter for language-focused crawling to achieve optimal use of computational resources. Moreover, we analyze UrduWeb20 using variety of traditional metrics such as web-traffic-rank, URL depth, duplicate documents, and vocabulary distribution along with our newly defined content-richness metrics. Furthermore, we compare different characteristics of our corpus with three datasets of CCC. In general, we observe that contrary to CCC that focuses on crawling the limited number of webpages from highly ranked Urdu websites, Corpulyzer performs an in-depth crawling of Urdu content-rich websites. Finally, we made available Corpulyzer framework for the research community for corpus building.},
keywords={Buildings;Tools;Task analysis;Social networking (online);Crawlers;Computational modeling;Vocabulary;Common crawl;web crawling;text corpus;corpus analysis;regional languages corpora},
doi={10.1109/ACCESS.2021.3049793},
ISSN={2169-3536},
month={},}
@ARTICLE{9095322,
author={Lan, Tian and Ye, Wenzheng and Lyu, Yilan and Zhang, Junyi and Liu, Qiao},
journal={IEEE Access},
title={Embedding Encoder-Decoder With Attention Mechanism for Monaural Speech Enhancement},
year={2020},
volume={8},
number={},
pages={96677-96685},
abstract={The auditory selection framework with attention and memory (ASAM), which has an attention mechanism, embedding generator, generated embedding array, and life-long memory, is used to deal with mixed speech. When ASAM is applied to speech enhancement, the discrepancy between the voice and noise feature memories is huge and the separability of noise and voice is increased. However, ASAM cannot achieve desirable performance in terms of speech enhancement because it fails to utilize the time-frequency dependence of the embedding vectors to generate a corresponding mask unit. This work proposes a novel embedding encoder-decoder (EED), and a convolutional neural network (CNN) is used as decoder. The CNN structure is good at detecting local patterns, which can be exploited to extract correlation embedding data from the embedding array to generate the target spectrogram. This work evaluates a similar ASAM, EED with an LSTM encoder and a CNN decoder (RC-EED), RC-EED with an attention mechanism (RC-AEED), other similar EED structures and baseline models. Experiment results show that RC-EED and RC-AEED networks have good performance on speech enhancement task at low signal-to-noise ratio conditions. In addition, RC-AEED exhibits superior speech enhancement performance over ASAM and achieves better speech quality than do deep recurrent network and convolutional recurrent network.},
keywords={Speech enhancement;Decoding;Noise measurement;Spectrogram;Signal to noise ratio;Feature extraction;Convolution;Speech enhancement;embedding encoder-decoder;convolutional neural network;attention mechanism;neural network},
doi={10.1109/ACCESS.2020.2995346},
ISSN={2169-3536},
month={},}
@ARTICLE{9206581,
author={Melo, Aurelio G. and Pinto, Milena F. and Honório, Leonardo M. and Dias, Felipe M. and Masson, Juliano E. N.},
journal={IEEE Access},
title={3D Correspondence and Point Projection Method for Structures Deformation Analysis},
year={2020},
volume={8},
number={},
pages={177823-177836},
abstract={Mining slopes, electrical power generation dams and several big construction enterprises demands continuous inspections. The size and diversity of these structures demands high precision and portable approach. In such environments, 3D reconstruction methodologies are able to capture and analyze the real world in detail. However, the accuracy and precision can affect the ability to process and interpret the acquired data. For instance, laser scanning is a very accurate method and can deliver a higher quality result. Meanwhile, 3D photogrammetry using a single camera and Structure From Motion (SFM) have their performance correlated with the image quality. In a typical application, 3D data from reconstruction is pre-processed by a specialist. Then, it is stored for comparison and analyzed over time. The posterior analysis has several challenges associated with the reconstruction process characteristics. Several techniques have been developed to allow the comparison of point cloud captured at different epochs. Therefore, this research work presents a new methodology to perform alignment and comparison of point clouds, namely 3D-CP2, an acronym for 3D Correspondence and Point Projection. This method intends to analyze the point cloud motion to be applied in terrestrial 3D SFM reconstructions. Besides, the technique can also be used in many other related applications. The methodology developed in this work is applied in controlled experiments and real use cases to show its potential for point cloud displacements analysis. The results showed that the proposed method is efficient and can produce results more accurately than the referenced literature.},
keywords={Three-dimensional displays;Strain;Inspection;Surface reconstruction;Surface treatment;Cameras;Image reconstruction;Aerial inspection;point cloud change detection;structural analysis;structure from motion;3D reconstruction},
doi={10.1109/ACCESS.2020.3027205},
ISSN={2169-3536},
month={},}
@ARTICLE{9433543,
author={Budhiraja, Ishan and Kumar, Neeraj and Tyagi, Sudhanshu and Tanwar, Sudeep and Han, Zhu and Piran, Md. Jalil and Suh, Doug Young},
journal={IEEE Access},
title={A Systematic Review on NOMA Variants for 5G and Beyond},
year={2021},
volume={9},
number={},
pages={85573-85644},
abstract={Over the last few years, interference has been a major hurdle for successfully implementing various end-user applications in the fifth-generation (5G) of wireless networks. During this era, several communication protocols and standards have been developed and used by the community. However, interference persists, keeping given quality of service (QoS) provision to end-users for different 5G applications. To mitigate the issues mentioned above, in this paper, we present an in-depth survey of state-of-the-art non-orthogonal multiple access (NOMA) variants having power and code domains as the backbone for interference mitigation, resource allocations, and QoS management in the 5G environment. These are future smart communication and supported by device-to-device (D2D), cooperative communication (CC), multiple-input and multiple-output (MIMO), and heterogeneous networks (HetNets). From the existing literature, it has been observed that NOMA can resolve most of the issues in the existing proposals to provide contention-based grant-free transmissions between different devices. The key differences between the orthogonal multiple access (OMA) and NOMA in 5G are also discussed in detail. Moreover, several open issues and research challenges of NOMA-based applications are analyzed. Finally, a comparative analysis of different existing proposals is also discussed to provide deep insights to the readers.},
keywords={NOMA;5G mobile communication;Cooperative communication;Wireless networks;Quality of service;Interference;Device-to-device communication;NOMA;OMA;uplink;downlink;device-to-device;machine-to-machine},
doi={10.1109/ACCESS.2021.3081601},
ISSN={2169-3536},
month={},}
@ARTICLE{8976074,
author={Yan, Tao and Ra, In-Ho and Wen, Hui and Weng, Min-Hang and Zhang, Qian and Che, Yan},
journal={IEEE Access},
title={CTU Layer Rate Control Algorithm in Scene Change Video for Free-Viewpoint Video},
year={2020},
volume={8},
number={},
pages={24549-24560},
abstract={At present, the rate control algorithm for multiview high-efficiency video coding (MV-HEVC) does not have the capability of efficient coding tree unit(CTU) layer bit allocation, and the video quality varies greatly for sequences with sudden scene changes or large motions. To overcome this limitation, this paper proposes a rate control algorithm for MV-HEVC based on scene detection. Firstly, we established ρ domain rate control model based on multi-objective optimization. Then, it uses image similarity to make reasonable bit allocation among viewpoints. If the video scene is switched, the image similarity is recalculated, and then the correlation between the weights of the interview point rates and the correlation between the viewpoints are analyzed. Finally, the frame layer rate control considers the layer B-frame and other factors in allocating the code rate, and the basic unit layer rate control adopts different quantization methods according to the content complexity of the CTU. Experimental results show that the proposed rate control algorithm can maintain good coding efficiency and decrease the average video quality variation by 25.29%.},
keywords={Lenses;Video coding;Encoding;Three-dimensional displays;Bit rate;Correlation;Semantics;3D video coding;scene detection;rate control;image similarity analysis;bit allocation},
doi={10.1109/ACCESS.2020.2970063},
ISSN={2169-3536},
month={},}
@ARTICLE{8963698,
author={Feng, Chong and Khan, Muzammil and Rahman, Arif Ur and Ahmad, Arshad},
journal={IEEE Access},
title={News Recommendation Systems - Accomplishments, Challenges & Future Directions},
year={2020},
volume={8},
number={},
pages={16702-16725},
abstract={News publishers have decreased disseminating news through conventional newspapers and have migrated to the use of digital means like websites and purpose-built mobile applications. It is observed that news recommendation systems can automatically process lengthy articles and identify similar articles for readers considering predefined criteria. The objectives of the current work are to identify and classify the challenges in news recommendation domain, to identify state-of-the-art approaches and classify on the application domain, to identify datasets used for evaluation and their sources, the evaluation approaches used and to highlight the challenges explicitly addressed. The literature is thoroughly studied over the time span of 2001-2019 and shortlisted 81 related studies, broadly classified into six categories and discussed. The analysis showed that 60% of news recommendation system adopted a hybrid approach, 66% studies little talk about datasets, and addresses a few challenges from a long list of challenges in the news domain. This article is the first in the field to draw a comprehensive big picture of news recommendation and explore different dimensions covered in the studies. The last section presents the future research opportunities that lead to improving the recommendation of news articles in the news domain.},
keywords={Object recognition;Bibliographies;Data collection;Quality assessment;Collaboration;Computer science;Recommendations;news recommendations;literature review;recommendation techniques;news recommendation overview},
doi={10.1109/ACCESS.2020.2967792},
ISSN={2169-3536},
month={},}
@ARTICLE{8492398,
author={Liu, Fagui and Deng, Dacheng and Jiang, Jun and Tang, Quan},
journal={IEEE Access},
title={Event-Driven Semantic Service Discovery Based on Word Embeddings},
year={2018},
volume={6},
number={},
pages={61030-61038},
abstract={Service discovery is vital to event handling in Internet of Things applications which are based on the event-driven service-oriented architecture. However, in service discovery, the problem of service matching that establishes relationships between services and events has been seldom investigated through a semantic way. In this paper, to facilitate the efficiency of service discovery triggered by events, we propose a novel method of semantic service matching based on word embeddings. In this method, two types of semantic services about events (i.e., event-recognition services and event-handing services) are specified and matched through semantic similarity assessment that is conducted with word embeddings. Besides, to obtain highquality word embeddings, we present a hybrid approach for learning word embedding which treats words in distinct means according to word frequency. Experiments demonstrated on different data sets show that our method of semantic service matching is an effective way to facilitate event-driven service discovery, and the proposed training approach for word embeddings outperforms existing works and is able to improve the accuracy of event-driven service discovery.},
keywords={Semantics;Training;Ontologies;Knowledge based systems;Internet of Things;Service-oriented architecture;Event-driven service discovery;service matching;semantic similarity assessment;word embedding},
doi={10.1109/ACCESS.2018.2876029},
ISSN={2169-3536},
month={},}
@ARTICLE{8558534,
author={Xu, Hansong and Yu, Wei and Griffith, David and Golmie, Nada},
journal={IEEE Access},
title={A Survey on Industrial Internet of Things: A Cyber-Physical Systems Perspective},
year={2018},
volume={6},
number={},
pages={78238-78259},
abstract={The vision of Industry 4.0, otherwise known as the fourth industrial revolution, is the integration of massively deployed smart computing and network technologies in industrial production and manufacturing settings for the purposes of automation, reliability, and control, implicating the development of an Industrial Internet of Things (I-IoT). Specifically, I-IoT is devoted to adopting the IoT to enable the interconnection of anything, anywhere, and at any time in the manufacturing system context to improve the productivity, efficiency, safety, and intelligence. As an emerging technology, I-IoT has distinct properties and requirements that distinguish it from consumer IoT, including the unique types of smart devices incorporated, network technologies and quality-of-service requirements, and strict needs of command and control. To more clearly understand the complexities of I-IoT and its distinct needs and to present a unified assessment of the technology from a systems’ perspective, in this paper, we comprehensively survey the body of existing research on I-IoT. Particularly, we first present the I-IoT architecture, I-IoT applications (i.e., factory automation and process automation), and their characteristics. We then consider existing research efforts from the three key system aspects of control, networking, and computing. Regarding control, we first categorize industrial control systems and then present recent and relevant research efforts. Next, considering networking, we propose a three-dimensional framework to explore the existing research space and investigate the adoption of some representative networking technologies, including 5G, machine-to-machine communication, and software-defined networking. Similarly, concerning computing, we again propose a second three-dimensional framework that explores the problem space of computing in I-IoT and investigate the cloud, edge, and hybrid cloud and edge computing platforms. Finally, we outline particular challenges and future research needs in control, networking, and computing systems, as well as for the adoption of machine learning in an I-IoT context.},
keywords={Manufacturing;Automation;Machine learning;Sensors;Production;Control systems;Cloud computing;Industrial Internet of Things;industrial cyber physical systems;application and service;control;networking;computing;machine learning;big data analytics;survey;future research directions},
doi={10.1109/ACCESS.2018.2884906},
ISSN={2169-3536},
month={},}
@ARTICLE{8755846,
author={Eseye, Abinet Tesfaye and Lehtonen, Matti and Tukia, Toni and Uimonen, Semen and Millar, R. John},
journal={IEEE Access},
title={Adaptive Predictor Subset Selection Strategy for Enhanced Forecasting of Distributed PV Power Generation},
year={2019},
volume={7},
number={},
pages={90652-90665},
abstract={Distributed photovoltaic (PV) solar power plants are playing an increasing role as a power generation resource in the modern electricity grid. However, PVs pose significant challenges to grid planners, operators, owners, investors, aggregators, and other stakeholders. This is due to the high uncertainty of the PV output power, which is caused by its entire dependence on intermittent environmental factors. This has brought a serious problem to the power industry to integrate and manage power grids containing significant penetration of PVs. Thus, an enhanced PV power forecast is very important to operate these power grids efficiently and reliably. Most previous methodologies have focused on predicting the aggregate amount of potential solar power generation at the national or regional scale and ignored the distributed PVs that are installed primarily for local electric supply. Furthermore, a few research groups have carried out predictor selection before training predictive models. This paper proposes an adaptive hybrid predictor subset selection (PSS) strategy to obtain the most relevant and nonredundant predictors for enhanced short-term forecasting of the power output of distributed PVs. In the proposed strategy, the binary genetic algorithm (BGA) is applied for the feature selection process and support vector regression (SVR) is used for measuring the fitness score of the predictors. In order to validate the effectiveness of the proposed strategy, it is applied to actual distributed PVs located in the Otaniemi area of Espoo, Finland. The findings are compared with those achieved by other PSS techniques. The proposed strategy enhances the quality and efficiency of the predictor subset selection, with minimal chosen predictors to achieve enhanced prediction accuracy. It outperforms the other prediction selection methods. Besides, a configuration of an adaptive forecasting model is introduced and the performance tests are presented to further validate the impact of the PSS results for the PV power prediction accuracy enhancement.},
keywords={Predictive models;Forecasting;Genetic algorithms;Computational modeling;Adaptation models;Analytical models;Power generation;Adaptive model;big data;binary genetic algorithm;distributed PV;forecasting;fitness evaluation measure;predictor subset selection;renewable energy;smart grid;solar energy;support vector regression},
doi={10.1109/ACCESS.2019.2926826},
ISSN={2169-3536},
month={},}
@ARTICLE{9064504,
author={Igartua, Mónica Aguilar and Mendoza, Florina Almenares and Redondo, Rebeca P. Díaz and Vicente, Manuela I. Martín and Forné, Jordi and Campo, Celeste and Fernández-Vilas, Ana and De La Cruz Llopis, Luis J. and García-Rubio, Carlos and López, Andrés Marín and Mezher, Ahmad Mohamad and Díaz-Sánchez, Daniel and Cerezo-Costas, Héctor and Rebollo-Monedero, David and Arias-Cabarcos, Patricia and Rico-Novella, Francisco José},
journal={IEEE Access},
title={INRISCO: INcident monitoRing in Smart COmmunities},
year={2020},
volume={8},
number={},
pages={72435-72460},
abstract={Major advances in information and communication technologies (ICTs) make citizens to be considered as sensors in motion. Carrying their mobile devices, moving in their connected vehicles or actively participating in social networks, citizens provide a wealth of information that, after properly processing, can support numerous applications for the benefit of the community. In the context of smart communities, the INRISCO [1] proposal intends for (i) the early detection of abnormal situations in cities (i.e., incidents), (ii) the analysis of whether, according to their impact, those incidents are really adverse for the community; and (iii) the automatic actuation by dissemination of appropriate information to citizens and authorities. Thus, INRISCO will identify and report on incidents in traffic (jam, accident) or public infrastructure (e.g., works, street cut), the occurrence of specific events that affect other citizens' life (e.g., demonstrations, concerts), or environmental problems (e.g., pollution, bad weather). It is of particular interest to this proposal the identification of incidents with a social and economic impact, which affects the quality of life of citizens.},
keywords={Smart cities;Twitter;Monitoring;Intelligent sensors;Early detection of incidents;smart cities;citizen sensor;vehicular communications;big data analysis;social networks},
doi={10.1109/ACCESS.2020.2987483},
ISSN={2169-3536},
month={},}
@ARTICLE{7876775,
author={Zhang, Lichen and Cai, Zhipeng and Li, Peng and Wang, Liang and Wang, Xiaoming},
journal={IEEE Access},
title={Spectrum-Availability Based Routing for Cognitive Sensor Networks},
year={2017},
volume={5},
number={},
pages={4448-4457},
abstract={With the occurrence of Internet of Things (IoT) era, the proliferation of sensors coupled with the increasing usage of wireless spectrums especially the ISM band makes it difficult to deploy real-life IoT. Currently, the cognitive radio technology enables sensors transmit data packets over the licensed spectrum bands as well as the free ISM bands. The dynamic spectrum access technology enables secondary users (SUs) access wireless channel bands that are originally licensed to primary users. Due to the high dynamic of spectrum availability, it is challenging to design an efficient routing approach for SUs in cognitive sensor networks. We estimate the spectrum availability and spectrum quality from the view of both the global statistical spectrum usage and the local instant spectrum status, and then introduce novel routing metrics to consider the estimation. In our novel routing metrics, one retransmission is allowed to restrict the number of rerouting and then increase the routing performance. Then, the related two routing algorithms according to the proposed routing metrics are designed. Finally, our routing algorithms in extensive simulations are implemented to evaluate the routing performance, and we find that the proposed algorithms achieve a significant performance improvement compared with the reference algorithm.},
keywords={Routing;Measurement;Wireless sensor networks;Cognitive radio;Algorithm design and analysis;Routing protocols;Internet of Things;cognitive sensor networks;data forwarding;spectrum-availability;retransmission},
doi={10.1109/ACCESS.2017.2681743},
ISSN={2169-3536},
month={},}
@ARTICLE{8463459,
author={Lu, Zhang and Yu, Zhang and Yali, Peng and Shigang, Liu and Xiaojun, Wu and Gang, Lu and Yuan, Rao},
journal={IEEE Access},
title={Fast Single Image Super-Resolution Via Dilated Residual Networks},
year={2019},
volume={7},
number={},
pages={109729-109738},
abstract={Recently, deep convolutional neural networks (CNNs) have been attracting considerable attention in single image super-resolution. Some CNN-based methods, such as VDSR verified that residual learning can speed up the training and significantly improve the performance of accuracy. However, with very deep networks, convergence speed is still a critical issue in training due to the cost of requiring enormous parameters. In order to deal with this issue, we redesign the residual networks based on dilated networks. In this paper, we propose symmetrical dilated residual convolution networks (FDSR) to tackle image super-resolution problems. Our network is on the basis of the dilated convolutions supported exponential expansion of the receptive field without loss of resolution and coverage. This means that FDSR can speed up the training and improve the performance of accuracy without increasing the model's depth or complexity. Meanwhile, we attempt to combine the image pre-processing approach of VGG-net with mean squared error (MSE) to enhance the performance. The experimental results demonstrate that the training time-consuming proposed model achieves nearly a half with even superior restoration quality. Further, we present a novel network with less layers and parameters can achieve real-time performance on a generic CPU and still maintain superior performance.},
keywords={Convolution;Image resolution;Training;Image reconstruction;Convolutional neural networks;Convergence;Image restoration;Image super-resolution;residual learning;dilated convolution},
doi={10.1109/ACCESS.2018.2865613},
ISSN={2169-3536},
month={},}
@ARTICLE{8758410,
author={Li, Jinxing and Zhang, Bob and Lu, Guangming and Zhang, David},
journal={IEEE Access},
title={Dual Asymmetric Deep Hashing Learning},
year={2019},
volume={7},
number={},
pages={113372-113384},
abstract={Due to the impressive learning power, deep learning has achieved a remarkable performance in supervised hash function learning. In this paper, we propose a novel asymmetric supervised deep hashing method to preserve the semantic structure among different categories and generate the binary codes simultaneously. Specifically, two asymmetric deep networks are constructed to reveal the similarity between each pair of images according to their semantic labels. Furthermore, since the binary codes in the Hamming space also should keep the semantic affinity existing in the original space, another asymmetric pairwise loss is introduced to capture the similarity between the binary codes and real-value features. This asymmetric loss not only improves the retrieval performance, but also contributes to a quick convergence at the training phase. By taking advantage of the two-stream deep structures and two types of asymmetric pairwise functions, an alternative algorithm is designed to efficiently optimize the deep features and high-quality binary codes. Experimental results on three real-world datasets substantiate the effectiveness and superiority of our approach as compared with the state-of-the-art.},
keywords={Binary codes;Semantics;Hash functions;Deep learning;Training;Quantization (signal);Neural networks;Hashing;asymmetric;deep learning;similarity},
doi={10.1109/ACCESS.2019.2927524},
ISSN={2169-3536},
month={},}
@ARTICLE{9229053,
author={Zhang, Siqi and Wang, Lei and Zhang, Jie and Gu, Ling and Jiang, Xiran and Zhai, Xiaoyue and Sha, Xianzheng and Chang, Shijie},
journal={IEEE Access},
title={Consecutive Context Perceive Generative Adversarial Networks for Serial Sections Inpainting},
year={2020},
volume={8},
number={},
pages={190417-190430},
abstract={Image inpainting is a hot topic in computer vision research and has been successfully applied to both traditional and digital mediums, such as oil paintings or old photos mending, image or video denoising and super-resolution. With the introduction of artificial intelligence (AI), a series of algorithms, represented by semantic inpainting, have been developed and better results were achieved. Medical image inpainting, as one of the most demanding applications, needs to meet both the visual effects and strict content correctness. 3D reconstruction of microstructures, based on serial sections, could provide more spatial information and help us understand the physiology or pathophysiology mechanism in histology study, in which extremely high-quality continuous images without any defects are required. In this article, we proposed a novel Consecutive Context Perceive Generative Adversarial Networks (CCPGAN) for serial sections inpainting. Our method can learn semantic information from its neighboring image, and restore the damaged parts of serial sectioning images to maximum extent. Validated with 2 sets of serial sectioning images of mouse kidney, qualitative and quantitative results suggested that our method could robustly restore breakage of any size and location while achieving near realtime performance.},
keywords={Image restoration;Generative adversarial networks;Feature extraction;Biomedical imaging;Training;Convolution;Semantics;Serial sectioning images;generative adversarial network;consecutive context perceive GAN},
doi={10.1109/ACCESS.2020.3031973},
ISSN={2169-3536},
month={},}
@ARTICLE{9181518,
author={Fraile, Lidia Pocero and Tsampas, Stelios and Mylonas, Georgios and Amaxilatis, Dimitrios},
journal={IEEE Access},
title={A Comparative Study of LoRa and IEEE 802.15.4-Based IoT Deployments Inside School Buildings},
year={2020},
volume={8},
number={},
pages={160957-160981},
abstract={IoT deployments for smart cities and smart buildings have been multiplying exponentially in recent years, benefiting from a steady rise in the number of new technologies that deal with the underlying networking and application challenges in indoor and outdoor spaces. Due to the overlap in their specifications, we are still trying to figure out which of these technologies fits better to certain application domains, such as building monitoring. In this work, we provide a comparative study between IEEE 802.15.4 and LoRa, based on our experiences from using both wireless networking technologies in the context of indoor deployments aimed at IoT-enabled school buildings in Europe. We provide an apples-to-apples comparison between the two technologies, comparing them in some cases in the same building and application context. Although these two technologies initially might not seem to be competing in the same application space, in practice we found out that both have strengths and weaknesses in the specific application domain we have been using them. Moreover, our LoRa-based networking implementation, on top of Arduino-based hardware, appears to be an option that allows for a robust, reliable and lower overall cost IoT deployment, especially in cases with multi-floor building installations and low bandwidth requirements. We also present a network-level dataset produced from our installations and upon which we based our findings and discussion. We provide data collected from 6 different school buildings, 8 networks and 49 devices, to compare the performance and cost-effectiveness of competing IoT technologies. In that effect, with LoRa we can achieve similar or better link quality to IEEE 802.15.4, with higher data rate and lower costs.},
keywords={IEEE 802.15 Standard;Europe;Reliability;Smart cities;Wireless sensor networks;Bandwidth;IEEE 802.15.4;IoT;LoRa;LPWAN;real-world deployment;smart cities},
doi={10.1109/ACCESS.2020.3020685},
ISSN={2169-3536},
month={},}
@ARTICLE{9913473,
author={Yan, Xiaojia and Liang, Weige and Xu, Dongxue},
journal={IEEE Access},
title={Remaining Useful Life Interval Prediction for Complex System Based on BiGRU Optimized by Log-Norm},
year={2022},
volume={10},
number={},
pages={108089-108102},
abstract={The task of remaining useful life (RUL) uncertainty management is the major challenge in solving the failure of the complex mechanical system. Primary research methods use statistical models or stochastic processes to fit the distribution of historical degradation data. However, it is difficult to accurately capture the degradation information of monitoring big data through statistics in practice. In this paper, the prediction interval (PI) obtained by the proposed feature attention-log-norm bidirectional gated recurrent unit (FA-LBiGRU) model is adopted to quantify the prediction uncertainty of RUL. Initially, the critical feature vectors are extracted from multi-dimensional, nonlinear, and large-scale sensor signals using the feature attention mechanism. Additionally, the BiGRU network is used to model and learn the time-varying characteristics of the attention-weighted features from the forward and backward directions, and the network parameters are trained by the maximum log-likelihood loss function. Ultimately, the probability density function based on the lognormal distribution is calculated to measure the uncertainty of the equipment RUL. The effectiveness of the proposed method is verified through the well-known benchmark data set of the turbofan engines provided by NASA. The experimental results show that the proposed methods can obtain higher point prediction accuracy for the complex system compared with state-of-the-art approaches and high-quality PIs satisfying real-time requirements.},
keywords={Degradation;Predictive models;Logic gates;Uncertainty;Mechanical systems;Feature extraction;Data models;Fusion model;gated recurrent unit;prediction intervals;remaining useful life;system prognostics;uncertainty management},
doi={10.1109/ACCESS.2022.3212694},
ISSN={2169-3536},
month={},}
@ARTICLE{9923925,
author={Hsu, Chih-Hung and Zeng, Jun-Yi and Chang, An-Yuan and Cai, Shu-Qi},
journal={IEEE Access},
title={Deploying Industry 4.0 Enablers to Strengthen Supply Chain Resilience to Mitigate Ripple Effects: An Empirical Study of Top Relay Manufacturer in China},
year={2022},
volume={10},
number={},
pages={114829-114855},
abstract={In the global marketplace, supply chains are becoming increasingly linked. The risk posed by the ripple effect of uncertainty will damage the normal operation of the entire supply chain, so it is an important issue to enhance the resilience of enterprises that manufacture electronic to reduce the ripple effects of supply chains. Quality function deployment (QFD) has been applied in many areas to solve multi-criteria decision making (MCDM) problems successfully. However, there is still lack of study has addressed the improvement of supply chain resilience through Industry 4.0 to alleviate the ripple effect, and the application of QFD to integrate and analyze the relationship between Industry 4.0 and supply chain resilience, and between supply chain resilience and the ripple effect. Therefore, this study proposes an integrated QFD-MCDM model to identify the key Industry 4.0 enablers (I4Es) to strengthen supply chain resilience indicators (SCRIs) and mitigate the ripple effect risk factors (RERFs), thus providing an effective method for enterprises to develop a resilient supply chain that can quickly respond to changes and uncertainties. The case study considered China’s largest relay manufacturing enterprise as the object and obtained important management insights, as well as practical significance, from implementing the proposed research framework. The study found the following to be the most urgent I4Es required to strengthen SCRIs and reduce the key RERFs: IT information technology structure and level, enterprise strategic management and new technology coordination, supply chain digitization, analysis and management of big Data, IT Infrastructure and use digital technology for new product innovation, intelligent. When these measures are improved, the SCRIs can be improved, such as risk awareness, efficiency, agility, sustainability, coordination and cooperation, supply chain structure and safety assurance. Finally, RERFs, such as the ripple effect caused by economic collapse, epidemic conditions, natural disasters, political factors, supply chain operation capability caused, can be alleviated or eliminated. With limited resources, enterprises can devote their most important resources to the most critical Industry 4.0 improvement strategies. This framework provides an effective method for electronics manufacturers to formulate I4Es and strengthen SCRIs to mitigate the RERFs, and also provides a reference for enterprises in other fields of supply chain management.},
keywords={Supply chains;Resilience;Fourth Industrial Revolution;Quality function deployment;Consumer electronics;Decision making;Business;Industry 4.0;supply chain resilience;ripple effects;multi-attribute decision making;quality function deployment},
doi={10.1109/ACCESS.2022.3215620},
ISSN={2169-3536},
month={},}
@ARTICLE{7782457,
author={Wang, Miao and Wang, Guiling and Zhang, Yujun and Li, Zhongcheng},
journal={IEEE Transactions on Services Computing},
title={A High-Reliability Multi-Faceted Reputation Evaluation Mechanism for Online Services},
year={2019},
volume={12},
number={6},
pages={836-850},
abstract={In today's society, there are plenty of services available, and customers are facing bigger challenge in choosing them than ever before. Therefore, it is important to build a reliable reputation mechanism for selecting a credible service. To address the challenges of reputation evaluation, including the diverse and dynamic natures of services, incompleteness of user feedback, and intricacy of malicious ratings, a High-reliability Multi-faceted Reputation evaluation mechanism for online services (HMRep) is proposed. First, HMRep starts with addressing the incomplete feedback and estimates missing ratings based on both the service quality and a user's rating behavior. Second, HMRep identifies and removes malicious collusive raters and irresponsible raters to improve the accuracy of reputation calculation. Further, the reputation calculation is based on the user credibility and incorporates historical information to reflect the change of the services. Finally, we provide a multi-faceted evaluation method to satisfy some specific needs of customers who are only concerned about a subset of a services features. Experimental results verify the design of HMRep, and reveal HMRep can effectively defend against malicious ratings, and accurately calculate the reputation values of services. HMRep can be applied in lots of sectors for different kinds of services, especially those complex ones.},
keywords={Feedback;Measurement;Reliability;Security;Indexes;Reputation evaluation;incomplete user feedback;malicious ratings;index weights},
doi={10.1109/TSC.2016.2638812},
ISSN={1939-1374},
month={Nov},}
@ARTICLE{8620498,
author={Scaloni, Andrea and Cirella, Pasquale and Sgheiz, Mauro and Diamanti, Riccardo and Micheli, Davide},
journal={IEEE Access},
title={Multipath and Doppler Characterization of an Electromagnetic Environment by Massive MDT Measurements From 3G and 4G Mobile Terminals},
year={2019},
volume={7},
number={},
pages={13024-13034},
abstract={This paper describes an innovative approach to radio channel characterization in UMTS and LTE mobile networks. In place of traditional drive tests (DT), which employ a single test mobile, a massive collection of georeferenced radio measurements is made from a wide population of user equipment (UE). This is possible with new 3GPP features, called “minimization of DTs” (MDT), which are implemented in the last generation UEs and enable the reporting of additional periodical measurements, including GPS position and estimated UE distance (i.e. delay) over the radio path. This opens to new fields of investigation in the mobile radio channel, unreachable with the legacy DT approach, such as multipath and Doppler analysis. The UE MDT data of UMTS and LTE RAN of Telecom Italia Mobile, in the Italian midsized city of Bologna, have been statistically analyzed. The big data elaboration has been performed with the Nokia proprietary system “GeoSynthesis.” The results give a high-resolution geographical view of the above-mentioned channel phenomena affecting the quality and user experience. They are in good accordance with network performance indicators: the higher the multipath time, the worse the decoding performance of radio blocks (block error rate). The estimated Doppler shift also fits the known mobility patterns in the urban environment.},
keywords={3G mobile communication;Fading channels;Long Term Evolution;Doppler shift;Coherence;Bandwidth;Telecommunications;Mobile phone;UMTS;LTE;minimization of drive tests;fading;multipath;Doppler;channel characterization},
doi={10.1109/ACCESS.2019.2892864},
ISSN={2169-3536},
month={},}
@ARTICLE{9353046,
author={Łysiak, Adam and Szmajda, Mirosław},
journal={IEEE Access},
title={Empirical Comparison of the Feature Evaluation Methods Based on Statistical Measures},
year={2021},
volume={9},
number={},
pages={27868-27883},
abstract={One of the most important classification problems is selecting proper features, i.e. features that describe the classified object in the most straightforward way possible. Then, one of the biggest challenges of the feature selection is the evaluation of the feature’s quality. There is a plethora of feature evaluation methods in the literature. This paper presents the results of a comparison between nine selected feature evaluation methods, both existing in literature and newly defined. To make a comparison, features from ten various sets were evaluated by every method. Then, from every feature set, best subset (according to each method) was chosen. Those subsets then were used to train a set of classifiers (including decision trees and forests, linear discriminant analysis, naive Bayes, support vector machines, k nearest neighbors and an artificial neural network). The maximum accuracy of those classifiers, as well as the standard deviation between their accuracies, were used as a quality measures of each particular method. Furthermore, it was determined, which method is the most universal in terms of the data set, i.e. for which method, obtained accuracies were dependent on the feature set the least. Finally, computation time of each method was compared. Results indicated that for applications with limited computational power, method based on the average overlap between feature’s values seem best suited. It led to high accuracies and proved to be fast to compute. However, if the data set is known to be normally distributed, method based on two-sample ${t}$ -test may be preferable.},
keywords={Feature extraction;Visualization;Task analysis;Satellite broadcasting;Probability distribution;Prediction algorithms;Classification;dimensionality reduction;distribution overlap;feature evaluation;feature extraction;feature selection;filter methods;machine learning;overlap coefficient;pattern recognition},
doi={10.1109/ACCESS.2021.3058428},
ISSN={2169-3536},
month={},}
@ARTICLE{8306964,
author={Neghabi, Ali Akbar and Jafari Navimipour, Nima and Hosseinzadeh, Mehdi and Rezaee, Ali},
journal={IEEE Access},
title={Load Balancing Mechanisms in the Software Defined Networks: A Systematic and Comprehensive Review of the Literature},
year={2018},
volume={6},
number={},
pages={14159-14178},
abstract={With the expansion of the network and increasing their users, as well as emerging new technologies, such as cloud computing and big data, managing traditional networks is difficult. Therefore, it is necessary to change the traditional network architecture. Lately, to address this issue, a notion named software-defined network (SDN) has been proposed, which makes network management more conformable. Due to limited network resources and to meet the requirements of quality of service, one of the points that must be considered is load balancing issue that serves to distribute data traffic among multiple resources in order to maximize the efficiency and reliability of network resources. Load balancing is established based on the local information of the network in the conventional network. Hence, it is not very precise. However, SDN controllers have a global view of the network and can produce more optimized load balances. Although load balancing mechanisms are important in the SDN, to the best of our knowledge, there exists no precise and systematic review or survey on investigating these issues. Hence, this paper reviews the load balancing mechanisms which have been used in the SDN systematically based on two categories, deterministic and non-deterministic. Also, this paper represents benefits and some weakness regarded of the selected load balancing algorithms and investigates the metrics of their algorithms. In addition, the important challenges of these algorithms have been reviewed, so better load balancing techniques can be applied by the researchers in the future.},
keywords={Load management;Servers;Control systems;Software defined networking;Protocols;Systematics;Routing;Load balancing;review;SDN;software defined networks;systematic},
doi={10.1109/ACCESS.2018.2805842},
ISSN={2169-3536},
month={},}
@ARTICLE{9444358,
author={Tschöke, Kilian and Mueller, Inka and Memmolo, Vittorio and Moix-Bonet, Maria and Moll, Jochen and Lugovtsova, Yevgeniya and Golub, Mikhail and Venkat, Ramanan Sridaran and Schubert, Lars},
journal={IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
title={Feasibility of Model-Assisted Probability of Detection Principles for Structural Health Monitoring Systems Based on Guided Waves for Fiber-Reinforced Composites},
year={2021},
volume={68},
number={10},
pages={3156-3173},
abstract={In many industrial sectors, structural health monitoring (SHM) is considered as an addition to nondestructive testing (NDT) that can reduce maintenance effort during the lifetime of a technical facility, structural component, or vehicle. A large number of SHM methods are based on ultrasonic waves, whose properties change depending on structural health. However, the wide application of SHM systems is limited due to the lack of suitable methods to assess their reliability. The evaluation of the system performance usually refers to the determination of the probability of detection (POD) of a test procedure. Up until now, only a few limited methods exist to evaluate the POD of SHM systems, which prevents them from being standardized and widely accepted in the industry. The biggest hurdle concerning the POD calculation is the large number of samples needed. A POD analysis requires data from numerous identical structures with integrated SHM systems. Each structure is then damaged at different locations and with various degrees of severity. All of these are connected to high costs. Therefore, one possible way to tackle this problem is to perform computer-aided investigations. In this work, the POD assessment procedure established in NDT according to the Berens model is adapted to guided wave-based SHM systems. The approach implemented here is based on solely computer-aided investigations. After efficient modeling of wave propagation phenomena across an automotive component made of a carbon-fiber-reinforced composite, the POD curves are extracted. Finally, the novel concept of a POD map is introduced to look into the effect of damage position on system reliability.},
keywords={Reliability;Acoustics;Monitoring;Industries;Standards;Inspection;Automotive engineering;Acousto ultrasonics (AUs);automotive industry;damage detection;elastodynamic finite integration technique (EFIT);reliability assessment},
doi={10.1109/TUFFC.2021.3084898},
ISSN={1525-8955},
month={Oct},}
@ARTICLE{9406802,
author={Situ, Zuxiang and Teng, Shuai and Liu, Hanlin and Luo, Jinhua and Zhou, Qianqian},
journal={IEEE Access},
title={Automated Sewer Defects Detection Using Style-Based Generative Adversarial Networks and Fine-Tuned Well-Known CNN Classifier},
year={2021},
volume={9},
number={},
pages={59498-59507},
abstract={Automated sewer defects detection has become an important trend for better management and maintenance of urban sewer systems. Deep learning technology has developed rapidly and offers an innovative solution for automated detection in engineering applications. However, insufficient data and unbalanced samples have proposed a big challenge to deep learning model training. This study adopts the state-of-the-art Style-based Generative Adversarial Networks (StyleGANs) model and compares the performances of its two variants in producing high-quality synthetic sewer defects images. Seven well-known CNN models are further fine-tuned and trained using the synthetic images for automated sewer defects detection to examine the effects of StyleGANs on augmenting the detection performance. Results show that both StyleGANs are efficient in producing high-quality images with various styles and high-level details for multiple types of sewer defects. Specifically, the StyleGAN2-Adaptive Discriminator Augmentation (StyleGAN2-ADA) with the aid of Freeze Discriminator (Freeze-D) yields the best model performance. Among the adopted CNN classifiers, Inception_v3 achieves the highest detection accuracy. The mean detection accuracy is 94% (with a specific accuracy of 99.7%, 97%, 95.3% and 84% for tree root, residential wall, disjoint and obstacle, respectively) and confirms the reliability of the StyleGANs' performance. The study shows that StyleGANs provide a promising method to alleviate the limited and uneven dataset problem and can improve the deep learning model performance.},
keywords={Training;Generators;Generative adversarial networks;Deep learning;Transfer learning;Image quality;Computational modeling;Automated detection;image synthesis;sewer defects;well-known CNN classifiers;StyleGANs},
doi={10.1109/ACCESS.2021.3073915},
ISSN={2169-3536},
month={},}
@ARTICLE{9787492,
author={Costa, Daniel G. and Peixoto, João Paulo J. and Jesus, Thiago C. and Portugal, Paulo and Vasques, Francisco and Rangel, Elivelton and Peixoto, Maycon},
journal={IEEE Access},
title={A Survey of Emergencies Management Systems in Smart Cities},
year={2022},
volume={10},
number={},
pages={61843-61872},
abstract={The rapid urbanization process in the last century has deeply changed the way we live and interact with each other. As most people now live in urban areas, cities are experiencing growing demands for more efficient and sustainable public services that may improve the perceived quality of life, specially with the anticipated impacts of climatic changes. In this already complex scenario with increasingly overcrowded urban areas, different types of emergency situations may happen anywhere and anytime, with unpredictable costs in human lives and economic losses. In order to cope with unexpected and potentially dangerous emergencies, smart cities initiatives have been developed in different cities, addressing multiple aspects of emergencies detection, alerting, and mitigation. In this context, this article surveys recent smart city solutions for crisis management, proposing definitions for emergencies-oriented systems and classifying them according to the employed technologies and provided services. Additionally, recent developments in the domains of Internet of Things, Artificial Intelligence and Big Data are also highlighted when associated to the management of urban emergencies, potentially paving the way for new developments while classifying and organizing them according to different criteria. Finally, open research challenges will be identified, indicating promising trends and research directions for the coming years.},
keywords={Emergency services;Smart cities;Crisis management;Internet of Things;Sustainable development;Hazards;Emergency services;Resiliency;Smart cities;emergencies management;Internet of Things;sustainable cities;resilient cities},
doi={10.1109/ACCESS.2022.3180033},
ISSN={2169-3536},
month={},}
@ARTICLE{9497114,
author={Torres, Renato and Fortes, Sergio and Baena, Eduardo and Barco, Raquel},
journal={IEEE Access},
title={Social-Aware Load Balancing System for Crowds in Cellular Networks},
year={2021},
volume={9},
number={},
pages={107812-107823},
abstract={Over time, load balancing systems in cellular networks have been key to avoiding overload problems in the network and to maintaining a correct resource allocation and performance. However, the classical approaches were not designed for the dynamism generated by user behavior. The big crowds of users at certain social venues are one of the main concerns of mobile operators due to the load imbalance generated. Additionally, the mobility of users who attend social events (e.g., sports events, concerts, etc.) greatly impacts network performance due to its high correlation with network traffic. The availability to inform about events, particularly regarding venue location (e.g., stadiums, concert halls, convention centers) is exponentially growing thanks to its proliferation in social networks through geolocation databases and other functionalities. Therefore, the present work proposes a novel load balancing system integrating a fuzzy logic controller algorithm with social-awareness, which considers the relative position between cell sites and the social event venue in order to configure the network parameters. This approach is evaluated for different configurations of load balancing methods simulated on an urban macro scenario, mitigating the impact of the number of users per cell without degrading the signal quality. In this way, results show that social event data information plus soft or aggressive transmission power changes in cells can help to maintain the balance in the number of users per cell during mass events.},
keywords={Load management;Load modeling;Social networking (online);Optimization;Heuristic algorithms;Cellular networks;Prediction algorithms;Social events;mobile communication;fuzzy control;load balancing;social-awareness;communication system operations and management},
doi={10.1109/ACCESS.2021.3100459},
ISSN={2169-3536},
month={},}
@ARTICLE{6261335,
author={Gluga, Richard and Kay, Judy and Lever, Tim},
journal={IEEE Transactions on Learning Technologies},
title={Foundations for Modeling University Curricula in Terms of Multiple Learning Goal Sets},
year={2013},
volume={6},
number={1},
pages={25-37},
abstract={It is important, but very challenging, to design degree programs, so that the sequence of learning activities, topics, and assessments over three to five years give an effective progression in learning of generic skills, discipline-specific learning goals and accreditation competencies. Our CUSP (Course and Unit of Study Portal) system tackles this challenge, by helping subject teachers define the curriculum of their subject, linking it to Faculty and institutional goals. The same information is available to students, enabling them to see how each subject relates to those goals. It then gives additional big-picture views of the degree for the academics responsible for the whole degree, including the ability to easily assess if a degree meets accreditation requirements. CUSP achieves this by exploiting a lightweight semantic mapping approach that gives a highly flexible and scalable way to map learning goals from multiple internal and external accrediting sources across the degree. We report its validation as used in a live university environment, across three diverse faculties, with 277 degrees and 7,810 subject sessions over a period of three years. Data from this evaluation indicates steady improvement in the documentation of the relationships between subjects, assessments, learning outcomes, and program level goals. This is driven by the reporting tools and visualizations provided by CUSP, which enable program designers and lecturers to identify parts of the curriculum that are unclear. This improved documentation of the curriculum enables more accurate and immediate quality reviews. Key contributions of this work are: a validated new approach for curriculum design that helps address the complexity of ensuring learners progressively develop generic skills; and a validated lightweight semantic mapping approach that can flexibly support visualizing the curriculum against multiple sets of learning goal frameworks.},
keywords={Education courses;Accreditation;Computer science education;Education;Electronic learning;Education courses;Accreditation;Computer science education;Education;Electronic learning;Curriculum mapping;Education courses;Accreditation;Computer science education;Education;Electronic learning;graduate attributes;Education courses;Accreditation;Computer science education;Education;Electronic learning;accreditation competencies;Education courses;Accreditation;Computer science education;Education;Electronic learning;learner model},
doi={10.1109/TLT.2012.17},
ISSN={1939-1382},
month={Jan},}
@ARTICLE{9696351,
author={Rathod, Nihesh and Sundaresan, Rajesh},
journal={IEEE Access},
title={Relay Placement Algorithms for IoT Connectivity and Coverage in an Outdoor Heterogeneous Propagation Environment},
year={2022},
volume={10},
number={},
pages={13270-13289},
abstract={A vast majority of the Internet of Things (IoT) devices will be connected in a topology where the edge-devices push data to a local gateway, which forwards the data to a cloud for further processing. In sizeable outdoor deployment regions, the edge-devices may experience poor connectivity due to their distant locations and limited transmission power. Repeaters or relays must be placed at a few locations to ensure reliable connectivity to either a gateway or another node in the network. A big challenge in achieving reliable connectivity and coverage is the outdoor propagation environment being heterogeneous. Engineers often deploy networks based on resource-intensive field visits, detailed surveys, measurements, initial test deployments, followed by fine-tuning. For scalability to large scale IoT deployments, automated network planning tools are essential. Such tools should predict connectivity based on the edge-device locations using available Geographical Information System (GIS) data, identify the need for relays/repeaters, and, if needed, suggest the number of relays needed with their locations. Furthermore, such tools should also be extended to suggest the minimum number and locations of base stations that maximise coverage. In this paper, we propose an automated network deployment framework using a black box received signal strength estimation oracle that provides signal strength estimates between candidate pairs of transceiver locations in a heterogeneous deployment region. Our proposed methodology uses either Ant Colony Optimisation (ACO) or Differential Evolution (DE) to identify the number and locations of relays for meeting specified quality of service constraints. We discuss adaptations of our techniques to handle scenarios with multiple gateways. Further, we show the effectiveness of these algorithms to find suitable candidate base station locations to provide coverage in a heterogeneous propagation environment that meets the specified quality of service constraints. We then demonstrate the effectiveness of our algorithms in two deployment regions.},
keywords={Relays;Logic gates;Quality of service;Internet of Things;Wireless sensor networks;Optimization;Base stations;Coverage;GIS;heterogeneous propagation environment;Internet of Things;IoT;RF propagation tool;RSSI;sub giga hertz;sub-GHz},
doi={10.1109/ACCESS.2022.3147488},
ISSN={2169-3536},
month={},}
@ARTICLE{8168250,
author={Yeow, Kimchai and Gani, Abdullah and Ahmad, Raja Wasim and Rodrigues, Joel J. P. C. and Ko, Kwangman},
journal={IEEE Access},
title={Decentralized Consensus for Edge-Centric Internet of Things: A Review, Taxonomy, and Research Issues},
year={2018},
volume={6},
number={},
pages={1513-1524},
abstract={With the exponential rise in the number of devices, the Internet of Things (IoT) is geared toward edge-centric computing to offer high bandwidth, low latency, and improved connectivity. In contrast, legacy cloud-centric platforms offer deteriorated bandwidth and connectivity that affect the quality of service. Edge-centric Internet of Things-based technologies, such as fog and mist computing, offer distributed and decentralized solutions to resolve the drawbacks of cloud-centric models. However, to foster distributed edge-centric models, a decentralized consensus system is necessary to incentivize all participants to share their edge resources. This paper is motivated by the shortage of comprehensive reviews on decentralized consensus systems for edge-centric Internet of Things that elucidates myriad of consensus facets, such as data structure, scalable consensus ledgers, and transaction models. Decentralized consensus systems adopt either blockchain or blockchainless directed acyclic graph technologies, which serve as immutable public ledgers for transactions. This paper scrutinizes the pros and cons of state-of-the-art decentralized consensus systems. With an extensive literature review and categorization based on existing decentralized consensus systems, we propose a thematic taxonomy. The pivotal features and characteristics associated with existing decentralized consensus systems are analyzed via a comprehensive qualitative investigation. The commonalities and variances among these systems are analyzed using key criteria derived from the presented literature. Finally, several open research issues on decentralized consensus for edge-centric IoT are presented, which should be highlighted regarding centralization risk and deficiencies in blockchain/blockchainless solutions.},
keywords={Cloud computing;Edge computing;Taxonomy;Data structures;Internet of Things;Electronic mail;Blockchain;decentralized consensus systems;directed acyclic graph;edge-centric Internet of Things},
doi={10.1109/ACCESS.2017.2779263},
ISSN={2169-3536},
month={},}
@ARTICLE{8485356,
author={Zhu, Shuqin and Zhu, Congxu and Wang, Wenhong},
journal={IEEE Access},
title={A Novel Image Compression-Encryption Scheme Based on Chaos and Compression Sensing},
year={2018},
volume={6},
number={},
pages={67095-67107},
abstract={In this paper, a novel image compression–encryption hybrid algorithm is proposed. First, a Gauss random matrix and a random scrambling matrix are generated by using Chebyshev mapping and Logistic mapping, respectively. Then, based on the principle that a scrambling Gauss matrix is still a Gauss matrix, a compression scheme for ciphertext images is designed. It is dependent on the Gauss random matrix and the random scrambling matrix, which mainly consists of three parts: the permutation-based encryption using random scrambling matrix by Alice, the encoding with Gauss random matrix by Charlie, and the joint decryption and decoding by Bob. It has a special application scenario, that is, Alice requires semi integrity Charlie to transmit images to Bob through a channel. Experimental results show that the scheme has strong robustness against noise and chosen-plaintext attack, and further the peak signal-to-noise ratio (PSNR) and subjective visual quality of reconstructed images can be improved by comparing with the similar methods.},
keywords={Image coding;Compressed sensing;Encryption;Image reconstruction;Sparse matrices;Chebyshev approximation;Compressed sensing;Gauss measurement matrix;random permutation matrix;chaos mapping;chosen plain text attack},
doi={10.1109/ACCESS.2018.2874336},
ISSN={2169-3536},
month={},}
@ARTICLE{8682040,
author={Liu, Fagui and Chen, Cheng and Gu, Dian and Zheng, Jingzhong},
journal={IEEE Access},
title={FTPN: Scene Text Detection With Feature Pyramid Based Text Proposal Network},
year={2019},
volume={7},
number={},
pages={44219-44228},
abstract={Scene text detection is to detect the position of a text in the natural scene, the quality of which will directly affect the subsequent text recognition. It plays an important role in fields such as image retrieval and autopilot. How to perform multi-scale and multi-oriented text detection in the scene still remains as a problem. This paper proposes an effective scene text detection method that combines the convolutional neural network (CNN) and recurrent neural network (RNN). In order to better adapt to texts in different scales, feature pyramid networks (FPN) have been applied in the CNN part to extract multi-scale features of the image. We then utilize bidirectional long-short-term memory (Bi-LSTM) to encode these features to make full use of the text sequence characteristics with the outputs as a series of text proposals. The generated proposals are finally linked into a text line through a well-designed text connector, which can be flexibly adapted to any oriented texts. The proposed method is evaluated on three public datasets: ICDAR2013, ICDAR2015, and USTB-SV1K. For ICDAR2013 and USTB-1K, we have reached 92.5% and 62.6% F-measure, respectively. Our method has reached 72.8% F-measure on the more challenging ICDAR2015 which demonstrates the effectiveness of our method.},
keywords={Feature extraction;Proposals;Text recognition;Recurrent neural networks;Connectors;Microsoft Windows;Gabor filters;Scene text detection;multi-orientation;convolutional neural network;recurrent neural network;residual network},
doi={10.1109/ACCESS.2019.2908933},
ISSN={2169-3536},
month={},}
@ARTICLE{8600319,
author={Tang, Shu and Zheng, Wanpeng and Xie, Xianzhong and He, Tao and Yang, Peng and Luo, Lei and Li, Zhixing and Hu, Yu and Zhao, Hao},
journal={IEEE Access},
title={Multi-Regularization-Constrained Blur Kernel Estimation Method for Blind Motion Deblurring},
year={2019},
volume={7},
number={},
pages={5296-5311},
abstract={Blur kernel (BK) estimation is the crucial technique to guarantee the success of blind image deblurring. In this paper, we propose a multi-regularization-constrained method to estimate an accurate BK from a single motion-blurred image. First, in order to generate sharp and reliable intermediate latent results, we propose a model which combines the spatial scale, ${L} _{0}$ norm, and the dark channel prior. Second, in order to preserve the continuity and the sparsity, and to remove the flaw in the BK, a dual-constrained regularization model, which combines the ${L} _{0}$ -regularized intensity prior and the ${L} _{2}$ -regularized gradient prior, is proposed for accurate BK estimation. The proposed model can not only preserve the continuity and the sparsity of the BK very well but also can remove the flaw thoroughly. Finally, we propose an efficient optimization strategy which can solve the proposed model efficiently. Extensive experiments compared with the state-of-the-art methods demonstrate that our method estimates more accurate BKs and obtains higher quality deblurring images in terms of both subjective vision and quantitative metrics.},
keywords={Image edge detection;Estimation;Mathematical model;Image restoration;Kernel;Reliability;Optimization;Blind image deblurring;blur kernel;spatial scale;L₀-regularized intensity prior;L₂-regularized gradient prior},
doi={10.1109/ACCESS.2018.2889466},
ISSN={2169-3536},
month={},}
@ARTICLE{9036051,
author={Zhao, Tian-Fang and Chen, Wei-Neng and Kwong, Sam and Gu, Tian-Long and Yuan, Hua-Qiang and Zhang, Jie and Zhang, Jun},
journal={IEEE Transactions on Cybernetics},
title={Evolutionary Divide-and-Conquer Algorithm for Virus Spreading Control Over Networks},
year={2021},
volume={51},
number={7},
pages={3752-3766},
abstract={The control of virus spreading over complex networks with a limited budget has attracted much attention but remains challenging. This article aims at addressing the combinatorial, discrete resource allocation problems (RAPs) in virus spreading control. To meet the challenges of increasing network scales and improve the solving efficiency, an evolutionary divide-and-conquer algorithm is proposed, namely, a coevolutionary algorithm with network-community-based decomposition (NCD-CEA). It is characterized by the community-based dividing technique and cooperative coevolution conquering thought. First, to reduce the time complexity, NCD-CEA divides a network into multiple communities by a modified community detection method such that the most relevant variables in the solution space are clustered together. The problem and the global swarm are subsequently decomposed into subproblems and subswarms with low-dimensional embeddings. Second, to obtain high-quality solutions, an alternative evolutionary approach is designed by promoting the evolution of subswarms and the global swarm, in turn, with subsolutions evaluated by local fitness functions and global solutions evaluated by a global fitness function. Extensive experiments on different networks show that NCD-CEA has a competitive performance in solving RAPs. This article advances toward controlling virus spreading over large-scale networks.},
keywords={Optimization;Viruses (medical);Resource management;Computer science;Genetic algorithms;Complex networks;Cooperative coevolution (CC);evolutionary algorithm (EA);networked system;resource allocation;spreading control},
doi={10.1109/TCYB.2020.2975530},
ISSN={2168-2275},
month={July},}
@ARTICLE{8727877,
author={Liu, Fagui and Gui, Mengke and Yi, Chen and Lan, Yulin},
journal={IEEE Access},
title={A Fast Decomposition and Reconstruction Framework for the Pickup and Delivery Problem With Time Windows and LIFO Loading},
year={2019},
volume={7},
number={},
pages={71813-71826},
abstract={The pickup and delivery problem with time windows and last-in-first-out (LIFO) loading (PDPTWL) is a combinational optimization problem extended from the well-known vehicle routing problem (VRP), in which the type of customer point is no longer single and the loading order of the requests must meet the LIFO constraint. Due to its NP-hard nature, it is difficult for exact algorithms and heuristics with a linear structure to solve a large-scale problem in a reasonable time. In this paper, we propose a fast decomposition and reconstruction framework (D&R) to solve the PDPTWL with high quality in a relatively short time. An angle-based sweep method is used to decompose a complete solution into multiple sub-solutions, each of which is assigned to a tabu search for optimization. To speed up the whole process, the optimization procedure of sub-solutions is performed by different processors of multi-core CPU in parallel. Three neighborhood operators and three strategies to reduce the number of vehicles are designed to cope with the tabu search for further improvement. Moreover, the adaptive memory mechanism is added to provide a better start when the optimization procedure falls into the local optima. We compare our framework against the best known solutions on 119 instances with up to 300 requests, the results show that our framework is able to improve over 85% (107 out of 119) of the best known solutions. More specifically, the number of vehicles is optimized by about 60% (74 out of 119) and the driving distance by about 50% (59 out of 119). In addition on instances with the largest size of requests, the computational time of our framework can be 1/50 of the comparative results, confirming its efficiency.},
keywords={Optimization;Microsoft Windows;Transportation;Loading;Logistics;Heuristic algorithms;Germanium;Pickup and delivery problem with time windows;LIFO loading;tabu search;adaptive memory},
doi={10.1109/ACCESS.2019.2920444},
ISSN={2169-3536},
month={},}
@ARTICLE{9103941,
author={El-Bouri, Rasheed and Eyre, David W. and Watkinson, Peter and Zhu, Tingting and Clifton, David A.},
journal={IEEE Journal of Biomedical and Health Informatics},
title={Hospital Admission Location Prediction via Deep Interpretable Networks for the Year-Round Improvement of Emergency Patient Care},
year={2021},
volume={25},
number={1},
pages={289-300},
abstract={Objective: This paper presents a deep learning method of predicting where in a hospital emergency patients will be admitted after being triaged in the Emergency Department (ED). Such a prediction will allow for the preparation of bed space in the hospital for timely care and admission of the patient as well as allocation of resource to the relevant departments, including during periods of increased demand arising from seasonal peaks in infections. Methods: The problem is posed as a multi-class classification into seven separate ward types. A novel deep learning training strategy was created that combines learning via curriculum and a multi-armed bandit to exploit this curriculum post-initial training. Results: We successfully predict the initial hospital admission location with area-under-receiver-operating-curve (AUROC) ranging between 0.60 to 0.78 for the individual wards and an overall maximum accuracy of 52% where chance corresponds to 14% for this seven-class setting. Our proposed network was able to interpret which features drove the predictions using a `network saliency' term added to the network loss function. Conclusion: We have proven that prediction of location of admission in hospital for emergency patients is possible using information from triage in ED. We have also shown that there are certain tell-tale tests which indicate what space of the hospital a patient will use. Significance: It is hoped that this predictor will be of value to healthcare institutions by allowing for the planning of resource and bed space ahead of the need for it. This in turn should speed up the provision of care for the patient and allow flow of patients out of the ED thereby improving patient flow and the quality of care for the remaining patients within the ED.},
keywords={Hospitals;Training;Neural networks;Deep learning;Optimization;Machine learning algorithms;Machine learning algorithms;multi-layer neural networks;patient flow;hospitals},
doi={10.1109/JBHI.2020.2990309},
ISSN={2168-2208},
month={Jan},}
@ARTICLE{8454706,
author={Zhou, Qian and Qin, Xiaolin and Xie, Xiaojun},
journal={IEEE Access},
title={Dimension Incremental Feature Selection Approach for Vertex Cover of Hypergraph Using Rough Sets},
year={2018},
volume={6},
number={},
pages={50142-50153},
abstract={The minimum vertex cover problem is a well-known optimization problem; it has been used in a wide variety of applications. This paper focuses on rough set-based approach for the minimum vertex cover problem of the dynamic and static hypergraphs. First, we demonstrate the relationship between the attribute reduction of decision table and the minimum vertex cover of hypergraph, and the minimum vertex cover problem is converted to an attribute reduction problem based on this relationship. Then, we discuss the update mechanism of minimum vertex cover from the perspective of attribute reduction, and two types of incremental attribute reduction algorithms are proposed, one is the dynamic increase of single vertex and the other is the dynamic increase of multiple vertices. Our algorithms can quickly update the minimum vertex cover in a dynamic hypergraph and improve the rough sets-based method for the minimum vertex cover problem of a static hypergraph in terms of the computational time and the solution quality. The experimental results show the advantages and limitations of the proposed algorithms compared with the existing algorithms.},
keywords={Rough sets;Heuristic algorithms;Search problems;Aerodynamics;Optimization;Graph theory;Roads;Vertex cover;attribute reduction;rough sets;hypergraph},
doi={10.1109/ACCESS.2018.2868846},
ISSN={2169-3536},
month={},}
@ARTICLE{9130666,
author={Yin, Ling and Li, Chunhui and Qin, Chao and Peng, Yili and Gu, Jiarong and Zhang, Fei and Li, Shuo and Song, Zhiqiang},
journal={IEEE Access},
title={Identification Method of Modal Parameters of Machine Tools Under Periodic Cutting Excitation},
year={2020},
volume={8},
number={},
pages={120850-120858},
abstract={The dynamics of a machine tool have an important influence on the quality and efficiency of the machining process. In this paper, the modal parameters identification method of machine tools under normal cutting excitation is proposed based on the fact that the random components in the cutting force can provide an effective excitation. However, the generated cutting force during machining contains strong periodic components and does not satisfy the white noise assumption. Directly applying the operational modal analysis (OMA) method will face serious harmonic interference. Therefore, it is difficult to ensure the accuracy of the identified parameters. The cepstrum editing method is proposed to eliminate the periodic component. And the modal parameters are extracted from the remaining signal by using the OMA method. The experimental results show that the proposed method works well under normal cutting conditions.},
keywords={Force;Machine tools;Vibrations;Harmonic analysis;Cepstrum;Tools;Milling;Cepstrum editing;cutting processing;harmonic elimination;machine tool;modal parameters},
doi={10.1109/ACCESS.2020.3006226},
ISSN={2169-3536},
month={},}
@ARTICLE{8307048,
author={Wang, Zi and Li, Chengcheng and Shao, Huiru and Sun, Jiande},
journal={IEEE Access},
title={Eye Recognition With Mixed Convolutional and Residual Network (MiCoRe-Net)},
year={2018},
volume={6},
number={},
pages={17905-17912},
abstract={Although iris recognition has achieved big successes on biometric identification in recent years, difficulties in the collection of iris images with high resolution and in the segmentation of valid regions prevent it from applying to large-scale practical applications. In this paper, we present an eye recognition framework based on deep learning, which relaxes the data collection procedure, improves the anti-fake quality, and promotes the performance of biometric identification. Specifically, we propose and train a mixed convolutional and residual network (MiCoRe-Net) for the eye recognition task. Such an architecture inserts a convolutional layer between every two residual layers and takes the advantages from both of convolutional networks and residual networks. Experiment results show that the proposed approach achieves accuracies of 99.08% and 96.12% on the CASIA-Iris-IntervalV4 and the UBIRIS.v2 datasets, respectively, which outperforms other classical classifiers and deep neural networks with other architectures.},
keywords={Iris recognition;Convolutional neural networks;Feature extraction;Image segmentation;Training;Task analysis;Eye recognition;iris recognition;deep learning;convolutional neural network;deep residual network;mixed convolutional and residual network (MiCoReNet)},
doi={10.1109/ACCESS.2018.2812208},
ISSN={2169-3536},
month={},}
@ARTICLE{9187621,
author={Ashraf, Shahzad and Gao, Mingsheng and Chen, Zhengming and Naeem, Hamad and Ahmad, Arshad and Ahmed, Tauqeer},
journal={IEEE Access},
title={Underwater Pragmatic Routing Approach Through Packet Reverberation Mechanism},
year={2020},
volume={8},
number={},
pages={163091-163114},
abstract={The advances in underwater sensor communication has become imperative getting up-to-date information about underwater happenings, especially when world has already faced the calamity like Tsunami. The underwater environment possessed freak and unpredictable movements which becomes more harsh time to time. The sensor nodes deployed under such juncture are the main source of information which in fact, facing numerous challenges. These nodes are mainly energy-constrained and rely on limited battery source. Due to most intricated underwater routing architecture, the biggest detriment is the limited battery lifespan. Therefore, it is imperative to adopt the pragmatic and possible alternate to improve the life expectancy of these sensor nodes. The solution of such shortcomings and identifying the varieties of impingements impelled by forwarding node on battery lifespan during packet transmission course are meticulously explored by developing an Underwater Pragmatic Routing Approach through Packet Reverberation mechanism (UPRA-PR). It is a novel approach and never considered in past. Through Packet Reverberation technique, the use of energy has been confined and the desired outcomes are achieved in four phases. In the first phase, the eligibility criteria for both packet and nodes have been computed by setting the Node Depth Factor ( $\text{N}_{\mathrm {df}}$ ). Second phase formulates the forwarding relay node mechanism and rummage out the path failure by complying a Data Rate criterion $D_{0}$ . The selection of the shrewd communication link is established in third phase by considering an Accepted Link Quality (ALQ) factor. The fourth phase where most prominent developments has been made regarding impingements effects on the battery lifespan left by the forwarding node after the packet transmission. The UPRA-PR performance metrics are assessed by staging extensive NS2 simulation with AquaSim 2.0 and compared to state-existing routing protocols i.e., DBR, H2DAB, GEDAR and FBR for Packet dissemination ratio, Path failure, Point-to-point delay estimation, System energy consumption, Network lifespan, Forwarding node impingement and Network throughput. The simulation results have ratified the UPRA-PR performance and justified the statements made in this respect.},
keywords={Batteries;Routing;Reverberation;Relays;Energy consumption;Throughput;Pragmatics;Packet reverberation;opportunistic routing;dearth node;energy efficiency;network throughput;forwarding relay node;sagacious link},
doi={10.1109/ACCESS.2020.3022565},
ISSN={2169-3536},
month={},}
@ARTICLE{8834787,
author={Shah, Umm E Mariya and Chiew, Thiam Kian},
journal={IEEE Access},
title={A Systematic Literature Review of the Pain Management Mobile Applications: Toward Building a Conceptual Model},
year={2019},
volume={7},
number={},
pages={131512-131526},
abstract={In healthcare, mobile-based interventions support the improvement of clinical process and result in a positive behavioral change and improve the patients' health condition. This study aims at reviewing mobile applications documented for pain management in the scientific databases, to identify the key factors that are vital for pain management. In this research, a systematic literature review was conducted on the selected studies collected from five scientific databases: Medline, PubMed, EMBASE, Web of Science and Scopus. After applying the inclusion and exclusion criteria and performing the quality assessment, twenty-five studies were finalized. It has been observed that the apps were not all-inclusive in features to provide an effective pain self-management solution. As found from the review, the general features of the pain management mobile applications are pain information, pain coping strategy, social support, sub-goals and achievements, self-reporting, feedback, and patient report. Some apps involved psychological interventions. A prominent technique found was cognitive behavior therapy. This study has contributed to the body of knowledge by proposing a conceptual model in guiding the development of pain management mobile applications. The conceptual model was evaluated by a panel of experts to evaluate comprehensiveness, accuracy, and dependencies among the elements of the model, and the appropriateness of the proposed model. Experts recognized the importance of pain management and provided positive feedback to the proposed model.},
keywords={Pain;Mobile applications;Systematics;Bibliographies;Databases;Usability;Conceptual model;mobile applications;m-health;pain;self-management;systematic literature review},
doi={10.1109/ACCESS.2019.2940772},
ISSN={2169-3536},
month={},}
@ARTICLE{9245582,
author={Bedru, Hayat D. and Zhao, Wenhong and Alrashoud, Mubarak and Tolba, Amr and Guo, He and Xia, Feng},
journal={IEEE Access},
title={TOSNet: A Topic-Based Optimal Subnetwork Identification in Academic Networks},
year={2020},
volume={8},
number={},
pages={201015-201027},
abstract={Subnetwork identification plays a significant role in analyzing, managing, and comprehending the structure and functions in big networks. Numerous approaches have been proposed to solve the problem of subnetwork identification as well as community detection. Most of the methods focus on detecting communities by considering node attributes, edge information, or both. This study focuses on discovering subnetworks containing researchers with similar or related areas of interest or research topics. A topic-aware subnetwork identification is essential to discover potential researchers on particular research topics and provide quality work. Thus, we propose a topic-based optimal subnetwork identification approach (TOSNet). Based on some fundamental characteristics, this paper addresses the following problems: 1)How to discover topic-based subnetworks with a vigorous collaboration intensity? 2) How to rank the discovered subnetworks and single out one optimal subnetwork? We evaluate the performance of the proposed method against baseline methods by adopting the modularity measure, assess the accuracy based on the size of the identified subnetworks, and check the scalability for different sizes of benchmark networks. The experimental findings indicate that our approach shows excellent performance in identifying contextual subnetworks that maintain intensive collaboration amongst researchers for a particular research topic.},
keywords={Collaboration;Image edge detection;Indexes;Clustering algorithms;Measurement;Semantics;Object recognition;Academic social networks;collaboration intensity;network science;subnetwork identification;subnetwork ranking;topic modeling},
doi={10.1109/ACCESS.2020.3034997},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9176504,
author={Zhang, Yi and Wang, Ziyue and Xu, Jingjing and Liu, Yiqi and Zhou, Bin and Zhang, Nan and He, Mingjie and Fan, Jipeng and Liu, Xianbo and Zhao, Jian and Yang, Qin and Zhang, Lifu and Cao, Yu and Su, Steven},
booktitle={2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)},
title={Association Between Consecutive Ambient Air Pollution and Chronic Obstructive Pulmonary Disease Hospitalization: Time Series Study During 2015-2017 in Chengdu China},
year={2020},
volume={},
number={},
pages={5378-5381},
abstract={This paper investigates the association between consecutive ambient air pollution and Chronic Obstructive Pulmonary Disease (COPD) hospitalization in Chengdu China. The three-year (2015-2017) time series data for both ambient air pollutant concentrations and COPD hospitalizations in Chengdu are approved for the study. The big data statistic analysis shows that Air Quality Index (AQI) exceeded the lighted air polluted level in Chengdu region are mainly attributed to particulate matters (i.e., PM2.5 and PM10). The time series study for consecutive ambient air pollutant concentrations reveal that AQI, PM2.5, and PM10 are significantly positive correlated, especially when the number of consecutive polluted days is greater than nine days. The daily COPD hospitalizations for every 10 μg/m3 increase in PM2.5 and PM10 indicate that consecutive ambient air pollution can lead to an appearance of an elevation of COPD admissions, and also present that dynamic responses before and after the peak admission are different. Support Vector Regression (SVR) is then used to describe the dynamics of COPD hospitalizations to consecutive ambient air pollution. These findings will be further developed for region specific, hospital early notifications of COPD in responses to consecutive ambient air pollution.},
keywords={Air pollution;Hospitals;Time series analysis;Correlation;Diseases;Indexes},
doi={10.1109/EMBC44109.2020.9176504},
ISSN={2694-0604},
month={July},}
@ARTICLE{9516909,
author={Fan, Qiang and Song, Xiaonan and Shi, Yue and Gao, Rui},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Influencing Factors of Spatial Heterogeneity of Land Surface Temperature in Nanjing, China},
year={2021},
volume={14},
number={},
pages={8341-8349},
abstract={The environment and climate significantly affect the land surface temperature (LST) of a city. Previous studies have revealed that LST exhibits significant spatial heterogeneity primarily caused by a combination of natural factors and human activities. Based on this, the introduction of point of interest data of the “production-living-ecological space” divides the influencing pattern into a comprehensive description of human activities supplemented by natural factors, resulting in the precise influencing factors of spatial heterogeneity of LST. Taking Nanjing (Jiangsu Province, China) as a case study, this study uses Landsat-8 remote sensing images, point of interest data, and other data to establish a geographically weighted regression model that combines natural factors and human activities. The main research results are as follows: First, the LST of Nanjing ranged from 19.9 °C to 47.6 °C, whereas the distribution trend was “low at both ends and high in the middle.” Second, there is no multicollinearity of the influencing factors, the fitting degree of LST and each influencing factor reached 0.87. The regression coefficients were high and exhibited both positive and negative values, implying that spatial heterogeneity exists among the influencing factors and LST. Finally, the ranking of how all factors influence the LST followed the order of water area > forest and grassland > ecological space > slope > production space > elevation > living space. The research results have practical significance for improving the quality of life of urban residents and providing a critical theoretical basis for optimizing urban human settlements.},
keywords={Land surface temperature;Urban areas;Remote sensing;Earth;Production;Artificial satellites;Temperature;Geographically weighted regression (GWR);human settlements;land surface temperature (LST);Nanjing;spatial heterogeneity},
doi={10.1109/JSTARS.2021.3105582},
ISSN={2151-1535},
month={},}
@ARTICLE{9093135,
author={Sun, Ziheng and Di, Liping and Fang, Hui and Burgess, Annie},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Deep Learning Classification for Crop Types in North Dakota},
year={2020},
volume={13},
number={},
pages={2200-2213},
abstract={Recently, agricultural remote sensing community has endeavored to utilize the power of artificial intelligence (AI). One important topic is using AI to make the mapping of crops more accurate, automatic, and rapid. This article proposed a classification workflow using deep neural network (DNN) to produce high-quality in-season crop maps from Landsat imageries for North Dakota. We use historical crop maps from the agricultural department and North Dakota ground measurements as training datasets. Processing workflows are created to automate the tedious preprocessing, training, testing, and postprocessing workflows. We tested this hybrid solution on new images and received accurate results on major crops such as corn, soybean, barley, spring wheat, dry beans, sugar beets, and alfalfa. The pixelwise overall accuracy in all three test regions is over 82% for all land types (including noncrop land), which is the same level of accuracy as the U.S. Department of Agriculture Cropland Data Layer. The texture of DNN maps is more consistent with fewer noises, which is more comfortable to read. We find DNN is better on recognizing big farmlands than recognizing the scattered wetlands and suburban regions in North Dakota. The model trained on multiple scenes of multiple years and months yields higher accuracy than any of the models trained only on a single scene, a single month, or a single year. These results reflect that DNN can produce reliable in-season maps for major crops in North Dakota big farms and could provide a relatively accurate reference for the minor crops in scattered wetland fields.},
keywords={Agriculture;Remote sensing;Earth;Satellites;Artificial satellites;Feature extraction;Task analysis;Agricultural remote sensing;crop mapping;deep neural network (dnn);geoprocessing workflow;image classification;Landsat;North Dakota},
doi={10.1109/JSTARS.2020.2990104},
ISSN={2151-1535},
month={},}
@ARTICLE{9359752,
author={Li, Linguo and Sun, Lijuan and Xue, Yu and Li, Shujing and Huang, Xuwen and Mansour, Romany Fouad},
journal={IEEE Access},
title={Fuzzy Multilevel Image Thresholding Based on Improved Coyote Optimization Algorithm},
year={2021},
volume={9},
number={},
pages={33595-33607},
abstract={Due to the computational complexity of multilevel image thresholding, Swarm Intelligence Optimization Algorithm (SIOA) has been widely applied to improve the calculation efficiency. Therefore, more and more attention has been paid to exploring the application of the latest SIOA in multilevel segmentation. This article takes Otsu and fuzzy entropy as the objective functions, using Coyote Optimization Algorithm (COA) for multilevel thresholds optimization selection, through fuzzy median aggregation of local neighborhood information and then forms the Fuzzy Coyote Optimization Algorithm (FCOA), so that the thresholding image segmentation can be achieved in the end. To prevent the COA algorithm from falling into the local optimum, this article follows the differential evolution strategy adopted by the standard COA, using the number of iterations to construct the differential scaling factor to form the Improved Coyote Optimization Algorithm (ICOA). The experimental results show that fuzzy Kapur entropy and fuzzy median value aggregation-based ICOA(FICOA) achieves better image segmentation quality. Compared with Grey Wolf Optimizer (GWO), Fuzzy Modified Quick Artificial Bee Colony and Aggregation Algorithm (FMQABCA) and Fuzzy Modified Discrete Grey Wolf Optimizer and Aggregation Algorithm (FMDGWOA), FCOA and FICOA have certain advantages in visual effects of image segmentation and PSNR, FSIM evaluation indices. Particularly compared with GWO (also a wolf evolutionary algorithm), FICOA shows significant advantages.},
keywords={Image segmentation;Optimization;Linear programming;Entropy;Particle swarm optimization;Histograms;Heuristic algorithms;Coyote optimization algorithm;information entropy;image segmentation;multilevel thresholding},
doi={10.1109/ACCESS.2021.3060749},
ISSN={2169-3536},
month={},}
@ARTICLE{9505639,
author={Shi, Huaifeng and Pan, Chengsheng and Wang, Yingzhi},
journal={IEEE Access},
title={BS-HTIS: Buffer Sizing for Heterogeneous Traffic and Integrated System},
year={2021},
volume={9},
number={},
pages={115237-115245},
abstract={Buffer sizing for switching and routing devices is of significance for guaranteeing the Quality of Service (QoS) of critical services on the Internet of Things (IoT), continuously evolving scheduling mechanisms and complex traffic characteristics pose new challenges for the traditional method of static buffer sizing based on rule-of-thumb. In this paper, the scope of buffer sizing is extended from a basic scheduling system under homogeneous arrival traffic input to an integrated scheduling system under heterogeneous arrival traffic input which is more ubiquitous. In this context, Voices, videos and other heterogeneous data in the IoT are categorized into short-range-dependent (SRD) and long-range-dependent (LRD) traffic, and the integrated scheduling system is decomposed into single-server-single-queue (SSSQ) systems by not only decoupling the complex dependencies among heterogeneous traffic inputs but also taking the impact of SRD and LRD traffic burstiness on the buffer sizing into account. On this basis, expressions for the relation between the minimum buffer size and the maximum overflow probability are presented. The numerical analysis results and simulation analysis results reveal that the average arrival rate, traffic burst level and scheduling priority are positively correlated with the required buffer size, and once the overflow probability is set, the minimum buffer size can be determined correspondingly. The achievements of this paper will provide theoretical guidance for IoT manufacturers and technicians to set buffers more reasonably and use resources more efficiently.},
keywords={Job shop scheduling;Quality of service;Queueing analysis;Markov processes;Diffserv networks;Bandwidth;Videos;xsSRD traffic;LRD traffic;integrated scheduling;buffer sizing},
doi={10.1109/ACCESS.2021.3102423},
ISSN={2169-3536},
month={},}
@ARTICLE{9641843,
author={Saleh, Majd and Abbas, Manuel and Prud’Homm, Joaquim and Somme, Dominique and Le Bouquin Jeannès, Régine},
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
title={A Reliable Fall Detection System Based on Analyzing the Physical Activities of Older Adults Living in Long-Term Care Facilities},
year={2021},
volume={29},
number={},
pages={2587-2594},
abstract={Fall detection systems are designed in view to reduce the serious consequences of falls thanks to the early automatic detection that enables a timely medical intervention. The majority of the state-of-the-art fall detection systems are based on machine learning (ML). For training and performance evaluation, they use some datasets that are collected following predefined simulation protocols i.e. subjects are asked to perform different types of activities and to repeat them several times. Apart from the quality of simulating the activities, protocol-based data collection results in big differences between the distribution of the activities of daily living (ADLs) in these datasets in comparison with the actual distribution in real life. In this work, we first show the effects of this problem on the sensitivity of the ML algorithms and on the interpretability of the reported specificity. Then, we propose a reliable design of an ML-based fall detection system that aims at discriminating falls from the ambiguous ADLs. The latter are extracted from 400 days of recorded activities of older adults experiencing their daily life. The proposed system can be used in neck- and wrist-worn fall detectors. In addition, it is invariant to the rotation of the wearable device. The proposed system shows 100% of sensitivity while it generates an average of one false positive every 25 days for the neck-worn device and an average of one false positive every 3 days for the wrist-worn device.},
keywords={Fall detection;Sensitivity;Temperature measurement;Sea measurements;Pressure measurement;Performance evaluation;Detectors;Fall detection;wearable sensors;machine learning;elderly health care},
doi={10.1109/TNSRE.2021.3133616},
ISSN={1558-0210},
month={},}
@ARTICLE{8845592,
author={Saldana-Perez, Magdalena and Torres-Ruiz, Miguel and Moreno-Ibarra, Marco},
journal={IEEE Access},
title={Geospatial Modeling of Road Traffic Using a Semi-Supervised Regression Algorithm},
year={2019},
volume={7},
number={},
pages={177376-177386},
abstract={Nowadays, big cities are facing many challenges with respect to traffic congestion, climate change, air and water pollution, among others. Thus, smart cities are intended to improve the life quality of the citizens, tackling such issues with the integration of information and communication technologies to reduce the impact and achieve a well-being state of citizens. In this work, a model to predict the traffic congestion applying a support vector machine method is proposed. In addition, a crowdsourcing approach based on mining the Twitter social networks collecting events associated with the traffic is also proposed. The main contribution of this research is focused on providing a methodology that characterizes the traffic congestion analyzing crowd-sensed data from a geospatial perspective. This approach was implemented over the Mexico City as a case study, in order to forecast possible future traffic events in the city, in which the citizens share their particular situation to discover alternatives routes for avoiding the traffic congestion. Future works are oriented towards designing mobile applications in order to introduce the proposed approach and integrate information from multiple platforms and navigation systems.},
keywords={Smart cities;Roads;Support vector machines;Machine learning;Prediction algorithms;Geospatial analysis;Climate change;Urban areas;Geospatial modeling;machine learning;regression;road traffic;urban computing;volunteered geographic information},
doi={10.1109/ACCESS.2019.2942586},
ISSN={2169-3536},
month={},}
@ARTICLE{8601324,
author={Yang, Yun and Nan, Fengtao and Yang, Po and Meng, Qiang and Xie, Yingfu and Zhang, Dehai and Muhammad, Khan},
journal={IEEE Access},
title={GAN-Based Semi-Supervised Learning Approach for Clinical Decision Support in Health-IoT Platform},
year={2019},
volume={7},
number={},
pages={8048-8057},
abstract={With the development of the Internet of Things (IoT) technology, its application in the medical field becomes more and more extensive. However, with a dramatic increase in medical data obtained from the IoT-based health service system, labeling a large number of medical data requires high cost and relevant domain knowledge. Therefore, how to use a small number of labeled medical data reasonably to build an efficient and high-quality clinical decision support model in the IoT-based platform has been an urgent research topic. In this paper, we propose a novel semi-supervised learning approach in association with generative adversarial networks (GANs) for supporting clinical decision making in the IoT-based health service system. In our approach, GAN is adopted to not only increase the number of labeled data but also to compensate the imbalanced labeled classes with additional artificial data in order to improve the semi-supervised learning performance. Extensive evaluations on a collection of benchmarks and real-world medical datasets show that the proposed technique outperforms the others and provides a potential solution for practical applications.},
keywords={Semisupervised learning;Generative adversarial networks;Gallium nitride;Support vector machines;Generators;Supervised learning;Medical diagnostic imaging;Internet of Things;clinical decision support;semi-supervised learning;generative adversarial networks},
doi={10.1109/ACCESS.2018.2888816},
ISSN={2169-3536},
month={},}
@ARTICLE{9293355,
author={Monteiro, Luís Guilherme and Macêdo, Wilson Negrão and Cavalcante, Renato L. and Júnior, Wilson Braga and Torres, Pedro Ferreira and Brito, Thiago R. and Silva, Márcio Melquiades and Lopes, Bruno Marciano and Fraga, Juliano M. and Alves, Danilo Derick and Chase, Otávio A. and Boaventura, Wallace C.},
journal={IEEE Access},
title={Field I-V Curve Measurements Methodology at String Level to Monitor Failures and the Degradation Process: A Case Study of a 1.42 MWp PV Power Plant},
year={2020},
volume={8},
number={},
pages={226845-226865},
abstract={This research shows the methodology results for outdoor characterization by the I-V curves of the PV power generators at the biggest Brazilian rooftop PV power plant mounted at the Mineirão Football Stadium. The experimental results of the methodology were obtained by a measurement campaign using two capacitive loads. This work identified a significant difference when these measurements were extrapolated to standard test conditions (STC) and compared to the rated power data shown on the PV modules' label at strings: between 24.12% to 26.19% lower. Results showed a contribution of soiling in a power reduction of about 6.7% on average. Additionally, it was considered an uncertain method, and AC electrical parameters were monitored. The reference PV module's calibration was carried out with great attention - the measurements were made with the PV device under test (DUT), guaranteeing the same real operating conditions for the reference PV module and the PV string as DUT. This measurement method has allowed a better characterization of the uncertainty associated with the measurement process. Finally, this study demonstrates the importance of investigating the actual power of the installed PV generators and how these measurements are essential to guaranteeing energy production following the owner's expectations.},
keywords={Power measurement;Uncertainty;Generators;Sports;Soil measurements;Photovoltaic systems;Monitoring;PV power plants;on-site I-V curve measurements;uncertainty method;translation to STC;commissioning/quality assurance;dust and soiling degradation taxes on PV modules},
doi={10.1109/ACCESS.2020.3044832},
ISSN={2169-3536},
month={},}
@ARTICLE{8409453,
author={Yang, Hongjuan and Li, Bo and Liu, Gongliang and Ma, Ruofei},
journal={IEEE Access},
title={Physical-Layer Network Coding Based Multi-User Cooperative Relay Transmission With Multi-Antennas in Cognitive Wireless Networks},
year={2018},
volume={6},
number={},
pages={40189-40197},
abstract={A promising way to improve the performance and guarantee the quality of service (QoS) of cognitive wireless cooperative relay networks is to jointly employ physical-layer network coding (PNC) and multi-antenna space-time block coding. This paper proposes a new multi-user transmission coding scheme, cooperative quadrature PNC (CQPNC), for cognitive wireless networks. In CQPNC scheme, two source nodes (users) first use quadrature carriers to transmit signals simultaneously, which are received and processed by a cooperative relay node using the PNC method. The processed signal is then transmitted to the destination node, which makes a combination of the signals from the direct path and relay path to obtain the information transmitted by the source node. Simulation results in difference cases of cognitive wireless networks show that the CQPNC scheme outperforms the traditional cooperation and cooperative network coding transmission schemes on the performance of anti-noise and throughput.},
keywords={Relays;Silicon;Broadcasting;Cooperative systems;Network coding;Receiving antennas;Wireless communication;Cognitive radio;cooperative relay networks;cooperative quadrature physical-layer network coding;multi-antennas;decode-and-forward},
doi={10.1109/ACCESS.2018.2854835},
ISSN={2169-3536},
month={},}
@ARTICLE{9517271,
author={Xiong, Zhangxi and Guo, Qing and Liu, Mingliang and Li, An},
journal={IEEE Access},
title={Pan-Sharpening Based on Panchromatic Colorization Using WorldView-2},
year={2021},
volume={9},
number={},
pages={115523-115534},
abstract={In order to overcome the lack of the multispectral image (MS) and adequately preserve the spatial information of panchromatic (PAN) image and the spectral information of MS image, this study proposes a method which adds the spectral information of the prior MS to the prior PAN during training, and only the posterior PAN is needed for predicting. Firstly, we introduce the autoencoder model based on image colorization and discuss its feasibility in the field of multi-band remote sensing image pan-sharpening. Then, the image quality evaluation functions including spatial and spectral indexes are formed as the loss function to control the image colorization model. Because the loss function contains spatial and spectral evaluation indexes, it could directly calculate the loss between the network output and the label considering characteristics of remote sensing images. Besides, the training data in our model is original PAN, this means that it is not necessary to make the simulated degraded MS and PAN data for training which is a big difference from most existing deep learning pan-sharpening methods. The new loss function including the spectral and spatial quality instead of the general MSE (mean square error), only the original PAN instead of the simulated degraded MS + PAN to be inputted, only the spectral feature instead of the direct fusion result to be learned, these three aspects change the current learning framework and optimization rule of deep learning pan-sharpening. Finally, thousands of remote sensing images from different scenes are adopted to make the training dataset to verify the effectiveness of the proposed method. In addition, we selected seven representative pan-sharpening algorithms and four widely recognized objective fusion metrics to evaluate and compare the performance on the WorldView-2 experimental data. The results show that the proposed method achieves optimal performance in terms of both the subjective visual effect and the object assessment.},
keywords={Spatial resolution;Feature extraction;Training;Deep learning;Decoding;Image resolution;Convolution;Pan-sharpening;deep learning;multispectral image;panchromatic image;image colorization;loss function},
doi={10.1109/ACCESS.2021.3104321},
ISSN={2169-3536},
month={},}
@ARTICLE{9312039,
author={Hijji, Mohammad and Alam, Gulzar},
journal={IEEE Access},
title={A Multivocal Literature Review on Growing Social Engineering Based Cyber-Attacks/Threats During the COVID-19 Pandemic: Challenges and Prospective Solutions},
year={2021},
volume={9},
number={},
pages={7152-7169},
abstract={The novel coronavirus (COVID-19) pandemic has caused a considerable and long-lasting social and economic impact on the world. Along with other potential challenges across different domains, it has brought numerous cybersecurity challenges that must be tackled timely to protect victims and critical infrastructure. Social engineering–based cyber-attacks/threats are one of the major methods for creating turmoil, especially by targeting critical infrastructure, such as hospitals and healthcare services. Social engineering–based cyber-attacks are based on the use of psychological and systematic techniques to manipulate the target. The objective of this research study is to explore the state-of-the-art and state-of-the-practice social engineering–based techniques, attack methods, and platforms used for conducting such cybersecurity attacks and threats. We undertake a systematically directed Multivocal Literature Review (MLR) related to the recent upsurge in social engineering–based cyber-attacks/threats since the emergence of the COVID-19 pandemic. A total of 52 primary studies were selected from both formal and grey literature based on the established quality assessment criteria. As an outcome of this research study; we discovered that the major social engineering–based techniques used during the COVID-19 pandemic are phishing, scamming, spamming, smishing, and vishing, in combination with the most used socio-technical method: fake emails, websites, and mobile apps used as weapon platforms for conducting successful cyber-attacks. Three types of malicious software were frequently used for system and resource exploitation are; ransomware, trojans, and bots. We also emphasized the economic impact of cyber-attacks performed on different organizations and critical infrastructure in which hospitals and healthcare were on the top targeted infrastructures during the COVID-19 pandemic. Lastly, we identified the open challenges, general recommendations, and prospective solutions for future work from the researcher and practitioner communities by using the latest technology, such as artificial intelligence, blockchain, and big data analytics.},
keywords={COVID-19;Pandemics;Organizations;Standards organizations;Computer hacking;Buildings;Social networking (online);Multivocal literature review;social engineering;COVID-19;security and privacy;prospective solutions;cyber-attacks and threats},
doi={10.1109/ACCESS.2020.3048839},
ISSN={2169-3536},
month={},}
@ARTICLE{9358144,
author={Jamil, Faisal and Iqbal, Naeem and Imran and Ahmad, Shabir and Kim, Dohyeun},
journal={IEEE Access},
title={Peer-to-Peer Energy Trading Mechanism Based on Blockchain and Machine Learning for Sustainable Electrical Power Supply in Smart Grid},
year={2021},
volume={9},
number={},
pages={39193-39217},
abstract={It is expected that peer to peer energy trading will constitute a significant share of research in upcoming generation power systems due to the rising demand of energy in smart microgrids. However, the on-demand use of energy is considered a big challenge to achieve the optimal cost for households. This paper proposes a blockchain-based predictive energy trading platform to provide real-time support, day-ahead controlling, and generation scheduling of distributed energy resources. The proposed blockchain-based platform consists of two modules; blockchain-based energy trading and smart contract enabled predictive analytics modules. The blockchain module allows peers with real-time energy consumption monitoring, easy energy trading control, reward model, and unchangeable energy trading transaction logs. The smart contract enabled predictive analytics module aims to build a prediction model based on historical energy consumption data to predict short-term energy consumption. This paper uses real energy consumption data acquired from the Jeju province energy department, the Republic of Korea. This study aims to achieve optimal power flow and energy crowdsourcing, supporting energy trading among the consumer and prosumer. Energy trading is based on day-ahead, real-time control, and scheduling of distributed energy resources to meet the smart grid’s load demand. Moreover, we use data mining techniques to perform time-series analysis to extract and analyze underlying patterns from the historical energy consumption data. The time-series analysis supports energy management to devise better future decisions to plan and manage energy resources effectively. To evaluate the proposed predictive model’s performance, we have used several statistical measures, such as mean square error and root mean square error on various machine learning models, namely recurrent neural networks and alike. Moreover, we also evaluate the blockchain platform’s effectiveness through hyperledger calliper in terms of latency, throughput, and resource utilization. Based on the experimental results, the proposed model is effectively used for energy crowdsourcing between the prosumer and consumer to attain service quality.},
keywords={Blockchain;Smart contracts;Predictive models;Crowdsourcing;Machine learning;Peer-to-peer computing;Energy consumption;Energy trading;energy prediction;predictive analysis;machine learning;blockchain},
doi={10.1109/ACCESS.2021.3060457},
ISSN={2169-3536},
month={},}
@ARTICLE{9146664,
author={Jiang, Xingyu and Li, Jiazhen and Lu, Yitao and Tian, Guangdong},
journal={IEEE Access},
title={Design of Reverse Logistics Network for Remanufacturing Waste Machine Tools Based on Multi-Objective Gray Wolf Optimization Algorithm},
year={2020},
volume={8},
number={},
pages={141046-141056},
abstract={The high uncertainty of the recovery time, quantity and quality of waste machine tools has led to dynamic changes in the recycling logistics network and is difficult to plan. Considering factors such as recycling efficiency, cost, and carbon emissions, an optimized model for the recycling network of waste machine tool recycling with the goal of minimizing total operating costs and total carbon tax penalties was proposed. The optimization of the combination of recycling efficiency, cost and carbon emissions of waste machine tools has been achieved. For model solving, an optimization model solving algorithm based on the multi-object gray wolf algorithm was proposed. Problems that are difficult to apply due to too slow convergence speed and too many solving parameters were solved. Finally, the recycling process of waste machine tools of a machine tool remanufacturing enterprise was taken as an example, and the proposed model and algorithm were used to optimize the logistics network of waste machine tools recycling. The results show that the optimal scheme of the optimization model of the recycling network of waste machine tools can be obtained from the proposed model. The gray wolf algorithm is superior to the multi-objective non-dominated sorting genetic algorithm in both the convergence speed and the total cost of recovered logistics. Therefore, the validity and feasibility of the model and algorithm in this paper have been verified.},
keywords={Recycling;Machine tools;Reverse logistics;Production facilities;Optimization;Carbon dioxide;Waste machine tool;reverse logsitics network;recycle;gray wolf algorithm},
doi={10.1109/ACCESS.2020.3011509},
ISSN={2169-3536},
month={},}
@ARTICLE{8572684,
author={Chu, Phuong Minh and Sung, Yunsick and Cho, Kyungeun},
journal={IEEE Access},
title={Generative Adversarial Network-Based Method for Transforming Single RGB Image Into 3D Point Cloud},
year={2019},
volume={7},
number={},
pages={1021-1029},
abstract={Three-dimensional (3D) point clouds are important for many applications, including object tracking and 3D scene reconstruction. Point clouds are usually obtained from laser scanners, but their high cost impedes the widespread adoption of this technology. We propose a method to generate the 3D point cloud corresponding to a single red–green–blue (RGB) image. The method retrieves high-quality 3D data from two-dimensional (2D) images captured by conventional cameras, which are generally less expensive. The proposed method comprises two stages. First, a generative adversarial network generates a depth image estimation from a single RGB image. Then, the 3D point cloud is calculated from the depth image. The estimation relies on the parameters of the depth camera employed to generate the training data. The experimental results verify that the proposed method provides high-quality 3D point clouds from single 2D images. Moreover, the method does not require a PC with outstanding computational resources, further reducing implementation costs, as only a moderate-capacity graphics processing unit can efficiently handle the calculations.},
keywords={Three-dimensional displays;Gallium nitride;Two dimensional displays;Cameras;Generators;Training;Generative adversarial networks;Artificial intelligence;image processing;sensors;machine learning;neural networks},
doi={10.1109/ACCESS.2018.2886213},
ISSN={2169-3536},
month={},}
@ARTICLE{8886417,
author={Mishra, Ashish and McDonnell, William and Wang, Jing and Rodriguez, Daniel and Li, Changzhi},
journal={IEEE Access},
title={Intermodulation-Based Nonlinear Smart Health Sensing of Human Vital Signs and Location},
year={2019},
volume={7},
number={},
pages={158284-158295},
abstract={This paper discusses the use of a nonlinear sensing technology based on radio frequency (RF) intermodulation response to track both the vital signs and location of human subjects. Smart health sensing was realized through the use of a wearable nonlinear tag and an intermodulation-based nonlinear sensor operating in both Doppler and frequency shift keying (FSK) modes. The Doppler mode was used to detect the heartbeat and breathing of the target subject while human subject localization was achieved in the FSK mode. One of the key advantages of this nonlinear smart sensor system was clutter rejection. This system identified the signal reflected from the wearable nonlinear tag and suppressed undesired signals and interferences that were reflected from other objects. The wearable tags used for the experiments were passive, hence they did not require any battery or power supply for their operation. Since the respiration signal is typically stronger than the heartbeat signal, the nonlinear detection setup was designed such that the respiratory signal receives less gain to avoid its sidelobes and harmonics from interfering heartbeat signal detection. This enhanced the heartbeat signal quality so that the cardiac activity could be easily tracked. Four types of experiments were performed on multiple subjects to demonstrate the advantages of this intermodulation-based nonlinear smart health sensing system. Previously, 2nd order harmonics were utilized for target localization and vital sign monitoring. However, these 2nd order harmonics suffer from high path loss and licensing issues. In this paper, target localization and smart health sensing were realized using 3rd order intermodulation with less path loss and no licensing issues compared with its harmonic counterparts. The experiment performed in nonlinear FSK mode was able to detect and locate the source of motion with high accuracy. Similarly, vital signs were recorded in the nonlinear Doppler mode. The design effectively made the amplitude of the heartbeat signal component more prominent, so that the sidelobes and harmonics of respiration do not suppress heartbeat signal.},
keywords={Frequency shift keying;Harmonic analysis;Heart beat;Biomedical monitoring;Sensors;Monitoring;Intermodulation;nonlinear response;passive tag;target localization;vital signs;wearable},
doi={10.1109/ACCESS.2019.2950347},
ISSN={2169-3536},
month={},}
@ARTICLE{9180261,
author={Liu, Yishu and Lyu, Pin and Gao, Wei},
journal={IEEE Access},
title={Consumer Marketing Brand Cultivation Path Based on Image Recognition Technology},
year={2020},
volume={},
number={},
pages={1-1},
abstract={In recent years, with the improvement of national income levels, people's consumption concept has begun to transition from enjoyment to quality, and people pay more and more attention to food safety and health. The research of intelligent brand image recognition technology is the most important to prevent the spread of counterfeit and shoddy products and brand cultivation. This article mainly studies the cultivation method of consumer marketing brand based on image recognition technology. This article is based on image recognition technology. Consumers can obtain relevant information and brand culture through the mobile phone camera function anytime, anywhere, establish a bridge between consumers and company brands, broaden the company's channels for obtaining consumer information big data, and deeply tap consumer potential. Demand, further enrich the brand cultivation methods, and achieve a new terminal service experience of "efficient, convenient and satisfactory" for consumers. In the experiment of this paper, the shape of the product packaging is relatively irregular, so there are fewer feature points generated in this part, which leads to the inaccurate extraction and distribution of visual words, which reduces the accuracy of single product extraction slightly, but the overall result is good. The maximum number of iterations in this experiment is 40, which can meet the requirements of most pictures. From the perspective of consumer perception, this paper explores and proposes a new path for consumer marketing brand cultivation of image recognition technology, promotes the deep integration of new artificial intelligence technology and brand industry, and fills the gaps in industry-related technologies and applications.},
keywords={Image color analysis;Image recognition;Consumer behavior;Customer relationship management;Psychology;Feature extraction;Brand management;Behavioral sciences;Image Recognition;HSI Color System;Marketing Brand;Brand Loyalty;Product Brand Recognition},
doi={10.1109/ACCESS.2020.3018112},
ISSN={2169-3536},
month={},}
@ARTICLE{9775692,
author={Chude-Okonkwo, Uche K and Paul, Babu S. and Vasilakos, Athanasios A.},
journal={IEEE Access},
title={Enabling Precision Medicine via Contemporary and Future Communication Technologies: A Survey},
year={2022},
volume={},
number={},
pages={1-1},
abstract={Precision medicine (PM) is an innovative medical approach that considers differences in the individuals’ omics, medical histories, lifestyles, and environmental information in treating diseases. To fully achieve the envisaged gains of PM, various contemporary and future technologies have to be employed, among which are nanotechnology, sensor network, big data, and artificial intelligence. These technologies and other applications require a communication network that will enable them to work in tandem for the benefit of PM. Hence, communication technology serves as the nervous system of PM, without which the entire system collapses. Therefore, it is essential to explore and determine the candidate communication technology requirements that can guarantee the envisioned gains of PM. To the best of our knowledge, no work exploring how communication technology directly impacts the development and deployment of PM solutions exists. This survey paper is designed to stimulate discussions on PM from the communication engineering perspective. We introduce the fundamentals of PM and the demands in terms of quality of service that each of the enabling technologies of PM places on the communication network. We explore the information in the literature to suggest the ideal metric values of the key performance indicators for the implementation of the different components of PM. The comparative analysis of the suitability of the contemporary and future communication technologies for PM implementation is discussed. Finally, some open research challenges for the candidate communication technologies that will enable the full implementation of PM solutions are highlighted.},
keywords={Communications technology;Diseases;5G mobile communication;Key performance indicator;Drugs;6G mobile communication;Communication networks;Precision medicine;Fourth industrial revolution (4IR) technologies;Communication technologies;5G;6G},
doi={10.1109/ACCESS.2022.3175573},
ISSN={2169-3536},
month={},}