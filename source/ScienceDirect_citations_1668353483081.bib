@incollection{DIMITRAKOPOULOS202119,
title = {Chapter 2 - AI and software enablers for highly automated and autonomous vehicles},
editor = {George Dimitrakopoulos and Aggelos Tsakanikas and Elias Panagiotopoulos},
booktitle = {Autonomous Vehicles},
publisher = {Elsevier},
pages = {19-31},
year = {2021},
isbn = {978-0-323-90137-6},
doi = {https://doi.org/10.1016/B978-0-323-90137-6.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390137600014X},
author = {George Dimitrakopoulos and Aggelos Tsakanikas and Elias Panagiotopoulos},
keywords = {Algorithms, Artificial Intelligence, Cognitive, Decision-making, Deep learning, Machine learning},
abstract = {The goal of this chapter is to present the latest advances and challenges on management algorithms and software for enabling highly automated and autonomous driving, enhanced by Artificial Intelligence (AI) tools and methods. Indicative examples include cognitive computing, knowledge-based systems, and noncausal reasoning algorithms, which are used for active and passive safety of autonomous vehicles, as well as emergency management systems for highly automated/autonomous vehicles.}
}
@article{CHOWDHURY2021e00597,
title = {Recent machine learning guided material research - A review},
journal = {Computational Condensed Matter},
volume = {29},
pages = {e00597},
year = {2021},
issn = {2352-2143},
doi = {https://doi.org/10.1016/j.cocom.2021.e00597},
url = {https://www.sciencedirect.com/science/article/pii/S2352214321000605},
author = {Mohammad Asaduzzaman Chowdhury and Nayem Hossain and Md Bengir {Ahmed Shuvho} and Mohammad Fotouhi and Md Sakibul Islam and Md Ramjan Ali and Mohammod Abul Kashem},
keywords = {ML, Material science, Design, Characterization, Advancements, Challenges},
abstract = {Sustainable development of modern society demands discovering new materials with superior properties in different applications such as aerospace, wind, civil, automotive, etc. Characterizing and predicting material properties using traditional methods are time consuming and expensive. Therefore, advanced methods have been developed to meet the need for quick and reliable design and characterizing of materials properties. ML methods have made it possible to optimize and automate design performance and discover new materials. This review paper gives an overview of the implementation of ML in i) discovery of new materials, and ii) characterization of materials ML. Various ML models for materials manufacturing as well as how ML is applied to model materials are discussed. Recent advances, ML applications, as well as upcoming challenges and perspectives are discussed.}
}
@article{HERRGARDH2021102694,
title = {Hybrid modelling for stroke care: Review and suggestions of new approaches for risk assessment and simulation of scenarios},
journal = {NeuroImage: Clinical},
volume = {31},
pages = {102694},
year = {2021},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2021.102694},
url = {https://www.sciencedirect.com/science/article/pii/S2213158221001388},
author = {Tilda Herrgårdh and Vince I. Madai and John D. Kelleher and Rasmus Magnusson and Mika Gustafsson and Lili Milani and Peter Gennemark and Gunnar Cedersund},
keywords = {Stroke, Mechanistic modelling, Machine learning, Bioinformatics, Precision medicine},
abstract = {Stroke is an example of a complex and multi-factorial disease involving multiple organs, timescales, and disease mechanisms. To deal with this complexity, and to realize Precision Medicine of stroke, mathematical models are needed. Such approaches include: 1) machine learning, 2) bioinformatic network models, and 3) mechanistic models. Since these three approaches have complementary strengths and weaknesses, a hybrid modelling approach combining them would be the most beneficial. However, no concrete approach ready to be implemented for a specific disease has been presented to date. In this paper, we both review the strengths and weaknesses of the three approaches, and propose a roadmap for hybrid modelling in the case of stroke care. We focus on two main tasks needed for the clinical setting: a) For stroke risk calculation, we propose a new two-step approach, where non-linear mixed effects models and bioinformatic network models yield biomarkers which are used as input to a machine learning model and b) For simulation of care scenarios, we propose a new four-step approach, which revolves around iterations between simulations of the mechanistic models and imputations of non-modelled or non-measured variables. We illustrate and discuss the different approaches in the context of Precision Medicine for stroke.}
}
@incollection{OPRISIU2021208,
title = {In Silico ADME Modeling},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {208-222},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11532-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383115326},
author = {Ioana Oprisiu and Susanne Winiwarter},
keywords = {Absorption, AI, Distribution, DMPK, Drug discovery, Excretion, In silico ADME, IVIVe, Machine learning, Metabolism, QSAR, QSPR},
abstract = {Modeling of absorption, distribution, metabolism and excretion properties is well established in drug discovery. Here we present principles, considerations when and how to build models and how to apply them in real life in the industrial setting. The availability and quality of experimental data is important. However, for in silico models to be utilized availability is a major factor. Additionally, model quality measures need to be presented in an easily understandable manner for chemists and DMPK scientists.}
}
@article{EYKE2021120,
title = {Toward Machine Learning-Enhanced High-Throughput Experimentation},
journal = {Trends in Chemistry},
volume = {3},
number = {2},
pages = {120-132},
year = {2021},
note = {Special Issue: Machine Learning for Molecules and Materials},
issn = {2589-5974},
doi = {https://doi.org/10.1016/j.trechm.2020.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2589597420303117},
author = {Natalie S. Eyke and Brent A. Koscher and Klavs F. Jensen},
keywords = {high-throughput experimentation, machine learning, active learning},
abstract = {Recent literature suggests that the fields of machine learning (ML) and high-throughput experimentation (HTE) have separately received considerable attention from chemists and engineers, leading to the development of powerful reactivity models and platforms capable of rapidly performing thousands of reactions. The merger of ML with HTE presents a wealth of opportunities for the exploration of chemical space, but the integration of the two has yet to be fully realized. We highlight examples of recent developments in ML and HTE that collectively suggest the utility of their integration. Our analysis highlights the complementarity of the two fields, while exposing a number of obstacles that can and should be overcome to take full advantage of this merger and thereby accelerate chemical research.}
}
@article{YANG2021243,
title = {The code of targeted poverty alleviation in China: A geography perspective},
journal = {Geography and Sustainability},
volume = {2},
number = {4},
pages = {243-253},
year = {2021},
issn = {2666-6839},
doi = {https://doi.org/10.1016/j.geosus.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666683921000420},
author = {Yuanyuan Yang and Yansui Liu},
keywords = {Targeted poverty alleviation, Sustainable development, China, Geography, Experience},
abstract = {Geography is suitable for the study of sustainability from a transdisciplinary perspective, which takes the human-land relationship as the core research. As a key obstacle to rural sustainability, poverty is an external manifestation of the coupling maladjustment of elements in human-land territorial systems. As the world's largest developing country, China eradicated extreme poverty in 2020 and made significant contributions to global poverty reduction. Especially over the last eight years, China has implemented a targeted poverty alleviation (TPA) strategy and has continuously promoted theoretical, organizational and institutional innovations for poverty reduction. From the perspective of geography, this paper extracts the experiences of China's TPA strategy, represented by the "5W2H" mode. The research concludes that: (1) Precise identification, as the foundation of TPA, aims to introduce a registration system to obtain records of all poor households and then answer the "5W" (what, where, why, who, when) issues of the geography of poverty. (2) Precise assistance is the key of TPA, which aims to solve the issue of "how to offer help and support". The barriers to escaping poverty can be accomplished through policies and measures that focus on the diverse causes of poverty and considering different situations. (3) Accurate assessments are an essential means of TPA, relevant to solve "how to measure the end of poverty alleviation", and third-party evaluations play an important role in improving the accuracy of poverty alleviation. (4) The TPA mechanism lies in reconstructing the human-land-industry structures in the impoverished areal system. It is urgent to introduce China's successful experience and typical modes of TPA for global human-earth system coordination and sustainable development and contribute to building a community of human destiny.}
}
@article{MALINI20216105,
title = {Investigation of factors affecting student performance evaluation using education materials data mining technique},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {6105-6110},
year = {2021},
note = {SI: TIME-2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321036026},
author = {J. Malini and Y. Kalpana},
keywords = {Educational datamining, Dataset, Students performance, Attributes, Features, Machine learning, Materials and methods, Analysis},
abstract = {Every year students success rate was analysed by the Educational Institutions to develop their Academic standard. To identify the success rate many kinds of techniques are used such as statistics, physical examination and currently ongoing data mining techniques. Data mining Techniques was widely used in many fields, it is also used in the Educational environment known as Educational Data Mining (EDM). Educational data mining generate prototype in solving the research problems in students data and used to locate the unseen patterns in the students detailed dateset. This paper uses the EDM to characterize the distinct factors affecting the students performance by making predictions with efficient algorithms. Educational professionals have to identify the causes for the student failure in academic performance and the students not succeed in completing their education which becomes a social problem these days. The machine learning techniques help the researchers to analyse the student’s learning habits and their performance in academic. This paper would discuss different kinds of algorithms to analyse the economic background of the students which mainly affects the students performance. The dataset was utilized from the UCI Repository of secondary school students performance and analysed using the Weka tool for the data mining process.}
}
@article{HUANG2021129558,
title = {Evaluating the performance of LBSM data to estimate the gross domestic product of China at multiple scales: A comparison with NPP-VIIRS nighttime light data},
journal = {Journal of Cleaner Production},
volume = {328},
pages = {129558},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129558},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621037379},
author = {Ziwei Huang and Shaoying Li and Feng Gao and Fang Wang and Jinyao Lin and Ziling Tan},
keywords = {Location based social media data, NPP/VIIRS, Economic, OLS, GWR, China},
abstract = {Regional economic development evaluation is essential for understanding social and environmental issues. Although the nighttime light (NTL) data have been proved to be effective in economic estimation, it cannot reflect the human activities that occur during the daytime. Recently, with the widespread use of smart mobile devices, the location based social media (LBSM) data are increasingly being used as a proxy for real-time human activities. However, little work was carried out to explore the potential of LBSM data in estimating economic development at different scales in China. This study filled this gap by evaluating the effectiveness of Tencent user density (TUD) data, a typical type of LBSM data in China, in Gross Domestic Product (GDP) modeling at the provincial, municipal, and county scales. In this study, we employed holiday and non-holiday TUD sample data to simulate the annual TUD data, and compared it with the new generation NTL data, NPP/VIIRS images. The results showed that although the simulated annual TUD data does not perform better than NPP/VIIRS-NTL data in provincial and municipal GDP estimation, it outperforms NPP/VIIRS-NTL data at the county scale. More importantly, the simulated annual TUD data are much more powerful and reliable than NPP/VIIRS-NTL data in underdeveloped areas with complex terrain, such as the Northwest and Southwest China, as well as in more developed areas with separation of work and housing, such as the North China and South China. This is mainly because TUD data can reduce the impact of natural factors such as terrain on data collection as well as reflect both daytime and nighttime human activities. This study confirmed that the LBSM-TUD data is a potential and promising data source for economic modeling in small scale areas of China, which will help to support China's regional economic evaluation.}
}
@article{QADEER2021112736,
title = {Developing machine learning models for relative humidity prediction in air-based energy systems and environmental management applications},
journal = {Journal of Environmental Management},
volume = {292},
pages = {112736},
year = {2021},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.112736},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721007982},
author = {Kinza Qadeer and Ashfaq Ahmad and Muhammad Abdul Qyyum and Abdul-Sattar Nizami and Moonyong Lee},
keywords = {Air quality parameters, Random forest, Machine learning-based estimation, Aspen hysys, Support vector machine, Environmental management operations},
abstract = {The prediction of relative humidity is a challenging task because of its nonlinear nature. The machine learning-based prediction strategies have attained significant attention in tackling a broad class of challenging nonlinear and complex problems. The random forest algorithm is a well-proven machine learning algorithm due to its ease of training and implementation, as it requires minimal preprocessing. The random forest algorithm has hitherto not been employed for estimating air quality parameters, such as relative humidity. In this study, the random forest approach is implemented to estimate the relative humidity as a function of dry- and wet-bulb temperatures. A well-known commercial process simulator called Aspen HYSYS® V10 is linked with MATLAB® version 2019a to establish a data mining environment. The robustness of the prediction model is evaluated against varying wet-bulb depressions. There is high absolute deviation that indicates a lower prediction performance of the model against the higher wet-bulb depression i.e., ~20.0 °C. The random forest model can predict relative humidity with a 1.1% mean absolute deviation compared to the values obtained through Aspen HYSYS. The performance of the RF estimation model is also compared with a well-known support vector regression model. The random forest model demonstrates 74.4% better performance than the support vector machine model for the problem of interest, i.e., relative humidity estimation. This study will significantly help the practitioners in efficient designing of air-dependent energy systems as well as in better environmental management through rigorous prediction of relative humidity.}
}
@article{PICCIONE2021308,
title = {Realistic interplays between data science and chemical engineering in the first quarter of the 21st century, part 2: Dos and don’ts},
journal = {Chemical Engineering Research and Design},
volume = {169},
pages = {308-318},
year = {2021},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2021.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0263876221001301},
author = {Patrick M. Piccione},
keywords = {Data science, Chemical engineering, Digital transformation, Industry 4.0, Smart manufacturing},
abstract = {Under various names, such as, data science, Industry 4.0, or smart manufacturing, digital technologies are transforming our world. Although value statements and promises are published in a steady stream, uptake in the chemical and process industries has been moderate. Successful transformations are not confined to tasks, the “whats”. They also require great care in how they are carried out. This overview, aimed at all participants in the digital transformation of the chemical industry, presents “dos and don’ts” method recommendations for three successive steps: strategy development to define goals, (organisational) mobilisation for implementation, and project delivery. Successful strategy development requires assembling an empowered and skilled team; truly understanding the data science and digital transformation topics; accepting emergence and iteration; and focusing on real needs. Mobilising an organisation is essential so that it can translate strategy to tactics and value. Within organisations, one must therefore: enable project identification; set up a supportive organisational structure and skilful people within it. Looking outside, participation in partnerships is essential to access external resources. Delivery of valuable projects is the end goal. A diverse portfolio is needed, as well as effective collaborations between subject matter experts and data scientists. Technically, the use of software best practice is beneficial, and care must be taken of the data themselves. In the longer term, data science opportunities will extend beyond merely improving traditional analytics to make them faster, better, and more user-friendly. The early identification of beneficial future trends requires encouraging those individuals who have an interest in disruptive currents, and the perceptiveness to sense their areas of application.}
}
@article{MENG2021102692,
title = {Modification of Newell’s car-following model incorporating multidimensional stochastic parameters for emission estimation},
journal = {Transportation Research Part D: Transport and Environment},
volume = {91},
pages = {102692},
year = {2021},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2020.102692},
url = {https://www.sciencedirect.com/science/article/pii/S1361920920308762},
author = {Dongli Meng and Guohua Song and Yizheng Wu and Zhiqiang Zhai and Lei Yu and Jianbo Zhang},
keywords = {Newell’s car-following model, Stochasticity, Emission estimation, Driving behavior heterogeneity, Vehicle trajectory data},
abstract = {Existing studies have indicated that the vehicle trajectories derived from Newell's car-following model (NCM) fail to capture driving behavior heterogeneity, resulting in considerable emission estimation errors. This study investigated the situation-dependent heterogeneity of car-following behavior, based on field vehicle trajectories in Beijing, and proposed a multidimensional stochastic Newell car-following model (MSNCM) incorporating three stochastic parameters: random response time, speed-dependent critical jam spacing, and speed difference- and spacing-dependent acceleration. The comparison between the field data and numerical simulations of the NCM and MSNCM shown that the MSNCM performed well in generating realistic vehicle trajectories for emission estimation. The relative errors of the emission factors derived from the field and the MSNCM simulated trajectories were 0.26%, 0.91%, 1.37%, and 0.25% for CO2, CO, HC, and NOx, respectively, which represented reductions of approximately 15%-46% compared with the traditional NCM.}
}
@article{XU2021e266,
title = {Wireless skin sensors for physiological monitoring of infants in low-income and middle-income countries},
journal = {The Lancet Digital Health},
volume = {3},
number = {4},
pages = {e266-e273},
year = {2021},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(21)00001-7},
url = {https://www.sciencedirect.com/science/article/pii/S2589750021000017},
author = {Shuai Xu and Alina Y Rwei and Bellington Vwalika and Maureen P Chisembele and Jeffrey S A Stringer and Amy Sarah Ginsburg and John A Rogers},
abstract = {Summary
Globally, neonatal mortality remains unacceptability high. Physiological monitoring is foundational to the care of these vulnerable patients to assess neonatal cardiopulmonary status, guide medical intervention, and determine readiness for safe discharge. However, most existing physiological monitoring systems require multiple electrodes and sensors, which are linked to wires tethered to wall-mounted display units, to adhere to the skin. For neonates, these systems can cause skin injury, prevent kangaroo mother care, and complicate basic clinical care. Novel, wireless, and biointegrated sensors provide opportunities to enhance monitoring capabilities, reduce iatrogenic injuries, and promote family-centric care. Early validation data have shown performance equivalent to (and sometimes exceeding) standard-of-care monitoring systems in premature neonates cared for in high-income countries. The reusable nature of these sensors and compatibility with low-cost mobile phones have the future potential to enable substantially lower monitoring costs compared with existing systems. Deployment at scale, in low-income countries, holds the promise of substantial improvements in neonatal outcomes.}
}
@article{SUN2021106276,
title = {High resolution 3D terrestrial LiDAR for cotton plant main stalk and node detection},
journal = {Computers and Electronics in Agriculture},
volume = {187},
pages = {106276},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106276},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921002933},
author = {Shangpeng Sun and Changying Li and Peng W. Chee and Andrew H. Paterson and Cheng Meng and Jingyi Zhang and Ping Ma and Jon S. Robertson and Jeevan Adhikari},
keywords = {High throughput phenotyping, Terrestrial LiDAR, Three-dimensional skeleton, Plant node detection, Minimum spanning tree},
abstract = {Dense three-dimensional point clouds provide opportunities to retrieve detailed characteristics of plant organ-level phenotypic traits, which are helpful to better understand plant architecture leading to its improvements via new plant breeding approaches. In this study, a high-resolution terrestrial LiDAR was used to acquire point clouds of plants under field conditions, and a data processing pipeline was developed to detect plant main stalks and nodes, and then to extract two phenotypic traits including node number and main stalk length. The proposed method mainly consisted of three steps: first, extract skeletons from original point clouds using a Laplacian-based contraction algorithm; second, identify the main stalk by converting a plant skeleton point cloud to a graph; and third, detect nodes by finding the intersection between the main stalk and branches. Main stalk length was calculated by accumulating the distance between two adjacent points from the lowest to the highest point of the main stalk. Experimental results based on 26 plants showed that the proposed method could accurately measure plant main stalk length and detect nodes; the average R2 and mean absolute percentage error were 0.94 and 4.3% for the main stalk length measurements and 0.7 and 5.1% for node counting, respectively, for point numbers between 80,000 and 150,000 for each plant. Three-dimensional point cloud-based high throughput phenotyping may expedite breeding technologies to improve crop production.}
}
@article{YAHAYA2021105851,
title = {Ensemble-based model selection for imbalanced data to investigate the contributing factors to multiple fatality road crashes in Ghana},
journal = {Accident Analysis & Prevention},
volume = {151},
pages = {105851},
year = {2021},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2020.105851},
url = {https://www.sciencedirect.com/science/article/pii/S0001457520316717},
author = {Mahama Yahaya and Runhua Guo and Xinguo Jiang and Kamal Bashir and Caroline Matara and Shiwei Xu},
keywords = {Multiple fatal injury crash, Classification, Model selection, Class imbalance, Oversampling, Ensemble classifiers},
abstract = {The study aims to identify relevant variables to improve the prediction performance of the crash injury severity (CIS) classification model. Unfortunately, the CIS database is invariably characterized by the class imbalance. For instance, the samples of multiple fatal injury (MFI) severity class are typically rare as opposed to other classes. The imbalance phenomenon may introduce a prediction bias in favour of the majority class and affect the quality of the learning algorithm. The paper proposes an ensemble-based variable ranking scheme that incorporates the data resampling. At the data pre-processing level, majority weighted minority oversampling (MWMOTE) is employed to treat the imbalanced training data. Ensemble of classifiers induced from the balanced data is used to evaluate and rank the individual variables according to their importance to the injury severity prediction. The relevant variables selected are then applied to the balanced data to form a training set for the CIS classification modelling. An empirical comparison is conducted through considering the variable ranking by: 1) the learning of single inductive algorithm with imbalanced data where the relevant variables are applied to the imbalanced data to form the training data; 2) the learning of single inductive algorithm with MWMOTE data and the relevant variables identified are applied to the balanced data to form the training data; and 3) the learning of ensembles with imbalanced data where the relevant variables identified are applied to the imbalanced data to form the training data. Bayesian Networks (BNs) classifiers are then developed for each ranking method, where nested subsets of the top ranked variables are adopted. The model predictions are captured in four performance indicators in the comparative study. Based on three-year (2014–2016) crash data in Ghana, the empirical results show that the proposed method is effective to identify the most prolific predictors of the CIS level. Finally, based on the inference results of BNs developed on the best subset, the study offers the most probable explanations to the occurrence of MFI crashes in Ghana.}
}
@article{MARTIN2021100255,
title = {Multi-Temperate Logical Data Warehouse Design for Large-Scale Healthcare Data},
journal = {Big Data Research},
volume = {25},
pages = {100255},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100255},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000721},
author = {Bryan Martin and Karen C. Davis},
keywords = {Data warehouse design, OLAP workloads, Healthcare data management, Data partitioning algorithms, Logical data warehouses, Columnar databases},
abstract = {Modern hardware architectures and advances in database technology are driving increased adoption of logical data warehouses (LDWs) that complement traditional physical data warehousing (PDW) approaches. In contrast to PDW design methodologies that emphasize physical consolidation of all data of interest on a single (perhaps distributed) computing platform, along with early-binding approaches that pre-materialize transformations and changes to the source data, LDW techniques allow for the integration and transformation of data at run-time and typically physically move or modify much less data in advance. In an environment with premium hardware such as multi-temperate storage, the successful design of LDWs depends on replication of high value data to their physical core to maximize spatial locality. Identifying and collocating high value data is a non-trivial task that has not been adequately explored in the context of LDWs in multi-temperate storage systems. In this paper, we gather queries to construct an OLAP workload for use in supporting and evaluating LDW design algorithms for a large healthcare organization. We introduce new algorithms to address the preprocessing of the workload, identification of data clusters to support OLAP queries, and assignment of clusters to appropriate (hot, warm, and cold) storage tiers, allowing the LDW to deliver results more efficiently by covering a higher percentage of its query workload using the fastest storage devices. Any use case involving copying data from sources to tiered storage targets for analytic querying could benefit from the techniques and solutions presented here.}
}
@article{THINGO2021505,
title = {Evaluation of deep learning algorithms for national scale landslide susceptibility mapping of Iran},
journal = {Geoscience Frontiers},
volume = {12},
number = {2},
pages = {505-519},
year = {2021},
issn = {1674-9871},
doi = {https://doi.org/10.1016/j.gsf.2020.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1674987120301687},
author = {Phuong Thao {Thi Ngo} and Mahdi Panahi and Khabat Khosravi and Omid Ghorbanzadeh and Narges Kariminejad and Artemi Cerda and Saro Lee},
keywords = {CNN, RNN, Deep learning, Landslide, Iran},
abstract = {The identification of landslide-prone areas is an essential step in landslide hazard assessment and mitigation of landslide-related losses. In this study, we applied two novel deep learning algorithms, the recurrent neural network (RNN) and convolutional neural network (CNN), for national-scale landslide susceptibility mapping of Iran. We prepared a dataset comprising 4069 historical landslide locations and 11 conditioning factors (altitude, slope degree, profile curvature, distance to river, aspect, plan curvature, distance to road, distance to fault, rainfall, geology and land-sue) to construct a geospatial database and divided the data into the training and the testing dataset. We then developed RNN and CNN algorithms to generate landslide susceptibility maps of Iran using the training dataset. We calculated the receiver operating characteristic (ROC) curve and used the area under the curve (AUC) for the quantitative evaluation of the landslide susceptibility maps using the testing dataset. Better performance in both the training and testing phases was provided by the RNN algorithm (AUC ​= ​0.88) than by the CNN algorithm (AUC ​= ​0.85). Finally, we calculated areas of susceptibility for each province and found that 6% and 14% of the land area of Iran is very highly and highly susceptible to future landslide events, respectively, with the highest susceptibility in Chaharmahal and Bakhtiari Province (33.8%). About 31% of cities of Iran are located in areas with high and very high landslide susceptibility. The results of the present study will be useful for the development of landslide hazard mitigation strategies.}
}
@article{MARABELLI2021101683,
title = {The lifecycle of algorithmic decision-making systems: Organizational choices and ethical challenges},
journal = {The Journal of Strategic Information Systems},
volume = {30},
number = {3},
pages = {101683},
year = {2021},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2021.101683},
url = {https://www.sciencedirect.com/science/article/pii/S0963868721000305},
author = {Marco Marabelli and Sue Newell and Valerie Handunge},
keywords = {Algorithmic decision-making systems (ADMS), Strategic choices, Ethical implications, IS strategizing, Automatic systems},
abstract = {In this viewpoint article we discuss algorithmic decision-making systems (ADMS), which we view as organizational sociotechnical systems with their use in practice having consequences within and beyond organizational boundaries. We build a framework that revolves around the ADMS lifecycle and propose that each phase challenges organizations with “choices” related to technical and processual characteristics – ways to design, implement and use these systems in practice. We argue that it is important that organizations make these strategic choices with awareness and responsibly, as ADMS’ consequences affect a broad array of stakeholders (the workforce, suppliers, customers and society at-large) and involve ethical considerations. With this article we make two main contributions. First, we identify key choices associated with the design, implementation and use in practice of ADMS in organizations, that build on past literature and are tied to timely industry-related examples. Second, we provide IS scholars with a broad research agenda that will promote the generation of new knowledge and original theorizing within the domain of the strategic applications of ADMS in organizations.}
}
@article{GRAINGER2021354,
title = {A Perspective on the Analytical Challenges Encountered in High-Throughput Experimentation},
journal = {Organic Process Research & Development},
volume = {25},
number = {3},
pages = {354-364},
year = {2021},
issn = {1083-6160},
doi = {https://doi.org/10.1021/acs.oprd.0c00463},
url = {https://www.sciencedirect.com/science/article/pii/S1083616021016492},
author = {Rachel Grainger and Stuart Whibley},
keywords = {analytical, HTE, mass spectrometry, nanoscale, optimization},
abstract = {ABSTRACT
High-throughput experimentation (HTE) is a well-established technique used in the pharmaceutical industry to accelerate compound synthesis and route optimization through automated chemical processes. A key part of any HTE workflow is the analytical component, through which the reaction outcome can be determined. The development of new analytical techniques capable of high-throughput data generation from nanoscale chemical reactions has been required to streamline the HTE process and address challenges generated through the recent move to miniaturize synthesis. In this Perspective, we review the currently available state-of-the-art analytical methods, discuss the challenges encountered in high-throughput analysis—with a particular focus on the analysis of nanoscale reactions, and provide a future outlook on potential developments in the field.}
}
@article{ZHOU20211274,
title = {Intelligent Ironmaking Optimization Service on a Cloud Computing Platform by Digital Twin},
journal = {Engineering},
volume = {7},
number = {9},
pages = {1274-1281},
year = {2021},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2021.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S209580992100299X},
author = {Heng Zhou and Chunjie Yang and Youxian Sun},
keywords = {Cloud factory, Blast furnace, Multi-objective optimization, Distributed computation},
abstract = {The shortage of computation methods and storage devices has largely limited the development of multi-objective optimization in industrial processes. To improve the operational levels of the process industries, we propose a multi-objective optimization framework based on cloud services and a cloud distribution system. Real-time data from manufacturing procedures are first temporarily stored in a local database, and then transferred to the relational database in the cloud. Next, a distribution system with elastic compute power is set up for the optimization framework. Finally, a multi-objective optimization model based on deep learning and an evolutionary algorithm is proposed to optimize several conflicting goals of the blast furnace ironmaking process. With the application of this optimization service in a cloud factory, iron production was found to increase by 83.91 t∙d−1, the coke ratio decreased 13.50 kg∙t−1, and the silicon content decreased by an average of 0.047%.}
}
@article{MORRIS2021199,
title = {Impact of the COVID-19 pandemic on the detection and management of colorectal cancer in England: a population-based study},
journal = {The Lancet Gastroenterology & Hepatology},
volume = {6},
number = {3},
pages = {199-208},
year = {2021},
issn = {2468-1253},
doi = {https://doi.org/10.1016/S2468-1253(21)00005-4},
url = {https://www.sciencedirect.com/science/article/pii/S2468125321000054},
author = {Eva J A Morris and Raphael Goldacre and Enti Spata and Marion Mafham and Paul J Finan and Jon Shelton and Mike Richards and Katie Spencer and Jonathan Emberson and Sam Hollings and Paula Curnow and Dominic Gair and David Sebag-Montefiore and Chris Cunningham and Matthew D Rutter and Brian D Nicholson and Jem Rashbass and Martin Landray and Rory Collins and Barbara Casadei and Colin Baigent},
abstract = {Summary
Background
There are concerns that the COVID-19 pandemic has had a negative effect on cancer care but there is little direct evidence to quantify any effect. This study aims to investigate the impact of the COVID-19 pandemic on the detection and management of colorectal cancer in England.
Methods
Data were extracted from four population-based datasets spanning NHS England (the National Cancer Cancer Waiting Time Monitoring, Monthly Diagnostic, Secondary Uses Service Admitted Patient Care and the National Radiotherapy datasets) for all referrals, colonoscopies, surgical procedures, and courses of rectal radiotherapy from Jan 1, 2019, to Oct 31, 2020, related to colorectal cancer in England. Differences in patterns of care were investigated between 2019 and 2020. Percentage reductions in monthly numbers and proportions were calculated.
Findings
As compared to the monthly average in 2019, in April, 2020, there was a 63% (95% CI 53–71) reduction (from 36 274 to 13 440) in the monthly number of 2-week referrals for suspected cancer and a 92% (95% CI 89–95) reduction in the number of colonoscopies (from 46 441 to 3484). Numbers had just recovered by October, 2020. This resulted in a 22% (95% CI 8–34) relative reduction in the number of cases referred for treatment (from a monthly average of 2781 in 2019 to 2158 referrals in April, 2020). By October, 2020, the monthly rate had returned to 2019 levels but did not exceed it, suggesting that, from April to October, 2020, over 3500 fewer people had been diagnosed and treated for colorectal cancer in England than would have been expected. There was also a 31% (95% CI 19–42) relative reduction in the numbers receiving surgery in April, 2020, and a lower proportion of laparoscopic and a greater proportion of stoma-forming procedures, relative to the monthly average in 2019. By October, 2020, laparoscopic surgery and stoma rates were similar to 2019 levels. For rectal cancer, there was a 44% (95% CI 17–76) relative increase in the use of neoadjuvant radiotherapy in April, 2020, relative to the monthly average in 2019, due to greater use of short-course regimens. Although in June, 2020, there was a drop in the use of short-course regimens, rates remained above 2019 levels until October, 2020.
Interpretation
The COVID-19 pandemic has led to a sustained reduction in the number of people referred, diagnosed, and treated for colorectal cancer. By October, 2020, achievement of care pathway targets had returned to 2019 levels, albeit with smaller volumes of patients and with modifications to usual practice. As pressure grows in the NHS due to the second wave of COVID-19, urgent action is needed to address the growing burden of undetected and untreated colorectal cancer in England.
Funding
Cancer Research UK, the Medical Research Council, Public Health England, Health Data Research UK, NHS Digital, and the National Institute for Health Research Oxford Biomedical Research Centre.}
}
@incollection{MALIK20213,
title = {Chapter 1 - Advances in Machine Learning and Data Analytics},
editor = {Hasmat Malik and Nuzhat Fatema and Atif Iqbal},
booktitle = {Intelligent Data-Analytics for Condition Monitoring},
publisher = {Academic Press},
pages = {3-29},
year = {2021},
isbn = {978-0-323-85510-5},
doi = {https://doi.org/10.1016/B978-0-323-85510-5.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323855105000016},
author = {Hasmat Malik and Nuzhat Fatema and Atif Iqbal},
keywords = {feature extraction, feature selection, data preprocessing, visualization, condition monitoring, open access, software, dataset sources},
abstract = {Artificial intelligence (AI) is the intelligence demonstrated by the machines. AI is also the representation of a machine (like computer), which imitates cognitive behavior/functions associated to human mind for the purpose of learning and problem solving. Also, the subset of AI is machine learning (ML), which is improved automatically through experience. ML algorithms are developed based on the data without explicit information of the system behavior. Data statistics and computational analysis are the subset of ML. The data analytics is a process of analyzing, cleaning, transforming, and modeling the data with respect to the useful information. In this chapter, detailed information of data analytics of smart grid application, data analytics for business, condition monitoring, data and its relation, data preprocessing, feature extraction, feature selection, and different application areas are studied. A wide list of software and dataset’s digital library are also included.}
}
@article{HE2021103749,
title = {Human resource management structure of communication enterprise based on microprocessor system and embedded network},
journal = {Microprocessors and Microsystems},
volume = {81},
pages = {103749},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103749},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120308942},
author = {Ying He and Ming Li},
keywords = {Information systems, Human resource management, HRISHR professionals},
abstract = {In the past few years, human resource management (HRM) has undergone significant changes. The focus from administrative tasks to become strategic partners into the organization's overall strategy, mainly in the field of development of information technology, has given strong support. The technology support using Microprocessor System and Embedded Network to help handle the HRM knowledge process. Long-term use of information systems today have a significant impact on how to manage HRM. The Human Resources (HR) processes and practices within the organization; in other words, the collection of information, storage, use, and sharing method have changed completely. The Microprocessor System using to store the condition base data and then communicate with internet support. Part of the HRM process becomes more efficient; due to these improved service levels, it can now be more participation. Human resources of this new business strategy in business strategy have a significant impact on the human resources function and its experts. This chapter reviews the existing literature on this topic and considers the advantages and benefits of HRM information systems. It also outlines some of the technical applications in the functional areas of HRM in the organization.}
}
@article{ZHANG2023102481,
title = {An update method for digital twin multi-dimension models},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {80},
pages = {102481},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102481},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001636},
author = {He Zhang and Qinglin Qi and Wei Ji and Fei Tao},
keywords = {Digital twin, Consistency, Model update, Machine tool, Tool wear},
abstract = {Digital twin, as an effective means to realize the fusion between physical and virtual spaces, has attracted more and more attention in the past few years. Based on ultra-fidelity models, more accurate service, e.g. real-time monitoring and failure prediction, can be reached. Against the background, some scholars studied the related theories and methods on modeling to depict various features of physical objects. Some scholars studied how to use Internet of Things to realize the connections and interactions, thereby keeping the consistency between the virtual and physical spaces. During this process, a new question arises that how to update the models once digital twin models are inconsistent with the practical situations. To solve the problem, this paper proposed a general digital twin model update framework at first. Then, the update methods for multi-dimension models are further explored. The cutting tool is the core component of machine tools which are the key equipment in industry. The precise cutting tool models are essential for realizing the digitalization and servitization of machine tools. Therefore, this paper takes a cutting tool as the application object to discuss how to conduct physics model update based on the proposed framework and methods. Through model update, a more accurate and updated tool wear model could be obtained, which contributes to the prognostics and health management for machine tools.}
}
@article{POURBOZORGILANGROUDI2021119,
title = {Backward simulation of temperature changes of District Heating networks for enabling loading history in predictive maintenance},
journal = {Energy Reports},
volume = {7},
pages = {119-127},
year = {2021},
note = {The 17th International Symposium on District Heating and Cooling},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721008374},
author = {Pakdad {Pourbozorgi Langroudi} and Ingo Weidlich and Stefan Hay},
keywords = {Backward simulation, Loading history, Predictive maintenance, Machine learning, Asset management, Artificial neural networks, System reliability, District Heating},
abstract = {District Heating (DH) networks, like most of industries, are in transition to the fourth​ industry age and they are retrofitting themselves with different sensing and inspection technologies to enable cyber connectivity for different purposes, such as system optimization, failure detection, maintenance, etc. Since DH pipes show different ageing behaviour under different conditions and initially the pre-insulated bounded pipes had been designed for a minimum of 30 years life span, a long-term loading history is required for predictive maintenance (PdM) purposes and it is necessary to understand the ageing of the DH pipes. These historical temperature changes of the networks are not available for such a long period and they are usually limited to the past few years. To exploit the available implemented technologies for PdM , the missing data must become available to understand the ageing patterns and expand the ageing model to the pipes in use. In this research, various Machine Learning (ML) techniques such as Support Vector Machine (SVM), Random Forest algorithm (RF), Artificial Neural Networks (ANN) have been tested to train a model and backward simulate the temperature changes of the system based on recorded weather data. Various none-temperature variables have been used to enhance the prediction qualities to the real-world data. The historical temperature changes of the system shall be used for different ageing estimation such as fatigue cycles or remaining useful life of the polyurethane (PUR) foam.}
}
@article{LISBOA2021709,
title = {Improve industrial performance based on systematic analyses of manufacturing data},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {709-716},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.083},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008260},
author = {M. Lisboa and E. Jesus and R. Seixas and P. Valle and F. Deschamps and C. Strobel},
keywords = {Industry 4.0, Industrial Internet of things (IIOT), Data Analytics, Predictive Maintenance},
abstract = {This Article was written based on a systematic review of the literature considering three reference axes, Industry 4.0, Data Analytics and Predictive Maintenance, including specific combination of search terms to ensure a reasonable quantity of articles keeping adherence to the topic that is decision making based on collection and analysis of relevant data on B2B (Business to Business). The study focuses on the area of predictive maintenance, which has been strongly highlighted by the fourth industrial revolution, due to the mechanical and mainly the electronic embedded systems complexity increases in the last decades, as well as network connectivity possibilities for equipment’s data acquisition, enabling technologies for predictive maintenance as a key factor for competitiveness, reducing costs, increasing equipment’s availability, or as a servitization strategy. The article is divided into five parts, starting with the research model and selection of articles, including the proposal for a data analysis framework and the application of systematic analysis of these data, concluding with the opening of a discussion and the indication of future directions.}
}
@article{KADAR2021103113,
title = {Tourism flows in large-scale destination systems},
journal = {Annals of Tourism Research},
volume = {87},
pages = {103113},
year = {2021},
issn = {0160-7383},
doi = {https://doi.org/10.1016/j.annals.2020.103113},
url = {https://www.sciencedirect.com/science/article/pii/S0160738320302577},
author = {Bálint Kádár and Mátyás Gede},
keywords = {Tourism networks, Network analysis, Tourist flows, Large-scale destinations, Multi-destination trips, Danube, Flickr analysis},
abstract = {Large-scale destination systems, especially cross-border regions are less studied in literature as their size and transnational nature makes these hard to analyse with traditional methods. Tourism systems like the Danube Region are composed of several local and regional destinations, and even when these are branded together for tourists the integration of these into one system is often compromised by national boundaries and socio-economic differences. This study shows how the Danube region is composed of different clusters of destinations, and how national boundaries have a strong shielding effect in the interregional movements of tourists. A methodology based on network analysis with efficient clustering algorithms applied on large geotagged datasets from User Generated Content is proposed. Flickr data was used to map short time-interval visitor flows along the linear system of the river Danube. 18 regional clusters integrated into 3 strong, but separated destination systems were identified by modularity analysis. The central integrating effect of the large capital cities and the boundary-shielding effect impeding the total integration of this large-scale system were made measurable.}
}
@article{MA2021106972,
title = {Mining truck platooning patterns through massive trajectory data},
journal = {Knowledge-Based Systems},
volume = {221},
pages = {106972},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106972},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121002355},
author = {Xiaolei Ma and Enze Huo and Haiyang Yu and Honghai Li},
keywords = {Energy consumption, Trajectory mining, Truck platooning, Spatial clustering, Association rule learning},
abstract = {Truck platooning refers to a series of trucks driving in close proximity via communication technologies, and it is considered one of the most implementable systems of connected and automated vehicles, bringing huge energy savings and safety improvements. Properly planning platoons and evaluating the potential of truck platooning are crucial to trucking companies and transportation authorities. This study proposes a series of data mining approaches to learn spontaneous truck platooning patterns from massive trajectories. An enhanced map matching algorithm is developed to identify truck headings by using digital map data, followed by an adaptive spatial clustering algorithm to detect trucks’ instantaneous co-moving sets. These sets are then aggregated to find the network-wide maximum platoon duration and size through frequent itemset mining for computational efficiency. The GPS data were collected from truck fleeting systems in Liaoning Province, China for platooning performance measures and spatiotemporal platooning distribution visualization. Results show that approximately 36% spontaneous truck platoons can be coordinated by speed adjustment without changing routes and schedules. The average platooning distance and duration ratios for these platooned trucks are 9.6% and 9.9%, respectively, leading to a 2.8% reduction in total fuel consumption. This study also distinguishes the optimal platooning periods and space headways for national freeways and trunk roads, and prioritize the road segments with high possibilities of truck platooning. The derived results are reproducible, providing useful policy implications and operational strategies for large-scale truck platoon planning and roadside infrastructure construction.}
}
@article{OBERDORF2021103481,
title = {Analytics-enabled escalation management: System development and business value assessment},
journal = {Computers in Industry},
volume = {131},
pages = {103481},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103481},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000889},
author = {Felix Oberdorf and Nikolai Stein and Christoph M. Flath},
keywords = {Escalation management, Industry 4.0, Machine learning, Business analytics},
abstract = {Industry 4.0 initiatives can help traditional manufacturing industry cope with increasing global competition. Such solutions facilitate transparency, automation as well as business process transformation. This paper elaborates on a collaboration with a medium-sized manufacturing company. We highlight the design, evaluation and roll-out of an escalation management system with integrated data-driven decision support. We do so by applying an action design research process. Thereby, our study focuses on the system design concerning the creation of business value. The system leverages state-of-the-art machine learning algorithms for disruption type classification and escalation handling duration prediction. These predictions can be embedded in an integrated planning procedure leveraging diverse organizational data sources (e.g., personnel availability, production plans) to instantiate a prescriptive analytics solution. Combined with informative analytics insights, this allows the proposed system to generate significant business value by reducing escalation durations. In the long run, the transformational business value enabled by the system is likely to exceed the automational business value. This highlights the special importance of tight integration of industrial analytics applications within business processes.}
}
@article{ALVAREZCOELLO2021658,
title = {Towards a Data-Centric Architecture in the Automotive Industry},
journal = {Procedia Computer Science},
volume = {181},
pages = {658-663},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.215},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002581},
author = {Daniel Alvarez-Coello and Daniel Wilms and Adnan Bekan and Jorge {Marx Gómez}},
keywords = {Connected vehicles, data-centric architecture, standardized data, semantic AI, modern data architecture},
abstract = {Vehicle software architectures have been evolving over the last twenty years to support data-driven functionalities. Several enterprises from different domains are currently focusing on improving their data architectures by re-defining the underlying data models to enable core support for analytics and artificial intelligence. Moreover, a common desire to add clear data provenance and explicit context impulses the field of semantics and knowledge graphs. Nevertheless, in the automotive industry, the scenario of connected vehicles implies extra complexity. Vehicle data has an enormous variety, making it essential to develop and adopt standards. This paper presents aspects of ongoing research at the BMW Research Department regarding a conceptual design for vehicle software architectures in the automotive industry. We discuss the principles of a modern data architecture with particular emphasis on the data-centric mindset. We also explore the current challenges and possible working points as the foundation to move from siloed data towards a so-called AI factory.}
}
@article{LI2021106183,
title = {A review of artificial neural network based chemometrics applied in laser-induced breakdown spectroscopy analysis},
journal = {Spectrochimica Acta Part B: Atomic Spectroscopy},
volume = {180},
pages = {106183},
year = {2021},
issn = {0584-8547},
doi = {https://doi.org/10.1016/j.sab.2021.106183},
url = {https://www.sciencedirect.com/science/article/pii/S0584854721001403},
author = {Lu-Ning Li and Xiang-Feng Liu and Fan Yang and Wei-Ming Xu and Jian-Yu Wang and Rong Shu},
keywords = {Laser-induced breakdown spectroscopy, Artificial neural network, Machine learning, Chemometrics, Classification},
abstract = {In the past decades various categories of chemometrics for laser-induced breakdown spectroscopy (LIBS) analysis have been developed, among which an important category is that based on artificial neural network (ANN). The most common ANN scheme employed in LIBS researches so far is back-propagation neural network (BPNN), while there are also several other kinds of neural networks appreciated by the LIBS community, including radial basis function neural network (RBFNN), convolutional neural network (CNN), self-organizing map (SOM), etc. In this paper, we introduce the principles of some representative ANN methods, and offer criticism on their features along with comparison between them. Then we afford an overview of ANN-based chemometrics applied in LIBS analysis, involving material identification/classification, component concentration quantification, and some unconventional applications as well. Furthermore, a comprehensive discussion on ANN-LIBS methodologies is provided from four aspects. First, a few general progressing trends are displayed. Next we expound some specific implementation techniques, including variable selection, network construction, data set utilization, network training, model evaluation, and chemometrics selection. In addition, the limitations of ANN approaches are remarked, mainly concerning overfitting and interpretability. Finally a prospect of future development of ANN-LIBS chemometrics is presented. Throughout the discussion quite a few good practices have been highlighted. This review is expected to shed light on the further upgrade of ANN-based LIBS chemometrics in the future.}
}
@incollection{KUBASSOVA20211,
title = {Chapter 1 - History, current status, and future directions of artificial intelligence},
editor = {Michael Mahler},
booktitle = {Precision Medicine and Artificial Intelligence},
publisher = {Academic Press},
pages = {1-38},
year = {2021},
isbn = {978-0-12-820239-5},
doi = {https://doi.org/10.1016/B978-0-12-820239-5.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202395000024},
author = {Olga Kubassova and Faiq Shaikh and Carlos Melus and Michael Mahler},
keywords = {Artificial intelligence, Neural networks, Rheumatoid arthritis, Fatty liver disease, Electrocardiography, Blockchain technology},
abstract = {Artificial intelligence (AI) as a technology concept is making a major impact on a wide range of industries and sectors. This is largely attributed to technical advancements in machine and especially deep learning methodologies fueled by improved computational capabilities which have led to sophisticated approaches in applying AI to various scenarios. These AI applications aim to improve productivity, decrease cost, and comprehend the ever-increasing volumes of data available to ultimately provide actionable insights. Included in this paradigm shift is medicine, where AI is beginning to enable clinical assistance, decision support, improved management and accelerated scientific discovery and development.}
}
@article{HASHMI2021101316,
title = {Transdisciplinary systems approach to realization of digital transformation},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101316},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101316},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000690},
author = {Muhammad A. Hashmi and John P.T. Mo and Ronald C. Beckett},
keywords = {Transdisciplinary engineering, System of systems, Cyber physical systems, Data analytics, Work 4.0, Data integrity},
abstract = {With advancement in technology and emergence of fast networks, operation of businesses and global companies increasingly depend on the Internet and digital transformation of their infrastructures. Adaptation to Industry 4.0 paradigm gives rise to societal, technological and communication issues due to challenges in product, services, social and inter-disciplinary interactions. A more fundamental approach that can mitigate complexity of the new business is necessary. This paper adopts a system of systems model embedded into a transdisciplinary system design to describe a typical X4.0 system where X can be any industry sector migrating to Industry 4.0 paradigm. Two industry sectors: Education 4.0 and Retail 4.0 are studied under the amalgamated transdisciplinary system of systems model. Results show that the four artifacts in X4.0 can form the foundation of new sectorial 4.0 development with focus on specific elements in Cyber Physical Systems and Work4.0 artifacts. The transdisciplinary system approach has the advantage of a self-improving model that drives realization of digital transformation in evolutionary cycles.}
}
@article{TONG2021106532,
title = {Personalized mobile technologies for lifestyle behavior change: A systematic review, meta-analysis, and meta-regression},
journal = {Preventive Medicine},
volume = {148},
pages = {106532},
year = {2021},
issn = {0091-7435},
doi = {https://doi.org/10.1016/j.ypmed.2021.106532},
url = {https://www.sciencedirect.com/science/article/pii/S009174352100116X},
author = {Huong Ly Tong and Juan C. Quiroz and A. Baki Kocaballi and Sandrine Chan Moi Fat and Kim Phuong Dao and Holly Gehringer and Clara K. Chow and Liliana Laranjo},
keywords = {Mobile applications[MeSH], Fitness trackers[MeSH], Personalization, Tailoring, Health behavior[MeSH]},
abstract = {Given that the one-size-fits-all approach to mobile health interventions have limited effects, a personalized approach might be necessary to promote healthy behaviors and prevent chronic conditions. Our systematic review aims to evaluate the effectiveness of personalized mobile interventions on lifestyle behaviors (i.e., physical activity, diet, smoking and alcohol consumption), and identify the effective key features of such interventions. We included any experimental trials that tested a personalized mobile app or fitness tracker and reported any lifestyle behavior measures. We conducted a narrative synthesis for all studies, and a meta-analysis of randomized controlled trials. Thirty-nine articles describing 31 interventions were included (n = 77,243, 64% women). All interventions personalized content and rarely personalized other features. Source of data included system-captured (12 interventions), user-reported (11 interventions) or both (8 interventions). The meta-analysis showed a moderate positive effect on lifestyle behavior outcomes (standardized difference in means [SDM] 0.663, 95% CI 0.228 to 1.10). A meta-regression model including source of data found that interventions that used system-captured data for personalization were associated with higher effectiveness than those that used user-reported data (SDM 1.48, 95% CI 0.76 to 2.19). In summary, the field is in its infancy, with preliminary evidence of the potential efficacy of personalization in improving lifestyle behaviors. Source of data for personalization might be important in determining intervention effectiveness. To fully exploit the potential of personalization, future high-quality studies should investigate the integration of multiple data from different sources and include personalized features other than content.}
}
@article{DU2021113581,
title = {Neighbor-aware review helpfulness prediction},
journal = {Decision Support Systems},
volume = {148},
pages = {113581},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2021.113581},
url = {https://www.sciencedirect.com/science/article/pii/S0167923621000919},
author = {Jiahua Du and Jia Rong and Hua Wang and Yanchun Zhang},
keywords = {Review helpfulness, Sequential bias, Review neighbors, Context clues, Deep learning},
abstract = {Helpfulness prediction techniques have been widely incorporated into online decision support systems to identify high-quality reviews. Most current studies on helpfulness prediction assume that a review's helpfulness only relies on information from itself. In practice, however, consumers hardly process reviews independently because reviews are displayed in sequence; a review is more likely to be affected by its adjacent neighbors in the sequence, which is largely understudied. In this paper, we proposed the first end-to-end neural architecture to capture the missing interaction between reviews and their neighbors. Our model allows for a total of 12 (three selection × four aggregation) schemes that contextualize a review into the context clues learned from its neighbors. We evaluated our model on six domains of real-world online reviews against a series of state-of-the-art baselines. Experimental results confirm the influence of sequential neighbors on reviews and show that our model significantly outperforms the baselines by 1% to 5%. We further revealed how reviews are influenced by their neighbors during helpfulness perception via extensive analysis. The results and findings of our work provide theoretical contributions to the field of review helpfulness prediction and offer insights into practical decision support system design.}
}
@article{STANITSA2023100049,
title = {Investigating pedestrian behaviour in urban environments: A Wi-Fi tracking and machine learning approach},
journal = {Multimodal Transportation},
volume = {2},
number = {1},
pages = {100049},
year = {2023},
issn = {2772-5863},
doi = {https://doi.org/10.1016/j.multra.2022.100049},
url = {https://www.sciencedirect.com/science/article/pii/S2772586322000491},
author = {Avgousta Stanitsa and Stephen H Hallett and Simon Jude},
keywords = {Pedestrian movement, Machine-learning, Urban environment, Wi-Fi tracking, Human behaviour},
abstract = {Urban geometry plays a critical role in determining paths for pedestrian flow in urban areas. To improve the urban planning processes and to enhance quality of life for end-users in urban spaces, a better understanding of the factors influencing pedestrian movement is required by decision-makers within the urban design and planning industry. The aim of this study is to present a novel means to assess pedestrian routing in urban environments. As a unique contribution to knowledge and practice, this study: (a) enhances the body of knowledge by developing a conceptual model to assess and classify pedestrian movement behaviours, utilising machine learning algorithms and location data in conjunction with spatial attributes, and (b) extends previous research by revealing spatial visibility as a driver for pedestrian movement in urban environments. The importance of the findings lies in the perspective of revealing novel insights concerning individual preferences and behaviours of end-users and the utilisation of urban spaces. The approaches developed can be utilised for observations in large-scale contexts, as an addition to traditional methods. Application of the model in a high pedestrian traffic-dense retail urban area in London reveals clear and consistent relationships amongst spatial visibility, individuals’ motivation, and knowledge of the area. Key behaviours established in the study area are grouped into two activity categories: (i) Utilitarian walking (with motivation - expert and novice striders) and (ii) Leisure walking (no motivation - expert and novice strollers). The approach offers an insightful and automated means to understand pedestrian flow in urban contexts and informs wider wayfinding, walkability, and transportation knowledge.}
}
@article{YAN2021128665,
title = {A stack-based set inversion model for smart water, carbon and ecological assessment in urban agglomerations},
journal = {Journal of Cleaner Production},
volume = {319},
pages = {128665},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128665},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621028651},
author = {Pengdong Yan and Hongwei Lu and Yizhong Chen and Ziheng Li and Hao Li},
keywords = {Water, Carbon and ecological footprints, Smart evaluation and prediction, Ensemble inversion model, Urban agglomeration, Yangtze river},
abstract = {Footprint evaluation is an important tool for assessing the appropriation of ecological assets, GHG emissions, freshwater consumption parameters, etc., within a specified region. However, traditional evaluation of footprints for mega cities or urban agglomerations requires overmuch different types of high-quality data. There is a great need of seeking a smart model/approach with declined data requirements for evaluation of footprints where part of data can hardly be accessed. Here we propose a new ensemble inversion model (EIM) based on integrated multitask machine learning (MML) and multi-modeling stacking (MMS) algorithms for smart evaluation and prediction of water, carbon and ecological footprints. The accuracy and generalization capability of the model are illustrated through three largest urban agglomerations in the middle reaches of the Yangtze River (MRYR). The testing results show that the EIM achieves similar prediction performance compared to traditional footprints calculation methods (R2 = 0.91, RMSE = 0.18, MAE = 0.11), yet greatly reduces the amount of required data by approximately 80%. Moreover, the accuracy of the EIM is improved by more than 20%, compared with other models using a single inversion algorithm. The modeling results also show that 1) water, carbon and ecological footprints are significantly positively correlated, and 2) an annual increase of 4.8% can be found in terms of the urban environmental pressure index (UEPI), and its projection is even less optimistic for the future.}
}
@article{ARNARDOTTIR2021447,
title = {The Future of Sleep Measurements: A Review and Perspective},
journal = {Sleep Medicine Clinics},
volume = {16},
number = {3},
pages = {447-464},
year = {2021},
note = {Sleep Medicine: Current Challenges and its Future},
issn = {1556-407X},
doi = {https://doi.org/10.1016/j.jsmc.2021.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1556407X21000333},
author = {Erna Sif Arnardottir and Anna Sigridur Islind and María Óskarsdóttir},
keywords = {Sleep measurement, Subjective data, Objective data, Sleep diary, Codesign, Machine learning, Data management platform, Data science}
}
@article{PRIKSHAT2021100860,
title = {AI-augmented HRM: Antecedents, assimilation and multilevel consequences},
journal = {Human Resource Management Review},
pages = {100860},
year = {2021},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2021.100860},
url = {https://www.sciencedirect.com/science/article/pii/S1053482221000395},
author = {Verma Prikshat and Ashish Malik and Pawan Budhwar},
keywords = {Technology-driven HRM, AI-adoption in HRM, AI-augmented HRM, Processual factors},
abstract = {The current literature on the use of disruptive innovative technologies, such as artificial intelligence (AI) for human resource management (HRM) function, lacks a theoretical basis for understanding. Further, the adoption and implementation of AI-augmented HRM, which holds promise for delivering several operational, relational and transformational benefits, is at best patchy and incomplete. Integrating the technology, organisation and people (TOP) framework with core elements of the theory of innovation assimilation and its impact on a range of AI-Augmented HRM outcomes, or what we refer to as (HRM(AI)), this paper develops a coherent and integrated theoretical framework of HRM(AI) assimilation. Such a framework is timely as several post-adoption challenges, such as the dark side of processual factors in innovation assimilation and system-level factors, which, if unattended, can lead to the opacity of AI applications, thereby affecting the success of any HRM(AI). Our model proposes several testable future research propositions for advancing scholarship in this area. We conclude with implications for theory and practice.}
}
@article{PAUL2021100296,
title = {Mobile phone technologies for disaster risk reduction},
journal = {Climate Risk Management},
volume = {32},
pages = {100296},
year = {2021},
issn = {2212-0963},
doi = {https://doi.org/10.1016/j.crm.2021.100296},
url = {https://www.sciencedirect.com/science/article/pii/S2212096321000255},
author = {Jonathan D. Paul and Emma Bee and Mirianna Budimir},
keywords = {Citizen science, Disaster risk management (DRM), Disaster risk reduction (DRR), Mobile phone technologies, Natural hazards, User-centered design (UCD)},
abstract = {The explosion of increasingly sophisticated mobile phone technologies can usefully be harnessed by disaster risk reduction (DRR) as a means of enhancing inclusivity and local relevance of knowledge production and resilience building. However, much new technology is designed on an ad hoc basis without considering user needs – especially mobile applications (apps), which often terminate at the proof-of-concept stage. Here, we examine best practice by marshalling learnings from 45 workers representing 20 organisations working globally across the disaster risk management (DRM) lifecycle, including physical and social science, NGOs, technological developers, and (inter)governmental regulatory bodies. We present a series of generalisable and scalable guidelines that are novel in being independent of any specific natural hazard or development setting, designed to maximise the positive societal impact of exploiting mobile technologies. Specifically, the local context, dynamics, and needs must be carefully interrogated a priori, while any product should ideally be co-developed with local stakeholders through a user-centered design approach.}
}
@article{PARRALOPEZ2021105537,
title = {Digital transformation of the agrifood system: Quantifying the conditioning factors to inform policy planning in the olive sector},
journal = {Land Use Policy},
volume = {108},
pages = {105537},
year = {2021},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2021.105537},
url = {https://www.sciencedirect.com/science/article/pii/S026483772100260X},
author = {Carlos Parra-López and Liliana Reina-Usuga and Carmen Carmona-Torres and Samir Sayadi and Laurens Klerkx},
keywords = {Digitalisation, Olive, SWOT, PESTLE, AHP, TOWS},
abstract = {Despite the growing importance of the digital transformation (DT) of the agrifood sector on the political agenda, traditional policies are not enough to provide proactive responses to rapid technological changes and new approaches for policy planning are necessary especially at regional level. This manuscript proposes and illustrates the implementation of a new methodological framework for DT policy planning in the case of Andalusia, the olive world leader region, but applicable to other regions and sectors, with two objectives: 1) to quantitatively determine the importance of the conditioning factors of DT in the olive sector in the short/medium term, by developing an AHP/SWOT/PESTLE model, and 2) to design public policies to strengthen the DT, taking advantage of the potentialities and alleviating the deficiencies, by carrying out a quantitative TOWS analysis. The knowledge of diverse groups of experts, i.e. stakeholders in the sector, has been used in all analyses due to the lack of reliable data and the complex nature of the issues analysed. The results show that the opportunities and strengths are more prominent than weaknesses and threats for DT. Environmental issues stand out as an opportunity to boost DT. There is also a growing interest in developing an interoperability strategy which is an opportunity to overcome the low technological integration of the value chain. DT can also enable a more transparent value chain and improved traceability. Some negative factors are the lack of evidence on the economic viability of investment in digital technologies, shortage of labour and young farmers, and potential unintended and unanticipated effects of DT. Important policies strategies to foster DT are: improving environmental efficiency though DT; promoting youth employment in the sector; enhancing coordination among innovation actors; developing a common interoperability strategy; and fostering technological integration in the sector.}
}
@article{DAMIANSEGRELLESQUILIS202338,
title = {A federated cloud architecture for processing of cancer images on a distributed storage},
journal = {Future Generation Computer Systems},
volume = {139},
pages = {38-52},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200303X},
author = {J. {Damián Segrelles Quilis} and Sergio López-Huguet and Pau Lozano and Ignacio Blanquer},
keywords = {Medical imaging, Biomarkers, Storage and computing backends},
abstract = {The increased accuracy and exhaustivity of modern Artificial Intelligence techniques in supporting the analysis of complex data, such as medical images, have exponentially increased real-world data collection for research purposes. This fact has led to the development of international repositories and high-performance computing solutions to deal with the computational demand for training models. However, other stages in the development of medical imaging biomarkers do not require such intensive computing resources, which has led to the convenience of integrating different computing backends tailored for the processing demands of the various stages of processing workflows. We present in this article a distributed and federated repository architecture for the development and application of medical image biomarkers that combines multiple cloud storages with cloud and HPC processing backends. The architecture has been deployed to serve the PRIMAGE (H2020 826494) project, aiming to collect and manage data from paediatric cancer. The repository seamlessly integrates distributed storage backends, an elastic Kubernetes cluster on a cloud on-premises and a supercomputer. Processing jobs are handled through a single control platform, synchronising data on demand. The article shows the specification of the different types of applications and a validation through a use case that make use of most of the features of the platform.}
}
@article{PRASTYO2021108,
title = {A Combination of Query Expansion Ranking and GA-SVM for Improving Indonesian Sentiment Classification Performance},
journal = {Procedia Computer Science},
volume = {189},
pages = {108-115},
year = {2021},
note = {AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.05.074},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921011698},
author = {Pulung Hendro Prastyo and Igi Ardiyanto and Risanuri Hidayat},
keywords = {Query Expansion Ranking, Genetic Algorithm, Feature Selection, Machine Learning, Sentiment Classification},
abstract = {The sentiment classification method is a research field that is proliferating in Indonesia since it is fast in extracting public opinion and provides essential and valuable information for stakeholders. Of the best-performing sentiment classification approaches, machine learning is one of them that has excellent performance. However, the method has several problems, such as noisy features and high dimensionality of features that significantly affect the sentiment classification performance. Therefore, to overcome the problems, this paper presents a novel feature selection using a combination of Query Expansion Ranking (QER) and Genetic Algorithm-Support Vector Machine (GA-SVM) for improving sentiment classification performance. Based on the experimental results, the proposed method could significantly improve sentiment classification performance, outperform all state-of-the-art algorithms, and decrease computational time. The method achieved the best performance in average precision, recall, and f-measure with the value of 96.78%, 96.76%, and 96.75%, respectively.}
}
@article{WIERINGA2021915,
title = {Data analytics in a privacy-concerned world},
journal = {Journal of Business Research},
volume = {122},
pages = {915-925},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319303078},
author = {Jaap Wieringa and P.K. Kannan and Xiao Ma and Thomas Reutterer and Hans Risselada and Bernd Skiera},
abstract = {Data is considered the new oil of the economy, but privacy concerns limit their use, leading to a widespread sense that data analytics and privacy are contradictory. Yet such a view is too narrow, because firms can implement a wide range of methods that satisfy different degrees of privacy and still enable them to leverage varied data analytics methods. Therefore, the current study specifies different functions related to data analytics and privacy (i.e., data collection, storage, verification, analytics, and dissemination of insights), compares how these functions might be performed at different levels (consumer, intermediary, and firm), outlines how well different analytics methods address consumer privacy, and draws several conclusions, along with future research directions.}
}
@incollection{2021419,
title = {Index},
editor = {Subhi J. Al'Aref and Gurpreet Singh and Lohendran Baskaran and Dimitris Metaxas},
booktitle = {Machine Learning in Cardiovascular Medicine},
publisher = {Academic Press},
pages = {419-424},
year = {2021},
isbn = {978-0-12-820273-9},
doi = {https://doi.org/10.1016/B978-0-12-820273-9.20001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202739200018}
}
@article{SAMAL2021100943,
title = {Multi-output TCN autoencoder for long-term pollution forecasting for multiple sites},
journal = {Urban Climate},
volume = {39},
pages = {100943},
year = {2021},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2021.100943},
url = {https://www.sciencedirect.com/science/article/pii/S2212095521001735},
author = {K. Krishna Rani Samal and Ankit Kumar Panda and Korra Sathya Babu and Santos Kumar Das},
keywords = {Temporal convolutional network, Spatio-temporal prediction, Autoencoder, Deep learning, Pollution},
abstract = {Air pollution is one of the major environmental issues attracting massive attention from researchers and policymakers. Both the developed and developing countries are undergoing a high concentration of pollution levels. Fine particulate matter PM2.5 (particles having a diameter less than 2.5μm) and PM10 (particles having a diameter less than 10μm) can easily penetrate the lungs and respiratory system and causes adverse health issues like heart attacks, cardiovascular diseases, lung function reduction. Real-time pollutant information is of great importance to providing prompt and complete information on air quality. Air pollution forecasting is another significant step of air pollution management, which can help policymakers and citizens make proper decisions to prevent air pollution-related diseases. This research study explores a novel pollutant forecasting model named as Multi-output temporal convolutional network autoencoder (MO-TCNA). The model accumulates each step's predicted values to perform multi-step ahead long-term forecasting for multiple pollutants and multiple sites in a single training model. The MO-TCNA network serves both the PM2.5 and PM10 pollutants forecasting for various locations instead of performing single output and site-specific pollutant forecasting. Consequently, the experimental results show that the MO-TCNA network is time-saving and has better performance than the traditional site-specific forecasting models.}
}
@article{THAPA2021104130,
title = {Precision health data: Requirements, challenges and existing techniques for data security and privacy},
journal = {Computers in Biology and Medicine},
volume = {129},
pages = {104130},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2020.104130},
url = {https://www.sciencedirect.com/science/article/pii/S0010482520304613},
author = {Chandra Thapa and Seyit Camtepe},
keywords = {Precision health, Legal requirements, Ethical guidelines, Security, Privacy, Artificial intelligence},
abstract = {Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical conditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Besides, the public, who is the data source, always expects the security, privacy, and trust of their data. Otherwise, they can avoid contributing their data to the precision health system. Consequently, as the public is the targeted beneficiary of the system, the effectiveness of precision health diminishes. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it illustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain.}
}
@article{ALI2021111073,
title = {Review of urban building energy modeling (UBEM) approaches, methods and tools using qualitative and quantitative analysis},
journal = {Energy and Buildings},
volume = {246},
pages = {111073},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111073},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821003571},
author = {Usman Ali and Mohammad Haris Shamsi and Cathal Hoare and Eleni Mangina and James O’Donnell},
keywords = {Urban building energy modeling, Top-down, Bottom-up, Data-driven, Energy modeling, UBEM, Energy efficiency, SWOT},
abstract = {The world has witnessed a significant population shift to urban areas over the past few decades. Urban areas account for about two-thirds of the world’s total primary energy consumption, of which the urban building sector constitutes a significant proportion approximately 40%. Stakeholders such as urban planners and policy makers face substantial challenges when targeting sustainable energy and climate goals related to the buildings’ sector, i.e. to reduce energy use and associated emissions. Urban energy modeling is one possible solution that leverages limited resources to estimate building energy use and support appropriate policy formation. Over the past few years, there have been only a few review studies on urban building energy modeling approaches. These studies lack an in-depth discussion of the challenges and future research opportunities related to data-driven, reduced-order, and simulation-based modeling methods. This paper proposes Strengths, Weaknesses, Opportunities, and Threats (SWOT) analysis of approaches, methods and tools used for urban building energy modeling. Furthermore, this paper proposes a generalized framework based on existing literature for different urban energy modeling methods. The aim of this study is to assist urban planners and energy policymakers when choosing appropriate methods to develop and implement in-depth sustainable building energy planning and analysis projects based on limited available resources.}
}
@article{RAMYA2021114141,
title = {Establishment of bioinformatics pipeline for deciphering the biological complexities of fragmented sperm transcriptome},
journal = {Analytical Biochemistry},
volume = {620},
pages = {114141},
year = {2021},
issn = {0003-2697},
doi = {https://doi.org/10.1016/j.ab.2021.114141},
url = {https://www.sciencedirect.com/science/article/pii/S0003269721000427},
author = {Laxman Ramya and Divakar Swathi and Santhanahalli Siddalingappa Archana and Maharajan Lavanya and Sivashanmugam Parthipan and Sellappan Selvaraju},
keywords = {Bioinformatics pipeline, Fragmented transcripts, Transcriptomics, Differential gene expression, Bovine spermatozoa},
abstract = {Despite the development of several tools for the analysis of the transcriptome data, non-availability of a standard pipeline for analyzing the low quality and fragmented mRNA samples pose a major challenge to the computational molecular biologist for effective interpretation of the data. Hence the present study aimed to establish a bioinformatics pipeline for analyzing the biologically fragmented sperm RNA. Sperm transcriptome data (2 x 75 PE sequencing) generated from bulls (n = 8) of high-fertile (n = 4) and low-fertile (n = 4) classified based on the fertility rate (41.52 ± 1.07 vs 36.04 ± 1.04%) were analyzed with different bioinformatics tools for alignment, quantitation, and differential gene expression studies. TopHat2 was effectual compared to HISAT2 and STAR for sperm mRNA due to the higher exonic (6% vs 2%) mapping percentage and quantitating the low expressed genes. TopHat2 also had significantly strong correlation with STAR (0.871, p = 0.05) and HISAT2 (0.933, p = 0.01). TopHat2 and Cufflinks combo quantitated the number of genes higher than the other combinations. Among the tools (Cuffdiff, DESeq, DESeq2, edgeR, and limma) used for the differential gene expression analysis, edgeR and limma identified the largest number of significantly differentially expressed genes (p < 0.05) with biological relevance. The concordance analysis concurred that edgeR had an edge over the other tools. It also identified a higher number (9.5%) of fertility-related genes to be differentially expressed between the two groups. The present study established that TopHat2, Cufflinks, and edgeR as a suitable pipeline for the analysis of fragmented mRNA from bovine spermatozoa.}
}
@article{SCHNEBELIN2021599,
title = {How digitalisation interacts with ecologisation? Perspectives from actors of the French Agricultural Innovation System},
journal = {Journal of Rural Studies},
volume = {86},
pages = {599-610},
year = {2021},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2021.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0743016721002205},
author = {Éléonore Schnebelin and Pierre Labarthe and Jean-Marc Touzard},
keywords = {Digitalisation, Agriculture, Digital technology, Agricultural innovation system, Organic farming, Institutional economics, Ecological transition},
abstract = {Two major agricultural transformations are currently being promoted worldwide: digitalisation and ecologisation, that include different practices such as organic farming and sustainable intensification. In literature and in societal debates, these two transformations are sometimes described as antagonistic and sometimes as convergent but are rarely studied together. Using an innovation system approach, this paper discusses how diverse ecologisation pathways grasp digitalisation in the French agricultural sector; and do not discriminate against organic farming. Based on interviews with key representatives of conventional agriculture, organic agriculture and organisations that promote or develop digital agriculture, we explore how these actors perceive and participate in digital development in agriculture. We show that although all the actors are interested and involved in digital development, behind this apparent convergence, organic and conventional actors perceive neither the same benefits nor the same risks and consequently do not implement the same innovation processes. We conclude that digitalisation has different meanings depending on the actors’ paradigm, but that digital actors fail to perceive these differences. This difference in perception should be taken into account if digital development is to benefit all kinds of agriculture and not discriminate against organic farming and more widely, against agroecology.}
}
@article{SINGH202153,
title = {Challenges and Opportunities in Machine-Augmented Plant Stress Phenotyping},
journal = {Trends in Plant Science},
volume = {26},
number = {1},
pages = {53-69},
year = {2021},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2020.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1360138520302405},
author = {Arti Singh and Sarah Jones and Baskar Ganapathysubramanian and Soumik Sarkar and Daren Mueller and Kulbir Sandhu and Koushik Nagasubramanian},
keywords = {image-based phenotyping, machine learning, deep learning, biotic stress, abiotic stress, standard area diagram},
abstract = {Plant stress phenotyping is essential to select stress-resistant varieties and develop better stress-management strategies. Standardization of visual assessments and deployment of imaging techniques have improved the accuracy and reliability of stress assessment in comparison with unaided visual measurement. The growing capabilities of machine learning (ML) methods in conjunction with image-based phenotyping can extract new insights from curated, annotated, and high-dimensional datasets across varied crops and stresses. We propose an overarching strategy for utilizing ML techniques that methodically enables the application of plant stress phenotyping at multiple scales across different types of stresses, program goals, and environments.}
}
@article{HAO2021e01512,
title = {Different response of alpine meadow and alpine steppe to climatic and anthropogenic disturbance on the Qinghai-Tibetan Plateau},
journal = {Global Ecology and Conservation},
volume = {27},
pages = {e01512},
year = {2021},
issn = {2351-9894},
doi = {https://doi.org/10.1016/j.gecco.2021.e01512},
url = {https://www.sciencedirect.com/science/article/pii/S2351989421000627},
author = {Aihua Hao and Hanchen Duan and Xufeng Wang and Guohui Zhao and Quangang You and Fei Peng and Heqiang Du and Feiyao Liu and Chengyang Li and Chimin Lai and Xian Xue},
keywords = {NDVI, Vegetation variation, Climate change, Anthropogenic disturbance, Qinghai-Tibetan Plateau},
abstract = {Climate change and anthropogenic disturbance are two main drivers for vegetation dynamics on the Qinghai-Tibetan Plateau (QTP). Alpine meadow and alpine steppe are the primary rangeland ecosystem types on the QTP. However, the vegetation trends of the two land cover types and the underlying mechanisms behind their variation remain under debate. In this study, we used Global Inventory Modeling and Mapping Studies (GIMMS) 3g Normalized Difference Vegetation Index (NDVI) (i.e., GIMMS NDVI3g) by coupling the Breaks for Additive Season and Trend (BFAST) model and the Boosted Regression Tree (BRT) model to analyze alpine meadow and alpine steppe vegetation trends on the QTP between 1982 and 2015. We also assessed vegetation variation response to climatic and anthropogenic indicators in conjunction with climatic and human footprint datasets. Results show that growing season NDVI (GSNDVI) values increased overall for both alpine meadow (0.0001 year−1, p = 0.33) and alpine steppe (0.0002 year−1, p < 0.05) throughout 1982–2015. Significant greening trends in both alpine meadow (0.0007 year−1; p < 0.05) and alpine steppe (0.0005 year−1; p < 0.05) ecosystems were obtained before 1998 and 2001, respectively. However, browning trends ascertained by GSNDVI (−0.0006 year−1; p = 0.12) in alpine meadows were observed throughout 1998–2015, while greening trends ascertained by GSNDVI (0.0002 year−1; p = 0.12) in alpine steppes were observed throughout during 2001–2015. Opposing trends in precipitation, solar radiation, and the Standardized Precipitation Evapotranspiration Index (SPEI) occurred before and after breakpoints in both ecosystems. For the alpine meadow ecosystem, adverse precipitation trends caused browning before 1998 followed by greening after 1998 in the Three-River-Source National Park (TNP). Conversely, opposing changes in precipitation, solar radiation, and SPEI resulted in greening before 1998 followed by browning after 1998 in southern Tibet and the southeastern QTP. Alpine meadow vegetation trends were generally dominated by solar radiation before 1998 and jointly by precipitation and solar radiation after 1998. Prior to 2001 variation in alpine steppe greenness was controlled by precipitation, while after 2001 solar radiation dominated. Along with an increase in human footprint pressure (HFP) gradients, greenness trends gradually increased before 1998 but reversed after 1998 in the alpine meadow ecosystem. Additionally, greenness trends gradually decreased before 2001 but remained unchanged after 2001 for the alpine steppe ecosystem. These results highlight the different effects that climate change and anthropogenic disturbances have had on alpine meadow and alpine steppe ecosystems on the QTP over different time frames.}
}
@article{ALBAHRI2021102873,
title = {IoT-based telemedicine for disease prevention and health promotion: State-of-the-Art},
journal = {Journal of Network and Computer Applications},
volume = {173},
pages = {102873},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102873},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520303374},
author = {A.S. Albahri and Jwan K. Alwan and Zahraa K. Taha and Sura F. Ismail and Rula A. Hamid and A.A. Zaidan and O.S. Albahri and B.B. Zaidan and A.H. Alamoodi and M.A. Alsalem},
keywords = {Telemedicine, Remote monitoring, Healthcare services, Diseases, Internet of things, Network},
abstract = {Numerous studies have focused on making telemedicine smart through the Internet of Things (IoT) technology. These works span a wide range of research areas to enhance telemedicine architecture such as network communications, artificial intelligence methods and techniques, IoT wearable sensors and hardware devices, smartphones and cloud computing. Accordingly, several telemedicine applications covering various human diseases have presented their works from a specific perspective and resulted in confusion regarding the IoT characteristics. Although such applications are useful and necessary for improving telemedicine contexts related to monitoring, detection and diagnostics, deriving an overall picture of how IoT characteristics are currently integrated with the telemedicine architecture is difficult. Accordingly, this study complements the academic literature with a systematic review covering all main aspects of advances in IoT-based telemedicine architecture. This study also provides a state-of-the-art telemedicine classification taxonomy under IoT and reviews works in different fields in relation to that classification. To this end, this study checked the ScienceDirect, Institute of Electrical and Electronics Engineers (IEEE) Xplore, and Web of Science databases. A total of 2121 papers were collected from 2014 to July 2020. The retrieved articles were filtered according to the defined inclusion criteria. A final set of 141 articles were selected and classified into two categories, each followed by subcategories and sections. The first category includes an IoT-based telemedicine network that accounts for 24.11% (n = 34/141). The second category includes IoT-based telemedicine healthcare services and applications that account for 75.89% (n = 107/141). This multi-field systematic review has exposed new research opportunities, motivations, recommendations and challenges that need attention for the synergistic integration of interdisciplinary works. This extensive study also lists a set of open issues and provides innovative key solutions along with a systematic review. The classification of diseases under IoT-based telemedicine is divided into 14 groups. Furthermore, the crossover in our taxonomy is demonstrated. The lifecycle of the context of IoT-based telemedicine healthcare applications is mapped for the first time, including the procedure sequencing and definition for each context. We believe that this study is a useful guide for researchers and practitioners in providing direction and valuable information for future research. This study can also address the ambiguity in the trends in IoT-based telemedicine.}
}
@article{JIANG2021106431,
title = {A comprehensive study of macro factors related to traffic fatality rates by XGBoost-based model and GIS techniques},
journal = {Accident Analysis & Prevention},
volume = {163},
pages = {106431},
year = {2021},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106431},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521004620},
author = {Feifeng Jiang and Jun Ma},
keywords = {Traffic fatality rates, Macro factors, National scale, XGBoost, GIS, Feature importance},
abstract = {With the fast development of economics, road safety is becoming a serious problem. Exploring macro factors is effective to improve road safety. However, the existing studies have some limitations: (1) The existing studies only considered one aspect of macro factors and constructed models based on a few data samples. (2) The methods commonly used cannot address the non-linear relationship or calculate the feature importance. The findings obtained from such models may be limited and biased. To address the limitations, this study proposes a BO-CV-XGBoost framework to explore the macro factors related to traffic fatality rate classes based on a high-dimensional dataset that fully considers the impact of multi-factor interaction with adequate data samples. The proposed framework is applied to a dataset in the US. 453 county-level macro factors are collected from various data sources, covering ten macro aspects, including topography, transportation, etc. The optimized BO-CV-XGBoost model obtains the best classification performance with an AUC of 0.8977 and an accuracy of 85.02%. Compared with other methods, the proposed model has superiority on fatality rate classification. Ten macro factors are identified, including ‘Current-dollar GDP’, ‘highway miles per person’, etc. The ten factors contain four aspects of information, including economics, transportation, education, and medical condition. Geographic information system (GIS) techniques are further used for spatial analysis of the identified macro factors. Therefore, targeted and effective measures are accordingly proposed to prevent traffic fatalities and improve road safety}
}
@article{LO2021101297,
title = {A review of digital twin in product design and development},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101297},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101297},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000513},
author = {C.K. Lo and C.H. Chen and Ray Y. Zhong},
keywords = {Digital twin, Product design, New product development, Product lifecycle, Review},
abstract = {In the era of digitalization, there are many emerging technologies, such as the Internet of Things (IoT), Digital Twin (DT), Cloud Computing and Artificial Intelligence (AI), which are quickly developped and used in product design and development. Among those technologies, DT is one promising technology which has been widely used in different industries, especially manufacturing, to monitor the performance, optimize the progresses, simulate the results and predict the potential errors. DT also plays various roles within the whole product lifecycle from design, manufacturing, delivery, use and end-of-life. With the growing demands of individualized products and implementation of Industry 4.0, DT can provide an effective solution for future product design, development and innovation. This paper aims to figure out the current states of DT research focusing on product design and development through summarizing typical industrial cases. Challenges and potential applications of DT in product design and development are also discussed to inspire future studies.}
}
@article{WANG2021126293,
title = {A hybrid deep learning model with 1DCNN-LSTM-Attention networks for short-term traffic flow prediction},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {583},
pages = {126293},
year = {2021},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2021.126293},
url = {https://www.sciencedirect.com/science/article/pii/S0378437121005665},
author = {Ke Wang and Changxi Ma and Yihuan Qiao and Xijin Lu and Weining Hao and Sheng Dong},
keywords = {Traffic flow prediction, Deep learning, One-dimensional convolutional neural network, Long short-term memory network, Attentional mechanism},
abstract = {With the rapid development of social economy, the traffic volume of urban roads has raised significantly, which has led to increasingly serious urban traffic congestion problems, and has caused much inconvenience to people’s travel. By focusing on the complexity and long-term dependence of traffic flow sequences on urban road, this paper considered the traffic flow data and weather conditions of the road section comprehensively, and proposed a short-term traffic flow prediction model based on the attention mechanism and the 1DCNN-LSTM network. The model combined the time expansion of the CNN and the advantages of the long-term memory of the LSTM. First, the model employs 1DCNN network to extract the spatial features in the road traffic flow data. Second, the output spatial features are considered as the input of LSTM neural network to extract the time features in road traffic flow data, and the long-term dependence characteristics of LSTM neural network are adopted to improve the prediction accuracy of traffic flow. Next, the spatio-temporal characteristics of road traffic flow were regarded as the input of the regression prediction layer, and the prediction results corresponding to the current input were calculated. Finally, the attention mechanism was introduced on the LSTM side to give enough attention to the key information, so that the model can focus on learning more important data features, and further improve the prediction performance. The experimental results showed that the prediction effect of the 1DCNN-LSTM-Attention model under the weather factor was better than that without considering the weather factor. At the same time, compared with traditional neural network models, the prediction effect of the proposed model revealed faster convergence speed and higher prediction accuracy. It can be found that for short-term traffic flow prediction on urban roads, the 1DCNN-LSTM network structure considering the attention mechanism provides superior features.}
}
@article{FU2021688,
title = {Automated Detection of Periprosthetic Joint Infections and Data Elements Using Natural Language Processing},
journal = {The Journal of Arthroplasty},
volume = {36},
number = {2},
pages = {688-692},
year = {2021},
issn = {0883-5403},
doi = {https://doi.org/10.1016/j.arth.2020.07.076},
url = {https://www.sciencedirect.com/science/article/pii/S088354032030869X},
author = {Sunyang Fu and Cody C. Wyles and Douglas R. Osmon and Martha L. Carvour and Elham Sagheb and Taghi Ramazanian and Walter K. Kremers and David G. Lewallen and Daniel J. Berry and Sunghwan Sohn and Hilal Maradit Kremers},
keywords = {total joint arthroplasty, periprosthetic joint infection, natural language processing, electronic health records, artificial intelligence},
abstract = {Background
Periprosthetic joint infection (PJI) data elements are contained in both structured and unstructured documents in electronic health records and require manual data collection. The goal of this study is to develop a natural language processing (NLP) algorithm to replicate manual chart review for PJI data elements.
Methods
PJI was identified among all total joint arthroplasty (TJA) procedures performed at a single academic institution between 2000 and 2017. Data elements that comprise the Musculoskeletal Infection Society (MSIS) criteria were manually extracted and used as the gold standard for validation. A training sample of 1208 TJA surgeries (170 PJI cases) was randomly selected to develop the prototype NLP algorithms and an additional 1179 surgeries (150 PJI cases) were randomly selected as the test sample. The algorithms were applied to all consultation notes, operative notes, pathology reports, and microbiology reports to predict the correct status of PJI based on MSIS criteria.
Results
The algorithm, which identified patients with PJI based on MSIS criteria, achieved an f1-score (harmonic mean of precision and recall) of 0.911. Algorithm performance in extracting the presence of sinus tract, purulence, pathologic documentation of inflammation, and growth of cultured organisms from the involved TJA achieved f1-scores that ranged from 0.771 to 0.982, sensitivity that ranged from 0.730 to 1.000, and specificity that ranged from 0.947 to 1.000.
Conclusion
NLP-enabled algorithms have the potential to automate data collection for PJI diagnostic elements, which could directly improve patient care and augment cohort surveillance and research efforts. Further validation is needed in other hospital settings.
Level of Evidence
Level III, Diagnostic.}
}
@article{WILHELM2021278,
title = {Overview on hybrid approaches to fault detection and diagnosis: Combining data-driven, physics-based and knowledge-based models},
journal = {Procedia CIRP},
volume = {99},
pages = {278-283},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.041},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121003152},
author = {Yannick Wilhelm and Peter Reimann and Wolfgang Gauchel and Bernhard Mitschang},
keywords = {Fault detection, Fault diagnosis, Hybrid methods, Diagnostics, maintenance, Knowledge-driven methods, Machine learning},
abstract = {In this paper, we review hybrid approaches for fault detection and fault diagnosis (FDD) that combine data-driven analysis with physics-based and knowledge-based models to overcome a lack of data and to increase the FDD accuracy. We categorize these hybrid approaches according to the steps of an extended common workflow for FDD. This gives practitioners indications of which kind of hybrid FDD approach they can use in their application.}
}
@article{ZENG2021111661,
title = {Biological characteristics of energy conversion in carbon fixation by microalgae},
journal = {Renewable and Sustainable Energy Reviews},
volume = {152},
pages = {111661},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111661},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121009369},
author = {Jing Zeng and Zhenjun Wang and Guobin Chen},
keywords = {Microalgae, Carbon fixation, Photosynthetic reaction, Carbon pump, Energy conversion},
abstract = {CO2-fixation by microalgae can be regarded as a biological process of energy conversion with CO2 and H2O in microalgae cells in the sunlight. The study of the biological intrinsic characteristics of energy conversion is helpful to reveal the high-efficiency carbon fixation mechanism of microalgae. Firstly, the CO2 emission control technology and the external influencing factors are summarized in this paper, which have laid the foundation for researching the internal biological intrinsic characteristics of carbon fixation by microalgae. Based on photosynthetic reactions, in-situ reaction experiments, hydrodynamic simulations and metabolic networks have been integrated to analyze the biological intrinsic characteristics of carbon fixation by microalgae. The collation of representative studies on theory and quantitative calculation methods reveals that free energy dissipation seriously affects the carbon fixation efficiency of microalgae; Secondly, thermodynamics and metabolic networks are discussed. The role of thermodynamics in addressing the constraints is explored mainly from the perspective of energy conversion mechanisms, free energy dissipation mechanisms, framework and methods. Metabolic networks are studied using sampling methods based on thermodynamic systems and metabolic engineering based on a systems perspective; Thirdly, the key supporting technologies and biological intrinsic characteristics are reviewed from an interdisciplinary perspective, and the researches on metabolic networks based on thermodynamic constraints are given; Finally, challenges are summarized to provide a basis and direction for future research.}
}
@article{ZHANG2021109380,
title = {Collision-avoidance navigation systems for Maritime Autonomous Surface Ships: A state of the art survey},
journal = {Ocean Engineering},
volume = {235},
pages = {109380},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109380},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821007940},
author = {Xinyu Zhang and Chengbo Wang and Lingling Jiang and Lanxuan An and Rui Yang},
keywords = {Collision avoidance, Autonomous navigation systems, Cognitive navigation, e-navigation, Maritime autonomous surface ships},
abstract = {The rapid development of artificial intelligence significantly promotes collision-avoidance navigation of maritime autonomous surface ships (MASS), which in turn provides prominent services in maritime environments and enlarges the opportunity for coordinated and interconnected operations. Clearly, full autonomy of the collision-avoidance navigation for the MASS in complex environments still faces huge challenges and highly requires persistent innovations. First, we survey relevant guidance of the International Maritime Organization (IMO) and industry code of each country on MASS. Then, major advances in MASS industry R&D, and collision-avoidance navigation technologies, are thoroughly overviewed, from academic to industrial sides. Moreover, compositions of collision-avoidance navigation, brain-inspired cognitive navigation, and e-navigation technologies are analyzed to clarify the mechanism and principles efficiently systematically in typical maritime environments, whereby trends in maritime collision-avoidance navigation systems are highlighted. Finally, considering a general study of existing collision avoidance and action planning technologies, it is pointed out that collision-free navigation would significantly benefit the integration of MASS autonomy in various maritime scenarios.}
}
@article{ZHANG2021109535,
title = {Data mining approach for automatic ship-route design for coastal seas using AIS trajectory clustering analysis},
journal = {Ocean Engineering},
volume = {236},
pages = {109535},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109535},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821009288},
author = {Daheng Zhang and Yingjun Zhang and Chuang Zhang},
keywords = {RU, FA-DBSCAN, Ship-route design, Data mining, AIS data},
abstract = {In this paper, we propose an automatic route design method based on simple recurrent unit (SRU) and automatic identification system (AIS) data. Laplacian eigen maps and Gaussian kernel functions are used to compress the AIS data and extract the turning points of all ships. Fuzzy adaptive density-based spatial clustering of applications with noise (FA-DBSCAN) technique is used to cluster the turning points obtained at the preprocessing stage to obtain the turning region. Optimal turn region matching is used to connect the turning regions of similar routes, and the SRU neural network algorithm is used to learn the relationship between different types, sizes, and drafts of ships in each turning region; extract the feature-turning points; and obtain the recommended coastal routes, speed, and course of each type of ship. In the experimental stage, a large variety of AIS data from two sea areas are used to compare and analyze the designed route and real-ship data through LSTM and SRU experiments. The results show that the SRU algorithm improves the training speed and accuracy in comparison to LSTM, while the generated automatic route meets the requirements of navigation practice.}
}
@article{YANG2021103303,
title = {Freeway accident detection and classification based on the multi-vehicle trajectory data and deep learning model},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {130},
pages = {103303},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103303},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21003120},
author = {Da Yang and Yuezhu Wu and Feng Sun and Jing Chen and Donghai Zhai and Chuanyun Fu},
keywords = {Freeway traffic accident, Vehicle trajectory, Deep Convolutional Neural Network, Accident detection and classification},
abstract = {The freeway accident detection and classification have attracted much attention of researchers in the past decades. With the popularity of Global Navigation Satellite System (GNSS) on mobile phones and onboard equipment, increasing amounts of real-time vehicle trajectory data can be obtained more and more easily, which provides a potential way to use the multi-vehicle trajectory data to detect and classify an accident on freeways. The data has the advantages of low cost, high penetration, high real-time performance, and being robust to the outdoor environment. Therefore, this paper proposes a new method for accident detection and classification based on the multi-vehicle trajectory data. Different from the existing methods using GNSS positioning data, the proposed method not only uses the position information of the related vehicles but also tries to capture the development tendencies of the trajectories of accident vehicles over a period of time. A Deep Convolutional Neural Network model is developed to recognize an accident from the normal driving of vehicles and also identify the type of the accident, and the six types of traffic accidents are considered in this study. To train and test the proposed model, the simulated trajectory data is generated based on PC-Crash, including the normal driving trajectories and the trajectories before, in, and after an accident. The results indicate that the detection accuracy of the proposed method can reach up to 100%, and the classification accuracy can reach up to 95%, which both outperform the existing methods using other data. In addition, to ensure the robustness of the detection accuracy, at least 1 s of duration and 5 Hz of frequency for the trajectory data should be adopted in practice. The study will help to accurately and fastly detect an accident, recognize the accident type, and future judge who is liable for the accident.}
}
@article{FADLELMOLA2021103900,
title = {Data Management Plans in the genomics research revolution of Africa: Challenges and recommendations},
journal = {Journal of Biomedical Informatics},
volume = {122},
pages = {103900},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103900},
url = {https://www.sciencedirect.com/science/article/pii/S153204642100229X},
author = {Faisal M. Fadlelmola and Lyndon Zass and Melek Chaouch and Chaimae Samtal and Verena Ras and Judit Kumuthini and Sumir Panji and Nicola Mulder},
keywords = {Data Management Plan, Africa, FAIR, Funders, Data sharing, Genomics data management},
abstract = {Drafting and writing a data management plan (DMP) is increasingly seen as a key part of the academic research process. A DMP is a document that describes how a researcher will collect, document, describe, share, and preserve the data that will be generated as part of a research project. The DMP illustrates the importance of utilizing best practices through all stages of working with data while ensuring accessibility, quality, and longevity of the data. The benefits of writing a DMP include compliance with funder and institutional mandates; making research more transparent (for reproduction and validation purposes); and FAIR (findable, accessible, interoperable, reusable); protecting data subjects and compliance with the General Data Protection Regulation (GDPR) and/or local data protection policies. In this review, we highlight the importance of a DMP in modern biomedical research, explaining both the rationale and current best practices associated with DMPs. In addition, we outline various funders’ requirements concerning DMPs and discuss open-source tools that facilitate the development and implementation of a DMP. Finally, we discuss DMPs in the context of African research, and the considerations that need to be made in this regard.}
}
@article{KORU2021979,
title = {Bringing Quality Health Care Home via Technology Innovations},
journal = {Journal of the American Medical Directors Association},
volume = {22},
number = {5},
pages = {979-980},
year = {2021},
issn = {1525-8610},
doi = {https://doi.org/10.1016/j.jamda.2021.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S1525861021003376},
author = {Güneş Koru}
}
@article{SUBRAMANIYAN2021734,
title = {Artificial intelligence for throughput bottleneck analysis – State-of-the-art and future directions},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {734-751},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.021},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001588},
author = {Mukund Subramaniyan and Anders Skoogh and Jon Bokrantz and Muhammad Azam Sheikh and Matthias Thürer and Qing Chang},
keywords = {Throughput bottlenecks, Artificial intelligence, Production system, Data-driven, Manufacturing},
abstract = {Identifying, and eventually eliminating throughput bottlenecks, is a key means to increase throughput and productivity in production systems. In the real world, however, eliminating throughput bottlenecks is a challenge. This is due to the landscape of complex factory dynamics, with several hundred machines operating at any given time. Academic researchers have tried to develop tools to help identify and eliminate throughput bottlenecks. Historically, research efforts have focused on developing analytical and discrete event simulation modelling approaches to identify throughput bottlenecks in production systems. However, with the rise of industrial digitalisation and artificial intelligence (AI), academic researchers explored different ways in which AI might be used to eliminate throughput bottlenecks, based on the vast amounts of digital shop floor data. By conducting a systematic literature review, this paper aims to present state-of-the-art research efforts into the use of AI for throughput bottleneck analysis. To make the work of the academic AI solutions more accessible to practitioners, the research efforts are classified into four categories: (1) identify, (2) diagnose, (3) predict and (4) prescribe. This was inspired by real-world throughput bottleneck management practice. The categories, identify and diagnose focus on analysing historical throughput bottlenecks, whereas predict and prescribe focus on analysing future throughput bottlenecks. This paper also provides future research topics and practical recommendations which may help to further push the boundaries of the theoretical and practical use of AI in throughput bottleneck analysis.}
}
@article{SHARMA2021102316,
title = {Fifty years of information management research: A conceptual structure analysis using structural topic modeling},
journal = {International Journal of Information Management},
volume = {58},
pages = {102316},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102316},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000098},
author = {Anuj Sharma and Nripendra P. Rana and Robin Nunkoo},
keywords = {Information management, Structural topic models, Topic modeling, Generative models, Text analytics},
abstract = {Information management is the management of organizational processes, technologies, and people which collectively create, acquire, integrate, organize, process, store, disseminate, access, and dispose of the information. Information management is a vast, multi-disciplinary domain that syndicates various subdomains and perfectly intermingles with other domains. This study aims to provide a comprehensive overview of the information management domain from 1970 to 2019. Drawing upon the methodology from statistical text analysis research, this study summarizes the evolution of knowledge in this domain by examining the publication trends as per authors, institutions, countries, etc. Further, this study proposes a probabilistic generative model based on structural topic modeling to understand and extract the latent themes from the research articles related to information management. Furthermore, this study graphically visualizes the variations in the topic prevalences over the period of 1970 to 2019. The results highlight that the most common themes are data management, knowledge management, environmental management, project management, service management, and mobile and web management. The findings also identify themes such as knowledge management, environmental management, project management, and social communication as academic hotspots for future research.}
}
@article{METCALF2021100507,
title = {Challenges in evaluating risks and policy options around endemic establishment or elimination of novel pathogens},
journal = {Epidemics},
volume = {37},
pages = {100507},
year = {2021},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2021.100507},
url = {https://www.sciencedirect.com/science/article/pii/S1755436521000566},
author = {C. Jessica E. Metcalf and Soa Fy Andriamandimby and Rachel E. Baker and Emma E. Glennon and Katie Hampson and T. Deirdre Hollingsworth and Petra Klepac and Amy Wesolowski},
keywords = {Endemic, Epidemic, Mathematical model, Elimination, Emergence},
abstract = {When a novel pathogen emerges there may be opportunities to eliminate transmission - locally or globally - whilst case numbers are low. However, the effort required to push a disease to elimination may come at a vast cost at a time when uncertainty is high. Models currently inform policy discussions on this question, but there are a number of open challenges, particularly given unknown aspects of the pathogen biology, the effectiveness and feasibility of interventions, and the intersecting political, economic, sociological and behavioural complexities for a novel pathogen. In this overview, we detail how models might identify directions for better leveraging or expanding the scope of data available on the pathogen trajectory, for bounding the theoretical context of emergence relative to prospects for elimination, and for framing the larger economic, behavioural and social context that will influence policy decisions and the pathogen’s outcome.}
}
@article{LI2021129113,
title = {Spatializing environmental footprint by integrating geographic information system into life cycle assessment: A review and practice recommendations},
journal = {Journal of Cleaner Production},
volume = {323},
pages = {129113},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129113},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621033023},
author = {Junjie Li and Yajun Tian and Yueling Zhang and Kechang Xie},
keywords = {Life cycle assessment, Geographic information system, Environmental footprint, GIS-LCA},
abstract = {Life cycle assessment (LCA) is a methodological tool that estimates the environmental footprint from a cradle-to-grave perspective. With the increased need for the geographically explicit assessment, the geographic information system (GIS) is integrating into LCA as a frontier methodology to spatialize the environmental footprint. This paper reviews a total of 105 publications about GIS-LCA, including 50 methodological studies that are analyzed following the four phases of LCA and 55 applied studies that are classified into different domains. The review shows that although GIS-LCA methodology has certain explorations and practices and a large number of cases are carried out in the energy industry, agricultural sector, urban facility, and waste management, the current knowledge system faces several challenges in spatializing environmental footprint. In this case, a universal methodology framework of GIS-LCA and specific schemes are proposed to address the following issues: (1) how to set up a geographically referenced system in the goal and scope definition phase; (2) how to spatialize life-cycle data and integrate and compute foreground and background data in the inventory analysis phase; (3) how to develop spatialized characterization factors with different requirements on resolution and data availability in the impact assessment phase; and (4) how to uniform the contribution analysis of different zones, unit processes, and elementary flows to visualize spatialized environmental footprint in the interpretation phase. The framework we developed provides preliminary practices and recommendations for spatializing environmental footprint, which lays a foundation to support future work.}
}
@article{MIAO2021108327,
title = {Federated deep reinforcement learning based secure data sharing for Internet of Things},
journal = {Computer Networks},
volume = {197},
pages = {108327},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108327},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003285},
author = {Qinyang Miao and Hui Lin and Xiaoding Wang and Mohammad Mehedi Hassan},
keywords = {Secure data sharing, Federated learning, Deep reinforcement learning, IoT},
abstract = {The increasing number of Internet of Things (IoT) devices motivate the data sharing that improves the quality of IoT services. However, data providers usually suffer from the privacy leakage caused by direct data sharing. To solve this problem, in this paper, we propose a Federated Learning based Secure data Sharing mechanism for IoT, named FL2S. Specifically, to accomplish efficient and secure data sharing, a hierarchical asynchronous federated learning (FL) framework is developed based on the sensitive task decomposition. In addition, to improve data sharing quality, the deep reinforcement learning (DRL) technology is utilized to select participants of sufficient computational capabilities and high quality datasets. By integrating task decomposition and participant selection, reliable data sharing is realized by sharing local data models instead of the source data with data privacy preserved. Experiment results show that the proposed FL2S achieves high accuracy in secure data sharing for various IoT applications.}
}
@article{WANG20211,
title = {Knowledge transfer methods for expressing product design information and organization},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {1-15},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301965},
author = {Haishuo Wang and Ke Chen and Hongmei Zheng and Guojun Zhang and Rui Wu and Xiaopeng Yu},
keywords = {Product design, Information gap, Hypercycle theory, Information processing, Information carrier},
abstract = {Product design information represents not only the carrier of design but also the significant digital assets of businesses. At present, manufacturing is facing an environment with mass, fragmented, real-time, and multi-scene digital information in the process of product design. To improve the availability of information resources and the efficiency of information reuse as well as to achieve the sharing of production means, the expression of product design information should be optimized in information storage. In addition, a new ecosystem should be built for information increments, making the participants of every link in the process of product design become the contributors of information. Considering the unique aspects of design group individuals, this paper builds a knowledge transfer model that capitalizes on hypercycle theory and proposes the concept of modularizing information carriers and information processing methods. It comprehensively analyses the expression methodology and organizational attributes of product design information and proposes a knowledge transfer carrier model constructed via discretized fragmented semantic information, which is concretely implemented as informative product file labelling. By combining personnel, information carriers and information dissemination networks, this paper provides the functionality and architecture needed to build an information processing platform using social networking software (SNS). Some application scenarios are described by using this processing method for production information; the development process for an automotive air filter is shown as an example. The results of this study suggest that the proposed method is conducive to improving the expressions of product design information and the interactions among participants. The simplicity of the labelling process and the intuitive label content greatly reduce the usability threshold and the losses caused by information gaps, creating a precondition for many types of people to fully participate in the product design information knowledge transfer cycle.}
}
@incollection{BRAHEM2021269,
title = {12 - Data perspective on environmental mobile crowd sensing},
editor = {Siddhartha Bhattacharyya and Naba Kumar Mondal and Jan Platos and Václav Snášel and Pavel Krömer},
booktitle = {Intelligent Environmental Data Monitoring for Pollution Management},
publisher = {Academic Press},
pages = {269-288},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-819671-7},
doi = {https://doi.org/10.1016/B978-0-12-819671-7.00012-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196717000129},
author = {Mariem Brahem and Hafsa E.L. Hafyani and Souheir Mehanna and Karine Zeitouni and Laurent Yeh and Yehia Taher and Zoubida Kedad and Ahmad Ktaish and Mohamed Chachoua and Cyril Ray},
keywords = {Mobile crowd sensing, Environmental sensing, Data management, Data mining, Exposure analysis, Big data framework, Scalability},
abstract = {The advent of the new generation of low-cost lightweight and connected sensors made a paradigm shift in environmental studies. In particular, nomadic sensors allow for a very precise personalized measurement, by continuously quantifying the individual exposure to air pollution components. Moreover, a broad dissemination among volunteers of these devices, or their deployment on vehicle fleets, is becoming a credible solution. Another major interest of such sensor deployment is to densify the air quality monitoring network, indoor, as in the outdoor, which is today restricted to sparse nodes. However, this high spatiotemporal resolution raises several issues related to their analysis. After an overview of the projects relying on this technology, this chapter points out the remaining challenges to be addressed. Part of these challenges constitutes the research program of the ongoing project Polluscope in France.}
}
@incollection{VANI202199,
title = {Chapter 6 - Impetus to machine learning in cardiac disease diagnosis},
editor = {Kalpana Chauhan and Rajeev Kumar Chauhan},
booktitle = {Image Processing for Automated Diagnosis of Cardiac Diseases},
publisher = {Academic Press},
pages = {99-116},
year = {2021},
isbn = {978-0-323-85064-3},
doi = {https://doi.org/10.1016/B978-0-323-85064-3.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323850643000091},
author = {T. Vani},
keywords = {Machine learning, disease diagnosis, healthcare, cardiac disease, disease detection, medical imaging, automated diagnosis, cardiac diagnosis},
abstract = {Machine learning is a branch of computer science, and it is a subset of artificial intelligence. It comprises many algorithms based on statistical methods to build automated systems for solving a particular problem. Due to its versatility, it is popular in many fields in real life, including scientific researches, healthcare field, industries, pharmaceutical field for drug discovery, social anomalies such as epidemic, and pandemic diseases spread. This chapter aims to identify the impact of machine learning techniques in the diagnosis of cardiac diseases. This chapter starts with the justification of the need for machine learning technology in the healthcare field. The basics of machine learning technology and its various algorithms are explained in the next section. The applications of these algorithms, which includes the diagnosis of various diseases such as diabetics, coronary artery disease (CAD), coronary heart disease (CHD), liver ailments, cancer detection and prevention, radiology, pathology, clinical trials, robotic surgery, drug discovery, and personalized treatments, are described from the contemporary researches. The challenges it faces in the healthcare field are also listed. In the end, the constraints of machine learning techniques in the healthcare field are explained with the suggestions to make accurate and efficient diagnoses in the future.}
}
@article{SINGH2021101322,
title = {Making Energy-transition headway: A Data driven assessment of German energy startups},
journal = {Sustainable Energy Technologies and Assessments},
volume = {47},
pages = {101322},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101322},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821003325},
author = {Mahendra Singh and Jiao Jiao and Marian Klobasa and Rainer Frietsch},
keywords = {Energy startups, Emerging technologies, Energy-transition, Funding, Innovation, Digitalisation, Data analysis, Business model, X-as-a-service, Digital platform},
abstract = {This paper explores the linkage between ongoing clean energy-transition, technology and business model emergence in the German energy sector. The speed of energy-transition is often led by innovative startups. Startups with innovative products, services, or value propositions are a key indicator, supporting successful energy-transition. Though, commercial databases cover comprehensive details to understand startup’s financial activity and stakeholder relation, but without considering their innovation and business activity. Measuring the actual activities of energy startups is pivotal to capture the impact of energy-transition. To put this into perspective, a hybrid approach of data collection combining structured and unstructured data has been proposed in the following work. A list of 240 innovative startups belonging to different categories and technology focus are examined. Furthermore, data-driven analysis is performed over the data collected from multiple sources. Renewable technologies are yet the most preferred technology focus among German entrepreneurs and stakeholders. 24.6% startups are identified in this category followed by 17.5% in energy management and 16.2% in energy storage. The evidence from this study suggests a clear shift in technology and the value proposition of successful innovative startups in Germany. Digitalisation of the energy sector is fostering the development of multi-sided digital platform driven business models. The result suggested that 8.0% of startups have implemented purely platform based services while 15.7% are experimenting with platform business models along with traditional business to business (B2B) and business to customer (B2C) business models. Findings could guide policymakers and federal agencies to provide a vision for future technology and business model adaptation in the German energy sector.}
}
@article{FRITZSCHE2021683,
title = {Industrial Applications of Artificial Intelligence: From Grand Stories of Digital Disruption to Actual Progress},
journal = {Procedia CIRP},
volume = {104},
pages = {683-688},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.115},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010131},
author = {Albrecht Fritzsche and Philipp Gölzer},
keywords = {Artificial Intelligence, Data-Driven Operations Management, Digital Transformation Narratives, Industry 4.0},
abstract = {Data-driven operations management goes along with narratives of disruptive change and new potential for innovation. We study how these narratives are reflected in the outcomes of 82 implementation projects that took place during the last ten years. The analysis of the projects identifies varying focal points in different industrial sectors. Radical steps towards new forms of data-driven operations management have only been achieved in exceptional cases. For the most part, new technical solutions follow given organizational structures and preserve extant business processes. We describe typical implementation patterns, compare them across industries and discuss different interpretations of the findings.}
}
@article{KOSIKOV2021492,
title = {Data Enrichment in the Information Graphs Environment Based on a Specialized Architecture of Information Channels},
journal = {Procedia Computer Science},
volume = {190},
pages = {492-499},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013922},
author = {Sergey Kosikov and Larisa Ismailova and Viacheslav Wolfengagen},
keywords = {data enrichment, information channels, conceptual constructions, informational graph, applicative computations, semantics},
abstract = {The paper considers the possibility of constructing a specialized computing system oriented at the transmission of data through information channels, that are determined taking into account the semantics of the selected data. In the process of computations the data is connected with semantic characteristics that describe the channel of computations, which can be considered as a method of semantic data enrichment. The system of information channels as a whole can be considered as an information graph describing the structuring of the processed data. The information graph supports the data model in the form of a network, the framework of which are objects and the relationships between them. The paper proposes language tools for determining the information graph and interpretation tools that provide practical computations. The set of information channels that make up the information graph can be considered as a low-level tool for data enrichment. The paper studies the possibility of determining tools of higher level. An applicative type language is proposed for defining information graphs, the syntax and semantics of the language are specified. The proposed language can be considered as an intermediate level tool for defining semantics. A procedure is proposed for compiling the language into a low-level construct, preserving the semantics of the language. The supporting system for the proposed computing system includes a low-level language interpreter, as well as an intermediate-level language compiler into a low-level language. The supporting system is implemented in an applicative programming environment. Some elements of the supporting system were tested when developing applied information systems in the field of jurisprudence.}
}
@article{VANDERVOORT2021121160,
title = {Data science as knowledge creation a framework for synergies between data analysts and domain professionals},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121160},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121160},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100593X},
author = {Haiko {van der Voort} and Sabine {van Bulderen} and Scott Cunningham and Marijn Janssen},
keywords = {Data science, Knowledge, Predictive model, Value creation, Risk-based inspection, Professionalism},
abstract = {The road from data generation to data use is commonly approached as a data-driven, functional process in which domain expertise is integrated as an afterthought. In this contribution we complement this functional view with an institutional view, that takes data analysis and domain professionalism as complementary (yet fallible) knowledge sources. We developed a framework that identifies and amplifies synergies between data analysts and domain professionals instead of taking one of them (i.e. data analytics) at the centre of the analytical process. The framework combines the often-cited CRISP-DM framework with a knowledge creation framework. The resulting framework is used in a data science project at a Dutch inspectorate that seeks to use data for risk-based inspection. The findings show first support of our framework. They also show that whereas more complex models have a higher predictive power, simpler models are sometimes preferred as they have the potential to create more synergies between inspectors and data analyst. Another issue driven by the integrated framework is about who of the involved actors should own the predictive model: data analysts or inspectors.}
}
@article{MATHIAS2021614,
title = {Exploring Distance Based Approaches for Reducing Sensor Data in Defect Related Prognosis},
journal = {Procedia Computer Science},
volume = {184},
pages = {614-621},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.076},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007092},
author = {Selvine G. Mathias and Daniel Grossmann and Tapanta Bhanja},
keywords = {sensors, data, reduced distance, machine learning, accuracy scores},
abstract = {Vibration data consists of batches of time series which if accumulated over a period of time is a huge collection of numeric data. Reducing such data for use in deep learning models for computational effciency is a challenge. Combinatorial and discrete approaches, on the other hand, is not an extensively explored area when it comes to datasets. This paper aims to identify feature reduction techniques based on discrete approaches such as euclidean distance using dot products on vibration data samples from accelerometers fitted on bearings. In this limited experimentation, the procured dataset by this approach is considerably smaller in size as compared to the actual complete data, and with comparable results in prediction models, it can be used as a smaller representation of a sensor timeline. The results based on different models show that such reductions can be considered in building IoT applications in industries based on sensors.}
}
@incollection{QUIGLEY202179,
title = {Chapter 5 - The design of blended learning experiences for clean data to allow proper observation of student participation},
editor = {Fun Man Fung and Christoph Zimmermann},
booktitle = {Technology-Enabled Blended Learning Experiences for Chemistry Education and Outreach},
publisher = {Elsevier},
pages = {79-94},
year = {2021},
isbn = {978-0-12-822879-1},
doi = {https://doi.org/10.1016/B978-0-12-822879-1.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228791000044},
author = {Cormac Quigley and Elaine Leavy and Etain Kiely and Garrett Jordan},
keywords = {Learning analytics, Clean data, Multidisciplinary team, Motivations, VLE, Moodle},
abstract = {This chapter shares the results and insights from a collaborative project to use learning analytics to capture and transform learning in the first year of undergraduate science programs. The multidisciplinary team is composed of academics and technical staff with a shared goal and numerous motivations. The shared goal was to use analytics to describe and optimize learning. This is an ongoing project first instigated in 2016, which has evolved from using descriptive analytics to create personalized feedback forms, to creating dashboards and is working toward using historical data to train models to monitor and predict engagement and disengagement (identify at-risk students). Data are collected through a blended learning model that has enabled students to take greater ownership of their learning and staff to enhance curriculum and learning strategies.}
}
@article{SHI2021116041,
title = {Mapping lead concentrations in urban topsoil using proximal and remote sensing data and hybrid statistical approaches},
journal = {Environmental Pollution},
volume = {272},
pages = {116041},
year = {2021},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2020.116041},
url = {https://www.sciencedirect.com/science/article/pii/S0269749120367300},
author = {Tiezhu Shi and Chao Yang and Huizeng Liu and Chao Wu and Zhihua Wang and He Li and Huifang Zhang and Long Guo and Guofeng Wu and Fenzhen Su},
keywords = {Jenny’s state factor model, Visible and near-infrared reflectance spectroscopy, Landsat image, Geographically weighted regression, Regression kriging},
abstract = {Due to rapid urbanization in China, lead (Pb) continues to accumulate in urban topsoil, resulting in soil degradation and increased public exposure. Mapping Pb concentrations in urban topsoil is therefore vital for the evaluation and control of this exposure risk. This study developed spatial models to map Pb concentrations in urban topsoil using proximal and remote sensing data. Proximal sensing reflectance spectra (350–2500 nm) of soils were pre-processed and used to calculate the principal components as landscape factors to represent the soil properties. Other landscape factors, including vegetation and land-use factors, were extracted from time-sequential Landsat images. Two hybrid statistical approaches, regression kriging (RK) and geographically weighted regression (GWR), were adopted to establish prediction models using the landscape factors. The results indicated that the use of landscape factors derived from combined remote and proximal sensing data improved the prediction of Pb concentrations compared with useing these data individually. GWR obtained better results than RK for predicting soil Pb concentration. Thus, joint proximal and remote sensing provides timely, easily accessible, and suitable data for extracting landscape factors.}
}
@article{MARMIER20211144,
title = {Towards a proactive vision of the training for the 4.0 Industry: From the required skills diagnostic to the training of employees},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {1144-1149},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.135},
url = {https://www.sciencedirect.com/science/article/pii/S240589632100896X},
author = {François Marmier and Ioana Deniaud and Ivana Rasovska and Jean-Louis Michalak},
keywords = {Active Learning, leaning path to 4.0, Engineering education, Human factor, Industry 4.0, Learning Factory, Skills},
abstract = {The digitalisation increase in industrial processes is perceived, by companies, as an opportunity to grow up their competitivity. Data are more and more accessible, potentially allowing making better decisions at all the level of the company. Then, job profiles and their required skills are changing. However, if competencies focused on software tools, programming, data analysis, simulation, virtual design, automatics and electronics becomes necessary, the initial trainings and continuous trainings are not changing as fast. Moreover, if new technologies are more available in companies, the workforce suffers of a lack of preparation. It generates risks of mistakes, improper use of tools and information, under performed activities, insufficiently informed decision. A global vision of how to train the whole industrial network is necessary to generate a progress of the whole industry. Workers must get the right skills for their activities in order to become a factor of efficiency for their workshop and consequently for the whole logistic chain. In that way, the role of the universities is to develop trainings for up-to-date needs as the industry 4.0. For this purpose, this paper introduced an overview of how to propose actual trainings on the topic of the Industry 4.0 both customized for the companies and for the learners. We detail more specifically in this paper 3 tools we develop at the University of Strasbourg: (1) a diagnostic tool to get the maturity level of companies and propose adapted learning paths. (2) a set of grids to design adapted learning path to the different work. (3) a Learning Factory to allow a learning by doing way.}
}
@article{MAK2021102868,
title = {Comparative assessments and insights of data openness of 50 smart cities in air quality aspects},
journal = {Sustainable Cities and Society},
volume = {69},
pages = {102868},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102868},
url = {https://www.sciencedirect.com/science/article/pii/S221067072100158X},
author = {Hugo Wai Leung Mak and Yun Fat Lam},
keywords = {Smart cities, Air quality, Open data policy, Environmental monitoring, Data sharing and privacy, Future development of air quality network},
abstract = {Data Openness is considered as an indispensable component for scientific innovation, community engagement and smart city development. In this study, a Data Openness in Air Quality (DOAQ) framework that consists of 3 tiers with a total of 23 open data principles was established to assess and monitor the status and development of data sharing, release and centralization of air quality information in the top 50 smart cities (Top50SC) around the world. The DOAQ utilizes additive formulas with predefined coefficients to obtain scores in each tier, thus reflecting the relative importance on data availability and visibility of different air quality data. The scores of DOAQ were compared with the smart cities scorings from Eden Strategy Institute and ONG&ONG Pte Ltd. (2018), and other socioeconomic attributes (i.e., social, political and humane) within the current study. Strong correlations (i.e., 0.4−0.6) among these indices implicate that the status of air quality reporting could be a good proxy to gauge the environmental data openness in a city. Lastly, good practices (e.g., apps and air quality forecasts), essential criteria and directions for future smart city development on air quality reporting were summarized, with the aim of laying down practical and efficient guidelines for individual smart city that desires to seek for improvements in air quality data openness.}
}
@article{NELSON202197,
title = {Crowdsourced data for bicycling research and practice},
journal = {Transport Reviews},
volume = {41},
number = {1},
pages = {97-114},
year = {2021},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2020.1806943},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722000447},
author = {Trisalyn Nelson and Colin Ferster and Karen Laberee and Daniel Fuller and Meghan Winters},
keywords = {Crowdsourced, bicycling, exposure, safety, infrastructure, attitudes},
abstract = {ABSTRACT
Cities are promoting bicycling for transportation as an antidote to increased traffic congestion, obesity and related health issues, and air pollution. However, both research and practice have been stalled by lack of data on bicycling volumes, safety, infrastructure, and public attitudes. New technologies such as GPS-enabled smartphones, crowdsourcing tools, and social media are changing the potential sources for bicycling data. However, many of the developments are coming from data science and it can be difficult evaluate the strengths and limitations of crowdsourced data. In this narrative review we provide an overview and critique of crowdsourced data that are being used to fill gaps and advance bicycling behaviour and safety knowledge. We assess crowdsourced data used to map ridership (fitness, bike share, and GPS/accelerometer data), assess safety (web-map tools), map infrastructure (OpenStreetMap), and track attitudes (social media). For each category of data, we discuss the challenges and opportunities they offer for researchers and practitioners. Fitness app data can be used to model spatial variation in bicycling ridership volumes, and GPS/accelerometer data offer new potential to characterise route choice and origin-destination of bicycling trips; however, working with these data requires a high level of training in data science. New sources of safety and near miss data can be used to address underreporting and increase predictive capacity but require grassroots promotion and are often best used when combined with official reports. Crowdsourced bicycling infrastructure data can be timely and facilitate comparisons across multiple cities; however, such data must be assessed for consistency in route type labels. Using social media, it is possible to track reactions to bicycle policy and infrastructure changes, yet linking attitudes expressed on social media platforms with broader populations is a challenge. New data present opportunities for improving our understanding of bicycling and supporting decision making towards transportation options that are healthy and safe for all. However, there are challenges, such as who has data access and how data crowdsourced tools are funded, protection of individual privacy, representativeness of data and impact of biased data on equity in decision making, and stakeholder capacity to use data given the requirement for advanced data science skills. If cities are to benefit from these new data, methodological developments and tools and training for end-users will need to track with the momentum of crowdsourced data.}
}
@article{LI2021101232,
title = {The Spring Festival Effect: The change in NO2 column concentration in China caused by the migration of human activities},
journal = {Atmospheric Pollution Research},
volume = {12},
number = {12},
pages = {101232},
year = {2021},
issn = {1309-1042},
doi = {https://doi.org/10.1016/j.apr.2021.101232},
url = {https://www.sciencedirect.com/science/article/pii/S1309104221002956},
author = {Dongqing Li and Qizhong Wu and Hui Wang and Han Xiao and Qi Xu and Lizhi Wang and Jinming Feng and Xiaochun Yang and Huaqiong Cheng and Lanning Wang and Yiming Sun},
keywords = {Spring festival effect, Tropospheric NO column concentration, Megacities, Human activity},
abstract = {The Spring Festival is the most important holiday in China, and human activity and population mobility may contribute greatly to air quality. According to the satellite-based tropospheric nitrogen dioxide (NO2) column and ground-based observational concentration of NO2 in megacities from 2013 to 2018 around the Spring Festival, we found that NO2 concentration obviously decreases, presenting a “tide phenomenon”, particularly in the megacities, with the tropospheric NO2 column density decreasing by 31.8%–44.5%. The tropospheric NO2 column density in Beijing decreased by 41.6% and rebounded by 22.3% after the festival. Vehicle sources were among the important causes of NOx emissions in the megacities, and traffic intensity decreased significantly during the festival. As the coronavirus disease 2019 (COVID-19) pandemic progresses, the traffic intensity in urban areas is decreasing significantly, with the tropospheric NO2 column density decreasing by 56.2% and rebounding by only 6.8% in 2020, without the “tide phenomenon”.}
}
@article{CECULA2021e06626,
title = {Applications of artificial intelligence to improve patient flow on mental health inpatient units - Narrative literature review},
journal = {Heliyon},
volume = {7},
number = {4},
pages = {e06626},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e06626},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021007295},
author = {Paulina Cecula and Jiakun Yu and Fatema Mustansir Dawoodbhoy and Jack Delaney and Joseph Tan and Iain Peacock and Benita Cox},
keywords = {Mental health, Patient flow, Artificial intelligence, National health service, Inpatient units},
abstract = {Background
Despite a growing body of research into both Artificial intelligence and mental health inpatient flow issues, few studies adequately combine the two. This review summarises findings in the fields of AI in psychiatry and patient flow from the past 5 years, finds links and identifies gaps for future research.
Methods
The OVID database was used to access Embase and Medline. Top journals such as JAMA, Nature and The Lancet were screened for other relevant studies. Selection bias was limited by strict inclusion and exclusion criteria.
Research
3,675 papers were identified in March 2020, of which a limited number focused on AI for mental health unit patient flow. After initial screening, 323 were selected and 83 were subsequently analysed. The literature review revealed a wide range of applications with three main themes: diagnosis (33%), prognosis (39%) and treatment (28%). The main themes that emerged from AI in patient flow studies were: readmissions (41%), resource allocation (44%) and limitations (91%). The review extrapolates those solutions and suggests how they could potentially improve patient flow on mental health units, along with challenges and limitations they could face.
Conclusion
Research widely addresses potential uses of AI in mental health, with some focused on its applicability in psychiatric inpatients units, however research rarely discusses improvements in patient flow. Studies investigated various uses of AI to improve patient flow across specialities. This review highlights a gap in research and the unique research opportunity it presents.}
}
@article{LAROUI2021210,
title = {Edge and fog computing for IoT: A survey on current research activities & future directions},
journal = {Computer Communications},
volume = {180},
pages = {210-231},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421003327},
author = {Mohammed Laroui and Boubakr Nour and Hassine Moungla and Moussa A. Cherif and Hossam Afifi and Mohsen Guizani},
keywords = {Internet of Things (IoT), Edge computing, Cloud computing},
abstract = {The Internet of Things (IoT) allows communication between devices, things, and any digital assets that send and receive data over a network without requiring interaction with a human. The main characteristic of IoT is the enormous quantity of data created by end-user’s devices that needs to be processed in a short time in the cloud. The current cloud-computing concept is not efficient to analyze very large data in a very short time and satisfy the users’ requirements. Analyzing the enormous quantity of data by the cloud will take a lot of time, which affects the quality of service (QoS) and negatively influences the IoT applications and the overall network performance. To overcome such challenges, a new architecture called edge computing — that allows to decentralize the process of data from the cloud to the network edge has been proposed to solve the problems occurred by using the cloud computing approach. Furthermore, edge computing supports IoT applications that require a short response time and consequently enhances the consumption of energy, resource utilization, etc. Motivated by the extensive research efforts in the edge computing and IoT applications, in this paper, we present a comprehensive review of edge and fog computing research in the IoT. We investigate the role of cloud, fog, and edge computing in the IoT environment. Subsequently, we cover in detail, different IoT use cases with edge and fog computing, the task scheduling in edge computing, the merger of software-defined networks (SDN) and network function virtualization (NFV) with edge computing, security and privacy efforts. Furthermore, the Blockchain in edge computing. Finally, we identify open research challenges and highlight future research directions.}
}
@article{SAIHI2023108701,
title = {Underpinning success factors of maintenance digital transformation: A hybrid reactive Delphi approach},
journal = {International Journal of Production Economics},
volume = {255},
pages = {108701},
year = {2023},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2022.108701},
url = {https://www.sciencedirect.com/science/article/pii/S0925527322002833},
author = {Afef Saihi and Mohamed Ben-Daya and Rami As'ad},
keywords = {Maintenance digital transformation, Success factors, Hybrid reactive Delphi method, Expert verification, Ranking},
abstract = {In today's competitive landscape, maintenance is one of the critical functions that may greatly benefit from digital transformation (DT). Advances in technological developments are proving to be a game changer for improving maintenance strategies and practices. However, several challenges related to the lack of adequate managerial aspects, and adoption of wrong approaches to this transformation hinder its success. Research has shown that technology alone is not sufficient for organizations to ensure the success of their digitalization attempts. Therefore, the aim of this research is to adopt a holistic approach to identify, evaluate and rank a comprehensive list of the various influencing variables, multifaceted in nature, that drive the success of maintenance DT initiatives. To that end, the authors solicited the different success factors based on a systematic literature review, then conducted a purification phase of these enablers and categorized them. Subsequently, a hybrid reactive Delphi approach, combined with AHP method, was adopted to evaluate and rank the success factors and their corresponding clusters through a panel of 17 highly qualified experts. The verified final list includes 41 factors mapped into 10 categories. The clusters are ranked according to their relative level of importance, and normalized weights are attributed to their factors. In particular, “Digital strategy - strategic alignment” cluster ranked highest followed by “Top management and leadership skills” then “Organizational development and change management”. The findings of this study contribute to enriching the extant literature, serve as a foundation to conduct further academic research, and provide proper guidance to practitioners in this field.}
}
@article{LIU20211,
title = {Visualization and visual analysis of vessel trajectory data: A survey},
journal = {Visual Informatics},
volume = {5},
number = {4},
pages = {1-10},
year = {2021},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2021.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X21000401},
author = {Haiyan Liu and Xiaohui Chen and Yidi Wang and Bing Zhang and Yunpeng Chen and Ying Zhao and Fangfang Zhou},
keywords = {Maritime traffic, Vessel trajectory data, Automatic identification system, Visualization and visual analysis},
abstract = {Maritime transports play a critical role in international trade and commerce. Massive vessels sailing around the world continuously generate vessel trajectory data that contain rich spatial–temporal patterns of vessel navigations. Analyzing and understanding these patterns are valuable for maritime traffic surveillance and management. As essential techniques in complex data analysis and understanding, visualization and visual analysis have been widely used in vessel trajectory data analysis. This paper presents a literature review on the visualization and visual analysis of vessel trajectory data. First, we introduce commonly used vessel trajectory data sets and summarize main operations in vessel trajectory data preprocessing. Then, we provide a taxonomy of visualization and visual analysis of vessel trajectory data based on existing approaches and introduce representative works in details. Finally, we expound on the prospects of the remaining challenges and directions for future research.}
}
@incollection{ROMEO20211,
title = {Chapter 1 - Baseline data for spill assessments: ambient conditions, socioeconomic data, sensitivity maps},
editor = {Oleg Makarynskyy},
booktitle = {Marine Hydrocarbon Spill Assessments},
publisher = {Elsevier},
pages = {1-25},
year = {2021},
isbn = {978-0-12-819354-9},
doi = {https://doi.org/10.1016/B978-0-12-819354-9.00007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128193549000077},
author = {Lucy Romeo and Patrick Wingo and Michael Sabbatino and Jennifer Bauer},
keywords = {Baseline, data, spill preparedness, spill response, ambient, socioeconomic, sensitivity mapping, machine learning},
abstract = {Effective oil spill preparedness and response relies heavily on the availability of baseline data. Baselines comprise measurements and information collected prior to natural or anthropogenic disasters and can be applied to predict the transport and fate of pollutants, plan for socioeconomic stressors, and overall mitigate impacts. Spatial and temporal in nature, these datasets represent the current state of a specific area. Baselines representing offshore areas comprise ambient conditions, socioeconomic statuses, and environmental sensitivities. This chapter will highlight the value of baselines and identify means to collect and build representative databases for marine and coastal ecosystems to aid in spill assessments.}
}
@article{ASHKOUTI20211,
title = {DI-Mondrian: Distributed improved Mondrian for satisfaction of the L-diversity privacy model using Apache Spark},
journal = {Information Sciences},
volume = {546},
pages = {1-24},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.07.066},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520307441},
author = {Farough Ashkouti and Keyhan khamforoosh and Amir Sheikhahmadi},
keywords = {Anonymization, PPDP, -anonymity, L-diversity, Information loss, Apache Spark, RDD},
abstract = {For the extraction of useful patterns, the collected data should be distributed to and shared with analyzers. This, however, creates problems and challenges for the individual with respect to their privacy and identity. In this paper, the Mondrian multidimensional anonymization method was developed and improved for satisfaction of the l-diversity privacy model, and it has been presented in a distributed fashion within the Apache Spark framework. Since one of the major challenges in data privacy is the tradeoff between privacy and data utility, the presented method focuses on information loss and classifier evaluation criteria. Therefore, the cut dimension was selected using the coefficient of variation and information gain criteria, and the cut points were chosen dynamically, which led to a decrease in the information loss parameter and an improvement in the classifier performance evaluation criteria such as accuracy and FMeasure compared to the previous algorithms in the literature. The processing speed is 100 times higher in Spark than in the Hadoop framework. Consequently, the proposed method was presented in a distributed fashion based on RDDs programming within Apache Spark framework. This will resolve the problem of speed in large-scale data anonymization as it exists in the previous Hadoop-based algorithms. The results of the experiments performed on the numerical datasets demonstrate the improvements made by the proposed method.}
}
@article{SCHUHMACHER20211,
title = {The present and future of project management in pharmaceutical R&D},
journal = {Drug Discovery Today},
volume = {26},
number = {1},
pages = {1-4},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2020.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S1359644620303007},
author = {Alexander Schuhmacher and Oliver Gassmann and Markus Hinder and Michael Kuss}
}
@article{ZHANG2023108919,
title = {An interpretable knowledge-based decision support method for ship collision avoidance using AIS data},
journal = {Reliability Engineering & System Safety},
volume = {230},
pages = {108919},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108919},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022005348},
author = {Jinfen Zhang and Jiongjiong Liu and Spyros Hirdaris and Mingyang Zhang and Wuliu Tian},
keywords = {Automatic identification system (AIS), Ship collision avoidance behavior, Scenario similarity measurement, Trajectory fusion, Collision avoidance path planning},
abstract = {AIS data include ship spatial-temporal and motion parameters which can be used to excavate the deep-seated information. In this article, an interpretable knowledge-based decision support method is established to guide the ship to make collision avoidance decisions with good seamanship and ordinary practice of seamen using AIS data. First, AIS data is preprocessed and trajectory reconstructed to restore the ship historical navigation state, and a ship encounter identification model is constructed according to the encounter characteristics; Second, a two-stage collision avoidance behavior extraction algorithm is formed to build a behavior knowledge base, and the scenario similarity model is constructed to measure and match similar scenarios based on ship position, motion tendency and collision risk. Then, the Delaunay Triangulation Network is used to fuse ship trajectories of similar scenario to form the collision avoidance path. Finally, a case study is performed using the real AIS data outside Ningbo-Zhoushan Port waters, China, and the effectiveness of the planned path is verified by setting the head-on and crossing situations and comparison between the planned and real paths. Results indicate that the proposed model can extract the ship collision avoidance behavior accurately, and the planned path can ensure navigation safety.}
}
@article{WACHTER2021105567,
title = {Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI},
journal = {Computer Law & Security Review},
volume = {41},
pages = {105567},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105567},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000406},
author = {Sandra Wachter and Brent Mittelstadt and Chris Russell},
keywords = {European union, Non-discrimination, Fairness, Discrimination, Bias, Algorithm, Law, Demographic parity, Machine learning, Artificial intelligence},
abstract = {In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in artificial intelligence (AI) and machine learning. Connecting this work to existing legal non-discrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automated fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ; we refer to this approach as “contextual equality.” This Article makes three contributions. First, we review the evidential requirements to bring a claim under EU non-discrimination law. Due to the disparate nature of algorithmic and human discrimination, the EU's current requirements are too contextual, reliant on intuition, and open to judicial interpretation to be automated. Many of the concepts fundamental to bringing a claim, such as the composition of the disadvantaged and advantaged group, the severity and type of harm suffered, and requirements for the relevance and admissibility of evidence, require normative or political choices to be made by the judiciary on a case-by-case basis. We show that automating fairness or non-discrimination in Europe may be impossible because the law, by design, does not provide a static or homogenous framework suited to testing for discrimination in AI systems. Second, we show how the legal protection offered by non-discrimination law is challenged when AI, not humans, discriminate. Humans discriminate due to negative attitudes (e.g. stereotypes, prejudice) and unintentional biases (e.g. organisational practices or internalised stereotypes) which can act as a signal to victims that discrimination has occurred. Equivalent signalling mechanisms and agency do not exist in algorithmic systems. Compared to traditional forms of discrimination, automated discrimination is more abstract and unintuitive, subtle, intangible, and difficult to detect. The increasing use of algorithms disrupts traditional legal remedies and procedures for detection, investigation, prevention, and correction of discrimination which have predominantly relied upon intuition. Consistent assessment procedures that define a common standard for statistical evidence to detect and assess prima facie automated discrimination are urgently needed to support judges, regulators, system controllers and developers, and claimants. Finally, we examine how existing work on fairness in machine learning lines up with procedures for assessing cases under EU non-discrimination law. A ‘gold standard’ for assessment of prima facie discrimination has been advanced by the European Court of Justice but not yet translated into standard assessment procedures for automated discrimination. We propose ‘conditional demographic disparity’ (CDD) as a standard baseline statistical measurement that aligns with the Court's ‘gold standard’. Establishing a standard set of statistical evidence for automated discrimination cases can help ensure consistent procedures for assessment, but not judicial interpretation, of cases involving AI and automated systems. Through this proposal for procedural regularity in the identification and assessment of automated discrimination, we clarify how to build considerations of fairness into automated systems as far as possible while still respecting and enabling the contextual approach to judicial interpretation practiced under EU non-discrimination law.}
}
@article{WU2021169,
title = {Hiding sensitive information in eHealth datasets},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {169-180},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330594},
author = {Jimmy Ming-Tai Wu and Gautam Srivastava and Alireza Jolfaei and Philippe Fournier-Viger and Jerry Chun-Wei Lin},
keywords = {Privacy, Preserving, Data mining, eHealth, Dynamic threshold, Sensitive, Evolutionary computation},
abstract = {Privacy in the realm of data mining known as PPDM has become a hot topic in both academic research and industry due to the fact it can discover implicit rules as well as hide sensitive information for data sanitization. Many different algorithms and heuristics have been investigated to hide sensitive information using the act of transaction deletion based on evolutionary computation techniques, but to date, these algorithms only consider a uniform threshold value for sanitization progress. This technique is not applicable in real-world situations, especially for eHealth based medical datasets. For example, a patient can still be identified if he/she has more confidential information (i.e., symptoms) that cause privacy threats and security leakage in medical applications. In this work, we investigate a unique novel methodology to set varied threshold values that lead to varied lengths of sensitive patterns within a Genetic Algorithm (GA)-based framework. As the pattern length increases, a tighter threshold manifests to provide better protection of sensitive information that can avoid individual patients to be identified in eHealth datasets. Two GA-based models are developed for data sanitization using record deletion techniques. The experimental results are conducted and compared with the traditional Evolutionary Computation (EC)-based PPDM approaches and the results showed that the designed methods offer greater protection than previous methods in terms of side effects. Therefore, the designed models are effective to hide sensitive information in medical situations that can be used in real-world scenarios.}
}
@incollection{SADRANI2021426,
title = {Sensors and Data Driven Approaches in Transport},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {426-431},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10790-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717107900},
author = {Mohammad Sadrani and Constantinos Antoniou},
keywords = {Driver behavior monitoring, Machine learning methods, Origin–destination matrices estimation, Safety analysis, Sensors, Sensor fusion approaches, Smartphone-based sensors, Traffic data, Traffic network surveillance, Transportation mode inference, Travel time estimation},
abstract = {Sensors play an important role in collecting real-time information for transportation systems. Nowadays, several different sensor technologies, ranging from traditional ones to mobile sensors in smartphones, are being used to collect a massive data volume on the real-time location and dynamics of users. For example, most of the modern smartphones are equipped with multiple motion sensors, such as accelerometer and magnetometer sensors, which can provide an unprecedented opportunity for the monitoring of the motion status of mobile phone users. On the other hand, there are a wide variety of data mining and prediction techniques, which can support transportation researchers in analyzing raw travel data collected from sensor technologies. This article provides a review of various applications of sensor technologies in transport networks, including travel time estimation, origin–destination matrices estimation, safety analysis, driver behavior monitoring, and transportation mode inference.}
}
@incollection{LATIF202363,
title = {Wearable Cyberphysical Systems for Biomedicine},
editor = {Roger Narayan},
booktitle = {Encyclopedia of Sensors and Biosensors (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {63-85},
year = {2023},
isbn = {978-0-12-822549-3},
doi = {https://doi.org/10.1016/B978-0-12-822548-6.00124-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128225486001242},
author = {Tahmid Latif and James Dieffenderfer and Rafael Luiz {da Silva} and Edgar Lobaton and Alper Bozkurt},
keywords = {Asthma, Biomarkers, Biosensor, Cardiac health, Cyberphysical systems, Environmental sensors, Machine learning, Physiologic sensors, Pulmonary health, Respiratory conditions, Wearables},
abstract = {This chapter surveys the state-of-the-art related to the building blocks of wearable cyberphysical systems for health monitoring and highlights its potential to revolutionize healthcare, specifically chronic disease management. The common sensing modalities and their corresponding wearable form factors are summarized for the application areas of cardiovascular diseases, asthma and chronic obstructive pulmonary disease. The use of these measurements with estimation approaches using signal processing and machine learning techniques is also reviewed. The outcomes of these estimation tasks can be used to provide feedback internally to optimize device performance and externally to the users about their health situation. The chapter concludes with a discussion of deployment barriers for wearable cyberphysical systems in real life.}
}
@article{COSOLI2021109966,
title = {Measurement of multimodal physiological signals for stimulation detection by wearable devices},
journal = {Measurement},
volume = {184},
pages = {109966},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.109966},
url = {https://www.sciencedirect.com/science/article/pii/S026322412100899X},
author = {Gloria Cosoli and Angelica Poli and Lorenzo Scalise and Susanna Spinsante},
keywords = {Acoustic stimulation detection, Wearable devices, Measurement systems, Multimodal physiological signals, Features selection, Machine learning},
abstract = {The presence of stimuli and the consequent reactions undoubtedly reflect in experience-related changes of physiological parameters, which can be monitored by wearable devices. Generally, reactions related to the sympathetic nervous system activity are assessed through heart rate variability analysis. However, the exploitation of multimodal physiological signals provides a broader fingerprint. This study aims to identify the elicitation of acoustic stimulation through a wearable device; physiological signals, including electrodermal activity and skin temperature, were measured on a test population wearing a wrist-worn medical device. Eight machine learning algorithms were evaluated in a binary classification (presence/absence of stimuli), using 22 meaningful metrics from the collected data. The experimental results showed that Linear Regression (LR) algorithm, followed by Support Vector Machine (SVM), performed satisfactorily across all the evaluation metrics, achieving 75.00% and 72.62% of accuracy rate, respectively. Finally, the trained LR and SVM algorithms have been validated on a publicly available dataset (WESAD).}
}
@article{SUN2021103751,
title = {A critical review of distributed fiber optic sensing for real-time monitoring geologic CO2 sequestration},
journal = {Journal of Natural Gas Science and Engineering},
volume = {88},
pages = {103751},
year = {2021},
issn = {1875-5100},
doi = {https://doi.org/10.1016/j.jngse.2020.103751},
url = {https://www.sciencedirect.com/science/article/pii/S1875510020306053},
author = {Yankun Sun and Jinquan Liu and Ziqiu Xue and Qi Li and Chengkai Fan and Xu Zhang},
keywords = {Distributed fiber-optic sensing, Geologic CO sequestration, Brillouin- Rayleigh backscattering, Strain response, Temperature profile, Microseismicity detection},
abstract = {Geologic CO2 sequestration (GCS) has been identified as the most viable option for effectively reducing greenhouse gases emissions to mitigate global warming and worldwide climate change. However, CO2 injection into subsurface can induce reservoir expansion and fault reactivation, which ultimately result in near-surface infrastructure damage and personnel insecurity. Distributed fiber optic sensing (DFOS) technologies function one single fiber as an array of sensors to in-situ monitor multi-parameters, such as geomechanical deformation (i.e., strain), temperature, acoustics and pressure along the entire fiber or cable length. Due to its superiority over conventional geophone and detector, DFOS tool possesses great potential to sense geofluid injection-induced small disturbances in deep subsurface. Here we begin by highlighting recent research efforts in available monitoring tools employed in GCS sites. Given the increasing attentions of optical sensing, we present a first-hand review of DFOS categories, sensing principles, and advantages for GCS related investigations from both laboratory and field scales. We discuss in detail three typical DFOS-deployed GCS projects and explore the implicit findings to guide subsequent GCS field applications. Finally, we summarize the major challenges and going forward in developing, utilizing, and extending DFOS systems to widely apply for the future large-scale all-optical GCS monitoring sites.}
}
@incollection{ILMUDEEN2021363,
title = {Chapter 16 - Design and development of IoT-based decision support system for dengue analysis and prediction: case study on Sri Lankan context},
editor = {Valentina E. Balas and Souvik Pal},
booktitle = {Healthcare Paradigms in the Internet of Things Ecosystem},
publisher = {Academic Press},
pages = {363-380},
year = {2021},
isbn = {978-0-12-819664-9},
doi = {https://doi.org/10.1016/B978-0-12-819664-9.00016-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196649000168},
author = {Aboobucker Ilmudeen},
keywords = {Decision support system, Dengue, Disease analysis and prediction, Fuzzy Rule Neural Classification, Internet of Things},
abstract = {Dengue fever is an epidemic viral disease that is spread by various types of dengue viruses of the genus Aedes, primarily Aedes aegypti. Dengue epidemics are common in humid and subhumid areas of the world, mostly in cities and suburban regions. The old methods were delay in diagnosing and restricting the growth of dengue eruption. This chapter proposes a fresh approach in Fuzzy Rule–based Neural Classification with Internet of Things (IoT), cloud computing, and fog computing to analyze and predict dengue outbreak. The proposed fog-driven IoT architecture in which each component is seamlessly connected with each other to execute activities such as disease management, preventative care, clinical monitoring, early warning systems, e-medicine, and drug and food recommender system. This IoT-based decision support system aims to stop, control, and enable forecasting of eruptions of dengue, facilitating medical officers the information and insights to handle the outbreak, well in advance.}
}
@article{RICONDO2021762,
title = {A digital twin framework for the simulation and optimization of production systems},
journal = {Procedia CIRP},
volume = {104},
pages = {762-767},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.128},
url = {https://www.sciencedirect.com/science/article/pii/S221282712101026X},
author = {Itziar Ricondo and Alain Porto and Miriam Ugarte},
keywords = {digital twin, discrete-event simulation, monitoring, optimization, servitization},
abstract = {Industry 4.0 has raised the expectations on productivity, automation, and resource efficiency of manufacturing systems. This paper proposes a digital twin framework for the simulation and optimization of production lines and cells that can be used in the design and operation stages. The framework is supported by an architecture that connects manufacturing and machine tool data (digital shadow), the discrete event simulation model and the optimization engine, allowing for a variety of functionalities to plan and manage the production system. A use case is provided to demonstrate this framework, implemented in an automated line for the manufacturing of railway axles.}
}
@article{BROWNE2021e893,
title = {Global antibiotic consumption and usage in humans, 2000–18: a spatial modelling study},
journal = {The Lancet Planetary Health},
volume = {5},
number = {12},
pages = {e893-e904},
year = {2021},
issn = {2542-5196},
doi = {https://doi.org/10.1016/S2542-5196(21)00280-1},
url = {https://www.sciencedirect.com/science/article/pii/S2542519621002801},
author = {Annie J Browne and Michael G Chipeta and Georgina Haines-Woodhouse and Emmanuelle P A Kumaran and Bahar H Kashef Hamadani and Sabra Zaraa and Nathaniel J Henry and Aniruddha Deshpande and Robert C Reiner and Nicholas P J Day and Alan D Lopez and Susanna Dunachie and Catrin E Moore and Andy Stergachis and Simon I Hay and Christiane Dolecek},
abstract = {Summary
Background
Antimicrobial resistance (AMR) is a serious threat to global public health. WHO emphasises the need for countries to monitor antibiotic consumption to combat AMR. Many low-income and middle-income countries (LMICs) lack surveillance capacity; we aimed to use multiple data sources and statistical models to estimate global antibiotic consumption.
Methods
In this spatial modelling study, we used individual-level data from household surveys to inform a Bayesian geostatistical model of antibiotic usage in children (aged <5 years) with lower respiratory tract infections in LMICs. Antibiotic consumption data were obtained from multiple sources, including IQVIA, WHO, and the European Surveillance of Antimicrobial Consumption Network (ESAC-Net). The estimates of the antibiotic usage model were used alongside sociodemographic and health covariates to inform a model of total antibiotic consumption in LMICs. This was combined with a single model of antibiotic consumption in high-income countries to produce estimates of antibiotic consumption covering 204 countries and 19 years.
Findings
We analysed 209 surveys done between 2000 and 2018, covering 284 045 children with lower respiratory tract infections. We identified large national and subnational variations of antibiotic usage in LMICs, with the lowest levels estimated in sub-Saharan Africa and the highest in eastern Europe and central Asia. We estimated a global antibiotic consumption rate of 14·3 (95% uncertainty interval 13·2–15·6) defined daily doses (DDD) per 1000 population per day in 2018 (40·2 [37·2–43·7] billion DDD), an increase of 46% from 9·8 (9·2–10·5) DDD per 1000 per day in 2000. We identified large spatial disparities, with antibiotic consumption rates varying from 5·0 (4·8–5·3) DDD per 1000 per day in the Philippines to 45·9 DDD per 1000 per day in Greece in 2018. Additionally, we present trends in consumption of different classes of antibiotics for selected Global Burden of Disease study regions using the IQVIA, WHO, and ESAC-net input data. We identified large increases in the consumption of fluoroquinolones and third-generation cephalosporins in North Africa and Middle East, and south Asia.
Interpretation
To our knowledge, this is the first study that incorporates antibiotic usage and consumption data and uses geostatistical modelling techniques to estimate antibiotic consumption for 204 countries from 2000 to 2018. Our analysis identifies both high rates of antibiotic consumption and a lack of access to antibiotics, providing a benchmark for future interventions.
Funding
Fleming Fund, UK Department of Health and Social Care; Wellcome Trust; and Bill & Melinda Gates Foundation.}
}
@article{CORO2021101384,
title = {An Open Science approach to infer fishing activity pressure on stocks and biodiversity from vessel tracking data},
journal = {Ecological Informatics},
volume = {64},
pages = {101384},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101384},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121001758},
author = {Gianpaolo Coro and Anton Ellenbroek and Pasquale Pagano},
keywords = {Vessel transmitted information, Vessel tracking data, Automatic Identification System, Statistical analysis, e-Infrastructures, Open Science, Biodiversity, Integrated Environmental Assessment},
abstract = {Vessel tracking data help study the potential impact of fisheries on biodiversity and produce risk assessments. Existing workflows process vessel tracks to identify fishing activity and integrate information on species vulnerability. However, there are significant data integration challenges across the data sources needed for an integrated impact assessment due to heterogeneous nomenclatures, data accessibility issues, geographical and computational scalability of the processes, and confidentiality and transparency towards decision making authorities. This paper presents an Open Science data integration approach to use vessel tracking data in integrated impact assessments. Our approach combines heterogeneous knowledge sources from fisheries, biodiversity, and environmental observations to infer fishing activity and risks to potentially impacted species. An Open Science e-Infrastructure facilitates access to data sources and maximises the reproducibility of the results and the method's reusability across several application domains. Our method's quality is assessed through three case studies: The first demonstrates cross-dataset consistency by comparing the results obtained from two different vessel data sources. The second performs a temporal pattern analysis of fishing activity and potentially impacted species over time. The third assesses the potential impact of reduced fishing pressure on marine biodiversity and threatened species due to the 2020 COVID-19 lockdown in Italy. The method is meant to be integrated with other systems through its Open Science-oriented features and can rapidly use new sources of findable, accessible, interoperable, and reusable (FAIR) data. Other systems can use it to (i) classify vessel activity in data-limited scenarios, (ii) identify bycatch species (when catchability data are available), and (iii) study the effects of fisheries on habitats and populations’ growth.}
}