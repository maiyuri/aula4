@inproceedings{10.1145/3341620.3341629,
author = {El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi},
title = {Big Data Quality Metrics for Sentiment Analysis Approaches},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341629},
doi = {10.1145/3341620.3341629},
abstract = {In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {36–43},
numpages = {8},
keywords = {Big data quality metrics, Sentiment analysis, Big data},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3419604.3419803,
author = {Reda, Oumaima and Sassi, Imad and Zellou, Ahmed and Anter, Samir},
title = {Towards a Data Quality Assessment in Big Data},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419803},
doi = {10.1145/3419604.3419803},
abstract = {In recent years, as more and more data sources have become available and the volumes of data potentially accessible have increased, the assessment of data quality has taken a central role whether at the academic, professional or any other sector. Given that users are often concerned with the need to filter a large amount of data to better satisfy their requirements and needs, and that data analysis can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality, it makes everyone wonder what the results of these analyses will really be like. However, there is a very complex process involved in the identification of new, valid, potentially useful and meaningful data from a large data collection and various information systems, and is critically dependent on a number of measures to be developed to ensure data quality. To this end, the main objective of this paper is to introduce a general study on data quality related with big data, by providing what other researchers came up with on that subject. The paper will be finalized by a comparative study between the different existing data quality models.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {16},
numpages = {6},
keywords = {Big Data, Data Quality, Quality Models, Data Quality evaluation},
location = {Rabat, Morocco},
series = {SITA'20}
}

@inproceedings{10.1145/3010089.3010090,
author = {Emmanuel, Isitor and Stanier, Clare},
title = {Defining Big Data},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010090},
doi = {10.1145/3010089.3010090},
abstract = {As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {5},
numpages = {6},
keywords = {Big Data, Data Quality Dimensions, Big Data characteristics, Data Quality},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/3281022.3281026,
author = {Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and Rivas Garcia, Bibiano and Piattini, Mario},
title = {From Big Data to Smart Data: A Data Quality Perspective},
year = {2018},
isbn = {9781450360548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281022.3281026},
doi = {10.1145/3281022.3281026},
abstract = {Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company’s core business data, using typically large datasets. However, data that doesn’t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI…). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to “smartizing” data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Ensemble-Based Software Engineering},
pages = {19–24},
numpages = {6},
keywords = {Big Data, Data Quality, Smart Data},
location = {Lake Buena Vista, FL, USA},
series = {EnSEmble 2018}
}

@inproceedings{10.1145/3141128.3141139,
author = {Pti\v{c}ek, Marina and Vrdoljak, Boris},
title = {Big Data and New Data Warehousing Approaches},
year = {2017},
isbn = {9781450353434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141128.3141139},
doi = {10.1145/3141128.3141139},
abstract = {Big data are a data trend present around us mainly through Internet -- social networks and smart devices and meters -- mostly without us being aware of them. Also they are a fact that both industry and scientific research needs to deal with. They are interesting from analytical point of view, for they contain knowledge that cannot be ignored and left unused. Traditional system that supports the advanced analytics and knowledge extraction -- data warehouse -- is not able to cope with large amounts of fast incoming various and unstructured data, and may be facing a paradigm shift in terms of utilized concepts, technologies and methodologies, which have become a very active research area in the last few years. This paper provides an overview of research trends important for the big data warehousing, concepts and technologies used for data storage and (ETL) processing, and research approaches done in attempts to empower traditional data warehouses for handling big data.},
booktitle = {Proceedings of the 2017 International Conference on Cloud and Big Data Computing},
pages = {6–10},
numpages = {5},
keywords = {NewSQL, MapReduce, big data, NoSQL, databases, data warehouse},
location = {London, United Kingdom},
series = {ICCBDC 2017}
}

@inproceedings{10.1145/2513591.2527071,
author = {Cuzzocrea, Alfredo and Sacc\`{a}, Domenico and Ullman, Jeffrey D.},
title = {Big Data: A Research Agenda},
year = {2013},
isbn = {9781450320252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513591.2527071},
doi = {10.1145/2513591.2527071},
abstract = {Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.},
booktitle = {Proceedings of the 17th International Database Engineering &amp; Applications Symposium},
pages = {198–203},
numpages = {6},
keywords = {privacy of big data, big data posting, big data, OLAP over big data},
location = {Barcelona, Spain},
series = {IDEAS '13}
}

@inproceedings{10.1145/2658840.2658845,
author = {Neamtu, Rodica and Ahsan, Ramoza and Stokes, Jeff and Hoxha, Armend and Bao, Jialiang and Gvozdenovic, Stefan and Meyer, Ted and Patel, Nilesh and Rangan, Raghu and Wang, Yumou and Zhang, Dongyun and Rundensteiner, Elke A.},
title = {Taming Big Data: Integrating Diverse Public Data Sources for Economic Competitiveness Analytics},
year = {2014},
isbn = {9781450331869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658840.2658845},
doi = {10.1145/2658840.2658845},
abstract = {In an era where Big Data can greatly impact a broad population, many novel opportunities arise, chief among them the ability to integrate data from diverse sources and "wrangle" it to extract novel insights. Conceived as a tool that can help both expert and non-expert users better understand public data, MATTERS was collaboratively developed by the Massachusetts High Tech Council, WPI and other institutions as an analytic platform offering dynamic modeling capabilities. MATTERS is an integrative data source on high fidelity cost and talent competitiveness metrics. Its goal is to extract, integrate and model rich economic, financial, educational and technological information from renowned heterogeneous web data sources ranging from The US Census Bureau, The Bureau of Labor Statistics to the Institute of Education Sciences, all known to be critical factors influencing economic competitiveness of states. This demonstration of MATTERS illustrates how we tackle challenges of data acquisition, cleaning, integration and wrangling into appropriate representations, visualization and story-telling with data in the context of state competitiveness in the high-tech sector.},
booktitle = {Proceedings of the First International Workshop on Bringing the Value of "Big Data" to Users (Data4U 2014)},
pages = {25–28},
numpages = {4},
keywords = {data integration, Big data, diverse data sources},
location = {Hangzhou, China},
series = {Data4U '14}
}

@inproceedings{10.1145/3538950.3538951,
author = {Zhang, Lin and Jiang, Rong and Wang, Meng and Yang, Yue and Wang, Chenguang},
title = {A Drug Safety Traceability Model Based on Big Data},
year = {2022},
isbn = {9781450395632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538950.3538951},
doi = {10.1145/3538950.3538951},
abstract = {In order to solve the storage and management problems of heterogeneous drug data, this paper uses big data technology to complete the cleaning and distributed storage of drug data, and improve the function of data sharing and traceability. At the same time, in order to improve the drug traceability function, ensure the reliable storage of traceability information, and make the traceability process more reliable. This paper will put forward a drug traceability system model based on big data on the basis of existing research. Secondly, an evidence chain framework is proposed to verify evidence files. At last, the simulation experiment is carried out to test and illustrate the credibility of the traceability verification model.},
booktitle = {Proceedings of the 4th International Conference on Big Data Engineering},
pages = {1–7},
numpages = {7},
keywords = {Traceability Verification, Drug Traceability, Big Data},
location = {Beijing, China},
series = {BDE '22}
}

@inproceedings{10.1145/3206157.3206166,
author = {Khan, Nawsher and Alsaqer, Mohammed and Shah, Habib and Badsha, Gran and Abbasi, Aftab Ahmad and Salehian, Soulmaz},
title = {The 10 Vs, Issues and Challenges of Big Data},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206166},
doi = {10.1145/3206157.3206166},
abstract = {In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {52–56},
numpages = {5},
keywords = {Data Management, Big Data},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/2640087.2644168,
author = {Cho, Wonhee and Lim, Yoojin and Lee, Hwangro and Varma, Mohan Krishna and Lee, Moonsoo and Choi, Eunmi},
title = {Big Data Analysis with Interactive Visualization Using R Packages},
year = {2014},
isbn = {9781450328913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2640087.2644168},
doi = {10.1145/2640087.2644168},
abstract = {Compared to the traditional data storing, processing, analyzing and visualization which have been performed, Big data requires evolutionary technologies of massive data processing on distributed and parallel systems, such as Hadoop system. Big data analytic systems, thus, have been popular to derive important decision making in various areas. However, visualization on analytic system faces various limitation due to the huge amount of data. This brings the necessity of interactive visualization techniques beyond the traditional static visualization. R has been used and improved for a big data analysis and mining tool. Also, R is supported with various and abundant packages for different targets with visualization. However interactive visualization packages are not easily found in the market. This paper compares and analyzes interactive web packages with visualization packages for R. This paper also proposes interactive web visualized analysis environment for big data with a combination of interactive web packages and visualization packages. In particular, Big data analysis techniques with sensed data are presented as the result by reflecting the decision view on sensing field.},
booktitle = {Proceedings of the 2014 International Conference on Big Data Science and Computing},
articleno = {18},
numpages = {6},
keywords = {Mining, Big data, R, Hadoop, Visualization},
location = {Beijing, China},
series = {BigDataScience '14}
}

@inproceedings{10.1145/3456389.3456390,
author = {Cao, Shuangshuang},
title = {Opportunities and Challenges of Marketing in the Context of Big Data},
year = {2021},
isbn = {9781450389945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456389.3456390},
doi = {10.1145/3456389.3456390},
abstract = {In the era of big data, under the conditions of rapid economic development in our country, various enterprises have also vigorously carried out marketing. In the context of big data, marketing research should be strengthened to effectively improve market. Market issues ensure that marketing has improved its status in the era of big data. This article has conducted a research and analysis on marketing in the context of big data. And then the opportunities and challenges of marketing in the context of big data has been explained, which gradually optimize the marketing implementation effect. The challenges faced by marketing has been understood which ensures that the big data model plays its best role in it.},
booktitle = {2021 Workshop on Algorithm and Big Data},
pages = {79–82},
numpages = {4},
keywords = {Big data, Marketing, Personalized service},
location = {Fuzhou, China},
series = {WABD 2021}
}

@inproceedings{10.1145/3404687.3404694,
author = {Raza, Muhammad Umair and XuJian, Zhao},
title = {A Comprehensive Overview of BIG DATA Technologies: A Survey},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404694},
doi = {10.1145/3404687.3404694},
abstract = {In as much as the approaches of the new revolution, machines including transmission media like social media sites, nowadays quantity of data swell hastily. So, size is the core and only facet that leaps the mention of BIG DATA. In this article, an effort to touch a comprehensive view of big data technologies, because of the swift evolution of data by an industry trying the academic press to catch up. This paper also offers a unified explanation of big data as well as the analytics methods. A practical discriminate characteristic of this paper is core analytics associated with unstructured data which is more than 90% of big data. To deal with complicated Big Data problems, great work has been done. This paper analyzes contemporary Big Data technologies. Therein article further strengthens the necessity to formulate new tools for analytics. It bestows not sole an intercontinental overview of big data techniques even though the valuation according to big data Hadoop Ecosystem. It classifies and debates the main technologies feature, challenges, and usage as well.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {23–31},
numpages = {9},
keywords = {Apache Hadoop, HDFS, Big Data Technology, MapReduce},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1145/2896825.2896837,
author = {Chen, Hong-Mei and Kazman, Rick and Garbajosa, Juan and Gonzalez, Eloy},
title = {Toward Big Data Value Engineering for Innovation},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896837},
doi = {10.1145/2896825.2896837},
abstract = {This article articulates the requirements for an effective big data value engineering method. It then presents a value discovery method, called Eco-ARCH (Eco-ARCHitecture), tightly integrated with the BDD (Big Data Design) method for addressing these requirements, filling a methodological void. Eco-ARCH promotes a fundamental shift in design thinking for big data system design -- from "bounded rationality" for problem solving to "expandable rationality" for design for innovation. The Eco-ARCH approach is most suitable for big data value engineering when system boundaries are fluid, requirements are ill-defined, many stakeholders are unknown and design goals are not provided, where no architecture pre-exists, where system behavior is non-deterministic and continuously evolving, and where co-creation with consumers and prosumers is essential to achieving innovation goals. The method was augmented and empirically validated in collaboration with an IT service company in the energy industry to generate a new business model that we call "eBay in the Grid".},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {44–50},
numpages = {7},
keywords = {innovation, energy industry, architecture landscape, value engineering, value discovery, big data, ecosystem},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.5555/2819289.2819302,
author = {Chen, Hong-Mei and Kazman, Rick and Haziyev, Serge and Hrytsay, Olha},
title = {Big Data System Development: An Embedded Case Study with a Global Outsourcing Firm},
year = {2015},
publisher = {IEEE Press},
abstract = {Big data system development is dramatically different from small (traditional, structured) data system development. At the end of 2014, big data deployment is still scarce and failures abound. Outsourcing has become a main strategy for many enterprises. We therefore selected an outsourcing company who has successfully deployed big data projects for our study. Our research results from analyzing 10 outsourced big data projects provide a glimpse into early adopters of big data, illuminates the challenges for system development that stem from the 5Vs of big data and crystallizes the importance of architecture design choices and technology selection. We followed a collaborative practice research (CPR) method to develop and validate a new method, called BDD. BDD is the first attempt to systematically combine architecture design with data modeling approaches to address big data system development challenges. The use of reference architectures and a technology catalog are advancements to architecture design methods and are proving to be well-suited for big data system architecture design and system development.},
booktitle = {Proceedings of the First International Workshop on BIG Data Software Engineering},
pages = {44–50},
numpages = {7},
keywords = {software architecture, system engineering, big data, collaborative practice research, data system design methods, embedded case study methodology},
location = {Florence, Italy},
series = {BIGDSE '15}
}

@inproceedings{10.1145/3335484.3335545,
author = {Zhi-peng, Sun and Gui-ming, Chen and Hui, Zhang},
title = {Evaluation of Large-Scale Complex Systems Effectiveness Based on Big Data},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335545},
doi = {10.1145/3335484.3335545},
abstract = {With the advent of the information age, big data technology came into being. The wide application of big data brings new opportunities and challenges to the construction of national defense and military information. Under the background of information-based joint operations characterized by large complex systems, how to scientifically and rationally plan the construction of large complex systems, and maximize the effectiveness of the complex system has become a key concern for system construction decision makers and researchers. This paper combines the application of big data in the construction of large complex systems, and focuses on the evaluation of the effectiveness of large complex systems based on big data, which can be used for reference by relevant researchers.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {72–76},
numpages = {5},
keywords = {large-scale complex systems, evaluation, big data, effectiveness},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@article{10.1145/3408314,
author = {Davoudian, Ali and Liu, Mengchi},
title = {Big Data Systems: A Software Engineering Perspective},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3408314},
doi = {10.1145/3408314},
abstract = {Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {110},
numpages = {39},
keywords = {Big Data systems, Big Data, requirements engineering, software reference architecture, quality assurance, software engineering}
}

@inproceedings{10.1145/2811222.2811235,
author = {Abell\'{o}, Alberto},
title = {Big Data Design},
year = {2015},
isbn = {9781450337854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811222.2811235},
doi = {10.1145/2811222.2811235},
abstract = {It is widely accepted today that Relational databases are not appropriate in highly distributed shared-nothing architectures of commodity hardware, that need to handle poorly structured heterogeneous data. This has brought the blooming of NoSQL systems with the purpose of mitigating such problem, specially in the presence of analytical workloads. Thus, the change in the data model and the new analytical needs beyond OLAP take us to rethink methods and models to design and manage these newborn repositories. In this paper, we will analyze state of the art and future research directions.},
booktitle = {Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP},
pages = {35–38},
numpages = {4},
keywords = {nosql, database design, big data},
location = {Melbourne, Australia},
series = {DOLAP '15}
}

@inproceedings{10.1145/3366030.3366121,
author = {Safhi, Hicham Moad and Frikh, Bouchra and Ouhbi, Brahim},
title = {Data Source Selection in Big Data Context},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366121},
doi = {10.1145/3366030.3366121},
abstract = {Big Data presents promising technological and economical opportunities. In fact, it has become the raw material of production for many organizations. Data is available in large quantities, and it continues generating abundantly. However, not all the data will have valuable knowledge. Unreliable sources provide misleading and biased information, and even reliable sources could suffer from low data quality.In this paper, we propose a novel methodology for the selectability of data sources, by both considering the presence and the absence of users' preferences. The proposed model integrates multiple factors that affect the reliability of data sources, including their quality, gain, cost and coverage. Experimental results on real world data-sets, show its capability to find the subset of relevant and reliable sources with the lowest cost.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {611–616},
numpages = {6},
keywords = {Big Data integration, Source reliability, Big Data Source Selection, Data quality},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3555962.3555969,
author = {Faccia, Alessio and Cavaliere, Luigi Pio Leonardo and Petratos, Pythagoras and Mosteanu, Narcisa Roxana},
title = {Unstructured Over Structured, Big Data Analytics and Applications In Accounting and Management},
year = {2022},
isbn = {9781450396578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555962.3555969},
doi = {10.1145/3555962.3555969},
abstract = {Generating value from big data is a task that requires models’ preparation and use of advanced technologies but which, above all, is based on the ability to extract, manage and analyse data. These processes’ effectiveness depends on the data's quality and their structured or unstructured nature. We are witnessing a growing number of applications based on unstructured data mining in the accounting and management fields. This research aims to demonstrating that despite the traditional association between accounting and quantitative analyses (expected to be based mainly on structured financial data). The findings show that several useful applications now rely on unstructured data in this field. A basic analysis of the cybersecurity risks is also presented, along with mitigating strategies to allow companies to comply with current regulations such as the GDPR. The result might appear surprising from the business perspective, but it is not from a data science perspective. In conclusion the growing number of unsctructured data business applications should orientate a better understanding of their potential and target better training of finance specialist on data processing skills.},
booktitle = {Proceedings of the 2022 6th International Conference on Cloud and Big Data Computing},
pages = {37–41},
numpages = {5},
keywords = {Unstructured data, Analytics, Structured data, Accounting, Management, Big Data, Applications},
location = {Birmingham, United Kingdom},
series = {ICCBDC '22}
}

@article{10.1145/2331042.2331058,
author = {Heer, Jeffrey and Kandel, Sean},
title = {Interactive Analysis of Big Data},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331058},
doi = {10.1145/2331042.2331058},
abstract = {New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.},
journal = {XRDS},
month = {sep},
pages = {50–54},
numpages = {5}
}

@inproceedings{10.1145/2656346.2656358,
author = {Mohammad, Atif and Mcheick, Hamid and Grant, Emanuel},
title = {Big Data Architecture Evolution: 2014 and Beyond},
year = {2014},
isbn = {9781450330282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656346.2656358},
doi = {10.1145/2656346.2656358},
abstract = {This paper aims at developing the Big Data Architecture, and its relation with Analytics, Cloud Services as well as Business Intelligence. The chief aim from all mentioned is to enable the Enterprise Architecture and the Vision of an Organizational target to utilize all the data they are ingesting and regressing data for their short-term or long-terms analytical needs, while making sure that they are addressing during the design phase of such data architecture for both directly and indirectly related stakeholder. Since all stakeholders have their relative interests to utilize the transformed data-sets. This paper also identifies most of the Big Data Architecture, threat analysis within a Big Data System and Big Data Analytic Roadmaps, in terms of smaller components by conducting a gap-analysis that has significant importance as Baseline Big Data Architecture, targeting the end resultant Architectures, once the distillation process of main Big Data Architecture is completed by the Data Architects.},
booktitle = {Proceedings of the Fourth ACM International Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications},
pages = {139–144},
numpages = {6},
keywords = {big data, cloud computing},
location = {Montreal, QC, Canada},
series = {DIVANet '14}
}

@inproceedings{10.1145/2699026.2699136,
author = {Thuraisingham, Bhavani},
title = {Big Data Security and Privacy},
year = {2015},
isbn = {9781450331913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2699026.2699136},
doi = {10.1145/2699026.2699136},
abstract = {This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.},
booktitle = {Proceedings of the 5th ACM Conference on Data and Application Security and Privacy},
pages = {279–280},
numpages = {2},
keywords = {big data, privacy, security},
location = {San Antonio, Texas, USA},
series = {CODASPY '15}
}

@inproceedings{10.1145/2351316.2351318,
author = {Gopalkrishnan, Vivekanand and Steier, David and Lewis, Harvey and Guszcza, James},
title = {Big Data, Big Business: Bridging the Gap},
year = {2012},
isbn = {9781450315470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351316.2351318},
doi = {10.1145/2351316.2351318},
abstract = {Business analytics, occupying the intersection of the worlds of management science, computer science and statistical science, is a potent force for innovation in both the private and public sectors. The successes of business analytics in strategy, process optimization and competitive advantage has led to data being increasingly recognized as a valuable asset in many organizations. In recent years, thanks to a dramatic increase in the volume, variety and velocity of data, the loosely defined concept of "Big Data" has emerged as a topic of discussion in its own right -- with different viewpoints in both the business and technical worlds. From our perspective, it is important for discussions of "Big Data" to start from a well-defined business goal, and remain moored to fundamental principles of both cost/benefit analysis as well as core statistical science. This note discusses some business case considerations for analytics projects involving "Big Data", and proposes key questions that businesses should ask. With practical lessons from Big Data deployments in business, we also pose a number of research challenges that may be addressed to enable the business analytics community bring best data analytic practices when confronted with massive data sets.},
booktitle = {Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications},
pages = {7–11},
numpages = {5},
location = {Beijing, China},
series = {BigMine '12}
}

@inproceedings{10.1145/3175684.3175687,
author = {Kuschicke, Felix and Thiele, Thomas and Meisen, Tobias and Jeschke, Sabina},
title = {A Data-Based Method for Industrial Big Data Project Prioritization},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175687},
doi = {10.1145/3175684.3175687},
abstract = {The application of Big Data Techniques (BDT) in discrete manufacturing appears to be very promising, considering lighthouse projects in this area. In general, the goal is to collect all data from manufacturing systems comprehensively, in order to enable new findings and decision support by means of appropriate Industrial Big Data (IBD) analysis procedures. However, due to limited human and economic resources, potential IBD projects need to get prioritized -- in the best case according to their cost-benefit ratio. Available methods for this purpose are insufficient, due to their limited ability to be operationalized, error-proneness, and lack of scientific evidence. In this paper, we discuss how cost-benefit-analysis frameworks can be applied to the preliminary selection of production use cases for the implementation of BDT in larger production systems. It supports the use case selection process from information about production needs, available BDT, and given condition(s) per use case. This concept paper attempts to consolidate the hitherto fragmented discourse on how to prioritize IBD projects, evaluates the challenges of prioritization in this field, and presents a prioritization concept to overcome these challenges.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {6–10},
numpages = {5},
keywords = {Industrial Big Data, Manufacturing, Project Prioritization, Project Selection, Framework},
location = {London, United Kingdom},
series = {BDIOT2017}
}

@inproceedings{10.1109/BDC.2014.10,
author = {Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay},
title = {A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning},
year = {2014},
isbn = {9781479918973},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BDC.2014.10},
doi = {10.1109/BDC.2014.10},
abstract = {In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing},
pages = {16–25},
numpages = {10},
keywords = {Big Data, Kepler, Scientific workflow, Distributed computing, Bayesian network, Hadoop, Ensemble learning},
series = {BDC '14}
}

@inproceedings{10.1145/3216122.3216124,
author = {Cappiello, Cinzia and Sam\'{a}, Walter and Vitali, Monica},
title = {Quality Awareness for a Successful Big Data Exploitation},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216124},
doi = {10.1145/3216122.3216124},
abstract = {The combination of data and technology is having a high impact on the way we live. The world is getting smarter thanks to the quantity of collected and analyzed data. However, it is necessary to consider that such amount of data is continuously increasing and it is necessary to deal with novel requirements related to variety, volume, velocity, and veracity issues. In this paper we focus on veracity that is related to the presence of uncertain or imprecise data: errors, missing or invalid data can compromise the usefulness of the collected values. In such a scenario, new methods and techniques able to evaluate the quality of the available data are needed. In fact, the literature provides many data quality assessment and improvement techniques, especially for structured data, but in the Big Data era new algorithms have to be designed. We aim to provide an overview of the issues and challenges related to Data Quality assessment in the Big Data scenario. We also propose a possible solution developed by considering a smart city case study and we describe the lessons learned in the design and implementation phases.},
booktitle = {Proceedings of the 22nd International Database Engineering &amp; Applications Symposium},
pages = {37–44},
numpages = {8},
keywords = {Veracity, Big Data, Data Quality Assessment},
location = {Villa San Giovanni, Italy},
series = {IDEAS '18}
}

@inproceedings{10.1145/3006299.3006311,
author = {Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Mar\'{\i}n-Torder, Eva},
title = {Towards a Comprehensive Data Lifecycle Model for Big Data Environments},
year = {2016},
isbn = {9781450346177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3006299.3006311},
doi = {10.1145/3006299.3006311},
abstract = {A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource; however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation.Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity; and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.},
booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {100–106},
numpages = {7},
keywords = {big data, data organization, data management, data lifecycle, data complexity, vs challenges},
location = {Shanghai, China},
series = {BDCAT '16}
}

@inproceedings{10.1145/2593882.2593889,
author = {Mockus, Audris},
title = {Engineering Big Data Solutions},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593889},
doi = {10.1145/2593882.2593889},
abstract = {Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles.},
booktitle = {Future of Software Engineering Proceedings},
pages = {85–99},
numpages = {15},
keywords = {Data Science, Data Engineering, Game Theory, Data Quality, Analytics, Operational Data, Statistics},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3524383.3524442,
author = {Cai, Mingjun and Sam, Francis and Asante Boadi, Evans},
title = {Research on the Application of Big Data in University's Public Opinion Monitoring and Processing},
year = {2022},
isbn = {9781450395793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524383.3524442},
doi = {10.1145/3524383.3524442},
abstract = {Higher educational institutions rely on reputation to attract and earn the legitimacy of the public. However, the emerging trend of public misinformation due to the chunk of information available on the internet affects public opinion formation (POF) about universities. Therefore, narrowing down on the fallouts in POF can provide evidence for managerial and policy interventions. This paper explores five-layer POF and its application with big data in university public opinion monitoring and processing. It reveals that big data technology can be actively employed to safeguard the image of university public opinion formation, but this can be inhibited by the lack of commitment to integrating multiple stakeholders on a common platform, privacy, and security concerns. The paper recommends collaboration between universities and the government to increase momentum on big data with requisite actions for mutual benefits.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Education},
pages = {121–126},
numpages = {6},
keywords = {Big data, public opinion system, university public opinion, data analysis},
location = {Shanghai, China},
series = {ICBDE '22}
}

@article{10.1145/2627534.2627561,
author = {Savas, Onur and Sagduyu, Yalin and Deng, Julia and Li, Jason},
title = {Tactical Big Data Analytics: Challenges, Use Cases, and Solutions},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627561},
doi = {10.1145/2627534.2627561},
abstract = {We discuss tactical challenges of the Big Data analytics regarding the underlying data, application space, and com- puting environment, and present a comprehensive solution framework motivated by the relevant tactical use cases. First, we summarize the unique characteristics of the Big Data problem in the Department of Defense (DoD) context and underline the main differences from the commercial Big Data problems. Then, we introduce two use cases, (i) Big Data analytics with multi-intelligence (multi-INT) sensor data and (ii) man-machine crowdsourcing using MapReduce framework. For these two use cases, we introduce Big Data analytics and cloud computing solutions in a coherent frame- work that supports tactical data, application, and computing needs.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {86–89},
numpages = {4},
keywords = {analytics, tactical environment, cloud computing, algorithms, big data}
}

@inproceedings{10.1109/CCGRID.2018.00100,
author = {Cuzzocrea, Alfredo and Damiani, Ernesto},
title = {Pedigree-Ing Your Big Data: Data-Driven Big Data Privacy in Distributed Environments},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00100},
doi = {10.1109/CCGRID.2018.00100},
abstract = {This paper introduces a general framework for supporting data-driven privacy-preserving big data management in distributed environments, such as emerging Cloud settings. The proposed framework can be viewed as an alternative to classical approaches where the privacy of big data is ensured via security-inspired protocols that check several (protocol) layers in order to achieve the desired privacy. Unfortunately, this injects considerable computational overheads in the overall process, thus introducing relevant challenges to be considered. Our approach instead tries to recognize the "pedigree" of suitable summary data representatives computed on top of the target big data repositories, hence avoiding computational overheads due to protocol checking. We also provide a relevant realization of the framework above, the so-called Data-dRIven aggregate-PROvenance privacy-preserving big Multidimensional data (DRIPROM) framework, which specifically considers multidimensional data as the case of interest.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {675–681},
numpages = {7},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/3418688.3418697,
author = {Liou, Teau-Jiuan and Weng, Ming-Wei and Lee, Liza},
title = {The Effect of Big Data Platforms on Multi-Stage Production System in Industrie 4.0},
year = {2020},
isbn = {9781450387866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418688.3418697},
doi = {10.1145/3418688.3418697},
abstract = {The aim of this paper is to analyze how Industrie 4.0 triggers changes in the business models of manufacturing SMEs (small and medium-sized enterprises) by big data platforms in selected casting manufacturer in Taiwan. A generalized model is presented to determine the optimal production run time, production rate, the advertising effort and demand with observation features that minimize the total cost per unit time. Advances in science and technology such as IoT technology, big data platform to investigate information asymmetry between manufacturer and customers. Numerical examples and sensitivity analysis are then provided by the collecting real data from Taiwan. Finally, concluding remarks are offered.},
booktitle = {2020 the 3rd International Conference on Computing and Big Data},
pages = {48–54},
numpages = {7},
keywords = {Multi-stage assembly, Industrie 4.0, Digital transformation, Big data},
location = {Taichung, Taiwan},
series = {ICCBD '20}
}

@inproceedings{10.1145/3404687.3404693,
author = {Xin, Li and Tianyun, Shi and Xiaoning, Ma},
title = {Research on the Big Data Platform and Its Key Technologies for the Railway Locomotive System},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404693},
doi = {10.1145/3404687.3404693},
abstract = {In order to improve the efficiency of locomotive organization and the quality of locomotive operation, this paper analyzes and discusses the big data platform and some key technologies suitable for the big data application of the railway locomotive system. Firstly, the definition of big data of the railway locomotive system is proposed, and the current data characteristics of the railway locomotive system are summarized, then the status quo and demands of big data application of the railway locomotive system are analyzed. Secondly, the overall architecture of the big data platform for the railway locomotive system is proposed. Furthermore, seven application scenarios available for the big data platform are analyzed, including locomotive running organization, high-speed railway, repair, maintenance and other fields. Finally, some key technologies, which consist of data collection system of front-line operations, locomotive equipment portrait analysis, staff portrait analysis, transmission and analysis of locomotive video, intelligent auxiliary driving system, are provided to increase efficiency of the locomotive organization and capability of safety management. The obtained results can play a positive role in the construction and application of big data of the railway locomotive system.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {6–12},
numpages = {7},
keywords = {Railway, Key technology, Application platform, Big data, Locomotive system},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1145/2479724.2479730,
author = {Bertot, John Carlo and Choi, Heeyoon},
title = {Big Data and E-Government: Issues, Policies, and Recommendations},
year = {2013},
isbn = {9781450320573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2479724.2479730},
doi = {10.1145/2479724.2479730},
abstract = {The promises and potential of Big Data in transforming digital government services, governments, and the interaction between governments, citizens, and the business sector, are substantial. From "smart" government to transformational government, Big Data can foster collaboration; create real-time solutions to challenges in agriculture, health, transportation, and more; and usher in a new era of policy- and decision-making. There are, however, a range of policy challenges to address regarding Big Data, including access and dissemination; digital asset management, archiving and preservation; privacy; and security. This paper selectively reviews and analyzes the U.S. policy context regarding Big Data and offers recommendations aimed at facilitating Big Data initiatives.},
booktitle = {Proceedings of the 14th Annual International Conference on Digital Government Research},
pages = {1–10},
numpages = {10},
keywords = {big data, open government},
location = {Quebec, Canada},
series = {dg.o '13}
}

@article{10.1145/3150226,
author = {Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Shyu, Mei-Ling and Iyengar, S. S.},
title = {Multimedia Big Data Analytics: A Survey},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3150226},
doi = {10.1145/3150226},
abstract = {With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {10},
numpages = {34},
keywords = {retrieval, 5V challenges, data mining, multimedia databases, mobile multimedia, Big data analytics, multimedia analysis, machine learning, survey, indexing}
}

@inproceedings{10.1145/3545801.3545802,
author = {Liu, Xiaobao and Li, Qi and Zhu, Shaosong and Wang, Cong and Meng, Lingzhen},
title = {A Multi-Head Attention Mechanism Base Multi-Dimensional Data Quality Evaluation Model},
year = {2022},
isbn = {9781450396097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545801.3545802},
doi = {10.1145/3545801.3545802},
abstract = {High-quality power data is the basis for reliable operation of power systems, efficient data processing, and effective mining of the potential value of power data. How to use big data, artificial intelligence and other technologies to evaluate the quality of power data is a hot research topic in the field of electric power. At present, most of the power data quality evaluation methods are simple and lack the research of general data quality evaluation model. Therefore, this paper proposes a multi-dimensional data quality evaluation model based on a multi-head attention mechanism. The model measures multiple indicators such as completeness, accuracy, smoothness, and correlation. The corresponding methods are used to quantify these indicators to form a data quality evaluation index system oriented to multi-dimensional indicators; then, an application feedback mechanism based on a multi-head attention network is used to correct the calculation weights and score outputs, so as to achieve the evaluation of power data quality. Finally, the validation analysis is carried out based on the electricity data of a region in China. The experimental results show that the proposed method can effectively evaluate the quality of electric power data.},
booktitle = {Proceedings of the 7th International Conference on Big Data and Computing},
pages = {1–5},
numpages = {5},
keywords = {multi-head attention, Electric power data, data quality evaluation, multi-dimensional indicators},
location = {Shenzhen, China},
series = {ICBDC '22}
}

@article{10.1145/3492546,
author = {Johnson, Justin M and Khoshgoftaar, Taghi M},
title = {A Survey on Classifying Big Data with Label Noise},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-1955},
url = {https://doi.org/10.1145/3492546},
doi = {10.1145/3492546},
abstract = {Class label noise is a critical component of data quality that directly inhibits the predictive performance of machine learning algorithms. While many data-level and algorithm-level methods exist for treating label noise, the challenges associated with big data call for new and improved methods. This survey addresses these concerns by providing an extensive literature review on treating label noise within big data. We begin with an introduction to the class label noise problem and traditional methods for treating label noise. Next, we present 30 methods for treating class label noise in a range of big data contexts, i.e. high volume, high variety, and high velocity problems. The surveyed works include distributed solutions capable of operating on data sets of arbitrary sizes, deep learning techniques for large-scale data sets with limited clean labels, and streaming techniques for detecting class noise in the presence of concept drift. Common trends and best practices are identified in each of these areas, implementation details are reviewed, empirical results are compared across studies when applicable, and references to 17 open-source projects and programming packages are provided. An emphasis on label noise challenges, solutions, and empirical results as they relate to big data distinguishes this work as a unique contribution that will inspire future research and guide machine learning practitioners.},
note = {Just Accepted},
journal = {J. Data and Information Quality},
month = {apr},
keywords = {data quality, deep learning, machine learning, label noise, big data, data streams, classification}
}

@inproceedings{10.1145/2896387.2900335,
author = {Cuzzocrea, Alfredo},
title = {Warehousing and Protecting Big Data: State-Of-The-Art-Analysis, Methodologies, Future Challenges},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900335},
doi = {10.1145/2896387.2900335},
abstract = {This paper proposes a comprehensive critical survey on the issues of warehousing and protecting big data, which are recognized as critical challenges of emerging big data research. Indeed, both are critical aspects to be considered in order to build truly, high-performance and highly-flexible big data management systems. We report on state-of-the-art approaches, methodologies and trends, and finally conclude by providing open problems and challenging research directions to be considered by future efforts.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {14},
numpages = {7},
keywords = {Big Data, Protecting Big Data, Big Data Analytics, Warehousing Big Data},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1145/3209281.3209372,
author = {Chatfield, Akemi Takeoka and Ojo, Adegboyega and Puron-Cid, Gabriel and Reddick, Christopher G.},
title = {Census Big Data Analytics Use: International Cross Case Analysis},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209372},
doi = {10.1145/3209281.3209372},
abstract = {Despite the growing practices in big data and big data analytics use, there is still the paucity of research on links between government big data analytics use and public value creation. This multi-case study of Australia, Ireland, Mexico, and U.S.A. examines the state of big data and big data analytics use in the national census context. The census agencies are at varying stages in digitally transforming their national census process, products and services through assimilating and using big data and big data analytics. The cross-case analysis of government websites and documents identified emerging agency challenges in creating public value in the national census context: (1) big data analytics capability development, (2) cross agency data access and data integration, and (3) data security, privacy &amp; trust. Based on the insights gained, a research model aims to postulate the possible links among challenges, big data/big data analytics use, and public value creation.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {10},
numpages = {10},
keywords = {census big data, public value creation, cross case analysis, use, big data analytics, big data challenges, electronic census},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3289100.3289108,
author = {Bahadi, Jihane and El Asri, Bouchra and Courtine, M\'{e}lanie and Rhanoui, Maryem and Kergosien, Yannick},
title = {Towards Efficient Big Data: Hadoop Data Placing and Processing},
year = {2018},
isbn = {9781450365079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289100.3289108},
doi = {10.1145/3289100.3289108},
abstract = {Currently, the generated data flow is growing at a high rate resulting to the problem of data obesity and abundance, but yet a lack of pertinent information. To handle this Big Data, Hadoop is a distributed framework that facilitates data storage and processing. Although Hadoop is designed to deal with demands of storage and analysis of ever-growing Data, its performance characteristics are still to improve. In this regard, many approaches have been proposed to enhance Hadoop capabilities. Nevertheless, an overview of these approaches shows that several aspects need to be improved in terms of performance and data relevancy. The main challenge is how to extract efficiently value from the big data sources. For this purpose, we propose in this paper to discuss Hadoop architecture and intelligent data discovery, and propose an effective on-demand Big Data contribution enabling to process relevant data in efficient and effective way according to the stakeholder's needs, and aiming to boost Data appointment by integrating multidimensional approach.},
booktitle = {Proceedings of the 2nd International Conference on Smart Digital Environment},
pages = {42–47},
numpages = {6},
keywords = {MapReduce jobs, Hadoop, Multidimensional approach, Data placing, Big Data, Intelligent processing},
location = {Rabat, Morocco},
series = {ICSDE'18}
}

@inproceedings{10.1145/3297730.3297743,
author = {Liu, Yi and Peng, Jiawen and Yu, Zhihao},
title = {Big Data Platform Architecture under The Background of Financial Technology: In The Insurance Industry As An Example},
year = {2018},
isbn = {9781450365826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297730.3297743},
doi = {10.1145/3297730.3297743},
abstract = {With the rise of the concept of financial technology, financial and technology gradually in-depth integration, scientific and technological means to become financial product innovation, improve financial efficiency and reduce financial transaction costs an important driving force. In this context, the new technology platform is from the business philosophy, business model, technical means, sales, internal management and other dimensions to re-shape the financial industry. In this paper, the existing big data platform architecture technology innovation, adding space-time data elements, combined with the insurance industry for practical analysis, put forward a meaningful product circle and customer circle.},
booktitle = {Proceedings of the 2018 International Conference on Big Data Engineering and Technology},
pages = {31–35},
numpages = {5},
keywords = {platform architecture, time and space data, big data platform, insurance industry, Financial technology},
location = {Chengdu, China},
series = {BDET 2018}
}

@article{10.1145/3331651.3331659,
author = {Schilling, Lisa M. and Pena-Jackson, Griselda and Russell, Seth and Corral, Janet and Kwan, Bethany and Ressalam, Julie},
title = {Co-Designing Learning Materials to Empower Laypersons to Better Understand Big Data and Big Data Methods},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/3331651.3331659},
doi = {10.1145/3331651.3331659},
abstract = {University of Colorado Anschutz Medical Campus' Data Science to Patient Value Program and 2040 Partners for Health sought to create open learning materials for engaged citizens and community leaders regarding big data and big data methods to support their collaboration in patient-centered and participatorybased community research and evaluation. 2040 is a local nonprofit organization that cultivates partnerships in Aurora, Colorado neighborhoods to tackle critical health needs. Our goal was to co-design and co-create a series of big data learning modules accessible to community laypeople, so they might better understand big data topics and be empowered more actively engage in health research and evaluation that uses big data methods.},
journal = {SIGKDD Explor. Newsl.},
month = {may},
pages = {41–44},
numpages = {4},
keywords = {co-design, education, big data, community engagement}
}

@inproceedings{10.1109/MET.2019.00019,
author = {Auer, Florian and Felderer, Michael},
title = {Addressing Data Quality Problems with Metamorphic Data Relations},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MET.2019.00019},
doi = {10.1109/MET.2019.00019},
abstract = {In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.},
booktitle = {Proceedings of the 4th International Workshop on Metamorphic Testing},
pages = {76–83},
numpages = {8},
keywords = {quality assessment, metamorphic data relations, metamorphic testing, big data, data quality},
location = {Montreal, Quebec, Canada},
series = {MET '19}
}

@inproceedings{10.1145/3297730.3297731,
author = {Chen, Rui-Yang},
title = {Target Data Optimization Based on Big Data-Streaming for Two-Stage Fuzzy Extraction System},
year = {2018},
isbn = {9781450365826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297730.3297731},
doi = {10.1145/3297730.3297731},
abstract = {How to extract target data effectively and intelligently is key point in the big data-streaming operation. Data extractions need focus on priority of data selection to reduce impact of 4Vs because of requirement of real-time computation. Corresponding big data-streaming for three-stage extraction system is presented in terms of hierarchal base and fuzzy representations. The proposed approach is based on a combination of clustering, classification and relationships method with fuzzy weighted similarity under hierarchical feature-based model. Moreover, heuristic fuzzy CBR-FDT- algorithms are provided to explore the target data optimization. Successful case study and experiment with simulations demonstrated the performance of the proposed approach.},
booktitle = {Proceedings of the 2018 International Conference on Big Data Engineering and Technology},
pages = {26–30},
numpages = {5},
keywords = {Target data optimization, fuzzy decision tree, big data-streaming, fuzzy case-based reasoning},
location = {Chengdu, China},
series = {BDET 2018}
}

@inproceedings{10.1145/3372454.3372478,
author = {Puangpontip, Supadchaya and Hewett, Rattikorn},
title = {Assessing Reliability of Big Data Stream for Smart City},
year = {2020},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372478},
doi = {10.1145/3372454.3372478},
abstract = {Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.},
booktitle = {Proceedings of the 2019 3rd International Conference on Big Data Research},
pages = {18–23},
numpages = {6},
keywords = {Theory of evidence, Smart City, IoT, Data Reliability},
location = {Cergy-Pontoise, France},
series = {ICBDR 2019}
}

@article{10.1145/3148238,
author = {Heinrich, Bernd and Hristova, Diana and Klier, Mathias and Schiller, Alexander and Szubartowicz, Michael},
title = {Requirements for Data Quality Metrics},
year = {2018},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3148238},
doi = {10.1145/3148238},
abstract = {Data quality and especially the assessment of data quality have been intensively discussed in research and practice alike. To support an economically oriented management of data quality and decision making under uncertainty, it is essential to assess the data quality level by means of well-founded metrics. However, if not adequately defined, these metrics can lead to wrong decisions and economic losses. Therefore, based on a decision-oriented framework, we present a set of five requirements for data quality metrics. These requirements are relevant for a metric that aims to support an economically oriented management of data quality and decision making under uncertainty. We further demonstrate the applicability and efficacy of these requirements by evaluating five data quality metrics for different data quality dimensions. Moreover, we discuss practical implications when applying the presented requirements.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {12},
numpages = {32},
keywords = {data quality assessment, requirements for metrics, Data quality, data quality metrics}
}

@inproceedings{10.1145/2723372.2742794,
author = {Huang, Yiqing and Zhu, Fangzhou and Yuan, Mingxuan and Deng, Ke and Li, Yanhua and Ni, Bing and Dai, Wenyuan and Yang, Qiang and Zeng, Jia},
title = {Telco Churn Prediction with Big Data},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742794},
doi = {10.1145/2723372.2742794},
abstract = {We show that telco big data can make churn prediction much more easier from the $3$V's perspectives: Volume, Variety, Velocity. Experimental results confirm that the prediction performance has been significantly improved by using a large volume of training data, a large variety of features from both business support systems (BSS) and operations support systems (OSS), and a high velocity of processing new coming data. We have deployed this churn prediction system in one of the biggest mobile operators in China. From millions of active customers, this system can provide a list of prepaid customers who are most likely to churn in the next month, having $0.96$ precision for the top $50000$ predicted churners in the list. Automatic matching retention campaigns with the targeted potential churners significantly boost their recharge rates, leading to a big business value.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {607–618},
numpages = {12},
keywords = {big data, customer retention, telco churn prediction},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/3047273.3047377,
author = {Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil and Meijer, Ronald},
title = {Exploiting Big Data for Evaluation Studies},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047377},
doi = {10.1145/3047273.3047377},
abstract = {The collection and analysis of relevant data for evaluating public policies is not a straightforward task. An important type of such studies is the so-called ex-post evaluation. The main objective of ex-post evaluations is to determine to what extent a realized intervention is successful in tackling a societal challenge, e.g., youth unemployment. At a first glance an obvious method is to collect some baseline measurements for a set of relevant variables, apply the intervention for a while and collect the new measurement values for the same set of variables. Then, comparing the measurement values of the variables before and after the intervention provides an insight into the extent of successfulness of the intervention. This, however, is only true if the "ceteris paribus" condition holds. In practice it is infeasible to enforce this condition for societal challenges. Often, after having the baseline measurements, several phenomena emerge that may impact the new measurements without being taken into account. This makes it difficult to determine how much of the measured differences between the values of the variables before and after the intervention should be attributed to the emerging phenomena (or the so-called counterfactuals) and how much of the differences can be attributed to the applied intervention.This paper discusses how exploiting big data may contribute to the task of elucidating the influences of counterfactuals (and interventions) in ex-post evaluation studies. The paper proposes a framework to utilize big data for accounting for the impact of emerging phenomena in ex-post evaluation studies.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {228–231},
numpages = {4},
keywords = {counterfactuals, ex-post policy evaluation, data linkage, Big data},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3345252.3345282,
author = {Petrova-Antonova, Dessislava and Krasteva, Iva and Ilieva, Sylvia and Pavlova, Irena},
title = {Conceptual Architecture of GATE Big Data Platform},
year = {2019},
isbn = {9781450371490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345252.3345282},
doi = {10.1145/3345252.3345282},
abstract = {Today we experience a data-driven society. All human activities, industrial processes and research lead to data generation of unprecedented scale, spurring new products, services and businesses. Big Data and its application have been a target for European Commission -- with more than 100 FP7 and about 50 H2020 funded projects under Big Data domain. GATE project aims to establish and sustain in the long run a Centre of Excellence as collaborative environment for conducting Big Data research and innovation, facilitated by GATE platform and Innovation Labs. This paper proposes a conceptual architecture of GATE platform, that is holistic, symbiotic, open, evolving and data-integrated. It is also modular and with component-based design that allows to position a mix of products and tools from different providers. GATE platform will enable start-ups, SMEs and large enterprises, as well as other organizations in a wide range of sectors, to build advanced Data driven services and applications. The usability of the proposed architecture is proven through a development of a sample time series data visualization application. Its architecture follows the proposed one through implementation of required components using open technology stack.},
booktitle = {Proceedings of the 20th International Conference on Computer Systems and Technologies},
pages = {261–268},
numpages = {8},
keywords = {Smart City, Big Data Value Chain, Emerging Architectures, GATE Platform, Big Data},
location = {Ruse, Bulgaria},
series = {CompSysTech '19}
}

@inproceedings{10.1145/3328833.3328841,
author = {Adenuga, Kayode I. and Muniru, Idris O. and Sadiq, Fatai I. and Adenuga, Rahmat O. and Solihudeen, Muhammad J.},
title = {Big Data in Healthcare: Are We Getting Useful Insights from This Avalanche of Data?},
year = {2019},
isbn = {9781450361057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328833.3328841},
doi = {10.1145/3328833.3328841},
abstract = {The benefits of deriving useful insights from avalanche of data available everywhere cannot be overemphasized. Big Data analytics can revolutionize the healthcare industry. It can also ensure functional productivity, help forecast and suggest feedbacks to disease outbreaks, enhance clinical practice, and optimize healthcare expenditure which cuts across all stakeholders in healthcare sectors. Notwithstanding these immense capabilities available in the general application of big data; studies on derivation of useful insights from healthcare data that can enhance medical practice have received little academic attention. Therefore, this study highlighted the possibility of making very insightful healthcare outcomes with big data through a simple classification problem which classifies the tendency of individuals towards specific drugs based on personality measures. Our model though trained with less than 2000 samples and with a simple neural network architecture achieved mean accuracies of 76.87% (sd=0.0097) and 75.86% (sd=0.0123) for the 0.15 and 0.05 validation sets respectively. The relatively acceptable performance recorded by our model despite the small dataset could largely be attributed to number of attributes in our dataset. It is essential to uncover some of the many complexities in our societies in relations to healthcare; and through many machine learning architectures like the neural networks these complex relationships can be discovered},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Information Engineering},
pages = {196–199},
numpages = {4},
keywords = {Challenges, Benefits, Analytics, Big Data},
location = {Cairo, Egypt},
series = {ICSIE '19}
}

@article{10.1145/2854006.2854008,
author = {Fan, Wenfei},
title = {Data Quality: From Theory to Practice},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2854006.2854008},
doi = {10.1145/2854006.2854008},
abstract = {Data quantity and data quality, like two sides of a coin, are equally important to data management. This paper provides an overview of recent advances in the study of data quality, from theory to practice. We also address challenges introduced by big data to data quality management.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {7–18},
numpages = {12}
}

@inproceedings{10.1145/2743065.2743099,
author = {Amudhavel, J. and Padmapriya, V. and Gowri, V. and Lakshmipriya, K. and Kumar, K. Prem and Thiyagarajan, B.},
title = {Perspectives, Motivations and Implications Of Big Data Analytics},
year = {2015},
isbn = {9781450334419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2743065.2743099},
doi = {10.1145/2743065.2743099},
abstract = {As today there is an enormous volume of data, examining these large sets contains structure and unstructured data of different types and sizes; big data analytics is used. Data Analytics allows the user to analyze the unusable data to make a faster and better decision. The Latest supply chain professionals are suffused with data, which provokes various new ways of thoughts regarding how the data are produced, ordered, controlled and analyzed. Data Quality in Supply-Chain Management is used to monitor and control the data. This paper presents the knowledge infrastructure about trends in big data analytics. Big Data can also be given as an all-encompassing term for any collection of data sets so large or complex that it becomes difficult to process using traditional data processing applications. The challenges include examination, confine, extent, exploration, sharing, storage, relocate, and visualization and privacy infringement. The Big Data have their application in various fields like in Tourism, Climate Research and many other fields.},
booktitle = {Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering &amp; Technology (ICARCSET 2015)},
articleno = {34},
numpages = {5},
keywords = {monitor, exploration, unstructured data, application, Big data, infringement, Data analytics},
location = {Unnao, India},
series = {ICARCSET '15}
}

@inproceedings{10.1145/3127942.3127961,
author = {Al-Qirim, Nabeel and Tarhini, Ali and Rouibah, Kamel},
title = {Determinants of Big Data Adoption and Success},
year = {2017},
isbn = {9781450352840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127942.3127961},
doi = {10.1145/3127942.3127961},
abstract = {This research investigates the large hype surrounding big data (BD) and Analytics (BDA) in both academia and the business world. Initial insights pointed to large and complex amalgamations of different fields, techniques and tools. Above all, BD as a research field and as a business tool found to be under developing and is fraught with many challenges. The intention here in this research is to develop an adoption model of BD that could detect key success predictors. The research finds a great interest and optimism about BD value that fueled this current buzz behind this novel phenomenon. Like any disruptive innovation, its assimilation in organizations oppressed with many challenges at various contextual levels. BD would provide different advantages to organizations that would seriously consider all its perspectives alongside its lifecycle in the pre-adoption or adoption or implementation phases. The research attempts to delineate the different facets of BD as a technology and as a management tool highlighting different contributions, implications and recommendations. This is of great interest to researchers, professional and policy makers.},
booktitle = {Proceedings of the International Conference on Algorithms, Computing and Systems},
pages = {88–92},
numpages = {5},
keywords = {Big data analytics, big data strategy, big data success factors, big data challenges},
location = {Jeju Island, Republic of Korea},
series = {ICACS '17}
}

@inproceedings{10.1145/3063955.3063968,
author = {Wang, Hongzhi and Gao, Hong and Yin, Shenjun and Zhu, Jie},
title = {The Design of Course Architecture for Big Data},
year = {2017},
isbn = {9781450348737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3063955.3063968},
doi = {10.1145/3063955.3063968},
abstract = {Big data is one of the hottest topic in not only academic but also enterprise, which provide grate requirements for the people with knowledge and experiences of big data. However, current education architecture of computer science could not provide sufficient training for big data. For the education for people suitable for big data era, we attempt to design a novel course architecture. Such course architecture will not change the skeleton of traditional course architecture of computer science but just add content and subjects that is adaptive for big data. In this paper, we discuss the goal, architecture and content of the course architecture.},
booktitle = {Proceedings of the ACM Turing 50th Celebration Conference - China},
articleno = {13},
numpages = {6},
keywords = {course architecture, big data, data science},
location = {Shanghai, China},
series = {ACM TUR-C '17}
}

@article{10.1145/3461015,
author = {Fugini, Mariagrazia and Finocchi, Jacopo},
title = {Data and Process Quality Evaluation in a Textual Big Data Archiving System},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3461015},
doi = {10.1145/3461015},
abstract = {The article presents a textual Big Data analytics solution developed in a real setting as a part of a high-capacity document digitization and storage system. A software based on machine learning techniques performs automated extraction and processing of textual contents. The work focuses on performance and data confidence evaluation and describes the approach to computing a set of indicators for textual data quality. It then presents experimental results.},
journal = {J. Comput. Cult. Herit.},
month = {mar},
articleno = {2},
numpages = {19},
keywords = {data quality, text analytics, machine learning, content management, Big Data analytics, unstructured Big Data}
}

@inproceedings{10.1145/3524383.3524431,
author = {Wu, Min and Hao, Xinxin and Wan, Xuehong and Ma, Chenwei and Wu, Yu},
title = {Opportunities and Challenges of Joint Training of Postgraduate Students by the University-Industry Collaboration Institutions in Big Data Era},
year = {2022},
isbn = {9781450395793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524383.3524431},
doi = {10.1145/3524383.3524431},
abstract = {University-industry cooperative education is an important way to cultivate graduate students' innovative ability and practical ability. However, there are some problems in the traditional joint training model of graduate students, such as low efficiency, conflict of objectives of cooperative subjects, a mismatch between supply and demand of cooperative entities, and so on. The big data technology has brought new opportunities and challenges to the joint training of graduate students by university-industry cooperation institutions. Based on analyzing the connotation and characteristics of the big data era, the paper points out that the arrival of the big data era can improve the information integration efficiency of university-industry cooperation institutions, optimize the traditional joint training model of graduate students, and provide an effective evaluation mechanism of educational quality for university-industry cooperation institutions. At the same time, the paper discusses the difficulties of data collection and disclosure of data privacy faced by university-industry cooperative education in the big data era. The paper also discusses how to deal with the challenges from the perspective of the government, colleges and universities, scientific research institutions and enterprises.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Education},
pages = {194–198},
numpages = {5},
keywords = {postgraduate education, big data, cooperative education of university-industry institutions},
location = {Shanghai, China},
series = {ICBDE '22}
}

@article{10.1145/2992786,
author = {Debattista, Jeremy and Auer, S\"{O}ren and Lange, Christoph},
title = {Luzzu—A Methodology and Framework for Linked Data Quality Assessment},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2992786},
doi = {10.1145/2992786},
abstract = {The increasing variety of Linked Data on the Web makes it challenging to determine the quality of this data and, subsequently, to make this information explicit to data consumers. Despite the availability of a number of tools and frameworks to assess Linked Data Quality, the output of such tools is not suitable for machine consumption, and thus consumers can hardly compare and rank datasets in the order of fitness for use. This article describes a conceptual methodology for assessing Linked Datasets, and Luzzu; a framework for Linked Data Quality Assessment. Luzzu is based on four major components: (1) an extensible interface for defining new quality metrics; (2) an interoperable, ontology-driven back-end for representing quality metadata and quality problems that can be re-used within different semantic frameworks; (3) scalable dataset processors for data dumps, SPARQL endpoints, and big data infrastructures; and (4) a customisable ranking algorithm taking into account user-defined weights. We show that Luzzu scales linearly against the number of triples in a dataset. We also demonstrate the applicability of the Luzzu framework by evaluating and analysing a number of statistical datasets against a variety of metrics. This article contributes towards the definition of a holistic data quality lifecycle, in terms of the co-evolution of linked datasets, with the final aim of improving their quality.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {4},
numpages = {32},
keywords = {Data quality, linked data, quality assessment}
}

@inproceedings{10.1145/3207677.3278000,
author = {Ke, Changwen and Wang, Kuisheng},
title = {Research and Application of Enterprise Big Data Governance},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3278000},
doi = {10.1145/3207677.3278000},
abstract = {With1 the further development of information technology, data has become one of the core resources of enterprises. In the current era of big data, data governance has gradually become an important means for enterprises to make intelligent decisions, helping enterprises to occupy a favorable position in a highly competitive market environment. This paper briefly describes the current situation of enterprise big data governance, and analyzes the problems of current data governance from the perspectives of enterprise management and data itself. To deal with these problems, this paper proposes several suggestions and strategies from the perspectives of organization and management system construction, construction of a data standard system, improve the level of data quality management, data technology surpport, etc. Fully combining the advanced theories and methods of domestic data governance, this paper designs a data governance framework from the perspective of data application and innovation. The framework includes supervision and control modules, data governance staff organization modules, application and service modules, and data processing and integration modules. And the framework is applied to the data governance of electric power enterprises, which has important reference significance and value for enterprises to carry out data governance.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {29},
numpages = {5},
keywords = {Data governance, governance framework, data quality},
location = {Hohhot, China},
series = {CSAE '18}
}

@inproceedings{10.1145/2513190.2513198,
author = {Ordonez, Carlos},
title = {Can We Analyze Big Data inside a DBMS?},
year = {2013},
isbn = {9781450324120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513190.2513198},
doi = {10.1145/2513190.2513198},
abstract = {Relational DBMSs remain the main data management technology, despite the big data analytics and no-SQL waves. On the other hand, for data analytics in a broad sense, there are plenty of non-DBMS tools including statistical languages, matrix packages, generic data mining programs and large-scale parallel systems, being the main technology for big data analytics. Such large-scale systems are mostly based on the Hadoop distributed file system and MapReduce. Thus it would seem a DBMS is not a good technology to analyze big data, going beyond SQL queries, acting just as a reliable and fast data repository. In this survey, we argue that is not the case, explaining important research that has enabled analytics on large databases inside a DBMS. However, we also argue DBMSs cannot compete with parallel systems like MapReduce to analyze web-scale text data. Therefore, each technology will keep influencing each other. We conclude with a proposal of long-term research issues, considering the "big data analytics" trend.},
booktitle = {Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP},
pages = {85–92},
numpages = {8},
keywords = {mapreduce, big data, parallel algorithms, dbms, sql},
location = {San Francisco, California, USA},
series = {DOLAP '13}
}

@inproceedings{10.1145/3368691.3368717,
author = {Mohammed, Tareq Abed and Ghareeb, Ahmed and Al-bayaty, Hussein and Aljawarneh, Shadi},
title = {Big Data Challenges and Achievements: Applications on Smart Cities and Energy Sector},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368717},
doi = {10.1145/3368691.3368717},
abstract = {In this paper, the Big Data challenges and the processing is analyzed, recently great attention has been paid to the challenges for great data, largely due to the wide spread of applications and systems used in real life, such as presentation, modeling, processing and large (often unlimited) data storage. Mass Data Survey, OLAP Mass Data, Mass Data Dissemination and Mass Data Protection. Consequently, we focus on further research trends and, as a default, we will explore a future research challenge research project in this area of research.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {26},
numpages = {5},
keywords = {machin learning, data processing, OLAP, data mining, big data},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@proceedings{10.1145/3545801,
title = {ICBDC '22: Proceedings of the 7th International Conference on Big Data and Computing},
year = {2022},
isbn = {9781450396097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

@inproceedings{10.1145/2983323.2983345,
author = {Zhu, Fangzhou and Luo, Chen and Yuan, Mingxuan and Zhu, Yijian and Zhang, Zhengqing and Gu, Tao and Deng, Ke and Rao, Weixiong and Zeng, Jia},
title = {City-Scale Localization with Telco Big Data},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983345},
doi = {10.1145/2983323.2983345},
abstract = {It is still challenging in telecommunication (telco) industry to accurately locate mobile devices (MDs) at city-scale using the measurement report (MR) data, which measure parameters of radio signal strengths when MDs connect with base stations (BSs) in telco networks for making/receiving calls or mobile broadband (MBB) services. In this paper, we find that the widely-used location based services (LBSs) have accumulated lots of over-the-top (OTT) global positioning system (GPS) data in telco networks, which can be automatically used as training labels for learning accurate MR-based positioning systems. Benefiting from these telco big data, we deploy a context-aware coarse-to-fine regression (CCR) model in Spark/Hadoop-based telco big data platform for city-scale localization of MDs with two novel contributions. First, we design map-matching and interpolation algorithms to encode contextual information of road networks. Second, we build a two-layer regression model to capture coarse-to-fine contextual features in a short time window for improved localization performance. In our experiments, we collect 108 GPS-associated MR records in the centroid of Shanghai city with 12 x 11 square kilometers for 30 days, and measure four important properties of real-world MR data related to localization errors: stability, sensitivity, uncertainty and missing values. The proposed CCR works well under different properties of MR data and achieves a mean error of 110m and a median error of $80m$, outperforming the state-of-art range-based and fingerprinting localization methods.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {439–448},
numpages = {10},
keywords = {regression models, localization, telco big data},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1145/2691195.2691196,
author = {Ramasamy, Ramachandran},
title = {Towards Big Data Analytics Framework: ICT Professionals Salary Profile Compilation Perspective},
year = {2014},
isbn = {9781605586113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2691195.2691196},
doi = {10.1145/2691195.2691196},
abstract = {This paper elucidates the opportunity on expanding the on-going preparation of salary profile of information communications technology (ICT) professionals of Malaysia into a big data analytics (BDA) activity. The current activity is based on structured database provided by the online job service providers. In essence, BDA framework entailing 5 Vs namely, volume, variety, velocity, veracity and value used in gauging the gaps and potential areas to consider in next stages.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance},
pages = {450–451},
numpages = {2},
keywords = {ICT salary profile, big data analytics, business intelligence},
location = {Guimaraes, Portugal},
series = {ICEGOV '14}
}

@inproceedings{10.1145/3312614.3312623,
author = {Khan, Nawsher and Naim, Arshi and Hussain, Mohammad Rashid and Naveed, Quadri Noorulhasan and Ahmad, Naim and Qamar, Shamimul},
title = {The 51 V's Of Big Data: Survey, Technologies, Characteristics, Opportunities, Issues and Challenges},
year = {2019},
isbn = {9781450366403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312614.3312623},
doi = {10.1145/3312614.3312623},
abstract = {Currently Big Data is the biggest buzzword, and definitely, we believe that Big Data is changing the world. Some researchers say Big Data will be even bigger buzzword than the Internet. With fast-growing computing resources, information and knowledge a new digital globe has emerged. Information is being created and stored at a fast rate and is being accessed by a vast range of applications through scientific computing, commercial workloads, and social media. In 2018, over 28 billion devices globally, are connected to the internet. In 2020, more than 50 billion smart appliances will be connected worldwide and internet traffic flow will be 92 times greater than it was in 2005. The usage of such a massive number of connected devices not only increase the data volume but also the velocity of data addition with speed of light on fiber optic and various wireless networks. This fast generation of enormous data creates numerous threats and challenges. There exist various approaches that are addressing issues and challenges of Big Data with the theory of Vs such as 3 V's, 5 V's, 7 V's etc. The objective of this work is to explore and investigate the status of the current Big Data domain. Further, a comprehensive overview of Big Data, its characteristics, opportunities, issues, and challenges have been explored and described with the help of 51 V's. The outcome of this research will help in understanding the Big Data in a systematic way.},
booktitle = {Proceedings of the International Conference on Omni-Layer Intelligent Systems},
pages = {19–24},
numpages = {6},
keywords = {data characteristics, Big Data, data storage, data generation},
location = {Crete, Greece},
series = {COINS '19}
}

@inproceedings{10.1145/3472163.3472195,
author = {Sahri, Soror and Moussa, Rim},
title = {Customized Eager-Lazy Data Cleansing for Satisfactory Big Data Veracity},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472195},
doi = {10.1145/3472163.3472195},
abstract = {Big data systems are becoming mainstream for big data management either for batch processing or real-time processing. In order to extract insights from data, quality issues are very important to address, particularly. A veracity assessment model is consequently needed. In this paper, we propose a model which ties quality of datasets and quality of query resultsets. We particularly examine quality issues raised by a given dataset, order attributes along their fitness for use and correlate veracity metrics to business queries. We validate our work using the open dataset NYC taxi’ trips.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {157–165},
numpages = {9},
keywords = {Big data, Veracity},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3168390.3168425,
author = {Samosir, Ridha Sefina and Hendric, Harco Leslie and Gaol, Ford Lumban and Abdurachman, Edi and Soewito, Benfano},
title = {Measurement Metric Proposed For Big Data Analytics System},
year = {2017},
isbn = {9781450353922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168390.3168425},
doi = {10.1145/3168390.3168425},
abstract = {Big data is defined as a very large data set (volume), velocity and variety. Big data analytics systems must be supports for parallel processing and large storage. The problem of this research is how to identify measurement metric based on big data analytics system characteristic. One device that support big data platform is Hadoop. Measurement is a process for assigning values or symbols to the attributes of an entity. The purpose of measurement is to distinguish between entities one to another. Indicator for software measurement represented with a metric. The aim of this research is to proposes some measurement metric for big data analytics system. This research using UML exactly a class diagram in system modelling to identify the measurement metric. Both of dynamic and static metric is proposed as solution to measure big data analytics system. Result for this researh are some measurement ndicator both of dynamic and static metric based on class diagram for big data analytics.},
booktitle = {Proceedings of the 2017 International Conference on Computer Science and Artificial Intelligence},
pages = {265–269},
numpages = {5},
keywords = {Big Data Analytics, Software, Metric, Measurement},
location = {Jakarta, Indonesia},
series = {CSAI 2017}
}

@article{10.1145/2935753,
author = {Berti-Equille, Laure and Ba, Mouhamadou Lamine},
title = {Veracity of Big Data: Challenges of Cross-Modal Truth Discovery},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2935753},
doi = {10.1145/2935753},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {12},
numpages = {3},
keywords = {data quality, Truth discovery, fact checking, data fusion, information extraction}
}

@inproceedings{10.1145/3003733.3003767,
author = {Petrou, Charilaos and Paraskevas, Michael},
title = {Signal Processing Techniques Restructure The Big Data Era},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003767},
doi = {10.1145/3003733.3003767},
abstract = {Big data science has been developed into a topic that attracts attention from industry, academia and governments. The main objective in Big Data science is to recognize and extract meaningful information from huge amounts of heterogeneous data and unstructured data (which constitute 95% of big data). Signal Processing (SP) techniques and related statistical learning (SL) tools such as Principal Component Analysis (PCA), R-PCA (Robust PCA), Compressive Sampling (CS), convex optimization (CO), stochastic approximation (SA), kernel based learning (KBL) tasks are used for robustness, compression and dimensionality reduction in Big Data arising challenges. This review paper introduces Big Data related SP techniques and presents applications of this emerging field.},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {52},
numpages = {6},
keywords = {signal processing techniques, convex optimization, big data, stochastic approximation, statistical learning tools},
location = {Patras, Greece},
series = {PCI '16}
}

@inproceedings{10.1145/3503928.3503929,
author = {Barzan Abdalla, Hemn and Mustafa, Nasser and Ihnaini, Baha},
title = {Big Data: Finding Frequencies of Faulty Multimedia Data},
year = {2022},
isbn = {9781450385220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503928.3503929},
doi = {10.1145/3503928.3503929},
abstract = {In many health care domains, big data has arrived. How to manage and use big data better has become the focus of all walks of life. Many data sources provide the repeated fault data—the repeated fault data forming the delay of processing time and storage capacity. Big data includes properties like volume, velocity, variety, variability, value, complexity, and performance put forward more challenges. Most healthcare domains face the problem of testing for structured and unstructured data validation in big data. It provides low-quality data and delays in response. In testing process is delay and not provide the correct response. In Proposed, pre-testing and post-testing are used for big data testing. In pre-testing, classify fault data from different data sources. After Classification to group big data using SVM algorithms such as Text, Image, Audio, and Video file. In post-testing, to implement the pre-processing, remove the zero file size, unrelated file extension, and de-duplication after pre-processing to implement the Map-reduce algorithm to find out the big data efficiently. This process reduces the pre-processing time, reduces the server energy, and increases the processing time. To remove the fault data before pre-processing means to increase the processing time and data storage.},
booktitle = {2021 the 6th International Conference on Information Systems Engineering},
pages = {1–6},
numpages = {6},
keywords = {Fault data detection, Pre-Processing, Classification using SVM, Map-reduce, Big Data},
location = {Shanghai, China},
series = {ICISE 2021}
}

@article{10.1145/3419634,
author = {Bansal, Maggi and Chana, Inderveer and Clarke, Siobh\'{a}n},
title = {A Survey on IoT Big Data: Current Status, 13 V’s Challenges, and Future Directions},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3419634},
doi = {10.1145/3419634},
abstract = {Driven by the core technologies, i.e., sensor-based autonomous data acquisition and the cloud-based big data analysis, IoT automates the actuation of data-driven intelligent actions on the connected objects. This automation enables numerous useful real-life use-cases, such as smart transport, smart living, smart cities, and so on. However, recent industry surveys reflect that data-related challenges are responsible for slower growth of IoT in recent years. For this reason, this article presents a systematic and comprehensive survey on IoT Big Data (IoTBD) with the aim to identify the uncharted challenges for IoTBD. This article analyzes the state-of-the-art academic works in IoT and big data management across various domains and proposes a taxonomy for IoTBD management. Then, the survey explores the IoT portfolio of major cloud vendors and provides a classification of vendor services for the integration of IoT and IoTBD on their cloud platforms. After that, the survey identifies the IoTBD challenges in terms of 13 V’s challenges and envisions IoTBD as “Big Data 2.0.” Then the survey provides comprehensive analysis of recent works that address IoTBD challenges by highlighting their strengths and weaknesses to assess the recent trends and future research directions. Finally, the survey concludes with discussion on open research issues for IoTBD.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {131},
numpages = {59},
keywords = {IoT big data survey, V’s challenges for IoT big data, IoT big data, cloud computing in IoT, big data 2.0, cloud IoT services}
}

@inproceedings{10.1145/3472163.3472171,
author = {Bhardwaj, Dave and Ormandjieva, Olga},
title = {Rigorous Measurement Model for Validity of Big Data: MEGA Approach},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472171},
doi = {10.1145/3472163.3472171},
abstract = {Big Data is becoming a substantial part of the decision-making processes in both industry and academia, especially in areas where Big Data may have a profound impact on businesses and society. However, as more data is being processed, data quality is becoming a genuine issue that negatively affects credibility of the systems we build because of the lack of visibility and transparency of the underlying data. Therefore, Big Data quality measurement is becoming increasingly necessary in assessing whether data can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses Big Data quality measurement modelling and automation by proposing a novel quality measurement framework for Big Data (MEGA) that objectively assesses the underlying quality characteristics of Big Data (also known as the V's of Big Data) at each step of the Big Data Pipelines. Five of the Big Data V's (Volume, Variety, Velocity, Veracity and Validity) are currently automated by the MEGA framework. In this paper, a new theoretically valid quality measurement model is proposed for an essential quality characteristic of Big Data, called Validity. The proposed measurement information model for Validity of Big Data is a hierarchy of 4 derived measures / indicators and 5 based measures. Validity measurement is illustrated on a running example.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {285–291},
numpages = {7},
keywords = {Quality Characteristics (V's), Representational Theory of Measurement,, Validity, Big Data, Measurement Hierarchical Model},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@article{10.1145/3186549.3186559,
author = {Sadiq, Shazia and Dasu, Tamraparni and Dong, Xin Luna and Freire, Juliana and Ilyas, Ihab F. and Link, Sebastian and Miller, Miller J. and Naumann, Felix and Zhou, Xiaofang and Srivastava, Divesh},
title = {Data Quality: The Role of Empiricism},
year = {2018},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/3186549.3186559},
doi = {10.1145/3186549.3186559},
abstract = {We outline a call to action for promoting empiricism in data quality research. The action points result from an analysis of the landscape of data quality research. The landscape exhibits two dimensions of empiricism in data quality research relating to type of metrics and scope of method. Our study indicates the presence of a data continuum ranging from real to synthetic data, which has implications for how data quality methods are evaluated. The dimensions of empiricism and their inter-relationships provide a means of positioning data quality research, and help expose limitations, gaps and opportunities.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {35–43},
numpages = {9}
}

@inproceedings{10.1145/2463676.2463707,
author = {Sumbaly, Roshan and Kreps, Jay and Shah, Sam},
title = {The Big Data Ecosystem at LinkedIn},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2463707},
doi = {10.1145/2463676.2463707},
abstract = {The use of large-scale data mining and machine learning has proliferated through the adoption of technologies such as Hadoop, with its simple programming semantics and rich and active ecosystem. This paper presents LinkedIn's Hadoop-based analytics stack, which allows data scientists and machine learning researchers to extract insights and build product features from massive amounts of data. In particular, we present our solutions to the ``last mile'' issues in providing a rich developer ecosystem. This includes easy ingress from and egress to online systems, and managing workflows as production processes. A key characteristic of our solution is that these distributed system concerns are completely abstracted away from researchers. For example, deploying data back into the online system is simply a 1-line Pig command that a data scientist can add to the end of their script. We also present case studies on how this ecosystem is used to solve problems ranging from recommendations to news feed updates to email digesting to descriptive analytical dashboards for our members.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1125–1134},
numpages = {10},
keywords = {data mining, hadoop, machine learning, offline processing, data pipeline, big data},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2623330.2623615,
author = {Anagnostopoulos, Christos and Triantafillou, Peter},
title = {Scaling out Big Data Missing Value Imputations: Pythia vs. Godzilla},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623615},
doi = {10.1145/2623330.2623615},
abstract = {Solving the missing-value (MV) problem with small estimation errors in big data environments is a notoriously resource-demanding task. As datasets and their user community continuously grow, the problem can only be exacerbated. Assume that it is possible to have a single machine (`Godzilla'), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which accesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to access only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla's imputation estimation errors? In this paper we derive answers to these fundamentals questions and develop principled methods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pillars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive experimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {651–660},
numpages = {10},
keywords = {clustering, big data, missing value},
location = {New York, New York, USA},
series = {KDD '14}
}

@article{10.1145/3178315.3178323,
author = {Arruda, Darlan},
title = {Requirements Engineering in the Context of Big Data Applications},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3178315.3178323},
doi = {10.1145/3178315.3178323},
abstract = {Requirements Engineering (RE) plays an essential role in the software engineering process, being considered as one of the most critical phases of the software development life-cycle. As we might expect, then, the Requirements Engineering would play a similar role in the context of Big Data applications. However, practicing Requirements Engineering is a challenging and complex task. It involves (i) stakeholders with diverse backgrounds and levels of knowledge, (ii) different application domains, (iii) it is expensive and error-prone, (iii) it is important to be aligned with business goals, to name a few. Because it involves such complex activities, a lot has to be understood in order to properly address Requirements Engineering. Especially, when the technology domain (e.g., Big Data) is not yet well explored. In this context, this paper describes a research plan on Requirements Engineering involving the development of Big Data applications. The high-level goal is to investigate: (i) On the technical front, the Requirements Engineering activities with respect to the analysis and specification of Big Data requirements and, (ii) on the management side, the relationship between RE and Business Goals in the development of Big Data Software applications.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {1–6},
numpages = {6},
keywords = {big data requirements engineering, big data applications, empirical studies, empirical software engineering., business goals}
}

@inproceedings{10.1145/2513190.2517828,
author = {Cuzzocrea, Alfredo and Bellatreche, Ladjel and Song, Il-Yeol},
title = {Data Warehousing and OLAP over Big Data: Current Challenges and Future Research Directions},
year = {2013},
isbn = {9781450324120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513190.2517828},
doi = {10.1145/2513190.2517828},
abstract = {In this paper, we highlight open problems and actual research trends in the field of Data Warehousing and OLAP over Big Data, an emerging term in Data Warehousing and OLAP research. We also derive several novel research directions arising in this field, and put emphasis on possible contributions to be achieved by future research efforts.},
booktitle = {Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP},
pages = {67–70},
numpages = {4},
keywords = {data warehousing, olap, big data, big multidimensional data},
location = {San Francisco, California, USA},
series = {DOLAP '13}
}

@inproceedings{10.1145/3396956.3398253,
author = {Potiguara Carvalho, Artur and Potiguara Carvalho, Fernanda and Dias Canedo, Edna and Potiguara Carvalho, Pedro Henrique},
title = {Big Data, Anonymisation and Governance to Personal Data Protection},
year = {2020},
isbn = {9781450387910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396956.3398253},
doi = {10.1145/3396956.3398253},
abstract = {In a massive processing data era, an emerging impasse has taking scenario: privacy. In this context, personal data receive particular attention, witch its laws and guidelines that ensure better and legal use of data. The General Data Protection Regulation (GDPR) - in the European Union - and the Brazilian General Data Protection Law (LGPD) - in Brazil - lead to anonymisation (and its processes and techniques) as a way to reach secure use of personal data. However, expectations placed on this tool must be reconsidered according to risks and limits of its use, mainly when this technique is applied to Big Data. We discussed whether anonymisation used in conjunction with good data governance practices could provide greater protection for privacy. We conclude that good governance practices can strengthen privacy in anonymous data belonging to a Big Data, and we present a suggestive governance framework aimed at privacy.},
booktitle = {The 21st Annual International Conference on Digital Government Research},
pages = {185–195},
numpages = {11},
keywords = {Big Data, Personal Data Protection, Anonymisation, Governance, Privacy},
location = {Seoul, Republic of Korea},
series = {dg.o '20}
}

@inproceedings{10.1145/3415958.3433082,
author = {Dessalk, Yared Dejene and Nikolov, Nikolay and Matskin, Mihhail and Soylu, Ahmet and Roman, Dumitru},
title = {Scalable Execution of Big Data Workflows Using Software Containers},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433082},
doi = {10.1145/3415958.3433082},
abstract = {Big Data processing involves handling large and complex data sets, incorporating different tools and frameworks as well as other processes that help organisations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, require taking advantage of the elasticity of cloud infrastructures for scalability. In this paper, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies and message-oriented middleware (MOM) to enable highly scalable workflow execution. The approach is demonstrated in a use case together with a set of experiments that demonstrate the practical applicability of the proposed approach for the scalable execution of Big Data workflows. Furthermore, we present a scalability comparison of our proposed approach with that of Argo Workflows - one of the most prominent tools in the area of Big Data workflows.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {76–83},
numpages = {8},
keywords = {Domain-specific languages, Software containers, Big Data workflows},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/3085228.3085275,
author = {Gong, Yiwei and Janssen, Marijn},
title = {Enterprise Architectures for Supporting the Adoption of Big Data},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085275},
doi = {10.1145/3085228.3085275},
abstract = {Governments from all over the world are struggling to take advantage of big data developments. Enterprise Architecture (EA) can be used as an instrument to integrate big data (BD) in the existing business processes and ICT-landscape. In this policy paper, we explore the role of EA in the adoption of BD. For this, we adopted a qualitative case study approach and investigated a large administrative organization that was in the process of adopting BD. We found in our case study that the first attempts were focused on integrating big data in the current landscape, but this encountered too many challenges that halt progress. To overcome the challenges, a separate BD department and accompanying infrastructure was created. The strategy was first to reap the benefits of BD and to understand what should be done, and thereafter integrating the working systems in the existing landscape. The findings suggest that current infrastructures might not be suitable for integrating BD and substantial changes are needed first. In the case the role of BD needed to be first clarified before EA could play a role in adopting BD. EA should deal with the uncertainties and complexities by ensuring a configurable landscape, by providing an incremental approach for adapting the infrastructure step-by-step, before the benefits of big data can be gained. Developing an incremental migration plan was found to be a key aspect for the adoption of BD.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {505–510},
numpages = {6},
keywords = {open data, e-government, BOLD, infrastructure, enterprise architecture, ICT-architecture, big data},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@article{10.1145/2998575,
author = {Labouseur, Alan G. and Matheus, Carolyn C.},
title = {An Introduction to Dynamic Data Quality Challenges},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2998575},
doi = {10.1145/2998575},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {6},
numpages = {3},
keywords = {relational systems, internet of things, graph systems, big data, Dynamic data quality}
}

@inproceedings{10.1145/3453187.3453340,
author = {Hu, Zhifeng and Zhao, Feng and Zhao, Xiaona},
title = {Research on Smart Education Service Platform Based on Big Data},
year = {2021},
isbn = {9781450389099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453187.3453340},
doi = {10.1145/3453187.3453340},
abstract = {The big data technology can be applied to build the education service platforms and construct the big data analysis and application system as well as the multi-dimensional perception system. The big data analysis assists in the teaching process and breaks the temporal and spatial restrictions of educational resources, to realize the diversification of educational resources and improve the effectiveness of teaching feedback. This paper proposes a smart education service platform based on big data, which can promote the organic integration of educational communication, educational research, learning activities, teaching affairs administration, and information infrastructures. At the same time, the platform provides smarter, more efficient, and accurate services for teaching.},
booktitle = {Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science},
pages = {228–233},
numpages = {6},
keywords = {Information-oriented education, Big data, Smart education},
location = {Wuhan, China},
series = {EBIMCS 2020}
}

@inproceedings{10.5555/2840819.2840927,
author = {Zhu, Yada and Xiong, Jinjun},
title = {Modern Big Data Analytics for "Old-Fashioned" Semiconductor Industry Applications},
year = {2015},
isbn = {9781467383899},
publisher = {IEEE Press},
abstract = {Big data analytics is the latest spotlight with all the glare of fame ranging from media coverage to booming start-up companies to eye-catching merges and acquisitions. On the contrary, the $336 billion industry of semiconductor was seen as an "old-fashioned" business, with fading interests from the best and brightest among young graduates and engineers. How will modern big data analytics help the semiconductor industry walk through this transition? This paper answers this question via a number of practical but challenging problems arising from semiconductor manufacturing process. We show that many existing machine learning algorithms are not well positioned to solve these problems, and novel techniques involving temporal, structural and hierarchical properties need to be developed to solve these problems.},
booktitle = {Proceedings of the IEEE/ACM International Conference on Computer-Aided Design},
pages = {776–780},
numpages = {5},
keywords = {analytics, manufacturing, semiconductor, Big data},
location = {Austin, TX, USA},
series = {ICCAD '15}
}

@inproceedings{10.1145/3445945.3445962,
author = {Zhao, Liangbin and Fu, Xiuju},
title = {A Visual Method for Ship Close Encounter Pattern Recognition Based on Fuzzy Theory and Big Data Intelligence},
year = {2021},
isbn = {9781450387750},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445945.3445962},
doi = {10.1145/3445945.3445962},
abstract = {As an important application of big data technology in the maritime field, big data driven visualization of ship encounter patterns helps to intuitively understand the risk situation in the water traffic. However traditional methods based on fixed thresholds do not consider the fuzziness of classification on ship encounter situations. We proposed a visual method to visualize the risk situations caused by the interaction between vessel traffic flows in more detail based on fuzzy theory and big data intelligence on large scale AIS data. A case study is conducted to verify the applicability based on the AIS data from Singapore Strait. Visualization results of density in grids show that the proposed method can effectively reflect the ship encounter patterns, which are consistent with the real situation and can show more valuable details for the safety assessment of water traffic.},
booktitle = {2020 the 4th International Conference on Big Data Research (ICBDR'20)},
pages = {94–100},
numpages = {7},
keywords = {Ship encounter, Visualization, AIS data, Fuzzy theory},
location = {Tokyo, Japan},
series = {ICBDR 2020}
}

@inproceedings{10.1145/3433996.3434027,
author = {Ding, Shifu and Liu, Yan and Zhang, Jianjun and Tan, Yaqi and Li, Xiaoxia and Tang, RuiChun},
title = {The Planning and Construction of Healthcare Big Data Platform},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434027},
doi = {10.1145/3433996.3434027},
abstract = {Healthcare Big Data Platform is the important content in the process of medical information industry. To a certain extent, it represents the overall level of the regional informatization. It is also a data exchange and sharing platform connecting the basic systems of local various medical and health institutions, and it is also the base and carrier to integrate the regional information system. This paper introduces the local regional medical informatization construction, planning architecture, data center construction mode and technical realization methods. Through this project, the informatization level of basic health agencies and all hospitals will have been greatly improved. It can provide more convenient and high-quality medical service for patients, alleviates "difficulty and expensive" problem effectively.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {170–176},
numpages = {7},
keywords = {SOA, EMR, Healthcare, Big Data, Electronic Health Record},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/3424978.3425010,
author = {Man, Rui and Zhou, Guomin and Fan, Jingchao},
title = {Research on Scientific Data Management in Big Data Era},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425010},
doi = {10.1145/3424978.3425010},
abstract = {Scientific data is an important strategic resource in the era of big data. Efficient management and wide circulation are the key ways to enhance the value of scientific data resources. With the transformation of the industrial society into the information society, the importance of scientific data management is also increasing all over the world, which continuously promotes the maturity of scientific data management and sharing. In this article, through comprehensive research of scientific data management ideas, policies, practices and results, the analysis summarizes the advanced experience of international scientific data management, for the similar problems and challenges existing in the research in China, puts forward the future a period of time the direction and suggestions on the development of scientific data management: 1. the specification of various kinds of degree of the standardization of scientific data resources; 2. To strengthen data mining capacity; 3. To strengthen the cultivation of talents in data science; 4. To strengthen international cooperation and enhance core competitiveness in the big data era.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {32},
numpages = {6},
keywords = {Scientific data, Big data, Scientific data management, Opening and sharing of data resource},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.1145/2676536.2676538,
author = {Chen, Xin and Vo, Hoang and Aji, Ablimit and Wang, Fusheng},
title = {High Performance Integrated Spatial Big Data Analytics},
year = {2014},
isbn = {9781450331326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676536.2676538},
doi = {10.1145/2676536.2676538},
abstract = {The growth of spatial big data has been explosive thanks to cost-effective and ubiquitous positioning technologies, and the generation of data from multiple sources in multi-forms. Such emerging spatial data has high potential to create new insights and values for our life through spatial analytics. However, spatial data analytics faces two major challenges. First, spatial data is both data-and compute-intensive due to the massive amounts of data and the multi-dimensional nature, which requires high performance spatial computing infrastructure and methods. Second, spatial big data sources are often isolated, for example, OpenStreetMap, census data and Twitter tweets are independent data sources. This leads to incompleteness of information and sometimes limited data accuracy, thus limited values from the data. Integrating spatial big data analytics by consolidating multiple data sources provides significant potential for data quality improvement in terms of completeness and accuracy, and much increased values derived from the data. In this paper, we present our vision of a high performance integrated spatial big data analytics framework. We provide a scalable spatial query based data integration engine with MapReduce, and demonstrate integrated spatial data analytics through a few use cases in our preliminary work. We then present our future plan on integrated spatial big data analytics for improving public health research and applications.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {11–14},
numpages = {4},
keywords = {database, GIS, spatial analytics, data warehouse, MapReduce},
location = {Dallas, Texas},
series = {BigSpatial '14}
}

@proceedings{10.1145/3555962,
title = {ICCBDC '22: Proceedings of the 2022 6th International Conference on Cloud and Big Data Computing},
year = {2022},
isbn = {9781450396578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Birmingham, United Kingdom}
}

@inproceedings{10.1145/3508259.3508284,
author = {Sun, Yu and Niu, Yanfang and Lu, Le},
title = {Research on Influencing Factors of Government Audit Big Data Capability},
year = {2022},
isbn = {9781450384162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508259.3508284},
doi = {10.1145/3508259.3508284},
abstract = {The widespread application of big data has had a profound impact on social and economic development. Government auditing is the guarantee for the modernization of national governance, and the development of big data audit capability has become the key to improving national governance capability. This paper summarizes the concept of government audit big data capability, and constructs the influencing factor model of government audit big data capability. This study finds that the construction degree of audit big data platform, big data management ability, big data audit technology and auditors' big data technology ability have a significant positive impact on the government audit big data ability, and the audit organization coordination ability plays a positive moderating effect in the whole impact process. This study provides guidance for the improvement and development of government audit big data capability.},
booktitle = {2021 4th Artificial Intelligence and Cloud Computing Conference},
pages = {172–178},
numpages = {7},
keywords = {Government audit, Influencing factors, Big data analysis capability},
location = {Kyoto, Japan},
series = {AICCC '21}
}

@article{10.1145/2481244.2481247,
author = {Lin, Jimmy and Ryaboy, Dmitriy},
title = {Scaling Big Data Mining Infrastructure: The Twitter Experience},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/2481244.2481247},
doi = {10.1145/2481244.2481247},
abstract = {The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on "big data". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life "in the trenches" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall "big picture" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as "plumbing". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.},
journal = {SIGKDD Explor. Newsl.},
month = {apr},
pages = {6–19},
numpages = {14}
}

@article{10.1145/3383464,
author = {Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv},
title = {SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3383464},
doi = {10.1145/3383464},
abstract = {Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {46},
numpages = {40},
keywords = {service level agreement, SLA metrics, service layer, big data analytics application, SLA, Big data}
}

@article{10.1145/2932707,
author = {Fang, Ruogu and Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Iyengar, S. S.},
title = {Computational Health Informatics in the Big Data Age: A Survey},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2932707},
doi = {10.1145/2932707},
abstract = {The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {12},
numpages = {36},
keywords = {machine learning, data mining, survey, clinical decision support, 4V challenges, Big data analytics, computational health informatics}
}

@inproceedings{10.1145/3402569.3409041,
author = {Han, Ping},
title = {Research on Foreign Exchange Management Model Based on Big Data},
year = {2020},
isbn = {9781450377546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402569.3409041},
doi = {10.1145/3402569.3409041},
abstract = {The rapid development of Internet technology has promoted the process of economic globalization, and the application of big data technology has injected new vitality into the innovation of foreign exchange management models. Businesses such as foreign exchange deposits and loans and foreign currency exchange have encountered new development opportunities. Under the effect of big data technology, it can quickly process massive amounts of information, respond to various exchange rate changes, and achieve the improvement of foreign exchange business management. Especially under the circumstances of the current RMB marketization, exchange rate reform and the diversification of the international situation, the difficulty of foreign exchange management is gradually increasing. How to better improve the efficiency of foreign exchange management has become a problem that must be solved at present. Therefore, it is of great significance to explore the foreign exchange management model based on the background of big data, build a big data computing mechanism, give full play to its advantages in foreign exchange management, and promote the improvement of foreign exchange management.},
booktitle = {Proceedings of the 5th International Conference on Distance Education and Learning},
pages = {162–165},
numpages = {4},
keywords = {Big data background, mode, foreign exchange management},
location = {Beijing, China},
series = {ICDEL 2020}
}

@inproceedings{10.1145/2791347.2791380,
author = {Zakerzadeh, Hessam and Aggarwal, Charu C. and Barker, Ken},
title = {Privacy-Preserving Big Data Publishing},
year = {2015},
isbn = {9781450337090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791347.2791380},
doi = {10.1145/2791347.2791380},
abstract = {The problem of privacy-preserving data mining has been studied extensively in recent years because of its importance as a key enabler in the sharing of massive data sets. Most of the work in privacy has focussed on issues involving the quality of privacy preservation and utility, though there has been little focus on the issue of scalability in privacy preservation. The reason for this is that anonymization has generally been seen as a batch and one-time process in the context of data sharing. However, in recent years, the sizes of data sets have grown tremendously to a point where the effective application of the current algorithms is becoming increasingly difficult. Furthermore, the transient nature of recent data sets has resulted in an increased need for the repeated application of such methods on the newer data sets which have been collected. Repeated application demands even greater computational efficiency in order to be practical. For example, an algorithm with quadratic complexity is unlikely to be implementable in reasonable time over terabyte scale data sets. A bigger issue is that larger data sets are likely to be addressed by distributed frameworks such as MapReduce. In such frameworks, one has to address the additional issue of minimizing data transfer across different nodes, which is the bottleneck. In this paper, we discuss the first approach towards privacy-preserving data mining of very massive data sets using MapReduce. We study two most widely-used privacy models k-anonymity and l-diversity for anonymization, and present experimental results illustrating the effectiveness of the approach.},
booktitle = {Proceedings of the 27th International Conference on Scientific and Statistical Database Management},
articleno = {26},
numpages = {11},
location = {La Jolla, California},
series = {SSDBM '15}
}

@inproceedings{10.1145/3374587.3374650,
author = {Shen, Shaoyi and Li, Bin and Li, Situo},
title = {Construction and Application of Big Data Analysis Platform for Enterprise},
year = {2020},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374650},
doi = {10.1145/3374587.3374650},
abstract = {A data revolution has been leading by big data, which have the extremely profound influence on the economic, social development and public life. This paper introduces the meaning of big data, and discusses the innovation and opportunity of enterprise under the perspective of big data. According to the information architecture, this paper supplies the basic construction of enterprise big data analysis platform, and suggests the strategy of application, which have certain realistic directive significance.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {54–58},
numpages = {5},
keywords = {Construction, Analysis of big data, Data asset, Distributed, Shared},
location = {Normal, IL, USA},
series = {CSAI2019}
}

@inproceedings{10.1145/2815782.2815793,
author = {Malaka, Iman and Brown, Irwin},
title = {Challenges to the Organisational Adoption of Big Data Analytics: A Case Study in the South African Telecommunications Industry},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815793},
doi = {10.1145/2815782.2815793},
abstract = {The purpose of this interpretive study was to explore the challenges to the adoption of Big Data Analytics (BDA) in organisations. The Technology-Organisation-Environment (TOE) model was used to guide the study. Data was collected from a large telecommunication organization in South Africa. Seven participants, from both Information Technology (IT) and business were interviewed to gain a holistic overview of challenges towards the adoption of BDA. An inductive approach was used for analysis. Findings revealed technological challenges to the adoption of BDA as being Data Integration; Data Privacy; Return on Investment; Data Quality; Cost; Data Integrity; and Performance and Scalability. From the organizational perspective, the major challenges were Ownership and Control; Skills Shortages; Business Focus and Prioritisation; Training and Exposure; Silos; and Unclear Processes. From the environmental context there were no major challenges highlighted. Organisational challenges were deemed to be the major inhibitors to adoption of BDA},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {27},
numpages = {9},
keywords = {Big Data Analytics, South Africa, Technology Adoption, Big Data},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3501409.3501593,
author = {Diao, Yanhua},
title = {Tourism Prediction Based on Multi-Source Big Data Fusion Technology},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501593},
doi = {10.1145/3501409.3501593},
abstract = {In the practical application of existing big data tourism prediction, there are some practical problems, such as complicated data sources and difficult fusion, low prediction accuracy and poor guiding practice effect. In view of this situation, this paper intends to build a tourism big data index prediction model suitable for the characteristics of tourism development through core data extraction, multi-source data fusion, complex data modeling and other key technologies. With the help of the improved tourism prediction model based on multi-source big data fusion technology, the tourist flow and consumption characteristics of Shandong province are more accurately identified and predicted. It can provide help for optimizing public service of tourism, strengthening early warning of tourist flow and improving marketing strategy of tourist destination. This study innovatively supplements the effective integration theory of multi-source tourism big data and the organic integration theory of big data and traditional sampling survey data. At the same time, the relevant methods of tourism big data forecasting model are extended.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1030–1036},
numpages = {7},
keywords = {Tourism prediction, Multi-source big data, Data fusion technology},
location = {Xiamen, China},
series = {EITCE 2021}
}

@inproceedings{10.1145/3193063.3193069,
author = {Cheng, Susu and Zhao, Haijun},
title = {An Overview of Techniques for Confirming Big Data Property Rights},
year = {2018},
isbn = {9781450363785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193063.3193069},
doi = {10.1145/3193063.3193069},
abstract = {The major premise of big data circulation is to identify the ownership of data resource. This paper summed some feasible techniques and methods for confirming big data property which are data citation technology, data provenance technology, data reversible hiding technology, computer forensic technology and block chain technology. The ownership of information property which from different sizes, different formats and different storage condition on distributed heterogeneous platforms can be confirmed by comprehensive application of these techniques and methods based on the coupling interface between them in the practice of big data.},
booktitle = {Proceedings of the 2018 International Conference on Intelligent Information Technology},
pages = {59–64},
numpages = {6},
keywords = {Confirmation of Information Property, Big Data, Method for confirming information property rights, Information property index},
location = {Ha Noi, Viet Nam},
series = {ICIIT 2018}
}

@inproceedings{10.1145/3297662.3365797,
author = {Musto, Jiri and Dahanayake, Ajantha},
title = {Integrating Data Quality Requirements to Citizen Science Application Design},
year = {2020},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365797},
doi = {10.1145/3297662.3365797},
abstract = {Data quality is an important aspect in many fields. In citizen science application databases, data quality is often found lacking, which is why there needs to be a method of integrating data quality into the design. This paper tackles the problem by dividing data quality into separate characteristics according to the ISO / IEC 25012 standard. These characteristics are integrated into a conceptual model of the system and data model for citizen science applications. Furthermore, the paper describes a way to measure data quality using the data quality characteristics. The models and measuring methods are theoretical and can be adapted into case specific designs.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {166–173},
numpages = {8},
keywords = {Data Quality Characteristics, Citizen science, Data quality, Conceptual model, Data Quality requirements},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1145/3357292.3357302,
author = {Kun-fa, Li and Jing-chun, Chen and Yan-xi, Wang},
title = {Big Data Informatization Applied to Optimization of Human Resource Performance Management},
year = {2019},
isbn = {9781450371445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357292.3357302},
doi = {10.1145/3357292.3357302},
abstract = {With the development of technology in the era of digital big data in the network and the promotion of network technology, big data is simultaneously integrated into different industry sectors to achieve Internet performance management, and enhance the new perspective of enterprise human resources performance management activities. Today's Internet, cloud computing, Internet of Things and other industrial technologies have undergone repeated changes, showing an unprecedented picture. At present, the subjective awareness of enterprise human resources performance management is too strong, lack of objective data understanding, and the theoretical framework of big data human resource management is not fully applied. This paper reconstructs the data system from four aspects: data source, collection, integration and analysis. Innovate the human resources performance management method from the system to provide more scientific and specific ideas for human resource performance management.},
booktitle = {Proceedings of the 2019 2nd International Conference on Information Management and Management Sciences},
pages = {12–17},
numpages = {6},
keywords = {Big data, human resources, performance management},
location = {Chengdu, China},
series = {IMMS 2019}
}

@inproceedings{10.1145/3167918.3167924,
author = {Al-Mansoori, Ahmed and Yu, Shui and Xiang, Yong and Sood, Keshav},
title = {A Survey on Big Data Stream Processing in SDN Supported Cloud Environment},
year = {2018},
isbn = {9781450354363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167918.3167924},
doi = {10.1145/3167918.3167924},
abstract = {Big data is the term which denotes data with features such as voluminous data, a variety of data and streaming data as well. Processing big data became essential for enterprises to garner general intelligence and avoid biased conclusions. Due to these features, big data processing is considered to be a challenging task. Big data Processing should rely on a robust network. Cloud computing offers a suitable environment for these processes. However, it is more challenging when we move big data to the cloud, as managing the cloud resources is the main issue. Software Defined Network (SDN) has a potential solution to this issue. In this paper, first, we survey the present state of the art of SDN, cloud computing, and Big data Stream processing (BDSP). Then, we discuss SDN in the context of Big Data Stream Processing in Cloud environment. Finally, critical issues and research opportunity are discussed.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {12},
numpages = {11},
keywords = {resource optimization, big data stream processing, cloud computing, cost optimization, big data, SDN},
location = {Brisband, Queensland, Australia},
series = {ACSW '18}
}

@proceedings{10.1145/3561801,
title = {BDIOT '22: Proceedings of the 2022 5th International Conference on Big Data and Internet of Things},
year = {2022},
isbn = {9781450390361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chongqing, China}
}

@inproceedings{10.1145/3220228.3220236,
author = {Saraee, Mo and Silva, Charith},
title = {A New Data Science Framework for Analysing and Mining Geospatial Big Data},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220236},
doi = {10.1145/3220228.3220236},
abstract = {Geospatial Big Data analytics are changing the way that businesses operate in many industries. Although a good number of research works have reported in the literature on geospatial data analytics and real-time data processing of large spatial data streams, only a few have addressed the full geospatial big data analytics project lifecycle and geospatial data science project lifecycle. Big data analysis differs from traditional data analysis primarily due to the volume, velocity and variety characteristics of the data being processed. One of a motivation of introducing new framework is to address these big data analysis challenges. Geospatial data science projects differ from most traditional data analysis projects because they could be complex and in need of advanced technologies in comparison to the traditional data analysis projects. For this reason, it is essential to have a process to govern the project and ensure that the project participants are competent enough to carry on the process. To this end, this paper presents, new geospatial big data mining and machine learning framework for geospatial data acquisition, data fusion, data storing, managing, processing, analysing, visualising and modelling and evaluation. Having a good process for data analysis and clear guidelines for comprehensive analysis is always a plus point for any data science project. It also helps to predict required time and resources early in the process to get a clear idea of the business problem to be solved.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {98–102},
numpages = {5},
keywords = {data mining, big data, geospatial big data, data science, machine learning},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.1145/3194206.3194229,
author = {Zhichao, Xu and Jiandong, Zhao and Huan, Huang},
title = {Based on Hadoop's Tech Big Data Combination and Mining Technology Framework},
year = {2018},
isbn = {9781450363457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194206.3194229},
doi = {10.1145/3194206.3194229},
abstract = {With the advent of the Internet + era in the field of Tech big data, the big data of Tech big data has a large amount of data and various characteristics. It is an important means to carry out research on the big data of Tech big data to realize the combination and mining of efficient multi-source foreign technology data. However, at present, the big data of Tech big data are divided into disciplines and different formats, which are difficult to realize the intersection of effective scientific and technological information and realize data sharing. This paper puts forward a kind of big data combined with Tech big data and mining technology based on the Hadoop framework.It includes a unified collection and preprocessing method of big data of Tech big data and the design of storage and management platform for data sources. It is based on Map/Reduce Tech big data parallelization computing model and system.Its correlation with important scientific data mining services.The framework has good practicability and expansibility.},
booktitle = {Proceedings of the 2nd International Conference on Innovation in Artificial Intelligence},
pages = {59–63},
numpages = {5},
keywords = {tech big data, mining, Hadoop, combination},
location = {Shanghai, China},
series = {ICIAI '18}
}

@techreport{10.5555/2849516,
author = {Markus, M. Lynne and Topi, Heikki},
title = {Big Data, Big Decisions for Science, Society, and Business: Report on a Research Agenda Setting Workshop},
year = {2015},
publisher = {National Science Foundation},
address = {USA},
abstract = {The report from the workshop, "Big Data, Big Decisions for Government, Business and Society," makes a number of astute contributions. There is no need to replicate them in this foreword - they are in the report. What might be missed comes between the lines, where provocative points are made. Big Data means big opinions and big stakes. Those who think Big Data important want to be proven right, those who think Big Data a passing fad want Big Data to fade, and those who think Big Data will bring profound change hope for change. Big Data, like everything important, is political.}
}

@article{10.1145/2094114.2094129,
author = {Bizer, Christian and Boncz, Peter and Brodie, Michael L. and Erling, Orri},
title = {The Meaningful Use of Big Data: Four Perspectives -- Four Challenges},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2094114.2094129},
doi = {10.1145/2094114.2094129},
abstract = {Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.},
journal = {SIGMOD Rec.},
month = {jan},
pages = {56–60},
numpages = {5}
}

@inproceedings{10.1145/3234698.3234723,
author = {El Bousty, Hicham and krit, Salah-ddine and Elasikri, Mohamed and Dani, Hassan and Karimi, Khaoula and Bendaoud, Kaoutar and Kabrane, Mustapha},
title = {Investigating Business Intelligence in the Era of Big Data: Concepts, Benefits and Challenges},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234723},
doi = {10.1145/3234698.3234723},
abstract = {Business intelligence suppose retrieving value from data floating in the organization environment. It provides methods and tools for collecting, storing, formatting and analyzing data for the purpose of helping managers in decision-making. At the start, only data from enterprise internal activities were examined. Now and in this turbulent business environment, organizations should incorporate analysis of the huge amount of external data gathered from multifarious sources. It is argued that BI systems accuracy depends on quantity of data at their disposal, yet some storage and analysis methods are phased out and should be reviewed by academics and practitioners.This paper presents an overview of BI challenges in the context of Big Data (BD) and some available solutions provided, either by using Cloud Computing (CC) or improving Data Warehouse (DW) efficiency.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {25},
numpages = {9},
keywords = {Big Data, Cloud Computing, Business Intelligence, Data Warehouse},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

@inproceedings{10.1145/3321454.3321474,
author = {Yu, Bangbo and Zhao, Haijun},
title = {Research on the Construction of Big Data Trading Platform in China},
year = {2019},
isbn = {9781450366335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321454.3321474},
doi = {10.1145/3321454.3321474},
abstract = {As a new type of asset, the value of big data resources can only be realized in the transaction circulation. Establishing and improving the big data trading platform market system is a systematic project to transform data from resources into assets. Through the comparative analysis of several typical big data trading platform construction practices in China, this research finds some problems, such as unclear positioning of some platforms leading to overlapping functions, extensive data transaction, lack of unified data pricing methods, unclear data ownership. In addition, there is no difference between the data escrow transaction mode and the aggregate transaction mode, and the rights of the data supply parties cannot be guaranteed. And it also discusses how to promote the construction and improvement of China's big data trading market more systematically, normally and institutionally. Finally, it proposes to build local big data trading platforms according to local conditions, establish a data transaction system based on blockchain, establish a big data transaction pricing index system, and establish a big data standard system.},
booktitle = {Proceedings of the 2019 4th International Conference on Intelligent Information Technology},
pages = {107–112},
numpages = {6},
keywords = {big data trading platform, Data assets, regulatory construction},
location = {Da, Nang, Viet Nam},
series = {ICIIT '19}
}

@inproceedings{10.1145/3510003.3510619,
author = {Gote, Christoph and Mavrodiev, Pavlin and Schweitzer, Frank and Scholtes, Ingo},
title = {Big Data = Big Insights? Operationalising Brooks' Law in a Massive GitHub Data Set},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510619},
doi = {10.1145/3510003.3510619},
abstract = {Massive data from software repositories and collaboration tools are widely used to study social aspects in software development. One question that several recent works have addressed is how a software project's size and structure influence team productivity, a question famously considered in Brooks' law. Recent studies using massive repository data suggest that developers in larger teams tend to be less productive than smaller teams. Despite using similar methods and data, other studies argue for a positive linear or even super-linear relationship between team size and productivity, thus contesting the view of software economics that software projects are diseconomies of scale.In our work, we study challenges that can explain the disagreement between recent studies of developer productivity in massive repository data. We further provide, to the best of our knowledge, the largest, curated corpus of GitHub projects tailored to investigate the influence of team size and collaboration patterns on individual and collective productivity. Our work contributes to the ongoing discussion on the choice of productivity metrics in the operationalisation of hypotheses about determinants of successful software projects. It further highlights general pitfalls in big data analysis and shows that the use of bigger data sets does not automatically lead to more reliable insights.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {262–273},
numpages = {12},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3134271.3134296,
author = {Peng, Michael Yao-Ping and Tuan, Sheng-Hwa and Liu, Feng-Chi},
title = {Establishment of Business Intelligence and Big Data Analysis for Higher Education},
year = {2017},
isbn = {9781450352765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134271.3134296},
doi = {10.1145/3134271.3134296},
abstract = {The applications on business intelligence and big data analysis to extract useful information are getting more mature, but the development and operations in higher education institutions are still be lack. This study aims to explore how HEIs employ business intelligence to analysis and mining student learning and HEIs' operation data from database. The outcomes are benefit for universities to support the management of decision-making.},
booktitle = {Proceedings of the International Conference on Business and Information Management},
pages = {121–125},
numpages = {5},
keywords = {Institutional Research, Business Intelligence, Database, Big data},
location = {Bei Jing, China},
series = {ICBIM 2017}
}

@inproceedings{10.1145/3331453.3361308,
author = {Jing, Furong and Cao, Yongsheng and Fang, Wei and Chen, Yanqing},
title = {Construction and Implementation of Big Data Framework for Crop Germplasm Resources},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3361308},
doi = {10.1145/3331453.3361308},
abstract = {Based on understanding the application of big data and the research status of crop germplasm resources, this paper proposes a system architecture that is suitable for crop germplasm resources big data. Among them, the overall architecture of germplasm resources is elaborated through six functional modules, including data source, data integration, data processing, data application, big data operation and maintenance platform, and data management and security. The logical functional architecture specification was formulated and the technical implementation and selection are defined. The technical implementation framework describes the technical implementation of germplasm resources big data, and jointly supports the construction and operation of germplasm resources big data. Finally, a verification system is established to verify the feasibility of the big data system framework for germplasm resources.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {27},
numpages = {7},
keywords = {Data management, Big data architecture, Data analysis, Crop germplasm resources},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/3349341.3349371,
author = {Li, Jiale and Liao, Shunbao},
title = {Quality Control Framework of Big Data for Early Warning of Agricultural Meteorological Disasters},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349371},
doi = {10.1145/3349341.3349371},
abstract = {Agricultural meteorological disasters, including floods, droughts, dry hot winds, low temperature chills, typhoons, hail and continuous rain, can lead to significant reduction in agricultural output. Big data platform for early warning of agricultural meteorological disaster is the basis of business operation system for early warning of agricultural meteorological disasters, and the data quality is an important guarantee for success of the early warning. Quality control of big data for early warning of agricultural meteorological disaster involves names of data sets, metadata, data documents and content of data sets. The quality control for contents of data sets is divided into quality control of attribute data and that of spatial data, and quality control of spatial data is divided into quality control of vector data and that of raster data. Methods for data quality control are divided into fully automatic, semi-automatic and full manual control methods.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {74–78},
numpages = {5},
keywords = {big data, agro-meteorological disasters, framework, quality control, early warning},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@article{10.1145/2968332,
author = {Geisler, Sandra and Quix, Christoph and Weber, Sven and Jarke, Matthias},
title = {Ontology-Based Data Quality Management for Data Streams},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2968332},
doi = {10.1145/2968332},
abstract = {Data Stream Management Systems (DSMS) provide real-time data processing in an effective way, but there is always a tradeoff between data quality (DQ) and performance. We propose an ontology-based data quality framework for relational DSMS that includes DQ measurement and monitoring in a transparent, modular, and flexible way. We follow a threefold approach that takes the characteristics of relational data stream management for DQ metrics into account. While (1) Query Metrics respect changes in data quality due to query operations, (2) Content Metrics allow the semantic evaluation of data in the streams. Finally, (3) Application Metrics allow easy user-defined computation of data quality values to account for application specifics. Additionally, a quality monitor allows us to observe data quality values and take counteractions to balance data quality and performance. The framework has been designed along a DQ management methodology suited for data streams. It has been evaluated in the domains of transportation systems and health monitoring.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {18},
numpages = {34},
keywords = {Data streams, ontologies, data quality assessment, data quality control}
}

@inproceedings{10.1145/3366030.3366044,
author = {Cuzzocrea, Alfredo},
title = {Big Data Management and Analytics in Intelligent Smart Environments: State-of-the-Art Analysis and Future Research Directions},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366044},
doi = {10.1145/3366030.3366044},
abstract = {This paper focuses on big data management and analytics in intelligent smart environments, with particular regards to intelligent transportation and logistics systems, and provides relevant research directions that may represent a milestone for future years.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {5–7},
numpages = {3},
keywords = {Big data analytics, Intelligent smart environments, Big data management},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3341620.3341624,
author = {Jia, Fengsheng and Gao, Yang and Wang, Yuming},
title = {Study on Standard System of Aerospace Quality Data Resources Integration under the Background of Big Data},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341624},
doi = {10.1145/3341620.3341624},
abstract = {The integration and application of aerospace product quality data resources is an important way to carry out quality improvement, quality evaluation and precise management. Standardization is the basis for promoting quality data resources integration. The unified and normative standard system is the guarantee for efficient development of integration standards. Firstly, we analyzed the features of quality data resources according to the status quo of integration. Integration structure of quality data resources in terms of vertical and horizontal integration was proposed by adopting the methods of "decomposition-integration" and "classification-association". Secondly, we constructed a three-dimensions architecture of quality data resource integration using the method of system engineering methodology, from the layer dimension (basis, common, special), technical dimension (description, collection, storage, transmission, processing, comprehensive management) and category dimension (rocket, spacecraft). Thirdly, we worked out 20 lists about basis and common standard by adopting the top-down approach. Some standard development suggestions are proposed based on the characteristics of quality data resources and standard research strategies. Finally, we applied the quality problem data resource standard construction and application to verify the proposed method.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {16–22},
numpages = {7},
keywords = {quality data resource integration, standard system, system planning, aerospace products},
location = {Hong Kong, Hong Kong},
series = {BDE 2019}
}

@inproceedings{10.1145/3379247.3379282,
author = {Liyao, Zhou and Xiaofang, Liu and Chunyu, Hu},
title = {Evaluation Method of Equipment Combat Effectiveness Based On Big Data Mining},
year = {2020},
isbn = {9781450376730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379247.3379282},
doi = {10.1145/3379247.3379282},
abstract = {In the evaluation of equipment combat effectiveness, it is necessary to comprehensively analyze the data of outfield test and infield test, including a variety of audio-visual, image and other combat test data. These data can be classified, extracted, stored and managed by building data model through big data mining technology. The evaluation method of equipment combat effectiveness based on big data mining is based on massive data, through machine learning, statistical analysis, neural network, database and other methods to analyze and process the data, mining the correlation between test data, evaluation index and evaluation conclusion, and extracting useful information and finding new knowledge from it to realize the evaluation of the combat effectiveness of the tested system.},
booktitle = {Proceedings of 2020 the 6th International Conference on Computing and Data Engineering},
pages = {131–135},
numpages = {5},
keywords = {Big data mining, combat effectiveness evaluation, combat test},
location = {Sanya, China},
series = {ICCDE 2020}
}

@inproceedings{10.1145/2896825.2896831,
author = {Scavuzzo, Marco and Tamburri, Damian A. and Di Nitto, Elisabetta},
title = {Providing Big Data Applications with Fault-Tolerant Data Migration across Heterogeneous NoSQL Databases},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896831},
doi = {10.1145/2896825.2896831},
abstract = {The recent growing interest on highly-available data-intensive applications sparked the need for flexible and portable storage technologies, e.g., NoSQL databases. Unfortunately, the lack of standard interfaces and architectures for NoSQLs makes it difficult and expensive to create portable applications, which results in vendor lock-in. Building on previous work, we aim at providing guaranteed fault-tolerant techniques and supporting architectures to port or migrate data to and across heterogeneous NoSQL technology. To prove the effectiveness of our approach we evaluate it on an industrial case-study. We conclude that our method and supporting architecture offer an efficient and fault-tolerant mechanism for NoSQL portability and interoperation.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {26–32},
numpages = {7},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/3371425.3371435,
author = {Liu, He and Wang, Xiaohui and Lei, Shuya and Zhang, Xi and Liu, Weiwei and Qin, Ming},
title = {A Rule Based Data Quality Assessment Architecture and Application for Electrical Data},
year = {2019},
isbn = {9781450376334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371425.3371435},
doi = {10.1145/3371425.3371435},
abstract = {Data quality assessment plays an important role in electricity consumption big data. It can help business people master the overall data situation, which can provide a strong guarantee for subsequent data improvement, analysis and decision. According to the electrical data quality issues, we design a rule-based data quality assessment architecture for electrical big data. It includes six types of data quality assessment indexes (such as comprehensiveness, accuracy, completeness), and the related data quality rules (such as non-empty rule and range rule), which can be used to guide the electrical data quality inspection. Meanwhile, for the accuracy, we propose an outlier detection method based on time time-relevant k-means, which is used to detect the voltage, curve and power data issues in electricity data. The experimental and simulation results show that the proposed architecture and method can work well for the comprehensive data quality assessment of electrical data.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing},
articleno = {40},
numpages = {6},
keywords = {electrical data, outlier, data quality, quality assessment},
location = {Sanya, China},
series = {AIIPCC '19}
}

@inproceedings{10.1145/2389686.2389688,
author = {Megler, V. M. and Maier, David},
title = {When Big Data Leads to Lost Data},
year = {2012},
isbn = {9781450317191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2389686.2389688},
doi = {10.1145/2389686.2389688},
abstract = {For decades, scientists bemoaned the scarcity of observational data to analyze and against which to test their models. Exponential growth in data volumes from ever-cheaper environmental sensors has provided scientists with the answer to their prayers: "big data". Now, scientists face a new challenge: with terabytes, petabytes or exabytes of data at hand, stored in thousands of heterogeneous datasets, how can scientists find the datasets most relevant to their research interests? If they cannot find the data, then they may as well never have collected it; that data is lost to them. Our research addresses this challenge, using an existing scientific archive as our test-bed. We approach this problem in a new way: by adapting Information Retrieval techniques, developed for searching text documents, into the world of (primarily numeric) scientific data. We propose an approach that uses a blend of automated and "semi-curated" methods to extract metadata from large archives of scientific data. We then perform searches over the extracted metadata, returning results ranked by similarity to the query terms. We briefly describe an implementation performed at an ocean observatory to validate the proposed approach. We propose performance and scalability research to explore how continued archive growth will affect our goal of interactive response, no matter the scale.},
booktitle = {Proceedings of the 5th Ph.D. Workshop on Information and Knowledge},
pages = {1–8},
numpages = {8},
keywords = {ranked data search, scientific data},
location = {Maui, Hawaii, USA},
series = {PIKM '12}
}

@inproceedings{10.1145/3512576.3512618,
author = {Sardjono, Wahyu and Retnowardhani, Astari and Emil Kaburuan, Robert and Rahmasari, Aninda},
title = {Artificial Intelligence and Big Data Analysis Implementation in Electronic Medical Records},
year = {2022},
isbn = {9781450384971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512576.3512618},
doi = {10.1145/3512576.3512618},
abstract = {Industry 4.0 is the pioneer of the Internet of Things (IoT). The Internet of Things (IoT) are often heard and successful in business revolution from all sectors. The IoT are widely used in numerous sectors including medical services and have become the rise of Internet of Medical Things (IoMT). One of the implementations is the Electronic Health Record (EHR) systems. Previously the health records were used in traditional manner such as print-out health record of a patient and stored to an archive room. With the innovation of EHR, patients’ health records are digitalized which provides advantages from space efficiency and paperless forms. EHR helps medical service management to provide better healthcare services. With the integration of Artificial Intelligence (AI) and Big Data Analysis in EHR, healthcare services provide more accurate and reliable diagnosis.},
booktitle = {2021 The 9th International Conference on Information Technology: IoT and Smart City},
pages = {231–237},
numpages = {7},
keywords = {Analysis, internet of things, artificial intelligence, electronic medical records, big data},
location = {Guangzhou, China},
series = {ICIT 2021}
}

@inproceedings{10.1145/2910019.2910033,
author = {Netten, Niels and van den Braak, Susan and Choenni, Sunil and van Someren, Maarten},
title = {A Big Data Approach to Support Information Distribution in Crisis Response},
year = {2016},
isbn = {9781450336406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910019.2910033},
doi = {10.1145/2910019.2910033},
abstract = {Crisis response organizations operate in very dynamic environments, in which it is essential for responders to acquire all information critical to their task execution in time. In reality, the responders are often faced with information overload, incomplete information, or a combination of both. This hampers their decision-making process, workflow, situational awareness and, consequently, effective execution of collaborative crisis response. Therefore, getting the right information to the right person at the right time is of crucial importance.The task of processing all data during crisis response situations and determining for whom at a particular moment the information is relevant is not straightforward. When developing an information system to support this task, some important challenges have to be taken into account. These challenges relate to the structure and truthfulness of the used data, the assessment of information relevance, and the dissemination of relevant information in time. While methods and techniques from big data can be used to collect and integrate data, machine learning can be used to build a model for relevance assessments. An example implementation of such a framework of big data is the TAID software system that collects and integrates data communicated between first responders and may send information to crisis responders that were not addressed in the initial communication. As an example of the impact of TAID on crisis response, we show its effect in a simulated crisis response scenario.},
booktitle = {Proceedings of the 9th International Conference on Theory and Practice of Electronic Governance},
pages = {266–275},
numpages = {10},
keywords = {Relevance Assessments, Information Distribution, Machine Learning, Crisis Response for Public Safety, Big Data},
location = {Montevideo, Uruguay},
series = {ICEGOV '15-16}
}

@inproceedings{10.1145/3529190.3529222,
author = {Pleimling, Xavier and Shah, Vedant and Lourentzou, Ismini},
title = {[Data] Quality Lies In The Eyes Of The Beholder},
year = {2022},
isbn = {9781450396318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529190.3529222},
doi = {10.1145/3529190.3529222},
abstract = {As large-scale machine learning models become more prevalent in assistive and pervasive technologies, the research community has started examining limitations and challenges that arise from training data, e.g., fairness, bias, and interpretability issues. To this end, data-centric approaches are increasingly prevailing over time, showing that high-quality data is a critical component in many applications. Several studies explore methods to define and improve data quality, however, no uniform definition exists. In this work, we present an empirical analysis of the multifaceted problem of evaluating data quality. Our work aims at identifying data quality challenges that are most commonly observed by data users and practitioners. Inspired by the need for generally applicable methods, we select a representative set of quality indicators, that covers a broad spectrum of issues, and investigate the utility of these indicators on a broad range of datasets through inter-annotator agreement analysis. Our work provides insights and presents open challenges in designing improved data life cycles.},
booktitle = {Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {118–124},
numpages = {7},
keywords = {data quality metrics, duplicate data, inconsistent data, data quality, user survey, datasets, incomplete data, data utility, data annotation, incorrect data},
location = {Corfu, Greece},
series = {PETRA '22}
}

@article{10.1145/3148239,
author = {Bertossi, Leopoldo and Milani, Mostafa},
title = {Ontological Multidimensional Data Models and Contextual Data Quality},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3148239},
doi = {10.1145/3148239},
abstract = {Data quality assessment and data cleaning are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD model), which can be used to model and represent contexts as logic-based ontologies. The data under assessment are mapped into the context for additional analysis, processing, and quality data extraction. The resulting contexts allow for the representation of dimensions, and multidimensional data quality assessment becomes possible. At the core of a multidimensional context, we include a generalized multidimensional data model and a Datalog± ontology with provably good properties in terms of query answering. These main components are used to represent dimension hierarchies, dimensional constraints, and dimensional rules and define predicates for quality data specification. Query answering relies on and triggers navigation through dimension hierarchies and becomes the basic tool for the extraction of quality data. The OMD model is interesting per se beyond applications to data quality. It allows for a logic-based and computationally tractable representation of multidimensional data, extending previous multidimensional data models with additional expressive power and functionalities.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {14},
numpages = {36},
keywords = {Datalog±, Ontology-based data access, query answering, weakly-sticky programs}
}

@inproceedings{10.1145/3331453.3360973,
author = {Dong, Wei and Xiao, Litian and Niu, Shengfen and Niu, Jianjun and Wang, Fei},
title = {Application Research of Big Data for Launch Support System at Space Launch Site},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3360973},
doi = {10.1145/3331453.3360973},
abstract = {At the space launch site, the big data of the launch support system comes from the construction of the launch site, the ground service, the comprehensive support process, and launch mission organization and command. The big data is extensive sources, various types, large scale, and rapid growth. The big data application can improve the data processing and management efficiency for the launch support system. Then the application can enhance the support capability of flight mission and success rate. This paper analyzes the existing data application of launch support system. The challenges and requirements of big data application are studied by the construction of intelligent launch site. The application pattern and target are put forward from four aspects of launch mission organization and command, mission application, comprehensive support, and information security. The classification of big data is proposed for a launch support system. The architecture of big data application system is designed, which meets the application pattern and target. It lays a foundation for the future big data project at the launch site.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {23},
numpages = {6},
keywords = {Space launch site, Big data, Launch support system, Application research},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/3268891.3268892,
author = {Liu, Zhao-ge and Li, Xiang-yang},
title = {Full View Scenario Model of Big Data Governance in Community Safety Service},
year = {2018},
isbn = {9781450365024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268891.3268892},
doi = {10.1145/3268891.3268892},
abstract = {In community safety service, big data governance is the prime mode to achieve community safety big data sharing and service value increasing. Although existing researches have preliminarily established the general big data governance framework, identification and governance of specific sharing problems lack comprehensive and systematic scenario description. Applying software engineering method, this paper proposes a kind of scenario expression model of big data governance in community safety service. Considering the common features of big data governance scenarios, construct the meta-scenario model of big data governance in community safety services. Considering the scenario expression difference under different levels, scales and particle sizes, construct the full view scenario model of big data governance in community safety services by meta-models nesting to complete the scenario expression under different applying situation. Finally, a use case is proposed to verify the rationality and effectiveness of the scenario expression models.},
booktitle = {Proceedings of the 8th International Conference on Information Communication and Management},
pages = {44–49},
numpages = {6},
keywords = {scenario model, community safety, safety service, big data governance},
location = {Edinburgh, United Kingdom},
series = {ICICM '18}
}

@inproceedings{10.1145/3149572.3149575,
author = {Francisco, Maritza M. C. and Alves-Souza, Solange N. and Campos, Edit G. L. and De Souza, Luiz S.},
title = {Total Data Quality Management and Total Information Quality Management Applied to Costumer Relationship Management},
year = {2017},
isbn = {9781450353373},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149572.3149575},
doi = {10.1145/3149572.3149575},
abstract = {Data quality (DQ) is an important issue for modern organizations, mainly for decision-making based on information, using solutions such as CRM, Business Analytics, and Big Data. In order to obtain quality data, it is necessary to implement methods, processes, and specific techniques that handle information as a product, with well established, controlled, and managed production processes. The literature provides several types of quality data management methodologies that treat structured data, and few treating semi- and non-structured data. Choosing the methodology to be adopted is one the major issues faced by organizations, when challenged to treat the data quality in a systematic manner. This paper makes a comparative analysis between TDQM -- Total Data Quality Management and TIQM -- Total Information Quality Management approaches, focusing on data quality problems in the context of a CRM -- Costumer Relationship Management application. Such analysis identifies the strengths and weaknesses of each methodology and suggests the most suitable for the CRM scenario.},
booktitle = {Proceedings of the 9th International Conference on Information Management and Engineering},
pages = {40–45},
numpages = {6},
keywords = {data quality methodology, Data quality, data quality management, data quality problems, data quality dimensions},
location = {Barcelona, Spain},
series = {ICIME 2017}
}

@inproceedings{10.1109/CCGrid.2016.63,
author = {Ordonez, Carlos and Garc\'{\i}a-Garc\'{\i}a, Javier},
title = {Managing Big Data Analytics Workflows with a Database System},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.63},
doi = {10.1109/CCGrid.2016.63},
abstract = {A big data analytics workflow is long and complex, with many programs, tools and scripts interacting together. In general, in modern organizations there is a significant amount of big data analytics processing performed outside a database system, which creates many issues to manage and process big data analytics workflows. In general, data preprocessing is the most time-consuming task in a big data analytics workflow. In this work, we defend the idea of preprocessing, computing models and scoring data sets inside a database system. In addition, we discuss recommendations and experiences to improve big data analytics workflows by pushing data preprocessing (i.e. data cleaning, aggregation and column transformation) into a database system. We present a discussion of practical issues and common solutions when transforming and preparing data sets to improve big data analytics workflows. As a case study validation, based on experience from real-life big data analytics projects, we compare pros and cons between running big data analytics workflows inside and outside the database system. We highlight which tasks in a big data analytics workflow are easier to manage and faster when processed by the database system, compared to external processing.},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {649–655},
numpages = {7},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@inproceedings{10.1145/3543106.3543115,
author = {Li, Kai},
title = {A Study on the Economic Model of Volume in the Age of Big Data},
year = {2022},
isbn = {9781450397162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543106.3543115},
doi = {10.1145/3543106.3543115},
abstract = {With the rapid development of China's economy, the degree of integration of economics and management is deepening. Based on the statistical analysis method of big data, the trend and law of economic development can be obtained. Big data statistical methods are widely used in the field of economics and improve work efficiency. Effective statistical analysis of data can not only reflect the operation of the product in time, but also reflect the market demand for the product. Therefore, this paper studies the role of big data statistical analysis methods in the field of economic management. This paper first classifies and sorts out the representative quantitative research methods and models in the era of big data, and then based on the BP neural network model and combines 36 indicator data to make multivariate forecasts for China's consumer price index (CPI). The research results show that the prediction results of the BP neural network are good.},
booktitle = {Proceedings of the 2022 International Conference on E-Business and Mobile Commerce},
pages = {54–58},
numpages = {5},
keywords = {Quantitative economy, Big data, CPI, BP neural network},
location = {Seoul, Republic of Korea},
series = {ICEMC '22}
}

@inproceedings{10.1145/2723372.2742784,
author = {G.C., Paul Suganthan and Sun, Chong and K., Krishna Gayatri and Zhang, Haojun and Yang, Frank and Rampalli, Narasimhan and Prasad, Shishir and Arcaute, Esteban and Krishnan, Ganesh and Deep, Rohit and Raghavendra, Vijay and Doan, AnHai},
title = {Why Big Data Industrial Systems Need Rules and What We Can Do About It},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742784},
doi = {10.1145/2723372.2742784},
abstract = {Big Data industrial systems that address problems such as classification, information extraction, and entity matching very commonly use hand-crafted rules. Today, however, little is understood about the usage of such rules. In this paper we explore this issue. We discuss how these systems differ from those considered in academia. We describe default solutions, their limitations, and reasons for using rules. We show examples of extensive rule usage in industry. Contrary to popular perceptions, we show that there is a rich set of research challenges in rule generation, evaluation, execution, optimization, and maintenance. We discuss ongoing work at WalmartLabs and UW-Madison that illustrate these challenges. Our main conclusions are (1) using rules (together with techniques such as learning and crowdsourcing) is fundamental to building semantics-intensive Big Data systems, and (2) it is increasingly critical to address rule management, given the tens of thousands of rules industrial systems often manage today in an ad-hoc fashion.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {265–276},
numpages = {12},
keywords = {rule management, classification, big data},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/2661829.2661837,
author = {Wang, Hongzhi and Li, Mingda and Bu, Yingyi and Li, Jianzhong and Gao, Hong and Zhang, Jiacheng},
title = {Cleanix: A Big Data Cleaning Parfait},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661837},
doi = {10.1145/2661829.2661837},
abstract = {In this demo, we present Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate - the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. We demonstrate that Cleanix is a practical tool that supports effective and efficient data cleaning at the large scale.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {2024–2026},
numpages = {3},
keywords = {big data, data cleaning, data quality},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/3510457.3513034,
author = {Zhu, Feng and Xu, Lijie and Ma, Gang and Ji, Shuping and Wang, Jie and Wang, Gang and Zhang, Hongyi and Wan, Kun and Wang, Mingming and Zhang, Xingchao and Wang, Yuming and Li, Jingpin},
title = {An Empirical Study on Quality Issues of EBay's Big Data SQL Analytics Platform},
year = {2022},
isbn = {9781450392266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510457.3513034},
doi = {10.1145/3510457.3513034},
abstract = {Big data SQL analytics platform has evolved as the key infrastructure for business data analysis. Compared with traditional costly commercial RDBMS, scalable solutions with open-source projects, such as SQL-on-Hadoop, are more popular and attractive to enterprises. In eBay, we build Carmel, a company-wide interactive SQL analytics platform based on Apache Spark. Carmel has been serving thousands of customers from hundreds of teams globally for more than 3 years. Meanwhile, despite the popularity of open-source based big data SQL analytics platforms, few empirical studies on service quality issues (e.g., job failure) were carried out for them. However, a deep understanding of service quality issues and taking right mitigation are significant to the ease of manual maintenance efforts. To fill this gap, we conduct a comprehensive empirical study on 1,884 real-word service quality issues from Carmel. We summarize the common symptoms and identify the root causes with typical cases. Stakeholders including system developers, researchers, and platform maintainers can benefit from our findings and implications. Furthermore, we also present lessons learned from critical cases in our daily practice, as well as insights to motivate automatic tool support and future research directions.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice},
pages = {33–42},
numpages = {10},
keywords = {SQL on hadoop, empirical study, big data, open source},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEIP '22}
}

@article{10.1145/3297720,
author = {M\"{u}ller, Daniel and Jain, Pratiksha and Te, Yieh-Funk},
title = {Augmenting Data Quality through High-Precision Gender Categorization},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3297720},
doi = {10.1145/3297720},
abstract = {Mappings of first name to gender have been widely recognized as a critical tool for the completion, study, and validation of data records in a range of areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 4 million people, which was provided by a car insurance company. Then, we study how naming conventions have changed over time and how they differ by nationality. Next, we build a probabilistic first-name-to-gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in two-label and three-label settings and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies’ outcomes and find that our mapping produces high-precision results. We validate that the additional information of nationality and year of birth improve the precision scores of name-to-gender mappings. Therefore, the proposed approach constitutes an efficient process for improving the data quality of organizations’ records, if the gender attribute is missing or unreliable.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {8},
numpages = {18},
keywords = {patenting, record completion, Data quality improvement, gender name mapping}
}

@article{10.1145/3362121,
author = {Firmani, Donatella and Tanca, Letizia and Torlone, Riccardo},
title = {Ethical Dimensions for Data Quality},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3362121},
doi = {10.1145/3362121},
journal = {J. Data and Information Quality},
month = {dec},
articleno = {2},
numpages = {5},
keywords = {source selection, Data integration, knowledge extraction}
}

@inproceedings{10.1145/3341069.3341086,
author = {Pengxi, Li},
title = {The Construction Study of College Informationization Teaching Service System under the Background of Big Data},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3341086},
doi = {10.1145/3341069.3341086},
abstract = {Under the background of the rapid development of big data technology, the construction of college informationization teaching service system is the basis of the informationization teaching in colleges and universities. That is very important for the success of the informatization in Universities what is service realization model, business logic, architecture and platform conform to the whole development strategy of universities. Information management organization supports the planning, implementation, operation, maintenance and management of business information system. This paper analyzes the reform mode of college education information service system supported by big data technology. Based on the analysis of the reform mode of college informationization teaching service system supported by big data technology, this paper puts forward the design idea of post system based on big data. At the same time, with the case of "big data assisted employment", the post design and adjustment were carried out. The results show that big data assisted employment has greatly improved the efficiency and quality of the school's employment department, providing students with better employment security. Finally, the problems that need to be solved in the informatization teaching service are sorted out.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {185–189},
numpages = {5},
keywords = {Service system, Big data, Teaching informatization},
location = {Guangzhou, China},
series = {HPCCT '19}
}

@inproceedings{10.1145/3399205.3399228,
author = {Moumen, Aniss},
title = {Adoption of Big Data, Cloud Computing &amp; IoT in Morocco Perception of Public Administrations Collaborators},
year = {2020},
isbn = {9781450375788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399205.3399228},
doi = {10.1145/3399205.3399228},
abstract = {The integration of new technologies such as big data, cloud computing, and the Internet of Things (IoT), constitute a new challenge for existing information systems of local administrations.In this work, we present the results of interviews conducted with the public administrations in the South-eastern region of Morocco, and as conclusion we expose two proposed models induced by this study.},
booktitle = {Proceedings of the 4th Edition of International Conference on Geo-IT and Water Resources 2020, Geo-IT and Water Resources 2020},
articleno = {21},
numpages = {4},
keywords = {Big data, Information System, IoT, Cloud Computing},
location = {Al-Hoceima, Morocco},
series = {GEOIT4W-2020}
}

@inproceedings{10.1145/3356998.3365776,
author = {Zhang, Mingke and Guo, Danhuai and Hu, Jinyong and Jin, Wei},
title = {Risk Prediction and Assessment of Foodborne Disease Based on Big Data},
year = {2020},
isbn = {9781450369657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356998.3365776},
doi = {10.1145/3356998.3365776},
abstract = {In recent years, the outbreak of foodborne diseases has been on an upward trend clearly. It is of great significance for us to predict the outbreak of foodborne diseases accurately and conduct quantitative risk assessment timely. Traditional prediction methods based on a single data source have drawbacks such as complex prediction processes and inaccurate prediction results. In this article, we figure out the scientific issues of how to improve the temporal and spatial accuracy of foodborne disease outbreak risk prediction in Beijing. Firstly, we analyze the different foodborne disease risk factors caused by the spread of water pollution in Beijing, and study the methods of collecting and preprocessing multi-source data. Then, through the comparison of different regression models and parameters tuning, we propose the use of Gradient Boosting Regression (GBR) model as a multi-source data fusion model to predict the outbreak of foodborne disease. Finally, we use the risk map to detect and predict foodborne disease outbreak in different business districts of Beijing based on visualization techniques, aiming to provide prevention and control assessment for decision-makers quickly and precisely.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management},
articleno = {8},
numpages = {6},
keywords = {machine learning, risk assessment, big data, foodborne disease},
location = {Chicago, Illinois},
series = {EM-GIS '19}
}

@article{10.14778/3352063.3352128,
author = {Chen, Zhimin and Wang, Yue and Narasayya, Vivek and Chaudhuri, Surajit},
title = {Customizable and Scalable Fuzzy Join for Big Data},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352128},
doi = {10.14778/3352063.3352128},
abstract = {Fuzzy join is an important primitive for data cleaning. The ability to customize fuzzy join is crucial to allow applications to address domain-specific data quality issues such as synonyms and abbreviations. While efficient indexing techniques exist for single-node implementations of customizable fuzzy join, the state-of-the-art scale-out techniques do not support customization, and exhibit poor performance and scalability characteristics. We describe the design of a scale-out fuzzy join operator that supports customization. We use a locality-sensitive-hashing (LSH) based signature scheme, and introduce optimizations that result in significant speed up with negligible impact on recall. We evaluate our implementation on the Azure Databricks version of Spark using several real-world and synthetic data sets. We observe speedups exceeding 50X compared to the best-known prior scale-out technique, and close to linear scalability with data size and number of nodes.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2106–2117},
numpages = {12}
}

@inproceedings{10.1145/3483816.3483836,
author = {Febiri, Frank and Yihum Amare, Meseret and Hub, Miloslav},
title = {Fusion from Big Data to Smart Data to Enhance Quality of Information Systems},
year = {2022},
isbn = {9781450390545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483816.3483836},
doi = {10.1145/3483816.3483836},
abstract = {The term “smartness” in the data framework indicates relevancy based on the intended purpose of data. The Internet of Things (IoT) and advancements in technology have resulted in an ever-increasing pool of data available to all institutions to derive meaning and make sound decisions from them. The research presented in this paper explored the role smart data play in information systems quality through a qualitative study of how using the large pool of data (big data) and fusing it to smart data organizations can make sound and intelligent decisions using the available techniques. We use an existing architecture for a public institution to analyze how data ingestion can be achieved with minimum challenges. The findings suggest that even though there is a large pool of data for most organizations, it is becoming more challenging to use this data to make organizational sense due to the challenges posed by such data. The realization of smart data and its benefits in information systems helps improve the quality of information systems, reducing cost and promoting the smartness agenda of today's organization.},
booktitle = {2021 8th International Conference on Management of E-Commerce and e-Government},
pages = {112–117},
numpages = {6},
keywords = {Information systems, Big Data, Smart data, Quality measures},
location = {Jeju, Republic of Korea},
series = {ICMECG 2021}
}

@inproceedings{10.1145/3281375.3281386,
author = {Rinaldi, Antonio M. and Russo, Cristiano},
title = {A Semantic-Based Model to Represent Multimedia Big Data},
year = {2018},
isbn = {9781450356220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281375.3281386},
doi = {10.1145/3281375.3281386},
abstract = {The use of formal representation is a key task in the era of big data. In the context of multimedia big data this issue is stressed due to the intrinsic complexity nature of this kind of data. Moreover, the relations among objects should be clearly expressed and formalized to give the right meaning of data correlation. For this reason the design of formal models to represent and manage information is a necessary task to implement intelligent information systems. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of big data applications. Using these models the visualization of data and information become an intrinsic and strategic task for the analysis and exploration of multimedia BigData. In this paper we propose the use of a semantic approach to formalize the model structure of multimedia BigData. In addition, the recognition of multimodal features to represent concepts and linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. The proposed model has been implemented in a NoSQL graph database populated from different knowledge sources and a visualization of this very large knowledge base has been presented and discussed as a case study.},
booktitle = {Proceedings of the 10th International Conference on Management of Digital EcoSystems},
pages = {31–38},
numpages = {8},
keywords = {semantics, multimedia ontologies, semantic bigdata},
location = {Tokyo, Japan},
series = {MEDES '18}
}

@inproceedings{10.1145/3393527.3393532,
author = {Shi, Bin and YabinXu},
title = {Research on Copyright Protection Method of Big Data Based on Nash Equilibrium and Constraint Optimization},
year = {2020},
isbn = {9781450375344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393527.3393532},
doi = {10.1145/3393527.3393532},
abstract = {Data watermarking technology is an effective means to protect the copyright of big data. In order to embed robust and highly available data watermarks, firstly, based on the game theory, a Nash equilibrium model between watermark robustness and data quality is established to solve the optimal number of data partitioning. Then, the mapping relationship between data partitioning and watermark bit is established by using secure hash algorithm. Finally, under the constraint of data usability, the improved particle swarm optimization algorithm is used to calculate the optimal solution of data change for each data partitioning, and then the data is changed accordingly to complete the embedding of watermark bit. In order to verify the copyright ownership of big data, this paper also gives the corresponding watermark extraction method. Watermark extraction is the inverse process of watermark embedding. First, traverse all partitions and extract the possible embedded bit values in each data partitioning. Then, the actual embedded watermark bit is finally determined by majority voting strategies. The experimental results show that our proposed method can not only detect watermarks under different attack conditions, ensure the robustness of big data watermarks, but also achieve better data quality, and the comprehensive effect of data watermarks is better than the existing methods.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
pages = {21–25},
numpages = {5},
keywords = {Copyright protection, Nash equilibrium, Majority voting strategy, Particle swarm optimization algorithm, Big data, Constrained optimization, Data watermarking},
location = {Hefei, China},
series = {ACM TURC'20}
}

@inproceedings{10.1145/3209582.3209599,
author = {Gong, Xiaowen and Shroff, Ness},
title = {Incentivizing Truthful Data Quality for Quality-Aware Mobile Data Crowdsourcing},
year = {2018},
isbn = {9781450357708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209582.3209599},
doi = {10.1145/3209582.3209599},
abstract = {Mobile data crowdsourcing has found a broad range of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the "wisdom" of a potentially large crowd of "workers" (i.e., mobile users). A key metric of crowdsourcing is data accuracy, which relies on the quality of the participating workers' data (e.g., the probability that the data is equal to the ground truth). However, the data quality of a worker can be its own private information (which the worker learns, e.g., based on its location) that it may have incentive to misreport, which can in turn mislead the crowdsourcing requester about the accuracy of the data. This issue is further complicated by the fact that the worker can also manipulate its effort made in the crowdsourcing task and the data reported to the requester, which can also mislead the requester. In this paper, we devise truthful crowdsourcing mechanisms for Quality, Effort, and Data Elicitation (QEDE), which incentivize strategic workers to truthfully report their private worker quality and data to the requester, and make truthful effort as desired by the requester. The truthful design of the QEDE mechanisms overcomes the lack of ground truth and the coupling in the joint elicitation of worker quality, effort, and data. Under the QEDE mechanisms, we characterize the socially optimal and the requester's optimal task assignments, and analyze their performance. We show that the requester's optimal assignment is determined by the largest "virtual valuation" rather than the highest quality among workers, which depends on the worker's quality and the quality's distribution. We evaluate the QEDE mechanisms using simulations which demonstrate the truthfulness of the mechanisms and the performance of the optimal task assignments.},
booktitle = {Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {161–170},
numpages = {10},
keywords = {Mobile data crowdsourcing, incentive mechanism, data quality},
location = {Los Angeles, CA, USA},
series = {Mobihoc '18}
}

@inproceedings{10.1145/3401895.3402092,
author = {Silva, Rodrigo Dantas da and de Ara\'{u}jo, Jean Jar Pereira and de Paiva, \'{A}lvaro Ferreira Pires and de Medeiros Valentim, Ricardo Alexsandro and Coutinho, Karilany Dantas and de Paiva, Jailton Carlos and Roussanaly, Azim and Boyer, Anne},
title = {A Big Data Architecture to a Multiple Purpose in Healthcare Surveillance: The Brazilian Syphilis Case},
year = {2021},
isbn = {9781450377119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401895.3402092},
doi = {10.1145/3401895.3402092},
abstract = {For many decades society did need to monitor and assess the standard of living of the population. In the 1950s, the United Nations (UN) saw this need and proposed 12 areas that should be evaluated, the first of which is listed under "Health and Demography", which focuses on what is expressed as the level of a population's health. Decades have passed and great results have been gained from similar initiatives such as reducing mortality from infectious diseases and even eradicating some others. In the age of the digital society, needs have grown. Monitoring demands that once perished from data to become concrete now suffer from the opposite effect, the excess of data from everywhere. Healthcare systems around the world use many different information systems, collecting and generating hundreds of data at unimaginable speed. We are billions of people on the planet and most of us are connected to the virtual world, sharing information, experiences and events with some kind of cloud. In this information age, the ability to aggregate and process this data is a major factor in raising public health to a new level. The development of tools capable of analyzing a large volume of data in seconds and producing knowledge for targeted decision making can help in the fight against specific diseases, in the process of continuing education of professionals, in the formation of new professionals, in the elaboration of new policies. with the specific locoregional look, in the analysis of hidden trends in front of so much information faced in everyday life and other possibilities. The present work proposes an architecture capable of storing and manipulating seeking to standardize the variables in order to allow to correlate this large amount of data in a systematic way, providing to several services and researchers the possibility of consuming health, social, economic and educational data for the promotion of public health.},
booktitle = {Proceedings of the 10th Euro-American Conference on Telematics and Information Systems},
articleno = {58},
numpages = {6},
keywords = {big data, epidemiology, healthcare surveillance, syphilis},
location = {Aveiro, Portugal},
series = {EATIS '20}
}

@article{10.1145/3418896,
author = {Christophides, Vassilis and Efthymiou, Vasilis and Palpanas, Themis and Papadakis, George and Stefanidis, Kostas},
title = {An Overview of End-to-End Entity Resolution for Big Data},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3418896},
doi = {10.1145/3418896},
abstract = {One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows&nbsp;for&nbsp;Big Data, critically review the pros and cons of existing methods, and conclude with the main open research&nbsp;directions.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {127},
numpages = {42},
keywords = {deep learning, crowdsourcing, strongly and nearly similar entities, Entity blocking and matching, block processing, batch and incremental entity resolution workflows}
}

@article{10.1145/3469890,
author = {Lv, Zhihan and Lou, Ranran and Feng, Hailin and Chen, Dongliang and Lv, Haibin},
title = {Novel Machine Learning for Big Data Analytics in Intelligent Support Information Management Systems},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3469890},
doi = {10.1145/3469890},
abstract = {Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {oct},
articleno = {7},
numpages = {21},
keywords = {lightGBM, intelligent support information system, accuracy rate, big data analysis, Machine learning}
}

@inproceedings{10.1145/3282278.3282282,
author = {Casado-Vara, Roberto and de la Prieta, Fernando and Prieto, Javier and Corchado, Juan M.},
title = {Blockchain Framework for IoT Data Quality via Edge Computing},
year = {2018},
isbn = {9781450360500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282278.3282282},
doi = {10.1145/3282278.3282282},
abstract = {Smart home presents a challenge in control and monitoring of its wireless sensors networks (WSN) and the internet of things (IoT) devices which form it. The current IoT architectures are centralized, complex, with poor security in its communications and with upstream communication channels mainly. As a result, there are problems with data reliability. These problems include data missing, malicious data inserted, communications network overload, and overload of computing power at the central node. In this paper a new architecture is presented. This architecture based in blockchain introduce the edge computing layer and a new algorithm to improve data quality and false data detection.},
booktitle = {Proceedings of the 1st Workshop on Blockchain-Enabled Networked Sensor Systems},
pages = {19–24},
numpages = {6},
keywords = {IoT, WSN, edge computing, Blockchain, data quality false data detection, non linear control},
location = {Shenzhen, China},
series = {BlockSys'18}
}

@inproceedings{10.1145/2663876.2663885,
author = {Freudiger, Julien and Rane, Shantanu and Brito, Alejandro E. and Uzun, Ersin},
title = {Privacy Preserving Data Quality Assessment for High-Fidelity Data Sharing},
year = {2014},
isbn = {9781450331517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663876.2663885},
doi = {10.1145/2663876.2663885},
abstract = {In a data-driven economy that struggles to cope with the volume and diversity of information, data quality assessment has become a necessary precursor to data analytics. Real-world data often contains inconsistencies, conflicts and errors. Such dirty data increases processing costs and has a negative impact on analytics. Assessing the quality of a dataset is especially important when a party is considering acquisition of data held by an untrusted entity. In this scenario, it is necessary to consider privacy risks of the stakeholders.This paper examines challenges in privacy-preserving data quality assessment. A two-party scenario is considered, consisting of a client that wishes to test data quality and a server that holds the dataset. Privacy-preserving protocols are presented for testing important data quality metrics: completeness, consistency, uniqueness, timeliness and validity. For semi-honest parties, the protocols ensure that the client does not discover any information about the data other than the value of the quality metric. The server does not discover the parameters of the client's query, the specific attributes being tested and the computed value of the data quality metric. The proposed protocols employ additively homomorphic encryption in conjunction with condensed data representations such as counting hash tables and histograms, serving as efficient alternatives to solutions based on private set intersection.},
booktitle = {Proceedings of the 2014 ACM Workshop on Information Sharing &amp; Collaborative Security},
pages = {21–29},
numpages = {9},
keywords = {privacy and confidentiality, data quality assessment, cryptographic protocols},
location = {Scottsdale, Arizona, USA},
series = {WISCS '14}
}

@inproceedings{10.1145/3301551.3301610,
author = {Li, Yonghong and Zhang, Shuwen and Jia, Nan},
title = {Research on the Transformation and Upgrading Path and Selection of Traditional Industries from the Perspective of Big Data},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301610},
doi = {10.1145/3301551.3301610},
abstract = {With the emergence of a new generation of information technology, big data has become an important driving force for current social development. Digitalization has become the main direction of the transformation and upgrading of traditional industries. As the product of current informatization, big data includes data quantity, data quality and data analysis ability. It is used as two different ways to interpret the value creation of big data, making it clear that it can promote the transformation and upgrading of traditional industries through value creation. Then, it puts forward the traditional industrial transformation and upgrading path from the perspective of big data, namely the linear path of "traditional industry + digital" and the transitional non-linear "digital + traditional industry". Its path selection will be analyzed by combining external and internal factors.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {54–59},
numpages = {6},
keywords = {Traditional industries, The path, Big data, Transformation and upgrading},
location = {Hong Kong, Hong Kong},
series = {ICIT 2018}
}

@article{10.1145/3449052,
author = {Aljawarneh, Shadi and Lara, Juan A.},
title = {Editorial: Special Issue on Quality Assessment and Management in Big Data—Part I},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3449052},
doi = {10.1145/3449052},
journal = {J. Data and Information Quality},
month = {may},
articleno = {6},
numpages = {3},
keywords = {quality management, Quality assessment, big data}
}

@article{10.1145/3449056,
author = {Aljawarneh, Shadi and Lara, Juan A.},
title = {Editorial: Special Issue on Quality Assessment and Management in Big Data—Part II},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3449056},
doi = {10.1145/3449056},
journal = {J. Data and Information Quality},
month = {may},
articleno = {13},
numpages = {3},
keywords = {quality management, Quality assessment, big data}
}

@inproceedings{10.1145/3539781.3539795,
author = {Satheesan, Sandeep Puthanveetil and Bhavya and Davies, Adam and Craig, Alan B. and Zhang, Yu and Zhai, ChengXiang},
title = {Toward a Big Data Analysis System for Historical Newspaper Collections Research},
year = {2022},
isbn = {9781450394109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539781.3539795},
doi = {10.1145/3539781.3539795},
abstract = {The availability and generation of digitized newspaper collections have provided researchers in several domains with a powerful tool to advance their research. More specifically, digitized historical newspapers give us a magnifying glass into the past. In this paper, we propose a scalable and customizable big data analysis system that enables researchers to study complex questions about our society as depicted in news media for the past few centuries by applying cutting-edge text analysis tools to large historical newspaper collections. We discuss our experience with building a preliminary version of such a system, including how we have addressed the following challenges: processing millions of digitized newspaper pages from various publications worldwide, which amount to hundreds of terabytes of data; applying article segmentation and Optical Character Recognition (OCR) to historical newspapers, which vary between and within publications over time; retrieving relevant information to answer research questions from such data collections by applying human-in-the-loop machine learning; and enabling users to analyze topic evolution and semantic dynamics with multiple compatible analysis operators. We also present some preliminary results of using the proposed system to study the social construction of juvenile delinquency in the United States and discuss important remaining challenges to be tackled in the future.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {12},
numpages = {11},
keywords = {text analysis, image analysis, natural language processing, big data analysis system, juvenile delinquency, social science research, social construction, data visualization, historical newspapers, newspaper article segmentation, information retrieval},
location = {Basel, Switzerland},
series = {PASC '22}
}

@inproceedings{10.1145/3495018.3495345,
author = {Chen, Xin and Yang, Lirong and Sun, Yanzhi},
title = {Human Resource Information System Performance Test under Big Data Technology},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495345},
doi = {10.1145/3495018.3495345},
abstract = {The era of big data has quietly arrived, which is a revolution that determines the development and future destiny of enterprises. Any enterprises that are not ready for this revolution will be eliminated by the era. This paper mainly studies the construction, analysis and management of human resource system in the era of big data. Based on the actual needs, this paper analyzes the business process and functional requirements of human resource management, completes the system architecture design, function module design, database design, realizes the system function module, and completes the test of the system function. The functional modules realized in this paper include: core personnel management, salary management and comprehensive inquiry. The human resource information system designed in this paper ensures the scientific nature, security, availability and portability of the system, meets the demand of data sharing, and plays a positive role in the whole human resource management cycle.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1107–1111},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3444370.3444614,
author = {Zhang, Yong},
title = {Human Resource Data Quality Management Based on Multiple Regression Analysis},
year = {2021},
isbn = {9781450387828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444370.3444614},
doi = {10.1145/3444370.3444614},
abstract = {The essence of human resource management informatization is data. Firstly, human information is transformed into data, and then the data can be used. This requires that the comprehensive, complete, timely and effective human resource data is entered into the system, and then the data is analyzed by using the human resource management system to provide decision support for the management. Due to the complexity of the internal law of objective things and the limitation of people's cognition, it is impossible to analyze the internal causality of the actual object and establish a mathematical model in accordance with the mechanism law. Therefore, when some mathematical models cannot be established by mechanism analysis, we usually adopt the method of collecting a large amount of data, and establish the model based on the statistical analysis of the data. Among them, the most widely used random model is statistical regression model.},
booktitle = {Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies},
pages = {465–470},
numpages = {6},
keywords = {Multiple regression analysis, human resources, data quality},
location = {Guangzhou, China},
series = {CIAT 2020}
}

@inproceedings{10.1145/3286606.3286788,
author = {Bibri, Simon Elias and Krogstie, John},
title = {The Big Data Deluge for Transforming the Knowledge of Smart Sustainable Cities: A Data Mining Framework for Urban Analytics},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286788},
doi = {10.1145/3286606.3286788},
abstract = {There has recently been much enthusiasm about the possibilities created by the big data deluge to better understand, monitor, analyze, and plan modern cities to improve their contribution to the goals of sustainable development. Indeed, much of our knowledge of urban sustainability has been gleaned from studies that are characterized by data scarcity. Therefore, this paper endeavors to develop a systematic framework for urban sustainability analytics based on a cross-industry standard process for data mining. The intention is to enable well-informed decision-making and enhanced insights in relation to diverse urban domains. We argue that there is tremendous potential to transform and advance the knowledge of smart sustainable cities through the creation of a big data deluge that seeks to provide much more sophisticated, wider-scale, finer-grained, real-time understanding, and control of various aspects of urbanity in the undoubtedly upcoming Exabyte Age.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {11},
numpages = {10},
keywords = {Smart sustainable cities, big data analytics, data mining},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.5555/3374138.3374194,
author = {Bawatna, Mohammed and Green, Bertram and Kovalev, Sergey and Deinert, Jan-Christoph and Knodel, Oliver and Spallek, Rainer G.},
title = {Research and Implementation of Efficient Parallel Processing of Big Data at TELBE User Facility},
year = {2019},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {In recent years, improvements in high-speed Analog-to-Digital Converters (ADC) and sensor technology has encouraged researchers to improve the performance of Data Acquisition (DAQ) systems for scientific experiments which require high speed and continuous data measurements --- in particular, measuring the electronic and magnetic properties of materials using pump-probe experiments at high repetition rates. Experiments at TELBE are capable of acquiring almost 100 Gigabytes of raw data every ten minutes. The DAQ system used at TELBE partitions the raw data into various subdirectories for further parallel processing utilizing the multicore structure of modern CPUs.Furthermore, several other types of processors that accelerate data processing like the GPU and FPGA have emerged to solve the challenges of processing the massive amount of raw data. However, the memory and network bottlenecks become a significant challenge in big data processing, and new scalable programming techniques are needed to solve these challenges. In this contribution, we will outline the design and implementation of our practical software approach for efficient parallel processing of our large data sets at the TELBE user facility.},
booktitle = {Proceedings of the 2019 Summer Simulation Conference},
articleno = {56},
numpages = {6},
keywords = {big data, data analytics, data acquisition systems, signal processing, data processing pipeline},
location = {Berlin, Germany},
series = {SummerSim '19}
}

@inbook{10.1145/3310205.3310211,
title = {Data Quality Rule Definition and Discovery},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310211},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3209415.3209427,
author = {Androutsopoulou, Aggeliki and Charalabidis, Yannis},
title = {A Framework for Evidence Based Policy Making Combining Big Data, Dynamic Modelling and Machine Intelligence},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209427},
doi = {10.1145/3209415.3209427},
abstract = {Governments and policy makers are striving to respond to contemporary socio-economic challenges, however, often neglecting the human factor and the multidimensionality of policy implications. In this chapter, a framework for evidence based policy making is proposed, which integrates the usage of open big data coming from a multiplicity of sources with policy simulations. It encompasses the application of dynamic modelling methodologies and data mining techniques to extract knowledge from two types of data. On the one hand, objective data such as governmental and statistical data, are used to capture the interlinked policy domains and their underlying casual mechanisms. On the other hand, behavioural patterns and citizens' opinions are extracted from Web 2.0 sources, social media posts, polls and statistical surveys. To combine this multimodal information, our approach suggests a modelling methodology that bases on big data acquisition and processing for the identification of significant factors and counterintuitive interrelations between them, which can be applied in any policy domain. Then, to allow the practical application of the framework an ICT architecture is designed, with the aim to overcome challenges related with big data management and processing. Finally, validation of the approach for driving policy design and implementation in the future in diverse policy domains, is suggested.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {575–583},
numpages = {9},
keywords = {dynamic simulation, evidence based policy making, impact assessment, behavioural patterns, policy Modelling, Big data, data mining},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@article{10.1145/2481528.2481537,
author = {Stonebraker, Michael and Madden, Sam and Dubey, Pradeep},
title = {Intel "Big Data" Science and Technology Center Vision and Execution Plan},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/2481528.2481537},
doi = {10.1145/2481528.2481537},
abstract = {Intel has moved to a collaboration model with universities consisting of "Science and Technology Centers" (ISTCs). These are located at a "hub" university with participation from other universities, contain embedded Intel personnel, and are focused on some research theme. Intel held a national competition for a 5th Science and Technology center in 2012 and selected a proposal from M.I.T. with a theme of "Big Data". This paper presents the big data vision of this technology center and the execution plan for the first few years.},
journal = {SIGMOD Rec.},
month = {may},
pages = {44–49},
numpages = {6}
}

@inproceedings{10.1145/3530050.3532928,
author = {Paasche, Simon and Groppe, Sven},
title = {Enhancing Data Quality and Process Optimization for Smart Manufacturing Lines in Industry 4.0 Scenarios},
year = {2022},
isbn = {9781450393461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530050.3532928},
doi = {10.1145/3530050.3532928},
abstract = {An essential component of today's industry is data, which is generated during manufacturing. The goal of industry 4.0 is efficient collection, processing and analysis of this data. In our work, we address these three tasks and present an extensible system to solve them. To the best of our knowledge, the combination of a consistency checker (CC) for data preparation and a digital twin (DT) for analysis activities represents a novel approach. Consistency checking in combination with a DT leads to increased data quality, which in turn has a positive effect on analyses, like reducing errors to decrease costs, identifying relevant parameters to increase the productivity, and determining the bottleneck of a manufacturing line for enhanced production planning.},
booktitle = {Proceedings of The International Workshop on Big Data in Emergent Distributed Environments},
articleno = {9},
numpages = {7},
keywords = {industry 4.0, consistency checking, digital twin},
location = {Philadelphia, Pennsylvania},
series = {BiDEDE '22}
}

@inproceedings{10.1145/3358331.3358336,
author = {Jia, Dong-Ming and Yuan, Cun-Feng and Guo, Song and Jiang, Zu-Zhen and Xu, Ding and Wang, Da-An},
title = {Application of "Artificial Intelligence and Big Data" in Sports Rehabilitation for Chinese Judicial Administrative Drug Addicts},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358336},
doi = {10.1145/3358331.3358336},
abstract = {Under the background of "Wisdom Drug Rehabilitation", we introduced "Artificial Intelligence and Big Data" into "exercise rehabilitation" work of drug addicts in judicial administrative system. It is a practical innovation of drug treatment in China. This article will elaborate this innovation of the construction and application of "Exercise Rehabilitation" intelligence platform system. This system will improve mental status, alleviate physical and psychological symptoms, ensure safety in places, lighten the burden of professional police officers, make rapid analysis, make accurate decisions and improve the integrity rate of the addict.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {5},
numpages = {5},
keywords = {judicial administrative, big data, Sports rehabilitation, rehabilitation training, artificial intelligence},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@inproceedings{10.1145/3377812.3390811,
author = {Khalajzadeh, Hourieh and Simmons, Andrew and Abdelrazek, Mohamed and Grundy, John and Hosking, John and He, Qiang and Ratnakanthan, Prasanna and Zia, Adil and Law, Meng},
title = {A Practical, Collaborative Approach for Modeling Big Data Analytics Application Requirements},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3390811},
doi = {10.1145/3377812.3390811},
abstract = {Data analytics application development introduces many challenges including: new roles not in traditional software engineering practices - e.g. data scientists and data engineers; use of sophisticated machine learning (ML) model-based approaches; uncertainty inherent in the models; interfacing with models to fulfill software functionalities; deploying models at scale and rapid evolution of business goals and data sources. We describe our Big Data Analytics Modeling Languages (BiDaML) toolset to bring all stakeholders around one tool to specify, model and document big data applications. We report on our experience applying BiDaML to three real-world large-scale applications. Our approach successfully supports complex data analytics application development in industrial settings.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {256–257},
numpages = {2},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/3210752,
author = {Orenga-Rogl\'{a}, Sergio and Chalmeta, Ricardo},
title = {Framework for Implementing a Big Data Ecosystem in Organizations},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3210752},
doi = {10.1145/3210752},
abstract = {Featuring the various dimensions of data management, it guides organizations through implementation fundamentals.},
journal = {Commun. ACM},
month = {dec},
pages = {58–65},
numpages = {8}
}

@inproceedings{10.1145/3102254.3102272,
author = {Weichselbraun, Albert and Kuntschik, Philipp},
title = {Mitigating Linked Data Quality Issues in Knowledge-Intense Information Extraction Methods},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102272},
doi = {10.1145/3102254.3102272},
abstract = {Advances in research areas such as named entity linking and sentiment analysis have triggered the emergence of knowledge-intensive information extraction methods that combine classical information extraction with background knowledge from the Web. Despite data quality concerns, linked data sources such as DBpedia, GeoNames and Wikidata which encode facts in a standardized structured format are particularly attractive for such applications.This paper addresses the problem of data quality by introducing a framework that elaborates on linked data quality issues relevant to different stages of the background knowledge acquisition process, their impact on information extraction performance and applicable mitigation strategies. Applying this framework to named entity linking and data enrichment demonstrates the potential of the introduced mitigation strategies to lessen the impact of different kinds of data quality problems. An industrial use case that aims at the automatic generation of image metadata from image descriptions illustrates the successful deployment of knowledge-intensive information extraction in real-world applications and constraints introduced by data quality concerns.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {17},
numpages = {12},
keywords = {semantic technologies, information extraction, named entity linking, applications, linked data quality, mitigation strategies},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/2737909.2737912,
author = {Fox, Geoffrey C. and Jha, Shantenu and Qiu, Judy and Luckow, Andre},
title = {Towards an Understanding of Facets and Exemplars of Big Data Applications},
year = {2014},
isbn = {9781450330312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737909.2737912},
doi = {10.1145/2737909.2737912},
abstract = {We study many Big Data applications from a variety of research and commercial areas and suggest a set of characteristic features and possible kernel benchmarks that stress those features for data analytics. We draw conclusions for the hardware and software architectures that are suggested by this analysis.},
booktitle = {Proceedings of the 20 Years of Beowulf Workshop on Honor of Thomas Sterling's 65th Birthday},
pages = {7–16},
numpages = {10},
location = {Annapolis, MD, USA},
series = {Beowulf '14}
}

@inproceedings{10.1145/3148055.3148072,
author = {Abdullah, Tariq and Ahmet, Ahmed},
title = {Genomics Analyser: A Big Data Framework for Analysing Genomics Data},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148072},
doi = {10.1145/3148055.3148072},
abstract = {Abstract Genomics data is unstructured and mostly stored on hard disks. It is both technically and culturally residing in big data domain due to the challenges of volume, velocity and variety. Huge volumes of data are generated from diverse sources in different formats and at a high frequency. Appropriate data models are required to accommodate these data formats for analysing and producing required results with a quick response time. Genomics data can be analysed for a variety of purposes. Existing genomics data analysis pipelines are disk I/O intensive and focus on optimizing data processing for individual analysis tasks. Intensive disk I/O operations and focus on optimizing individual analysis tasks are the biggest bottleneck of existing genomics analysis pipelines. Making any updates in genomics data require reading the whole data set again. In this paper, we present a genomics data analysis framework that addresses both the issues of existing genomics analysis pipelines. It reads unstructured genomics data from sources, transforms it in a structured format and stores this data into a NoSQL database. In this way, genomics data can be queried like any other data and an update in the genomics data does not require reading the whole data set. The framework also presents an efficient analysis pipeline for analysing the genomics data for a variety of purposes like genotype clustering, gene expression microarrays, chromosome variations or gene linkage analysis. A case study of genotype clustering is presented to demonstrate and evaluate the effectiveness of the presented framework. Our results show that the framework improves overall performance of the genomics data analysis pipeline by 49% from existing genomics data analysis pipelines. Furthermore, our approach is robust and is able sustain high performance with high system workloads.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {189–197},
numpages = {9},
keywords = {resource management, big data, population scale clustering, in-memory computing, machine learning, data analysis, compute cluster, algorithms},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.1145/3107514.3107518,
author = {Wu, Jinrong and Sinnott, Richard O. and Effendy, Jemie and Gl\"{o}ckner, Stephan and Hu, William and Li, Jiajie},
title = {Usage Patterns and Data Quality: A Case Study of a National Type-1 Diabetes Study},
year = {2017},
isbn = {9781450352246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107514.3107518},
doi = {10.1145/3107514.3107518},
abstract = {The Environmental Determinants of Islet Auto- immunity (ENDIA) project is Australia's largest study into the causes of Type-1 Diabetes (T1D). The ENDIA study is supported by a Cloud-based software platform including a clinical registry comprising extensive longitudinal information on families at risk of having a child that might go on to develop T1D. This registry includes both demographic and clinical information on families and children as well as the environmental factors that might influence the onset of T1D. A multitude of samples are obtained through the study and used to support a diverse portfolio of bioinformatics data analytics. The quality of the data in the registry is essential to the overall success of the project. This paper presents a Cloud-based log-analytics platform that supports the detailed analysis of patterns of usage of the registry by the clinical centres and collaborators involved. We explore the impact that the usage patterns have on the overall data quality. We also consider ways of improving data quality by mothers entering their own data through targeted mobile applications that have been developed for dietary data collection.},
booktitle = {Proceedings of the 1st International Conference on Medical and Health Informatics 2017},
pages = {18–27},
numpages = {10},
keywords = {Type-1 diabetes, Cloud, auditing, log analysis},
location = {Taichung City, Taiwan},
series = {ICMHI '17}
}

@inproceedings{10.1145/3314527.3314537,
author = {Cabanban-Casem, Christianne Lynnette},
title = {Analytical Visualization of Higher Education Institutions' Big Data for Decision Making},
year = {2019},
isbn = {9781450366212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314527.3314537},
doi = {10.1145/3314527.3314537},
abstract = {Education is an important element towards learning and human development, thus, it is the key towards identifying competencies and better productivity for the workforce. As part of the Commission on Higher Education's (CHED) thrust for improving efficiency and effectiveness by simplifying the collection process for all the stakeholders, the developed system will drastically improve the availability of data for making informed decisions and efficient generation of reports.This research outlines opportunities and challenges associated with the implementation and governance of Big Data in higher education through development and implementation of data analytics tool.},
booktitle = {Proceedings of the 2019 Asia Pacific Information Technology Conference},
pages = {61–64},
numpages = {4},
keywords = {Data Science, Knowledge Management, Higher Education Data},
location = {Jeju Island, Republic of Korea},
series = {APIT 2019}
}

@inproceedings{10.1145/3209281.3209300,
author = {Chotvijit, Sarunkorn and Thiarai, Malkiat and Jarvis, Stephen},
title = {Big Data Analytics in Social Care Provision: Spatial and Temporal Evidence from Birmingham},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209300},
doi = {10.1145/3209281.3209300},
abstract = {There is significant national interest in tackling issues surrounding the needs of vulnerable children and adults. At the same time, UK local authorities face severe financial challenges as a result of decreasing financial settlements and increasing demands from growing urban populations. This research employs state-of-the-art data analytics and visualisation techniques to analyse six years of local government social care data for the city of Birmingham, the UK's second most populated city. This analysis identifies: (i) service cost profiles over time; (ii) geographical dimensions to service demand and delivery; (iii) patterns in the provision of services, and (iv) the extent to which data value and data protection interact. The research accesses data held by the local authority to discover patterns and insights that may assist in the understanding of service demand, support decision making and resource management, whilst protecting and safeguarding its most vulnerable citizens. The use of data in this manner could also inform the approach a local authority has to its data, its capture and use, and the potential for supporting data-led management and service improvements.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {5},
numpages = {8},
keywords = {data analytics, local authority, spatio-temporal analysis, service provision, social care, Birmingham},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/2695664.2695753,
author = {Nascimento, Dimas C. and Pires, Carlos Eduardo and Mestre, Demetrio Gomes},
title = {A Data Quality-Aware Cloud Service Based on Metaheuristic and Machine Learning Provisioning Algorithms},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695753},
doi = {10.1145/2695664.2695753},
abstract = {Cloud Computing as a service has become a topic of increasing interest. The outsourcing of duties and infrastructure to external parties became a crucial concept for many business models. In this paper we discuss the design and experimental evaluation of provisioning algorithms, in a Data Quality-aware Service (DQaS) context, that enables dynamic Data Quality Service Level Agreements (DQSLA) management and optimization of cloud resources. The DQaS has been designed to respond effectively to the DQSLA requirements of the service customers, by minimizing SLA penalties and provisioning the cloud infrastructure for the execution of data quality algorithms. An experimental evaluation of the proposed provisioning algorithms, carried out through simulation, has provided very encouraging results that confirm the adequacy of these algorithms in the DQaS context.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1696–1703},
numpages = {8},
keywords = {cloud computing, data quality, machine learning, metaheuristic, provisioning},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1145/3513135,
author = {Santoro, Donatello and Thirumuruganathan, Saravanan and Papotti, Paolo},
title = {Editorial: Special Issue on Deep Learning for Data Quality},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3513135},
doi = {10.1145/3513135},
abstract = {This editorial summarizes the content of the Special Issue on Deep Learning for Data Quality of the Journal of Data and Information Quality (JDIQ).},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {14},
numpages = {3},
keywords = {schema matching, data labeling, Deep learning}
}

@inproceedings{10.1145/3465631.3465664,
author = {Yu, Xiaomu and Yin, Yuelin},
title = {Application Strategies of Medical Big Data in Health Economic Management},
year = {2021},
isbn = {9781450385015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465631.3465664},
doi = {10.1145/3465631.3465664},
abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.},
booktitle = {The Sixth International Conference on Information Management and Technology},
articleno = {33},
numpages = {5},
location = {Jakarta, Indonesia},
series = {ICIMTECH 21}
}

@inproceedings{10.1145/3365871.3365900,
author = {Papst, Franz and Saukh, Olga and R\"{o}mer, Kay and Grandl, Florian and Jakovljevic, Igor and Steininger, Franz and Mayerhofer, Martin and Duda, J\"{u}rgen and Egger-Danner, Christa},
title = {Embracing Opportunities of Livestock Big Data Integration with Privacy Constraints},
year = {2019},
isbn = {9781450372077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365871.3365900},
doi = {10.1145/3365871.3365900},
abstract = {Today's herd management undergoes a major transformation triggered by the penetration of cheap sensor solutions into cattle farms, and the promise of predictive analytics to detect animal health issues and product-related problems before they occur. The latter is particularly important to prevent disease spread, ensure animal health, animal welfare and product quality. Sensor businesses entering the market tend to build their solutions as end-to-end pipelines spanning sensors, proprietary algorithms, cloud services, and mobile apps. Since data privacy is an important issue in this industry, as a result, disconnected data silos, heterogeneity of APIs, and lack of common standards limit the value the sensor technologies could provide for herd management. In the last few years, researchers and communities proposed a number of data integration architectures to enable exchange between streams of sensor data. This paper surveys the existing efforts and outlines the opportunities they fail to address by treating sensor data as a black box. We discuss alternative solutions to the problem based on privacy-preserving collaborative learning, and provide a set of scenarios to show their benefits for both farmers and businesses.},
booktitle = {Proceedings of the 9th International Conference on the Internet of Things},
articleno = {27},
numpages = {4},
keywords = {agriculture, data privacy, privacy-preserving data analysis},
location = {Bilbao, Spain},
series = {IoT 2019}
}

@inproceedings{10.1145/3209914.3234639,
author = {Song, Zhendong},
title = {Application of Big Data and Intelligent Processing Technology in Modern Chinese Multi-Category Words Part of Speech Tagging Corpus},
year = {2018},
isbn = {9781450364218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209914.3234639},
doi = {10.1145/3209914.3234639},
abstract = {The application of modern Chinese multi-category words corpus is very wide. With the development of the Internet, data from the corpus is getting bigger and bigger during collection. The data gradually develops so big that the current relational database is difficult to deal with them. This article analyzes the important role of the big data technology in corpu},
booktitle = {Proceedings of the 2018 International Conference on Information Science and System},
pages = {107–111},
numpages = {5},
keywords = {Multi-category words, Intelligent processing, Tagging, Big data, Corpus},
location = {Jeju, Republic of Korea},
series = {ICISS '18}
}

@article{10.1145/3420038,
author = {Liu, Yu and Wang, Yangtao and Gao, Lianli and Guo, Chan and Xie, Yanzhao and Xiao, Zhili},
title = {Deep Hash-Based Relevance-Aware Data Quality Assessment for Image Dark Data},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2691-1922},
url = {https://doi.org/10.1145/3420038},
doi = {10.1145/3420038},
abstract = {Data mining can hardly solve but always faces a problem that there is little meaningful information within the dataset serving a given requirement. Faced with multiple unknown datasets, to allocate data mining resources to acquire more desired data, it is necessary to establish a data quality assessment framework based on the relevance between the dataset and requirements. This framework can help the user to judge the potential benefits in advance, so as to optimize the resource allocation to those candidates. However, the unstructured data (e.g., image data) often presents dark data states, which makes it tricky for the user to understand the relevance based on content of the dataset in real time. Even if all data have label descriptions, how to measure the relevance between data efficiently under semantic propagation remains an urgent problem. Based on this, we propose a Deep Hash-based Relevance-aware Data Quality Assessment framework, which contains off-line learning and relevance mining parts as well as an on-line assessing part. In the off-line part, we first design a Graph Convolution Network (GCN)-AutoEncoder hash (GAH) algorithm to recognize the data (i.e., lighten the dark data), then construct a graph with restricted Hamming distance, and finally design a Cluster PageRank (CPR) algorithm to calculate the importance score for each node (image) so as to obtain the relevance representation based on semantic propagation. In the on-line part, we first retrieve the importance score by hash codes and then quickly get the assessment conclusion in the importance list. On the one hand, the introduction of GCN and co-occurrence probability in the GAH promotes the perception ability for dark data. On the other hand, the design of CPR utilizes hash collision to reduce the scale of graph and iteration matrix, which greatly decreases the consumption of space and computing resources. We conduct extensive experiments on both single-label and multi-label datasets to assess the relevance between data and requirements as well as test the resources allocation. Experimental results show our framework can gain the most desired data with the same mining resources. Besides, the test results on Tencent1M dataset demonstrate the framework can complete the assessment with a stability for given different requirements.},
journal = {ACM/IMS Trans. Data Sci.},
month = {apr},
articleno = {11},
numpages = {26},
keywords = {CPR, data quality assessment, GAH, relevance, data mining, Resource allocation}
}

@inproceedings{10.1145/3468945.3468964,
author = {Hou, Hanfang and Fu, Qiang and Zhang, Yang},
title = {An Empirical Study on the Classification, Grading, Sharing and Opening of Healthcare Big Data Based on Current Policies and Standards},
year = {2021},
isbn = {9781450390057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468945.3468964},
doi = {10.1145/3468945.3468964},
abstract = {This paper expounds the connotations and features of healthcare big data, as well as the concepts and logical relations for its classification, grading, sharing and opening. It also summarizes overseas policies related thereto and their status quo, and emphatically introduces main policies, laws, regulations and national standards regarding the classification, grading, sharing and opening of healthcare big data in China, as well as practices in three places represented by Shandong Province, Guangdong Province and Fuzhou. Finally, principles and practical suggestions for the classification, grading, sharing and opening of healthcare big data were proposed based on current polices, regulations and standards},
booktitle = {2021 3rd International Conference on Intelligent Medicine and Image Processing},
pages = {116–121},
numpages = {6},
keywords = {Healthcare big data, Grading, Opening, Sharing, Classification},
location = {Tianjin, China},
series = {IMIP '21}
}

@inproceedings{10.1145/3437120.3437352,
author = {Markopoulos, Dimitris and Tsolakidis, Anastasios and N. Karanikolas, Nikitas and Skourlas, Christos},
title = {Towards the Design of a Conceptual Framework for the Operation of Intensive Care Units Based on Big Data Analysis},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437352},
doi = {10.1145/3437120.3437352},
abstract = {The development of Big Data Analytics (BDA) technology and the maturity of the Machine Learning (ML) sector offer great opportunities for applications in Intensive Care Units (ICUs). This paper describes a Conceptual Framework and proposes its use in designing architectures and big data applications in ICUs. The Conceptual Framework is based on BDA,MLNatural Language Processing (NLP) and consists of the following subsystems: The "Big Data Integration and ICUs" module, the "ICUs and critical care services" module, the "Use of standards and ICUs" module, the "Machine Learning and ICUs" module, and the “NLP and ICUs” module. The framework is developed using Soft System Methodology (SSM) and Design Science Research Methodology (DSRM).},
booktitle = {24th Pan-Hellenic Conference on Informatics},
pages = {411–415},
numpages = {5},
keywords = {Conceptual Framework, Intensive Care Unit, Machine Learning, Big Data Analysis},
location = {Athens, Greece},
series = {PCI 2020}
}

@article{10.1145/2627534.2627558,
author = {Sharma, Abhishek B. and Ivan\v{c}i\'{c}, Franjo and Niculescu-Mizil, Alexandru and Chen, Haifeng and Jiang, Guofei},
title = {Modeling and Analytics for Cyber-Physical Systems in the Age of Big Data},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627558},
doi = {10.1145/2627534.2627558},
abstract = {In this position paper we argue that the availability of "big" monitoring data on Cyber-Physical Systems (CPS) is challenging the traditional CPS modeling approaches by violating their fundamental assumptions. However, big data alsobrings unique opportunities in its wake by enabling new modeling and analytics approaches as well as facilitating novel applications. We highlight a few key challenges andopportunities, and outline research directions for addressing them. To provide a proper context, we also summarize CPS modeling approaches, and discuss how modeling and analytics for CPS differs from general purpose IT systems.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {74–77},
numpages = {4}
}

@inproceedings{10.1145/3018009.3018040,
author = {Xu, Gang and Wu, Shunyu and Xie, Pengfei},
title = {Integration and Exchange Method of Multi-Source Heterogeneous Big Data for Intelligent Power Distribution and Utilization},
year = {2016},
isbn = {9781450348195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018009.3018040},
doi = {10.1145/3018009.3018040},
abstract = {With the development of smart grid and big data technologies, the stability and economy of distribution network operation are enhanced effectively. Intelligent power distribution and utilization (IPDU) big data platform, which exchanges operation data with other related distribution network management systems, makes decisions for demand side management, power system and distributed energy operation strategies by analyzing the big data. In order to solve the data fusion and exchange problems among all information systems, we proposed a kind of general information model for multi-source heterogeneous big data. In addition, a data fusion and exchange mechanism is established based on circle buffer to ensure the data quality. Finally, this paper demonstrates the effective of the method of IPDU big data fusion method by the example of distribution network reconfiguration. The method proposed in this paper can satisfy the data exchanging demands of future smart grid and demand side management, and it also has good confluent and extensible feature.},
booktitle = {Proceedings of the 2nd International Conference on Communication and Information Processing},
pages = {38–42},
numpages = {5},
keywords = {data fusion and exchange, multi-source and heterogeneous, intelligent power distribution and utilization, information model},
location = {Singapore, Singapore},
series = {ICCIP '16}
}

@inproceedings{10.1145/3510858.3511394,
author = {Chen, Xiaoyu},
title = {Research on Visual Analysis Method of Food Safety Big Data Based on Artificial Intelligence},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3511394},
doi = {10.1145/3510858.3511394},
abstract = {In the background of today's big data era, the popularization of computer network and information technology helps us to understand and disseminate the hot information in the current society more quickly. By quickly understanding these hot issues, we can better supervise and prevent these problems in our life. In recent years, the problem of food safety appears frequently in our field of vision, which makes people have to regard food safety as a hot issue in today's social development. The state and relevant food safety supervision departments are also paying attention to the food safety problems. In order to better supervise food safety issues in this era of big data, this paper will analyze and study food safety issues with the help of popular technologies in the new era, such as artificial intelligence technology and big data technology, so as to formulate a new scheme to meet the needs of people in the new era for food safety supervision. Through the research, it can be found that a series of methods proposed in this paper can effectively provide new ideas for food safety big data visual analysis research method based on artificial intelligence.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {815–819},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3514221.3522568,
author = {Li, Huan and Tang, Bo and Lu, Hua and Cheema, Muhammad Aamir and Jensen, Christian S.},
title = {Spatial Data Quality in the IoT Era: Management and Exploitation},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3522568},
doi = {10.1145/3514221.3522568},
abstract = {Within the rapidly expanding Internet of Things (IoT), growing amounts of spatially referenced data are being generated. Due to the dynamic, decentralized, and heterogeneous nature of the IoT, spatial IoT data (SID) quality has attracted considerable attention in academia and industry. How to invent and use technologies for managing spatial data quality and exploiting low-quality spatial data are key challenges in the IoT. In this tutorial, we highlight the SID consumption requirements in applications and offer an overview of spatial data quality in the IoT setting. In addition, we review pertinent technologies for quality management and low-quality data exploitation, and we identify trends and future directions for quality-aware SID management and utilization. The tutorial aims to not only help researchers and practitioners to better comprehend SID quality challenges and solutions, but also offer insights that may enable innovative research and applications.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {2474–2482},
numpages = {9},
keywords = {geo-sensory data, internet of things, quality management},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3398329.3398330,
author = {Qin, Yana and Yang, Haolin and Guo, Mengjie and Guo, Meicheng},
title = {An Advanced Data Science Model Based on Big Data Analytics for Urban Driving Cycle Construction in China},
year = {2020},
isbn = {9781450377713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3398329.3398330},
doi = {10.1145/3398329.3398330},
abstract = {In recent years, with the rapid growth of car ownership, Chinese road traffic conditions have changed a lot. Governments, enterprises, and the public are increasingly finding that the increasing deviation between the actual fuel consumption and the results of the regulatory certification based on NEDC (New European Driving Cycle). In addition, this deviation has seriously affected the credibility of the government, energy saving and emission reduction of automobiles and environmental pollution. Thus, need to improve urban driving cycle construction methods to adapt the Chinese traffic and automobiles driving cycles.This paper proposes an advanced data science model based on big data analysis for accurate urban driving cycle construction in Chinese cities. In addition, we conduct a lot of data analysis and statistics. Then we design a data preprocessing method for cleaning the noise data to use in driving cycle construction. Extensive experiments and analysis on real-world datasets demonstrate that the proposed methods can significantly reduce the impact of missing and abnormal data on microtrips segmentation, and thus the proposed methods can be used for driving cycle construction in China more accurately.},
booktitle = {Proceedings of the 2020 International Conference on Computing, Networks and Internet of Things},
pages = {1–7},
numpages = {7},
keywords = {urban driving cycle construction, feature engineering, data preprocessing, PCA, Big data analytics},
location = {Sanya, China},
series = {CNIOT2020}
}

@inproceedings{10.1145/2797433.2797467,
author = {Collins, Graham and Varilly, Hugh and Yoshinori, Tanabe},
title = {Pedagogical Lessons from an International Collaborative Big Data Undergraduate Research Project},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797467},
doi = {10.1145/2797433.2797467},
abstract = {This experience report covers the collaboration between UCL and NII Tokyo students in development of data analytics research projects: the challenges, contributing student pedagogy and changes to teaching. Students are often taught technology management separate from other computing modules. The teaching team designed a more coherent learning experience linking the technology management teaching more closely to engineering processes, specifically to engage students whose interest lies more with computing. This project has given rise to a re-evaluation of how technology management is taught to undergraduate students, adoption of architecture as a key aspect and inclusion of students with different levels of academic attainment within a class.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {32},
numpages = {6},
keywords = {contributing student pedagogy, retrospectives, Data analytics, software architecture, peer assessment},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1145/3325112.3325212,
author = {Hagen, Loni and Seon Yi, Hye and Pietri, Siana and E. Keller, Thomas},
title = {Processes, Potential Benefits, and Limitations of Big Data Analytics: A Case Analysis of 311 Data from City of Miami},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325212},
doi = {10.1145/3325112.3325212},
abstract = {As part of the open government movement, an increasing number of 311 call centers have made their datasets available to the public. Studies have found that 311 request patterns are associated with personal attributes and living conditions. Most of these studies used New York City 311 data. In this study, we use 311 data from the City of Miami, a smaller local government, as a case study. This study contributes to digital government research and practices by making suggestions on best practices regarding the use of big data analytics on 311 data. In addition, we discuss limitations of 311 data and analytics results. Finally, we expect our results to inform decision making within the City of Miami government and other local governments.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {1–10},
numpages = {10},
keywords = {e-government, information visualization, 311 data, big data analytics},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@inproceedings{10.1109/CCGrid.2015.46,
author = {Peng, Shaoliang and Liao, Xiangke and Yang, Canqun and Lu, Yutong and Liu, Jie and Cui, Yingbo and Wang, Heng and Wu, Chengkun and Wang, Bingqiang},
title = {The Challenge of Scaling Genome Big Data Analysis Software on TH-2 Supercomputer},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.46},
doi = {10.1109/CCGrid.2015.46},
abstract = {Whole genome re-sequencing plays a crucial role in biomedical studies. The emergence of genomic big data calls for an enormous amount of computing power. However, current computational methods are inefficient in utilizing available computational resources. In this paper, we address this challenge by optimizing the utilization of the fastest supercomputer in the world - TH-2 supercomputer. TH-2 is featured by its neo-heterogeneous architecture, in which each compute node is equipped with 2 Intel Xeon CPUs and 3 Intel Xeon Phi coprocessors. The heterogeneity and the massive amount of data to be processed pose great challenges for the deployment of the genome analysis software pipeline on TH-2. Runtime profiling shows that SOAP3-dp and SOAPsnp are the most time-consuming components (up to 70% of total runtime) in a typical genome-analyzing pipeline. To optimize the whole pipeline, we first devise a number of parallel and optimization strategies for SOAP3-dp and SOAPsnp, respectively targeting each node to fully utilize all sorts of hardware resources provided both by CPU and MIC. We also employ a few scaling methods to reduce communication between different nodes. We then scaled up our method on TH-2. With 8192 nodes, the whole analyzing procedure took 8.37 hours to finish the analysis of a 300 TB dataset of whole genome sequences from 2,000 human beings, which can take as long as 8 months on a commodity server. The speedup is about 700x.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {823–828},
numpages = {6},
keywords = {TH-2 supercomputer, sequence alignment, SNP detection, whole genome re-sequencing, parallel optimization},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@article{10.1145/3498338,
author = {Li, Huan and Lu, Hua and Jensen, Christian S. and Tang, Bo and Cheema, Muhammad Aamir},
title = {Spatial Data Quality in the Internet of Things: Management, Exploitation, and Prospects},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3498338},
doi = {10.1145/3498338},
abstract = {With the continued deployment of the Internet of Things (IoT), increasing volumes of devices are being deployed that emit massive spatially referenced data. Due in part to the dynamic, decentralized, and heterogeneous architecture of the IoT, the varying and often low quality of spatial IoT data (SID) presents challenges to applications built on top of this data. This survey aims to provide unique insight to practitioners who intend to develop IoT-enabled applications and to researchers who wish to conduct research that relates to data quality in the IoT setting. The survey offers an inventory analysis of major data quality dimensions in SID and covers significant data characteristics and associated quality considerations. The survey summarizes data quality related technologies from both task and technique perspectives. Organizing the technologies from the task perspective, it covers recent progress in SID quality management, encompassing location refinement, uncertainty elimination, outlier removal, fault correction, data integration, and data reduction; and it covers low-quality SID exploitation, encompassing querying, analysis, and decision-making techniques. Finally, the survey covers emerging trends and open issues concerning the quality of SID.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {57},
numpages = {41},
keywords = {Internet of Things, spatial computing, quality management, location refinement, geo-sensory data, spatiotemporal dependencies, spatial queries, spatiotemporal data cleaning}
}

@article{10.1109/TNET.2019.2934026,
author = {Gong, Xiaowen and Shroff, Ness B.},
title = {Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality},
year = {2019},
issue_date = {October 2019},
publisher = {IEEE Press},
volume = {27},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2934026},
doi = {10.1109/TNET.2019.2934026},
abstract = {Mobile crowdsensing has found a variety of applications e.g., spectrum sensing, environmental monitoring by leveraging the “wisdom” of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users’ data e.g., users’ received SNRs for measuring a transmitter’s transmit signal strength. However, the quality of a user can be its private information which, e.g., may depend on the user’s location that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data’s accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data’s accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation QEE, which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user’s data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester’s optimal RO effort assignment assigns effort only to the best user that has the smallest “virtual valuation”, which depends on the user’s quality and the quality’s distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.},
journal = {IEEE/ACM Trans. Netw.},
month = {oct},
pages = {1959–1972},
numpages = {14}
}

@inproceedings{10.1145/3482632.3484095,
author = {Wu, Rui and Cheng, Qian and He, Lisong and Cao, Zhenyu},
title = {Environmental Big Data Model and Recognition of Abnormal Emission from Enterprise Data},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484095},
doi = {10.1145/3482632.3484095},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2041–2046},
numpages = {6},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3472163.3472185,
author = {Zhao, Yan and Megdiche, Imen and Ravat, Franck and Dang, Vincent-nam},
title = {A Zone-Based Data Lake Architecture for IoT, Small and Big Data},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472185},
doi = {10.1145/3472163.3472185},
abstract = {Data lakes are supposed to enable analysts to perform more efficient and efficacious data analysis by crossing multiple existing data sources, processes and analyses. However, it is impossible to achieve that when a data lake does not have a metadata governance system that progressively capitalizes on all the performed analysis experiments. The objective of this paper is to have an easily accessible, reusable data lake that capitalizes on all user experiences. To meet this need, we propose an analysis-oriented metadata model for data lakes. This model includes the descriptive information of datasets and their attributes, as well as all metadata related to the machine learning analyzes performed on these datasets. To illustrate our metadata solution, we implemented a web application of data lake metadata management. This application allows users to find and use existing data, processes and analyses by searching relevant metadata stored in a NoSQL data store within the data lake. To demonstrate how to easily discover metadata with the application, we present two use cases, with real data, including datasets similarity detection and machine learning guidance.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {94–102},
numpages = {9},
keywords = {Metadata, Stream IoT Data, Zone-based, Technical Architecture, Data Lake},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3495018.3495364,
author = {Ding, Gaohu},
title = {Optimization of Modern Teaching System with Computer Technology under the Background of Big Data},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495364},
doi = {10.1145/3495018.3495364},
abstract = {The rapid development of science and technology has promoted the rapid development and wide application of CT(computer Technology), especially the emergence of BDT(big data technology) in recent years. The integration and improvement of CT and it has promoted great changes in all aspects of society, and these changes are beneficial, and they have brought us great convenience and help Help. For the education industry, under the background of BDT, the integration of CT and modern teaching system can provide new development thinking and new direction for the reform of modern education. In order to study what effect the combination of the two will bring, this paper selects two universities and their students as the experimental research objects to explore how the modern teaching system will be innovated and developed under the effect of this new technology. The experimental results show that the students of a university who have applied CT in the modern teaching system have a high degree of satisfaction, and the percentage of those who are satisfied has reached 67%. Moreover, the score of teaching and research group of a university is relatively high, and the highest score is 95.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1197–1200},
numpages = {4},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3361821.3361825,
author = {Podhoranyi, Michal and Vojacek, Lukas},
title = {Social Media Data Processing Infrastructure by Using Apache Spark Big Data Platform: Twitter Data Analysis},
year = {2019},
isbn = {9781450372411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361821.3361825},
doi = {10.1145/3361821.3361825},
abstract = {Social media provide continuous data streams that contain information with different level of sensitivity, validity and accuracy. Therefore, this type of information has to be properly filtered, extracted and processed to avoid noisy and inaccurate results. The main goal of this work is to propose architecture and workflow able to process Twitter social network data in near real-time. The primary design of the introduced modern architecture covers all processing aspects from data ingestion and storing to data processing and analysing. This paper presents Apache Spark and Hadoop implementation. The secondary objective is to analyse tweets with the defined topic --- floods. The word frequency method (Word Clouds) is shown as a major tool to analyse the content of the input dataset. The experimental architecture confirmed the usefulness of many well-known functions of Spark and Hadoop in the social data domain. The platforms which were used provided effective tools for optimal data ingesting, storing as well as processing and analysing. Based on the analytical part, it was observed that the word frequency method (n-grams) can effectively reveal the tweets content. According to the results of this study, the tweets proved their high informative potential regarding data quality and quantity.},
booktitle = {Proceedings of the 2019 4th International Conference on Cloud Computing and Internet of Things},
pages = {1–6},
numpages = {6},
keywords = {data processing architecture, social network data, Apache Spark, Twitter},
location = {Tokyo, Japan},
series = {CCIOT 2019}
}

@inproceedings{10.1145/3482632.3484007,
author = {Li, Jicai and Liu, Dan},
title = {A Method of Constructing Distributed Big Data Analysis Model for Machine Learning Based on Cloud Computing},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484007},
doi = {10.1145/3482632.3484007},
abstract = {There are many big data analysis methods, and it is an effective method to build big data analysis model through machine learning. Big data is characterized by large data scale and long calculation cycle. In order to speed up the calculation speed and shorten the calculation cycle, distributed computing method is one of the effective methods to solve the above problems. With the wide application and rapid development of information technology, cloud computing as a new business computing model has attracted more and more attention. However, the security of cloud computing data storage model lacks reliability. Under the mainstream cloud computing and big data basic environment, building a better model from resource aggregation to analysis and mining, and modeling distributed big data analysis can provide high-reliability, high-security, high-efficiency analysis services for practical analysis and mining applications such as intelligence judgment, information deployment and control, stakeholder analysis and intelligent decision-making.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1634–1638},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3482632.3487514,
author = {Wang, Wenwen},
title = {Research on the Application of Big Data Cloud Cleaning System in Physical Function Sports Training Management},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3487514},
doi = {10.1145/3482632.3487514},
abstract = {The main purpose of physical function training is to solve the problems of many injuries, incorrect movement patterns, movement compensation and so on. In the era of big data, the scientific research field of sports training is gradually beginning to adopt the big data model for development, which will better form a new insight after data generation, collection, analysis and transformation. Therefore, this paper analyzes the application of big data intelligent cleaning system based on cloud computing in physical function training management. Students can quickly query running results through WEB query system, and administrators can download and modify students' results through WEB system. By using big data analysis technology, students are grouped according to their physical fitness test scores and BMI index, and their sports plans are formulated accordingly. A high-reliability real-time wireless receiving and sending sports monitoring system for big data is adopted, which realizes the functions of networking, unmanned and intelligent sports management.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2783–2786},
numpages = {4},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@article{10.1145/2992787,
author = {De, Sushovan and Hu, Yuheng and Meduri, Venkata Vamsikrishna and Chen, Yi and Kambhampati, Subbarao},
title = {BayesWipe: A Scalable Probabilistic Framework for Improving Data Quality},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2992787},
doi = {10.1145/2992787},
abstract = {Recent efforts in data cleaning of structured data have focused exclusively on problems like data deduplication, record matching, and data standardization; none of the approaches addressing these problems focus on fixing incorrect attribute values in tuples. Correcting values in tuples is typically performed by a minimum cost repair of tuples that violate static constraints like Conditional Functional Dependencies (which have to be provided by domain experts or learned from a clean sample of the database). In this article, we provide a method for correcting individual attribute values in a structured database using a Bayesian generative model and a statistical error model learned from the noisy database directly. We thus avoid the necessity for a domain expert or clean master data. We also show how to efficiently perform consistent query answering using this model over a dirty database, in case write permissions to the database are unavailable. We evaluate our methods over both synthetic and real data.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {5},
numpages = {30},
keywords = {statistical data cleaning, Data quality, offline and online cleaning}
}

@article{10.1145/3507467,
author = {Shen, Yanyan and Dinh, Anh and Jagadish, H. V.},
title = {Introduction to the Special Issue on Data Science for Next Generation Big Data},
year = {2022},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1922},
url = {https://doi.org/10.1145/3507467},
doi = {10.1145/3507467},
journal = {ACM/IMS Trans. Data Sci.},
month = {mar},
articleno = {31},
numpages = {2}
}

@inproceedings{10.1145/3457784.3457816,
author = {Farhana Jamaludin, Ain and Najib Razali, Muhammad and jalil, Rohaya and Othman, Hajar and Adnan, Yasmin},
title = {Identification of Business Intelligence in Big Data Maintenance of Government Sector in Putrajaya},
year = {2021},
isbn = {9781450388825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457784.3457816},
doi = {10.1145/3457784.3457816},
abstract = {This paper contributes significantly, which focuses on an intelligent system that lets the government make an integral part of decision-making and can be applied horizontally to solve the problems in maintenance practice through business intelligence. Accordingly, a real-time data management system for maintenance management is proposed in this paper. It looks at a real case study highlighting the need for proper data management in the government sector. Our findings bridge the gap of information technology inserted in government office buildings, with maintenance management being the domain. This paper demonstrates the underlying structure of the developed simulation model.},
booktitle = {2021 10th International Conference on Software and Computer Applications},
pages = {201–207},
numpages = {7},
keywords = {Maintenance Management, Data Management,, Business Intelligence},
location = {Kuala Lumpur, Malaysia},
series = {ICSCA 2021}
}

@inproceedings{10.1145/3278312.3278316,
author = {Win, Thee Zin and Kham, Nang Saing Moon},
title = {Mutual Information-Based Feature Selection Approach to Reduce High Dimension of Big Data},
year = {2018},
isbn = {9781450365567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278312.3278316},
doi = {10.1145/3278312.3278316},
abstract = {As increasing the massive amount of data demands effective and efficient mining strategies, practitioners and researchers are trying to develop scalable mining algorithms, machine learning algorithms and strategies to be successful data mining in turning mountains of data into nuggets. Data of high dimension significantly increases the memory storage requirements and computational costs for data analytics. Therefore, reducing dimension can mainly improve three data mining performance: speed of learning, predictive accuracy and simplicity and comprehensibility of mined result. Feature selection, data preprocessing technique, is effective and efficient in data mining, data analytics and machine learning problems particularly in high dimension reduction. Most feature selection algorithms can eliminate only irrelevant features but redundant features. Not only irrelevant features but also redundant features can degrade learning performance. Mutual information measured feature selection is proposed in this work to remove both irrelevant and redundant features.},
booktitle = {Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence},
pages = {3–7},
numpages = {5},
keywords = {Redundant Features, High Dimensional Data, Mutual Information, Feature Selection},
location = {Ha Noi, Viet Nam},
series = {MLMI2018}
}

@inproceedings{10.1145/3468264.3468613,
author = {Wang, Zehao and Zhang, Haoxiang and Chen, Tse-Hsun (Peter) and Wang, Shaowei},
title = {Would You like a Quick Peek? Providing Logging Support to Monitor Data Processing in Big Data Applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468613},
doi = {10.1145/3468264.3468613},
abstract = {To analyze large-scale data efficiently, developers have created various big data processing frameworks (e.g., Apache Spark). These big data processing frameworks provide abstractions to developers so that they can focus on implementing the data analysis logic. In traditional software systems, developers leverage logging to monitor applications and record intermediate states to assist workload understanding and issue diagnosis. However, due to the abstraction and the peculiarity of big data frameworks, there is currently no effective monitoring approach for big data applications. In this paper, we first manually study 1,000 randomly sampled Spark-related questions on Stack Overflow to study their root causes and the type of information, if recorded, that can assist developers with motioning and diagnosis. Then, we design an approach, DPLOG, which assists developers with monitoring Spark applications. DPLOG leverages statistical sampling to minimize performance overhead and provides intermediate information and hint/warning messages for each data processing step of a chained method pipeline. We evaluate DPLOG on six benchmarking programs and find that DPLOG has a relatively small overhead (i.e., less than 10% increase in response time when processing 5GB data) compared to without using DPLOG, and reduce the overhead by over 500% compared to the baseline. Our user study with 20 developers shows that DPLOG can reduce the needed time to debug big data applications by 63% and the participants give DPLOG an average of 4.85/5 for its usefulness. The idea of DPLOG may be applied to other big data processing frameworks, and our study sheds light on future research opportunities in assisting developers with monitoring big data applications.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {516–526},
numpages = {11},
keywords = {Logging, Monitoring, Apache Spark},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3132847.3133187,
author = {Wang, Hongzhi and Ding, Xiaoou and Chen, Xiangying and Li, Jianzhong and Gao, Hong},
title = {CleanCloud: Cleaning Big Data on Cloud},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133187},
doi = {10.1145/3132847.3133187},
abstract = {We describe CleanCloud, a system for cleaning big data based on Map-Reduce paradigm in cloud. Using Map-Reduce paradigm, the system detects and repairs various data quality problems in big data. We demonstrate the following features of CleanCloud: (a) the support for cleaning multiple data quality problems in big data; (b) a visual tool for watching the status of big data cleaning process and tuning the parameters for data cleaning; (c) the friendly interface for data input and setting as well as cleaned data collection for big data. CleanCloud is a promising system that provides scalable and effect data cleaning mechanism for big data in either files or databases.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2543–2546},
numpages = {4},
keywords = {entity resolution, data cleaning, parallel computing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3482632.3483061,
author = {Han, Caibao},
title = {The Architecture and Security Design of Big Data Platform of the Physical Teaching Information System in Colleges and Universities},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3483061},
doi = {10.1145/3482632.3483061},
abstract = {This paper constructs the logic structure of big data platform of physical teaching information system in Colleges and universities through Hadoop distributed system, which ensures the realization of management and monitoring function, data collection function, data storage and query function, data calculation function, security and management function. The security design is carried out from access control, data authorization and management, Hadoop security configuration to deal with the possible security problems of big data platform.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {960–963},
numpages = {4},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@article{10.1145/3394957,
author = {Beneventano, Domenico and Bergamaschi, Sonia and Gagliardelli, Luca and Simonini, Giovanni},
title = {BLAST2: An Efficient Technique for Loose Schema Information Extraction from Heterogeneous Big Data Sources},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3394957},
doi = {10.1145/3394957},
abstract = {We present BLAST2, a novel technique to efficiently extract loose schema information, i.e., metadata that can serve as a surrogate of the schema alignment task within the Entity Resolution (ER) process, to identify records that refer to the same real-world entity when integrating multiple, heterogeneous, and voluminous data sources. The loose schema information is exploited for reducing the overall complexity of ER, whose na\"{\i}ve solution would imply O(n2) comparisons, where n is the number of entity representations involved in the process and can be extracted by both structured and unstructured data sources. BLAST2 is completely unsupervised yet able to achieve almost the same precision and recall of supervised state-of-the-art schema alignment techniques when employed for Entity Resolution tasks, as shown in our experimental evaluation performed on two real-world datasets (composed of 7 and 10 data sources, respectively).},
journal = {J. Data and Information Quality},
month = {nov},
articleno = {18},
numpages = {22},
keywords = {data integration, Entity resolution, big data}
}

@article{10.1145/2996198,
author = {Shankaranarayanan, G. and Blake, Roger},
title = {From Content to Context: The Evolution and Growth of Data Quality Research},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2996198},
doi = {10.1145/2996198},
abstract = {Research in data and information quality has made significant strides over the last 20 years. It has become a unified body of knowledge incorporating techniques, methods, and applications from a variety of disciplines including information systems, computer science, operations management, organizational behavior, psychology, and statistics. With organizations viewing “Big Data”, social media data, data-driven decision-making, and analytics as critical, data quality has never been more important. We believe that data quality research is reaching the threshold of significant growth and a metamorphosis from focusing on measuring and assessing data quality—content—toward a focus on usage and context. At this stage, it is vital to understand the identity of this research area in order to recognize its current state and to effectively identify an increasing number of research opportunities within. Using Latent Semantic Analysis (LSA) to analyze the abstracts of 972 peer-reviewed journal and conference articles published over the past 20 years, this article contributes by identifying the core topics and themes that define the identity of data quality research. It further explores their trends over time, pointing to the data quality dimensions that have—and have not—been well-studied, and offering insights into topics that may provide significant opportunities in this area.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {9},
numpages = {28},
keywords = {text mining, information quality, Data quality}
}

@inbook{10.5555/3042094.3042407,
author = {Chien, Chen-Fu and Chen, Ying-Jen and Wu, Jei-Zheng},
title = {Big Data Analytics for Modeling WAT Parameter Variation Induced by Process Tool in Semiconductor Manufacturing and Empirical Study},
year = {2016},
isbn = {9781509044849},
publisher = {IEEE Press},
abstract = {With the feature size shrinkage in advanced technology nodes, the modeling of process variations has become more critical for troubleshooting and yield enhancement. Misalignment among equipment tools or chambers in process stages is a major source of process variations. Because a process flow contains hundreds of stages during semiconductor fabrication, tool/chamber misalignment may more significantly affect the variation of transistor parameters in a wafer acceptance test. This study proposes a big data analytic framework that simultaneously considers the mean difference between tools and wafer-to-wafer variation and identifies possible root causes for yield enhancement. An empirical study was conducted to demonstrate the effectiveness of proposed approach and obtained promising results.},
booktitle = {Proceedings of the 2016 Winter Simulation Conference},
pages = {2512–2522},
numpages = {11}
}

