@ARTICLE{9511451,
author={Wang, Zhao-Xu and Wang, Hao-Quan and Ren, Shi-Lei},
journal={IEEE Access},
title={Research on ADMM Reconstruction Algorithm of Photoacoustic Tomography With Limited Sampling Data},
year={2021},
volume={9},
number={},
pages={113631-113641},
abstract={Photoacoustic imaging is a new non-destructive biomedical imaging method. When limited independent data is available, the restoration of the initial pressure rise distribution is often an ill-posed problem. In this paper, based on the study of photoacoustic effects, the sparse prior information of photoacoustic images is integrated into the reconstruction process by using the compressed sensing (CS) theory and the L2 norm optimization technique, combining the augmented Langrange weighting of the alternating direction method of multipliers (ADMM) with the total variation (TV) minimization problem, and the reconstruction artifacts are effectively eliminated. The simulation data from the real numerical model show that compared with the common time reversal algorithm, interpolation algorithm and truncated back projection algorithm, the total variational regularization method based on ADMM can effectively improve the quality of reconstructed images under the condition of limited viewing angles and incomplete projection data.},
keywords={Image reconstruction;Absorption;Mathematical model;Detectors;Biological tissues;TV;Photoacoustic imaging;Photoacoustic imaging;alternating direction method of multipliers;total variation;image reconstruction},
doi={10.1109/ACCESS.2021.3104154},
ISSN={2169-3536},
month={},}
@ARTICLE{8528834,
author={Yang, Zi-Yi and Liang, Yong and Zhang, Hui and Chai, Hua and Zhang, Bowen and Peng, Cheng},
journal={IEEE Access},
title={Robust Sparse Logistic Regression With the $L_{q}$ ( $0 < \text{q} < 1$ ) Regularization for Feature Selection Using Gene Expression Data},
year={2018},
volume={6},
number={},
pages={68586-68595},
abstract={Microarray technology is a popular technique that has been extensively applied in cancer diagnosis. Many studies have used high-dimensional microarray data to identify informative features to classify the types of cancer, yet numerous irrelevant features that exist in microarray data may introduce the noise and decrease classification accuracy. Regularization techniques are common methods for feature selection, which can be used to reduce irrelevant features and avoid overfitting. In recent years, different regularization methods have been proposed. Theoretically, the Lq (0 <; q <; 1) type penalty function with the lower value of q would acquire better sparse solutions. In addition, the loss function in most regression models is based on least-squares minimization. However, the least-square method is sensitive to noise and has poor robustness, especially when the error has a heavy-tailed distribution. It is well known that the least absolute deviation regression is the most common method for the robust regression, which can overcome the big noise problem. In general, there is a high level of noise in microarray data, which deter the development of microarray technology. To solve the above-mentioned problems, we propose a robust logistic regression based on the Lq (0 <; q <; 1) regularization approach, which is a feasible and effective approach for feature selection in microarray classification. The Lq (0 <; q <; 1) regularization leads to a non-convex optimization problem that is difficult to be solved. In this paper, we utilize a genetic algorithm based on the global search strategy to obtain an optimal solution.},
keywords={Logistics;Feature extraction;Cancer;Gene expression;Optimization;Genetic algorithms;Machine learning;Robust logistic regression;feature selection;Lq regularization;genetic algorithm},
doi={10.1109/ACCESS.2018.2880198},
ISSN={2169-3536},
month={},}
@ARTICLE{9262939,
author={Al-Jaroodi, Jameela and Mohamed, Nader and Abukhousa, Eman},
journal={IEEE Access},
title={Health 4.0: On the Way to Realizing the Healthcare of the Future},
year={2020},
volume={8},
number={},
pages={211189-211210},
abstract={Health 4.0 establishes a new promising vision for the healthcare industry. It creatively integrates and employ innovative technologies such as the Internet of Health Things (IoHT), medical Cyber-Physical Systems (medical CPS), health cloud, health fog, big data analytics, machine learning, blockchain, and smart algorithms. The goal is to deliver improved, value-added and cost-effective healthcare services to patients and enhance the effectiveness and efficiency or the healthcare industry. Health 4.0 (adapted from the Industry 4.0 principles) changes the healthcare business model to enhance the interactions across the healthcare clients (the patients), stakeholders, infrastructure, and value chain. This effectively will improve the quality, flexibility, productivity, cost-effectiveness, and reliability of healthcare services in addition to increasing patients’ satisfaction. However, building and utilizing healthcare applications that follow the Health 4.0 concept is a non-trivial and complex endeavor. In addition, advanced potential applications based on Health 4.0 capabilities are not yet being investigated. In this paper we define the main objectives of Health 4.0 and discuss advanced potential Health 4.0 applications. To have a clear understanding of these applications, we categorize them in 4 groups based on the primary beneficiary of these applications. Thus we have patient targeted applications, applications supporting healthcare professionals, resource management applications and high-level healthcare systems management applications. In addition, as we studied the different applications, we realized that these is a certain collection of services that these most of them need regardless of their goals or business context. Services supporting data collection and transfer, security and privacy, reliable operations are some examples. As a result we propose creating a service-oriented middleware framework to offers the common services to the applications developers and facilitate the integration of different services to build applications under the Health 4.0 umbrella.},
keywords={Medical services;Industries;Middleware;Cloud computing;Medical diagnostic imaging;Reliability;Information and communication technology;Health 40;Industry 40;healthcare systems;service-oriented middleware;service integration;health cloud;health fog;Internet of Health Things;medical cyber-physical systems;COVID-19},
doi={10.1109/ACCESS.2020.3038858},
ISSN={2169-3536},
month={},}
@ARTICLE{9797696,
author={Mahi, Md. Julkar Nayeen and Chaki, Sudipto and Ahmed, Shamim and Biswas, Milon and Kaiser, M. Shamim and Islam, Mohammad Shahidul and Sookhak, Mehdi and Barros, Alistair and Whaiduzzaman, Md},
journal={IEEE Access},
title={A Review on VANET Research: Perspective of Recent Emerging Technologies},
year={2022},
volume={10},
number={},
pages={65760-65783},
abstract={Recent technology has modeled VANET (vehicular adhoc network) communication well in terms of privileges to derive vehicular communication technologically to save time, energy, and money. Due to the increase in powerful technology in modern times, VANETs play a vital role in uplifting daily concerns across vehicles and vehicular identities. Hence, to tune VANETs to become compatible with traditional technologies and increase demand, VANETs require upgrading. The severity and frequency of unwanted occurrences have become a considerable concern for our day-to-day lives relating to vehicular position. Thus, verily updated methodologies or working procedures are needed for the future VANET interplay to eradicate such problems occurring through vehicular identities. This article outlines in technology related to VANETS, future developments, and coping issues by deriving comprehensive frameworks, workflow patterns, upgrading procedures including big data, fog computing, SDN (software defined networking), and SIoT (social Internet of Things). This article provides a high-level overview of a complete VANET upgrade solution to address future problem management issues under a range of acceptable scientific themes, indicators, and combinations.},
keywords={Vehicular ad hoc networks;Cloud computing;Edge computing;Big Data;Global Positioning System;Quality of service;Protocols;Vehicular ad hoc network;social Internet of Things;Internet of Vehicle;big data;fog computing},
doi={10.1109/ACCESS.2022.3183605},
ISSN={2169-3536},
month={},}
@ARTICLE{9383229,
author={Chen, Hongqian and Guan, Mengxi and Li, Hui},
journal={IEEE Access},
title={ArCycleGAN: Improved CycleGAN for Style Transferring of Fruit Images},
year={2021},
volume={9},
number={},
pages={46776-46787},
abstract={CycleGAN can realize image translation and style transferring among unpaired images. However, it will easily generate inappropriate image results when the number and shapes of objects in the style offering image and the source image are greatly different. The paper proposed an improved network, named arCycleGAN, which introduced the mechanism of attribute registration into CycleGAN to solve the problem. The arCycleGAN can transfer the freshness styles from the style offering images to the unpaired input source images. The generated target images will have the freshness attributes of the style offering images, while maintaining the shapes and key features of the input source images. The realization of mechanism of attribute registration consists of three modules. The first module is attribute recognition module, which can identify and label the attributes of objects in images. The second module is image pre-screening module, which selects appropriate image subset as screened training set from raw image set according to the attributes of the input source images. The third module is similarity matching module, which matches the images in screened training set based on the similarity. The generator and discriminator in the new network are similar to that in the CycleGAN network. Experimental results demonstrate the effectiveness and better performance of the arCycleGAN. Compared with the CycleGAN, the new network can generate more convincing images. It can generate the target images of similar quality based on a smaller training set and less training time than the original CycleGAN. For generating images of similar quality, the number of images in the required training set can be reduced by 50%, while training time is reduced by 5.8%.},
keywords={Training;Convolution;Generators;Image reconstruction;Image recognition;Generative adversarial networks;Diseases;Image generation;style transferring;attribute registration;image registration;improved CycleGAN},
doi={10.1109/ACCESS.2021.3068094},
ISSN={2169-3536},
month={},}
@ARTICLE{8304571,
author={Liao, Zhifang and He, Dayu and Chen, Zhijie and Fan, Xiaoping and Zhang, Yan and Liu, Shengzong},
journal={IEEE Access},
title={Exploring the Characteristics of Issue-Related Behaviors in GitHub Using Visualization Techniques},
year={2018},
volume={6},
number={},
pages={24003-24015},
abstract={Feedback from software users, such as bug reports, is vital in the management of software projects. In GitHub, the feedback is typically expressed as new issues. Through filing issue reports, users may help identify and fix bugs, document software code, and enhance software quality via feature requests. In this paper, we aim at investigating some characteristics of issues to facilitate issue management and software management. We investigate the important degrees of behaviors that are related to issues in popular projects to assess the importance of issues in GitHub and analyze the effectiveness of issue labeling for issue handling. Then, we explore the patterns of issue commits over time in popular projects based on visual analysis and obtain the following results: we find that the behaviors that are related to issues play important roles in the GitHub. We also find that the time distribution of issue commits follows a three-period development model, which approximately corresponds to the project life cycle. These results may provide a new knowledge about issues that can help managers manage and allocate project resources more effectively and even reduce software failures.},
keywords={Software;Data visualization;Analytical models;Visualization;Labeling;Computer bugs;Sentiment analysis;Open-source software community;project development model;visual analysis;issue commit;software management},
doi={10.1109/ACCESS.2018.2810295},
ISSN={2169-3536},
month={},}
@ARTICLE{8642321,
author={Huang, Mingfeng and Liu, Wei and Wang, Tian and Song, Houbing and Li, Xiong and Liu, Anfeng},
journal={IEEE Access},
title={A Queuing Delay Utilization Scheme for On-Path Service Aggregation in Services-Oriented Computing Networks},
year={2019},
volume={7},
number={},
pages={23816-23833},
abstract={In services-oriented computing networks, packets in the process of routing to a data center must wait for a sufficient amount of data before service aggregation to reduce the network transmission load. However, packets must be uploaded to the data center as soon as possible to reduce delay. With the exponential growth in the number of IoT connected devices, the wait time for packets is longer at routers due to massive amounts of data, which causes a large queuing delay. If this queuing time can be utilized for service aggregation in a service-oriented computing network, the network performance will be substantially improved. Therefore, a queuing delay utilization scheme for on-path service aggregation (SAQD) is proposed in this paper. This scheme has the following innovations: 1) SAQD fully utilizes the queuing delay of packets for service aggregation, which can effectively reduce the transmission volume and communication overhead. Based on the proposed service aggregation algorithm, packets are divided into forwarding packets and aggregating packets, and the service aggregation of aggregating packets is completed by utilizing the transmission time of forwarding packets to ensure that the transmission volume and communication overhead are effectively reduced without additional latency. 2) SAQD can effectively alleviate the traffic pressure of the data center and balance the workload of routers. By the service aggregation and intranet cache of routers, some requests for the data center can be handled by routers, which reduces the traffic pressure of the data center, especially in the peak period. Compared with conventional schemes, the experimental results demonstrate that SAQD reduces the workload of the data center by 55.8%-66.26% and provides users with a better quality of experience by reducing the request response delay by 31.33%~51.41%.},
keywords={Delays;Data centers;Service computing;Mobile handsets;Propagation delay;Relays;Routing;Internet of things;big data;queuing delay;service aggregation},
doi={10.1109/ACCESS.2019.2899402},
ISSN={2169-3536},
month={},}
@ARTICLE{9042874,
author={Lv, Kai and Sheng, Hao and Xiong, Zhang and Li, Wei and Zheng, Liang},
journal={IEEE Transactions on Image Processing},
title={Pose-Based View Synthesis for Vehicles: A Perspective Aware Method},
year={2020},
volume={29},
number={},
pages={5163-5174},
abstract={In this paper, we focus on the problem of novel view synthesis for vehicles. Some previous works solve the problem of novel view synthesis in a controlled 3D environment by exploiting additional 3D details (i.e., camera viewpoints and underlying 3D models). However, in real scenarios, the 3D details are difficult to obtain. In this case, we find that introducing vehicle pose to represent the views of vehicles is an alternative paradigm to solve the lack of 3D details. In novel view synthesis, preserving local details is one of the most challenging problems. To address this problem, we propose a perspective-aware generative model (PAGM). We are motivated by the prior that vehicles are made of quadrilateral planes. Preserving these rigid planes during image generation ensures that image details are kept. To this end, a classic image transformation method is leveraged, i.e., perspective transformation. In our GAN-based system, the perspective transformation is applied to the encoder feature maps, and the resulting maps are regarded as new conditions for the decoder. This strategy preserves the quadrilateral planes all the way through the network, thus shuttling the texture details from the input image to the generated image. In the experiments, we show that PAGM can generate high-quality vehicle images with fine details. Quantitatively, our method is superior to several competing approaches employing either GAN or the perspective transformation. Code is available at: https://github.com/ilvkai/view-synthesis-for-vehicles},
keywords={Three-dimensional displays;Solid modeling;Gallium nitride;Generative adversarial networks;Licenses;Cameras;Task analysis;Novel view synthesis;generative adversarial nets;perspective transformation;generative model;vehicle pose},
doi={10.1109/TIP.2020.2980130},
ISSN={1941-0042},
month={},}
@ARTICLE{8936960,
author={Lu, Li and Hua, Bei},
journal={IEEE Access},
title={Query-Sensitive Graph Partitioner for Pattern Matching Applications},
year={2019},
volume={7},
number={},
pages={184668-184675},
abstract={Searching and mining in large graphs is critical to a variety of applications, at the core of which is the pattern matching activity. The scalable processing of large graphs requires careful distribution of graphs across clusters. Graph partitioning is the technique that divides a big graph into several non-overlapped subgraphs and assigns each subgraph to a compute node. Traditional workload agnostic partitioners aim to minimize the number of inter-partition edges using only graph topology, which, however, may not obtain the best solution if the workload exhibits skew. Some workload-aware partitioners choose to mine information from a specific workload and use it to minimize the number of inter-partition traversals during execution; however, their methods are not suitable for pattern matching applications. In this work, we propose a query-sensitive graph partitioner that aims to improve existing partitioning for a given pattern matching workload. The partitioner takes any initial partitioning as a starting point and iteratively adjusts it by exchanging chosen clusters across partitions, heuristically reducing the probability of inter-partition traversals. We determine a few implementation-irrelative factors that may increase the traversal probability of an edge and quantify them into a calculable indicator with information from query patterns and graph topology. Then, we propose an efficient algorithm to calculate the indicator and implement a graph repartitioner by combining the indicator with a greedy cluster-exchanging mechanism. Finally, we generate a large heterogeneous labeled graph with real-world data crawled from the Netease Music website and evaluate the partitioning quality of our repartitioner with a few meaningful query patterns of common topologies including line, loop and branching. Compared with a hash-based partitioning, our system can reduce the inter-partition traversals by at least 70%. Compared with the state-of-the-art graph partitioner Metis, our repartitioner can reduce the inter-partition traversals by at least 50%.},
keywords={Pattern matching;Topology;Partitioning algorithms;Clustering algorithms;Complexity theory;Computational modeling;Search problems;Graph computing;graph partitioning;pattern match;workload mining},
doi={10.1109/ACCESS.2019.2960868},
ISSN={2169-3536},
month={},}
@ARTICLE{9525388,
author={Gambín, Ángel Fernández and Angelats, Eduard and González, Jesús Soriano and Miozzo, Marco and Dini, Paolo},
journal={IEEE Access},
title={Sustainable Marine Ecosystems: Deep Learning for Water Quality Assessment and Forecasting},
year={2021},
volume={9},
number={},
pages={121344-121365},
abstract={An appropriate management of the available resources within oceans and coastal regions is vital to guarantee their sustainable development and preservation, where water quality is a key element. Leveraging on a combination of cross-disciplinary technologies including Remote Sensing (RS), Internet of Things (IoT), Big Data, cloud computing, and Artificial Intelligence (AI) is essential to attain this aim. In this paper, we review methodologies and technologies for water quality assessment that contribute to a sustainable management of marine environments. Specifically, we focus on Deep Leaning (DL) strategies for water quality estimation and forecasting. The analyzed literature is classified depending on the type of task, scenario and architecture. Moreover, several applications including coastal management and aquaculture are surveyed. Finally, we discuss open issues still to be addressed and potential research lines where transfer learning, knowledge fusion, reinforcement learning, edge computing and decision-making policies are expected to be the main involved agents.},
keywords={Sea measurements;Water quality;Ecosystems;Aquaculture;Forecasting;Feature extraction;Europe;Sustainable coastal management;sustainable aquaculture;remote sensing;artificial intelligence;machine learning;water quality;blue economy},
doi={10.1109/ACCESS.2021.3109216},
ISSN={2169-3536},
month={},}
@ARTICLE{8704713,
author={Ahmad, Arshad and Feng, Chong and Li, Kan and Asim, Syed Mohammad and Sun, Tingting},
journal={IEEE Access},
title={Toward Empirically Investigating Non-Functional Requirements of iOS Developers on Stack Overflow},
year={2019},
volume={7},
number={},
pages={61145-61169},
abstract={Context: Mobile application developers are getting more concerned due to the importance of quality requirements or non-functional requirements (NFR) in software quality. Developers around the globe are actively asking a question(s) and sharing solutions to the problems related to software development on Stack Overflow (SO). The knowledge shared by developers on SO contains useful information related to software development such as feature requests (functional/non-functional), code snippets, reporting bugs or sentiments. Extracting the NFRs shared by iOS developers on programming Q&A website SO has become a challenge and a less researched area. Objective: To identify and understand the real problems, needs, trends, and the critical NFRs or quality requirements discussed on Stack Overflow related to iOS mobile application development. Method: We extracted and used only the iOS posts data of SO. We applied the well-known statistical topical model Latent Dirichlet Allocation (LDA) to identify the main topics in iOS posts on SO. Then, we labeled the extracted topics with quality requirements or NFRs by using the wordlists to assess the trend, evolution, hot and unresolved NFRs in all iOS discussions. Results: Our findings revealed that the highly frequent topics the iOS developers discussed are related to usability, reliability, and functionality followed by efficiency. Interestingly, the most problematic areas unresolved are also usability, reliability, and functionality though followed by portability. Besides, the evolution trend of each of the six different quality requirements or NFRs over time is depicted through comprehensive visualization. Conclusion: Our first empirical investigation on approximately 1.5 million iOS posts and comments of SO gives insight on comprehending the NFRs in iOS application development through the lens of real-world practitioners.},
keywords={Software;Market research;Data mining;Application programming interfaces;Mobile communication;Mobile applications;Technological innovation;Non-functional requirements (NFRs);quality requirements;iOS;Latent Dirichlet allocation (LDA);Stack Overflow},
doi={10.1109/ACCESS.2019.2914429},
ISSN={2169-3536},
month={},}
@ARTICLE{8701671,
author={Papież, Bartłomiej W. and Markelc, Boštjan and Brown, Graham and Muschel, Ruth J. and Brady, Sir Michael and Schnabel, Julia A.},
journal={IEEE Transactions on Biomedical Engineering},
title={Image-Based Artefact Removal in Laser Scanning Microscopy},
year={2020},
volume={67},
number={1},
pages={79-87},
abstract={Recent developments in laser scanning microscopy have greatly extended its applicability in cancer imaging beyond the visualization of complex biology, and opened up the possibility of quantitative analysis of inherently dynamic biological processes. However, the physics of image acquisition intrinsically means that image quality is subject to a tradeoff between a number of imaging parameters, including resolution, signal-to-noise ratio, and acquisition speed. We address the problem of geometric distortion, in particular, jaggedness artefacts that are caused by the variable motion of the microscope laser, by using a combination of image processing techniques. Image restoration methods have already shown great potential for post-acquisition image analysis. The performance of our proposed image restoration technique was first quantitatively evaluated using phantom data with different textures, and then qualitatively assessed using in vivo biological imaging data. In both cases, the presented method, comprising a combination of image registration and filtering, is demonstrated to have substantial improvement over state-of-the-art microscopy acquisition methods.},
keywords={Microscopy;Lasers;Signal to noise ratio;Laser noise;Biology;Tumors;Image restoration;image processing;laser scanning microscopy},
doi={10.1109/TBME.2019.2908345},
ISSN={1558-2531},
month={Jan},}
@ARTICLE{7314854,
author={Jia, Gangyong and Han, Guangjie and Zhang, Daqiang and Liu, Li and Shu, Lei},
journal={IEEE Access},
title={An Adaptive Framework for Improving Quality of Service in Industrial Systems},
year={2015},
volume={3},
number={},
pages={2129-2139},
abstract={Limited memory bandwidth is considered as the major bottleneck in multimedia cloud computing for more and more virtual machines (VMs) of multimedia processing requiring high memory bandwidth simultaneously. Moreover, contending memory bandwidth among parallel running VMs leads to poor quality of service (QoS) of the multimedia applications, missing the deadlines of these soft real-time multimedia applications. In this paper, we present an adaptive framework, Service Maximization Optimization (SMO), which is designed to improve the QoS of the soft real-time multimedia applications in multimedia cloud computing. The framework consists of an automatic detection mechanism and an adaptive memory bandwidth control mechanism. With the automatic detection mechanism, the critical section to the multimedia application performance in the VMs is detected. Then, our adaptive memory bandwidth control mechanism adjusts the memory access rates of all the parallel running VMs to protect the QoS of the soft real-time multimedia applications. From the case studies with real-world multimedia applications, our SMO significantly improves the QoS of the soft real-time multimedia applications with a negligible penalty on system throughput.},
keywords={Bandwidth allocation;Multimedia communication;Quality of service;Real-time systems;Memory management;Memory bandwidth;multimedia application;memory access;quality of service;soft real-time;Memory bandwidth;multimedia application;memory access;quality of service;soft real-time},
doi={10.1109/ACCESS.2015.2496959},
ISSN={2169-3536},
month={},}
@ARTICLE{9606887,
author={Halawa, Waleed F. and Darwish, Saad M. and Elzoghabi, Adel A.},
journal={IEEE Access},
title={Cotton Warehousing Improvement for Bale Management System Based on Neutrosophic Classifier},
year={2021},
volume={9},
number={},
pages={159413-159420},
abstract={One of the big factors affecting yarn quality is the cotton mix. There is always a considerable variation in the fiber characteristics from one bale to another, even within the same lot. This variation will result in the yarn quality difference, which leads to many fabric defects if the bales are mixed in an uncontrolled manner. The bale management system is based on the categorization of cotton bales according to their fiber quality characteristics. It includes the measurement of the fiber characteristics concerning each bale by using a High Volume Instrument (HVI). The separation of bales into categories for cotton lay-down to achieve balanced bale mixes must be based on a robust clustering algorithm. This paper discusses the utilization of the neutrosophic classifier, for the first time, to categorize the cotton in the warehouse. Although the traditional categorizing method using fuzzy logic came out with some satisfying results, it was missing the way of excluding the outlier’s data points (off-quality bales) which can affect the fabric quality. Neutrosophic classifier deals with cotton bale’s data type by excluding some bale data points that affect the fabric quality through falsity and indeterminacy membership functions to increase the accuracy of the bale management system. Our proposed method has been tested on mill cotton data. The results have been compared with the results of the traditional fuzzy logic algorithms and revealed higher accuracy.},
keywords={Cotton;Fabrics;Clustering algorithms;Yarn;Fuzzy logic;Manuals;Warehousing;Bale management system;cotton lay-down;cotton warehousing;neutrosophic clustering},
doi={10.1109/ACCESS.2021.3126790},
ISSN={2169-3536},
month={},}
@ARTICLE{8974217,
author={Sun, Han and Fan, Yejia and Shen, Jiaquan and Liu, Ningzhong and Liang, Dong and Zhou, Huiyu},
journal={IEEE Access},
title={A Novel Semantics-Preserving Hashing for Fine-Grained Image Retrieval},
year={2020},
volume={8},
number={},
pages={26199-26209},
abstract={With the advent of the era of big data, the storage and retrieval of data have become a research hotspot. Hashing methods that transform high-dimensional data into compact binary codes have received increasing attention. Recently, with the successful application of convolutional neural networks in computer vision, deep hashing methods utilize an end-to-end framework to learn feature representations and hash codes mutually, which achieve better retrieval performance than conventional hashing methods. However, deep hashing methods still face some challenges in image retrieval. Firstly, most existing deep hashing methods preserve similarity between original data space and hash coding space using loss functions with high time complexity, which cannot get a win-win situation in time and accuracy. Secondly, few existing deep hashing methods are designed for fine-grained image retrieval, which is necessary in practice. In this study, we propose a novel semantics-preserving hashing method which solves the above problems. We add a hash layer before the classification layer as a feature switch layer to guide the classification. At the same time, we replace the complicated loss with the simple classification loss, combining with quantization loss and bit balance loss to generate high-quality hash codes. Besides, we incorporate feature extractor designed for fine-grained image classification into our network for better representation learning. The results on three widely-used fine-grained image datasets show that our method is superior to other state-of-the-art image retrieval methods.},
keywords={Feature extraction;Image retrieval;Hash functions;Binary codes;Sun;Quantization (signal);Training data;Deep hashing;fine-grained images;feature switch layer;image retrieval},
doi={10.1109/ACCESS.2020.2970223},
ISSN={2169-3536},
month={},}
@ARTICLE{8995472,
author={Wang, Xiaopeng and Pan, Jeng-Shyang and Chu, Shu-Chuan},
journal={IEEE Access},
title={A Parallel Multi-Verse Optimizer for Application in Multilevel Image Segmentation},
year={2020},
volume={8},
number={},
pages={32018-32030},
abstract={Multi-version optimizer (MVO) inspired by the multi-verse theory is a new optimization algorithm for challenging multiple parameter optimization problems in the real world. In this paper, a novel parallel multi-verse optimizer (PMVO) with the communication strategy is proposed. The parallel mechanism is implemented to randomly divide the initial solutions into several groups, and share the information of different groups after each fixed iteration. This can significantly promote the cooperation individual of MVO algorithm, and reduce the deficiencies that the original MVO is premature convergence, search stagnation and easily trap into local optimal search space. To confirm the performance of the proposed scheme, the PMVO algorithm was compared with the other well-known optimization algorithms, such as gray wolf optimizer (GWO), particle swarm optimization (PSO), multi-version optimizer (MVO), and parallel particle swarm optimization (PPSO) under CEC2013 test suite. The experimental results prove that the PMVO is superior to the other compared algorithms. In addition, PMVO is also applied to solve complex multilevel image segmentation problems based on minimum cross entropy thresholding. The application results appear that the proposed PMVO algorithm can achieve higher quality image segmentation compared to other similar algorithms.},
keywords={Image segmentation;Optimization;Entropy;Particle swarm optimization;Indexes;Convergence;Genetic algorithms;Meta-heuristic optimization;parallel multi-verse optimizer;multilevel image segmentation;minimum cross entropy thresholding},
doi={10.1109/ACCESS.2020.2973411},
ISSN={2169-3536},
month={},}
@ARTICLE{8528424,
author={Sun, Zejun and Wang, Bin and Sheng, Jinfang and Yu, Zhongjing and Shao, Junming},
journal={IEEE Access},
title={Overlapping Community Detection Based on Information Dynamics},
year={2018},
volume={6},
number={},
pages={70919-70934},
abstract={Identifying overlapping communities is essential for analyzing network structures, exploring the interactions of groups, studying network functions, and obtaining insight into the dynamics of networks. Many algorithms have been proposed for detecting overlapping communities but identifying the intrinsic communities is still a non-trivial problem because of the difficulties with parameter tuning, user bias criteria, and the lack of ground truth information. In this paper, we propose a new model called OCDID (Overlapping Community Detection based on Information Dynamics) to uncover the overlapping communities, which treats the network as a dynamical system that allows an individual to communicate and share information with its neighbors. The information flow in the network is controlled by the underlying topology structure (e.g., the community structure), and the community structure is also reflected by the information dynamics. Overlapping nodes act as bridges between multiple communities and the information from multiple communities flows through these nodes. Thus, the overlapping nodes can be identified by analyzing the information flow among communities. In addition, we use the monotone convergence theorem to confirm the convergence of our model. Experiments based on synthetic and real-world networks demonstrate that in most cases, our proposed approach is superior to other representative algorithms in terms of the quality of overlapping community detection.},
keywords={Heuristic algorithms;Partitioning algorithms;Bridges;Steady-state;Convergence;Social network services;Complex networks;Complex network;diffusion;information dynamics;overlapping community detection},
doi={10.1109/ACCESS.2018.2879648},
ISSN={2169-3536},
month={},}
@ARTICLE{9056814,
author={Meng, Hui and Gao, Yuan and Yang, Xin and Wang, Kun and Tian, Jie},
journal={IEEE Transactions on Medical Imaging},
title={K-Nearest Neighbor Based Locally Connected Network for Fast Morphological Reconstruction in Fluorescence Molecular Tomography},
year={2020},
volume={39},
number={10},
pages={3019-3028},
abstract={Fluorescence molecular tomography (FMT) is a highly sensitive and noninvasive imaging modality for three-dimensional visualization of fluorescence probe distribution in small animals. However, the simplified photon propagation model and ill-posed inverse problem limit the improvement of FMT reconstruction. In this work, we proposed a novel K-nearest neighbor based locally connected (KNN-LC) network to improve the performance of morphological reconstruction in FMT. It directly builds the inverse process of photon transmission by learning the mapping relation between the surface photon intensity and the distribution of fluorescent source. KNN-LC network cascades a fully connected (FC) sub-network with a locally connected (LC) sub-network, where the FC part provides a coarse reconstruction result and LC part fine-tunes the morphological quality of reconstructed result. To assess the performance of our proposed network, we implemented both numerical simulation and in vivo studies. Furthermore, split Bregman-resolved total variation (SBRTV) regularization method and inverse problem simulation (IPS) method were utilized as baselines in all comparisons. The results demonstrated that KNN-LC network achieved accurate reconstruction in both source localization and morphology recovery in a short time. This promoted the in vivo application of FMT for visualizing the distribution of biomarkers inside biological tissue.},
keywords={Image reconstruction;Photonics;Fluorescence;Inverse problems;Surface reconstruction;In vivo;Surface morphology;Fluorescence tomography;machine learning;brain},
doi={10.1109/TMI.2020.2984557},
ISSN={1558-254X},
month={Oct},}
@ARTICLE{9178818,
author={Hou, Yan-E and Dang, Lanxue and Dong, Weichuan and Kong, Yunfeng},
journal={IEEE Access},
title={A Metaheuristic Algorithm for Routing School Buses With Mixed Load},
year={2020},
volume={8},
number={},
pages={158293-158305},
abstract={Designing a system to solve school bus routing problems (SBRP), especially in a large school district, is very complex and expensive. One of the challenges resides in designing routes for the school buses when they have mixed loads, where each bus transports students for one or more schools at the same time to save the total number of buses required. This article aims to explore whether the algorithm originally developed for pickup and delivery problem with time windows (PDPTW) can be employed for solving mixed load SBRP. We present a PDPTW-based algorithm to address the mixed load SBRP focusing on minimizing the number of buses required. Our algorithm combines a record-to-record travel framework with three neighborhood operators, single paired insertion, swapping pairs between routes, and within route insertion, to improve solution iteratively. Results from implementing this algorithm show that our PDPTW-based algorithm is feasible for mixed load SBRP. Moreover, we found that guided strategy is better than random for permuting nodes that would be selected to relocate positions in a route or among routes. In addition, the results also show that the spatiotemporal connectivity index used in our algorithm can reduce the computation time needed for searching for solutions without affecting the quality of the solutions.},
keywords={Routing;Load modeling;Vehicle routing;Search problems;Approximation algorithms;Microsoft Windows;Indexes;School bus routing problem;mixed load;pickup and delivery problem with time windows;record to record travel},
doi={10.1109/ACCESS.2020.3019806},
ISSN={2169-3536},
month={},}
@ARTICLE{9108246,
author={Yao, Zisheng and Fan, Shangchun and Nakajima, Yoshikazu and Qu, Xiaolei},
journal={IEEE Access},
title={A Combined Regularization Method Using Prior Structural Information for Sound-Speed Image Reconstruction of Ultrasound Computed Tomography},
year={2020},
volume={8},
number={},
pages={106832-106842},
abstract={Ultrasound computed tomography (USCT) is a promising technique for breast imaging. It provides three modalities: echo image, sound speed image (SSI), and attenuation image. The echo image reveals structural details of the breast with high resolution but is not quantitative. The SSI is quantitative, but its resolution is generally lower than that of the echo image. The SSI reconstruction is an ill-posed problem, thus the Tikhonov or total variation (TV) regularization methods are generally used. Tikhonov regularization is stable, but it tends to smooth the SSI and results in low resolution. TV regularization can preserve the sharp edges and provide higher resolution, but it may result in staircase artifacts. To combine the advantages of Tikhonov and TV regularizations, this article proposes a combined regularization method using prior structural information from the echo image. The proposed method mainly includes three steps. First, the echo image is reconstructed using the synthetic aperture (SA) technique and then segmented into breast and water regions. The segmentation result is used as prior structural information. Second, the USCT sound propagation forward model was built. Finally, with a penalty term according to the prior structural information, the combined regularization method is used to reconstruct the SSI. Both simulation and ex vivo experiments were conducted for evaluation. In the simulation, the proposed method has a low root-mean-square-error (13.78 m/s), a high correlation coefficient (0.812), a high structural similarity (0.755), and a low standard deviation of water sound speed (2.33 m/s). In the ex vivo experiment, the method has a low standard deviation of water sound speed (1.44 and 3.13 m/s for small and large objects, respectively), a low contrast to noise ratio (3.09 and 5.08), and a high Dice coefficient (0.92 and 0.97). Thus, the proposed combined regularization method using prior structural information can improve reconstruction quality.},
keywords={Image reconstruction;Ultrasonic imaging;Imaging;Transducers;Image resolution;Breast cancer;Ultrasound computed tomography;sound speed image;combined regularization;Tikhonov regularization;total variation regularization},
doi={10.1109/ACCESS.2020.3000062},
ISSN={2169-3536},
month={},}
@ARTICLE{9373305,
author={Alnafessah, Ahmad and Gias, Alim Ul and Wang, Runan and Zhu, Lulai and Casale, Giuliano and Filieri, Antonio},
journal={IEEE Access},
title={Quality-Aware DevOps Research: Where Do We Stand?},
year={2021},
volume={9},
number={},
pages={44476-44489},
abstract={DevOps is an emerging paradigm that reduces the barriers between developers and operations teams to offer continuous fast delivery and enable quick responses to changing requirements within the software life cycle. A significant volume of activity has been carried out in recent years with the aim of coupling DevOps stages with tools and methods to improve the quality of the produced software and the underpinning delivery methodology. While the research community has produced a sustained effort by conducting numerous studies and innovative development tools to support quality analyses within DevOps, there is still a limited cohesion between the research themes in this domain and a shortage of surveys that holistically examine quality engineering work within DevOps. In this paper, we address the gap by comprehensively surveying existing efforts in this area, categorizing them according to the stage of the DevOps lifecycle to which they primarily contribute. The survey holistically spans across all the DevOps stages, identify research efforts to improve architectural design, modeling and infrastructure-as-code, continuous-integration/continuous-delivery (CI/CD), testing and verification, and runtime management. Our analysis also outlines possible directions for future work in quality-aware DevOps, looking in particular at AI for DevOps and DevOps for AI software.},
keywords={Software;Testing;Artificial intelligence;Computer architecture;Tools;Production;Software architecture;DevOps;CI/CD;infrastructure as code;testing;artificial intelligence;verification},
doi={10.1109/ACCESS.2021.3064867},
ISSN={2169-3536},
month={},}
@ARTICLE{8844722,
author={Xiong, Xiangguang},
journal={IEEE Access},
title={Novel Scheme of Reversible Watermarking With a Complementary Embedding Strategy},
year={2019},
volume={7},
number={},
pages={136592-136603},
abstract={In this paper, a new scheme of reversible watermarking is proposed using a complementary embedding strategy in the spatial domain. The proposed scheme consists of two stages: horizontal direction embedding and vertical direction embedding. A complementary embedding strategy is designed to increase the embedding capacity and decrease the distortion of the watermarked image in the vertical direction embedding. Specifically, in the horizontal direction, the proposed scheme embeds one secret data bit by increasing values of pixels in even rows and decreasing values of pixels in odd rows by one. In the vertical direction, it embeds another secret data bit by decreasing values of pixels in even rows and increasing values of pixels in odd rows by one. In addition, a histogram shrinkage technique is adopted to prevent overflow and underflow problems. Experimental results demonstrate that the proposed reversible watermarking scheme outperforms state-of-the-art methods in terms of both embedding capacity and watermarked image quality.},
keywords={Image coding;Watermarking;Histograms;Visualization;Distortion;Airplanes;Image quality;Complementary embedding strategy;prediction error;reversible watermarking},
doi={10.1109/ACCESS.2019.2942449},
ISSN={2169-3536},
month={},}
@ARTICLE{8732318,
author={Li, Yating and Cai, Jianghui and Yang, Haifeng and Zhang, Jifu and Zhao, Xujun},
journal={IEEE Access},
title={A Novel Algorithm for Initial Cluster Center Selection},
year={2019},
volume={7},
number={},
pages={74683-74693},
abstract={As one of the most important techniques in data mining, clustering has always been highly concerned. Most clustering algorithms have encountered challenges, such as the difficulty of cluster centers selection, the artificial determination of the number of clusters K, low accuracy of clustering, and uneven clustering efficiency of different data sets. Considering the difficulty of cluster centers chosen, a new algorithm of fast selecting the initial cluster centers is proposed in this paper. Generally, cluster centers are those data points with higher density, smaller radius threshold and far away from each other, this method uses MNN (M nearest neighbors), density and distance to determine the initial cluster centers. First, the neighborhood radius r of each point is measured by MNN based on distance, and the average value of all r is marked as r̅; second, the densities ρ of each point in the region within r̅ are calculated; and then, factor f is defined to describe the probability that points become cluster centers, based on which, the initial cluster centers are determined by the candidates with bigger f . In the end, the method proposed in this paper is tested by using 12 groups of typical benchmark data sets and applied in the stellar spectral data of LAMOST survey. The experiment results compared with the other six algorithms indicate that the initial cluster centers obtained by this method are of higher quality than that of the six algorithms. Meanwhile, the initial cluster centers of spectral data are of good agreement with the actual stellar classifications.},
keywords={Clustering algorithms;Clustering methods;Partitioning algorithms;Data mining;Heuristic algorithms;Benchmark testing;Convergence;Clustering;data mining;initial cluster centers;stellar spectra},
doi={10.1109/ACCESS.2019.2921320},
ISSN={2169-3536},
month={},}
@ARTICLE{9139220,
author={Alarifi, Abdulaziz and Sankar, Syam and Altameem, Torki and Jithin, K. C. and Amoon, Mohammed and El-Shafai, Walid},
journal={IEEE Access},
title={A Novel Hybrid Cryptosystem for Secure Streaming of High Efficiency H.265 Compressed Videos in IoT Multimedia Applications},
year={2020},
volume={8},
number={},
pages={128548-128573},
abstract={In this modernistic age of innovative technologies like big data processing, cloud computing, and Internet of things, the utilization of multimedia information is growing daily. In contrast to other forms of multimedia, videos are extensively utilized and streamed over the Internet and communication networks in numerous Internet of Multimedia Things (IoMT) applications. Consequently, there is an immense necessity to achieve secure video transmission over modern communication networks due to the third-party exploitation and falsification of transmitted and stored digital multimedia data. The present methods for secure communication of multimedia content between clouds and mobile devices have constraints in terms of processing load, memory support, data size, and battery power. These methods are not the optimum solutions for large-sized multimedia content and are not appropriate for the restricted resources of mobile devices and clouds. The High-Efficiency Video Coding (HEVC) is the latest and modern video codec standard introduced for efficiently storing and streaming of high-resolution videos with suitable size and higher quality. In this paper, a novel hybrid cryptosystem combining DNA (Deoxyribonucleic Acid) sequences, Arnold chaotic map, and Mandelbrot sets is suggested for secure streaming of compressed HEVC streams. Firstly, the high-resolution videos are encoded using the H.265/HEVC codec to achieve efficient compression performance. Subsequently, the suggested Arnold chaotic map ciphering process is employed individually on three channels (Y, U, and V) of the compressed HEVC frame. Then, the DNA encoding sequences are established on the primary encrypted frames resulted from the previous chaotic ciphering process. After that, a modified Mandelbrot set-based conditional shift process is presented to effectively introduce confusion features on the Y, U, and V channels of the resulted ciphered frames. Massive simulation results and security analysis are performed to substantiate that the suggested HEVC cryptosystem reveals astonishing robustness and security accomplishment in contrast to the literature cryptosystems.},
keywords={Cryptography;Videos;Streaming media;Chaotic communication;DNA;Multimedia communication;Video cryptography;H265-HEVC;DNA;Mandelbrot sets;IoMT;Arnold chaotic map},
doi={10.1109/ACCESS.2020.3008644},
ISSN={2169-3536},
month={},}
@ARTICLE{9166510,
author={Chen, Zhaoya and Wang, Daobo and Ma, Lijun and Chen, Yaheng},
journal={IEEE Access},
title={Internet of Things Technology in Monitoring System of Sustainable Use of Soil and Land Resources},
year={2020},
volume={8},
number={},
pages={152932-152940},
abstract={With the development of global economy, the sustainable use of land resources has become a key research topic in the current social and economic development. To do a good job in the sustainable use of land resources can solve such problems as waste of land resources, quality degradation, environmental damage, ecological imbalance and so on. How to realize the sustainable utilization of resources, the first thing to be solved is the sustainable monitoring of land resources. At present, many researches have been done in this field at home and abroad, but in conclusion, there are some problems, that is, the basic theory is not deep and the practical application is less. Therefore, this paper will study the application of the monitoring system of the sustainable use of soil and land resources under the support of the Internet of things big data technology, and establish a practical monitoring system of the sustainable use of land resources. The monitoring system in this paper makes full use of modern science and technology. On the basis of intelligent technology, combined with the actual situation of land resources in China, according to the practical problems, the construction principles of evaluation index system and index construction are established. In the aspect of monitoring system construction, we have greatly optimized the traditional construction method, which makes the system in this paper have better adaptability, scalability and accuracy. In order to further verify the reliability of the system, this paper carries out the performance test, accuracy test, and monitoring test in extremely harsh environment. Finally, the data show that the system can be applied to most of the land environment to monitor the sustainability of land resources.},
keywords={Monitoring;Internet of Things;Indexes;Economics;Sustainable development;Soil;Sensors;Internet of Things Technology;land resources;sustainable utilization;Internet of Things monitoring system},
doi={10.1109/ACCESS.2020.3016303},
ISSN={2169-3536},
month={},}
@ARTICLE{8892516,
author={Liang, Tingting and Chen, Yishan and Gao, Wei and Chen, Ming and Zheng, Meilian and Wu, Jian},
journal={IEEE Access},
title={Exploiting User Tagging for Web Service Co-Clustering},
year={2019},
volume={7},
number={},
pages={168981-168993},
abstract={We propose a novel Web services clustering framework by considering the word distribution of WSDL documents and tags. Typically, tags are annotated to Web services by users for organization. In this paper, four strategies are proposed to integrate the tagging data and WSDL documents in the process of service clustering. Tagging data is inherently uncontrolled, ambiguous, and overly personalized. Two tag recommendation approaches are proposed to improve the tagging data quality and service clustering performance. Comprehensive experiments demonstrate the effectiveness of the proposed framework using a real-world dataset.},
keywords={Web services;Tagging;Search engines;Clustering algorithms;Task analysis;Feature extraction;Web service;WSDL documents clustering;co-clustering;tag recommendation},
doi={10.1109/ACCESS.2019.2950355},
ISSN={2169-3536},
month={},}
@ARTICLE{9783176,
author={Gantassi, Rahma and Masood, Zaki and Choi, Yonghoon},
journal={IEEE Access},
title={Enhancing QoS and Residual Energy by Using of Grid-Size Clustering, K-Means, and TSP Algorithms With MDC in LEACH Protocol},
year={2022},
volume={10},
number={},
pages={58199-58211},
abstract={Some recent researches have shown that the energy consumption problem caused by data collection in a wireless sensor network (WSN) based on a static data collector is a main threat to the network lifetime. However, with the progress of the mobile terminal technology, the implementation of mobile data collectors (MDCs) has become more popular in large-scale WSNs, but it remains a big problem to improve the Quality of Service (QoS) criteria and minimize the energy consumption at the same time. However, most existing systems based on MDCs do not successfully strike a balance between routing energy consumption and QoS. In addition, most WSN protocols fail to maintain their impact when the network topology changes. Thus, for a dynamic WSN, it is important to support an intelligent MDC to continue data propagation despite the inevitable changes in the WSN topology. Considering all the above challenges, we propose a new intelligent MDC based on the traveling salesman problem (TSP) to determine the optimal path traveled by the MDC for energy efficiency and latency. Specifically, our proposed Mobile Data Collectors-Traveling Salesman Problem-Low Energy Adaptive Clustering Hierarchy-K-Means (MDC-TSP-LEACH-K) protocol uses K-Means and Grid clustering algorithm to decrease energy consumption in the cluster head (CH) election phase. Additionally, MDC is utilized as an intermediate between CH and the sink to further enhance the QoS of WSNs, to reduce delays while collecting data, and improve the transmission phase of the LEACH protocol.},
keywords={Clustering algorithms;Wireless sensor networks;Quality of service;Energy consumption;Routing protocols;Throughput;Voting;Energy consumption;large-scale wireless sensor networks;optimal path;QoS},
doi={10.1109/ACCESS.2022.3178434},
ISSN={2169-3536},
month={},}
@ARTICLE{9138396,
author={Suganeshwari, G. and Ibrahim, S. P. Syed},
journal={IEEE Access},
title={Rule-Based Effective Collaborative Recommendation Using Unfavorable Preference},
year={2020},
volume={8},
number={},
pages={128116-128123},
abstract={The biggest challenge in collaborative filtering recommendation system research is data sparsity; it mainly occurs as user rates very few items from widely available options. Data Imputation techniques address the data sparsity problem by filling the missing values and then predicting the likeliness of the user. Most of the existing imputation systems assign high ratings to the items or incorporate additional information to enhance the performance of collaborative filtering recommendations. This paper proposes an association rule-based imputation method (RUBLE) to improve the top-N prediction performance of the collaborative filtering recommendation. The proposed method identifies the unfavorable items of each user using the association rule mining technique and imputes them with low values. The proposed method not only addresses the sparsity problem but also provides a better quality of recommendation by eliminating the unfavorable items in top-N predictions. Existing collaborative methods can quickly adapt to the proposed method as it is method agnostic. The experimental results show that the proposed method enhances the accuracy of the traditional recommender methods by two times on average and significantly outperforms existing imputation based approaches.},
keywords={Collaboration;Motion pictures;Sparse matrices;Recommender systems;Object recognition;Prediction algorithms;Task analysis;Association-rule mining;collaborative filtering;data sparsity;recommendation system;unfavourable items},
doi={10.1109/ACCESS.2020.3008514},
ISSN={2169-3536},
month={},}
@ARTICLE{8600738,
author={Li, Xiang and Wang, Qixu and Lan, Xiao and Chen, Xingshu and Zhang, Ning and Chen, Dajiang},
journal={IEEE Access},
title={Enhancing Cloud-Based IoT Security Through Trustworthy Cloud Service: An Integration of Security and Reputation Approach},
year={2019},
volume={7},
number={},
pages={9368-9383},
abstract={The Internet of Things (IoT) provides a new paradigm for the development of heterogeneous and distributed systems, and it has increasingly become a ubiquitous computing service platform. However, due to the lack of sufficient computing and storage resources dedicated to the processing and storage of huge volumes of the IoT data, it tends to adopt a cloud-based architecture to address the issues of resource constraints. Hence, a series of challenging security and trust concerns have arisen in the cloud-based IoT context. To this end, a novel trust assessment framework for the security and reputation of cloud services is proposed. This framework enables the trust evaluation of cloud services in order to ensure the security of the cloud-based IoT context via integrating security- and reputation-based trust assessment methods. The security-based trust assessment method employs the cloud-specific security metrics to evaluate the security of a cloud service. Furthermore, the feedback ratings on the quality of cloud service are exploited in the reputation-based trust assessment method in order to evaluate the reputation of a cloud service. The experiments conducted using a synthesized dataset of security metrics and a real-world web service dataset show that our proposed trust assessment framework can efficiently and effectively assess the trustworthiness of a cloud service while outperforming other trust assessment methods.},
keywords={Security;Cloud computing;Quality of service;Internet of Things;Measurement;Computer architecture;Big Data;Cloud-based IoT;cloud service trust assessment;security and reputation assessment;trustworthy cloud service selection},
doi={10.1109/ACCESS.2018.2890432},
ISSN={2169-3536},
month={},}
@ARTICLE{9878068,
author={Yang, Shanshan and Yang, Yu and Zhou, Linna and Zhan, Rui and Man, Yufei},
journal={IEEE Access},
title={Intermediate-Layer Transferable Adversarial Attack With DNN Attention},
year={2022},
volume={10},
number={},
pages={95451-95461},
abstract={The widespread deployment of deep learning models in practice necessitates an assessment of their vulnerability, particularly in security-sensitive areas. As a result, transfer-based adversarial attacks have elicited increasing interest in assessing the security of deep learning models. However, adversarial samples usually exhibit poor transferability over different models because of overfitting of the particular architecture and feature representation of a source model. To address this problem, the Intermediate Layer Attack with Attention guidance (IAA) is proposed to alleviate overfitting and enhance the black-box transferability. The IAA works on an intermediate layer $l$ of the source model. Guided by the model’s attention (i.e., gradients) to the features of layer $l$ , the attack algorithm seeks and undermines the key features that are likely to be adopted by diverse architectures. Significantly, IAA focuses on improving existing white-box attacks without introducing significant visual perceptual quality degradation. Namely, IAA maintains the white-box attack performance of the original algorithm while significantly enhancing its black-box transferability. Extensive experiments on ImageNet classifiers confirmed the effectiveness of our method. The proposed IAA outperformed all state-of-the-art benchmarks in various white-box and black-box settings, i.e., improving the success rate of BIM by 29.65% against normally trained models and 27.16% against defense models.},
keywords={Perturbation methods;Optimization;Deep learning;Feature extraction;Training data;Security;Data models;Adversarial machine learning;Deep learning;adversarial samples;black-box attack;transferability;intermediate layer;attention-guided},
doi={10.1109/ACCESS.2022.3204696},
ISSN={2169-3536},
month={},}
@ARTICLE{9718103,
author={Zaidi, Syed Farhan Alam and Woo, Honguk and Lee, Chan-Gun},
journal={IEEE Access},
title={A Graph Convolution Network-Based Bug Triage System to Learn Heterogeneous Graph Representation of Bug Reports},
year={2022},
volume={10},
number={},
pages={20677-20689},
abstract={Many bugs and defects occur during software testing and maintenance. These bugs should be resolved as soon as possible, to improve software quality. However, bug triage aims to solve these bugs by assigning the reported bugs to an appropriate developer or list of developers. It is an arduous task for a human triager to assign an appropriate developer to a bug report, when there are several developers with different skills, and several automated and semi-automated triage systems have been proposed in the last decade. Some recent techniques have suggested possibilities for the development of an effective triage system. However, these techniques require improvement. In previous work, we proposed a heterogeneous graph representation for bug triage, using word–word edges and word-bug document co-occurrences to build a heterogeneous graph of bug data. Cosine similarity is used to weight the word–word edges. Then, a graph convolution network is used to learn a heterogeneous graph representation. This paper extends our previous work by adopting different similarity metrics and correlation metrics for weighting word–word edges. The method was validated using different small and large datasets obtained from large-scale open-source projects. The top-k accuracy metric was used to evaluate the performance of the bug triage system. The experimental results showed that the point-wise mutual information of the proposed model was better than that of other word–word weighting methods, and our method had better accuracy for large datasets than other recent state-of-the-art methods. The proposed method with point-wise mutual information showed 3% to 6% higher top-1 accuracy than state-of-the-art methods for large datasets.},
keywords={Computer bugs;Feature extraction;Task analysis;Deep learning;Convolutional neural networks;Convolution;Social networking (online);Bug triage;bug report;software maintenance;defect triage;bug assignment;bug report;bug fixer recommendation},
doi={10.1109/ACCESS.2022.3153075},
ISSN={2169-3536},
month={},}
@ARTICLE{8966357,
author={Wang, Na and Cai, Yuanyuan and Fu, Junsong and Chen, Xiqi},
journal={IEEE Access},
title={Information Privacy Protection Based on Verifiable (t, n)-Threshold Multi-Secret Sharing Scheme},
year={2020},
volume={8},
number={},
pages={20799-20804},
abstract={General secret sharing schemes comprise a secret dealer and a number of participants. The dealer can randomly generate a secret and then distribute shares of the secret to the participants. Verifiable multi-secret sharing enables a dealer to share multiple secrets among a group of participants such that the deceptive behaviors of the dealer and the participants can be detected. However, in the absence of secure channels, few verifiable secret sharing schemes can simultaneously share multiple secrets at one time. In this paper, we present an information privacy protection and verifiable multi-secret sharing scheme with a simple structure and high security. Each participant can verify correctness of the share distribution process based on public information published by the dealer and guarantee validity of the received share to avoid offering fake information in the process of restoring the original secret. Our performance and security analysis indicate that the newly proposed scheme is more efficient and practical while maintaining the same level of security compared with similar protocols available.},
keywords={Cryptography;Privacy;Protocols;Resists;Telecommunications;Information security;Privacy protection;verifiable;threshold;multi-secret},
doi={10.1109/ACCESS.2020.2968728},
ISSN={2169-3536},
month={},}
@ARTICLE{9770051,
author={Sugandhika, Chinthani and Ahangama, Supunmali},
journal={IEEE Access},
title={Assessing Information Quality of Wikipedia Articles Through Google’s E-A-T Model},
year={2022},
volume={10},
number={},
pages={52196-52209},
abstract={Along with the emergence of Web 2.0, User Generated Content (UGC) is becoming increasingly important for knowledge sharing. Wikipedia being the world’s largest-ever community-based collaborative encyclopedia, is also one of the biggest UGC databases in the world. Wikipedia is dealing with a significant problem of Information Quality (IQ) because of its open-source and collaborative nature. When carrying out attacks such as link spamming, malicious users take advantage of Wikipedia’s popularity on the World Wide Web (WWW). As a result, Wikipedia is generally not recommended for academic-related work. There are, however, some articles that are both rich in information and quality. Existing approaches for assessing Wikipedia’s IQ involve statistical models and machine learning algorithms; however, the existing models do not produce satisfactory results. In this study, a novel theoretical model based on Google’s E-A-T framework is introduced to assess Wikipedia’s IQ. The model comprises three IQ constructs Expertise, Authority and Trustworthiness. Based on the empirical findings and study results, a set of IQ dimensions that influence the above three IQ constructs, as well as 45 IQ attributes to measure the IQ dimensions, were identified. The IQ attributes were automatically and inexpensively extracted from the content and meta-data statistics of Wikipedia articles using a Selenium 3.14 web automation script. A sample of 2000 articles comprising 1000 Featured Articles (FA) and 1000 non-FA articles from six WikiProjects was used for the data analysis. The proposed model was compared with three previously published models in terms of classification and clustering accuracy. It received classification and clustering accuracies of 95% and 93% respectively, which is a drastic improvement over the existing models. Furthermore, an average inter-rater agreement of 84% was observed. Thus, the proposed model’s effectiveness is fairly validated by this extensive experiment. This study contributes to the related knowledge area by introducing a novel framework to assess Wikipedia articles’ IQ.},
keywords={Encyclopedias;Internet;Online services;Collaboration;Electronic publishing;Feature extraction;Bibliographies;Authority;expertise;Google’s E-A-T;hybrid approach;information quality;trustworthiness;web 2.0;Wikipedia},
doi={10.1109/ACCESS.2022.3172962},
ISSN={2169-3536},
month={},}
@ARTICLE{9220139,
author={Feng, Jie Xu and Si, Guannan and Zhou, Fengyu},
journal={IEEE Access},
title={Overview and Framework of Quality Service Metrics for Cloud-Based Robotics Platforms},
year={2020},
volume={8},
number={},
pages={185885-185898},
abstract={With the rapid development of big data, cloud computing and other technologies, Cloud-based robotic has become one of the key research directions for service robot, such as used in hospitals. A framework and set of metrics for evaluating the quality of service (QOS) of a cloud robotic platform would be greatly facilitate research into and actual practice of service robots. In this paper, a QOS metrics framework of cloud robotic computing is summarized and the research of components and metrics of a cloud robotic platform is reviewed. QOS metrics are organized into software, network, and robotic services. By summarizing and analyzing the above three groups of metrics, a QOS framework or index system is proposed. Finally, future research towards open source and standardization of components of robotic cloud platform is discussed.},
keywords={Measurement;Quality of service;Software;Cloud computing;Service robots;Reliability;Service robot;cloud robotic platform;quality of service;software;network;metrics framework},
doi={10.1109/ACCESS.2020.3030069},
ISSN={2169-3536},
month={},}
@ARTICLE{9907002,
author={Sverko, Mladen and Grbac, Tihana Galinac and Mikuc, Miljenko},
journal={IEEE Access},
title={SCADA Systems With Focus on Continuous Manufacturing and Steel Industry: A Survey on Architectures, Standards, Challenges and Industry 5.0},
year={2022},
volume={10},
number={},
pages={109395-109430},
abstract={Recent technological advances encompassed by the smart factory concept have fundamentally changed industrial control systems in the way they are structured and how they operate. Majority of these changes affect Supervisory Control And Data Acquisition (SCADA) systems, shifting them to a higher level of interoperability, heterogeneous networks, big data and toward internet technologies and services in general. However, this transformation does not affect all SCADA systems equally. The immediate industrial environment and controlled processes have a significant impact as well. This paper presents a holistic approach to SCADA systems implemented in continuous flow production control within the steel industry production environment. We outline the multi-layer architecture of the SCADA control framework and the aspects of interoperability and interconnectivity within the architecture reference models, together with the research challenges and opportunities arising from the recent rapid increasement of the industrial control systems complexity and digital transformation under the Industry 4.0 paradigm, resulting in disrupting levels of the traditional automation pyramid based on Purdue model toward a higher level of integration and interoperability enabling cross-level data exchange empowered by the Industrial Internet of Things. Furthermore, the paper addresses the problem of proprietary SCADA systems and elaborates the causal correlation between SCADA quality requirements and adoption of new technologies in relation to the specific industrial environment of the steel manufacturing process.},
keywords={SCADA systems;Production processes;Data acquisition;Supervisory control;Manufacturing processes;Smart manufacturing;Steel industry;Process control;Cyber-physical systems;Fourth Industrial Revolution;Fifth Industrial Revolution;Supervisory control and data acquisition;SCADA;supervisory control;data acquisition;industrial process control;cyber-physical;continuous flow production;manufacturing;steel industry;industry 4.0;industry 5.0;smart factory},
doi={10.1109/ACCESS.2022.3211288},
ISSN={2169-3536},
month={},}
@ARTICLE{9051670,
author={Hu, Chunling and Wu, Xiaona and Li, Bixin},
journal={IEEE Access},
title={A Framework for Trustworthy Web Service Composition and Optimization},
year={2020},
volume={8},
number={},
pages={73508-73522},
abstract={Recently, Web service composition and optimization have received an increasing attention from both academia and industrial community. Most current methods have not paid enough attention to the user specific trust requirement for composite services. However, Trust is an important metric to judge whether a composite service can behave as user expected. In this work, firstly, a multifactor concept of trust of composite service is defined based on the trust of component services, interface compatibility and optimal binding schemes. Secondly, a trustworthy Web service composition and optimization framework called TWSCO is proposed to guarantee the trust of composite service and efficiency of Web service composition process. The interface-matching problem among component services and user preference are considered in TWSCO, which firstly uses component services filter to remove untrusted component services. Secondly, the concrete services, based on interface similarity, are organized as clusters. Thirdly, a composite template among component services is formed at the cluster level to guarantee the trust and efficiency of composite service. finally, the best binding scheme is discovered by an optimization method based on user specific trust metrics. In the end, experiments based on real Web services are presented to illustrate the proposed framework TWSCO can effectively guarantee the user preference trust of the composite service.},
keywords={Optimization;Web services;Quality of service;Reliability;Atomic measurements;Service composition;service optimization;trustworthiness;interface matching;optimal binding scheme;composite template},
doi={10.1109/ACCESS.2020.2984648},
ISSN={2169-3536},
month={},}
@ARTICLE{8089328,
author={Sahi, Muneeb Ahmed and Abbas, Haider and Saleem, Kashif and Yang, Xiaodong and Derhab, Abdelouahid and Orgun, Mehmet A. and Iqbal, Waseem and Rashid, Imran and Yaseen, Asif},
journal={IEEE Access},
title={Privacy Preservation in e-Healthcare Environments: State of the Art and Future Directions},
year={2018},
volume={6},
number={},
pages={464-478},
abstract={e-Healthcare promises to be the next big wave in healthcare. It offers all the advantages and benefits imaginable by both the patient and the user. However, current e-Healthcare systems are not yet fully developed and mature, and thus lack the degree of confidentiality, integrity, privacy, and user trust necessary to be widely implemented. Two primary aspects of any operational healthcare enterprise are the quality of healthcare services and patient trust over the healthcare enterprise. Trust is intertwined with issues like confidentiality, integrity, accountability, authenticity, identity, and data management, to name a few. Privacy remains one of the biggest obstacles to ensuring the success of e-Healthcare solutions in winning patient trust as it indirectly covers most security concerns. Addressing privacy concerns requires addressing security issues like access control, authentication, non-repudiation, and accountability, without which end-to-end privacy cannot be ensured. Achieving privacy from the point of data collection in wireless sensor networks, to incorporating the Internet of Things, to communication links, and to data storage and access, is a huge undertaking and requires extensive work. Privacy requirements are further compounded by the fact that the data handled in an enterprise are of an extremely personal and private nature, and its mismanagement, either intentionally or unintentionally, could seriously hurt both the patient and future prospects of an e-Healthcare enterprise. Research carried out in order to address privacy concerns is not homogenous in nature. It focuses on the failure of certain parts of the e-Healthcare enterprise to fully address all aspects of privacy. In the middle of this ongoing research and implementation, a gradual shift has occurred, moving e-Healthcare enterprise controls away from an organizational level toward the level of patients. This is intended to give patients more control and authority over decision making regarding their protected health information/electronic health record. A lot of works and efforts are necessary in order to better assess the feasibility of this major shift in e-Healthcare enterprises. Existing research can be naturally divided on the basis of techniques used. These include data anonymization/pseudonymization and access control mechanisms primarily for stored data privacy. This, however, results in giving a back seat to certain privacy requirements (accountability, integrity, non-repudiation, and identity management). This paper reviews research carried out in this regard and explores whether this research offers any possible solutions to either patient privacy requirements for e-Healthcare or possibilities for addressing the (technical as well as psychological) privacy concerns of the users.},
keywords={Privacy;Medical services;Electronic mail;Wireless sensor networks;Access control;Data privacy;e-Healthcare;privacy;anonymity;access control},
doi={10.1109/ACCESS.2017.2767561},
ISSN={2169-3536},
month={},}
@ARTICLE{9481139,
author={Zhu, Yi and Yang, Yang and Li, Yun and Qiang, Jipeng and Yuan, Yunhao and Zhang, Runmei},
journal={IEEE Access},
title={Representation Learning With Dual Autoencoder for Multi-Label Classification},
year={2021},
volume={9},
number={},
pages={98939-98947},
abstract={Multi-label classification aims to deal with the problem that an object may be associated with one or more labels, which is a more difficult task due to the complex nature of multi-label data. The crucial problem of multi-label classification is the more robust and higher-level feature representation learning, which can reduce non-helpful feature attributes from the input space prior to training. In recent years, deep learning methods based on autoencoders have achieved excellent performance in multi-label classification for the advantages of powerful representations learning ability and fast convergence speed. However, most existing autoencoder-based methods only rely on the single autoencoder model, which pose challenges for multi-label feature representations learning and fail to measure similarities between data spaces. To address this problem, in this paper, we propose a novel representation learning method with dual autoencoder for multi-label classification. Compared to the existing autoencoder-based methods, our proposed method can capture different characteristics and more abstract features from data by the serially connection of two different types of autoencoders. More specifically, firstly, the algorithm of Reconstruction Independent Component Analysis (RICA) in sparse autoencoder is trained on patches on all training and test dataset for robust global feature representations learning. Secondly, with the output of RICA, stacked autoencoder with manifold regularization (SAMR) is introduced to ameliorate the quality of multi-label features learning. Comprehensive experiments on several real-world data sets demonstrate the effectiveness of our proposed approach compared with several competing state-of-the-art methods.},
keywords={Training;Manifolds;Independent component analysis;Data models;Correlation;Linear programming;Multi-label classification;dual autoencoder;RICA;manifold regularization;representation learning},
doi={10.1109/ACCESS.2021.3096194},
ISSN={2169-3536},
month={},}
@ARTICLE{8576500,
author={Thar, Kyi and Tran, Nguyen H. and Oo, Thant Zin and Hong, Choong Seon},
journal={IEEE Access},
title={DeepMEC: Mobile Edge Caching Using Deep Learning},
year={2018},
volume={6},
number={},
pages={78260-78275},
abstract={Caching popular contents at edge nodes such as base stations is a crucial solution for improving users' quality of services in next-generation networks. However, it is very challenging to correctly predict the future popularity of contents and decide which contents should be stored in the base station cache. Recently, with the advances in big data and high computing power, deep learning models have achieved high prediction accuracy. Hence, in this paper, deep learning is used to learn and predict the future popularity of contents to support cache decision. First, deep learning models are trained and utilized in the cloud data center to make an efficient cache decision. Then, the final cache decision is sent to each base station to store the popular contents proactively. The proposed caching scheme involves three distinct parts: 1) predicting the future class label of each content; 2) predicting the future popularity score of contents based on the predicted class label; and 3) caching the predicted contents with high popularity scores. The prediction models using the Keras and Tensorflow libraries are implemented in this paper. Finally, the performance of the caching schemes is tested with a Python-based simulator. In terms of a cache hit, simulation results show that the proposed scheme outperforms 38%, convolutional recurrent neural network-based scheme outperforms 33%, and convolutional neural network-based scheme outperforms 25% compared to the baseline scheme.},
keywords={Predictive models;Computational modeling;Data models;Videos;Training;Base stations;Mobile edge computing;deep learning;proactive caching;prediction model searching},
doi={10.1109/ACCESS.2018.2884913},
ISSN={2169-3536},
month={},}
@ARTICLE{8653381,
author={Shi, Peining and Zhang, Zhiyong and Choo, Kim-Kwang Raymond},
journal={IEEE Access},
title={Detecting Malicious Social Bots Based on Clickstream Sequences},
year={2019},
volume={7},
number={},
pages={28855-28862},
abstract={With the significant increase in the volume, velocity, and variety of user data (e.g., user-generated data) in online social networks, there have been attempted to design new ways of collecting and analyzing such big data. For example, social bots have been used to perform automated analytical services and provide users with improved quality of service. However, malicious social bots have also been used to disseminate false information (e.g., fake news), and this can result in real-world consequences. Therefore, detecting and removing malicious social bots in online social networks is crucial. The most existing detection methods of malicious social bots analyze the quantitative features of their behavior. These features are easily imitated by social bots; thereby resulting in low accuracy of the analysis. A novel method of detecting malicious social bots, including both features selection based on the transition probability of clickstream sequences and semi-supervised clustering, is presented in this paper. This method not only analyzes transition probability of user behavior clickstreams but also considers the time feature of behavior. Findings from our experiments on real online social network platforms demonstrate that the detection accuracy for different types of malicious social bots by the detection method of malicious social bots based on transition probability of user behavior clickstreams increases by an average of 12.8%, in comparison to the detection method based on quantitative analysis of user behavior.},
keywords={Feature extraction;Twitter;Real-time systems;Security;IEEE Senior Members;Big Data;Online social network;social bots;user behavior;semi-supervised clustering},
doi={10.1109/ACCESS.2019.2901864},
ISSN={2169-3536},
month={},}
@ARTICLE{9420732,
author={Li, Baosheng and Qin, Chuandong},
journal={IEEE Access},
title={Predictive Analytics for Octane Number: A Novel Hybrid Approach of KPCA and GS-PSO-SVR Model},
year={2021},
volume={9},
number={},
pages={66531-66541},
abstract={Octane number is the most important indicator of reflecting the combustion performance, and a great deal of research has been devoted to improving it. In this paper, a new analytical framework is proposed to predict octane number, kernel principal component analysis (KPCA) is used to reduce the dimension of the variables in the process of Fluid Catalytic Cracking (FCC), support vector regression (SVR) is used to construct the gasoline octane number prediction model and the particle swarm optimization algorithm (PSO) is used to select the optimal combination of parameters for the model. The experiments show that the octane number can be improved under a given production environment with a guaranteed desulfurization effect of gasoline products. Furthermore, several key attributes that have a significantly positive or negative correlation with the improvement of gasoline product quality are identified through computing the feature score. The findings can help engineers adjust operational variables to obtain a series of high-quality products.},
keywords={Support vector machines;Petroleum;Prediction algorithms;FCC;Principal component analysis;Predictive models;Spectroscopy;Gasoline octane number;kernel principal component analysis;support vector regression;particle swarm optimization},
doi={10.1109/ACCESS.2021.3077028},
ISSN={2169-3536},
month={},}
@ARTICLE{9399408,
author={Nguyen, Minh Hieu and Le Nguyen, Phi and Nguyen, Kien and Le, Van An and Nguyen, Thanh-Hung and Ji, Yusheng},
journal={IEEE Access},
title={PM2.5 Prediction Using Genetic Algorithm-Based Feature Selection and Encoder-Decoder Model},
year={2021},
volume={9},
number={},
pages={57338-57350},
abstract={The concentration of fine particulate matter (PM2.5), which represents inhalable particles with diameters of 2.5 micrometers and smaller, is a vital air quality index. Such particles can penetrate deep into the human lungs and severely affect human health. This paper studies accurate PM2.5 prediction, which can potentially contribute to reducing or avoiding the negative consequences. Our approach’s novelty is to utilize the genetic algorithm (GA) and an encoder-decoder (E-D) model for PM2.5 prediction. The GA benefits feature selection and remove outliers to enhance the prediction accuracy. The encoder-decoder model with long short-term memory (LSTM), which relaxes the restrictions between the input and output of the model, can be used to effectively predict the PM2.5 concentration. We evaluate the proposed model on air quality datasets from Hanoi and Taiwan. The evaluation results show that our model achieves excellent performance. By merely using the E-D model, we can obtain more accurate (up to 53.7%) predictions than those of previous works. Moreover, the GA in our model has the advantage of obtaining the optimal feature combination for predicting the PM2.5 concentration. By combining the GA-based feature selection algorithm and the E-D model, our proposed approach further improves the accuracy by at least 13.7%.},
keywords={Predictive models;Atmospheric modeling;Genetic algorithms;Feature extraction;Air quality;Decoding;Statistics;PM 25;genetic algorithm;feature selection;long short-term memory;encoder-decoder model},
doi={10.1109/ACCESS.2021.3072280},
ISSN={2169-3536},
month={},}
@ARTICLE{9218922,
author={Abdelrahman, Osama and Keikhosrokiani, Pantea},
journal={IEEE Access},
title={Assembly Line Anomaly Detection and Root Cause Analysis Using Machine Learning},
year={2020},
volume={8},
number={},
pages={189661-189672},
abstract={Anomaly detection is becoming widely used in Manufacturing Industry to enhance product quality. At the same time, it plays a great role in several other domains due to the fact that anomaly may reveal rare but represent an important phenomenon. The objective of this paper is to detect anomalies and identify the possible variables that caused these anomalies on historical assembly data for two series of products. Multiple anomaly detection techniques were performed; HBOS, IForest, KNN, CBLOF, OCSVM, LOF, and ABOD. Moreover, we used AUROC and Rank Power as performance metrics, followed by Boosting ensemble learning method to ensure the best anomaly detectors robustness. The techniques that gave the highest performance are KNN, ABOD for both product series datasets with 0.95 and 0.99 AUROC respectively. Finally, we applied a statistical root cause analysis on the detected anomalies with the use of Pareto chart to visualize the frequency of the possible causes and its cumulative occurrence. The results showed that there are seven rejection causes for both product series, whereas the first three causes are responsible for 85% of the rejection rates. Besides, assembly machines engineers reported a significant reduction in the rejection rates in both assembly machines after tuning the specification limits of the rejection causes identified by this research results.},
keywords={Anomaly detection;Machine learning;Measurement;Product design;Inspection;Manufacturing industries;Anomaly detection;assembly lines;big data;machine learning;manufacturing industries;root cause analysis;unsupervised learning},
doi={10.1109/ACCESS.2020.3029826},
ISSN={2169-3536},
month={},}
@ARTICLE{8502208,
author={Al-Shabandar, Raghad and Hussain, Abir Jaafar and Liatsis, Panos and Keight, Robert},
journal={IEEE Access},
title={Analyzing Learners Behavior in MOOCs: An Examination of Performance and Motivation Using a Data-Driven Approach},
year={2018},
volume={6},
number={},
pages={73669-73685},
abstract={Massive open online courses (MOOCs) have been experiencing increasing use and popularity in highly ranked universities in recent years. The opportunity of accessing high quality courseware content within such platforms, while eliminating the burden of educational, financial, and geographical obstacles has led to a rapid growth in participant numbers. The increasing number and diversity of participating learners has opened up new horizons to the research community for the investigation of effective learning environments. Learning Analytics has been used to investigate the impact of engagement on student performance. However, the extensive literature review indicates that there is little research on the impact of MOOCs, particularly in analyzing the link between behavioral engagement and motivation as predictors of learning outcomes. In this paper, we consider a dataset, which originates from online courses provided by Harvard University and the Massachusetts Institute of Technology, delivered through the edX platform. Two sets of empirical experiments are conducted using both statistical and machine learning techniques. Statistical methods are used to examine the association between engagement level and performance, including the consideration of learner educational backgrounds. The results indicate a significant gap between success and failure outcome learner groups, where successful learners are found to read and watch course material to a higher degree. Machine learning algorithms are used to automatically detect learners who are lacking in motivation at an early time in the course, thus providing instructors with insight in regards to student withdrawal.},
keywords={Hidden Markov models;Task analysis;Machine learning;Tools;Education;Discussion forums;Bibliographies;Machine learning;massive open online courses;statistical analysis;big data},
doi={10.1109/ACCESS.2018.2876755},
ISSN={2169-3536},
month={},}
@ARTICLE{9732997,
author={Ruíz, Lluís Mas and Pueyo, Pere Piñol and Mateo-Fornés, Jordi and Mayoral, Jordi Vilaplana and Tehàs, Francesc Solsona},
journal={IEEE Access},
title={Autoscaling Pods on an On-Premise Kubernetes Infrastructure QoS-Aware},
year={2022},
volume={10},
number={},
pages={33083-33094},
abstract={Cloud systems and microservices are becoming powerful tools for businesses. The evidence of the advantages of offering infrastructure, hardware or software as a service (IaaS, PaaS, SaaS) is overwhelming. Microservices and decoupled applications are increasingly popular. These architectures, based on containers, have facilitated the efficient development of complex SaaS applications. A big challenge is to manage and design microservices with a massive range of different facilities, from processing and data storage to computing predictive and prescriptive analytics. Computing providers are mainly based on data centers formed of massive and heterogeneous virtualized systems, which are continuously growing and diversifying over time. Moreover, these systems require integrating into current systems while meeting the Quality of Service (QoS) constraints. The primary purpose of this work is to present an on-premise architecture based on Kubernetes and Docker containers aimed at improving QoS regarding resource usage and service level objectives (SLOs). The main contribution of this proposal is its dynamic autoscaling capabilities to adjust system resources to the current workload while improving QoS.},
keywords={Quality of service;Containers;Cloud computing;Monitoring;Measurement;Microservice architectures;Computer architecture;Cloud;microservices;Kubernetes;SLO;QoS},
doi={10.1109/ACCESS.2022.3158743},
ISSN={2169-3536},
month={},}
@ARTICLE{6522590,
author={Xie, Feng and Chen, Zhen and Xu, Hongfeng and Feng, Xiwei and Hou, Qi},
journal={Tsinghua Science and Technology},
title={TST: Threshold based similarity transitivity method in collaborative filtering with cloud computing},
year={2013},
volume={18},
number={3},
pages={318-327},
abstract={Collaborative filtering solves information overload problem by presenting personalized content to individual users based on their interests, which has been extensively applied in real-world recommender systems. As a class of simple but efficient collaborative filtering method, similarity based approaches make predictions by finding users with similar taste or items that have been similarly chosen. However, as the number of users or items grows rapidly, the traditional approach is suffering from the data sparsity problem. Inaccurate similarities derived from the sparse user-item associations would generate the inaccurate neighborhood for each user or item. Consequently, its poor recommendation drives us to propose a Threshold based Similarity Transitivity (TST) method in this paper. TST firstly filters out those inaccurate similarities by setting an intersection threshold and then replaces them with the transitivity similarity. Besides, the TST method is designed to be scalable with MapReduce framework based on cloud computing platform. We evaluate our algorithm on the public data set MovieLens and a real-world data set from AppChina (an Android application market) with several well-known metrics including precision, recall, coverage, and popularity. The experimental results demonstrate that TST copes well with the tradeoff between quality and quantity of similarity by setting an appropriate threshold. Moreover, we can experimentally find the optimal threshold which will be smaller as the data set becomes sparser. The experimental results also show that TST significantly outperforms the traditional approach even when the data becomes sparser.},
keywords={cloud computing;recommender systems;big data;collaborative filtering;data mining;similaritytransitivity;machine learning;mapReduce;android applications},
doi={10.1109/TST.2013.6522590},
ISSN={1007-0214},
month={June},}
@ARTICLE{8998286,
author={Zhang, Meili and Li, Baizhou},
journal={IEEE Access},
title={How to Improve Regional Innovation Quality From the Perspective of Green Development? Findings From Entropy Weight Method and Fuzzy-Set Qualitative Comparative Analysis},
year={2020},
volume={8},
number={},
pages={32575-32586},
abstract={Innovation is increasingly becoming the most crucial and effective way to tackle climate change and environmental pollution. Recently, innovation quality has received continuous attention from scholars, entrepreneurs and policymakers. It is necessary to measure innovation quality from the perspective of green development. Based on the statistics data of 30 provincial-level innovation systems in China, this study comprehensively evaluates regional innovation quality and identifies the configuration of factors which could lead to high regional innovation quality. Firstly, this study establishes an indicator system of measuring regional innovation quality which includes seven indicators from three aspects of technological, economic and ecological benefit of innovative activities, and entropy weight method is used to comprehensively evaluate regional innovation quality. The result reveals there are big differences between different regions in China in innovation quality. In terms of three economic regions in China, innovation quality in the eastern region is the highest, followed by the central region and the lowest in the western region. Then, fuzzy-set qualitative comparative analysis method is applied, which uncovers three configurations of factors that could lead to high regional innovation quality. Finally, the conclusion of this study provides policy pathways for improving regional innovation quality.},
keywords={Technological innovation;Economics;Green products;Entropy;Government policies;Air pollution;Climate change;Regional innovation quality;configuration of factors;entropy weight method;fuzzy-set qualitative comparative analysis;china;green development perspective},
doi={10.1109/ACCESS.2020.2973703},
ISSN={2169-3536},
month={},}
@ARTICLE{9681298,
author={Wudhikarn, Ratapol and Charoenkwan, Phasit and Malang, Kanokwan},
journal={IEEE Access},
title={Deep Learning in Barcode Recognition: A Systematic Literature Review},
year={2022},
volume={10},
number={},
pages={8049-8072},
abstract={The use of deep learning (DL) for barcode recognition and analysis has achieved remarkable success and has attracted great attention in various domains. Unlike other barcode recognition methods, DL-based approaches can significantly improve the speed and accuracy of both barcode detection and decoding. However, after almost a decade of progress, the current status of DL-based barcode recognition has yet to be thoroughly explored. Specifically, summaries of key insights and gaps remain unavailable in the literature. Therefore, this study aims to comprehensively review recent applications of DL methods in barcode recognition. We mainly conducted a well-constructed systematic literature review (SLR) approach to collect relevant articles and evaluate and summarize the state of the art. This study’s contributions are threefold. First, the paper highlights new DL approaches’ applicability to barcode localization and decoding processes and their potential to either reduce the time required or provide higher quality. Second, another main finding of this study signifies an increasing demand for public and specific barcode datasets that allow DL methods to learn more efficiently in the big data era. Finally, we conclude with a discussion on the crucial challenges of DL with respect to barcode recognition, incorporating promising directions for future research development.},
keywords={Feature extraction;Image recognition;Bibliographies;Task analysis;Deep learning;Decoding;Media;Barcode;barcode recognition;barcode detection;barcode localization;deep learning;literature review},
doi={10.1109/ACCESS.2022.3143033},
ISSN={2169-3536},
month={},}
@ARTICLE{9146141,
author={Wang, Yitu and Nakachi, Takayuki},
journal={IEEE Access},
title={A Privacy-Preserving Learning Framework for Face Recognition in Edge and Cloud Networks},
year={2020},
volume={8},
number={},
pages={136056-136070},
abstract={Offloading the computationally intensive workloads to the edge and cloud not only improves the quality of computation, but also creates an extra degree of diversity by collecting information from devices in service. Nevertheless, significant concerns on privacy are raised as the aggregated information could be misused without the permission by the third party. Sparse coding, which has been successful in computer vision, is finding application in this new domain. In this paper, we develop a secured face recognition framework to orchestrate sparse coding in edge and cloud networks. Specifically, 1). To protect the privacy, a low-complexity encrypting algorithm is developed based on random unitary transform, where its influence on dictionary learning and sparse representation is analysed. Furthermore, it is proved that such influence will not affect the accuracy of face recognition. 2). To fully utilize the multi-device diversity and avoid big data transmission between edge and cloud, a distributed learning framework is established, which extracts deeper features in an intermediate space, expanded according to the dictionaries from each device. Classification is performed in this new feature space to combat the noise and modeling error. Finally, the efficiency and effectiveness of the proposed framework is demonstrated through simulation results.},
keywords={Cloud computing;Servers;Face recognition;Machine learning;Training;Cryptography;Privacy;Information security;edge and cloud networks;face recognition;sparse representation},
doi={10.1109/ACCESS.2020.3011112},
ISSN={2169-3536},
month={},}
@ARTICLE{8654618,
author={Liu, Fagui and Chang, Yufei},
journal={IEEE Access},
title={An Energy Aware Adaptive Kernel Density Estimation Approach to Unequal Clustering in Wireless Sensor Networks},
year={2019},
volume={7},
number={},
pages={40569-40580},
abstract={Energy conservation is one of the most important challenges in wireless sensor networks (WSNs). Therefore, compared with the traditional networks, the WSNs not only need high-quality services with high throughput or low transmission delay, but also pay greater attention to energy utilization to extend network lifetime. The clustering routing algorithm is considered to be among the effective ways to collect and transmit data in WSNs. Cluster head (CH) plays a vital role in the cluster which is in charge of data aggregation and data transmission, so their energy consumption is higher than non-CH nodes. The traditional clustering algorithm tends to have the same size in each cluster. However, due to the randomness of the node distribution, the equal clustering mechanism obviously cannot reduce energy consumption. In order to solve this problem, this paper contributes a new unequal clustering algorithm, an energy-aware adaptive kernel density estimation algorithm (EAKDE), which aims to balance the energy dissipation among the CHs. EAKDE utilizes fuzzy logic to determine the priority of nodes competing for CH. In order to adapt the dynamic change of node conditions, adaptive kernel density estimation algorithm is utilized to assign the appropriate unequal cluster radius to sensor nodes. The simulation results demonstrate that, in different scenarios, EAKDE outperforms the other well-known algorithms in terms of network stability, network lifetime, and energy efficiency.},
keywords={Clustering algorithms;Wireless sensor networks;Routing;Kernel;Estimation;Energy consumption;Fuzzy logic;Unequal clustering;fuzzy logic;kernel density estimation;wireless sensor networks},
doi={10.1109/ACCESS.2019.2902243},
ISSN={2169-3536},
month={},}
@ARTICLE{9133078,
author={Ren, Jianji and Wang, Haichao and Hou, Tingting and Zheng, Shuai and Tang, Chaosheng},
journal={IEEE Access},
title={Collaborative Edge Computing and Caching With Deep Reinforcement Learning Decision Agents},
year={2020},
volume={8},
number={},
pages={120604-120612},
abstract={Large amounts of data will be generated due to the rapid development of the Internet of Things (IoT) technologies and 5th generation mobile networks (5G), the processing and analysis requirements of big data will challenge existing networks and processing platforms. As the most promising technology in 5G networks, edge computing will greatly ease the pressure on network and data processing analysis on the edge. In this paper, we considered the coordination between compute and cache resources between multi-level edge computing nodes (ENs), users under this system can offload computing tasks to ENs to improve quality of service (QoS). We aimed to maximize the long-term profit on the edge, while satisfying the low-latency computing of the users, and jointly optimize the edge-side node offloading strategy and resource allocation. However, it is challenging to obtain an optimal strategy in such a dynamic and complex system. To solve the complex resource allocation problem on the edge and make edge have certain adaptation and cooperation, we used double deep Q-learning (DDQN) to make decisions, ability to maximize long-term gains while making quick decisions. The simulation results prove the effectiveness of DDQN in maximizing revenue when allocation resources on the edge.},
keywords={Edge computing;Collaboration;Resource management;Cloud computing;Task analysis;Reinforcement learning;Internet of Things;Collaborative computing;edge computing;optimization strategy},
doi={10.1109/ACCESS.2020.3007002},
ISSN={2169-3536},
month={},}
@ARTICLE{9430132,
author={Tong, Zhao and Ye, Feng and Yan, Ming and Liu, Hong and Basodi, Sunitha},
journal={Big Data Mining and Analytics},
title={A survey on algorithms for intelligent computing and smart city applications},
year={2021},
volume={4},
number={3},
pages={155-172},
abstract={With the rapid development of human society, the urbanization of the world's population is also progressing rapidly. Urbanization has brought many challenges and problems to the development of cities. For example, the urban population is under excessive pressure, various natural resources and energy are increasingly scarce, and environmental pollution is increasing, etc. However, the original urban model has to be changed to enable people to live in greener and more sustainable cities, thus providing them with a more convenient and comfortable living environment. The new urban framework, the smart city, provides excellent opportunities to meet these challenges, while solving urban problems at the same time. At this stage, many countries are actively responding to calls for smart city development plans. This paper investigates the current stage of the smart city. First, it introduces the background of smart city development and gives a brief definition of the concept of the smart city. Second, it describes the framework of a smart city in accordance with the given definition. Finally, various intelligent algorithms to make cities smarter, along with specific examples, are discussed and analyzed.},
keywords={Smart cities;Urban areas;Cloud computing;Task analysis;Edge computing;Statistics;Sociology;cyber physical systems;Internet of Things (IoT);intelligent computing algorithm;Quality of Service(QoS);smart city},
doi={10.26599/BDMA.2020.9020029},
ISSN={2096-0654},
month={Sep.},}
@ARTICLE{9826662,
author={Igawa, Kousaku and Higa, Kunihiko and Takamiya, Tsutomu},
journal={International Journal of Crowd Science},
title={Utilizing short version big five traits on crowdsouring},
year={2020},
volume={4},
number={2},
pages={117-132},
abstract={Purpose – The purpose of this paper is to examine the efficacy of the Japanese ten-item personality inventory (TIPI-J), a short version of the big five (BF) questionnaire, on crowdsourcing. The BF traits are indicators of personality and are said to be an effective predictor of study performance in various occupations. BF can be used in crowdsourcing to predict crowd workers' performance; however, it will be difficult to use in practice for two reasons like the time-and-effort issue and the bias issue. In this study, an empirical analysis is conducted on crowdsourcing to examine if TIPI-J can solve those issues. Design/methodology/approach – To investigate the issues, two tasks are posted on a crowdsourcing provider. Both TIPI-J and full version BF are conducted before and after selecting crowd workers. Structural validity and convergence validity are tested with correlation analysis between before (TIPI-J) and after (full version BF) data to examine the bias issue. Additionally, those correlations are compared with previous study and significances are examined. Findings – The correlations in “conscientiousness” is 0.45-0.50, respectively, compared with a previous study, those two correlations did not show significance. This indicates that no clear bias exists. Originality/value – This is the first research to investigate the efficacy of TIPI-J on crowdsourcing and showed that TIPI-J can be a useful tool for predicting crowd workers' performance and thus it can help to select appropriate crowd workers.},
keywords={Crowdsourcing;Correlation;Focusing;Estimation;Regression analysis;Resource management;Reliability;Human resource;Quality evaluation;Work performance;Crowdsourcing;Big five;Task assignment},
doi={10.1108/IJCS-11-2019-0031},
ISSN={2398-7294},
month={June},}
@ARTICLE{9097181,
author={Belgaum, Mohammad Riyaz and Musa, Shahrulniza and Alam, Muhammad Mansoor and Su’ud, Mazliham Mohd},
journal={IEEE Access},
title={A Systematic Review of Load Balancing Techniques in Software-Defined Networking},
year={2020},
volume={8},
number={},
pages={98612-98636},
abstract={The traditional networks are facing difficulties in managing the services offered by cloud computing, big data, and the Internet of Things as the users have become more dependent on their services. Software-Defined Networking (SDN) has pulled enthusiasm in the integration process of technologies and function as per the user's requirements for both academia and industry, and it has begun to be embraced in actual framework usage. The emergence of SDN has given another idea to empower the focal programmability of the system. Because of the increasing demand and the scarcity of resources, the load balancing issue needs to be addressed efficiently to manage the incoming traffic and resources and to improve network performance. One of the most critical issues is the role of the controller in SDN to balance the load for having a better Quality of Service (QoS). Though there are few survey articles written on load balancing, there is no detail and systematic review conducted in load balancing in SDN. Hence, this paper extends and reviews the discussion with a taxonomy of current emerging load balancing techniques in SDN systematically by categorizing the techniques as conventional and artificial intelligence-based techniques to improve the service quality. The review also includes the study of metrics and parameters which have been used to measure the performance. This review would allow gaining more information on load balancing approaches in SDN and enables the researchers to fill the current research gaps.},
keywords={Load management;Quality of service;Switches;Systematics;Measurement;Software;Artificial intelligence;conventional;load balancing;review;SDN;software-defined networking;systematic},
doi={10.1109/ACCESS.2020.2995849},
ISSN={2169-3536},
month={},}
@ARTICLE{8710242,
author={Pan, Su and Yan, Yan and Bonsu, Kusi Ankrah and Zhou, Weiwei},
journal={IEEE Access},
title={Resource Allocation Algorithm for MU-MIMO Systems With Double-Objective Optimization Under the Existence of the Rank Deficient Channel Matrix},
year={2019},
volume={7},
number={},
pages={61307-61319},
abstract={This paper proposes a double-objective optimization resource allocation algorithm for the multi-user multiple-input/multiple-output (MU-MIMO) system in the general wireless environment and demonstrates the maximum number of simultaneously supportable users and the achievable bit rates of users in the general wireless environment with full rank and rank-deficient channels. The double-objective joint optimization algorithm proposed in this paper simultaneously optimizes energy efficiency and system throughput by user selection and power allocation. On this basis, the proposed algorithm guarantees the different QoS requirements of various services, including rate requirements and delay requirements.},
keywords={Optimization;Throughput;Resource management;Quality of service;Energy efficiency;Wireless communication;Convex functions;MU-MIMO;rank deficient;double-objective optimization;QoS guarantee;resource allocation},
doi={10.1109/ACCESS.2019.2915573},
ISSN={2169-3536},
month={},}
@ARTICLE{9091940,
author={Mou, Lichao and Lu, Xiaoqiang and Li, Xuelong and Zhu, Xiao Xiang},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={Nonlocal Graph Convolutional Networks for Hyperspectral Image Classification},
year={2020},
volume={58},
number={12},
pages={8246-8257},
abstract={Over the past few years making use of deep networks, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), classifying hyperspectral images has progressed significantly and gained increasing attention. In spite of being successful, these networks need an adequate supply of labeled training instances for supervised learning, which, however, is quite costly to collect. On the other hand, unlabeled data can be accessed in almost arbitrary amounts. Hence it would be conceptually of great interest to explore networks that are able to exploit labeled and unlabeled data simultaneously for hyperspectral image classification. In this article, we propose a novel graph-based semisupervised network called nonlocal graph convolutional network (nonlocal GCN). Unlike existing CNNs and RNNs that receive pixels or patches of a hyperspectral image as inputs, this network takes the whole image (including both labeled and unlabeled data) in. More specifically, a nonlocal graph is first calculated. Given this graph representation, a couple of graph convolutional layers are used to extract features. Finally, the semisupervised learning of the network is done by using a cross-entropy error over all labeled instances. Note that the nonlocal GCN is end-to-end trainable. We demonstrate in extensive experiments that compared with state-of-the-art spectral classifiers and spectral-spatial classification networks, the nonlocal GCN is able to offer competitive results and high-quality classification maps (with fine boundaries and without noisy scattered points of misclassification).},
keywords={Hyperspectral imaging;Convolution;Task analysis;Recurrent neural networks;Semisupervised learning;Support vector machines;Graph convolutional network (GCN);hyperspectral image classification;nonlocal graph;semisupervised learning},
doi={10.1109/TGRS.2020.2973363},
ISSN={1558-0644},
month={Dec},}
@ARTICLE{9085905,
author={Wang, Cheng and Zhu, Hangyu},
journal={IEEE Transactions on Dependable and Secure Computing},
title={Representing Fine-Grained Co-Occurrences for Behavior-Based Fraud Detection in Online Payment Services},
year={2022},
volume={19},
number={1},
pages={301-315},
abstract={The vigorous development of e-commerce breeds cybercrime. Online payment fraud detection, a challenge faced by online service, plays an important role in rapidly evolving e-commerce. Behavior-based methods are recognized as a promising method for online payment fraud detection. However, it is a big challenge to build high-resolution behavioral models by using low-quality behavioral data. In this work, we mainly address this problem from data enhancement for behavioral modeling. We extract fine-grained co-occurrence relationships of transactional attributes by using a knowledge graph. Furthermore, we adopt the heterogeneous network embedding to learn and improve representing comprehensive relationships. Particularly, we explore customized network embedding schemes for different types of behavioral models, such as the population-level models, individual-level models, and generalized-agent-based models. The performance gain of our method is validated by the experiments over the real dataset from a commercial bank. It can help representative behavioral models improve significantly the performance of online banking payment fraud detection. To the best of our knowledge, this is the first work to realize data enhancement for diversified behavior models by implementing network embedding algorithms on attribute-level co-occurrence relationships.},
keywords={Data models;Feature extraction;Predictive models;Heterogeneous networks;Computer crime;Semantics;Online payment services;fraud detection;network embedding;user behavioral modeling},
doi={10.1109/TDSC.2020.2991872},
ISSN={1941-0018},
month={Jan},}
@ARTICLE{9112172,
author={Hu, Wei-Jian and Fan, Jie and Du, Yong-Xing and Li, Bao-Shan and Xiong, Naixue and Bekkering, Ernst},
journal={IEEE Access},
title={MDFC–ResNet: An Agricultural IoT System to Accurately Recognize Crop Diseases},
year={2020},
volume={8},
number={},
pages={115287-115298},
abstract={Crop disease diagnosis is an essential step in crop disease treatment and is a hot issue in agricultural research. However, in agricultural production, identifying only coarse-grained diseases of crops is insufficient because treatment methods are different in different grades of even the same disease. Inappropriate treatments are not only ineffective in treating diseases but also affect crop yield and food safety. We combine IoT technology with deep learning to build an IoT system for crop fine-grained disease identification. This system can automatically detect crop diseases and send diagnostic results to farmers. We propose a multidimensional feature compensation residual neural network (MDFC-ResNet) model for fine-grained disease identification in the system. MDFC-ResNet identifies from three dimensions, namely, species, coarse-grained disease, and fine-grained disease and sets up a compensation layer that uses a compensation algorithm to fuse multidimensional recognition results. Experiments show that the MDFC-ResNet neural network has better recognition effect and is more instructive in actual agricultural production activities than other popular deep learning models.},
keywords={Agriculture;Diseases;Deep learning;Production;Training;Cameras;IoT;multiple crops;fine-grained disease recognition;ResNet;singular value decomposition},
doi={10.1109/ACCESS.2020.3001237},
ISSN={2169-3536},
month={},}
@ARTICLE{8082505,
author={Yang, Wei and Zhang, Huijuan and Yang, Jian and Wu, Jiasong and Yin, Xiangrui and Chen, Yang and Shu, Huazhong and Luo, Limin and Coatrieux, Gouenou and Gui, Zhiguo and Feng, Qianjin},
journal={IEEE Access},
title={Improving Low-Dose CT Image Using Residual Convolutional Network},
year={2017},
volume={5},
number={},
pages={24698-24705},
abstract={Low-dose CT is an effective solution to alleviate radiation risk to patients, it also introduces additional noise and streak artifacts. In order to maintain a high image quality for low-dose scanned CT data, we propose a post-processing method based on deep learning and using 2-D and 3-D residual convolutional networks. Experimental results and comparisons with other competing methods show that the proposed approach can effectively reduce the low-dose noise and artifacts while preserving tissue details. It is also pointed out that the 3-D model can achieve better performance in both edge-preservation and noise-artifact suppression. Factors that may influence the model performance, such as model width, depth, and dropout, are also examined.},
keywords={Convolution;Computed tomography;Three-dimensional displays;Training;Two dimensional displays;Kernel;Solid modeling;Low-dose CT;convolution neural network;residual learning;3D convolution},
doi={10.1109/ACCESS.2017.2766438},
ISSN={2169-3536},
month={},}
@ARTICLE{8746164,
author={Lv, Hongli and Wang, Renfang},
journal={IEEE Access},
title={Denoising 3D Magnetic Resonance Images Based on Low-Rank Tensor Approximation With Adaptive Multirank Estimation},
year={2019},
volume={7},
number={},
pages={85995-86003},
abstract={The magnetic resonance (MR) imaging technique is widely used in clinical diagnosis. Unfortunately, in practice, the MR images inevitably suffer from noise, which severely degrades image quality and accordingly impacts on the accuracy of clinical diagnosis. By exploiting both the nonlocal similarity over space and the inherent correlation across the slices of the 3D MR images, in this paper, we present a novel Rician noise reduction method for the 3D MR images. Specifically, the 3D nonlocal similar patches are first extracted from the input noisy 3D MR data and then grouped to form a noisy fourth-order tensor. Since 3D patches used to construct the fourth-order tensor share similar structures, a latent noise-free tensor can be approximated by a low-rank tensor. To this end, the higher-order singular value decomposition (HOSVD) is adopted to recover the latent noise-free tensor. Furthermore, the rank of each mode of the tensor is adaptively determined by an enhanced low-rank method. The experimental results on synthetic and real 3D MR images show that the proposed method outperforms several state-of-the-art denoising methods in terms of objective metrics and visual inspection.},
keywords={Three-dimensional displays;Noise reduction;Rician channels;Noise measurement;Matrix decomposition;Correlation;3D image denoising;magnetic resonance image;low-rank tensor approximation;nonlocal similarity;higher-order singular value decomposition},
doi={10.1109/ACCESS.2019.2924907},
ISSN={2169-3536},
month={},}
@ARTICLE{9625754,
author={Liu, Menghang and Ma, Haitao and Bai, Yu},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Understanding the Drivers of Land Surface Temperature Based on Multisource Data: A Spatial Econometric Perspective},
year={2021},
volume={14},
number={},
pages={12263-12272},
abstract={Urban thermal condition has seriously affected the quality of residents’ daily life and triggered some environmental issues. Understanding spatial patterns of land surface temperature (LST) and its driving mechanism is important for the sustainable development of cities. Taking Beijing as an example, this study employed spatial econometric models to investigate spatial and temporal heterogeneity of LST from 2014 to 2018 based on multisource remote sensing and statistical data. The global autocorrelation Moran's I index showed the existence of significant positive correlations of LST among regions, indicating the regions with high thermal environments are spatially adjacent. The temperature of a region would increase by more than 0.6% for every 1% increase in LST of surrounding areas based on the spatial Durbin model. In terms of spatial interactions of influencing factors, elevation, normalized difference vegetation index, modified normalized difference water index, nighttime light, and fossil energy consumption of neighbors exhibited significantly positive spatial agglomeration effects on local LST, whereas albedo, GDP, and population density of adjacent areas had negative effects on LST in local areas. Particularly, the indirect effects of drivers were greater than their direct effects, indicating urban thermal condition was an interregional issue and joint control measures should be adopted to mitigate the urban heat island effects as a whole.},
keywords={Land surface temperature;Land surface;Surface topography;Remote sensing;Vegetation mapping;Statistics;Sociology;Beijing;direct and indirect effects;land surface temperature (LST);spatial durbin model (SDM)},
doi={10.1109/JSTARS.2021.3129842},
ISSN={2151-1535},
month={},}
@ARTICLE{8736742,
author={Liu, Yaxi and Huangfu, Wei and Zhang, Haijun and Long, Keping},
journal={IEEE Access},
title={Multi-Criteria Coverage Map Construction Based on Adaptive Triangulation-Induced Interpolation for Cellular Networks},
year={2019},
volume={7},
number={},
pages={80767-80777},
abstract={Wireless cellular communications lead to huge demands for estimating and visualizing the data about Quality of Service (QoS) for mobile network operators in the 5th Generation (5G) networks. Constructing a coverage map is an important step to visualize global information about QoS. Inspired by the characteristic of the base station, we present an adaptive triangulation method to divide the region of interest into triangles. Then, we propose a novel area-wise Multi-criteria Triangulation-induced Interpolation (MTI) algorithm which utilizes the linear interpolation to estimate the key performance indicators of the QoS inside a triangle with the known values of its three vertexes, to construct the coverage maps and provide the closed-form solution of the covered region for the multi-criteria problems. We check the accuracy and the efficiency of the MTI algorithm both in 19-cells network scenario and in real big city scenario. The experiment results manifest that the MTI algorithm shows a good performance in constructing the coverage maps and it is significantly lower-cost and higher-efficiency than the traditional point-wise algorithms.},
keywords={Interpolation;Numerical models;Quality of service;Wireless communication;Adaptation models;Cellular networks;Data visualization;Wireless cellular communication;coverage map;triangulation method;linear interpolation},
doi={10.1109/ACCESS.2019.2923047},
ISSN={2169-3536},
month={},}
@ARTICLE{8606940,
author={Bharill, Neha and Patel, Om Prakash and Tiwari, Aruna and Mu, Lifeng and Li, Dong-Lin and Mohanty, Manoranjan and Kaiwartya, Omprakash and Prasad, Mukesh},
journal={IEEE Access},
title={A Generalized Enhanced Quantum Fuzzy Approach for Efficient Data Clustering},
year={2019},
volume={7},
number={},
pages={50347-50361},
abstract={Data clustering is a challenging task to gain insights into data in various fields. In this paper, an Enhanced Quantum-Inspired Evolutionary Fuzzy C-Means (EQIE-FCM) algorithm is proposed for data clustering. In the EQIE-FCM, quantum computing concept is utilized in combination with the FCM algorithm to improve the clustering process by evolving the clustering parameters. The improvement in the clustering process leads to improvement in the quality of clustering results. To validate the quality of clustering results achieved by the proposed EQIE-FCM approach, its performance is compared with the other quantum-based fuzzy clustering approaches and also with other evolutionary clustering approaches. To evaluate the performance of these approaches, extensive experiments are being carried out on various benchmark datasets and on the protein database that comprises of four superfamilies. The results indicate that the proposed EQIE-FCM approach finds the optimal value of fitness function and the fuzzifier parameter for the reported datasets. In addition to this, the proposed EQIE-FCM approach also finds the optimal number of clusters and more accurate location of initial cluster centers for these benchmark datasets. Thus, it can be regarded as a more efficient approach for data clustering.},
keywords={Clustering algorithms;Qubit;Partitioning algorithms;Proteins;Computer science;Information technology;Clustering;quantum computing;evolutionary algorithm;fuzzy set theory;bioinformatics},
doi={10.1109/ACCESS.2019.2891956},
ISSN={2169-3536},
month={},}
@ARTICLE{9099843,
author={Su, Jindian and Yu, Shanshan and Luo, Da},
journal={IEEE Access},
title={Enhancing Aspect-Based Sentiment Analysis With Capsule Network},
year={2020},
volume={8},
number={},
pages={100551-100561},
abstract={Existing feature-based neural approaches for aspect-based sentiment analysis (ABSA) try to improve their performance with pre-trained word embeddings and by modeling the relations between the text sequence and the aspect (or category), thus heavily depending on the quality of word embeddings and task-specific architectures. Although the recently pre-trained language models, i.e., BERT and XLNet, have achieved state-of-the-art performance in a variety of natural language processing (NLP) tasks, they still subject to the aspect-specific, local feature-aware and task-agnostic challenges. To address these challenges, this paper proposes a XLNet and capsule network based model XLNetCN for ABSA. XLNetCN firstly constructs auxiliary sentence to model the sequence-aspect relation and generate global aspect-specific representations, which enables to enhance aspect-awareness and ensure the full pre-training of XLNet for improving task-agnostic capability. After that, XLNetCN also employs a capsule network with the dynamic routing algorithm to extract the local and spatial hierarchical relations of the text sequence, and yield its local feature representations, which are then merged with the global aspect-related representations for downstream classification via a softmax classifier. Experimental results show that XLNetCN outperforms significantly than the classical BERT, XLNet and traditional feature-based approaches on the two benchmark datasets of SemEval 2014, Laptop and Restaurant, and achieves new state-of-the-art results.},
keywords={Task analysis;Bit error rate;Context modeling;Sun;Sentiment analysis;Neural networks;Training data;Aspect-based sentiment analysis;natural language processing;text analysis;deep learning;neural network},
doi={10.1109/ACCESS.2020.2997675},
ISSN={2169-3536},
month={},}
@ARTICLE{8902108,
author={Crivello, Antonino and Barsocchi, Paolo and Girolami, Michele and Palumbo, Filippo},
journal={IEEE Access},
title={The Meaning of Sleep Quality: A Survey of Available Technologies},
year={2019},
volume={7},
number={},
pages={167374-167390},
abstract={Sleep is an important part of the human daily routine. Restoring sleep is strongly related to a better physical, cognitive, and psychological well-being. By contrast, poor or disordered sleep leads to possible impairments of cognitive and psychological functioning and to a worsened general physical health. In this context, understanding changes in sleep quality becomes a research imperative that leads to the need for the definition of what restoring or quality sleep means. This understanding of what “sleep quality” means requires a cross-domain investigation. It arises the need for a comprehensive study that offers a complete taxonomy of sleep monitoring systems, with a focus on sleep quality, and that gives useful insights about which combination of metrics, signals, and sleep variables is the best in relation to different categories of users. The proposed study is focused on systematically categorizing the methods and approaches for sleep quality understanding, with an emphasis on technological approaches, including wearable, on-bed, and actigraphy devices. It offers a systematic review for researchers who are interested in sleep quality identification tasks, and highlights strengths and weaknesses of state-of-the-art metrics and solutions in order to suggest the best choice for new potential research challenges in the field. Another important outcome of the proposed work is the study of the impact on the identified signal metrics and solutions of the different target user populations with their specific user requirements.},
keywords={Sleep;Biomedical monitoring;Monitoring;Manuals;Psychology;Sociology;Sleep quality;in-home sensing;sleep diaries;polysomnography;sleep devices;sleep monitoring;physiological parameters;smart health},
doi={10.1109/ACCESS.2019.2953835},
ISSN={2169-3536},
month={},}
@ARTICLE{8090525,
author={Hu, Cheng and Deng, Yuhui and Yang, Laurence T.},
journal={IEEE Access},
title={On-Demand Capacity Provisioning in Storage Clusters Through Workload Pattern Modeling},
year={2017},
volume={5},
number={},
pages={24830-24841},
abstract={Internet of Things (IoT), which is the inter-networking of a wide variety of physical devices, is widely used in our daily life. The exponential increase in the number of diverse devices has resulted in a significant increase in the volume, variety, velocity, and veracity of data (i.e., big data). These data present a large requirement on modern storage systems both for capacity and scale, and energy cost has become a critical problem. For storage clusters, much research effort has been invested in alleviating this problem by providing suitable resource capacity (i.e., on-demand providing). However, it is challenging to match the offered resource capacity with the real system workloads, thus resulting in a violation of service level agreement. By considering a storage cluster as a queueing system, this paper proposes a QoS-oriented capacity provisioning mechanism. Based on workload features, the mechanism models the pattern of current workloads as a suitable queueing model. In accordance with the model, our mechanism can well forecast the actual resource capacity demand without violating the service level agreement, and then offer the required resource capacity in terms of the real workloads. Experimental results demonstrate that the proposed mechanism significantly reduces the energy consumption of a typical storage cluster, while meeting the QoS requirements. It also significantly outperforms two classic and two state-of-the-art capacity provisioning mechanisms.},
keywords={Servers;Quality of service;Clustering algorithms;Energy consumption;Heuristic algorithms;Power demand;IoT;energy efficient;energy saving;storage cluster;capacity demand estimation;capacity provisioning;QoS;SLA},
doi={10.1109/ACCESS.2017.2767703},
ISSN={2169-3536},
month={},}
@ARTICLE{9906043,
author={Huang, Chu and Zhang, Qianzhen and Guo, Deke and Zhao, Xiang and Wang, Xi},
journal={Tsinghua Science and Technology},
title={Discovering Association Rules with Graph Patterns in Temporal Networks},
year={2023},
volume={28},
number={2},
pages={344-359},
abstract={Discovering regularities between entities in temporal graphs is vital for many real-world applications (e.g., social recommendation, emergency event detection, and cyberattack event detection). This paper proposes temporal graph association rules (TGARs) that extend traditional graph-pattern association rules in a static graph by incorporating the unique temporal information and constraints. We introduce quality measures (e.g., support, confidence, and diversification) to characterize meaningful TGARs that are useful and diversified. In addition, the proposed support metric is an upper bound for alternative metrics, allowing us to guarantee a superset of patterns. We extend conventional confidence measures in terms of maximal occurrences of TGARs. The diversification score strikes a balance between interestingness and diversity. Although the problem is NP-hard, we develop an effective discovery algorithm for TGARs that integrates TGARs generation and TGARs selection and shows that mining TGARs is feasible over a temporal graph. We propose pruning strategies to filter TGARs that have low support or cannot make top-$k$ as early as possible. Moreover, we design an auxiliary data structure to prune the TGARs that do not meet the constraints during the TGARs generation process to avoid conducting repeated subgraph matching for each extension in the search space. We experimentally verify the effectiveness, efficiency, and scalability of our algorithms in discovering diversified top-$k$ TGARs from temporal graphs in real-life applications.},
keywords={Measurement;Matched filters;Upper bound;Event detection;Heuristic algorithms;Scalability;Filtering algorithms;temporal networks;graph association rule;subgraph pattern matching;graph mining;big graphs},
doi={10.26599/TST.2021.9010090},
ISSN={1007-0214},
month={April},}
@ARTICLE{9772633,
author={Seo, Won-Ki and Rhee, Chae Eun},
journal={IEEE Access},
title={Low Latency Streaming for Path-Walking VR Systems},
year={2022},
volume={10},
number={},
pages={50702-50714},
abstract={Highly immersive content in the form of extended reality (XR) is attracting attention as an alternative to conventional video services such as YouTube and Facebook. Many galleries and museums already offer online virtual reality (VR) tours where users are free to choose the spot they want to move to, beyond merely looking around. Although the ease of implementation, this key-spot hopping is still far from giving the real feeling of walking. Meanwhile, in recent volumetric or light-field-based studies, view rendering that supports free and continuous viewpoint movements has been attempted. With online services in mind, however, the high data volume and computational complexity are a big obstacle to practical applications. Path-walking VR, the target system of this paper, can be a good compromise, where the viewer can enjoy the virtual space while walking along the route. The interactive path-walking VR service is entry-level immersive video, but streaming over the network is still challenging. One of the main problems to be tackled is that the movement patterns of viewers need to be reflected in the streaming strategy to improve the quality of experience. Unlike unidirectional video, the movement of the viewer determines which images and how many images should be transmitted. This paper proposes schemes to reduce streaming delays by reflecting the viewer’s movement characteristics. It is differentiated from existing studies for omnidirectional video in that the proposed schemes control not only image quality but also view update rate. The first is a caching strategy which takes advantage of the geometrical locality of the virtual space that the viewer will soon reach a position close to the current position. This not only reduces the communication delay from the server, but also decreases the burden of server-side request handling. The second scheme uses the relationship between the viewer’s speed and the field of vision. The image quality is adjusted according to the viewer’s speed and head direction. Experimental results show that the proposed schemes achieve stable viewer’s experience by considering walking characteristics in virtual space. It is expected that the results of this paper will provide insight to those who design interactive streaming systems for immersive media applications.},
keywords={Streaming media;Servers;Point cloud compression;Legged locomotion;Head;Bit rate;Quality of experience;Immersive media;interactive virtual reality;video streaming;omni-directional video;low-latency;caching},
doi={10.1109/ACCESS.2022.3174351},
ISSN={2169-3536},
month={},}
@ARTICLE{9828495,
author={Yu, Mengzhu and Tang, Zhenjun and Zhang, Xianquan and Zhong, Bineng and Zhang, Xinpeng},
journal={IEEE Transactions on Circuits and Systems for Video Technology},
title={Perceptual Hashing With Complementary Color Wavelet Transform and Compressed Sensing for Reduced-Reference Image Quality Assessment},
year={2022},
volume={32},
number={11},
pages={7559-7574},
abstract={Image quality assessment (IQA) is an important task of image processing and has diverse applications, such as image super-resolution reconstruction, image transmission and monitoring systems. This paper proposes a perceptual hashing algorithm with complementary color wavelet transform (CCWT) and compressed sensing (CS) for reduced-reference (RR) IQA. The CCWT is exploited to decompose input color image into different sub-bands. Since the calculation of CCWT uses all color channels without discarding any information, the distortions introduced by digital operations on color channels are preserved in the CCWT sub-bands. The block-based CS is used to extract features from the CCWT sub-bands. As the Euclidean distance between the block-based CS features is slightly influenced by content-preserving operations, perceptual features constructed by Euclidean distances are robust, discriminative and compact. Hash sequence is finally determined by quantifying the perceptual features. Effectiveness of the proposed hashing is verified by various experiments on four open image databases. Experimental results demonstrate that the proposed hashing is superior to some state-of-the-art algorithms in terms of classification and RR IQA application.},
keywords={Feature extraction;Image color analysis;Distortion;Robustness;Transforms;Image coding;Visualization;Image quality assessment;image hashing;complementary color wavelet transform (CCWT);compressed sensing (CS);Sobel operator},
doi={10.1109/TCSVT.2022.3190273},
ISSN={1558-2205},
month={Nov},}
@ARTICLE{7914634,
author={Qin, Cheng and Ni, Wei and Tian, Hui and Liu, Ren Ping},
journal={IEEE Access},
title={Fronthaul Load Balancing in Energy Harvesting Powered Cloud Radio Access Networks},
year={2017},
volume={5},
number={},
pages={7762-7775},
abstract={Enhanced with wireless power transfer capability, cloud radio access network (C-RAN) enables energy-restrained mobile devices to function uninterruptedly. Beamforming of C-RAN has potential to improve the efficiency of wireless power transfer, in addition to transmission data rates. In this paper, we design the beamforming jointly for data transmission and energy transfer, under finite fronthaul capacity of C-RAN. A non-convex problem is formulated to balance the fronthaul requirements of different remote radio heads (RRHs). Norm approximations and relaxations are carried out to convexify the problem to second-order cone programming (SOCP). To improve the scalability of the design to large networks, we further decentralize the SOCP problem using the alternating direction multiplier method (ADMM). A series of reformulations and transformations are conducted, such that the resultant problem conforms to the state-of-the-art ADMM solver and can be efficiently solved in real time. Simulation results show that the distributed algorithm can remarkably reduce the time complexity without compromising the fronthaul load balancing of its centralized counterpart. The proposed algorithms can also reduce the fronthaul bandwidth requirements by 25% to 50%, compared with the prior art.},
keywords={Array signal processing;Energy harvesting;Quality of service;Bandwidth;Load management;Radio access networks;Wireless communication;Cloud radio access network (C-RAN);energy harvesting;fronthaul;decentralization},
doi={10.1109/ACCESS.2017.2699198},
ISSN={2169-3536},
month={},}
@ARTICLE{9455417,
author={Luque, Joaquin and Personal, Enrique and Garcia-Delgado, Antonio and Leon, Carlos},
journal={IEEE Access},
title={Monthly Electricity Demand Patterns and Their Relationship With the Economic Sector and Geographic Location},
year={2021},
volume={9},
number={},
pages={86254-86267},
abstract={In a highly competitive and liberalized energy market, where the retail of electricity is open to many potential companies, it is essential to have tools that help make decisions and guide the design of marketing strategies. In this sense, it is essential for retailers to know the behavior of their customers to correctly define their commercial strategies. One of the most commonly used methods for this is the characterization of their consumption profiles. Fortunately, for regulatory reasons, in some countries, the monthly electricity demand of each customer is openly available to any competitor. This paper explores whether this information, especially the economic sector and geographic location of a client, is useful for determining the client’s demand profile. Specifically, data on electricity demand in Spain from more than 27 million users and for a period of 3 years are analyzed. For this purpose, the electricity consumption of every client is grouped by month and normalized. The resulting demand profiles are later clustered according to different criteria. The main finding of the research is that the combined information on economic activity and location definitely enables prediction of the demand profile. Additionally, profile quality metrics are defined and obtained for the entire dataset. The resulting profiles have a mean dispersion of 10% and a confidence interval of ±17%. To clarify the use of these metrics, several examples are detailed, showing how this profile information can be used to improve the marketing decision-making process for electricity retailers.},
keywords={Economics;Companies;Meters;Meter reading;Energy consumption;Supply chains;Energy demand;customer profiling;data engineering;big data applications;statistical learning;pattern analysis},
doi={10.1109/ACCESS.2021.3089443},
ISSN={2169-3536},
month={},}
@ARTICLE{9527089,
author={Gao, Xinyu and Li, Yi and Qiu, Yanqing and Mao, Bangning and Chen, Miaogen and Meng, Yanlong and Zhao, Chunliu and Kang, Juan and Guo, Yong and Shen, Changyu},
journal={IEEE Photonics Journal},
title={Improvement of Image Classification by Multiple Optical Scattering},
year={2021},
volume={13},
number={5},
pages={1-5},
abstract={Multiple optical scattering occurs when light propagates in a non-uniform medium. During the multiple scattering, images were distorted and the spatial information they carried became scrambled. However, the image information is not lost but presents in the form of speckle patterns (SPs). In this study, we built up an optical random scattering system based on an liquid crystal display (LCD) and an RGB laser source. We found that the image classification can be improved by the help of random scattering which is considered as a feedforward neural network to extracts features from image. Along with the ridge classification deployed on computer, we achieved excellent classification accuracy higher than 94%, for a variety of data sets covering medical, agricultural, environmental protection and other fields. In addition, the proposed optical scattering system has the advantages of high speed, low power consumption, and miniaturization, which is suitable for deploying in edge computing applications.},
keywords={Scattering;Optical scattering;Optical imaging;Adaptive optics;Liquid crystal displays;Optical reflection;Optical distortion;Optical computing;machine learning;random media;feedforward neural networks},
doi={10.1109/JPHOT.2021.3109016},
ISSN={1943-0655},
month={Oct},}
@ARTICLE{9729745,
author={Kazemi, Arefeh and Mozafari, Jamshid and Nematbakhsh, Mohammad Ali},
journal={IEEE Access},
title={PersianQuAD: The Native Question Answering Dataset for the Persian Language},
year={2022},
volume={10},
number={},
pages={26045-26057},
abstract={Developing Question Answering systems (QA) is one of the main goals in Artificial Intelligence. With the advent of Deep Learning (DL) techniques, QA systems have witnessed significant advances. Although DL performs very well on QA, it requires a considerable amount of annotated data for training. Many annotated datasets have been built for the QA task; most of them are exclusively in English. In order to address the need for a high-quality QA dataset in the Persian language, we present PersianQuAD, the native QA dataset for the Persian language. We create PersianQuAD in four steps: 1) Wikipedia article selection, 2) question-answer collection, 3) three-candidates test set preparation, and 4) Data Quality Monitoring. PersianQuAD consists of approximately 20,000 questions and answers made by native annotators on a set of Persian Wikipedia articles. The answer to each question is a segment of the corresponding article text. To better understand PersianQuAD and ensure its representativeness, we analyze PersianQuAD and show it contains questions of varying types and difficulties. We also present three versions of a deep learning-based QA system trained with PersianQuAD. Our best system achieves an F1 score of 82.97% which is comparable to that of QA systems on English SQuAD, made by the Stanford University. This shows that PersianQuAD performs well for training deep-learning-based QA systems. Human performance on PersianQuAD is significantly better (96.49%), demonstrating that PersianQuAD is challenging enough and there is still plenty of room for future improvement. PersianQuAD and all QA models implemented in this paper are freely available.},
keywords={Internet;Online services;Encyclopedias;Training;Task analysis;Machine translation;Buildings;Dataset;deep learning;natural language processing;Persian;question answering;machine reading comprehension},
doi={10.1109/ACCESS.2022.3157289},
ISSN={2169-3536},
month={},}
@ARTICLE{9761192,
author={Yang, Yu Lu and Wan, Guo Chun and Tong, Mei Song},
journal={IEEE Access},
title={A Novel Wireless Propagation Model Based on Bi-LSTM Algorithm},
year={2022},
volume={10},
number={},
pages={43837-43847},
abstract={Establishing accurate wireless propagation models is essential for high-quality communications. Aiming at the low accuracy and complexity of the traditional wireless propagation model, a novel accurate wireless propagation model is proposed based on the bi-directional long short-term memory (Bi-LSTM) algorithm of machine learning. The model uses machine learning technology driven by big data and can achieve high real-time performance with low complexity. Also, it can accurately predict the wireless signal coverage intensity in a new environment. To allow the model to accommodate the actual environment of target areas, the propagation model can be dynamically corrected by deep learning and training. The Bi-LSTM is used to describe the relationship between features themselves and the relationship between features and target values of reference signal receiving power (RSRP). The Bi-LSTM is also used to represent the relationship through a full-connection layer to obtain the results so that sufficient parameter space can be provided for the model. The propagation model parameters are searched and fitted through a full-connection optimization. After training and tuning, the model’s predicted value of poor coverage recognition rate (PCRR) can reach 0.2371, while the predicted value of root mean squared error (RMSE) can be 10.4855, which demonstrates the better accuracy of the proposed model.},
keywords={Wireless communication;Data models;Transmitters;Propagation losses;Predictive models;Mathematical models;Buildings;Bi-LSTM;deep learning;feature extraction;fully connected layer;wireless propagation},
doi={10.1109/ACCESS.2022.3169174},
ISSN={2169-3536},
month={},}
@ARTICLE{9320517,
author={Zhang, Qiyun and Zhang, Yuan and Li, Caizhong and Yan, Chao and Duan, Yucong and Wang, Hao},
journal={IEEE Access},
title={Sport Location-Based User Clustering With Privacy-Preservation in Wireless IoT-Driven Healthcare},
year={2021},
volume={9},
number={},
pages={12906-12913},
abstract={The gradual prevalence of Internet of Things (IoT) and wireless communication technologies has enabled the wide adoption of various smart devices (e.g., smart watches) in provisioning the healthcare services to massive users. Besides monitoring the real-time health signals or conditions of users, smart devices can also record a series of sport-related user information such as user location information at a certain time point. The location sequence information is valuable to cluster the users who share the similar sport preferences or habits and therefore, is also playing a key role in providing wireless healthcare services to these users. However, the user location information is often sensitive to certain wireless users as they decline to reveal their daily sport behavior patterns to others. In this situation, a natural challenge is raised in securing the sensitive user location information while mining the users’ daily sport behavior patterns and provisioning better healthcare services to the users. Considering this challenge, we take advantage of the well-known SimHash technique to protect users’ location privacy while clustering the users who share similar sport preferences or habits for better healthcare services. At last, we validate the feasibility of the proposal through a set of simulated experiments conducted on a real-world dataset. Reported results demonstrate that our solution performs better than the other two competitive ones while securing user location information.},
keywords={Quality of service;Sports;Medical services;Wireless communication;Communication system security;Wireless sensor networks;Privacy;Sport location;user clustering;privacy;healthcare service;simhash;wireless network},
doi={10.1109/ACCESS.2021.3051051},
ISSN={2169-3536},
month={},}
@ARTICLE{9040423,
author={Jin, Yong and Qian, Zhenjiang and Yang, Weiyong},
journal={IEEE Access},
title={UAV Cluster-Based Video Surveillance System Optimization in Heterogeneous Communication of Smart Cities},
year={2020},
volume={8},
number={},
pages={55654-55664},
abstract={Video surveillance system is the integration of computers, networks, communications, and video CODEC, etc. Because of its distributed architecture, parallel image processing and ease of installation and expansion, it is widely used in many fields such as education, transportation and industry. However, there are some challenges of video surveillance applications in smart cities such as large scale of video events, low quality and big delay of video data transmission, and the loss of video surveillance data integrity. In order to solve the above problems, this paper designs a series of optimization algorithms and scheduling strategies based on Unmanned Aerial Vehicle (UAV) cluster. Firstly, we construct a full device coverage network with UAV cluster in heterogeneous communication environment of smart cities. Secondly, we formulate the scheduling problem of UAV cluster as bi-objective fragile bin packing problem, and design an optimal scheduling algorithm with constant approximation performance ratio. The simulation experimental results fully demonstrate the effectiveness, feasibility and robustness of the proposed solution in terms of system life cycle, video decodable frame rate, the ratio of UAV flight time to system life cycle, throughput and delay.},
keywords={Video surveillance;Unmanned aerial vehicles;Smart cities;Optimization;Data communication;Scheduling;Smart city;video surveillance;unmanned aerial vehicle (UAV) cluster;scheduler;bin packing;heterogeneous communication},
doi={10.1109/ACCESS.2020.2981647},
ISSN={2169-3536},
month={},}
@ARTICLE{9351910,
author={Haider, Waleej and Rehman, Aqeel-Ur and Durrani, Nouman M. and Rehman, Sadiq Ur},
journal={IEEE Access},
title={A Generic Approach for Wheat Disease Classification and Verification Using Expert Opinion for Knowledge-Based Decisions},
year={2021},
volume={9},
number={},
pages={31104-31129},
abstract={Crop diseases have mainly affected crop production due to the lack of modern approaches for disease identification. For many years, farmers have identified various crop diseases and have local knowledge about disease management. However, the local knowledge of one agricultural region is not utilized in other regions due to the unavailability of knowledge sharing platforms. Agricultural research also suggests that crop production has mainly decreased due to diseases, methods of cultivation, irrigation, and lack of local agricultural knowledge. In this research, the experience of agricultural experts, farmers, and cultivators is gathered through a crowd-sourced platform. The data is then processed for various disease identification. Hence, timely identification of various crop diseases can benefit farmers to apply relevant management methods. In literature, researchers have proposed various methods for disease management, mostly based on the classification of crop diseases using Machine Learning (ML) algorithms. However, these algorithms are unable to give trustful results due to static data provisioning and the dynamic nature of various diseases in different agricultural regions. Further, the agricultural expert's experience is also not considered in verifying the classification results. To identify the dynamic nature of wheat diseases, we acquired high-quality images and symptoms-based text data from farmers, domain experts, and users using a crowd-sourced platform. Different augmentation techniques were also used to enhance the size of training data. In this paper, a modern generic approach has been proposed for the identification and classification of wheat diseases using Decision Trees (DT) and different deep learning models. Also, results of both algorithms were then verified by domain experts that improved the decision trees accuracy by 28.5%, CNN accuracy by 4.3% (leading to 97.2%), and resulted in decision rules for wheat diseases in a knowledge-based system.},
keywords={Diseases;Agriculture;Insects;Production;Machine learning algorithms;Knowledge based systems;Genetics;Agricultural DSS;artificial intelligence;agricultural knowledge management;classification of crop diseases;machine learning;wheat crop diseases},
doi={10.1109/ACCESS.2021.3058582},
ISSN={2169-3536},
month={},}
@ARTICLE{9931128,
author={Alghanim, Firas and Azzeh, Mohammad and El-Hassan, Ammar and Qattous, Hazem},
journal={IEEE Access},
title={Software Defect Density Prediction Using Deep Learning},
year={2022},
volume={10},
number={},
pages={114629-114641},
abstract={Delivering a reliable and high-quality software system to client is a big challenge in software development and evolution process. One of the software measures that confirm the quality of the system is the defect density. Practitioners usually need this measure during software development process or during a period of operation to indicate the reliability of software system. However, since predicting defect density before testing the modules is time consuming, managers need to build a prediction model that can help in detecting the defective modules. This process can reduce the testing cost and improve testing resources utilizations. The most intrinsic feature of software defect datasets is the data sparsity in the defect density which might bias the final prediction. Therefore, we use deep learning to build defect density prediction models and handle the inherit challenge of data sparsity in defect density. Deep learning has shown to be effective with sparse data. The constructed model has been evaluated against well-known machine learning methods over 28 public datasets. The obtained results confirmed that the deep learning model is generally more adequate than other machine models over the datasets with high and very high sparsity ratios, and competitive choice when the sparsity ratio is either medium or low.},
keywords={Machine learning;Deep learning;Measurement;Data models;Predictive models;Codes;Testing;Defect density prediction;deep learning;data sparsity;machine learning},
doi={10.1109/ACCESS.2022.3217480},
ISSN={2169-3536},
month={},}
@ARTICLE{9792246,
author={Chen, Yange and He, Suyu and Wang, Baocang and Duan, Pu and Zhang, Benyu and Hong, Zhiyong and Ping, Yuan},
journal={IEEE Internet of Things Journal},
title={Cryptanalysis and Improvement of DeepPAR: Privacy-Preserving and Asynchronous Deep Learning for Industrial IoT},
year={2022},
volume={9},
number={21},
pages={21958-21970},
abstract={Industrial Internet of Things (IIoT) is gradually changing the mode of traditional industries with the rapid development of big data. Besides, thanks to the development of deep learning, it can be used to extract useful knowledge from the large amount of data in the IIoT to help improve production and service quality. However, the lack of large-scale data sets will lead to low performance and overfitting of learning models. Therefore, federated deep learning with distributed data sets has been proposed. Nevertheless, the research has shown that federated learning can also leak the private data of participants. In IIoT, once the privacy of participants in some special application scenarios is leaked, it will directly affect national security and people’s lives, such as smart power grid and smart medical care. At present, several privacy-preserving federated learning schemes have been proposed to preserve data privacy of participants, but security issues prevent them from being fully applied. In this article, we analyze the security of the DeepPAR scheme proposed by Zhang et al., and point out that the scheme is insecure in the re-encryption key generation process, which will cause the leakage of the secret key of participants or the proxy server. In addition, the scheme is not resistant to collusion attacks between the parameter server and participants. Based on this, we propose an improved scheme. The security proof shows that the improved scheme solves the security problem of the original scheme and is resistant to collusion attacks. Finally, the security and accuracy of the scheme is illustrated by performance analysis.},
keywords={Deep learning;Servers;Training;Privacy;Industrial Internet of Things;Production;Homomorphic encryption;Asynchronous deep learning;homomorphic encryption;privacy preserving;proxy re-encryption},
doi={10.1109/JIOT.2022.3181665},
ISSN={2327-4662},
month={Nov},}
@ARTICLE{8327574,
author={Mahajan, Sachit and Liu, Hao-Min and Tsai, Tzu-Chieh and Chen, Ling-Jyh},
journal={IEEE Access},
title={Improving the Accuracy and Efficiency of PM2.5 Forecast Service Using Cluster-Based Hybrid Neural Network Model},
year={2018},
volume={6},
number={},
pages={19193-19204},
abstract={Information and communication technologies have been widely used to achieve the objective of smart city development. A smart air quality sensing and forecasting system is an important part of a smart city. One of the major challenges in designing such a forecast system is ensuring high accuracy and an acceptable computation time. In this paper, we show that it is possible to accurately forecast fine particulate matter (PM2.5) concentrations with low computation time by using different clustering techniques. An Internet of Things framework comprising of Airbox devices for PM2.5 monitoring has been used to acquire the data. Our main focus is to achieve high forecasting accuracy with reduced computation time. We use a hybrid model to do the forecast and a grid based system to cluster the monitoring stations based on the geographical distance. The experiments and evaluation is done using Airbox devices data from 557 stations deployed all over Taiwan. We are able to demonstrate that a proper clustering based on geographical distance can reduce the forecasting error rate and also the computation time. Also, in order to further evaluate our system, we have applied wavelet-based clustering to group the monitoring stations. A final comparative analysis is done for different clustering schemes with respect to accuracy and computational time.},
keywords={Predictive models;Atmospheric modeling;Monitoring;Computational modeling;Air quality;Forecasting;Neural networks;Internet of Things;forecasting;smart cities;neural networks},
doi={10.1109/ACCESS.2018.2820164},
ISSN={2169-3536},
month={},}
@ARTICLE{9773114,
author={Jiang, Yuying and Li, Guangming and Ge, Hongyi and Wang, Faye and Li, Li and Chen, Xinyu and Lu, Ming and Zhang, Yuan},
journal={IEEE Access},
title={Machine Learning and Application in Terahertz Technology: A Review on Achievements and Future Challenges},
year={2022},
volume={10},
number={},
pages={53761-53776},
abstract={Terahertz (THz) radiation ( $0.1\sim 10$ THz) shows great potential in agricultural products detection, biomedical, and security inspection in recent years. Machine learning methods are widely used to support the user demand of higher efficiency and high prediction accuracy. The technological and key challenges of machine learning methods are for THz spectroscopy and image data preprocessing, reconstruction algorithms, and qualitative and quantitative analysis. In this paper, an exhaustive review of recent related works of THz detection and imaging techniques and machine learning methods are presented. The application of machine learning methods combined with THz technology in quality inspection of agricultural products, biomedical, security inspection, and materials science are highlighted. Challenges of machine learning methods for these applications are addressed. The development trend and future perspectives of THz technology are also discussed.},
keywords={Imaging;Terahertz wave imaging;Laser beams;Machine learning;Ultrafast optics;Spectroscopy;Probes;Terahertz spectrum;terahertz imaging;machine learning;agricultural products;detection application},
doi={10.1109/ACCESS.2022.3174595},
ISSN={2169-3536},
month={},}
@ARTICLE{9104973,
author={Zhang, Hongbin and Wu, Jinpeng and Shi, Haowei and Jiang, Ziliang and Ji, Donghong and Yuan, Tian and Li, Guangli},
journal={IEEE Access},
title={Multidimensional Extra Evidence Mining for Image Sentiment Analysis},
year={2020},
volume={8},
number={},
pages={103619-103634},
abstract={Image sentiment analysis is a hot research topic in the field of computer vision. However, two key issues need to be addressed. First, high-quality training samples are scarce. There are numerous ambiguous images in the original datasets owing to diverse subjective cognitions from different annotators. Second, the cross-modal sentimental semantics among heterogeneous image features has not been fully explored. To alleviate these problems, we propose a novel model called multidimensional extra evidence mining (ME2M) for image sentiment analysis, it involves sample-refinement and cross-modal sentimental semantics mining. A new soft voting-based sample-refinement strategy is designed to address the former problem, whereas the state-of-the-art discriminant correlation analysis (DCA) model is used to completely mine the cross-modal sentimental semantics among diverse image features. Image sentiment analysis is conducted based on the cross-modal sentimental semantics and a general classifier. The experimental results verify that the ME2M model is effective and robust and that it outperforms the most competitive baselines on two well-known datasets. Furthermore, it is versatile owing to its flexible structure.},
keywords={Sentiment analysis;Semantics;Analytical models;Correlation;Kernel;Training;Visualization;Image sentiment analysis;discriminant correlation analysis;sample-refinement;cross-modal sentimental semantics;multidimensional extra evidence mining},
doi={10.1109/ACCESS.2020.2999128},
ISSN={2169-3536},
month={},}
@ARTICLE{9064538,
author={Wei, Wei and Xia, Pengfei and Xue, Wenchao and Zuo, Min},
journal={IEEE Access},
title={On the Disturbance Rejection of a Piezoelectric Driven Nanopositioning System},
year={2020},
volume={8},
number={},
pages={74771-74781},
abstract={Nanopositioning systems are very popular and playing an increasingly vital role in micro and nano-scale positioning industry due to their unique ability to achieve high-precision and high-speed operation. However, hysteresis, commonly existing in piezoelectric actuators, degrades the precision seriously. Uncertain dynamics and sensor noises also greatly affect the accuracy. To address those challenges, a variable bandwidth active disturbance rejection control (VBADRC) is proposed and realized on a nanopositioning stage. All undesired issues are estimated by a time-varying extended state observer (TESO), and cancelled out by a variable bandwidth controller. Convergence of the TESO, advantages of a TESO over a linear extended state observer (LESO), and the closed-loop stability of the VBADRC are proven theoretically. Improvements of the VBADRC versus the linear active disturbance rejection control (LADRC) are validated by simulations and experiments. Both numerical and experimental results demonstrate that the VBADRC is not only able to provide the same disturbance estimation ability as the LADRC, but also more powerful in noise attenuation and reference tracking.},
keywords={Nanopositioning;Hysteresis;Bandwidth;Robust control;Uncertainty;Estimation error;Nanopositioning;active disturbance rejection control;time-varying extended state observer;variable bandwidth control;noise attenuation},
doi={10.1109/ACCESS.2020.2987469},
ISSN={2169-3536},
month={},}
@ARTICLE{9284428,
author={Jiang, Peichao and Wang, Xiaodong},
journal={IEEE Access},
title={Preference Cognitive Diagnosis for Student Performance Prediction},
year={2020},
volume={8},
number={},
pages={219775-219787},
abstract={Knowledge states modeling is a fundamental issue in online education. One of its tasks is to discover the potential knowledge capacity of students for predicting their performance (i.e., scores on exercises). Current studies either depend on cognitive diagnosis approaches or apply collaborative filtering. However, the prediction accuracy of traditional cognitive diagnosis is insufficient, and collaborative filtering has difficulty ensuring the interpretability of prediction. Actually, students usually read some auxiliary text learning materials that they are interested in, namely, preferred learning material, to consolidate what they have learned. Preference cognitive diagnosis means that the preferred learning materials can reflect the students’ knowledge states (i.e., proficiency for knowledge concepts) to some extent, which is beneficial for predicting students’ performance. Therefore, we propose a preference cognitive diagnosis method (PreferenceCD) to model students’ knowledge states. Specifically, we first design the Direct-Indirect method to acquire students’ preferred learning materials. This method mines important information from students’ reading content that can reflect their preference for learning materials to acquire those preferred learning materials directly. Moreover, it discovers preferred learning materials indirectly by analyzing the similarity of students’ learning behaviors during the reading process. Subsequently, we calculate students’ preference degree for knowledge concepts based on the acquired preferred learning materials and diagnose their proficiency for knowledge concepts by applying a cognitive diagnosis model. After that, we combine the above two aspects to model students’ knowledge states and further predict their scores on exercises. The experimental results on a real-world dataset demonstrate the effectiveness of PreferenceCD with both accuracy and interpretability. The accuracy, root mean square error (RMSE), and mean absolute error (MAE) of PreferenceCD are 0.7614, 0.4805, and 0.2386, respectively, which outperforms related works by about 2-12% in terms of these evaluation metrics.},
keywords={Collaboration;Art;Text mining;Correlation;Psychology;Predictive models;Online education;preferred learning material;preference cognotive diagnosis;knowledge states;student performance prediction},
doi={10.1109/ACCESS.2020.3042775},
ISSN={2169-3536},
month={},}
@ARTICLE{9336017,
author={An, Jue and Zhao, Feng},
journal={IEEE Access},
title={Trajectory Optimization and Power Allocation Algorithm in MBS-Assisted Cell-Free Massive MIMO Systems},
year={2021},
volume={9},
number={},
pages={30417-30425},
abstract={As the mobile networks become even denser and the traffic demand is increasing drastically, it is becoming more heterogeneous. Recently, it is intensive to investigate the Cell-Free Massive multiple-input multiple-output Massive (MIMO) system, where a large number of access points (APs) simultaneously serve a much smaller number of users, and the APs and users are clustered to provide good service for all users. This paper designs a mobile base station (MBS)-assisted Cell-Free Massive MIMO system: Utilizing an MBS deployed on the unmanned aerial vehicle (UAV) to form an air-ground heterogeneous system with the Cell-Free Massive MIMO system and offload part of traffic to the MBS to further improve the system performance. To provide better service to all users, this paper design an MBS flight trajectory optimal method to improve the wireless coverage performance and user experience, and proposed a joint power allocation algorithm based on the consideration of the fairness of the service quality. The simulation results indicate that the performance of the system designed in this paper has a significant improvement compared with the normal Cell-Free Massive MIMO system.},
keywords={Wireless communication;Base stations;System performance;Simulation;Massive MIMO;Unmanned aerial vehicles;User experience;Cell-free massive MIMO;mobile base station;trajectory optimization;power allocation},
doi={10.1109/ACCESS.2021.3054652},
ISSN={2169-3536},
month={},}
@ARTICLE{8935233,
author={Li, Hui and Pi, Dechang and Chen, Chuanming and Li, Hongyi},
journal={IEEE Access},
title={A Novel Prediction Method for Zinc-Binding Sites in Proteins by an Ensemble of SVM and Sample-Weighted Probabilistic Neural Network},
year={2019},
volume={7},
number={},
pages={186147-186157},
abstract={In the prediction of zinc-binding sites in proteins, there are few real binding-site residues, whereas most residues are non-binding-site residues, resulting in a typical imbalanced classification problem. This paper proposes a novel method, SSWPNN (an ensemble of support vector machine and sample-weighted probabilistic neural network), based on downsampling and an ensemble of different classifiers, in view of the imbalance of zinc-binding sites in proteins. Multiple random downsampling techniques without replacement are performed on the whole set, and the support vector machine is trained as the base classifier on each subset to calculate the weights of samples, while the sample-weighted probabilistic neural network is constructed as a strong classifier for prediction. The experimental results showed that our method is superior to other methods not only in the overall prediction performance for the four types of residues but also in the prediction performance for any type of residue. The results of experimental testing on an independent test set collected by the authors in recent years showed that our method achieved better prediction performance than others not only for the four types of residues overall but also for any one type of residue. In addition, the importance of the features selected by the method is analyzed by reducing certain feature to calculate the scores of the performance index. The source code and datasets are available at http://net.jitsec.cn:88/UploadedImages/SSWPNN.rar.},
keywords={Tools;Support vector machines;Zinc;Ions;Protein sequence;Ensemble;imbalanced classification;sample-weighted;probabilistic neural network;support vector machine;zinc-binding sites},
doi={10.1109/ACCESS.2019.2960374},
ISSN={2169-3536},
month={},}
@ARTICLE{9878326,
author={Bhutto, Adil Bin and Vu, Xuan Son and Elmroth, Erik and Tay, Wee Peng and Bhuyan, Monowar},
journal={IEEE Access},
title={Reinforced Transformer Learning for VSI-DDoS Detection in Edge Clouds},
year={2022},
volume={10},
number={},
pages={94677-94690},
abstract={Edge-driven software applications often deployed as online services in the cloud-to-edge continuum lack significant protection for services and infrastructures against emerging cyberattacks. Very-Short Intermittent Distributed Denial of Service (VSI-DDoS) attack is one of the biggest factors for diminishing the Quality of Services (QoS) and Quality of Experiences (QoE) for users on edge. Unlike conventional DDoS attacks, these attacks live for a very short time (on the order of a few milliseconds) in the traffic to deceive users with a legitimate service experience. To provide protection, we propose a novel and efficient approach for detecting VSI-DDoS attacks using reinforced transformer learning that mitigates the tail latency and service availability problems in edge clouds. In the presence of attacks, the users’ demand for availing ultra-low latency and high throughput services deployed on the edge, can never be met. Moreover, these attacks send very-short intermittent requests towards the target services that enforce longer delays in users’ responses. The assimilation of transformer with deep reinforcement learning accelerates detection performance under adverse conditions by adapting the dynamic and the most discernible patterns of attacks (e.g., multiplicative temporal dependency, attack dynamism). The extensive experiments with testbed and benchmark datasets demonstrate that the proposed approach is suitable, effective, and efficient for detecting VSI-DDoS attacks in edge clouds. The results outperform state-of-the-art methods with $0.9\%-3.2\%$ higher accuracy in both datasets.},
keywords={Transformers;Image edge detection;Quality of service;Denial-of-service attack;Cloud computing;Computer crime;Throughput;Reinforcement learning;Reinforced transformer learning;VSI-DDoS;edge clouds;QoS/QoE;cloud applications},
doi={10.1109/ACCESS.2022.3204812},
ISSN={2169-3536},
month={},}
@ARTICLE{9051814,
author={Liu, Guoqi and Li, Xusheng and Chang, Baofang and Dong, Yifei},
journal={IEEE Access},
title={A Superpixel Boundary Optimization (SBO) Framework Based on Information Measure Function},
year={2020},
volume={8},
number={},
pages={64783-64798},
abstract={Superpixel is an essential tool for computer vision. In practice, classic superpixel algorithms do not exhibit good boundary adherence with fewer superpixels, which will greatly hamper further analysis. To remedy the defect, a superpixel boundary optimization framework is proposed in this paper. There are three steps in the framework. Firstly, based on the proposed information measure function, the under-segmented superpixels generated by classic superpixel algorithms are screened out. Secondly, with the two invariant centroids method, these under-segmented superpixels are re-segmented to improve the accuracy in boundary adherence. Finally, smaller superpixels are merged to maintain the same number with initial superpixels. Quantitative evaluations on the BSDS500 exhibit that the performance of the classic superpixel algorithms is improved by employing the framework, especially on the condition of fewer superpixels.},
keywords={Entropy;Gray-scale;Optimization methods;Tools;Weight measurement;Directed graphs;Superpixel;boundary optimization;framework;information measure function;weighted directed graph},
doi={10.1109/ACCESS.2020.2984720},
ISSN={2169-3536},
month={},}
@ARTICLE{8315009,
author={Kim, Sungwook},
journal={IEEE Access},
title={An Effective Sensor Cloud Control Scheme Based on a Two-Stage Game Approach},
year={2018},
volume={6},
number={},
pages={20430-20439},
abstract={Motivated by complementing the ubiquitous sensor networks and cloud computing technique, a lot of attentions have been drawn to Sensor-Cloud (SC). The idea of SC thrives on the principle of virtualization of physical sensor nodes and has introduced the intermediate processing between physical sensor nodes and end users. This paper proposes an efficient interactive SC control scheme to provide on-demand sensing services for multiple applications. By adopting the game theory, we develop a new two-stage game model, which consists of a judicious mixture of selection and incentive algorithms. In our game model, a user can choose the most adaptable data center to execute its task, and each data center can give appropriate incentives to the participating sensors. The main merit possessed by our two-stage game approach is to shed light on the practical SC control problem while providing excellent adaptability and flexibility to satisfy the different application requirements. To the best of our knowledge, this is the first work to include a novel incentive algorithm in the SC system. Simulation results demonstrate that our approach can outperform existing schemes by about 5%~15% in terms of the normalized user profit, service delay, and SC system throughput. Finally, we discuss future directions for designing SC control frameworks including other issues.},
keywords={Games;Data centers;Sensors;Cloud computing;Quality of service;Virtualization;Heuristic algorithms;Sensor Cloud;game theory;incentive mechanism;two-stage game approach;Internet of Everything},
doi={10.1109/ACCESS.2018.2815578},
ISSN={2169-3536},
month={},}
@ARTICLE{8876654,
author={Maiti, Ananda and Raza, Ali and Kang, Byeong Ho and Hardy, Lachlan},
journal={IEEE Access},
title={Estimating Service Quality in Industrial Internet-of-Things Monitoring Applications With Blockchain},
year={2019},
volume={7},
number={},
pages={155489-155503},
abstract={Internet of Things (IoT) plays a big role in automating information generation and consumption in industrial monitoring applications. Blockchain can allow this information to be stored in a manner that is both accessible and reliable for the IoT devices to work with. Blockchain has the capability to collect data from IoT devices and store it in a distributed manner that prevents tampering with the data. This paper discusses the use of blockchain to calculate the Service Quality (SQ) in an Industrial IoT for monitoring application. The proposed framework looks at the blockchain as a finite number of fragmented pieces of data corresponding to a specific industrial process. The SQ is expressed as penalties which is the difference between the expected IoT sensor values and the actual sensor data in reported events from the IoT devices. It also moderates the penalty between similar industrial processes based on each other. The moderation allows better understanding of the system functions and identification of specific problems rather than simply recording the sensor data for a single process. Furthermore, this paper analyzes private blockchains for suitability in IIoT and summarizes some key challenges for IoT to be used with blockchain in context of the proposed framework. The paper uses supply chain as a use case scenario for describing the proposed framework and presents results on its technical feasibility.},
keywords={Blockchain;Internet of Things;Monitoring;Supply chains;Industries;Security;Blockchain;internet of things;cyber-physical systems;logistics;p2p network;smart contract},
doi={10.1109/ACCESS.2019.2948269},
ISSN={2169-3536},
month={},}
@ARTICLE{9247392,
author={Mahesh, Priyanka and Tiwari, Akash and Jin, Chenglu and Kumar, Panganamala R. and Reddy, A. L. Narasimha and Bukkapatanam, Satish T. S. and Gupta, Nikhil and Karri, Ramesh},
journal={Proceedings of the IEEE},
title={A Survey of Cybersecurity of Digital Manufacturing},
year={2021},
volume={109},
number={4},
pages={495-516},
abstract={The Industry 4.0 concept promotes a digital manufacturing (DM) paradigm that can enhance quality and productivity, which reduces inventory and the lead time for delivering custom, batch-of-one products based on achieving convergence of additive, subtractive, and hybrid manufacturing machines, automation and robotic systems, sensors, computing, and communication networks, artificial intelligence, and big data. A DM system consists of embedded electronics, sensors, actuators, control software, and interconnectivity to enable the machines and the components within them to exchange data with other machines, components therein, the plant operators, the inventory managers, and customers. This article presents the cybersecurity risks in the emerging DM context, assesses the impact on manufacturing, and identifies approaches to secure DM.},
keywords={Smart manufacturing;Digital systems;Intelligent sensors;Process control;Sensor systems;Service robots;Computer crime;Fourth Industrial Revolution;Robot sensing systems;Machine components;Virtual manufacturing;Digital manufacturing (DM)},
doi={10.1109/JPROC.2020.3032074},
ISSN={1558-2256},
month={April},}
@ARTICLE{9078126,
author={Lin, Rui and Udalcovs, Aleksejs and Ozolins, Oskars and Pang, Xiaodan and Gan, Lin and Tang, Ming and Fu, Songnian and Popov, Sergei and Silva, Thiago Ferreira Da and Xavier, Guilherme B. and Chen, Jiajia},
journal={IEEE Access},
title={Telecommunication Compatibility Evaluation for Co-existing Quantum Key Distribution in Homogenous Multicore Fiber},
year={2020},
volume={8},
number={},
pages={78836-78846},
abstract={Quantum key distribution (QKD) is regarded as an alternative to traditional cryptography methods for securing data communication by quantum mechanics rather than computational complexity. Towards the massive deployment of QKD, embedding it with the telecommunication system is crucially important. Homogenous optical multi-core fibers (MCFs) compatible with spatial division multiplexing (SDM) are essential components for the next-generation optical communication infrastructure, which provides a big potential for co-existence of optical telecommunication systems and QKD. However, the QKD channel is extremely vulnerable due to the fact that the quantum states can be annihilated by noise during signal propagation. Thus, investigation of telecom compatibility for QKD co-existing with high-speed classical communication in SDM transmission media is needed. In this paper, we present analytical models of the noise sources in QKD links over heterogeneous MCFs. Spontaneous Raman scattering and inter-core crosstalk are experimentally characterized over spans of MCFs with different refractive index profiles, emulating shared telecom traffic conditions. Lower bounds for the secret key rates and quantum bit error rate (QBER) due to different core/wavelength allocation are obtained to validate intra- and inter-core co-existence of QKD and classical telecommunication.},
keywords={Photonics;Resource management;Telecommunications;Multiplexing;Optical fiber networks;Media;Quantum key distribution;spatial division multiplexing;telecommunications;communication system security},
doi={10.1109/ACCESS.2020.2990186},
ISSN={2169-3536},
month={},}
@ARTICLE{8886413,
author={Naeem, Muhammad Rashid and Lin, Tao and Naeem, Hamad and Ullah, Farhan and Saeed, Saqib},
journal={IEEE Access},
title={Scalable Mutation Testing Using Predictive Analysis of Deep Learning Model},
year={2019},
volume={7},
number={},
pages={158264-158283},
abstract={Software testing plays a crucial role in ensuring the quality of software systems. Mutation testing is designed to measure the adequacy of test suites by detecting artificially induced software faults. Despite their potential, the expensive cost and the scalability of mutation testing with large programs is a big obstacle in its practical use. The selective mutation has been widely investigated and considered to be an effective approach to reduce the cost of mutation testing. In the case of large programs where source code has hundreds of classes and more than 10 KLOC lines of code, the selective mutation can still generate thousands of mutants. Executing each mutant against the test suite is cost-intensive in terms of robustness, resource usage, and computational cost. In this paper, we introduce a new approach to extract features from mutant programs based on mutant killing conditions, i.e. reachability, necessity and sufficiency along with mutant significance and test suite metrics to extract features from mutant programs. A deep learning Keras model is proposed to predict killed and alive mutants from each program. First, the features are extracted using the Eclipse JDT library and program dependency analysis. Second, preprocessing techniques such as Principal Component Analysis and Synthetic Minority Oversampling are used to reduce the high dimensionality of data and to overcome the imbalanced class problem respectively. Lastly, the deep learning model is optimized using fine-tune parameters such as dropout and dense layers, activation function, error and loss rate respectively. The proposed work is analyzed on five opensource programs from GitHub repository consisting of thousands of classes and LOC. The experimental results are appreciable in terms of effectiveness and scalable mutation testing with a slight loss of accuracy.},
keywords={Feature extraction;Benchmark testing;Deep learning;Scalability;Predictive models;Software;Scalable mutation testing;static analysis;deep learning;binary classification},
doi={10.1109/ACCESS.2019.2950171},
ISSN={2169-3536},
month={},}
@ARTICLE{9913697,
author={Nguyen, Hieu and Noor-A-Rahim, Md. and Guan, Yong Liang and Pesch, Dirk},
journal={IEEE Transactions on Vehicular Technology},
title={Cellular V2X Communications in the Presence of Big Vehicle Shadowing: Performance Analysis and Mitigation},
year={2022},
volume={},
number={},
pages={1-13},
abstract={In Intelligent Transportation Systems (ITS), vehicular networks are enabling technologies for onboard data services such as traffic safety, user infotainment, etc. Vehicular networks face many challenges when it comes to providing satisfactory quality of service, mainly because of issues that arise from unreliable communication in unfavorable propagation conditions. One prime example is Vehicle-to-Vehicle (V2V) communication in the presence of big vehicles that present obstacles in the communication path of smaller vehicles, where the signal strength decays drastically due to the big vehicle shadowing. As a result, the communication range is shortened, and the safety message dissemination capability is reduced. In this paper, we analyze the impact of big vehicle shadowing on V2V communications, taking into account Cellular Vehicle-to-Everything (C-V2X) networks. A geometric as well as a stochastic approach is employed to analyze the length of shadow regions for conventional cars and big vehicles on the road in different scenarios. This paper analyses the effect of large vehicles shadowing on a V2V communication link as a function of the shadow region length. A beamforming-based signal reception technique is proposed in order to mitigate packet collisions caused by hidden nodes. Considering relaying operation by big vehicles, three relaying schemes are proposed to improve the V2V message dissemination performance. Extensive simulations are conducted to demonstrate the effectiveness of the proposed schemes.},
keywords={Shadow mapping;Safety;Vehicular ad hoc networks;Automobiles;Roads;Vehicle-to-everything;Mathematical models;C-V2X;LTE Sidelink Mode-4;5G-NR Sidelink Mode-2a;Autonomous Mode;Smart Beamforming;V2V Communications;Relaying;Cooperative V2X Communications;Big Vehicle Shadowing},
doi={10.1109/TVT.2022.3212704},
ISSN={1939-9359},
month={},}
@ARTICLE{8713985,
author={Du, Chun and Zhang, Fan and Ma, Shuai and Tang, Yixiao and Li, Hang and Wang, Hongmei and Li, Shiyin},
journal={IEEE Access},
title={Secure Transmission for Downlink NOMA Visible Light Communication Networks},
year={2019},
volume={7},
number={},
pages={65332-65341},
abstract={In this paper, we consider the two key problems in the physical-layer security of nonorthogonal multiple access (NOMA) visible light communication (VLC) networks: investigating a closed-form achievable security rate and studying the optimal security beamforming design. Specifically, under the dimming control, practical power, and successive interference cancellation constraints, we derive both the outer and inner bounds of the security capacity region with closed-form expressions, which are evaluated via numerical results. Then, based on the proposed security-rate expression, we investigated the optimal security beamforming design to minimize the total LED power, and to maximize the minimum secrecy rate, respectively. Both the problems are nonconvex. We apply different relaxation techniques to efficiently solve them. The simulation results demonstrate the efficacy of the proposed security beamforming design schemes in the NOMA VLC networks.},
keywords={NOMA;Security;Array signal processing;Light emitting diodes;Silicon carbide;Quality of service;Visible light communication;NOMA;visible light communication;physical-layer secrecy;optimal beamforming},
doi={10.1109/ACCESS.2019.2916548},
ISSN={2169-3536},
month={},}
@ARTICLE{9138393,
author={Fan, Zijia and Wang, Zhongyang and Xin, Junchang and Wang, Zhiqiong and Liu, Lu and Zhang, Xia and Liu, Jiren},
journal={IEEE Access},
title={Dual-Enhanced Registration for Field of View Ultrasound Sonography},
year={2020},
volume={8},
number={},
pages={128602-128612},
abstract={Extended Field of View Ultrasound Sonography (EFOV-US) uses the existing ultrasound images for image stitching, so as to display the shape and scope of organ occupation and the relationship with surrounding tissues comprehensively. However, there are still some problems in Extended Field of View Ultrasound Sonography, such as matching error and unstable quality of image stitching. In view of these problems, we propose Dual-enhanced EFOV-US method that overcomes the limitation and produces higher quality results. Firstly, the gray enhancement method is used to improve the image contrast and reduce the noise interference. Then the super-resolution method based on the generative adversarial network is used to improve the resolution of the ultrasonic image further and increase the number of feature point matching between stitching images. The high quality ultrasound wide-range image is gotten by stitching and fusing the double enhanced image. The experimental results show that the proposed method is effective and practical.},
keywords={Ultrasonic imaging;Generative adversarial networks;Feature extraction;Training;Gallium nitride;Extended field of view ultrasound sonography;gray enhancement;generative adversarial network;super-resolution;image registration},
doi={10.1109/ACCESS.2020.3008525},
ISSN={2169-3536},
month={},}
@ARTICLE{9598922,
author={Zhou, Xiaofei and Fang, Hao and Fei, Xiaobo and Shi, Ran and Zhang, Jiyong},
journal={IEEE Access},
title={Edge-Aware Multi-Level Interactive Network for Salient Object Detection of Strip Steel Surface Defects},
year={2021},
volume={9},
number={},
pages={149465-149476},
abstract={The performance of the salient object detection of strip surface defects has been promoted largely by deep learning based models. However, due to the complexity of strip surface defects, the existing models perform poorly in the challenging scenes such as noise disturbance, and low contrast between defect regions and background. Meanwhile, the detection results of existing models often suffer from coarse boundary details. Therefore, we propose a novel saliency model, namely an Edge-aware Multi-level Interactive Network, to detect the defects from the strip steel surface. Concretely, our model adopts the U-shape architecture where the two crucial points are the interactive feature integration and the edge-guided saliency fusion. Firstly, except the skip connection that combines the same stage of encoder and decoder, we deploy another connection, where the features from adjacent levels of encoder are transferred to the same stage of decoder. By this way, we are able to provide an effective fusion of multi-level deep features, yielding a well depiction for defects. Secondly, to give well-defined boundaries for prediction results, we add the edge extraction branch after each decoder block, where the progressive feature aggregation endows the edge with precise details and complete object cues. Meanwhile, together with the edge extraction branches, we deploy the saliency prediction branch at each decoder stage. After that, coupled with the fine edge information, we fuse all outputs of saliency prediction branches into the final saliency map, where the edge cue steers the saliency result to pay more attention to the boundary details. Following this way, we can provide a high-quality saliency map which can accurately locate and segment the defects. Extensive experiments are performed on the public dataset, and the results prove the effectiveness and robustness of our model which consistently outperforms the state-of-the-art models.},
keywords={Strips;Steel;Image edge detection;Feature extraction;Decoding;Computational modeling;Object detection;Salient object detection;surface defects;multi-level feature;fusion;edge},
doi={10.1109/ACCESS.2021.3124814},
ISSN={2169-3536},
month={},}
@ARTICLE{8694988,
author={Guo, Lantian and Mu, Dejun and Cai, Xiaoyan and Tian, Gang and Hao, Fei},
journal={IEEE Access},
title={Personalized QoS Prediction for Service Recommendation With a Service-Oriented Tensor Model},
year={2019},
volume={7},
number={},
pages={55721-55731},
abstract={Quality of Service (QoS) value is usually unknown in service recommendation practice. There are some matrix factorization approaches for predicting the unknown value with a user-service model, which uses a single collaboration with the user's neighbor when looking for different services. However, the QoS value is highly related to the service provider and participants. The services are considered in various collaboration based on different users. By considering the context of services, this paper proposes a QoS prediction model using tensor decomposition based on service collaboration called Service-oriented Tensor (SOT). The prediction approach analyzes service collaboration from other similar services and relevant users by using a three-order tensor. Compared with the traditional model, the experiment results show that the proposed model achieves better prediction accuracy.},
keywords={Quality of service;Predictive models;Collaboration;Data models;Computational modeling;Solid modeling;Service-oriented tensor;service collaboration;service recommendation;QoS prediction;tensor decomposition},
doi={10.1109/ACCESS.2019.2912505},
ISSN={2169-3536},
month={},}
@ARTICLE{9373354,
author={Kuldeep, Gajraj and Zhang, Qi},
journal={IEEE Access},
title={A Novel Efficient Secure and Error-Robust Scheme for Internet of Things Using Compressive Sensing},
year={2021},
volume={9},
number={},
pages={40903-40914},
abstract={In most of existing Internet of Things (IoT) applications, data compression, data encryption and error/erasure correction are implemented separately. To achieve reliable communication, in particular, in harsh wireless environment with strong interference, error/erasure correction codes with higher correction capability or Automatic repeat request (ARQ) scheme are desirable but at the cost of increasing complexity and energy consumption. Due to resource-constrained IoT device, it is often challenging to implement all of them. In this paper, we propose a novel lightweight efficient secure error-robust scheme, ENCRUST, which is able to achieve these three functions using simple matrix multiplication. ENCRUST is built on the new theoretical foundation of projection-based encoding presented in this paper, by leveraging the sparsity inherent in the signal. We perform theoretical analysis and experimental study of the proposed scheme in comparison with the conventional schemes. It shows that the proposed scheme can work in low SINR range and the reconstructed signal quality shows graceful degradation. Furthermore, we apply the proposed scheme on real-life electrocardiogram (ECG) dataset and images. The results demonstrate that ENCRUST achieves decent compression, information secrecy as well as strong error recovery in one go.},
keywords={Internet of Things;Encryption;Forward error correction;Image coding;Sensors;Wireless communication;Wireless sensor networks;Error robust encryption;joint compression and error recovery;projection matrix;wireless body area network;resource-constrained;Industrial Internet of Things},
doi={10.1109/ACCESS.2021.3064700},
ISSN={2169-3536},
month={},}
@ARTICLE{8794499,
author={Alshammari, Mohammed and Nasraoui, Olfa and Sanders, Scott},
journal={IEEE Access},
title={Mining Semantic Knowledge Graphs to Add Explainability to Black Box Recommender Systems},
year={2019},
volume={7},
number={},
pages={110563-110579},
abstract={Recommender systems are being increasingly used to predict the preferences of users on online platforms and recommend relevant options that help them cope with information overload. In particular, modern model-based collaborative filtering algorithms, such as latent factor models, are considered state-of-the-art in recommendation systems. Unfortunately, these black box systems lack transparency, as they provide little information about the reasoning behind their predictions. White box systems, in contrast, can, by nature, easily generate explanations. However, their predictions are less accurate than sophisticated black box models. Recent research has demonstrated that explanations are an essential component in bringing the powerful predictions of big data and machine learning methods to a mass audience without compromising trust. Explanations can take a variety of formats, depending on the recommendation domain and the machine learning model used to make predictions. The objective of this work is to build a recommender system that can generate both accurate predictions and semantically rich explanations that justify the predictions. We propose a novel approach to build an explanation generation mechanism into a latent factor-based black box recommendation model. The designed model is trained to learn to make predictions that are accompanied by explanations that are automatically mined from the semantic web. Our evaluation experiments, which carefully study the trade-offs between the quality of predictions and explanations, show that our proposed approach succeeds in producing explainable predictions without a significant sacrifice in prediction accuracy.},
keywords={Recommender systems;Semantics;Motion pictures;Semantic Web;Encyclopedias;Electronic publishing;Artificial intelligence;recommender systems;collaborative filtering;matrix factorization;explanations;semantic web},
doi={10.1109/ACCESS.2019.2934633},
ISSN={2169-3536},
month={},}