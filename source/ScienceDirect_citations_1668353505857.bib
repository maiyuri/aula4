@article{KLAIB2021114037,
title = {Eye tracking algorithms, techniques, tools, and applications with an emphasis on machine learning and Internet of Things technologies},
journal = {Expert Systems with Applications},
volume = {166},
pages = {114037},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114037},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420308071},
author = {Ahmad F. Klaib and Nawaf O. Alsrehin and Wasen Y. Melhem and Haneen O. Bashtawi and Aws A. Magableh},
keywords = {Eye tracking techniques, Eye tracking applications, Electrooculography, Infrared oculography, Internet of Things, Machine learning, Scleral coil, Video oculography, Cloud computing, Fog computing, Choice modeling, Consumer psychology, Marketing},
abstract = {Eye tracking is the process of measuring where one is looking (point of gaze) or the motion of an eye relative to the head. Researchers have developed different algorithms and techniques to automatically track the gaze position and direction, which are helpful in different applications. Research on eye tracking is increasing owing to its ability to facilitate many different tasks, particularly for the elderly or users with special needs. This study aims to explore and review eye tracking concepts, methods, and techniques by further elaborating on efficient and effective modern approaches such as machine learning (ML), Internet of Things (IoT), and cloud computing. These approaches have been in use for more than two decades and are heavily used in the development of recent eye tracking applications. The results of this study indicate that ML and IoT are important aspects in evolving eye tracking applications owing to their ability to learn from existing data, make better decisions, be flexible, and eliminate the need to manually re-calibrate the tracker during the eye tracking process. In addition, they show that eye tracking techniques have more accurate detection results compared with traditional event-detection methods. In addition, various motives and factors in the use of a specific eye tracking technique or application are explored and recommended. Finally, some future directions related to the use of eye tracking in several developed applications are described.}
}
@article{WAGNER2021863,
title = {The Digital Twin in Order Processing},
journal = {Procedia CIRP},
volume = {104},
pages = {863-868},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.145},
url = {https://www.sciencedirect.com/science/article/pii/S221282712101043X},
author = {Sarah Bernadette Wagner and Michael Milde and Gunther Reinhart},
keywords = {digital twin, order processing, decision support, production, logistic network},
abstract = {Environmental factors, such as the high number of product variants, increase the complexity of order processing (OP). Holistic decision making presents a challenge due to the lack of transparency in OP, an inadequate data basis, and the unknown effects of decision alternatives. Today’s information systems and traditional calculation formulas for managing production only consider subsystems and isolated abstraction levels. As digital twins introduce new opportunities for decision support, their potential in OP is widely recognized, while their designing process still lacks research and methodologies. Consequently, we present a digital twin in OP, which enables efficient decision support.}
}
@incollection{MOURRE2023335,
title = {Chapter 10 - Mediterranean observing and forecasting systems},
editor = {Katrin Schroeder and Jacopo Chiggiato},
booktitle = {Oceanography of the Mediterranean Sea},
publisher = {Elsevier},
pages = {335-386},
year = {2023},
isbn = {978-0-12-823692-5},
doi = {https://doi.org/10.1016/B978-0-12-823692-5.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128236925000017},
author = {Baptiste Mourre and Emanuela Clementi and Giovanni Coppini and Laurent Coppola and Gerasimos Korres and Antonio Novellino and Enrique Alvarez-Fanjul and Pierre Daniel and George Zodiatis and Katrin Schroeder and Joaquín Tintoré},
keywords = {Data integration, Data management, Data product, Forecasting systems, Observing systems, Open access},
abstract = {Sustained observations and forecasting systems are fundamental to advance our knowledge and understanding of the functioning of the Mediterranean Sea and its ecosystems, and to efficiently respond to maritime emergencies, societal needs, and preservation threats. This chapter describes the present status of the Mediterranean observing and forecasting systems that were successfully developed over the recent decades, thanks to both national and regional investments and fruitful international collaborations. On the one hand, the complementarity between global observing systems and multiplatform regional observatories enables a systematic monitoring of the Mediterranean Sea from the basin to the coastal scales. On the other hand, operational high-resolution modeling combined with data assimilation procedures provide the base for integrated model-data forecasting systems generating timely predictions of the short-term evolution of marine conditions and realistic representations of past periods. Data assembly centers are essential backbone of these systems, allowing data standardization, quality control, distribution, and archiving.}
}
@incollection{ASHENDEN202127,
title = {Chapter 3 - Data types and resources},
editor = {Stephanie Kay Ashenden},
booktitle = {The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry},
publisher = {Academic Press},
pages = {27-60},
year = {2021},
isbn = {978-0-12-820045-2},
doi = {https://doi.org/10.1016/B978-0-12-820045-2.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200452000040},
author = {Stephanie Kay Ashenden and Sumit Deswal and Krishna C. Bulusu and Aleksandra Bartosik and Khader Shameer},
keywords = {Data, Omics, FAIR, Big data, SMILES, InChI},
abstract = {Recent innovation in the field of machine learning has been enabled by the confluence of three advances: rapid expansion of affordable computing power in the form of cloud computing environments, the accelerating pace of infrastructure associated with large-scale data collection and rapid methodological advancements, particularly neural network architecture improvements. Development and adoption of these advances have lagged in the health care domain largely due to restrictions around public use of data and siloed nature of these datasets with respect to providers, payers and clinical trial sponsors.}
}
@article{SCHARLER2021105020,
title = {Network construction, evaluation and documentation: A guideline},
journal = {Environmental Modelling & Software},
volume = {140},
pages = {105020},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105020},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221000633},
author = {U.M. Scharler and S.R. Borrett},
keywords = {Weighted networks, Ecosystem, Socio-economic, Best-practice, Plausibility, Sensitivity},
abstract = {Network analysis of complex systems is a rapidly growing field. Both theoretical and empirical network studies have permeated many different ecological, biological, social, and economic fields, investigating the interrelationships between nodes as structural and functional attributes in static, time-dynamic, or spatially explicit formats. We consider the network construction phase as a vital, but neglected component, and therefore provide recommended guidelines, describe how to evaluate the resulting network model quality, and highlight tools to assess their plausibility. Thereby we stress the importance of constructing multiple plausible networks to comply with basic scientific standards, and to pave the way for better informed evaluations. Finally, we provide recommendations for the management and policy arena where we advocate a thorough interrogation of network analyses outcomes (metrics) especially with regard to their sensitivity to the construction process, and a focus on relative changes between and within systems (e.g. as indication of vulnerability), rather than strict benchmarks.}
}
@article{PUCCETTI2023122160,
title = {Technology identification from patent texts: A novel named entity recognition method},
journal = {Technological Forecasting and Social Change},
volume = {186},
pages = {122160},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.122160},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522006813},
author = {Giovanni Puccetti and Vito Giordano and Irene Spada and Filippo Chiarello and Gualtiero Fantoni},
keywords = {Information retrieval, Named entity recognition, Natural language processing, Patents, Technology analysis},
abstract = {Identifying technologies is a key element for mapping a domain and its evolution. It allows managers and decision makers to anticipate trends for an accurate forecast and effective foresight. Researchers and practitioners are taking advantage of the rapid growth of the publicly accessible sources to map technological domains. Among these sources, patents are the widest technical open access database used in the literature and in practice. Nowadays, Natural Language Processing (NLP) techniques enable new methods for the analysis of patent texts. Among these techniques, in this paper we explore the use of Named Entity Recognition (NER) with the purpose to identify the technologies mentioned in patents' text. We compare three different NER methods, gazetteer-based, rule-based and deep learning-based (e.g. BERT), measuring their performances in terms of precision, recall and computational time. We test the approaches on 1600 patents from four assorted IPC classes as case studies. Our NER systems collected over 4500 fine-grained technologies, achieving the best results thanks to the combination of the three methodologies. The proposed method overcomes the literature thanks to the ability to filter generic technological terms. Our study delineates a valid technology identification tool that can be integrated in any text analysis pipeline to support academics and companies in investigating a technological domain.}
}
@article{ZHANG2021112265,
title = {An automated, generalized, deep-learning-based method for delineating the calving fronts of Greenland glaciers from multi-sensor remote sensing imagery},
journal = {Remote Sensing of Environment},
volume = {254},
pages = {112265},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.112265},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720306386},
author = {Enze Zhang and Lin Liu and Lingcao Huang and Ka Shing Ng},
abstract = {In the past two decades, the data volume of remote sensing imagery in the polar regions has increased dramatically. The calving fronts of many Greenland glaciers have been undergoing substantial variations, and a comprehensive front dataset is necessary for better understanding such frontal dynamics. Therefore, there is a need for an automated approach to identifying glaciological features such as calving fronts. In 2019, three deep-learning-based methods were applied to calving front delineation, but were restricted to a specific area or dataset. Here, we develop a more generalized method that can be applied to a major outlet glacier or remote sensing datasets that are not included in the training. We integrate seven remote sensing datasets into a single deep learning network. The core datasets include optical (Landsat-8 and Sentinel-2) and synthetic aperture radar images (Envisat, ALOS-1 TerraSAR-X, Sentinel-1, and ALOS-2) taken over Jakobshavn Isbræ, Kangerlussuaq, and Helheim, spanning from 2002 to 2019. We evaluate four neural network architectures (e.g., U-Net, DeepLabv3+ with ResNet, DRN, and MobileNet as the backbones) and three histogram modification strategies (e.g., histogram normalization, linear stretching, and no histogram modification). We find that the combination of histogram normalization and DRN-DeepLabv3+ has the lowest test error, at 86 m. These promising results show that our method has a high generalization ability on various glaciers and data types.}
}
@article{BRIEN2023104668,
title = {Automated crack classification for the CERN underground tunnel infrastructure using deep learning},
journal = {Tunnelling and Underground Space Technology},
volume = {131},
pages = {104668},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2022.104668},
url = {https://www.sciencedirect.com/science/article/pii/S088677982200308X},
author = {Darragh O 'Brien and John {Andrew Osborne} and Eliseo Perez-Duenas and Roddy Cunningham and Zili Li},
keywords = {Image processing, Transfer learning, Crack detection, Crack classification, Tunnel assessment, Convolution Neural Network},
abstract = {One early sign of tunnel structure deterioration originates in the form of cracking, and therefore crack detection and resultant classification is integral for tunnel structural inspection and maintenance. Conventionally tunnel cracks are manually recorded and classified by trained professionals, which is costly, time-consuming and inevitably subjective. Recent advances in deep learning space have allowed for automatic cracks detection algorithms to be developed and subsequently utilized in structural health assessment of surface buildings, bridges, roads and other civil infrastructure. Nevertheless, these methods of development underperform when implemented for a tunnel structure in an underground environment due to the disparity of illumination combined with the congested image data caused by pipes, steel mesh, wires, and other tunnel facilities. To overcome these challenges, this paper proposes an innovative image-based crack detection method accompanied with crack classification using Convolution Neural Network (CNN) specifically for underground infrastructure environment. Unlike conventional CNN development from scratch, the proposed CNN incorporates Transfer Learning in the form of the VGG16 model with weights transferred from ImageNet. The transfer model was trained under different scenarios in order to find the optimal model for the novel dataset. The various models are trained using over 10′000 images validated on 2′500 images all of which are 256 × 256 pixels in size, these models are all subsequently tested using 30 images 3072 × 4096 pixels in size all of which are collected from the underground infrastructure at CERN. The Transfer Learning model used outperforms that of the traditional CNN training method of training from scratch. The optimum transfer model accomplished better testing metrics 96.6%,87.3%,92.4%,89.3% for Accuracy, Precision, Recall and F1 score respectively all of which were achieved with a shorter training time. The proposed CNN determines the existence and location of cracks within an image which are then subjected to a secondary classification CNN where the crack is categorized into one of the four crack classes which include the three directional classes of Horizontal, vertical and diagonal with the last crack classes incorporated to represent complex crack regions. The secondary classification CNN attains an Accuracy of 92.3% a Precision of 83.9% a Recall value of 82.3 % and a final F1 score of 81.5%. The performance of the proposed methods shows that a CNN crack detector/classifier can effectively overwhelm the unfavourable tunnel environment and accomplish results to a high standard.}
}
@article{NAKALEMBE2021100543,
title = {A review of satellite-based global agricultural monitoring systems available for Africa},
journal = {Global Food Security},
volume = {29},
pages = {100543},
year = {2021},
issn = {2211-9124},
doi = {https://doi.org/10.1016/j.gfs.2021.100543},
url = {https://www.sciencedirect.com/science/article/pii/S2211912421000523},
author = {Catherine Nakalembe and Inbal Becker-Reshef and Rogerio Bonifacio and Guangxiao Hu and Michael Laurence Humber and Christina Jade Justice and John Keniston and Kenneth Mwangi and Felix Rembold and Shraddhanand Shukla and Ferdinando Urbano and Alyssa Kathleen Whitcraft and Yanyun Li and Mario Zappacosta and Ian Jarvis and Antonio Sanchez},
keywords = {Satellite Earth Observations, Scalable, Operational agriculture monitoring, Open access, Africa},
abstract = {The increasing frequency and severity of extreme climatic events and their impacts are being realized in many regions of the world, particularly in smallholder crop and livestock production systems in Sub-Saharan Africa (SSA). These events underscore the need for timely early warning. Satellite Earth Observation (EO) availability, rapid developments in methodology to archive and process them through cloud services and advanced computational capabilities, continue to generate new opportunities for providing accurate, reliable, and timely information for decision-makers across multiple cropping systems and for resource-constrained institutions. Today, systems and tools that leverage these developments to provide open access actionable early warning information exist. Some have already been employed by early adopters and are currently operational in selecting national monitoring programs in Angola, Kenya, Rwanda, Tanzania, and Uganda. Despite these capabilities, many governments in SSA still rely on traditional crop monitoring systems, which mainly rely on sparse and long latency in situ reports with little to no integration of EO-derived crop conditions and yield models. This study reviews open-access operational agricultural monitoring systems available for Africa. These systems provide the best-available open-access EO data that countries can readily take advantage of, adapt, adopt, and leverage to augment national systems and make significant leaps (timeliness, spatial coverage and accuracy) of their monitoring programs. Data accessible (vegetation indices, crop masks) in these systems are described showing typical outputs. Examples are provided including crop conditions maps, and damage assessments and how these have integrated into reporting and decision-making. The discussion compares and contrasts the types of data, assessments and products can expect from using these systems. This paper is intended for individuals and organizations seeking to access and use EO to assess crop conditions who might not have the technical skill or computing facilities to process raw data into informational products.}
}
@article{ATTYE2021117927,
title = {TractLearn: A geodesic learning framework for quantitative analysis of brain bundles},
journal = {NeuroImage},
volume = {233},
pages = {117927},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.117927},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921002044},
author = {Arnaud Attyé and Félix Renard and Monica Baciu and Elise Roger and Laurent Lamalle and Patrick Dehail and Hélène Cassoudesalle and Fernando Calamante},
keywords = {Diffusion MRI, Fiber tractography, Precision medicine, Manifold learning},
abstract = {Deep learning-based convolutional neural networks have recently proved their efficiency in providing fast segmentation of major brain fascicles structures, based on diffusion-weighted imaging. The quantitative analysis of brain fascicles then relies on metrics either coming from the tractography process itself or from each voxel along the bundle. Statistical detection of abnormal voxels in the context of disease usually relies on univariate and multivariate statistics models, such as the General Linear Model (GLM). Yet in the case of high-dimensional low sample size data, the GLM often implies high standard deviation range in controls due to anatomical variability, despite the commonly used smoothing process. This can lead to difficulties to detect subtle quantitative alterations from a brain bundle at the voxel scale. Here we introduce TractLearn, a unified framework for brain fascicles quantitative analyses by using geodesic learning as a data-driven learning task. TractLearn allows a mapping between the image high-dimensional domain and the reduced latent space of brain fascicles using a Riemannian approach. We illustrate the robustness of this method on a healthy population with test-retest acquisition of multi-shell diffusion MRI data, demonstrating that it is possible to separately study the global effect due to different MRI sessions from the effect of local bundle alterations. We have then tested the efficiency of our algorithm on a sample of 5 age-matched subjects referred with mild traumatic brain injury. Our contributions are to propose: 1/ A manifold approach to capture controls variability as standard reference instead of an atlas approach based on a Euclidean mean. 2/ A tool to detect global variation of voxels’ quantitative values, which accounts for voxels’ interactions in a structure rather than analyzing voxels independently. 3/ A ready-to-plug algorithm to highlight nonlinear variation of diffusion MRI metrics. With this regard, TractLearn is a ready-to-use algorithm for precision medicine.}
}
@article{HE2021107889,
title = {Rapidly assessing earthquake-induced landslide susceptibility on a global scale using random forest},
journal = {Geomorphology},
volume = {391},
pages = {107889},
year = {2021},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2021.107889},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X2100297X},
author = {Qian He and Ming Wang and Kai Liu},
keywords = {Landslide susceptibility, Random forest model, Earthquake, Global scale},
abstract = {Earthquake-induced landslides (EQILs) are an incredibly destructive geological disaster. Rapid landslide susceptibility assessments are indispensable and critical for risk analysis and emergency management. Previous studies mainly focus on the regional-scale assessment of EQIL susceptibility, while the global analyses of that are lacking. In this study, we constructed a global model for rapidly assessing earthquake-induced landslide susceptibility based on the random forest (RF) algorithm using globally available data. In total, 288,114 landslides from 16 high-quality EQIL inventories were utilized to develop the global landslide model. We split the data into 70% training dataset for model training and 30% testing data for model evaluation. We also used three blind test events to validate the model performance. The model showed excellent performance on the testing data (accuracy = 0.945, and AUC = 0.985). The RF model exhibited strong spatial generalizability and robustness, with an AUC exceeding 0.8 for each landslide inventory and showing good performance on the blind test events. The resulting landslide susceptibility maps also match relatively well with the actual landslide locations. Among the conditioning factors, modified Mercalli intensity (MMI), elevation and slope are the three most important conditioning factors. The susceptibility maps for each landslide event were produced. The developed RF model would be useful in studies of earthquake-induced landslide susceptibility and emergency response after an earthquake.}
}
@article{NINCEVICPASALIC2021102127,
title = {Smart city research advances in Southeast Europe},
journal = {International Journal of Information Management},
volume = {58},
pages = {102127},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102127},
url = {https://www.sciencedirect.com/science/article/pii/S026840121931477X},
author = {Ivana {Ninčević Pašalić} and Maja Ćukušić and Mario Jadrić},
keywords = {Smart cities, Southeast Europe, Descriptive literature review},
abstract = {Smart city (SC) research is an engaging research area as evidenced by a rising number of publications indexed in the most relevant global citation databases. However, research advances are not equally discussed and distributed within Europe. This study puts a focus on the specific geographic location of Southeast Europe (SEE), intending to fill the gap in understanding the research advances in this part of Europe. The aim of this descriptive review was to systematically investigate peer-reviewed publications focused on SC research in SEE in order to present the findings and the state-of-art in this research domain. Seventy-four papers were thoroughly studied, analysed and classified based on their focus on SC themes and common sub-themes. While smart governance had been studied extensively in the SEE region, topics related to the smart economy and smart people received low attention from researchers. Mapping the selected papers to the Plan-Do-Check-Act (PDCA) cycle showed that SC research in SEE is still in the conceptualising and planning stages, with very little evidence from the real implementation and follow-up activities. From the stakeholders’ perspective, the focus is on the institutional point of view as most of the papers present their findings in relation to (national or local) government bodies or policies, without balancing with corresponding businesses’ or individuals’ (users’) point of view. In general, user involvement was found to be very low in regards to current SC research in the SEE region.}
}
@article{YANG2021103346,
title = {Quantifying spatiotemporal patterns of shrinking cities in urbanizing China: A novel approach based on time-series nighttime light data},
journal = {Cities},
volume = {118},
pages = {103346},
year = {2021},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2021.103346},
url = {https://www.sciencedirect.com/science/article/pii/S0264275121002468},
author = {Yang Yang and Jianguo Wu and Ying Wang and Qingxu Huang and Chunyang He},
keywords = {Shrinking cities, Urban shrinkage, Nighttime light, Urban sustainability, Urbanization, China},
abstract = {The shrinking of cities has become an increasingly global phenomenon, posing challenges for sustainable urban development. However, most focus remains on Europe and North America, and relatively little attention has been paid to the East Asia, especially the urbanizing China. Nighttime light (NL) dataset and its features (long-term time-series free access and large coverage) provide an alternative means to quantify shrinking cities. Here, we developed a new approach to identify shrinking cities and measure urban shrinkage, using corrected-integrated DMSP/OLS and NPP/VIIRS NL data. Based on this approach, we quantified the spatiotemporal patterns of shrinking cities in China from 1992 to 2019. Our study identified 153 shrinking cities in China during the study period, accounting for 23.39% of all 654 cities. These shrinking cities were widely distributed across eight economic regions and most provinces. The number of shrinking cities changed periodically and peaked following the Asian Financial Crisis in 1997 and again after the Global Economic Crisis in 2008. The cities that experienced the greatest shrinkage intensity were mainly distributed in northeast China, with severe urban shrinkage occurring between 2008 and 2013. The new approach proposed in this study can effectively identify shrinking city hotspots and key periods of urban shrinkage. Our findings suggest that sustainable urban development in China must consider shrinking cities, which are faced with challenging and urgent sustainability issues different from those by rapidly growing cities.}
}
@article{WANG2021128952,
title = {How and when higher climate change risk perception promotes less climate change inaction},
journal = {Journal of Cleaner Production},
volume = {321},
pages = {128952},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128952},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621031449},
author = {Changcheng Wang and Liuna Geng and Julián D. Rodríguez-Casallas},
keywords = {Climate change risk perception, Sustainable development, Climate change inaction, Mindfulness, Climate change belief, Environmental efficacy},
abstract = {Climate change has been positioned as one of the most severe environmental threats facing us today. To address climate change, enhancing climate change risk perception and reducing climate change inaction are both critical. However, little research has touched on the issue of whether climate change risk perception is linked to climate change inaction in a negative manner. Moreover, there is still much unknown about the complex process behind this relationship, and the boundary conditions of this process awaits clarification. To address these gaps in the literature, two studies were conducted to first confirm the possible negative association between climate change risk perception and climate change inaction and, second, explore through a parallel mediational model whether climate change belief and environmental efficacy mediate simultaneously the relation between climate change risk perception and climate change inaction. Finally, a moderated sequential mediational model was used to investigate whether climate change risk perception is associated with climate change inaction through the sequential mediation of climate change belief and environmental efficacy, and to clarify underlying boundary conditions by analyzing the moderation of mindfulness as well. The results showed that, as expected, higher levels of climate change risk perception were related to less climate change inaction, and this relation was mediated by enhanced climate change belief and heightened environmental efficacy in a sequential manner. Furthermore, the sequential mediating effect of climate change belief and environmental efficacy was stronger among those who had a higher level of mindfulness. These findings advance the emerging research on climate change inaction by elucidating the mechanisms underlying the effect of climate change risk perception. Moreover, they extend the Domain-Context-Behavior (DCB) model and Gateway belief model (GBM). In practice, climate change education and climate change inaction interventions can be designed and implemented to nudge clean production, green supply chain and green consumption, which finally contribute to sustainable development and the ‘green transformation’ of society.}
}
@article{MA2021224,
title = {A novel rumor detection algorithm based on entity recognition, sentence reconfiguration, and ordinary differential equation network},
journal = {Neurocomputing},
volume = {447},
pages = {224-234},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.03.055},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221004392},
author = {Tinghuai Ma and Honghao Zhou and Yuan Tian and Najla Al-Nabhan},
keywords = {Rumor detection, Entity recognition, Sentence reconfiguration, ODE-net},
abstract = {Social media has recently become one of the most used media in the world. This has resulted in a great hotbed for the growth of rumors, as anyone can spread knowledge and opinions without confirmation. Previous works on rumor detection focused on hand-extracted features and spent less effort on text representation. In this research, a novel method for rumor detection on social media is proposed, which integrates entity recognition, sentence reconfiguration and ordinary differential equation network under a unified framework called ESODE. An entity recognition method to enhance the semantic understanding of rumor texts is used. Then, a sentence reconfiguration to improve the frequency of important words is designed. The complete feature map is established by further collecting statistical features from three aspects: linguistic features on the content of rumors, characteristics of users involved in rumor propagating, and propagation network structures. Finally, the ordinary differential equation network (ODEnet) is applied to detect rumors. Experimental results on datasets from Twitter and Weibo show that the proposed method achieves better performance than previous ones.}
}
@article{ALMEIDA2021113538,
title = {Tun-OCM: A model-driven approach to support database tuning decision making},
journal = {Decision Support Systems},
volume = {145},
pages = {113538},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2021.113538},
url = {https://www.sciencedirect.com/science/article/pii/S0167923621000488},
author = {Ana Carolina Almeida and Fernanda Baião and Sérgio Lifschitz and Daniel Schwabe and Maria Luiza M. Campos},
keywords = {Database systems, Tuning decision, Heuristics, Configuration management, Ontology pattern language},
abstract = {Database tuning is a task executed by Database Administrators (DBAs) based on their practical experience and on tuning systems, which support DBA actions towards improving the performance of a database system. It is notoriously a complex task that requires precise domain knowledge about possible database configurations. Ideally, a DBA should keep track of several Database Management Systems (DBMS) parameters, configure data structures, and must be aware about possible interferences among several database (DB) configurations. We claim that an automatic tuning system is a decision support system and DB tuning may also be seen as a configuration management task. Therefore, we may characterize it by means of a formal domain conceptualization, benefiting from existing control practices and computational support in the configuration management domain. This work presents Tun-OCM, a conceptual model represented as a well-founded ontology, that encompasses a novel characterization of the database tuning domain as a configuration management conceptualization to support decision making. We develop and represent Tun-OCM using the CM-OPL methodology and its underlying language. The benefits of Tun-OCM are discussed by instantiating it in a real scenario.}
}
@article{TERZIYAN2021676,
title = {Taxonomy of generative adversarial networks for digital immunity of Industry 4.0 systems},
journal = {Procedia Computer Science},
volume = {180},
pages = {676-685},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.290},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003392},
author = {Vagan Terziyan and Svitlana Gryshko and Mariia Golovianko},
keywords = {Industry 4.0, Generative Adversarial Networks, cybersecurity, artificial digital immunity},
abstract = {Industry 4.0 systems are extensively using artificial intelligence (AI) to enable smartness, automation and flexibility within variety of processes. Due to the importance of the systems, they are potential targets for attackers trying to take control over the critical processes. Attackers use various vulnerabilities of such systems including specific vulnerabilities of AI components. It is important to make sure that inappropriate adversarial content will not break the security walls and will not harm the decision logic of critical systems. We believe that the corresponding security toolset must be organized as a trainable self-protection mechanism similar to immunity. We found certain similarities between digital vs. biological immunity and we study the possibilities of Generative Adversarial Networks (GANs) to provide the basis for the digital immunity training. We suggest the taxonomy of GANs (including new architectures) suitable to simulate various aspects of the immunity for Industry 4.0 applications.}
}
@article{DORFLEITNER2021111378,
title = {Blockchain applications for climate protection: A global empirical investigation},
journal = {Renewable and Sustainable Energy Reviews},
volume = {149},
pages = {111378},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111378},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121006638},
author = {Gregor Dorfleitner and Franziska Muck and Isabel Scheckenbach},
keywords = {Blockchain, Distributed ledger, Green finance, Consensus mechanisms, Peer-to-peer transactions, Sustainability goals},
abstract = {Our research consolidates the actual environment of blockchain applications that contribute in a certain way to climate protection. In view of the growing interest in climate change and the need to act on a global scale, knowledge about these applications enables investors, politicians, and citizens to drive this development forward through diverse support opportunities. This article provides an extensive overview of existing mitigation and adaptation measures based on blockchain technology. We collect data on 85 such applications and describe the empirical distributions of different attributes of these applications. In a logit regression, we analyze which application-specific and blockchain-specific characteristics determine the success of an application in the sense of an advanced operational status. We find evidence that applications of the type “energy trading” exhibit reduced chances of success, while green blockchain-based applications implementing a proof-of-stake consensus mechanism are more likely to become operational. Moreover, pursuing an initial coin offering has no significant effect on the success of an application. Our work provides the basis for a better understanding of the success factors of this new technology.}
}
@article{BRANCHER2021117153,
title = {Increased ozone pollution alongside reduced nitrogen dioxide concentrations during Vienna’s first COVID-19 lockdown: Significance for air quality management},
journal = {Environmental Pollution},
volume = {284},
pages = {117153},
year = {2021},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2021.117153},
url = {https://www.sciencedirect.com/science/article/pii/S0269749121007351},
author = {Marlon Brancher},
keywords = {COVID-19 lockdown, Air quality data, Atmospheric composition, Meteorology, Machine learning},
abstract = {Background
Lockdowns amid the COVID-19 pandemic have offered a real-world opportunity to better understand air quality responses to previously unseen anthropogenic emission reductions.
Methods and main objective
This work examines the impact of Vienna’s first lockdown on ground-level concentrations of nitrogen dioxide (NO2), ozone (O3) and total oxidant (Ox). The analysis runs over January to September 2020 and considers business as usual scenarios created with machine learning models to provide a baseline for robustly diagnosing lockdown-related air quality changes. Models were also developed to normalise the air pollutant time series, enabling facilitated intervention assessment.
Core findings
NO2 concentrations were on average −20.1% [13.7–30.4%] lower during the lockdown. However, this benefit was offset by amplified O3 pollution of +8.5% [3.7–11.0%] in the same period. The consistency in the direction of change indicates that the NO2 reductions and O3 increases were ubiquitous over Vienna. Ox concentrations increased slightly by +4.3% [1.8–6.4%], suggesting that a significant part of the drops in NO2 was compensated by gains in O3. Accordingly, 82% of lockdown days with lowered NO2 were accompanied by 81% of days with amplified O3. The recovery shapes of the pollutant concentrations were depicted and discussed. The business as usual-related outcomes were broadly consistent with the patterns outlined by the normalised time series. These findings allowed to argue further that the detected changes in air quality were of anthropogenic and not of meteorological reason. Pollutant changes on the machine learning baseline revealed that the impact of the lockdown on urban air quality were lower than the raw measurements show. Besides, measured traffic drops in major Austrian roads were more significant for light-duty than for heavy-duty vehicles. It was also noted that the use of mobility reports based on cell phone movement as activity data can overestimate the reduction of emissions for the road transport sector, particularly for heavy-duty vehicles. As heavy-duty vehicles can make up a large fraction of the fleet emissions of nitrogen oxides, the change in the volume of these vehicles on the roads may be the main driver to explain the change in NO2 concentrations.
Interpretation and implications
A probable future with emissions of volatile organic compounds (VOCs) dropping slower than emissions of nitrogen oxides could risk worsened urban O3 pollution under a VOC-limited photochemical regime. More holistic policies will be needed to achieve improved air quality levels across different regions and criteria pollutants.}
}
@article{SHI2021103873,
title = {Working stage identification of excavators based on control signals of operating handles},
journal = {Automation in Construction},
volume = {130},
pages = {103873},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103873},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003241},
author = {Yupeng Shi and Yimin Xia and Lianglin Luo and Zhihong Xiong and Chengyu Wang and Laikuang Lin},
keywords = {Excavator, Working stage identification, Control signal, Operating handle, Long short-term memory (LSTM)},
abstract = {To improve the automated management level on construction sites, real-time monitoring of excavators' working stage for production efficiency and economic consumption analysis has been implemented in many projects, revealing great advantages. However, existing vision-based and non-vision-based working stage identification methods ignore the influence of response delay of hydraulic system on the recognition results. To overcome this problem, three machine learning algorithms, which select the control signals of operating handles that can reflect the actuator real-time operating status as segmentation marks, are used to establish the excavator working stage identification model in this study. The results show that the Long Short-Term Memory (LSTM) classifier has an accuracy of 93.21% and effectively reduces the lagging misidentification to 4.68%. This study contributes to automatic measurement of the excavator operational efficiency. For future work, the approach is combined with the adjustment strategy of engine working point to realize the staged energy-saving control of excavators.}
}
@article{TIAN2021102727,
title = {Using data monitoring algorithms to physiological indicators in motion based on Internet of Things in smart city},
journal = {Sustainable Cities and Society},
volume = {67},
pages = {102727},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102727},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721000226},
author = {Jian Tian and Lulu Gao},
keywords = {Internet of Things, Data fusion algorithm, Physiological indicators, Monitoring, Smart city},
abstract = {This article discusses the monitoring of physiological indicators during exercise, combined with the data fusion algorithm of the smart city Internet of Things health. We use the hash value of the tuple key to the corresponding data block of the node, use the data block record to obtain the response of the target node, and output the data tuple. It is used as a measure of the load balance of health data streams to determine whether load migration is needed and to determine the way and amount of migration tasks to make migration decisions. The simulation experiments show that the method has good computational performance and dynamic load balancing. A series of mean arterial pressure and heart rate of patients and non-stationary health data, and a series of blood pressure and heart rate of health individuals in different postures are selected to perform experiments to analyze the transfer function and power spectra in the model, validating that the model can be used to reveal the changes associated with severe systemic response syndrome (SIRS), providing a hypothesis for the decomposition of autoregulation of physiological control under health and disease conditions.}
}
@article{WATERMANN2021103591,
title = {Predicting the self-regulated job search of mature-aged job seekers: The use of elective selection, loss-based selection, optimization, and compensation strategies},
journal = {Journal of Vocational Behavior},
volume = {128},
pages = {103591},
year = {2021},
issn = {0001-8791},
doi = {https://doi.org/10.1016/j.jvb.2021.103591},
url = {https://www.sciencedirect.com/science/article/pii/S0001879121000634},
author = {Henriette Watermann and Ulrike Fasbender and Ute-Christine Klehe},
keywords = {Aging, Job search, Mature-aged job seekers, Reemployment efficacy, Self-regulation, SOC strategies},
abstract = {Job search is a demanding and often demotivating process, challenging job-seekers' self-regulation. Particularly, mature-aged job seekers face lower reemployment chances – and may benefit from strategies known from the lifespan literature. The current study examined whether and when the use of aging strategies (elective selection, loss-based selection, optimization, and compensation; SOC strategies) can support mature-aged job seekers in their self-regulated job search process (goal establishment and goal pursuit). We collected data from 659 mature-aged job seekers in three countries (Germany, United Kingdom, and United States) at four different times over two months. Results of multi-level modeling showed no support for gain-oriented strategies, namely elective selection (prioritizing one instead of multiple goals) and optimization (investing every effort to reach one's goal). In contrast, loss-oriented strategies, namely loss-based selection (prioritizing or selecting a new goal after a setback) and compensation (using new or previously unused means in the face of obstacles), supported mature-aged job seekers' goal establishment and goal pursuit. Moreover, with increasing age, mature-aged job seekers reported lower reemployment efficacy (the confidence to find a new job), which moderated the relation between compensation with goal pursuit. Compensation was particularly helpful for mature-aged job seekers' goal pursuit in weeks in which they reported lower (vs. higher) reemployment efficacy. These findings highlight the importance of loss-oriented aging strategies as beneficial coping strategies. With regard to practice, the present study speaks to the benefits of SOC strategies and points to the development of interventions targeted toward mature-aged job seekers.}
}
@article{GALLIN2021101734,
title = {Measuring aggregate housing wealth: New insights from machine learning ☆},
journal = {Journal of Housing Economics},
volume = {51},
pages = {101734},
year = {2021},
issn = {1051-1377},
doi = {https://doi.org/10.1016/j.jhe.2020.101734},
url = {https://www.sciencedirect.com/science/article/pii/S105113772030070X},
author = {Joshua Gallin and Raven Molloy and Eric Nielsen and Paul Smith and Kamila Sommer},
keywords = {Residential real estate, Consumer economics and finance, Data collection and estimation, Flow of funds},
abstract = {We construct a new measure of aggregate housing wealth for the U.S. based on (1) home-value estimates derived from machine learning algorithms applied to detailed information on property characteristics and recent transaction prices, and (2) Census housing unit counts. According to our new measure, the timing and amplitude of the recent house-price cycle differs materially but plausibly from commonly-used measures, which are based on survey data or repeat-sales price indexes. Thus, our methodology generates estimates that should be of considerable value to researchers and policymakers interested in the dynamics of aggregate housing wealth.}
}
@article{MARTINEZRIOS2021102813,
title = {A review of machine learning in hypertension detection and blood pressure estimation based on clinical and physiological data},
journal = {Biomedical Signal Processing and Control},
volume = {68},
pages = {102813},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102813},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421004109},
author = {Erick Martinez-Ríos and Luis Montesinos and Mariel Alfaro-Ponce and Leandro Pecchia},
keywords = {Hypertension, Clinical data, Physiological data, Machine learning},
abstract = {The use of machine learning techniques in medicine has increased in recent years due to a rise in publicly available datasets. These techniques have been applied in high blood pressure studies following two approaches: hypertension stage classification based on clinical data and blood pressure estimation based on related physiological signals. This paper presents a literature review on such studies. We aimed to identify the best practices, challenges, and opportunities in developing machine learning models to detect hypertension or estimate blood pressure using clinical data and physiological signals. Hence, we identified and examined the machine learning techniques, publicly available datasets, and predictors used in previous studies. The feature selection techniques used to reduce model complexity are also reviewed. We found a lack of studies combining socio-demographic or clinical data with physiological signals, despite the correlation of blood pressure with photoplethysmography waveforms and variables such as age, gender, body mass index, and heart rate. Therefore, there is an opportunity to increase model performance by using both types of data for hypertension detection or blood pressure monitoring.}
}
@article{GREENLEAF2021105414,
title = {How far can Convention 108+ ‘globalise’? Prospects for Asian accessions},
journal = {Computer Law & Security Review},
volume = {40},
pages = {105414},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2020.105414},
url = {https://www.sciencedirect.com/science/article/pii/S0267364920300194},
author = {Graham Greenleaf},
abstract = {The ‘globalisation’ of Council of Europe data protection Convention 108 through non-European accessions has continued steadily, with eight such accessions since the first in 2013. The ‘modernisation’ of the Convention was completed on 10 October 2018 when the amending protocol for the new ‘Convention 108+’ became open for signature. Any new countries from outside Europe wishing to accede will have to accede to both Convention 108 and the amending Protocol (ie to 108+). The standards required of the laws of acceding countries by 108+ are higher than those required by 108, and are arguably mid-way between 108 and those of the European Union's General Data Protection Regulation (GDPR). This article examines to what extent each of the 26 ‘countries’ (separate jurisdictions) in Asia are likely to be able to accede to 108+, if they wish to. As yet, none have acceded to 108. It proposes an efficient way to consider such a question across such a complex set of jurisdictions. Fifteen of the 26 Asian countries already have data privacy laws, and two others have official Bills for such laws. An assessment of the prospects for accession can be done by considering in order the following grounds which may be impediments to accession: Jurisdictions which are not States; States which are not democratic; Laws of inadequate scope; Laws lacking an independent data protection authority; Laws with substantive provisions falling short of 108+ ‘accession standards’; States with proposed Bills only; and States with no relevant laws or proposed Bills. The most difficult step in this procedure is in deciding which of the substantive provisions of 108+ constitute its ‘accession standards’, or elements essential for accession to be invited. Neither the Convention, nor the guidelines issued by its Consultative Committee, shed much light on this question. However, previous practice under Convention 108, show there is some flexibility involved. The article concludes with suggestions as to how such flexibility can be made more transparent, and observations on which Asian countries, in light of the seven step assessment carried out in the article, are the most likely candidates to be able to accede to 108+, in both the short and medium terms.}
}
@article{JIN2021111345,
title = {Building occupancy forecasting: A systematical and critical review},
journal = {Energy and Buildings},
volume = {251},
pages = {111345},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111345},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821006290},
author = {Yuan Jin and Da Yan and Adrian Chong and Bing Dong and Jingjing An},
keywords = {Occupancy prediction, Forecast, Occupant behavior, Building, Operation, Energy conservation},
abstract = {Indoor environment construction for occupants has high energy consumption; as such, occupancy plays a noteworthy role in the complete life cycle phase of buildings, including design, operation, and retrofitting. In the past few years, building occupancy, which is considered the basis of occupant behavior, has attracted increasing attention from researchers. There are increasing requirements for buildings to be both comfortable and energy efficient; with the development of detection methods and analyzing algorithms, occupancy prediction has become a topic of interest for building automation and energy conservation. Therefore, this article reviews the literature regarding future building occupancy predictions (forecasting). This review is distinguished from occupancy simulation and detection research and focuses on the research purpose, physical routine, and complete methodology of occupancy forecasting. First, the research purposes, including the application field and detailed requirements for occupancy forecasting, are summarized and analyzed. Next, an overall methodology of occupancy forecasting, including data acquisition, modeling techniques, and evaluation, is discussed in terms of issues affecting prediction performance. Finally, the current challenges and perspectives of occupancy forecasting are highlighted, considering the insights of natural characteristics, on-site implementation, valid dataset sharing, and research techniques. Overall, accurate and robust future occupancy predictions will help to improve building system operations and energy conservation.}
}
@article{IMKER2021102369,
title = {An examination of data reuse practices within highly cited articles of faculty at a research university},
journal = {The Journal of Academic Librarianship},
volume = {47},
number = {4},
pages = {102369},
year = {2021},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2021.102369},
url = {https://www.sciencedirect.com/science/article/pii/S0099133321000604},
author = {Heidi J. Imker and Hoa Luong and William H. Mischo and Mary C. Schlembach and Chris Wiley},
keywords = {Data reuse, Data sharing, Data management, Data services, Scopus API},
abstract = {Data sharing and reuse are regarded as important components of the research workflow and key elements in open science. While reuse is well-documented in some circumstances, the utility of data sharing for all domains is less clear, and limited evidence of wide-spread demand can make it challenging to justify effort and funds required to format, document, share, and preserve data. This paper describes a project that: (1) surveyed authors of highly cited papers published in 2015 at the University of Illinois at Urbana-Champaign in nine STEM disciplines to determine if data were generated for their article and their knowledge of reuse by other researchers, and (2) surveyed authors who cited these 2015 articles to ascertain whether they reused data from the original article and how that data was obtained. The project goal was to better understand data reuse in practice and to explore if research data from an initial publication was reused in subsequent publications. While the results revealed reuse in many situations (and deemed important in these cases), the survey results and researcher supplied comments also indicated that data does not play the same role in all studies or even in studies that build on previous ones.}
}
@article{VANLOOY2021103413,
title = {A quantitative and qualitative study of the link between business process management and digital innovation},
journal = {Information & Management},
volume = {58},
number = {2},
pages = {103413},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103413},
url = {https://www.sciencedirect.com/science/article/pii/S0378720620303517},
author = {Amy {Van Looy}},
keywords = {Business process management, Process change management, Life cycle management, Adoption, Process innovation, Digital innovation, Digital transformation, Survey, Expert panel},
abstract = {The information revolution leaves its mark on businesses, resulting in organizations looking for digital innovation (DI) to apply to their business processes and anticipate competitors. Since the interplay between business process management (BPM) and DI has been underdeveloped, this mixed-methods article investigates the strength and nature of the relationship. We supplement the findings of an international survey (stage 1) with explanations from an expert panel (stage 2) to generalize a positive yet moderate link because of manifold contextual factors affecting strategic decision-making. We extend the technology–organization–environment (TOE) framework and profile organizations along their digital process innovation (DPI) mastery in a readiness matrix.}
}
@incollection{BAKER20211,
title = {1 - Future directions in digital information: Scenarios and themes},
editor = {David Baker and Lucy Ellis},
booktitle = {Future Directions in Digital Information},
publisher = {Chandos Publishing},
pages = {1-15},
year = {2021},
series = {Chandos Digital Information Review},
isbn = {978-0-12-822144-0},
doi = {https://doi.org/10.1016/B978-0-12-822144-0.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822144000001X},
author = {David Baker and Lucy Ellis},
keywords = {COVID-19, Public memory, Digital disruption, Delphi study, Digital education and training, Service innovation, Digital-first, Harvester paradigm, Digital literacy, Digital inclusiveness},
abstract = {This chapter introduces the purpose and aims of the book and provides an overview of each of the 19 chapters in terms of methodological approach and what they tell us about the future of digital information. Subheadings indicate the major areas of discussion that emerge from the whole and which reflect the issues of our time that occupy the minds and principles of scholars and practitioners. The chapter takes a look at the source of trends in digital information access and provision and considers the current issues and contexts for service innovation. The discussion of the key themes is reinforced and augmented by the results of the Delphi exercise and by selected thought pieces both of which are presented in text boxes.}
}
@article{ALGAN2021106771,
title = {Image classification with deep learning in the presence of noisy labels: A survey},
journal = {Knowledge-Based Systems},
volume = {215},
pages = {106771},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106771},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121000344},
author = {Görkem Algan and Ilkay Ulusoy},
keywords = {Deep learning, Label noise, Classification with noise, Noise robust, Noise tolerant},
abstract = {Image classification systems recently made a giant leap with the advancement of deep neural networks. However, these systems require an excessive amount of labeled data to be adequately trained. Gathering a correctly annotated dataset is not always feasible due to several factors, such as the expensiveness of the labeling process or difficulty of correctly classifying data, even for the experts. Because of these practical challenges, label noise is a common problem in real-world datasets, and numerous methods to train deep neural networks with label noise are proposed in the literature. Although deep neural networks are known to be relatively robust to label noise, their tendency to overfit data makes them vulnerable to memorizing even random noise. Therefore, it is crucial to consider the existence of label noise and develop counter algorithms to fade away its adverse effects to train deep neural networks efficiently. Even though an extensive survey of machine learning techniques under label noise exists, the literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels. This paper aims to present these algorithms while categorizing them into one of the two subgroups: noise model based and noise model free methods. Algorithms in the first group aim to estimate the noise structure and use this information to avoid the adverse effects of noisy labels. Differently, methods in the second group try to come up with inherently noise robust algorithms by using approaches like robust losses, regularizers or other learning paradigms.}
}
@incollection{2021345,
title = {Index},
editor = {Shuai Li and John L. Hopper},
booktitle = {Twin and Family Studies of Epigenetics},
publisher = {Academic Press},
pages = {345-354},
year = {2021},
volume = {27},
series = {Translational Epigenetics},
isbn = {978-0-12-820951-6},
doi = {https://doi.org/10.1016/B978-0-12-820951-6.09992-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128209516099920}
}
@article{WANG20213829,
title = {Human population history at the crossroads of East and Southeast Asia since 11,000 years ago},
journal = {Cell},
volume = {184},
number = {14},
pages = {3829-3841.e21},
year = {2021},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2021.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0092867421006358},
author = {Tianyi Wang and Wei Wang and Guangmao Xie and Zhen Li and Xuechun Fan and Qingping Yang and Xichao Wu and Peng Cao and Yichen Liu and Ruowei Yang and Feng Liu and Qingyan Dai and Xiaotian Feng and Xiaohong Wu and Ling Qin and Fajun Li and Wanjing Ping and Lizhao Zhang and Ming Zhang and Yalin Liu and Xiaoshan Chen and Dongju Zhang and Zhenyu Zhou and Yun Wu and Hassan Shafiey and Xing Gao and Darren Curnoe and Xiaowei Mao and E. Andrew Bennett and Xueping Ji and Melinda A. Yang and Qiaomei Fu},
keywords = {ancient DNA, 12,000-year-old humans, deeply diverged ancestry, pre-farming, cross-interactions, admixture},
abstract = {Summary
Past human genetic diversity and migration between southern China and Southeast Asia have not been well characterized, in part due to poor preservation of ancient DNA in hot and humid regions. We sequenced 31 ancient genomes from southern China (Guangxi and Fujian), including two ∼12,000- to 10,000-year-old individuals representing the oldest humans sequenced from southern China. We discovered a deeply diverged East Asian ancestry in the Guangxi region that persisted until at least 6,000 years ago. We found that ∼9,000- to 6,000-year-old Guangxi populations were a mixture of local ancestry, southern ancestry previously sampled in Fujian, and deep Asian ancestry related to Southeast Asian Hòabìnhian hunter-gatherers, showing broad admixture in the region predating the appearance of farming. Historical Guangxi populations dating to ∼1,500 to 500 years ago are closely related to Tai-Kadai and Hmong-Mien speakers. Our results show heavy interactions among three distinct ancestries at the crossroads of East and Southeast Asia.}
}
@article{SUN20211865,
title = {In vivo structural characterization of the SARS-CoV-2 RNA genome identifies host proteins vulnerable to repurposed drugs},
journal = {Cell},
volume = {184},
number = {7},
pages = {1865-1883.e20},
year = {2021},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2021.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0092867421001586},
author = {Lei Sun and Pan Li and Xiaohui Ju and Jian Rao and Wenze Huang and Lili Ren and Shaojun Zhang and Tuanlin Xiong and Kui Xu and Xiaolin Zhou and Mingli Gong and Eric Miska and Qiang Ding and Jianwei Wang and Qiangfeng Cliff Zhang},
keywords = {SARS-CoV-2, RNA secondary structure, host factor, RBP binding prediction, drug reproposing},
abstract = {Summary
Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the cause of the ongoing coronavirus disease 2019 (COVID-19) pandemic. Understanding of the RNA virus and its interactions with host proteins could improve therapeutic interventions for COVID-19. By using icSHAPE, we determined the structural landscape of SARS-CoV-2 RNA in infected human cells and from refolded RNAs, as well as the regulatory untranslated regions of SARS-CoV-2 and six other coronaviruses. We validated several structural elements predicted in silico and discovered structural features that affect the translation and abundance of subgenomic viral RNAs in cells. The structural data informed a deep-learning tool to predict 42 host proteins that bind to SARS-CoV-2 RNA. Strikingly, antisense oligonucleotides targeting the structural elements and FDA-approved drugs inhibiting the SARS-CoV-2 RNA binding proteins dramatically reduced SARS-CoV-2 infection in cells derived from human liver and lung tumors. Our findings thus shed light on coronavirus and reveal multiple candidate therapeutics for COVID-19 treatment.}
}
@incollection{GRASSO2021301,
title = {11 - Process monitoring of laser powder bed fusion},
editor = {Igor Yadroitsev and Ina Yadroitsava and Anton {du Plessis} and Eric MacDonald},
booktitle = {Fundamentals of Laser Powder Bed Fusion of Metals},
publisher = {Elsevier},
pages = {301-326},
year = {2021},
series = {Additive Manufacturing Materials and Technologies},
isbn = {978-0-12-824090-8},
doi = {https://doi.org/10.1016/B978-0-12-824090-8.00012-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240908000123},
author = {Marco Grasso and Bianca Maria Colosimo and Kevin Slattery and Eric MacDonald},
keywords = {Anomaly detection, In-situ inspection, In-situ monitoring, Process control, Process signatures},
abstract = {The layerwise production paradigm entailed by the Laser Powder Bed Fusion process makes potentially available a large amount of information on a layer-by-layer basis, to determine the stability of the process and anticipate quality inspections of the part while it is being produced. Such information can be gathered through a variety of sensors used in-situ and in-line, ranging from pyrometers to high spatial and/or temporal resolution cameras or acoustic emission sensors. This chapter provides an overview of the quantities that can be acquired as signatures of the process and proxies of the final quality of the part, the methods suitable to make sense of acquired signals to detect anomalies, and process control solutions for defect mitigation, correction, or avoidance. The chapter also provides an up-to-date perspective on the current open issues for a wider industrial adoption of these techniques and most promising future research directions.}
}
@article{GAITERO2021110960,
title = {System quality and security certification in seven weeks: A multi-case study in Spanish SMEs},
journal = {Journal of Systems and Software},
volume = {178},
pages = {110960},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110960},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000571},
author = {Domingo Gaitero and Marcela Genero and Mario Piattini},
keywords = {SME, Quality management system, Security management system, ISO 9001, ISO/IEC 27001, Multi-case study},
abstract = {Every company wishes to improve its system quality and security, all the more so in these times of digital transformation, since having a good quality and security management system is essential to any company’s commercial survival. Such needs are even more pressing for small and medium-sized enterprises (SMEs), given their limited time and resources. To address these needs, a Spanish company, Proceso Social, has developed an innovative method called “SevenWeeks” to allow SMEs to create or improve their quality and security management systems in just seven weeks, with a view to obtaining one or both of the ISO 9001 and ISO/IEC 27001 certifications. We have evaluated the effectiveness and usefulness of SevenWeeks by carrying out a multi-case study of 26 Spanish companies, based on independent sources of evidence. This allowed us to corroborate that SevenWeeks was indeed effective for and perceived as useful by all the companies, as it enabled them to create their own quality and security management systems in only seven weeks and to obtain the necessary ISO certification. The interviewees found SevenWeeks to be an agile and intuitive method, easy to implement, which reduces costs and effort. We also include some recommendations to improve and further develop the method.}
}
@article{PICCIALLI2021111,
title = {A survey on deep learning in medicine: Why, how and when?},
journal = {Information Fusion},
volume = {66},
pages = {111-137},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1566253520303651},
author = {Francesco Piccialli and Vittorio Di Somma and Fabio Giampaolo and Salvatore Cuomo and Giancarlo Fortino},
keywords = {Deep learning, Medicine, Artificial intelligence, Data science, Neural networks},
abstract = {New technologies are transforming medicine, and this revolution starts with data. Health data, clinical images, genome sequences, data on prescribed therapies and results obtained, data that each of us has helped to create. Although the first uses of artificial intelligence (AI) in medicine date back to the 1980s, it is only with the beginning of the new millennium that there has been an explosion of interest in this sector worldwide. We are therefore witnessing the exponential growth of health-related information with the result that traditional analysis techniques are not suitable for satisfactorily management of this vast amount of data. AI applications (especially Deep Learning), on the other hand, are naturally predisposed to cope with this explosion of data, as they always work better as the amount of training data increases, a phase necessary to build the optimal neural network for a given clinical problem. This paper proposes a comprehensive and in-depth study of Deep Learning methodologies and applications in medicine. An in-depth analysis of the literature is presented; how, where and why Deep Learning models are applied in medicine are discussed and reviewed. Finally, current challenges and future research directions are outlined and analysed.}
}
@incollection{2021311,
title = {Index},
editor = {Siddhartha Bhattacharyya and Naba Kumar Mondal and Jan Platos and Václav Snášel and Pavel Krömer},
booktitle = {Intelligent Environmental Data Monitoring for Pollution Management},
publisher = {Academic Press},
pages = {311-323},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-819671-7},
doi = {https://doi.org/10.1016/B978-0-12-819671-7.09991-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196717099917}
}
@article{JENKINS2021111239,
title = {Changing the approach to energy compliance in residential buildings – re-imagining EPCs},
journal = {Energy and Buildings},
volume = {249},
pages = {111239},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111239},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821005235},
author = {D.P. Jenkins and  S.Semple and S. Patidar and P. McCallum},
keywords = {EPC, Dynamic simulation, Urban energy modelling},
abstract = {As our need for energy information of buildings evolves, and the tools and methods at our disposal increase in scale and complexity, it is perhaps reasonable to expect a similar level of change in the way energy in buildings is assessed within national energy compliance frameworks. By comparing the available opportunities for building energy modelling with the current methodologies underlying Energy Performance Certificates, this study proposes future directions for standardised energy assessment of residential buildings and the impact this could have on different facets of energy policy. In carrying out this exercise, a number of criteria are proposed that could be used to appraise methodologies that align with future requirements of energy assessment, with two potential candidates for future energy assessment considered as part of this appraisal. An argument is thus proposed for better aligning future forms of standardised energy assessment with directions and requirements of future low-carbon energy policy.}
}
@article{NGUYEN2021102121,
title = {Research manuscript: The Bullwhip Effect in rule-based supply chain planning systems–A case-based simulation at a hard goods retailer},
journal = {Omega},
volume = {98},
pages = {102121},
year = {2021},
issn = {0305-0483},
doi = {https://doi.org/10.1016/j.omega.2019.102121},
url = {https://www.sciencedirect.com/science/article/pii/S0305048318308855},
author = {Duy Tan Nguyen and Yossiri Adulyasak and Sylvain Landry},
keywords = {Flowcasting, Distribution, Logistics, Bullwhip Effect, Simulation, Factor analysis, Regression analysis},
abstract = {The vision of a well-integrated supply chain (SC) was developed as early as 1958 by Forrester, who addressed what would eventually be called the Bullwhip Effect (BWE). The Flowcasting concept, originally called Retail Resource Planning, was proposed to connect all SC upper tiers to the storefront through fulfillment logic based on the Distribution Resource Planning (DRP) system. This method can therefore be understood as fully or SC-wide integrated DRP with a focus on the role of retailers instead of that of vendors or distributors. We studied a Canadian retailer that implemented Flowcasting in order to gain insight into the benefits and operational logic of this system. Based on the data obtained from the company, we simulate Flowcasting operations across a 3-tier SC compared to the Reorder Point (ROP) system, which was previously used at the firm, as well as a combination of ROP and DRP (ROP/DRP or partially integrated DRP), which are some of the most common implementations in use. The simulation is configured based on the company's settings, including historical average demand, demand estimates, lead time, etc. Then, multivariate regression is deployed to statistically compare the efficacy of these methods in SC management using various assessment criteria, including BWE measures. The results show that the requirement calculation logic used in an SC-wide integrated DRP system (Flowcasting) generally outperforms the other two approaches, and its benefits in curtailing the BWE become more noticeable in the upper tiers of the SC. This paper indicates the enormous potential of SC-wide integrated DRP logic in rule-based replenishment planning systems.}
}
@article{ZHANG2023118921,
title = {Static and incremental robust kernel factorization embedding graph regularization supporting ill-conditioned industrial data recovery},
journal = {Expert Systems with Applications},
volume = {213},
pages = {118921},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118921},
url = {https://www.sciencedirect.com/science/article/pii/S095741742201939X},
author = {Ming-Qing Zhang and Xiong-Lin Luo},
keywords = {Ill-conditioned data recovery, Incremental robust kernel factorization, Graph regularization, Distributed adaptive proximal Newton gradient descent},
abstract = {Low-rank approximation algorithms aim to utilize convex nuclear norm constraint of linear matrices to recover ill-conditioned entries caused by multi-sampling rates, sensor drop-out. However, these existing algorithms are often limited in solving high-dimensionality and rank minimization relaxation. In this paper, a robust kernel factorization embedding graph regularization method is developed to statically impute missing measurements. Specifically, the implicit high-dimensional feature space of ill-conditioned data is factorized by kernel sparse dictionary. Then, a robust sparse-norm and graph regularization constraints are performed in the objective function to ensure the consistency of the spatial information. For the optimization of the parameters involved in the model, a distributed adaptive proximal Newton gradient descent learning strategy is proposed to accelerate the convergence. Furthermore, considering the dynamic time-series and potentially non-stationary structure of industrial data, we propose extended incremental versions to alleviate the complexity of the overall model computation. Extensive data recovery experiments are conducted on two real industrial processes to evaluate the proposed method in comparison with existing state-of-the-art restorers. The results show that the proposed methods can impute better with different missing rates and have strong competitiveness in practical application.}
}
@article{LIANG2021107548,
title = {Industrial time series determinative anomaly detection based on constraint hypergraph},
journal = {Knowledge-Based Systems},
volume = {233},
pages = {107548},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107548},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121008108},
author = {Zheng Liang and Hongzhi Wang and Xiaoou Ding and Tianyu Mu},
keywords = {Industrial time series, Anomaly detection, Constraint hypergraph},
abstract = {The explosive growth of time series captured by sensors in industrial pipelines gives rise to the flourish of intelligent industry. Exploiting the value of these time series is conductive to workload balancing and production optimization. Unfortunately, knowledge obtained from the mining process turns out to be insufficient for use due to widespread anomalies, indicating machine breakdown, sensor failure or working status shifts. To tackle this problem, we propose a constraint hypergraph-based method, combining multiple constraints for anomaly detection. We develop strategies for adaptive determinative anomaly detection and anomaly pattern mining. We also investigate the problem of Anomaly Pattern Matching, prove its NP-completeness, and propose algorithms to obtain its global and local optimum. Finally, we demonstrate our approach with three real world datasets from a real powerplant, a chemical production pipeline and a hydraulic system. The experimental results show that our approach can effectively and efficiently work under different circumstances.}
}
@article{PHAOSATHIANPHAN2021100882,
title = {An intelligent travel technology assessment model for destination impacts of tourist adoption},
journal = {Tourism Management Perspectives},
volume = {40},
pages = {100882},
year = {2021},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2021.100882},
url = {https://www.sciencedirect.com/science/article/pii/S2211973621000957},
author = {Noppadol Phaosathianphan and Adisorn Leelasantitham},
keywords = {Technology Acceptance Model, IS Success, IS Continuance, Intelligent travel technology assessment model, Destination Impacts, Human-Computer Interaction, Socio-Technical System},
abstract = {As intelligent travel technology creates a business trend to support the travel and tourism industry, it is necessary to create a complete model to assess the users in terms of continuous user acceptance and destination impacts which are Competitiveness, Loyalty, and Sustainability with entire stakeholders (travellers, service providers, and destinations). The proposed conceptual model is formulated based on four studies: A Plenary Free Individual Traveler Life Cycle, IS Success, IS Continuance, and Destination Impacts of Travel and Tourism. The sample and data collections were done through the online questionnaire and survey via social media platforms such as Facebook and Line with individuals who have used intelligent personal assistants such as Google Assistant, Apple Siri, Microsoft Cortana, Amazon Alexa, and Samsung Bixby for travel and tourism. 400 respondents were analyzed with descriptive statistics and inferential statistics using the measurement model and the structural model by PASW Statistics and SmartPLS.}
}
@article{RALLO2021106645,
title = {Updated single and dual crop coefficients for tree and vine fruit crops},
journal = {Agricultural Water Management},
volume = {250},
pages = {106645},
year = {2021},
issn = {0378-3774},
doi = {https://doi.org/10.1016/j.agwat.2020.106645},
url = {https://www.sciencedirect.com/science/article/pii/S0378377420321892},
author = {G. Rallo and T.A. Paço and P. Paredes and À. Puig-Sirera and R. Massai and G. Provenzano and L.S. Pereira},
keywords = {K and K values, Vineyards, Evergreen fruit trees, Deciduous fruit trees, Nut trees, Tropical fruit crops, Berries and hop},
abstract = {The present study reviews the research on the FAO56 crop coefficients of fruit trees and vines performed over the past twenty years. The main objective was to update information and extend tabulated single (Kc) and basal (Kcb) standard crop coefficients. The selection and analysis of the literature for this review have been done to consider only studies that adhere to FAO56 method, computing the reference ET with the FAO Penman–Monteith​ ETo equation and field measuring crop ET with proved accuracy. The crops considered refer to vine fruit crops, berries and hops, temperate climate evergreen fruit trees, temperate climate deciduous fruit trees and, tropical and subtropical fruit crops. Papers satisfying the conditions expressed above, and that studied the crops under pristine or appropriate eustress conditions, were selected to provide for standard Kc and Kcb data. Preference was given to studies reporting on the fraction of ground cover (fc), crop height (h), planting density, crop age and adopted training systems. The Kc and Kcb values obtained from the selected literature generally show coherence relative to the crop biophysical characteristics and reflect those characteristics, mainly fc, h and training systems. The ranges of reported Kc and Kcb values were grouped according to crop density, particularly fc and h, and were compared with FAO56 (Allen et al., 1998) previously tabulated Kc and Kcb values, as well as by Allen and Pereira (2009) and Jensen and Allen (2016), which lead to define update indicative standard Kc and Kcb values. These values are aimed for use in crop water requirement computations and modeling for irrigation planning and scheduling, thus also aimed at supporting improved water use and saving in orchards and vines.}
}
@article{HALAWA2021115696,
title = {Integrated framework of process mining and simulation–optimization for pod structured clinical layout design},
journal = {Expert Systems with Applications},
volume = {185},
pages = {115696},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115696},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421010800},
author = {Farouq Halawa and Sreenath {Chalil Madathil} and Mohammad T. Khasawneh},
keywords = {Process mining, Particle swarm optimization, Simulation-optimization, Facility layout, Healthcare},
abstract = {This paper proposes a three-phase framework to leverage hospital tracking data of patient visits while designing healthcare layouts with pod structures. The first phase proposes a process mining algorithm that modifies the Probabilistic Determining Finite Automata (PDFA) with Particle Swarm Optimization (PDFA-PSO) algorithm to predict the significant patient workflows from hospital historical data. The second phase employs simulation modeling to solve a right-sizing problem to determine the optimal size of the layout pods and the frequency of flows between the different clinical locations. The final phase uses an Unequal Area Facility Layout Problem (UAFLP) to determine the layout typology. The proposed process mining and simulation model are vital steps to measure the frequency between spaces and pod areas, which are needed to solve the UAFLP for outpatient settings. The proposed framework is validated using a case study for a renovation project of a large heart and vascular clinic in the US. The research shows that process mining is an efficient tool to extract a subset of significant patient pathways among 90 pathway variants and build a more realistic simulation that reflects behavioral and operational aspects. The research shows that the PSO algorithm is efficient in estimating the PDFA parameters and improving the prediction accuracy of the extracted patient pathways. In addition, the research shows that Genetic Algorithm with Placement Staretegy is an efficient algorithm for layout automation.}
}
@article{PAN2021103564,
title = {A BIM-data mining integrated digital twin framework for advanced project management},
journal = {Automation in Construction},
volume = {124},
pages = {103564},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103564},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521000157},
author = {Yue Pan and Limao Zhang},
keywords = {Digital twin, Building information modeling (BIM), Process mining, Time series analysis},
abstract = {With the focus of smart construction project management, this paper presents a closed-loop digital twin framework under the integration of Building Information Modeling (BIM), Internet of Things (IoT), and data mining (DM) techniques. To be specific, IoT connects the physical and cyber world to capture real-time data for modeling and analyzing, and data mining methods incorporated in the virtual model aim to discover hidden knowledge in collected data. The proposed digital twin has been verified in a practical BIM-based project. Based on large inspection data from IoT devices, the 4D visualization and task-centered or worker-centered process model are built as the virtual model to simulate both the task execution and worker cooperation. Then, the high-fidelity virtual model is investigated by process mining and time series analysis. Results show that possible bottlenecks in the current process can be foreseen using the fuzzy miner, while the number of finished tasks in the next phase can be predicted by the multivariate autoregressive integrated moving average (ARIMAX) model. Consequently, tactic decision-making can realize to not only prevent possible failure in advance, but also arrange work and staffing reasonably to make the process adapt to changeable conditions. In short, the significance of this paper is to build a data-driven digital twin framework integrating with BIM, IoT, and data mining for advanced project management, which can facilitate data communication and exploration to better understand, predict, and optimize the physical construction operations. In future works, more complex cases with multiple data streams will be used to test the developed framework, and more detailed interpretations with the actual observations of construction activities will be given.}
}
@article{ABOELMAGED2021102247,
title = {Predicting subjective well-being among mHealth users: a readiness – value model},
journal = {International Journal of Information Management},
volume = {56},
pages = {102247},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102247},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220314468},
author = {Mohamed Aboelmaged and Gharib Hashem and Samar Mouakket},
keywords = {mHealth, Subjective well-being, Utilitarian value, Hedonic value, Technology readiness, Post-adoption},
abstract = {mHealth applications (MHA) have recently attracted great attention from various stakeholders as they are indeed important means to enhance users’ subjective well-being. While prior research has mainly focused on intention or adoption phase, little work has empirically examined the post-adoption effects of MHA with scarce attention given to the well-being outcome. Actual users are likely to conceive the values of MHA based mainly on their direct experience with it. In this paper, the dimensions of users’ technology readiness are regarded as major impetuses for perceived utilitarian and hedonic values, which in turn influence subjective well-being among MHA users. The proposed readiness-value model is analyzed using survey data collected from 731 users of MHA. The findings show that the model significantly predicts users’ subjective well-being considering that utilitarian value is more important for male users, whereas hedonic value has a more salient effect for female users. It also reveals that enablers of technology readiness (i.e., innovativeness and optimism) exert a stronger influence than that of inhibitors (i.e., discomfort and insecurity) on the perceived values of MHA. These results have essential implications for theory and practice.}
}
@article{YAN2021102489,
title = {Data analytics for fuel consumption management in maritime transportation: Status and perspectives},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {155},
pages = {102489},
year = {2021},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2021.102489},
url = {https://www.sciencedirect.com/science/article/pii/S1366554521002519},
author = {Ran Yan and Shuaian Wang and Harilaos N. Psaraftis},
keywords = {Maritime transportation, Ship fuel consumption prediction, Ship performance prediction, Ship energy efficiency optimization, Ship performance optimization},
abstract = {The shipping industry is associated with approximately three quarters of all world trade. In recent years, the sustainability of shipping has become a public concern, and various emissions control regulations to reduce pollutants and greenhouse gas (GHG) emissions from ships have been proposed and implemented globally. These regulations aim to drive the shipping industry in a low-carbon and low-pollutant direction by motivating it to switch to more efficient fuel types and reduce energy consumption. At the same time, the cyclical downturn of the world economy and high bunker prices make it necessary and urgent for the shipping industry to operate in a more cost-effective way while still satisfying global trade demand. As bunker fuel bunker (e.g., heavy fuel oil [HFO], liquified natural gas [LNG]) consumption is the main source of emissions and bunker fuel costs account for a large proportion of operating costs, shipping companies are making unprecedented efforts to optimize ship energy efficiency. It is widely accepted that the key to improving the energy efficiency of ships is the development of accurate models to predict ship fuel consumption rates under different scenarios. In this study, ship fuel consumption prediction models presented in the literature (including the academic literature and technical reports as a typical type of “grey literature”) are reviewed and compared, and models that optimize ship operations based on fuel consumption prediction results are also presented and discussed. Current research challenges and promising research questions on ship performance monitoring and operational optimization are identified.}
}
@article{MEHMOOD2021106845,
title = {Customizing SVM as a base learner with AdaBoost ensemble to learn from multi-class problems: A hybrid approach AdaBoost-MSVM},
journal = {Knowledge-Based Systems},
volume = {217},
pages = {106845},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106845},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001088},
author = {Zafar Mehmood and Sohail Asghar},
keywords = {Machine learning classifiers, Class overlapping, Imbalanced distribution of data, Imbalanced problem, Decomposition techniques},
abstract = {Learning from a multi-class problem has not been an easy task for most of the classifiers, because of multiple issues. In the complex multi-class scenarios, samples of different classes overlap with each other by sharing attribute, and hence the visibility of least represented samples decrease even more. Learning from imbalanced data studied extensively in the research community, however, the overlapping issues and the co-occurrence impact of overlapping with data imbalance have received comparatively less attention, even though their joint impact is more thoughtful on classifiers’ performance. In this paper, we introduce a modified SVM, MSVM to use as a base classifier with the AdaBoost ensemble classifier (MSVM-AdB) to enhance the learning capability of the ensemble classifier. To implement the proposed technique, we divide the multi-class dataset into overlapping and non-overlapping region. The overlapping region is further filter into the Critical and less Critical region depending upon their sample contribution in the overlapped region. The MSVM is designed to map the overlapped samples in a higher dimension by modifying the kernel mapping function of the standard SVM by using the mean distance of the Critical region samples. To highlight the learning enhancement of the MSVM-AdB, we use 20 real datasets with varying imbalance ratio and the overlapping degree to compare the significance of the AdaBoost-MSVM with the standard SVM, and AdaBoost with standard base classifiers. Experimental results show the superiority of the MSVM-AdB on a collection of benchmark datasets to its standard counterpart classifiers.}
}
@article{THOMPSON2021101628,
title = {Platform, or technology project? A spectrum of six strategic ‘plays’ from UK government IT initiatives and their implications for policy},
journal = {Government Information Quarterly},
volume = {38},
number = {4},
pages = {101628},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101628},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000642},
author = {Mark Thompson and Will Venters},
keywords = {Platform innovation, Platform strategy, Government policy, Digital innovation, UK},
abstract = {There is a markedly broad range of definitions and illustrative examples of the role played by governments themselves within the literature on government platforms. In response we conduct an inductive and deductive qualitative review of the literature to clarify this landscape and so to develop a typology of six definitions of government platforms, organised within three genres along a spectrum from fully centralised, through to fully decentralised. For each platform definition we offer illustrative ‘mini-cases’ drawn from the UK government experience as well as further insights and implications for each genre, drawn from the broader information systems literature on platforms. A range of benefits, risks, governance challenges, policy recommendations, and suggestions for further research are then identified and discussed.}
}
@article{YU2021103754,
title = {Identification of pediatric respiratory diseases using a fine-grained diagnosis system},
journal = {Journal of Biomedical Informatics},
volume = {117},
pages = {103754},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103754},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421000836},
author = {Gang Yu and Zhongzhi Yu and Yemin Shi and Yingshuo Wang and Xiaoqing Liu and Zheming Li and Yonggen Zhao and Fenglei Sun and Yizhou Yu and Qiang Shu},
keywords = {Respiratory diseases, Fine-grained diagnosis, Pediatric diagnosis, Clinical notes, Multi-modal},
abstract = {Respiratory diseases, including asthma, bronchitis, pneumonia, and upper respiratory tract infection (RTI), are among the most common diseases in clinics. The similarities among the symptoms of these diseases precludes prompt diagnosis upon the patients’ arrival. In pediatrics, the patients’ limited ability in expressing their situation makes precise diagnosis even harder. This becomes worse in primary hospitals, where the lack of medical imaging devices and the doctors’ limited experience further increase the difficulty of distinguishing among similar diseases. In this paper, a pediatric fine-grained diagnosis-assistant system is proposed to provide prompt and precise diagnosis using solely clinical notes upon admission, which would assist clinicians without changing the diagnostic process. The proposed system consists of two stages: a test result structuralization stage and a disease identification stage. The first stage structuralizes test results by extracting relevant numerical values from clinical notes, and the disease identification stage provides a diagnosis based on text-form clinical notes and the structured data obtained from the first stage. A novel deep learning algorithm was developed for the disease identification stage, where techniques including adaptive feature infusion and multi-modal attentive fusion were introduced to fuse structured and text data together. Clinical notes from over 12000 patients with respiratory diseases were used to train a deep learning model, and clinical notes from a non-overlapping set of about 1800 patients were used to evaluate the performance of the trained model. The average precisions (AP) for pneumonia, RTI, bronchitis and asthma are 0.878, 0.857, 0.714, and 0.825, respectively, achieving a mean AP (mAP) of 0.819. These results demonstrate that our proposed fine-grained diagnosis-assistant system provides precise identification of the diseases.}
}
@incollection{2021697,
title = {Index},
editor = {Anthony J. Martyr and David R. Rogers},
booktitle = {Engine Testing (Fifth Edition)},
publisher = {Butterworth-Heinemann},
edition = {Fifth Edition},
address = {Oxford},
pages = {697-722},
year = {2021},
isbn = {978-0-12-821226-4},
doi = {https://doi.org/10.1016/B978-0-12-821226-4.00026-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212264000267}
}
@article{ACUTO2021105295,
title = {Mobilising urban knowledge in an infodemic: Urban observatories, sustainable development and the COVID-19 crisis},
journal = {World Development},
volume = {140},
pages = {105295},
year = {2021},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2020.105295},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X20304228},
author = {Michele Acuto and Ariana Dickey and Stephanie Butcher and Carla-Leanne Washbourne},
keywords = {Urban observatories, Infodemic, Boundary-spanning, Knowledge translation, Science-policy interface},
abstract = {Along with disastrous health and economic implications, COVID-19 has also been an epidemic of misinformation and rumours - an ‘infodemic’. The desire for robust, evidence-based policymaking in this time of disruption has been at the heart of the multilateral response to the crisis, not least in terms of supporting a continuing agenda for global sustainable development. The role of boundary-spanning knowledge institutions in this context could be pivotal, not least in cities, where much of the pandemic has struck. ‘Urban observatories’ have emerged as an example of such institutions; harbouring great potential to produce and share knowledge supporting sustainable and equitable processes of recovery. Building on four ‘live’ case studies during the crisis of institutions based in Johannesburg, Karachi, Freetown and Bangalore, our research note aims to capture the role of these institutions, and what it means to span knowledge boundaries in the current crisis. We do so with an eye towards a better understanding of their knowledge mobilisation practices in contributing towards sustainable urban development. We highlight that the crisis offers a key window for urban observatories to play a progressive and effective role for sustainable and inclusive development. However, we also underline continuing challenges in these boundary knowledge dynamics: including issues of institutional trust, inequality of voices, collective memory, and the balance between normative and advisory roles for observatories.}
}
@article{OMETOV2021108074,
title = {A Survey on Wearable Technology: History, State-of-the-Art and Current Challenges},
journal = {Computer Networks},
volume = {193},
pages = {108074},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108074},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001651},
author = {Aleksandr Ometov and Viktoriia Shubina and Lucie Klus and Justyna Skibińska and Salwa Saafi and Pavel Pascacio and Laura Flueratoru and Darwin Quezada Gaibor and Nadezhda Chukhno and Olga Chukhno and Asad Ali and Asma Channa and Ekaterina Svertoka and Waleed Bin Qaim and Raúl Casanova-Marqués and Sylvia Holcer and Joaquín Torres-Sospedra and Sven Casteleyn and Giuseppe Ruggeri and Giuseppe Araniti and Radim Burget and Jiri Hosek and Elena Simona Lohan},
keywords = {Wearables, Communications, Standardization, Privacy, Security, Data processing, Interoperability, User adoption, Localization, Classification, Future perspective},
abstract = {Technology is continually undergoing a constituent development caused by the appearance of billions new interconnected “things” and their entrenchment in our daily lives. One of the underlying versatile technologies, namely wearables, is able to capture rich contextual information produced by such devices and use it to deliver a legitimately personalized experience. The main aim of this paper is to shed light on the history of wearable devices and provide a state-of-the-art review on the wearable market. Moreover, the paper provides an extensive and diverse classification of wearables, based on various factors, a discussion on wireless communication technologies, architectures, data processing aspects, and market status, as well as a variety of other actual information on wearable technology. Finally, the survey highlights the critical challenges and existing/future solutions.}
}
@article{STOYKOVA2021105575,
title = {Digital evidence: Unaddressed threats to fairness and the presumption of innocence},
journal = {Computer Law & Security Review},
volume = {42},
pages = {105575},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105575},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000480},
author = {Radina Stoykova},
keywords = {Presumption of innocence, Digital evidence, Reliability, Digital forensics, Fair trial},
abstract = {Contemporary criminal investigation assisted by computing technology imposes challenges to the right to a fair trial and the scientific validity of digital evidence. This paper identifies three categories of unaddressed threats to fairness and the presumption of innocence during investigations – (i) the inappropriate and inconsistent use of technology; (ii) old procedural guarantees, which are not adapted to contemporary digital evidence processes and services; (iii) and the lack of reliability testing in digital forensics practice. Further, the solutions that have been suggested to overcome these issues are critically reviewed to identify their shortcomings. Ultimately, the paper argues for the need of legislative intervention and enforcement of standards and validation procedures for digital evidence in order to protect innocent suspects and all parties in the criminal proceedings from the negative consequences of technology-assisted investigations.}
}
@incollection{AWOTUNDE2021235,
title = {Chapter Nine - Prediction and classification of diabetes mellitus using genomic data},
editor = {Arun Kumar Sangaiah and Subhas Mukhopadhyay},
booktitle = {Intelligent IoT Systems in Personalized Health Care},
publisher = {Academic Press},
pages = {235-292},
year = {2021},
series = {Cognitive Data Science in Sustainable Computing},
isbn = {978-0-12-821187-8},
doi = {https://doi.org/10.1016/B978-0-12-821187-8.00009-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211878000095},
author = {Joseph Bamidele Awotunde and Femi Emmanuel Ayo and Rasheed Gbenga Jimoh and Roseline Oluwaseun Ogundokun and Opeyemi Emmanuel Matiluko and Idowu Dauda Oladipo and Muyideen Abdulraheem},
keywords = {Genomic data, Diabetes mellitus, Classification, Genetic algorithm, Deep neural networks},
abstract = {Diabetes mellitus (DM) is one of the chronic and debilitating diseases in modern society, hence the urgent need to prevent epidemic growth in society. This chapter is motivated by the studies of several scholars in the field of microarrays datasets for gene expression. Nonetheless, there are very few available gene signatures across datasets, thereby generating sample selection bias and over selection sets matching. Subjective selection of this gene and sample pairings could be addressed through large average submatrices and a unique method of biclustering using objective statistical assumptions to reconstruct robust signatures of expression. Hence, SWITCH (SupWald Identification of CHanges DNA copy) was created to label CNAs in platforms of aCGH and to connect them with subtypes. Therefore, the process of selecting the most informative gene biomarker was done using the genetic algorithm (GA) and deep neural networks (DNN) for biological sample classification. The simulated genomics datasets were divided into 95% training and 5% test samples and the DNN classifier is modified using these sets of SNPs and fine-tuned to classify type II DM analyses. The datasets are cleaned into four single-nucleotide polymorphism (SNP) function sets: 96 (P-value: 1×10−5), 214 (P-value: 1×10−4), 399 (P-value: 1×10−3), and 678 (P-value: 1×10−2) using P-value thresholds. The classifier was built using the training data while testing its efficiency on the test sample. MATLAB was used to implement the GA and DNN. The DNN model showed a significant predictive output with type II DM having AUC=0.9537 in male and AUC=0.9349 in female. An experimental test was carried out to determine the associations of all SNP datasets with the type II diabetes phenotype. The results from the test showed an enhanced model performance with 399 and 678 SNPs, respectively. The test result also showed that the higher the number of SNPs the better the predictive performance of the designed model. Finally, the result showed that DNNs can be used to predict type II diabetes using genomic-based data.}
}
@article{JIANG2021103210,
title = {A selective ensemble model for cognitive cybersecurity analysis},
journal = {Journal of Network and Computer Applications},
volume = {193},
pages = {103210},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103210},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002125},
author = {Yuning Jiang and Yacine Atif},
keywords = {Information security, Vulnerability analysis, Data correlation, Machine learning, Ensemble, Data mining, Database management},
abstract = {Dynamic data-driven vulnerability assessments face massive heterogeneous data contained in, and produced by SOCs (Security Operations Centres). Manual vulnerability assessment practices result in inaccurate data and induce complex analytical reasoning. Contemporary security repositories’ diversity, incompleteness and redundancy contribute to such security concerns. These issues are typical characteristics of public and manufacturer vulnerability reports, which exacerbate direct analysis to root out security deficiencies. Recent advances in machine learning techniques promise novel approaches to overcome these notorious diversity and incompleteness issues across massively increasing vulnerability reports corpora. Yet, these techniques themselves exhibit varying degrees of performance as a result of their diverse methods. We propose a cognitive cybersecurity approach that empowers human cognitive capital along two dimensions. We first resolve conflicting vulnerability reports and preprocess embedded security indicators into reliable data sets. Then, we use these data sets as a base for our proposed ensemble meta-classifier methods that fuse machine learning techniques to improve the predictive accuracy over individual machine learning algorithms. The application and implication of this methodology in the context of vulnerability analysis of computer systems are yet to unfold the full extent of its potential. The proposed cognitive security methodology in this paper is shown to improve performances when addressing the above-mentioned incompleteness and diversity issues across cybersecurity alert repositories. The experimental analysis conducted on actual cybersecurity data sources reveals interesting tradeoffs of our proposed selective ensemble methodology, to infer patterns of computer system vulnerabilities.}
}
@article{BASTIANELLI2021109239,
title = {Survival and cause-specific mortality of European wildcat (Felis silvestris) across Europe},
journal = {Biological Conservation},
volume = {261},
pages = {109239},
year = {2021},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2021.109239},
url = {https://www.sciencedirect.com/science/article/pii/S0006320721002913},
author = {Matteo Luca Bastianelli and Joseph Premier and Mathias Herrmann and Stefano Anile and Pedro Monterroso and Tobias Kuemmerle and Carsten F. Dormann and Sabrina Streif and Saskia Jerosch and Malte Götz and Olaf Simon and Marcos Moleón and José María Gil-Sánchez and Zsolt Biró and Jasja Dekker and Analena Severon and Axel Krannich and Karsten Hupe and Estelle Germain and Dominique Pontier and René Janssen and Pablo Ferreras and Francisco Díaz-Ruiz and José María López-Martín and Fermín Urra and Lolita Bizzarri and Elena Bertos-Martín and Markus Dietz and Manfred Trinzen and Elena Ballesteros-Duperón and José Miguel Barea-Azcón and Andrea Sforzi and Marie-Lazarine Poulle and Marco Heurich},
keywords = {Anthropogenic landscapes, European wildcat, Survival, Human-caused mortality, Roadkill, Road density},
abstract = {Humans have transformed most landscapes across the globe, forcing other species to adapt in order to persist in increasingly anthropogenic landscapes. Wide-ranging solitary species, such as wild felids, struggle particularly in such landscapes. Conservation planning and management for their long-term persistence critically depends on understanding what determine survival and what are the main mortality risks. We carried out the first study on annual survival and cause-specific mortality of the European wildcat with a large and unique dataset of 211 tracked individuals from 22 study areas across Europe. Furthermore, we tested the effect of environmental and human disturbance variables on the survival probability. Our results show that mortalities were mainly human-caused, with roadkill and poaching representing 57% and 22% of the total annual mortality, respectively. The annual survival probability of wildcat was 0.92 (95% CI = 0.87–0.98) for females and 0.84 (95% CI = 0.75–0.94) for males. Road density strongly impacted wildcat annual survival, whereby an increase in the road density of motorways and primary roads by 1 km/km2 in wildcat home-ranges increased mortality risk ninefold. Low-traffic roads, such as secondary and tertiary roads, did not significantly affect wildcat's annual survival. Our results deliver key input parameters for population viability analyses, provide planning-relevant information to maintain subcritical road densities in key wildcat habitats, and identify conditions under which wildcat-proof fences and wildlife crossing structures should be installed to decrease wildcat mortality.}
}
@article{DEBAUCHE2021100378,
title = {Data management and internet of things : A methodological review in smart farming},
journal = {Internet of Things},
volume = {14},
pages = {100378},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100378},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000226},
author = {Olivier Debauche and Jean-Philippe Trani and Saïd Mahmoudi and Pierre Manneback and Jérôme Bindelle and Sidi Ahmed Mahmoudi and Adriano Guttadauria and Frédéric Lebeau},
keywords = {Internet of things, Network protocols, Technological selection methodology, Cloud architecture, Security, Smart farming},
abstract = {Introduction. In the field of research, we are familiar to employ ready-to-use commercial solutions. This bibliographic review highlights the various technological paths that can be used in the context of agriculture digitalization and illuminates the reader on the capacities and limits of each one. Literature. Based on a literature review that we conducted, we describe the main components of the Internet of Things. Also, we analyzed the different technological pathways used by researchers to develop their projects. Finally, these versatile approaches are summarized in the form of tables and a methodological flowchart of communication protocols choices. Conclusions. In this article, we propose a methodology and a reflection on the technological choices and their implication on the durability and valorization of research projects in the field of smart agriculture.}
}
@article{LIN2021100894,
title = {Prevalence and intervention of preoperative anemia in Chinese adults: A retrospective cross-sectional study based on national preoperative anemia database},
journal = {EClinicalMedicine},
volume = {36},
pages = {100894},
year = {2021},
issn = {2589-5370},
doi = {https://doi.org/10.1016/j.eclinm.2021.100894},
url = {https://www.sciencedirect.com/science/article/pii/S2589537021001747},
author = {Jie Lin and Chao Wang and Junting Liu and Yang Yu and Shufang Wang and Aiqing Wen and Jufeng Wu and Long Zhang and Futing Sun and Xiaojun Guo and Fenghua Liu and Hailan Li and Na Li and Haibao Wang and Yi Lv and Zhonghua Jia and Xiaoyan Li and Jun Zhang and Zunyan Li and Shanshan Liu and Shuhuai Zhong and Jun Yang and Shuxuan Ma and Lingling Zhou and Xiaozhen Guan and Chunya Ma and Shijun Cheng and Shengxiong Chen and Zhenhua Xu and Gang Li and Deqing Wang},
keywords = {Preoperative anemia, Transfusion, Iron, Erythropoietin},
abstract = {Background
Preoperative anemia is an important pillar of perioperative patient blood management. However, there was no literature comprehensively described the current situation of preoperative anemia in China.
Methods
We conducted a national retrospective cross-sectional study to assess the prevalence and intervention of preoperative anemia in Chinese adults. Data were from the National Preoperative Anemia Database based on hospital administration data from January 1, 2013 to December 31, 2018.
Findings
A total of 797,002 patients were included for analysis. Overall, 27.57% (95% CI 27.47–27.67) of patients had preoperative anemia, which varied by gender, age, regions, and type of operation. Patients who were female, age over 60 years old, from South China, from provinces with lower per capita GDP, underwent operations on the lymphatic and hematopoietic system, with laboratory abnormalities were more likely to have a high risk of preoperative anemia. Among patients with preoperative anemia, 5.16% (95% CI 5.07–5.26) received red blood cell transfusion, 7.79% (95% CI 7.67–7.91) received anemia-related medications such as iron, erythropoietin, folic acid or vitamin B12, and 12.25% (95% CI 12.10–12.40) received anemia-related therapy (red blood cell transfusion or anemia-related medications) before operation. The probability of preoperative RBC transfusion decreased by 54.92% (OR 0.46, 95% CI 0.46–0.47) as each 10-g/L increase in preoperative hemoglobin. Patients with preoperative hemoglobin less than 130 g/L was associated with longer hospital stay and more hospital costs. Patients with severe preoperative anemia given iron preoperatively had lower intra/post-operative RBC transfusion rate, shorter length of stay and less hospitalization costs, but no similar correlation was found in patients with mild and moderate preoperative anemia and patients given erythropoietin preoperatively.
Interpretation
Our present study shows that preoperative anemia is currently a relatively prevalent problem that has not been fully appreciated in China. More researches will be required to optimize the treatment of preoperative anemia.
Funding
National Natural Science Foundation of China and the Logistics Support Department of the Central Military Commission.}
}
@article{SKYDT2021108691,
title = {A probabilistic sequence classification approach for early fault prediction in distribution grids using long short-term memory neural networks},
journal = {Measurement},
volume = {170},
pages = {108691},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2020.108691},
url = {https://www.sciencedirect.com/science/article/pii/S0263224120311994},
author = {Mathis Riber Skydt and Mads Bang and Hamid Reza Shaker},
keywords = {Fault prediction, Predictive maintenance, Grid management, Risk assessment, Neural networks, LSTM},
abstract = {As the global power grid must undergo a profound transformation in the coming decades to ensure reliable and cost-effective operation in a system with large shares of intermittent renewable energy generation, a critical element will be to leverage advanced data-driven predictive tools to optimise grid management activities. As it is expected that existing grids will be operated more to their limits, it is important to obtain better operational insights and estimations of the time to equipment failure to provide useful operational guidance and maintenance prioritisation support for grid operators. In this regard, this paper proposes a novel and real-time applicable method for fault prediction in 10 kV underground oil-insulated power cables using low-resolution data from a real case study from a Danish distribution system operator. The developed method is based on a sequence classification approach using long short-term memory neural networks where three different operational states are defined (Normal, Early warning, and Critical warning) to allow for prediction flexibility and better indication of the presence of systemic faults. Moreover, to enhance the data foundation, this paper investigates a Virtual Sample Generation method based on an adaptive Gaussian distribution. The capability of the proposed method yields satisfying results with prediction accuracy on the test set reaching as high as ~90%, hence proving the usefulness of the proposed approach and paving the way for smarter maintenance protocols.}
}
@article{YIN2021312,
title = {Field data analysis and risk assessment of gas kick during industrial deepwater drilling process based on supervised learning algorithm},
journal = {Process Safety and Environmental Protection},
volume = {146},
pages = {312-328},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2020.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0957582020316815},
author = {Qishuai Yin and Jin Yang and Mayank Tyagi and Xu Zhou and Xinxin Hou and Bohan Cao},
keywords = {Industrial deep-water drilling, Gas kick, Field data analysis, Risk assessment, Early gas, Kick detection, Supervised learning},
abstract = {During industrial offshore deep-water drilling process, gas kick event occurs frequently due to extremely narrow Mud Weight (MW) window (minimum 0.01sg) and negligible safety margins for the well control purposes. Further, traditional gas kick detection methods in such environments have significant time-lag and can often lead to severe well control issues, and occasionally to well blowouts or borehole abandonment. In this study, firstly, the raw field data is processed through data collection, data cleaning, feature scaling, outlier detection, data labeling and dataset splitting. Additionally, a novel data labeling criterion for gas kick risks is proposed where five kick risks (Indicated by different colors in this study) are defined based on three key indicators: differential flow out (DFO), kick gain volume (Vol), and kick duration time (Time). Kick risk status represents one of the following cases: Case 0 - No indicators are activated (Green), Case 1 - Multi-drilling parameters deviation or DFO is activated (Orange), Case 2 - DFO and Vol are simultaneously activated (Light Red), Case 3 - DFO and Time are simultaneously activated (Light Red), Case 4 - DFO, Vol and Time alarms are simultaneously activated (Dark Red). Then, a novel data mining method using Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) is presented for early detection of gas kick events by analyzing time series data from field drilling process. The network parameters such as number of hidden layers and number of neurons are initialized to build the LSTM network. The learned LSTM model is evaluated using the testing set, and the best LSTM model (six (6)-layers eighty (80)-nodes (6 L*80 N)) is optimally selected and deployed. The accuracy of deployed LSTM model is 87 % in the testing dataset, which is reliable enough to identify the kick fault during the deep-water drilling field operation. Lastly, the LSTM model detected the gas kick events earlier than the “Tank Volume” detection method in several representative case studies to conclude that the application of LSTM model can potentially improve well control safety in the deep-water wells with narrow MW windows.}
}
@article{TSEKOS2021105180,
title = {Estimation of lignocellulosic biomass pyrolysis product yields using artificial neural networks},
journal = {Journal of Analytical and Applied Pyrolysis},
volume = {157},
pages = {105180},
year = {2021},
issn = {0165-2370},
doi = {https://doi.org/10.1016/j.jaap.2021.105180},
url = {https://www.sciencedirect.com/science/article/pii/S0165237021001662},
author = {C. Tsekos and S. Tandurella and W. {de Jong}},
keywords = {Pyrolysis, Artificial neural networks, Biomass modelling},
abstract = {As the push towards more sustainable ways to produce energy and chemicals intensifies, efforts are needed to refine and optimize the systems that can give an answer to these needs. In the present work, the use of neural networks as modelling tools for lignocellulosic biomass pyrolysis main products yields estimation was evaluated. In order to achieve this, the most relevant compositional and reaction parameters for lignocellulosic biomass pyrolysis were reviewed and their effect over the main products yields was assessed. Based on relevant literature data, a database was set up, containing parameters and experimental results from 32 published studies for a total of 482 samples, including both fast and slow pyrolysis experiments performed on a heterogeneous collection of lignocellulosic biomasses. The parameters that in the database configured as best predictors for the solid, liquid and gaseous products were determined through preliminary tests and were then used to build reduced models, one for each of the main products, which use five parameters instead of the full set for the estimation of yields. The procedures included hyperparameter optimizations steps. The performances of these reduced models were compared to those of the ones obtained using the full set of parameters as inputs by using the root mean squared error (RMSE) as metric. For both the char and gas products, the best results were consistently achieved by the reduced versions of the network (RMSE 5.1 wt% ar and 5.6 wt% ar respectively), while for the liquid product the best result was given by the full network (RMSE 6.9 wt% ar) indicating substantial value in proper selection of the input features. In general, the char models were the best performing ones. Additional models for the liquid and gas product featuring char as additional input to the system were also devised and obtained better performance (RMSE 5.5 wt% ar and 4.9 wt% ar respectively) compared to the original ones. Models based on single studies were also included in order to showcase both the capabilities of the tool and the challenges that arise when trying to build a generalizable model of this kind. Overall, artificial neural networks were shown to be an interesting tool for the construction of setup-unspecific biomass pyrolysis product yield models. The obstacles standing currently in the way of a more accurate modelling of the system were highlighted, along with certain literature discrepancies, which hinder reliable quantitative comparison of experimental conditions and results among separate studies.}
}
@incollection{CORRIGAN2021159,
title = {Chapter 9 - Image analysis in drug discovery},
editor = {Stephanie Kay Ashenden},
booktitle = {The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry},
publisher = {Academic Press},
pages = {159-189},
year = {2021},
isbn = {978-0-12-820045-2},
doi = {https://doi.org/10.1016/B978-0-12-820045-2.00010-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200452000106},
author = {Adam M. Corrigan and Daniel Sutton and Johannes Zimmermann and Laura A.L. Dillon and Kaustav Bera and Armin Meier and Fabiola Cecchi and Anant Madabhushi and Günter Schmidt and Jason Hipp},
keywords = {Artificial intelligence, Computational pathology, Radiomics, Deep learning},
abstract = {Across all stages of drug discovery and development, experimental assays are performed to understand the effect of a drug or drug candidate—at the molecular, cellular, organ, or organism level. Imaging is a key technology in this process, and an imaging assay consisting of sample preparation, image acquisition, and image analysis provides a quantitative readout of a system. Historically, chemical assays have been the workhorse of early discovery, screening millions of compounds for a simple endpoint, whereas imaging was primarily used in lower throughput mechanistic studies. However, with development of high-throughput high-content microscopy platforms, the throughput of imaging assays now rivals chemical screens. Similarly, innovations in image analysis mean that robust quantitative conclusions can be derived from complex and multimodal image data, driving informed decision making later in the drug development process. For these reasons, imaging is widely used throughout the pharmaceutical industry and throughout multiple stages of the drug discovery process.}
}
@article{JIANG2021105141,
title = {Utility of integrated IMERG precipitation and GLEAM potential evapotranspiration products for drought monitoring over mainland China},
journal = {Atmospheric Research},
volume = {247},
pages = {105141},
year = {2021},
issn = {0169-8095},
doi = {https://doi.org/10.1016/j.atmosres.2020.105141},
url = {https://www.sciencedirect.com/science/article/pii/S0169809520310772},
author = {Shanhu Jiang and Linyong Wei and Liliang Ren and Chong-Yu Xu and Feng Zhong and Menghao Wang and Linqi Zhang and Fei Yuan and Yi Liu},
keywords = {IMERG, GLEAM, Standardized Precipitation Evapotranspiration Index (SPEI), Drought monitoring, Mainland China},
abstract = {In this paper, we comprehensively evaluated the utility of integrated long-term satellite-based precipitation and evapotranspiration products for drought monitoring over mainland China. The latest Integrated Multi-satelliteE Retrievals for Global Precipitation Measurement V06 three Runs precipitation products, i.e., the near real-time Early Run (IMERG-E) and Late Run (IMERG-L) and the post-real time Final Run (IMERG-F), and the Global Land Evaporation Amsterdam Model V3.3a (GLEAM) potential evapotranspiration (PET) products from 2001 to 2017 were considered. The accuracy of IMERG precipitation and GLEAM PET products was first evaluated against observed precipitation and Penman-Monteith method estimated PET, respectively, based on dense meteorological station network. The Standard Precipitation Evapotranspiration Index (SPEI) calculated based on IMERG precipitation and GLEAM PET products (SPEIs, including SPEIE, SPEIL and SPEIF corresponding to IMERG-E, IMERG-L and IMERG-F, respectively) were then validated by using SPEI calculated based on meteorological data (SPEIm) at multiple temporal-spatial scales. Finally, four typical drought events were selected to analyse the ability of SPEIs to characterize the temporal-spatial evolution of drought situations. The results showed that the IMERG-F presents much better performance than IMERG-E and IMERG-L in terms of higher CC and smaller BIAS and RMSE values over mainland China. The GLEAM PET well simulated the change trend of reference PET, but generally underestimated reference PET in Northwest China (NW), Xinjiang (XJ) and Qinghai–Tibet plateau (TP). In general, the performances of SPEIs over eastern China and Southwest China (SW) were significantly superior to their performances in the NW, XJ, and TP regions. Even though the SPEIF performed the best, the SPEIE and SPEIL also performed reasonably well in some specific regions. SPEIs can well capture the temporal process and reasonably reflect the spatial characteristics for four typical drought events. It is thus highlighted that the latest IMERG precipitation (especially for IMERG-F) and GLEAM PET products could be used as alternative data sources for comprehensive drought monitoring, on account of the water balance principle over mainland China, particularly in eastern China and SW China. The outcomes of this study will provide valuable references for drought monitoring by integration of multi-source remote-sensing datasets in the GPM era.}
}
@article{TANG2021106866,
title = {The exposome in practice: an exploratory panel study of biomarkers of air pollutant exposure in Chinese people aged 60–69 years (China BAPE Study)},
journal = {Environment International},
volume = {157},
pages = {106866},
year = {2021},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2021.106866},
url = {https://www.sciencedirect.com/science/article/pii/S0160412021004918},
author = {Song Tang and Tiantian Li and Jianlong Fang and Renjie Chen and Yu'e Cha and Yanwen Wang and Mu Zhu and Yi Zhang and Yuanyuan Chen and Yanjun Du and Tianwei Yu and David C. Thompson and Krystal J. {Godri Pollitt} and Vasilis Vasiliou and John S. Ji and Haidong Kan and Junfeng Jim Zhang and Xiaoming Shi},
keywords = {PM, Exposomics, Panel Study, Personal Exposure Monitoring, Metabolomics, Exposome-Wide Association Study},
abstract = {The exposome overhauls conventional environmental health impact research paradigms and provides a novel methodological framework that comprehensively addresses the complex, highly dynamic interplays of exogenous exposures, endogenous exposures, and modifiable factors in humans. Holistic assessments of the adverse health effects and systematic elucidation of the mechanisms underlying environmental exposures are major scientific challenges with widespread societal implications. However, to date, few studies have comprehensively and simultaneously measured airborne pollutant exposures and explored the associated biomarkers in susceptible healthy elderly subjects, potentially resulting in the suboptimal assessment and management of health risks. To demonstrate the exposome paradigm, we describe the rationale and design of a comprehensive biomarker and biomonitoring panel study to systematically explore the association between individual airborne exposure and adverse health outcomes. We used a combination of personal monitoring for airborne pollutants, extensive human biomonitoring, advanced omics analysis, confounding information, and statistical methods. We established an exploratory panel study of Biomarkers of Air Pollutant Exposure in Chinese people aged 60–69 years (China BAPE), which included 76 healthy residents from a representative community in Jinan City, Shandong Province. During the period between September 2018 and January 2019, we conducted prospective longitudinal monitoring with a 3-day assessment every month. This project: (1) leveraged advanced tools for personal airborne exposure monitoring (external exposures); (2) comprehensively characterized biological samples for exogenous and endogenous compounds (e.g., targeted and untargeted monitoring) and multi-omics scale measurements to explore potential biomarkers and putative toxicity pathways; and (3) systematically evaluated the relationships between personal exposure to air pollutants, and novel biomarkers of exposures and effects using exposome-wide association study approaches. These findings will contribute to our understanding of the mechanisms underlying the adverse health impacts of air pollution exposures and identify potential adverse clinical outcomes that can facilitate the development of effective prevention and targeted intervention techniques.}
}
@article{KWOK2021100785,
title = {Trends, topics, and lessons learnt from real case studies using mesoscale atmospheric models for urban climate applications in 2000–2019},
journal = {Urban Climate},
volume = {36},
pages = {100785},
year = {2021},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2021.100785},
url = {https://www.sciencedirect.com/science/article/pii/S2212095521000158},
author = {Yu Ting Kwok and Edward Yan Yung Ng},
keywords = {Urban climate, Urban climate modelling, Urban climate application, Mesoscale atmospheric model, Urban parameterization},
abstract = {Researchers have made immense progress in understanding the urban-induced microclimate by numerical modelling. It has been around two decades since urban canopy models now commonly employed in mesoscale atmospheric models for operational and applied research purposes have emerged. To drive further advancement, it is timely to conduct a review of the state-of-the-art and lessons learnt from the relevant literature. In this paper, 102 urban climate real case modelling studies published in 2000–2019 are reviewed. Patterns and preferences in their study locations, periods, model choices, land cover databases, topics discussed, and scenarios investigated are holistically examined. There is an evident improvement in model complexity and urban surface data precision during the period reviewed. Most studies focus on the urban thermal climate and effects of urbanization. Based on the research gaps identified, more work is needed on the currently underrepresented but vulnerable cities in developing countries with tropical, arid, and cold climates. Collaborative field campaigns, initiatives to characterize cities in a consistent manner, and multi-scale modelling approaches have proven to benefit the progress in urban climate studies and should therefore be encouraged. More importantly, efforts should be invested in translating the science into information relevant to human well-being, urban planning, and policymaking.}
}
@article{GAO20211053,
title = {Hepatic transcriptomic adaptation from prepartum to postpartum in dairy cows},
journal = {Journal of Dairy Science},
volume = {104},
number = {1},
pages = {1053-1072},
year = {2021},
issn = {0022-0302},
doi = {https://doi.org/10.3168/jds.2020-19101},
url = {https://www.sciencedirect.com/science/article/pii/S0022030220309590},
author = {S.T. Gao and D.D. Girma and M. Bionaz and L. Ma and D.P. Bu},
keywords = {RNA sequencing, peripartum cow, metabolic adaptation, hepatic transcriptome},
abstract = {ABSTRACT
The transition from pregnancy to lactation is the most challenging period for high-producing dairy cows. The liver plays a key role in biological adaptation during the peripartum. Prior works have demonstrated that hepatic glucose synthesis, cholesterol metabolism, lipogenesis, and inﬂammatory response are increased or activated during the peripartum in dairy cows; however, those works were limited by a low number of animals used or by the use of microarray technology, or both. To overcome such limitations, an RNA sequencing analysis was performed on liver biopsies from 20 Holstein cows at 7 ± 5d before (Pre-P) and 16 ± 2d after calving (Post-P). We found 1,475 upregulated and 1,199 downregulated differently expressed genes (DEG) with a false discovery rate adjusted P-value < 0.01 between Pre-P and Post-P. Bioinformatic analysis revealed an activation of the metabolism, especially lipid, glucose, and amino acid metabolism, with increased importance of the mitochondria and a key role of several signaling pathways, chiefly peroxisome proliferators-activated receptor (PPAR) and adipocytokines signaling. Fatty acid oxidation and gluconeogenesis, with a likely increase in amino acid utilization to produce glucose, were among the most important functions revealed by the transcriptomic adaptation to lactation in the liver. Although gluconeogenesis was induced, data indicated decrease in expression of glucose transporters. The analysis also revealed high activation of cell proliferation but inhibition of xenobiotic metabolism, likely due to the liver response to inflammatory-like conditions. Co-expression network analysis disclosed a tight connection and coordination among genes driving biological processes associated with protein synthesis, energy and lipid metabolism, and cell proliferation. Our data confirmed the importance of metabolic adaptation to lipid and glucose metabolism in the liver of early Post-P cows, with a pivotal role of PPAR and adipocytokines.}
}
@article{BERRA2021118663,
title = {Remote sensing of temperate and boreal forest phenology: A review of progress, challenges and opportunities in the intercomparison of in-situ and satellite phenological metrics},
journal = {Forest Ecology and Management},
volume = {480},
pages = {118663},
year = {2021},
issn = {0378-1127},
doi = {https://doi.org/10.1016/j.foreco.2020.118663},
url = {https://www.sciencedirect.com/science/article/pii/S0378112720314328},
author = {Elias F. Berra and Rachel Gaulton},
keywords = {Satellite data, Land surface phenology, Phenometrics, Ground observations, SOS, EOS, Validation},
abstract = {Vegetation phenology is the study of recurring plant life cycle stages, seasonality which is linked to many ecosystem processes and is an important proxy of climate and environmental change. Remote sensing has been playing an important and increasing role in the monitoring and assessment of vegetation phenology. The aim of this review is to critically examine key studies related to remote sensing of vegetation phenology, with a special focus on temperate and boreal forests. Specifically, we focus on how the latest ground, near-surface and aerial data have been used to assess the satellite-derived Land Surface Phenology (LSP) metrics and the agreements that has been achieved in the last 15 years. Results demonstrated that the timing of satellite-derived LSP events can be detected, in the best-case scenarios, with a certainty of around half-week for spring metrics (e.g. Day of Year -DOY- of start of growing season) and around one week for autumn metrics (e.g. DOY of end of growing season). With expected shifts in plant phenology averaging <1 day per decade, such LSP uncertainties (in terms of absolute phenological dates) could greatly over- or under-estimate these species-level shifts; but the spatial variation in phenology can be consistently monitored. An increasing number of studies have investigated autumn phenology in the last decade, but autumn phenological dates continue to be more challenging to retrieve and interpret than spring dates. Emerging opportunities to further advance remote sensing of forest phenology is presented that includes synergetic use of multiple orbital sensors and its LSP evaluation with data from new sensors at a ground, near-surface and airborne level; yet traditional ground-based observations will continue to be highly useful to accurately record the timing of species-specific phenological events. This review might provide a guide for planning and managing remote sensing of forest phenology.}
}
@article{BOGDANOVIC2021100624,
title = {On revealing shared conceptualization among open datasets},
journal = {Journal of Web Semantics},
volume = {66},
pages = {100624},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100624},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300573},
author = {Miloš Bogdanović and Nataša Veljković and Milena {Frtunić Gligorijević} and Darko Puflović and Leonid Stoimenov},
keywords = {Open data, Formal concept analysis, Semantic similarity, Categorization, Natural language processing},
abstract = {Openness and transparency initiatives are not only milestones of science progress but have also influenced various fields of organization and industry. Under this influence, varieties of government institutions worldwide have published a large number of datasets through open data portals. Government data covers diverse subjects and the scale of available data is growing every year. Published data is expected to be both accessible and discoverable. For these purposes, portals take advantage of metadata accompanying datasets. However, a part of metadata is often missing which decreases users’ ability to obtain the desired information. As the scale of published datasets grows, this problem increases. An approach we describe in this paper is focused towards decreasing this problem by implementing knowledge structures and algorithms capable of proposing the best match for the category where an uncategorized dataset should belong to. By doing so, our aim is twofold: enrich datasets metadata by suggesting an appropriate category and increase its visibility and discoverability. Our approach relies on information regarding open datasets provided by users — dataset description contained within dataset tags. Since dataset tags express low consistency due to their origin, in this paper we will present a method of optimizing their usage through means of semantic similarity measures based on natural language processing mechanisms. Optimization is performed in terms of reducing the number of distinct tag values used for dataset description. Once optimized, dataset tags are used to reveal shared conceptualization originating from their usage by means of Formal Concept Analysis. We will demonstrate the advantage of our proposal by comparing concept lattices generated using Formal Concept Analysis before and after the optimization process and use generated structure as a knowledge base to categorize uncategorized open datasets. Finally, we will present a categorization mechanism based on the generated knowledge base that takes advantage of semantic similarity measures to propose a category suitable for an uncategorized dataset.}
}
@article{YIN2021100482,
title = {The data-intensive scientific revolution occurring where two-dimensional materials meet machine learning},
journal = {Cell Reports Physical Science},
volume = {2},
number = {7},
pages = {100482},
year = {2021},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2021.100482},
url = {https://www.sciencedirect.com/science/article/pii/S266638642100182X},
author = {Hang Yin and Zhehao Sun and Zhuo Wang and Dawei Tang and Cheng Heng Pang and Xuefeng Yu and Amanda S. Barnard and Haitao Zhao and Zongyou Yin},
keywords = {machine learning, 2D materials, materials preparation, structure analysis, property exploration},
abstract = {Summary
Machine learning (ML) has experienced rapid development in recent years and been widely applied to assist studies in various research areas. Two-dimensional (2D) materials, due to their unique chemical and physical properties, have been receiving increasing attention since the isolation of graphene. The combination of ML and 2D materials science has significantly accelerated the development of new functional 2D materials, and a timely review may inspire further ML-assisted 2D materials development. In this review, we provide a horizontal and vertical summary of the recent advances at the intersection of the fields of ML and 2D materials, discussing ML-assisted 2D materials preparation (design, discovery, and synthesis of 2D materials), atomistic structure analysis (structure identification and formation mechanism), and properties prediction (electronic properties, thermodynamic properties, mechanical properties, and other properties) and revealing their connections. Finally, we highlight current research challenges and provide insight into future research opportunities.}
}
@article{LI202128,
title = {DNA methylation methods: Global DNA methylation and methylomic analyses},
journal = {Methods},
volume = {187},
pages = {28-43},
year = {2021},
note = {Advance Epigenetics Methods in Biomedicine},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2020.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1046202320302164},
author = {Shizhao Li and Trygve O. Tollefsbol},
keywords = {DNA methylation, DNA hydroxymethylation, Next-generation sequencing, Bisulfite conversion, Endonuclease digestion, Affinity enrichment, Microarray},
abstract = {DNA methylation provides a pivotal layer of epigenetic regulation in eukaryotes that has significant involvement for numerous biological processes in health and disease. The function of methylation of cytosine bases in DNA was originally proposed as a “silencing” epigenetic marker and focused on promoter regions of genes for decades. Improved technologies and accumulating studies have been extending our understanding of the roles of DNA methylation to various genomic contexts including gene bodies, repeat sequences and transcriptional start sites. The demand for comprehensively describing DNA methylation patterns spawns a diversity of DNA methylation profiling technologies that target its genomic distribution. These approaches have enabled the measurement of cytosine methylation from specific loci at restricted regions to single-base-pair resolution on a genome-scale level. In this review, we discuss the different DNA methylation analysis technologies primarily based on the initial treatments of DNA samples: bisulfite conversion, endonuclease digestion and affinity enrichment, involving methodology evolution, principles, applications, and their relative merits. This review may offer referable information for the selection of various platforms for genome-wide analysis of DNA methylation.}
}
@article{YU2021125976,
title = {Information disclosure decisions in an organic food supply chain under competition},
journal = {Journal of Cleaner Production},
volume = {292},
pages = {125976},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.125976},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621001967},
author = {Yanan Yu and Yong He},
keywords = {Asymmetric and private information, Production and pricing, Organic certification, Information disclosure, Competition, Food supply chain},
abstract = {This paper mainly investigates the information disclosure decisions in an organic food supply chain including one farmers’ organization and one small-scale producer (producer 2). We find that when demand competition is fierce, the organization is reluctant to disclose demand information. Producer 2 is willing to disclose product information facing a low portion of revenue sharing and a high safety perception. A “win-win” situation for two producers can occur by means of group certification with a rational portion of revenue sharing. Interestingly, a more transparent market is not necessarily desirable for customers and social welfare.}
}
@incollection{MAHLER2021237,
title = {Chapter 11 - Regulatory aspects of artificial intelligence and machine learning-enabled software as medical devices (SaMD)},
editor = {Michael Mahler},
booktitle = {Precision Medicine and Artificial Intelligence},
publisher = {Academic Press},
pages = {237-265},
year = {2021},
isbn = {978-0-12-820239-5},
doi = {https://doi.org/10.1016/B978-0-12-820239-5.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202395000103},
author = {Michael Mahler and Carolina Auza and Roger Albesa and Carlos Melus and Jungen Andrew Wu},
keywords = {SaMD, FDA, Regulatory, Artificial intelligence, Machine learning, Culture of quality and organizational excellence, Pre-Cert Program, MDR, NMPA, Cybersecurity},
abstract = {With the introduction of artificial intelligence (AI) and machine learning (ML) in healthcare and the development of an increasing number of commercial products based on software as medical device (SaMD), regulatory processes need to evolve in parallel. This book chapter aims to provide a high-level overview of the history of SaMD and reviews some of the various regulatory aspects associated with this group of medical devices. Although we aim to cover the regulations globally, the main focus is on markets dependent on the Food and Drug Administration (FDA) based review process mostly due to the early stages of regulations in other geographies.}
}
@article{YU2021111191,
title = {Prioritizing urban planning factors on community energy performance based on GIS-informed building energy modeling},
journal = {Energy and Buildings},
volume = {249},
pages = {111191},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111191},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821004758},
author = {Hang Yu and Meng Wang and Xiaoyu Lin and Haijin Guo and He Liu and Yingru Zhao and Hongxin Wang and Chaoen Li and Rui Jing},
keywords = {Residential community, Energy use, Two-stage clustering process, Urban planning factor, Sensitivity analysis},
abstract = {The residential sector accounts for an increasing amount of global energy use with continued urbanization. Residential energy-informed urban planning offers an economical and easy-to-operate approach to achieve more efficient urban energy utilization. However, quantifying the interactions between residential energy and urban planning remains an open challenge. This study proposes a holistic approach integrating GIS techniques, building energy modeling, and a global sensitivity analysis to prioritize eight key urban planning factors on the community energy performance based on a building energy dataset. The dataset, including urban planning and building information, was first established using GIS techniques and validated using survey data. The residential energy performance model at the community scale was developed using the clustering tree structure of residential building prototypes and building performance simulations. A combined data-driven and global sensitivity analysis approach was further applied to prioritize the impacts of eight vital urban planning factors on energy use intensity and peak load intensity. A case study of 1963 communities in Shanghai revealed that, for the energy performance of residential communities, the floor area ratio and building coverage ratio are the most influential factors, followed by the maximum height and high-rise proportion having a relatively low impact but higher than other factors. Overall, the proposed holistic approach generates robust insights into urban-scale residential energy performance, which can effectively inform urban planners to achieve more energy-efficient regulatory planning.}
}
@article{BILORIA20213,
title = {From smart to empathic cities},
journal = {Frontiers of Architectural Research},
volume = {10},
number = {1},
pages = {3-16},
year = {2021},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2020.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2095263520300698},
author = {Nimish Biloria},
keywords = {Empathic city, Smart city, Wellbeing, Neoliberalism, Regenerative model},
abstract = {This paper acknowledges the contemporary neoliberal mode of operation of Smart Cities. The pitfalls of Smart Cities concerning its propensity towards techno-centric and efficiency-focused governance are identified, with diminutive emphasis on social equity and human-centric urban growth. Thus, the paper elaborates upon an alternative mode of person-environment-interaction based approach towards placemaking: Empathic Cities. This approach implies embracing a shift from efficiency to sufficiency and wellbeing embedded regenerative perspective for conceiving the built environment. First, the variable dimensions of urban growth and governance, which gave rise to the smart city, are contextualized. The embedded neoliberal operational agenda of smart cities are established. On this basis, the underpinnings of an empathic city are established by acknowledging the shift from techno-centric to human-centric and from product-based to context-based smart city and wellbeing perspectives. Strategies toward urban development are proposed, such as embracing a regenerative perspective wherein the city and its constituents need to be understood as interdependent systemic elements while embracing a human-centric and ethical approach. Additionally, a transition from efficiency to sufficiency-oriented practices and a shift towards inclusive modes of participatory governance are proposed as fundamental principles for an empathic future of the built environment.}
}
@article{DEKEYSER202152,
title = {Opportunities and challenges of using biometrics for business: Developing a research agenda},
journal = {Journal of Business Research},
volume = {136},
pages = {52-62},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321005014},
author = {Arne {De Keyser} and Yakov Bart and Xian Gu and Stephanie Q. Liu and Stacey G. Robinson and P.K. Kannan},
keywords = {Biometrics, Technology, Ethics, Privacy, Security, AI, Bias},
abstract = {Recently, biometric data generated by fingerprints, hand geometry, heart rate, voice patterns, facial characteristics and expressions, brain activity and body movement has increased in both volume and prominence. Surprisingly, academic business literature has remained relatively silent on the immense potential of biometric data, as well as on the various dangers that come with its collection and usage. This article sets out to (1) detail what biometric data entails and how it may be used, (2) describe opportunities associated with using biometric data in various business applications, (3) discuss challenges related to biometric data collection and usage, privacy and security, storage and safety, and potential for reduced inclusiveness and enhanced biases, and (4) outline related directions for future research.}
}
@article{GIRAY2021111031,
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
journal = {Journal of Systems and Software},
volume = {180},
pages = {111031},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111031},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100128X},
author = {Görkem Giray},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review},
abstract = {Context:
Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems.
Objective:
The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems.
Method:
I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies.
Results:
The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions.
Conclusion:
The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.}
}
@article{HALTAS2021102086,
title = {A comprehensive flood event specification and inventory: 1930–2020 Turkey case study},
journal = {International Journal of Disaster Risk Reduction},
volume = {56},
pages = {102086},
year = {2021},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2021.102086},
url = {https://www.sciencedirect.com/science/article/pii/S2212420921000522},
author = {Ismail Haltas and Enes Yildirim and Fatih Oztas and Ibrahim Demir},
keywords = {Flooding, Flood event inventory, Flood event specification, Data model},
abstract = {Flooding is one of the most frequent natural disasters that have significant impact on communities in terms of loss of life, direct and indirect economic losses, and disruption of daily life. Decision makers often depend on flood data inventories to make more informed decisions on the development of flood mitigation plans to protect flood prone communities. A comprehensive inventory that covers multiple aspects of a flood event is critical to identify vulnerable regions, historical trends, and mitigate possible flood impacts. This study proposes an integrated flood data specification to support multi-stakeholder use cases, community-based sustainable domain specific maintenance, and crowdsourced data collection and management. The specification is designed based on comprehensive review of existing global and national repositories, scientific studies and needs and requirements of stakeholders. The specification is designed to include metadata on environmental, economic, and demographic impact, hydraulic, hydrologic, and meteorological features, and detailed location information of a flood event. As a case study, a flood event inventory was compiled for Turkey between 1930 and 2020 using existing national and global data sources and digitized media archives. A total of 2101 flood events with 64 data attributes have been collected over the period of 90 years. An initial statistical analysis of the inventory is also presented for assessment of the seasonal and regional characteristics of flooding in Turkey.}
}
@article{SCHIRMER2021101972,
title = {Neuropsychiatric disease classification using functional connectomics - results of the connectomics in neuroimaging transfer learning challenge},
journal = {Medical Image Analysis},
volume = {70},
pages = {101972},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.101972},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521000189},
author = {Markus D. Schirmer and Archana Venkataraman and Islem Rekik and Minjeong Kim and Stewart H. Mostofsky and Mary Beth Nebel and Keri Rosch and Karen Seymour and Deana Crocetti and Hassna Irzan and Michael Hütel and Sebastien Ourselin and Neil Marlow and Andrew Melbourne and Egor Levchenko and Shuo Zhou and Mwiza Kunda and Haiping Lu and Nicha C. Dvornek and Juntang Zhuang and Gideon Pinto and Sandip Samal and Jennings Zhang and Jorge L. Bernal-Rusiel and Rudolph Pienaar and Ai Wern Chung},
keywords = {Functional connectomics, Disease classification, ADHD, Challenge},
abstract = {Large, open-source datasets, such as the Human Connectome Project and the Autism Brain Imaging Data Exchange, have spurred the development of new and increasingly powerful machine learning approaches for brain connectomics. However, one key question remains: are we capturing biologically relevant and generalizable information about the brain, or are we simply overfitting to the data? To answer this, we organized a scientific challenge, the Connectomics in NeuroImaging Transfer Learning Challenge (CNI-TLC), held in conjunction with MICCAI 2019. CNI-TLC included two classification tasks: (1) diagnosis of Attention-Deficit/Hyperactivity Disorder (ADHD) within a pre-adolescent cohort; and (2) transference of the ADHD model to a related cohort of Autism Spectrum Disorder (ASD) patients with an ADHD comorbidity. In total, 240 resting-state fMRI (rsfMRI) time series averaged according to three standard parcellation atlases, along with clinical diagnosis, were released for training and validation (120 neurotypical controls and 120 ADHD). We also provided Challenge participants with demographic information of age, sex, IQ, and handedness. The second set of 100 subjects (50 neurotypical controls, 25 ADHD, and 25 ASD with ADHD comorbidity) was used for testing. Classification methodologies were submitted in a standardized format as containerized Docker images through ChRIS, an open-source image analysis platform. Utilizing an inclusive approach, we ranked the methods based on 16 metrics: accuracy, area under the curve, F1-score, false discovery rate, false negative rate, false omission rate, false positive rate, geometric mean, informedness, markedness, Matthew’s correlation coefficient, negative predictive value, optimized precision, precision, sensitivity, and specificity. The final rank was calculated using the rank product for each participant across all measures. Furthermore, we assessed the calibration curves of each methodology. Five participants submitted their method for evaluation, with one outperforming all other methods in both ADHD and ASD classification. However, further improvements are still needed to reach the clinical translation of functional connectomics. We have kept the CNI-TLC open as a publicly available resource for developing and validating new classification methodologies in the field of connectomics.}
}
@article{ABAIMOV2021100077,
title = {A survey on the application of deep learning for code injection detection},
journal = {Array},
volume = {11},
pages = {100077},
year = {2021},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2021.100077},
url = {https://www.sciencedirect.com/science/article/pii/S2590005621000254},
author = {Stanislav Abaimov and Giuseppe Bianchi},
keywords = {Machine learning, Deep learning, Network intrusion detection, Code injection, Preprocessing},
abstract = {Code injection is one of the top cyber security attack vectors in the modern world. To overcome the limitations of conventional signature-based detection techniques, and to complement them when appropriate, multiple machine learning approaches have been proposed. While analysing these approaches, the surveys focus predominantly on the general intrusion detection, which can be further applied to specific vulnerabilities. In addition, among the machine learning steps, data preprocessing, being highly critical in the data analysis process, appears to be the least researched in the context of Network Intrusion Detection, namely in code injection. The goal of this survey is to fill in the gap through analysing and classifying the existing machine learning techniques applied to the code injection attack detection, with special attention to Deep Learning. Our analysis reveals that the way the input data is preprocessed considerably impacts the performance and attack detection rate. The proposed full preprocessing cycle demonstrates how various machine-learning-based approaches for detection of code injection attacks take advantage of different input data preprocessing techniques. The most used machine learning methods and preprocessing stages have been also identified.}
}
@article{SHRESTHA2021588,
title = {Augmenting organizational decision-making with deep learning algorithms: Principles, promises, and challenges},
journal = {Journal of Business Research},
volume = {123},
pages = {588-603},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.068},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320306512},
author = {Yash Raj Shrestha and Vaibhav Krishna and Georg {von Krogh}},
keywords = {Case studies, Decision-making, Deep learning, Artificial intelligence},
abstract = {The current expansion of theory and research on artificial intelligence in management and organization studies has revitalized the theory and research on decision-making in organizations. In particular, recent advances in deep learning (DL) algorithms promise benefits for decision-making within organizations, such as assisting employees with information processing, thereby augment their analytical capabilities and perhaps help their transition to more creative work. We conceptualize the decision-making process in organizations augmented with DL algorithm outcomes (such as predictions or robust patterns from unstructured data) as deep learning–augmented decision-making (DLADM). We contribute to the understanding and application of DL for decision-making in organizations by (a) providing an accessible tutorial on DL algorithms and (b) illustrating DLADM with two case studies drawing on image recognition and sentiment analysis tasks performed on datasets from Zalando, a European e-commerce firm, and Rotten Tomatoes, a review aggregation website for movies, respectively. Finally, promises and challenges of DLADM as well as recommendations for managers in attending to these challenges are also discussed.}
}
@article{ZHANG2021103372,
title = {A customized deep learning approach to integrate network-scale online traffic data imputation and prediction},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {132},
pages = {103372},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103372},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21003740},
author = {Zhengchao Zhang and Xi Lin and Meng Li and Yinhai Wang},
keywords = {Traffic prediction, Online data imputation, Deep learning, Bidirectional recurrent neural network, Graph convolution, 1 × 1 Convolution},
abstract = {Online data imputation and traffic prediction based on real-time data streams are essential for the intelligent transportation systems, particularly online navigation applications based on the real-time traffic information. However, the inevitable data missing problem caused by various disturbances undermines the information contained in such real-time data, thereby threatening the reliability of data acquisition as well as the prediction results. Such scenarios raise a strong need for integrating the tasks of network-scale online data imputation and traffic prediction, because the existing two-step approaches that separate the above procedures cannot be implemented in an online manner. In this paper, we propose a customized spatiotemporal deep learning architecture, named the graph convolutional bidirectional recurrent neural network (GCBRNN), to combine network-scale online data imputation and traffic prediction into an integrated task. The imputation mechanism and bidirectional framework are developed to cooperatively estimate missing entries and infer future values. We further design a network-scale graph convolutional gated recurrent unit (NGC-GRU) within the GCBRNN, which applies the graph convolution operation and 1×1 convolution module to capture the spatiotemporal dependencies in the traffic data. Experiments are carried out on two real-world traffic networks, including traffic speed and flow datasets. The comparison results demonstrate that our approach significantly outperforms several classical benchmark models with respect to both the imputation and prediction tasks on two datasets under various missing data rates.}
}
@article{WIK2021100168,
title = {Proximity Extension Assay in Combination with Next-Generation Sequencing for High-throughput Proteome-wide Analysis},
journal = {Molecular & Cellular Proteomics},
volume = {20},
pages = {100168},
year = {2021},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2021.100168},
url = {https://www.sciencedirect.com/science/article/pii/S1535947621001407},
author = {Lotta Wik and Niklas Nordberg and John Broberg and Johan Björkesten and Erika Assarsson and Sara Henriksson and Ida Grundberg and Erik Pettersson and Christina Westerberg and Elin Liljeroth and Adam Falck and Martin Lundberg},
keywords = {biomarker, proteomics, next-generation sequencing, proximity extension assay, multiplex, immunoassay, plasma, serum, antibody},
abstract = {Understanding the dynamics of the human proteome is crucial for developing biomarkers to be used as measurable indicators for disease severity and progression, patient stratification, and drug development. The Proximity Extension Assay (PEA) is a technology that translates protein information into actionable knowledge by linking protein-specific antibodies to DNA-encoded tags. In this report we demonstrate how we have combined the unique PEA technology with an innovative and automated sample preparation and high-throughput sequencing readout enabling parallel measurement of nearly 1500 proteins in 96 samples generating close to 150,000 data points per run. This advancement will have a major impact on the discovery of new biomarkers for disease prediction and prognosis and contribute to the development of the rapidly evolving fields of wellness monitoring and precision medicine.}
}
@article{YU2021101136,
title = {Tracing the main path of interdisciplinary research considering citation preference: A case from blockchain domain},
journal = {Journal of Informetrics},
volume = {15},
number = {2},
pages = {101136},
year = {2021},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2021.101136},
url = {https://www.sciencedirect.com/science/article/pii/S1751157721000079},
author = {Dejian Yu and Tianxing Pan},
keywords = {Blockchain, Main path analysis, Discipline difference, Citation preference},
abstract = {Main path analysis has been widely used in various fields to detect their development trajectories. However, the previous methods treat every citation equally. In fact, it leaves a question open to scholars considering that there are different citation preferences in different disciplines and at different publication times. There are different citation preferences in different disciplines and at different periods, which are ignored by scholars. In order to deal with the problem in identifying development paths in interdisciplinary research areas, this paper proposes a new main path analysis method. The improved main path analysis considers two factors involved in citation preference, including discipline bias and time bias. An evidence analysis from blockchain domain is conducted to demonstrate the effectiveness of the proposed method. The research result shows that the proposed main path analysis method in this paper can resolve the problem of discipline bias and time bias in interdisciplinary research. Moreover, the improved method provides a more differentiated ranking for citation linkages in the network. Our research can enhance the objectivity of the resulting main paths and promote broader application of the main path analysis.}
}
@article{CHEN2021103226,
title = {Scalable low-rank tensor learning for spatiotemporal traffic data imputation},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {129},
pages = {103226},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103226},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21002400},
author = {Xinyu Chen and Yixian Chen and Nicolas Saunier and Lijun Sun},
keywords = {Spatiotemporal traffic data, High-dimensional data, Missing data imputation, Low-rank tensor completion, Linear unitary transformation, Quadratic variation},
abstract = {Missing value problem in spatiotemporal traffic data has long been a challenging topic, in particular for large-scale and high-dimensional data with complex missing mechanisms and diverse degrees of missingness. Recent studies based on tensor nuclear norm have demonstrated the superiority of tensor learning in imputation tasks by effectively characterizing the complex correlations/dependencies in spatiotemporal data. However, despite the promising results, these approaches do not scale well to large data tensors. In this paper, we focus on addressing the missing data imputation problem for large-scale spatiotemporal traffic data. To achieve both high accuracy and efficiency, we develop a scalable tensor learning model—Low-Tubal-Rank Smoothing Tensor Completion (LSTC-Tubal)—based on the existing framework of Low-Rank Tensor Completion, which is well-suited for spatiotemporal traffic data that is characterized by multidimensional structure of location × time of day × day. In particular, the proposed LSTC-Tubal model involves a scalable tensor nuclear norm minimization scheme by integrating linear unitary transformation. Therefore, tensor nuclear norm minimization can be solved by singular value thresholding on the transformed matrix of each day while the day-to-day correlation can be effectively preserved by the unitary transform matrix. Before setting up the experiment, we consider some real-world data sets, including two large-scale 5-min traffic speed data sets collected by the California PeMS system with 11160 sensors: 1) PeMS-4W covers the data over 4 weeks (i.e., 288×28 time points), and 2) PeMS-8W covers the data over 8 weeks (i.e., 288×56 time points). We compare LSTC-Tubal with some state-of-the-art baseline models, and find that LSTC-Tubal can achieve competitively accuracy with a significantly lower computational cost. In addition, the LSTC-Tubal will also benefit other tasks in modeling large-scale spatiotemporal traffic data, such as network-level traffic forecasting.}
}
@article{MELLIT2021110889,
title = {Artificial intelligence and internet of things to improve efficacy of diagnosis and remote sensing of solar photovoltaic systems: Challenges, recommendations and future directions},
journal = {Renewable and Sustainable Energy Reviews},
volume = {143},
pages = {110889},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.110889},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121001830},
author = {Adel Mellit and Soteris Kalogirou},
keywords = {Deep learning, Fault detection and diagnosis, Internet of things, Machine learning, Photovoltaic systems, Remote sensing, Smart monitoring},
abstract = {Currently, a huge number of photovoltaic plants have been installed worldwide and these plants should be carefully protected and supervised continually in order to be safe and reliable during their working lifetime. Photovoltaic plants are subject to different types of faults and failures, while available fault detection equipment are mainly used to protect and isolate the photovoltaic plants from some faults (such as arc fault, line-to-line, line-to-ground and ground faults). Although a good number of international standards (IEC, NEC, and UL) exists, undetectable faults continue to create serious problems in photovoltaic plants. Thus, designing smart equipment, including artificial intelligence and internet of things for remote sensing and fault detection and diagnosis of photovoltaic plants, will considerably solve the shortcomings of existing methods and commercialized equipment. This paper presents an overview of artificial intelligence and internet of things applications in photovoltaic plants. This research presents also the most advanced algorithms such as machine and deep learning, in terms of cost implementation, complexity, accuracy, software suitability, and feasibility of real-time applications. The embedding of artificial intelligence and internet of things techniques for fault detection and diagnosis into simple hardware, such as low-cost chips, may be economical and technically feasible for photovoltaic plants located in remote areas, with costly and challenging accessibility for maintenance. Challenging issues, recommendations, and trends of these techniques will also be presented in this paper.}
}
@article{MARQUES2021100037,
title = {Policy report on FinTech data gaps},
journal = {Latin American Journal of Central Banking},
volume = {2},
number = {3},
pages = {100037},
year = {2021},
issn = {2666-1438},
doi = {https://doi.org/10.1016/j.latcb.2021.100037},
url = {https://www.sciencedirect.com/science/article/pii/S266614382100017X},
author = {José Manuel Marqués and Fernando Ávila and Anahí Rodríguez-Martínez and Raúl Morales-Reséndiz and Antonio Marcos and Tamara Godoy and Pablo Villalobos and Andrea Ocontrillo and Valerie Ann Lankester and Clemente Blanco and Karla Reyes and Silvia Irina Lopez and Ana Fernández and Román Santos and Luis Ángel Maza and Manuel Sánchez and Carlos Domínguez and Natalie Haynes and Novelette Panton and Mario Griffiths and Kurt Murray and Michelle Doyle-Lowe and Leslie Ann {Des Vignes} and Michelle Francis-Pantor},
abstract = {This document aims to provide an overview of the main issues related to data gaps to facilitate monitoring of FinTech and overcome the significant challenges towards incorporating FinTech activities in regular statistics. Moreover, the document explains the implications of data gaps on some of the Central Banks’ main areas, in particular, monetary policy, financial stability, payment systems, and economic activity. Additionally, other implications related to the activity of BigTech companies, the impact of COVID-19 and Cybersecurity issues are explained, which represent an important challenge for data gathering at Central Banks. Also, it describes the main findings of the Irving Fisher Committee (IFC) survey “Central Banks and FinTech data” based on the answers provided by Latin American and Caribbean (LAC) countries, which identify their different positions regarding this topic and the current initiatives that each one is launching. Finally, a number of next steps are proposed based on a policy discussion and how LAC countries could overcome data gaps and improve data collection based on their current experience.}
}
@article{ORTMEIER2021163,
title = {Framework for the integration of Process Mining into Life Cycle Assessment},
journal = {Procedia CIRP},
volume = {98},
pages = {163-168},
year = {2021},
note = {The 28th CIRP Conference on Life Cycle Engineering, March 10 – 12, 2021, Jaipur, India},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121000470},
author = {Christian Ortmeier and Nadja Henningsen and Adrian Langer and Alexander Reiswich and Alexander Karl and Christoph Herrmann},
keywords = {process mining, process discovery, life cycle assesment, hotspot analyzes, life cycle inventory},
abstract = {An increasing product variance, shorter life cycles and the integration of new products and technologies into existing factories lead to a high complexity in today’s production systems. This makes it difficult to carry out Life Cycle Assessments (LCA) continuously and effectively. Major challenges of LCA are on the one hand a high expenditure of time and on the other hand a static evaluation of individual products. In order to perform dynamic and continuous process analyses, companies increasingly rely on new technologies and methods. Numerous studies have already been able to tap promising potentials by using Process Mining (PM). In contrast to traditional methods of process modeling, PM uses event log data to model the actual production processes. Based on this data, a real-time model is built to identify waste. In a holistic approach, PM is able to automatically uncover social and organizational networks and map them in a simulation model. According to the current state of research, there is no concept for integrating PM into LCA. Therefore, the present work focuses on the investigation of interfaces between PM and LCA in order to systematically identify and evaluate the potentials of using PM. The results are applied to a use case in the sector of commercial vehicles.}
}
@article{DAO2021103852,
title = {Semantic framework for interdependent infrastructure resilience decision support},
journal = {Automation in Construction},
volume = {130},
pages = {103852},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103852},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003034},
author = {Jicao Dao and S. Thomas Ng and Yifan Yang and Shenghua Zhou and Frank J. Xu and Martin Skitmore},
keywords = {Semantic Web, Ontology, Infrastructure systems, Interdependency, Data integration, Resilient decisions},
abstract = {The increasing need for interdependent infrastructure systems to withstand natural disasters has called for the co-creation of resilience decisions to minimize the impact on society. However, issues related to information integration across different infrastructure systems hamper decision making from a system-to-systems perspective. To resolve this problem, the Semantic Web technologies are presented in this paper to serve four functions: (i) linking cross domains through ontology development to represent different domain knowledge; (ii) integrating multiple-source heterogeneous data by a common data format; (iii) retrieving useful information using semantic query language; and (iv) deriving machine automatic logical reasoning by rule languages and logic engines to provide informed resilience decision making support. The proposed framework is tested by a case scenario involving intertwined drainage-transport-building systems under the influence of urban flooding. The result indicates that the framework effectively facilitates information integration between diverse infrastructure systems and helps decision-makers by providing resilience decision-making support.}
}
@article{WU2021112247,
title = {Does internet development improve green total factor energy efficiency? Evidence from China},
journal = {Energy Policy},
volume = {153},
pages = {112247},
year = {2021},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2021.112247},
url = {https://www.sciencedirect.com/science/article/pii/S0301421521001166},
author = {Haitao Wu and Yu Hao and Siyu Ren and Xiaodong Yang and Guo Xie},
keywords = {Internet development, Green total factor energy efficiency, Spatial durbin model, Dynamic threshold model},
abstract = {Information and communication technology supported by the internet has become an important driving force that promotes the intelligent development of environmental governance in China. Using Chinese provincial panel data for the period 2006–2017, this study investigates whether the internet has improved China's green total factor energy efficiency (GTFEE) using a dynamic spatial Durbin model, mediation effect model and dynamic threshold panel model. The empirical results indicate that the GTFEE has a significant positive spatial correlation. Internet development can not only directly improve local GTFEE but also improve GTFEE in neighboring regions. After accounting for potential endogeneity, this conclusion is still valid. Meanwhile, internet development can indirectly improve regional GTFEE by reducing the degree of resource mismatch while enhancing GTFEE by improving regional innovation capabilities and promoting industrial structure upgrades. In addition, the regression results of the dynamic threshold model show that there is a nonlinear relationship between the influence of the internet development and GTFEE. Specifically, due to an increase in the degree of labor resource mismatch and capital resource mismatch, the impact of the internet on GTFEE has gradually decreased, and this effect has gradually increased with the improvement of regional innovation capabilities and the industrial structure.}
}
@article{WILSON2021101526,
title = {Beyond the supply side: Use and impact of municipal open data in the U.S},
journal = {Telematics and Informatics},
volume = {58},
pages = {101526},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2020.101526},
url = {https://www.sciencedirect.com/science/article/pii/S0736585320301854},
author = {Bev Wilson and Cong Cong},
keywords = {Open data, Local government, Civic technology, Digital equity},
abstract = {While the number of open government data initiatives has increased considerably over the past decade, the impact of these initiatives remains uncertain. Recent studies have been critical of the “bias toward the supply side” and lack of “sufficient attention to the user perspective” in the way that open government data initiatives are implemented. This article asks: (1) who is using municipal open government data resources and for what purposes? and (2) what impact are municipal open government data having in cities where they have been implemented? We performed a qualitative analysis of 26 semi-structured telephone interviews conducted with government staff, civic technologists, and private sector stakeholders in nine cities around the United States. Each of these 30 to 45-minute telephone interviews were transcribed and analyzed to distill insights regarding the use and impact of municipal open government data in the nine cities considered. We find that the array of actors within open government data ecosystems at the local level is expanding as distinctions between the public and private sectors becomes increasingly blurred and that the demands of managing and sustaining these initiatives has led to changes in the services offered by local government, as well as in the duties of government staff. The impact of these data resources has been primarily felt within local government itself, although the lack of monitoring mechanisms makes it difficult to systematically evaluate their broader effects. We conclude that open government data initiatives should be coordinated and better integrated with digital equity and digital inclusion efforts in order to advance their political and social goals.}
}
@incollection{2021679,
title = {Index},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {679-692},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.09991-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539099918}
}
@article{CAESAR20212041,
title = {Metabolomics and genomics in natural products research: complementary tools for targeting new chemical entities},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {2041-2065},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00036e},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008820},
author = {Lindsay K. Caesar and Rana Montaser and Nancy P. Keller and Neil L. Kelleher},
abstract = {ABSTRACT
Covering: 2010 to 2021 Organisms in nature have evolved into proficient synthetic chemists, utilizing specialized enzymatic machinery to biosynthesize an inspiring diversity of secondary metabolites. Often serving to boost competitive advantage for their producers, these secondary metabolites have widespread human impacts as antibiotics, anti-inflammatories, and antifungal drugs. The natural products discovery field has begun a shift away from traditional activity-guided approaches and is beginning to take advantage of increasingly available metabolomics and genomics datasets to explore undiscovered chemical space. Major strides have been made and now enable -omics-informed prioritization of chemical structures for discovery, including the prospect of confidently linking metabolites to their biosynthetic pathways. Over the last decade, more integrated strategies now provide researchers with pipelines for simultaneous identification of expressed secondary metabolites and their biosynthetic machinery. However, continuous collaboration by the natural products community will be required to optimize strategies for effective evaluation of natural product biosynthetic gene clusters to accelerate discovery efforts. Here, we provide an evaluative guide to scientific literature as it relates to studying natural product biosynthesis using genomics, metabolomics, and their integrated datasets. Particular emphasis is placed on the unique insights that can be gained from large-scale integrated strategies, and we provide source organism-specific considerations to evaluate the gaps in our current knowledge.}
}
@article{DEFRAEYE2021245,
title = {Digital twins are coming: Will we need them in supply chains of fresh horticultural produce?},
journal = {Trends in Food Science & Technology},
volume = {109},
pages = {245-258},
year = {2021},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2021.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S092422442100025X},
author = {Thijs Defraeye and Chandrima Shrivastava and Tarl Berry and Pieter Verboven and Daniel Onwude and Seraina Schudel and Andreas Bühlmann and Paul Cronje and René M. Rossi},
keywords = {Postharvest, Physics-based, Virtual, Modeling, Simulation, Cyber-physical},
abstract = {Background
Digital twins have advanced fast in various industries, but are just emerging in postharvest supply chains. A digital twin is a virtual representation of a certain product, such as fresh horticultural produce. This twin is linked to the real-world product by sensors supplying data of the environmental conditions near the target fruit or vegetable. Statistical and data-driven twins quantify how quality loss of fresh horticultural produce occurs by grasping patterns in the data. Physics-based twins provide an augmented insight into the underlying physical, biochemical, microbiological and physiological processes, enabling to explain also why this quality loss occurs.
Scope and approach
We identify what the key advantages are of digital twins and how the supply chain of fresh horticultural produce can benefit from them in the future.
Key findings and conclusions
A digital twin has a huge potential to help horticultural produce to tell its history as it drifts along throughout its postharvest life. The reason is that each shipment is subject to a unique and unpredictable set of temperature and gas atmosphere conditions from farm to consumer. Digital twins help to identify the resulting, largely uncharted, postharvest evolution of food quality. The benefit of digital twins particularly comes forward for perishable species and at low airflow rates. Digital twins provide actionable data for exporters, retailers, and consumers, such as the remaining shelf life for each shipment, on which logistics decisions and marketing strategies can be based. The twins also help diagnose and predict potential problems in supply chains that will reduce food quality and induce food loss. Twins can even suggest preventive shipment-tailored measures to reduce retail and household food losses.}
}
@article{MALAGNINO2021127716,
title = {Building Information Modeling and Internet of Things integration for smart and sustainable environments: A review},
journal = {Journal of Cleaner Production},
volume = {312},
pages = {127716},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.127716},
url = {https://www.sciencedirect.com/science/article/pii/S095965262101934X},
author = {Ada Malagnino and Teodoro Montanaro and Mariangela Lazoi and Ilaria Sergi and Angelo Corallo and Luigi Patrono},
keywords = {Environmental sustainability, Smart environment, Internet of Things, BIM (Building Information Modeling), IoT},
abstract = {During the last decades, society has increasingly moved towards the adoption of digital solutions in almost every aspect of people's lives with the aim of enhancing daily activities. At the same time, the environmental impact of the built environment has attracted the attention of public opinion that is gradually perceiving the necessity of limiting its negative effects in order to safeguard the Earth and people's wellbeing. The Internet of Things is one of the biggest ecosystems that is bringing innovations encompassing digital solutions in almost every sector. On the other hand, the Building Information Modeling approach allows for data sharing among stakeholders, traceability, and the integrated management of the building or infrastructure life-cycle through a 3D informative virtual model. Our study reviews existing research works and technological solutions that integrate these two important topics to enhance the sustainability of the built environment, making it smarter. The presented review analyses the existing papers available in literature from January 2015 to December 2020, to present the best practices in this integration and discuss limitations of the identified solutions. Based on the outcomes of the analysis and aiming at the creation of a solid knowledge basis for the community interested in the sector, a comprehensive modular architecture has been proposed. Finally, new directions for future works are presented by discussing how the proposed architecture can actually facilitate the design and development phases.}
}
@article{STEPANYAN2021104929,
title = {Multiple rotations of Gaussian quadratures: An efficient method for uncertainty analyses in large-scale simulation models},
journal = {Environmental Modelling & Software},
volume = {136},
pages = {104929},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104929},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220309865},
author = {Davit Stepanyan and Harald Grethe and Georg Zimmermann and Khalid Siddig and Andre Deppermann and Arndt Feuerbacher and Jonas Luckmann and Hugo Valin and Takamasa Nishizawa and Tatiana Ermolieva and Petr Havlik},
keywords = {Uncertainty analysis, Systematic sensitivity analysis, Stochastic modeling, Multiple rotations of Gaussian quadratures, Monte Carlo sampling, Computable general equilibrium models, Partial equilibrium models},
abstract = {Concerns regarding the impact of climate change, food price volatility, and weather uncertainty have motivated users of simulation models to consider uncertainty in their simulations. One way to do this is to integrate uncertainty components in the model equations, thus turning the model into a problem of numerical integration. Most of these problems do not have analytical solutions, and researchers, therefore, apply numerical approximation methods. This article presents a novel approach to conducting an uncertainty analysis as an alternative to the computationally burdensome Monte Carlo-based (MC) methods. The developed method is based on the degree three Gaussian quadrature (GQ) formulae and is tested using three large-scale simulation models. While the standard single GQ method often produces low-quality approximations, the results of this study demonstrate that the proposed approach reduces the approximation errors by a factor of nine using only 3.4% of the computational effort required by the MC-based methods in the most computationally demanding model.}
}
@article{YU2021129386,
title = {Quantification and management of urban traffic emissions based on individual vehicle data},
journal = {Journal of Cleaner Production},
volume = {328},
pages = {129386},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129386},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621035708},
author = {Zhi Yu and Weichi Li and Yonghong Liu and Xuelan Zeng and Yongming Zhao and Kaiying Chen and Bin Zou and Jiajun He},
keywords = {Individual vehicle emissions, Emission quantification, Licence plate recognition data, Spatiotemporal emission characteristics, Traffic emission reduction policy},
abstract = {Urban traffic pollution poses a serious threat to the environment and human health, especially in urban centres with high population density. Traditional traffic pollution quantification and management methods can be improved based on fine-grained individual vehicle data provided by intelligent transportation systems. Traditional traffic emission quantification and management are often based on simulated or relatively coarse-grained measured data. Such data lack a comprehensive reflection of the actual conditions of all vehicles travelling on roads, which leads to deviations in emission quantification; thus, they cannot support the delicate control policy of traffic pollution. This paper presents a high-resolution individual vehicle emission quantification method based on real-time, real-world individual vehicle data, with a combination of automatic licence plate recognition data and vehicle registration data currently used for traffic management. In this study, we quantified the emissions of each vehicle driving in the urban centre of the case city and analysed regional traffic emission characteristics. We found that there was an apparent uneven distribution of vehicle emissions; that is, the emissions from a small number of high-emission vehicles accounted for a large proportion of the regional traffic emissions. Different pollutants and vehicle types had different emission distribution characteristics. Furthermore, we explored emission reduction policies based on the management of high-emission vehicles identified by individual vehicle data and conduct fine-scale analysis of the link-level hourly emission reduction effects. In addition, a comparison between traditional methods and the method used in this paper for emission quantification was performed. This paper provides a basis for the accurate analysis of regional traffic emission characteristics, individual-based emission reduction policy formulation, and refined policy effect analysis, which has great significance for the control of traffic pollution.}
}
@article{JIN2023104104,
title = {Machine learning techniques for pulmonary nodule computer-aided diagnosis using CT images: A systematic review},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104104},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104104},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422005638},
author = {Haizhe Jin and Cheng Yu and Zibo Gong and Renjie Zheng and Yinan Zhao and Quanwei Fu},
keywords = {Computed tomography, Pulmonary nodule, Computer-aided diagnosis, Machine learning, Deep learning},
abstract = {Objective
Early detection of pulmonary nodules is critical for the prevention and treatment of lung cancer.Concomitant with recent advancements in computer performance and intelligent algorithms, the efficacy of pulmonary nodule computer-aided diagnosis (CAD) has been continuously improving, and various algorithms have been proposed using different datasets. This study systematically analyzed and compared the performance of machine learning algorithms using the same dataset in the diagnosis of pulmonary nodules through a literature review.
Methods
The widely used LIDC-IDRI dataset and its subset LUNA16 were used as data objects.The SpringerLink, Science Direct, IEEE Xplore, and PubMed scientific databases were searched, and seventy-five papers were analyzed.
Results
Deep-learning-based CAD was found to be superior to conventional machine-learning-based CAD in terms of the number of published studies and algorithm performance.The best performances were as follows: feedforward neural network (FNN) and convolutional neural network (CNN) for detecting pulmonary nodules;region-based CNN (R-CNN) for the segmentation of pulmonary nodules;residual neural network (ResNet) for the classification of nodules and non-nodules; anddeep neural network (DNN) for the classification of benign and malignancy.
Conclusion
To further extend the application of CAD in clinical practice, the appropriate algorithm type should be used based on the characteristics of the task.The CAD process should be divided into logical stages and the optimal algorithm for each stage should be used to increase the reliability of the process.
Significance
The CAD performance of numerous algorithms on the same dataset is systematically compared and ideas for future exploration are provided.}
}
@article{CUGNO2021120756,
title = {Openness to Industry 4.0 and performance: The impact of barriers and incentives},
journal = {Technological Forecasting and Social Change},
volume = {168},
pages = {120756},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120756},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521001888},
author = {Monica Cugno and Rebecca Castagnoli and Giacomo Büchi},
keywords = {Industry 4.0, Openness, Performance, Barriers, Incentives, Mediators},
abstract = {The impact of barriers and incentives on the relationship between openness to Industry 4.0 and performance have so far received little scholarly attention. As a result, this paper explores this relationship by employing a mixed methods approach. A qualitative analysis using in-depth interviews and multiple case studies identifies prominent barriers and incentives, whilst a quantitative analysis on a representative sample of 500 local manufacturing units in Piedmont (a region of Northern Italy) is undertaken via an OLS regression-based path analysis. The results of the parallel-serial multiple mediation model show that: (1) greater openness to Industry 4.0 is related to better performance; (2) greater openness to Industry 4.0 leads to a higher perception of barriers; (3) greater knowledge-related and economic and financial barriers improve performance, abstracting from the adoption of incentives; and (4) greater openness to Industry 4.0 drives the adoption of incentives. However, perceived economic and financial barriers are found not to drive firms to adopt more incentives. The study contributes to the Industry 4.0 literature by identifying previously unidentified strengths and weaknesses to barriers and incentives, and highlights the necessity of policies that reflect real firms’ needs.}
}
@article{BONESTROO2021104698,
title = {Diagnostic properties of milk diversion and farmer-reported mastitis to indicate clinical mastitis status in dairy cows using Bayesian latent class analysis},
journal = {Livestock Science},
volume = {253},
pages = {104698},
year = {2021},
issn = {1871-1413},
doi = {https://doi.org/10.1016/j.livsci.2021.104698},
url = {https://www.sciencedirect.com/science/article/pii/S1871141321003061},
author = {John Bonestroo and Nils Fall and Mariska {van der Voort} and Ilka Christine Klaas and Henk Hogeveen and Ulf Emanuelson},
keywords = {antibiotic treatment, proxy, Automatic milking system, Milk withdrawal, Latent class analysis},
abstract = {The development of digital farming gives bovine mastitis research and management tools access to large datasets. However, the quality of registered data on clinical mastitis cases or treatments may be inadequate (e.g. due to missing records). In automatic milking systems, the decision to divert milk from the bulk milk tank during milking is registered (i.e. milk diversion indicator) for every milking and could potentially indicate a clinical mastitis case. This study accordingly estimated the diagnostic performance of a milk diversion indicator in relation to farmer-recorded clinical mastitis cases in the absence of a “gold standard”. Data on milk diversion and farmer-reported clinical mastitis from 3,443 lactations in 13 herds were analyzed. Each cow lactation was split into 30-DIM periods in which it was registered whether milk was diverted and whether clinical mastitis was reported. One 30-DIM period was randomly sampled for each lactation and this was the unit of analysis, this procedure was repeated 300 times, resulting in 300 datasets to create autocorrelation-robust results during analysis. We used Bayesian latent class analysis to assess the diagnostic properties of milk diversion and farmer-reported clinical status. We analyzed different episode lengths of milk diversion of 1 or more milk diversion days until 10 or more milk diversion days for two scenarios: farmers with poor-quality (51% sensitivity, 99% specificity) and high-quality (90% sensitivity, 99% specificity) mastitis registrations. The analysis was done for all 300 datasets. The results showed that for the scenario where the quality of clinical mastitis reporting was high, the sensitivity was similar for milk-diversion threshold durations of 1–4 days (0.843 to 0.793 versus 0.893). Specificity increased when the number of days of milk diversion increased and was ≥98% at a milk-diversion threshold durations of 8 or more consecutive milk diversion days. In the scenario where the quality of clinical mastitis reporting was low, the sensitivity of milk diversion and reported clinical mastitis cases was similar at milk-diversion threshold durations of 1–7 days (0.687 to 0.448 versus 0.503 to 0.504) while specificity exceeded the 98% at milk-diversion threshold durations of 7 or more consecutive milk diversion days. In both scenarios, a milk diversion threshold duration of 4–7 days achieved the most desirable combined sensitivity and specificity. This study concluded that milk diversion can be a valid alternative to farmer-reported clinical mastitis as it performs similarly in indicating actual clinical mastitis.}
}