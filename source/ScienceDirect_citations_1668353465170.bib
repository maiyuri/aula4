@article{ZHAO2021101580,
title = {Understanding the key factors and configurational paths of the open government data performance: Based on fuzzy-set qualitative comparative analysis},
journal = {Government Information Quarterly},
volume = {38},
number = {3},
pages = {101580},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101580},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000162},
author = {Yupan Zhao and Bo Fan},
keywords = {OGD, Implementation performance, fsQCA, Configurational path},
abstract = {The governments worldwide have attached great importance to open government data (OGD), and many OGD projects have emerged in recent years. However, the performance of OGD greatly differs in various districts and governments. Therefore, the influencing factors of OGD performance should be explored. However, the existing research has not yet established a systematic analytical framework for OGD performance, and the explanation degree of performance differences in OGD implementation is limited. Thus, this study takes technical management capacity, financial resource, organization arrangement, rules and regulations, organization culture, public demand, and inter-government competition as antecedent conditions under the perspective of technology–organization–environment framework and resource-based theory. From the cases of 16 provincial OGD practice in China, we employ fuzzy-set qualitative comparative analysis to explore the influencing mechanism of the interaction and coordination of multiple conditions on OGD performance. Results indicate that OGD performance depends on the integration of the total effect of various factors. Moreover, four configurational paths could be utilized to achieve high OGD performance, namely, organization–balanced path, organization–environment path, balanced path, and organization–technology path. Furthermore, a substitution relationship exists among different conditional variables, which points out the direction and focus of the implementation of OGD for governments with different endowment characteristics. This study enriches the existing studies of OGD implementation and provides references for OGD practice.}
}
@article{LI202347,
title = {Human activity recognition based on multienvironment sensor data},
journal = {Information Fusion},
volume = {91},
pages = {47-63},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001841},
author = {Yang Li and Guanci Yang and Zhidong Su and Shaobo Li and Yang Wang},
keywords = {Environmental sensor, Data constraint, Neural network, Human activity recognition, Smart home},
abstract = {With the development of artificial intelligence and the broad application of sensors, human activity recognition (HAR) technologies based on noninvasive environmental sensors have received extensive attention and have shown great application value. Owing to the initiative of human activities and machine learning-based methods relying on domain knowledge, obtaining a uniform model to understand the daily behaviors of different residents is difficult. From the perspective of data feature constraints to recognition methods, we constructed a methodology for single user's daily behavior recognition that can adaptively constrain the sensor noise during human activities in multitenant smart home scenarios. We propose a sensor data contribution significance analysis (CSA) method based on the sensor status frequency-inverse type frequency for HAR. This method is employed to measure the contribution of a particular type of sensor to a certain type of behavior recognition. We then build a spatial distance matrix based on the layout of environmental sensors for context-awareness and reducing data noise. Finally, we propose a HAR algorithm based on wide time-domain convolutional neural network and multienvironment sensor data (HAR_WCNN) for daily behavior recognition. Comparative experiment results on the CASAS dataset show that the proposed HAR_WCNN outperforms the compared state-of-the-art methods in terms of HAR accuracy and time consumption.}
}
@article{NITSCHKE2021642,
title = {Conceptualizing the Internet of Things Data Supply},
journal = {Procedia Computer Science},
volume = {181},
pages = {642-649},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.213},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002568},
author = {Patrick Nitschke and Susan P. Williams},
keywords = {Internet of Things, Data Supply, Value Capturing},
abstract = {By enmeshing the digital and physical worlds, the Internet of Things is envisioned to generate a wealth of real-world data which enables new ways of creating value based on data. Organizations and researchers in the Information Systems community have already begun to transform existing approaches by applying concepts such as Nonownership Business Models that use data as the new main resource to create value. However, as our critical literature review shows, there has been limited attention to the actual characteristics and challenges of data supply in the IoT. Based on the critical review, two distinct themes were identified regarding the conceptualization of data and Things. Data is conceptualized either as Shallow Data or as Provenance Data, whereas things are conceptualized in terms of Things as Sensors or Things as Agents. Based on these themes, two research implications are developed. Firstly, things are more than sensors, they inherit agency and intention from their respective owners. Secondly, the provenance of data is essential. Data is considered a resource, quality control, assessment of legality are essential to securely rely on data to create value.}
}
@article{YIN2021103579,
title = {Development of cultural tourism platform based on FPGA and convolutional neural network},
journal = {Microprocessors and Microsystems},
volume = {80},
pages = {103579},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103579},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120307298},
author = {Xinzhe Yin and Jinghua Li},
keywords = {Data mining, Field programmable gate array (FPGA) using xilinx, Predictive Modeling, Association Analysis},
abstract = {Data mining can be described as a typical analysis of large datasets to investigate early unknown types, styles, and interpersonal relationships to generate the right decision information. It improves their markets and today to maintain control over whether these companies are forced into the data mining tools and technologies they use to develop and manage tourism products and services in the market. It is falling out of the favorable situation of the travel and tourism industry. Objective work is to provide and display its application in data mining and tourism. Advances in mobile technology provide an opportunity to obtain real-time information of travelers, such as time and space behavior, at the destination they visit. This study analyzed a large-scale mobile phone data set to capture the mobile phone traces of international tourists who visited South Korea. We adopt the trajectory data mining method to understand tourism activities’ spatial structure in three different destinations. The research reveals tourist destinations and multiple “hot spots” (or popular areas) that interact spatially in these places through spatial cluster analysis and sequential pattern mining. Therefore, this article provides the planning of spatial model destinations to integrate important tourism influences, which is based on tourism design. The proposed system is modelled in Field Programmable Gate Array (FPGA) using Xilinx software.}
}
@incollection{CAVALIERE2021479,
title = {Chapter 23 - Molecular Docking: A Contemporary Story About Food Safety},
editor = {Mohane S. Coumar},
booktitle = {Molecular Docking for Computer-Aided Drug Design},
publisher = {Academic Press},
pages = {479-492},
year = {2021},
isbn = {978-0-12-822312-3},
doi = {https://doi.org/10.1016/B978-0-12-822312-3.00025-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128223123000254},
author = {Francesca Cavaliere and Giulia Spaggiari and Pietro Cozzini},
keywords = {Big data, Consensus scoring, Database, Food safety, In silico methods, Molecular docking, Virtual screening},
abstract = {The application of computational methods (repository or database design, screening, and molecular docking) in food safety is a relatively recent challenge. Docking/scoring techniques could be applied to a wide range of food safety problems. An important milestone for screening/docking approaches is the availability of a three-dimensional database to collect the huge amount of food contact chemicals to make possible testing these compounds otherwise unfeasible with traditional in vitro tests. In silico applications could be applied to predict the interaction between food contact chemicals and different receptors/targets involved in human diseases and/or to decipher their mechanism of binding. Another important purpose is the design of chemosensors for mycotoxins detection. The use of docking techniques as an alternative to animal tests is an emerging field and we will illustrate these concepts using recently published cases.}
}
@article{JAVAID2021209,
title = {Internet of Things (IoT) enabled healthcare helps to take the challenges of COVID-19 Pandemic},
journal = {Journal of Oral Biology and Craniofacial Research},
volume = {11},
number = {2},
pages = {209-214},
year = {2021},
issn = {2212-4268},
doi = {https://doi.org/10.1016/j.jobcr.2021.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S2212426821000154},
author = {Mohd Javaid and Ibrahim Haleem Khan},
keywords = {Internet of things (IoT), COVID-19, Information technology applications, Healthcare, Smart hospital},
abstract = {Background/objectives
The Internet of Things (IoT) can create disruptive innovation in healthcare. Thus, during COVID-19 Pandemic, there is a need to study different applications of IoT enabled healthcare. For this, a brief study is required for research directions.
Methods
Research papers on IoT in healthcare and COVID-19 Pandemic are studied to identify this technology’s capabilities. This literature-based study may guide professionals in envisaging solutions to related problems and fighting against the COVID-19 type pandemic.
Results
Briefly studied the significant achievements of IoT with the help of a process chart. Then identifies seven major technologies of IoT that seem helpful for healthcare during COVID-19 Pandemic. Finally, the study identifies sixteen basic IoT applications for the medical field during the COVID-19 Pandemic with a brief description of them.
Conclusions
In the current scenario, advanced information technologies have opened a new door to innovation in our daily lives. Out of these information technologies, the Internet of Things is an emerging technology that provides enhancement and better solutions in the medical field, like proper medical record-keeping, sampling, integration of devices, and causes of diseases. IoT’s sensor-based technology provides an excellent capability to reduce the risk of surgery during complicated cases and helpful for COVID-19 type pandemic. In the medical field, IoT’s focus is to help perform the treatment of different COVID-19 cases precisely. It makes the surgeon job easier by minimising risks and increasing the overall performance. By using this technology, doctors can easily detect changes in critical parameters of the COVID-19 patient. This information-based service opens up new healthcare opportunities as it moves towards the best way of an information system to adapt world-class results as it enables improvement of treatment systems in the hospital. Medical students can now be better trained for disease detection and well guided for the future course of action. IoT’s proper usage can help correctly resolve different medical challenges like speed, price, and complexity. It can easily be customised to monitor calorific intake and treatment like asthma, diabetes, and arthritis of the COVID-19 patient. This digitally controlled health management system can improve the overall performance of healthcare during COVID-19 pandemic days.}
}
@article{WU2021110,
title = {Transcriptional regulation and functional analysis of Nicotiana tabacum under salt and ABA stress},
journal = {Biochemical and Biophysical Research Communications},
volume = {570},
pages = {110-116},
year = {2021},
issn = {0006-291X},
doi = {https://doi.org/10.1016/j.bbrc.2021.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0006291X21010366},
author = {Hui Wu and Huayang Li and Wenhui Zhang and Heng Tang and Long Yang},
keywords = {Tobacco, Transcriptome, MAPK, Soil salinization mechanism},
abstract = {Soil salinization is an important factor that restricts crop quality and yield and causes an enormous toll to human beings. Salt stress and abscisic acid (ABA) stress will occur in the process of soil salinization. In this study, transcriptome sequencing of tobacco leaves under salt and ABA stress in order to further study the resistance mechanism of tobacco. Compared with controlled groups, 1654 and 3306 DEGs were obtained in salt and ABA stress, respectively. The genes function enrichment analysis showed that the up-regulated genes in salt stress were mainly concentrated in transcription factor WRKY family and PAR1 resistance gene family, while the up-regulated genes were mainly concentrated on bHLH transcription factor, Kunitz-type protease inhibitor, dehydrin (Xero1) gene and CAT (Catalase) family protein genes in ABA stress. Tobacco MAPK cascade triggered stress response through up-regulation of gene expression in signal transduction. The expression products of these up-regulated genes can improve the abiotic stress resistance of plants. These results have an important implication for further understanding the mechanism of salinity tolerance in plants.}
}
@article{PEI2021207,
title = {GIScience and remote sensing in natural resource and environmental research: Status quo and future perspectives},
journal = {Geography and Sustainability},
volume = {2},
number = {3},
pages = {207-215},
year = {2021},
issn = {2666-6839},
doi = {https://doi.org/10.1016/j.geosus.2021.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666683921000389},
author = {Tao Pei and Jun Xu and Yu Liu and Xin Huang and Liqiang Zhang and Weihua Dong and Chengzhi Qin and Ci Song and Jianya Gong and Chenghu Zhou},
keywords = {Natural resource, Environmental science, GIScience, Remote sensing, Information technology},
abstract = {Geographic information science (GIScience) and remote sensing have long provided essential data and methodological support for natural resource challenges and environmental problems research. With increasing advances in information technology, natural resource and environmental science research faces the dual challenges of data and computational intensiveness. Therefore, the role of remote sensing and GIScience in the fields of natural resources and environmental science in this new information era is a key concern of researchers. This study clarifies the definition and frameworks of these two disciplines and discusses their role in natural resource and environmental research. GIScience is the discipline that studies the abstract and formal expressions of the basic concepts and laws of geography, and its research framework mainly consists of geo-modeling, geo-analysis, and geo-computation. Remote sensing is a comprehensive technology that deals with the mechanisms of human effects on the natural ecological environment system by observing the earth surface system. Its main areas include sensors and platforms, information processing and interpretation, and natural resource and environmental applications. GIScience and remote sensing provide data and methodological support for resource and environmental science research. They play essential roles in promoting the development of resource and environmental science and other related technologies. This paper provides forecasts of ten future directions for GIScience and eight future directions for remote sensing, which aim to solve issues related to natural resources and the environment.}
}
@article{ZIEGENBEIN2021869,
title = {Data-based quality analysis in machining production: Influence of data pre-processing on the results of machine learning models},
journal = {Procedia CIRP},
volume = {104},
pages = {869-874},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.146},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010441},
author = {Amina Ziegenbein and Joachim Metternich},
keywords = {Machine Learning, Part Quality, Drilling},
abstract = {Quality assurance as a non-value-adding process is constantly reviewed for cost optimisation and potential savings. In the pursuit of utilising advanced data analysis and machine learning methods to improve efficiency of quality assurance in machining processes there are several influencing factors severely impacting the performance and hence the value of said methods. Especially data preparation is a time consuming task requiring both domain and data expert knowledge and yielding various options for data preparation. In this paper, the impact of different input data sets for predicting part quality in a drilling process is investigated, using machine control data.}
}
@article{KALUZNY2021970,
title = {Data analytics in military human performance: Getting in the game: Summary of a keynote address},
journal = {Journal of Science and Medicine in Sport},
volume = {24},
number = {10},
pages = {970-974},
year = {2021},
issn = {1440-2440},
doi = {https://doi.org/10.1016/j.jsams.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1440244021000992},
author = {Bohdan L. Kaluzny},
keywords = {Data analytics, Operations research, Military human performance},
abstract = {Objectives
The rise of data analytics has been not only central to the digital transformation of many industries and governments, but is now ubiquitous in daily life. But what is it? Researchers in military human performance may very well ask themselves: What is new? After all, aren't they already collecting, analysing, interpreting, and presenting data? Do they need to adapt?
Discussion
Defence and security have often been at the forefront of new technologies, but has lagged other industries with respect to data analytics. Sports science is one of the industries that are on the leading edge and this presents an opportunity that researchers in military human performance must seize.
Conclusions
Researchers must embrace data analytics and seek opportunities to ‘operationalize’ their research via data science: responsible analytics respecting scientific development supporting decision making at the necessary speed of relevance.}
}
@article{ZHANG2021119139,
title = {Nondestructive evaluation of soluble solids content in tomato with different stage by using Vis/NIR technology and multivariate algorithms},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {248},
pages = {119139},
year = {2021},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2020.119139},
url = {https://www.sciencedirect.com/science/article/pii/S1386142520311185},
author = {Dongyan Zhang and Yi Yang and Gao Chen and Xi Tian and Zheli Wang and Shuxiang Fan and Zhenghua Xin},
keywords = {Vis/NIR, Soluble solids content, Tomato, PLS, LS-SVM, Effective wavelength},
abstract = {In this study Vis/NIR spectroscopy was applied to evaluate soluble solids content (SSC) of tomato. A total of 168 tomato samples with five different maturity stages, were measured by two developed systems with the wavelength ranges of 500–930 nm and 900–1400 nm, respectively. The raw spectral data were pre-processed by first derivative and standard normal variate (SNV), respectively, and then the effective wavelengths were selected using competitive adaptive reweighted sampling (CARS) and random frog (RF). Partial least squares (PLS) and least square-support vector machines (LS-SVM) were employed to build the prediction models to evaluate SSC in tomatoes. The prediction results revealed that the best performance was obtained using the PLS model with the optimal wavelengths selected by CARS in the range of 900–1400 nm (Rp = 0.820 and RMSEP = 0.207 °Brix). Meanwhile, this best model yielded desirable results with Rp and RMSEP of 0.830 and 0.316 °Brix, respectively, in 60 samples of the independent set. The method proposed from this study can provide an effective and quick way to predict SSC in tomato.}
}
@article{ADAMYAN20211774,
title = {Gene Expression Signature of Endometrial Samples from Women with and without Endometriosis},
journal = {Journal of Minimally Invasive Gynecology},
volume = {28},
number = {10},
pages = {1774-1785},
year = {2021},
issn = {1553-4650},
doi = {https://doi.org/10.1016/j.jmig.2021.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1553465021001709},
author = {Leila Adamyan and Yana Aznaurova and Assia Stepanian and Daniil Nikitin and Andrew Garazha and Maria Suntsova and Maxim Sorokin and Anton Buzdin},
keywords = {Endometriosis, Molecular diagnostics, RNA sequencing, Gene expression signature, Big data in clinical medicine},
abstract = {ABSTRACT
Study Objective
To develop a prototype of a complex gene expression biomarker for the diagnosis of endometriosis on the basis of differences between the molecular signatures of the endometrium from women with and without endometriosis.
Design
Prospective observational cohort study. Evidence obtained from a well-designed, controlled trial without randomization.
Setting
Department of reproductive medicine and surgery, A.I. Evdokimov Moscow State University of Medicine and Dentistry.
Patients
A total of 33 women (aged 32–38 years) were included in this study. Patients with and without endometriosis were divided into 2 separate groups. The group composed of patients with endometriosis included 19 living patients with endometriosis who underwent laparoscopic excision of endometriosis. The control group included 6 living patients who underwent laparoscopic excision of incompetent uterine scar after cesarean section, with both surgically and histologically confirmed absence of endometriosis and adenomyosis. An additional control/verification group included various previously RNA-sequencing–profiled tissue samples (endocervix, ovarian surface epithelium) of 8 randomly selected healthy female cadaveric donors aged 32 to 38 years. The exclusion criteria for all patients were hormone therapy and any intrauterine device use for more than 1 year preceding surgery, as well as absence of other diseases of the uterus, fallopian tubes, and ovaries.
Interventions
Laparoscopic excision of endometriotic foci and hysteroscopy with endometrial sampling were performed. The cadaveric tissue samples included endocervix and ovarian surface epithelium. Endometrial sampling was obtained from the women in the control group. RNA sequencing was performed using Illumina HiSeq 3000 equipment (Illumina, Inc., San Diego, CA) for single-end sequencing. Unique bioinformatics algorithms were developed and validated using experimental and public gene expression datasets.
Measurements and Main Results
We generated a characteristic signature of 5 genes downregulated in the endometrium and endometriotic tissue of the patients with endometriosis, selected after comparison with the endometrium of the women without endometriosis. This gene signature showed a capacity for nearly perfect separation of all 52 analyzed tissue samples of the patients with endometriosis (endometrial as well as endometriotic samples) from the 14 tissue samples of both living and cadaveric donors without endometriosis (area under the curve = 0.982, Matthews correlation coefficient = 0.832).
Conclusion
The gene signature of the endometrium identified in this study may potentially serve as a nonsurgical diagnostic method for endometriosis detection. Our data also suggest that the statistical method of 5-fold cross-validation of differential gene expression analysis can be used to generate robust gene signatures using real-world clinical data.}
}
@article{SCANNAPIECO2021100393,
title = {How to be responsible in all the steps of a data science pipeline: The case of the Italian public sector},
journal = {Patterns},
volume = {2},
number = {12},
pages = {100393},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100393},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921002609},
author = {Monica Scannapieco and Antonino Virgillito},
abstract = {The paper highlights how each step of a data science pipeline can be performed in a “responsible” way, taking into account privacy, ethics, and quality issues. Several examples from the Italian public sector contribute to clarifying how data collections and data analyses can be carried out under a responsible view.}
}
@article{UREN2023102588,
title = {Technology readiness and the organizational journey towards AI adoption: An empirical study},
journal = {International Journal of Information Management},
volume = {68},
pages = {102588},
year = {2023},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2022.102588},
url = {https://www.sciencedirect.com/science/article/pii/S0268401222001220},
author = {Victoria Uren and John S. Edwards},
keywords = {Artificial Intelligence, Data, IT-business alignment, System development, Technology adoption},
abstract = {Artificial Intelligence (AI) is viewed as having potential for significant economic and social impact. However, its history of boom and bust cycles can make potential adopters wary. A cross-sectional, qualitative study was carried out, with a purposive sample of AI experts from research, development and business functions, to gain a deeper understanding of the adoption process. Technology Readiness Levels were used as a benchmark against which the experts could align their experiences. A model of AI adoption is proposed which embeds an extended version of the People, Processes, Technology lens, incorporating Data. The model suggests that people, process and data readiness are required in addition to technology readiness to achieve long term operational success with AI. The findings further indicate that innovative organizations should build bridges between technical and business functions.}
}
@incollection{BHUNIA202327,
title = {Chapter 3 - GI Science application for groundwater resources management and decision support},
editor = {Pravat Shit and Gouri Bhunia and Partha Adhikary},
booktitle = {Case Studies in Geospatial Applications to Groundwater Resources},
publisher = {Elsevier},
pages = {27-38},
year = {2023},
isbn = {978-0-323-99963-2},
doi = {https://doi.org/10.1016/B978-0-323-99963-2.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323999632000146},
author = {Gouri Sankar Bhunia and Pravat Kumar Shit and Soumen Brahma},
keywords = {GIS, Machine learning, Big data, Expert knowledge, Data imbalance},
abstract = {In the context of sustainable groundwater development, the current paper focuses on aspects concerning the role of GISciences approaches and new technological advances for geoenvironmental analysis. A "data imbalance" and "Professional Expert" effect are also highlighted in the paper, which is a core concern in the long-term analysis of environmental progression and geosphere–anthroposphere interrelations. GIScience examines the role of technology in retrieving and analyzing groundwater data, as well as the challenges encountered by the increasing complexities and heterogeneity of data sources.}
}
@article{JIN2021102059,
title = {Bus network assisted drone scheduling for sustainable charging of wireless rechargeable sensor network},
journal = {Journal of Systems Architecture},
volume = {116},
pages = {102059},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102059},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121000497},
author = {Yong Jin and Jia Xu and Sixu Wu and Lijie Xu and Dejun Yang and Kaijian Xia},
keywords = {Wireless rechargeable sensor network, Bus network, Drone scheduling, Traveling salesman path problem, Submodular orienteering problem},
abstract = {Wireless Rechargeable Sensor Network (WRSN) is largely used in monitoring of environment and traffic, video surveillance and medical care, etc., and helps to improve the quality of urban life. However, it is challenging to provide the sustainable energy for sensors deployed in buildings, soil or other places, where it is hard to harvest the energy from environment. To address this issue, we design a new wireless charging system, which levers the bus network assisted drone in urban areas. We formulate the drone scheduling problem based on this new wireless charging system to minimize the total time cost of drone subject to all sensors can be charged under the energy constraint of drone. Then, we propose an approximation algorithm DSA for the energy tightened drone scheduling problem. To make the tasks of WRSN sustainable, we further formulate the drone scheduling problem with deadlines of sensors, and present the approximation algorithm DDSA to find the drone schedule with the maximal number of sensors charged by the drone before deadlines. Through the extensive simulations, we demonstrate that DSA can reduce the total time cost by 84.83% compared with Greedy Replenished Energy algorithm, and uses at most 5.98 times of the total time cost of optimal solution on average. Then, we also demonstrate that DDSA can increase the survival rate of sensors by 51.95% compared with Deadline Greedy Replenished Energy algorithm, and can obtain 77.54% survival rate of optimal solution on average.}
}
@article{SHALA2021104603,
title = {Completion of electronic nursing documentation of inpatient admission assessment: Insights from Australian metropolitan hospitals},
journal = {International Journal of Medical Informatics},
volume = {156},
pages = {104603},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104603},
url = {https://www.sciencedirect.com/science/article/pii/S138650562100229X},
author = {Danielle Ritz Shala and Aaron Jones and Greg Fairbrother and Duong {Thuy Tran}},
keywords = {Electronic clinical documentation, Nursing admission, eMR data, Health informatics, Data science, Nursing informatics},
abstract = {Introduction
Electronic nursing documentation is an essential aspect of inpatient care and multidisciplinary communication. Analysing data in electronic medical record (eMR) systems can assist in understanding clinical workflows, improving care quality, and promoting efficiency in the healthcare system. This study aims to assess timeliness of completion of an electronic nursing admission assessment form and identify patient and facility factors associated with form completion in three metropolitan hospitals.
Materials and Methods
Records of 37,512 adult inpatient admissions (November 2018-November 2019) were extracted from the hospitals’ eMR system. A dichotomous variable descriptive of completion of the nursing assessment form (Yes/No) was created. Timeliness of form completion was calculated as the interval between date and time of admission and form completion. Univariate and multivariate multilevel logistic regression were used to identify factors associated with form completion.
Results
An admission assessment form was completed for 78.4% (n = 29,421) of inpatient admissions. Of those, 78% (n = 22,953) were completed within the first 24 h of admission, 13.3% (n = 3,910) between 24 and 72 h from admission, and 8.7% (n = 2,558) beyond 72 h from admission. Patient length of hospital stay, admission time, and admitting unit’s nursing hours per patient day were associated with form completion. Patient gender, age, and admitting unit type were not associated with form completion.
Discussion
Form completion rate was high, though more emphasis needs to be placed on the importance of timely completion to allow for adequate patient care planning. Staff education, qualitative understanding of delayed form completion, and streamlined guidelines on nursing admission and eMR use are recommended.}
}
@article{FENZA2021107366,
title = {Data set quality in Machine Learning: Consistency measure based on Group Decision Making},
journal = {Applied Soft Computing},
volume = {106},
pages = {107366},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107366},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621002891},
author = {Giuseppe Fenza and Mariacristina Gallo and Vincenzo Loia and Francesco Orciuoli and Enrique Herrera-Viedma},
keywords = {Consistency, Learning to Rank, Group Decision Making, Training data consistency, Data set quality},
abstract = {Performance of Machine Learning models heavily depends on the quality of the training dataset. Among others, the quality of training data relies on the consistency of the labels assigned to similar items. Indeed, the labels should be coherently assigned (or collected) by avoiding inconsistencies for increasing the performance of the machine learning model. This study focuses on evaluating training data consistency for machine learning algorithms dealing with ranking problems, i.e., the Learning to Rank methods (LTR). This work defines a training data consistency measure based on the consensus value introduced in Group Decision Making. It investigates the statistical relationship between the proposed consistency measure and the performance of a deep neural network implementing an LTR method. This measure could drive data filtering at the training stage and guide model update decisions. Experimentation reveals a strong correlation between the proposed consistency measure and the performance of the model.}
}
@article{LI2023118543,
title = {Time series clustering based on complex network with synchronous matching states},
journal = {Expert Systems with Applications},
volume = {211},
pages = {118543},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118543},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422016153},
author = {Hailin Li and Zechen Liu and Xiaoji Wan},
keywords = {Time series clustering, Complex network, Synchronous matching, Data mining},
abstract = {Due to the extensive existence of time series in various fields, more and more research on time series data mining, especially time series clustering, has been done in recent years. Clustering technology can extract valuable information and potential patterns from time series data. This paper proposes a time series Clustering method based on Synchronous matching of Complex networks (CSC). This method uses density peak clustering algorithm to identify the state of each time point and obtains the state sequence according to the timeline of the original time series. State sequences is a new method to represent time series. By comparing two state sequences synchronously, the length of state sequence with step is calculated and the similarity is presented, which forms a new method to calculate the similarity of time series. Based on the obtained time series similarity, the relationship network of time series is constructed. Simultaneously, the community discovery technology is applied to cluster the relationship network and further achieve the complete time series clustering. The detailed process and simulation experiments of CSC method are given. Experimental results on different datasets show that CSC method is superior to other traditional time series clustering methods.}
}
@article{LI2021104262,
title = {Robust CSEM data processing by unsupervised machine learning},
journal = {Journal of Applied Geophysics},
volume = {186},
pages = {104262},
year = {2021},
issn = {0926-9851},
doi = {https://doi.org/10.1016/j.jappgeo.2021.104262},
url = {https://www.sciencedirect.com/science/article/pii/S0926985121000094},
author = {Guang Li and Zhushi He and Juzhi Deng and Jingtian Tang and Youyao Fu and Xiaoqiong Liu and Changming Shen},
keywords = {CSEM data processing, Periodic signal de-noising, Signal-noise identification, Fuzzy -means clustering (FCM), Machine Learning, Correlation Analysis},
abstract = {The ambient noise in controlled-source electromagnetic (CSEM) data seriously affects the accuracy and reliability of the exploration result. Traditional correlation-based data selection method requires manually setting the threshold. To overcome the deficiency, we analyze the typical noises in CSEM data and find that normalized cross-correlation (NCC), absolute maximum value of the amplitude (Max), and detrend fluctuation analysis (DFA) can be used to accurately identify high-quality time series. Based on this discovery, we replace traditional manually intervention with unsupervised machine learning and propose a novel CSEM data processing method. We applied the newly proposed method to synthetic and measured CSEM data to verify the feasibility and effectiveness. Experimental results demonstrate that the newly proposed method is superior to the conventional data selection method because it accurately selects the best data fragments from noisy data automatically. The newly proposed method requires no human intervention which makes the results obtained free of subjective distortion caused by the operator.}
}
@article{KARTHIK2023104176,
title = {Convolution neural networks for optical coherence tomography (OCT) image classification},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104176},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104176},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422006309},
author = {Karri Karthik and Manjunatha Mahadevappa},
keywords = {Image classification, Optical coherence tomography (OCT), Convolution neural networks, Retinal diseases, Cross activation},
abstract = {Optical coherence tomography (OCT) is an imaging modality used to obtain a cross-sectional image of the retina for retinal disease diagnosis. Modern diagnosis systems use Convolutional Neural Networks. Our model increases the contrast in the residual connection, so high contrast regions, such as the retinal layers, are prominent in feature maps. Our model increases the contrast of the derivatives to generate sharper feature maps. We replaced the residual connection in standard ResNet architectures with our design. The proposed activation function retains negative weights and reinforces smaller gradients. We have used two OCT datasets with four and eight classes of diseases, respectively. We performed graphical analysis using Precision–Recall curves. We used accuracy, precision, recall, and F1 score for evaluation. In our laboratory conditions, We have successfully increased the classification accuracy with our proposed design. The gain in accuracy is limited, i.e. <1% when the initial accuracy is more than 98%, and 1.6% when the initial accuracy is lower. In confusion matrices, we observed the maximum performance increase when the number of samples is less in one class, which will be helpful if data is imbalanced. The retinal boundary is enhanced, with the background (the region outside the retinal layers) suppressed but not entirely removed. In ablation studies, We observed an average accuracy loss of 0.875% with OCT-C4 data and 1.39% for OCT-C8 data. The p-values from Wilcoxon signed-rank test range from 1.65 × 10−6 to 0.025, and 0.51 for ResNet50 with the OCT-C8 dataset.}
}
@incollection{CROMPTON202193,
title = {5 - Data management from the DCS to the historian and HMI},
editor = {Patrick Bangert},
booktitle = {Machine Learning and Data Science in the Power Generation Industry},
publisher = {Elsevier},
pages = {93-122},
year = {2021},
isbn = {978-0-12-819742-4},
doi = {https://doi.org/10.1016/B978-0-12-819742-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197424000056},
author = {Jim Crompton},
keywords = {Machine learning, Value chain optimization, Incremental value chain economics, Digital platform, Asset utilization, Grow new markets, Integrated strategies, Innovation},
abstract = {In today's global and fast changing business environment, the urgency with multinational companies to find readily implementable digital solutions has increased significantly as the underlying science yielding operational and business improvements has matured over the past decade. Companies lacking innovative ways to extract incremental value from existing and new business ventures will be left behind. Manufacturing plants, in general, manage billion dollars’ worth of raw and finished products daily across the United States and globally. The quantities of data analyzed in a product lifecycle, capturing cost to produce, logistics, working capital, and other ancillary costs are massive and difficult to both consolidate and integrate. Many production companies still rely heavily on manual processes to evaluate opportunities and economics.}
}
@article{DOMINIEK2021102988,
title = {Exploring variation in the performance of planned birth: A mixed method study},
journal = {Midwifery},
volume = {98},
pages = {102988},
year = {2021},
issn = {0266-6138},
doi = {https://doi.org/10.1016/j.midw.2021.102988},
url = {https://www.sciencedirect.com/science/article/pii/S026661382100067X},
author = {Coates Dominiek and Henry Amanda and Chambers Georgina and Paul Repon and Makris Angela and Clerke Teena and Natasha Donnolley},
keywords = {Unwarranted variation, Induction of labour, Planned caesarean section, Clinician perspectives},
abstract = {Objective: Variation in practice in relation to indications and timing for both induction of labour (IOL) and planned caesarean section (CS) clearly exists. However, the extent of this variation, and how this variation is explained by clinicians remains unclear. The aim of this study was to map the variation in IOL and planned CS at eight Australian hospitals, and understand why variation occurs from the perspective of clinicians at these hospitals. Our ultimate aim was to identify opportunities for improvement as evidenced by hospital data, clinician experiences, and feedback. Design: A two-phased mixed method study using sequential explanatory study design. The first phase consisted of an analysis of routinely collected patient data to map variation between hospitals. The second phase consisted of focus groups with clinicians to gain their perspectives on the reasons for variation. Setting and Participants: Patient data consisted of routine data from 19,073 women giving birth at eight Sydney hospitals between November 2017 and October 2018. Focus groups were attended by a total of 61 medical staff and 121 midwives. Results: Hospital data analysis found substantial variation, before and after adjustment for case-mix, in rates of both IOL (adjusted rates 27.6%–42%) and planned CS (adjusted rate 15.4%–22.6%). Planned CS by gestation also showed variation, although after restricting analysis to term (≥37 weeks gestation) births, variation was reduced. At focus groups, five main themes explaining variation emerged: local guidelines, policies and procedures (inconsistency and ambiguity); uncertainty of the evidence/what is best practice (contradictory research and different interpretations of evidence); clinician preferences, beliefs and values; the culture of the unit; and organisational influences (access to specialised clinics, theatre time). Key conclusions: Considerable variation in IOL and planned CS, even after case-mix adjustment, was found in this sample of Australian hospitals. Engagement with hospital clinicians identified likely sources of this variation and enabled clinicians at each hospital to consider appropriate local responses to address variation, such as more detailed review of their planned birth cases. Implications for practice: At a macro level, measures to reduce unwarranted variation should initially focus on consistent national guidelines, while supporting equitable access to operating theatres for optimal CS timing, and shared decision-making training to reduce influence of clinician preference.}
}
@article{ABUSALIH2021103076,
title = {Domain-specific knowledge graphs: A survey},
journal = {Journal of Network and Computer Applications},
volume = {185},
pages = {103076},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103076},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521000990},
author = {Bilal Abu-Salih},
keywords = {Knowledge graph, Domain-specific knowledge graph, Knowledge graph construction, Knowledge graph embeddings, Knowledge graph evaluation, Domain ontology, Survey},
abstract = {Knowledge Graphs (KGs) have made a qualitative leap and effected a real revolution in knowledge representation. This is leveraged by the underlying structure of the KG which underpins a better comprehension, reasoning and interpretation of knowledge for both human and machine. Therefore, KGs continue to be used as the main means of tackling a plethora of real-life problems in various domains. However, there is no consensus in regard to a plausible and inclusive definition of a domain-specific KG. Further, in conjunction with several limitations and deficiencies, various domain-specific KG construction approaches are far from perfect. This survey is the first to offer a comprehensive definition of a domain-specific KG. Also, the paper presents a thorough review of the state-of-the-art approaches drawn from academic works relevant to seven domains of knowledge. An examination of current approaches reveals a range of limitations and deficiencies. At the same time, uncharted territories on the research map are highlighted to tackle extant issues in the literature and point to directions for future research.}
}
@article{CARETTATEIXEIRA20211097,
title = {Proposal for a health information management model based on Lean thinking},
journal = {Procedia Computer Science},
volume = {181},
pages = {1097-1104},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003550},
author = {Jéssica Cristina {Caretta Teixeira} and Filipe Andrade Bernardi and Rui Pedro Charters {Lopes Rijo} and Domingos Alves},
keywords = {Lean Healthcare, Health Information Systems, Quality Improvement},
abstract = {Although assessing quality in the health field is a prominent challenge, there is unanimity among managers that it is necessary to select appropriate assessment systems and methods to assist the administration of services and provide decision-making with the least degree of uncertainty possible. Lean, also known as lean philosophy, is a management model that has been used in the area of Health. The management of data, knowledge, and health services must be carefully performed, so that quality care can be offered. at all levels of care. In this way, when implementing Lean strategies in Information Technology, it is necessary to evaluate all its processes within the institution to eliminate waste, structure functions within the applied methodology and measure improvement at all levels of the organization. Thus, the general objective of this article is that of a study that leads to a health information management model based on Lean thinking in the municipality of Ituverava. The highly heterogeneous, and sometimes ambiguous, nature of the medical language and its constant evolution, the high amount of data generated constantly by the automation of processes and the emergence of new technologies constitute the foundation for the inevitable computerization of health to promote the production and management of knowledge. Adopting Lean thinking in health may seem a challenge initially for managers and team members, but as the first results begin to appear, profound and concrete changes are visible for positive transformation for improvements in the quality of the service provided, until the culture can be learned completely in order to have the perfect care.}
}
@article{SUN202128,
title = {Laboratory information management system for biosafety laboratory: Safety and efficiency},
journal = {Journal of Biosafety and Biosecurity},
volume = {3},
number = {1},
pages = {28-34},
year = {2021},
issn = {2588-9338},
doi = {https://doi.org/10.1016/j.jobb.2021.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2588933821000042},
author = {Dingzhong Sun and Linhuan Wu and Guomei Fan},
keywords = {Laboratory information management system, Biosafety, Biological research laboratory, Laboratory safety, Workflow management},
abstract = {Laboratory information management system (LIMS) has been widely used to facilitate laboratory activities. However, the current LIMSs do not contain functions to improve the safety of laboratory work, which is the major concern of biosafety laboratories (BSLs). With tons of biosafety information that need to be managed and an increasing number of biosafety-related research projects under way, it is worthy of expanding the current framework of LIMS and building a system that is more suitable for BSL usage. Such a system should carefully trade off between the safety and efficiency of regular lab activities, allowing the laboratory staff to conduct their research as free as possible while ensuring their and the environment’s safety. In order to achieve this goal, the information on the research contents, laboratory personnel, experimental materials and experimental equipment need to be well collected and fully utilized by a centralized system and its databases.}
}
@article{MULLAHY2021e103,
title = {Embracing Uncertainty: The Value of Partial Identification in Public Health and Clinical Research},
journal = {American Journal of Preventive Medicine},
volume = {61},
number = {2},
pages = {e103-e108},
year = {2021},
issn = {0749-3797},
doi = {https://doi.org/10.1016/j.amepre.2021.01.041},
url = {https://www.sciencedirect.com/science/article/pii/S0749379721001707},
author = {John Mullahy and Atheendar Venkataramani and Daniel L. Millimet and Charles F. Manski},
abstract = {Introduction
This paper describes the methodology of partial identification and its applicability to empirical research in preventive medicine and public health.
Methods
The authors summarize findings from the methodologic literature on partial identification. The analysis was conducted in 2020–2021.
Results
The applicability of partial identification methods is demonstrated using 3 empirical examples drawn from published literature.
Conclusions
Partial identification methods are likely to be of considerable interest to clinicians and others engaged in preventive medicine and public health research.}
}
@article{ZEISER2021597,
title = {Requirements towards optimizing analytics in industrial processes},
journal = {Procedia Computer Science},
volume = {184},
pages = {597-605},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.074},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007079},
author = {Alexander Zeiser and Bas van Stein and Thomas Bäck},
keywords = {Industry 4.0, Predictive Modelling, Optimizing analytics, Information fusion, Machine Learning, Industrial processes},
abstract = {Modern production systems are composed of complex manufacturing processes with highly technology specific cause-effect relationships. Developments in sensor technology and computational science allow for data-driven decision making that facilitate effcient and objective production management. However, process data may only be beneficial if it is enriched with meta information and process expertise, reduced to relevant information and modelling results interpreted correctly. The importance of data integration in the heterogeneous industrial environment rises at the same momentum as new metrology techniques are deployed. In this paper, we focus on optimizing analytics, containing data-driven decision making for predictive quality and maintenance. We summarize key requirements for data analytics and machine learning application in industrial processes. With a use case from automotive component manufacturing we characterize industrial production, categorize process data and put requirements in context to a real-world example.}
}
@article{TRUANT2021121173,
title = {Digitalisation boosts company performance: an overview of Italian listed companies},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121173},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121173},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006065},
author = {Elisa Truant and Laura Broccardo and Léo-Paul Dana},
keywords = {Digitalization, Company performance, Listed company, Benefit, Obstacle},
abstract = {Digitalisation has become embedded in products and services, and it increasingly supports corporate business processes. However, few empirical studies have analysed the state of digitalisation and its implementation within companies, and the extant literature has painted an inconsistent picture concerning the effects of digitalisation. This survey-based study explores the diffusion of digitalisation, the advantages and difficulties in the practical transition to digitalisation, and its impact on performance. The sample includes Italian listed companies across diverse industries. The results highlight the still embryonic adoption of digital tools to support daily company operations; however, the impacts of digitalisation on company performance are noticeable. This research contributes to the literature on digitalisation and performance, breaks new ground by focusing on listed companies, and has implications for management investment in digitalisation for value creation.}
}
@article{CHAKRABORTY2021106410,
title = {Machine learning based digital twin for dynamical systems with multiple time-scales},
journal = {Computers & Structures},
volume = {243},
pages = {106410},
year = {2021},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2020.106410},
url = {https://www.sciencedirect.com/science/article/pii/S0045794920302133},
author = {S. Chakraborty and S. Adhikari},
keywords = {Digital twin, Multi-timescale dynamics, Mixture of experts, Gaussian process, Frequency},
abstract = {Digital twin technology has a huge potential for widespread applications in different industrial sectors such as infrastructure, aerospace, and automotive. However, practical adoptions of this technology have been slower, mainly due to a lack of application-specific details. Here we focus on a digital twin framework for linear single-degree-of-freedom structural dynamic systems evolving in two different operational time scales in addition to its intrinsic dynamic time-scale. Our approach strategically separates into two components – (a) a physics-based nominal model for data processing and response predictions, and (b) a data-driven machine learning model for the time-evolution of the system parameters. The physics-based nominal model is system-specific and selected based on the problem under consideration. On the other hand, the data-driven machine learning model is generic. For tracking the multi-timescale evolution of the system parameters, we propose to exploit a mixture of experts as the data-driven model. Within the mixture of experts model, Gaussian Process (GP) is used as the expert model. The primary idea is to let each expert track the evolution of the system parameters at a single time-scale. For learning the hyperparameters of the ‘mixture of experts using GP’, an efficient framework that exploits expectation-maximization and sequential Monte Carlo sampler is used. Performance of the digital twin is illustrated on a multi-timescale dynamical system with stiffness and/or mass variations. The digital twin is found to be robust and yields reasonably accurate results. One exciting feature of the proposed digital twin is its capability to provide reasonable predictions at future time-steps. Aspects related to the data quality and data quantity are also investigated.}
}
@article{JI2021365,
title = {Blog text quality assessment using a 3D CNN-based statistical framework},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {365-370},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330028},
author = {Fang Ji and Heqing Zhang and Zijiang Zhu and Weihuang Dai},
keywords = {Blog data, VQA, 3D CNN, Video-related text, Bi-LSTM, Text quality evaluation},
abstract = {Aiming at the problem that blog texts are the streaming data captured by different acquisition modality, each kind of which has its particular quality evaluation mode, this paper proposes a text quality evaluation (TQA) model based on 3D CNN correlated with blog text data. In order to achieve accurate TQA value, the model adopted a Bi-LSTM-based architecture to process video-related blog text as auxiliary part to provide additional information for our TQA architecture. First, the auxiliary part constructs feature vector for each video-related text by the model originating from Bi-LSTM and Seq2Seq. Then, the feature vector was feed to a well-trained decoder to reconstruct the original input data. Then, the feature vector complied with the blog textual data are inputted into end-to-end TQA modal based on the 3D CNN straightly. Comprehensive experimental results on the blog text/video dataset from the well-known truism website “http://www.mafengwo.cn/” have shown that the proposed model reflects the subjective quality of online texts more accurately, and has better overall blog TQA assessment performance than the other state-of-the-art non-reference methods.}
}
@article{ZHANG2021116452,
title = {A review of machine learning in building load prediction},
journal = {Applied Energy},
volume = {285},
pages = {116452},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116452},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921000209},
author = {Liang Zhang and Jin Wen and Yanfei Li and Jianli Chen and Yunyang Ye and Yangyang Fu and William Livingood},
keywords = {Building energy system, Building load prediction, Building energy forecasting, Machine learning, Feature engineering, Data engineering},
abstract = {The surge of machine learning and increasing data accessibility in buildings provide great opportunities for applying machine learning to building energy system modeling and analysis. Building load prediction is one of the most critical components for many building control and analytics activities, as well as grid-interactive and energy efficiency building operation. While a large number of research papers exist on the topic of machine-learning-based building load prediction, a comprehensive review from the perspective of machine learning is missing. In this paper, we review the application of machine learning techniques in building load prediction under the organization and logic of the machine learning, which is to perform tasks T using Performance measure P and based on learning from Experience E. Firstly, we review the applications of building load prediction model (task T). Then, we review the modeling algorithms that improve machine learning performance and accuracy (performance P). Throughout the papers, we also review the literature from the data perspective for modeling (experience E), including data engineering from the sensor level to data level, pre-processing, feature extraction and selection. Finally, we conclude with a discussion of well-studied and relatively unexplored fields for future research reference. We also identify the gaps in current machine learning application and predict for future trends and development.}
}
@article{LI2021102231,
title = {Towards high-throughput microstructure simulation in compositionally complex alloys via machine learning},
journal = {Calphad},
volume = {72},
pages = {102231},
year = {2021},
issn = {0364-5916},
doi = {https://doi.org/10.1016/j.calphad.2020.102231},
url = {https://www.sciencedirect.com/science/article/pii/S0364591620304946},
author = {Yue Li and Bjørn Holmedal and Boyu Liu and Hongxiang Li and Linzhong Zhuang and Jishan Zhang and Qiang Du and Jianxin Xie},
keywords = {Materials informatics, Machine learning, High-throughput computing, Microstructure simulation, Tabulation},
abstract = {The coupling of computational thermodynamics and kinetics has been the central research theme in Integrated Computational Material Engineering (ICME). Two major bottlenecks in implementing this coupling and performing efficient ICME-guided high-throughput multi-component industrial alloys discovery or process parameters optimization, are slow responses in kinetic calculations to a given set of compositions and processing conditions and the quality of a large amount of calculated thermodynamic data. Here, we employ machine learning techniques to eliminate them, including (1) intelligent corrupt data detection and re-interpolation (i.e. data purge/cleaning) to a big tabulated thermodynamic dataset based on an unsupervised learning algorithm and (2) parameterization via artificial neural networks of the purged big thermodynamic dataset into a non-linear equation consisting of base functions and parameterization coefficients. The two techniques enable the efficient linkage of high-quality data with a previously developed microstructure model. This proposed approach not only improves the model performance by eliminating the interference of the corrupt data and stability due to the boundedness and continuity of the obtained non-linear equation but also dramatically reduces the running time and demand for computer physical memory simultaneously. The high computational robustness, efficiency, and accuracy, which are prerequisites for high-throughput computing, are verified by a series of case studies on multi-component aluminum, steel, and high-entropy alloys. The proposed data purge and parameterization methods are expected to apply to various microstructure simulation approaches or to bridging the multi-scale simulation where handling a large amount of input data is required. It is concluded that machine learning is a valuable tool in fueling the development of ICME and high throughput materials simulations.}
}
@article{DING2021100662,
title = {Towards the next generation of the LinkedGeoData project using virtual knowledge graphs},
journal = {Journal of Web Semantics},
volume = {71},
pages = {100662},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100662},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000378},
author = {Linfang Ding and Guohui Xiao and Albulen Pano and Claus Stadler and Diego Calvanese},
keywords = {LinkedGeoData, Virtual knowledge graph, GeoSPARQL, Ontop, OpenStreetMap},
abstract = {With the advancement of Semantic Technologies, large geospatial data sources have been increasingly published as Linked data on the Web. The LinkedGeoData project is one of the most prominent such projects to create a large knowledge graph from OpenStreetMap (OSM) with global coverage and interlinking of other data sources. In this paper, we report on the ongoing effort of exposing the relational database in LinkedGeoData as a SPARQL endpoint using Virtual Knowledge Graph (VKG) technology. Specifically, we present two realizations of VKGs, using the two systems Sparqlify and Ontop. In order to improve compliance with the OGC GeoSPARQL standard, we have implemented GeoSPARQL support in Ontop v4. Moreover, we have evaluated the VKG-powered LinkedGeoData in the test areas of Italy and Germany. Our experiments demonstrate that such system supports complex GeoSPARQL queries, which confirms that query answering in the VKG approach is efficient.}
}
@article{ZAPPATORE20231,
title = {Semantic models for IoT sensing to infer environment–wellness relationships},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {1-17},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003211},
author = {Marco Zappatore and Antonella Longo and Angelo Martella and Beniamino {Di Martino} and Antonio Esposito and Serena Angela Gracco},
keywords = {IoT interoperability, Semantic API, Environmental sensing, Mobile Crowd Sensing, Ontology patterns},
abstract = {Every time an Internet of Things (IoT) solution is deployed, every time a smartphone owner connects her/his wireless device to a wearable activity-tracker, every time groups of citizens use geo-mapping applications to move around the city, choosing the least crowded path, data are produced and information have to be exchanged appropriately via APIs. Even if novel added-value IoT-based applications appear on the market with increasing speed, true semantic interoperability is far from being achieved, thus limiting the large-scale exploitation, the scalability and the time-to-market of novel apps. Currently, connecting different data prosumers with multiple data sources is still hampered by the lack of standardized and sustainable solutions, especially due to the significant heterogeneity of IoT platforms. In such a landscape, ontologies come to the rescue, thanks to their formal semantics, knowledge representation formats, and shared vocabularies. In this paper we examine, from an ontological perspective, how to describe environmental sensing and wellness monitoring, two of the most popular application cases of Mobile Crowd Sensing (MCS) and IoT, respectively. To this purpose, an ontology of sensor-agnostic APIs is proposed, along with a set of MCS-dedicated ontology modules (and the supporting platform), leveraging on standard and reusable domain ontologies. Moreover, it will be shown how to properly combine the proposed ontologies in order to support complex functionalities based on inference rules addressing the environment–wellness relationships. Finally, specific semantic modeling patterns suitable for typical IoT and MCS scenarios will be discussed.}
}
@article{LIANG2021100249,
title = {Data Price Determinants Based on a Hedonic Pricing Model},
journal = {Big Data Research},
volume = {25},
pages = {100249},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100249},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000666},
author = {Ji Liang and Chunhui Yuan},
keywords = {Data market, Pricing management, Hedonic, Data pricing model},
abstract = {Data have become an emerging factor of production; the development of the data trading market is accelerating, but data pricing remains in the initial exploration stage. Based on hedonic price theory, this paper analyzes the factors influencing data prices. From the perspective of data transactions, this paper proposes construction of a data price characteristic index system from three aspects: data objects, data demanders and data suppliers. An empirical analysis is conducted based on 1010 data commodity sample data points from the Jingdong Vientiane platform. Through testing and estimation, it is considered that the hedonic price models can be fitted by a logarithmic function, taken as the function form of data hedonic price analysis. According to the analysis of the sample data sources from the data market, the number of free trials, data specification and quantity, data attention (number of data visits), data demand (sales volume), and preferential policies for businesses are negatively correlated with data price. Data evaluation satisfaction (number of collected users), request parameters (business), return parameters (business), data range, and response speed are positively correlated with data price. In addition, further analysis is made on the reason of the processing complexity variable of the data algorithm which does not conform to the expected estimation.}
}
@article{ZUIDERWIJK2021101577,
title = {Implications of the use of artificial intelligence in public governance: A systematic literature review and a research agenda},
journal = {Government Information Quarterly},
volume = {38},
number = {3},
pages = {101577},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101577},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000137},
author = {Anneke Zuiderwijk and Yu-Che Chen and Fadi Salem},
keywords = {Public governance, Artificial intelligence, Artificial intelligence for government, Public sector, Digital government, Systematic literature review, Research agenda},
abstract = {To lay the foundation for the special issue that this research article introduces, we present 1) a systematic review of existing literature on the implications of the use of Artificial Intelligence (AI) in public governance and 2) develop a research agenda. First, an assessment based on 26 articles on this topic reveals much exploratory, conceptual, qualitative, and practice-driven research in studies reflecting the increasing complexities of using AI in government – and the resulting implications, opportunities, and risks thereof for public governance. Second, based on both the literature review and the analysis of articles included in this special issue, we propose a research agenda comprising eight process-related recommendations and seven content-related recommendations. Process-wise, future research on the implications of the use of AI for public governance should move towards more public sector-focused, empirical, multidisciplinary, and explanatory research while focusing more on specific forms of AI rather than AI in general. Content-wise, our research agenda calls for the development of solid, multidisciplinary, theoretical foundations for the use of AI for public governance, as well as investigations of effective implementation, engagement, and communication plans for government strategies on AI use in the public sector. Finally, the research agenda calls for research into managing the risks of AI use in the public sector, governance modes possible for AI use in the public sector, performance and impact measurement of AI use in government, and impact evaluation of scaling-up AI usage in the public sector.}
}
@article{ZHAO2021116175,
title = {Data-driven framework for large-scale prediction of charging energy in electric vehicles},
journal = {Applied Energy},
volume = {282},
pages = {116175},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116175},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920315798},
author = {Yang Zhao and Zhenpo Wang and Zuo-Jun Max Shen and Fengchun Sun},
keywords = {Charging energy, Large-scale prediction, Machine learning, Electric vehicle},
abstract = {Large-scale and high-precision predictions of the charging energy required for electric vehicles (EVs) are essential to ensure the safety of EVs and provide reliable inputs for grid-load calculations. However, the complex and dynamic operating conditions of EVs make it challenging to accurately predict the charging energy under real-world conditions, especially for large-scale EV utilization. In this study, a novel data-driven framework for large-scale charging energy predictions is developed by individually controlling the strongly linear and weakly nonlinear contributions. The proposed framework concurrently addresses the overfitting of nonlinear networks using a low proportion of training data as well as the poorly descriptive ability of linear networks under complex environments. For each charging session, the charging energy predictions appropriately account for important factors such as the variations in the state of charge (SOC) of the battery, ambient temperatures, charging rates, and total driving distances. The results suggest that, compared with existing prediction models (such as the random forest, xgboost, and neural network), the proposed framework persists with evidently higher accuracy and stability over a wide range of the ratio between the number of EVs used for testing and training; its mean absolute percentage error (MAPE) is maintained at 2.5–3.8% when the ratio ranges from 0.1 to 1000. The proposed models can be further utilized for cloud-based battery diagnoses and large-scale forecasting of the energy demands of EVs.}
}
@article{OLIVEIRA2021893,
title = {Steps towards an Healthcare Information Model based on openEHR},
journal = {Procedia Computer Science},
volume = {184},
pages = {893-898},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007900},
author = {Daniela Oliveira and Rui Miranda and Francini Hak and Nuno Abreu and Pedro Leuschner and António Abelha and José Machado},
keywords = {Electronic Health Record, Healthcare Information Systems, OpenEHR, Clinical Information model, Reference Model},
abstract = {During COVID-19 pandemic crisis, healthcare institutions globally were experiencing a VUCA - Volatile, Uncertain, Complex, and Ambiguous - environment. Effcient clinical and administrative management had never been so emergent. To achieve this goal, different components of the Healthcare Information System (HIS) must cooperate and interoperate flawlessly. Data standardization is a necessary step towards normalization and interoperability between existing Legacy Systems (LSs), and provides for longitudinal, highly reliable and persistent Electronic Health Records (EHRs). The openEHR standard was chosen for its overall dual domain architecture, where the more dynamic clinical information model may evolve independently from the relatively stable Reference Model (RM). Its Information Model (IM) comprises demographic, administrative and clinical systems. Critical clinical terms have been aligned to the FHIR HL7 standard, as to further support interoperability.}
}
@article{KHAN2021130,
title = {Robustness of AI-based prognostic and systems health management},
journal = {Annual Reviews in Control},
volume = {51},
pages = {130-152},
year = {2021},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2021.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1367578821000195},
author = {Samir Khan and Seiji Tsutsumi and Takehisa Yairi and Shinichi Nakasuka},
keywords = {Prognostics and system health management, Robust AI, Machine learning, PHM, Fault diagnosis},
abstract = {Prognostic and systems Health Management (PHM) is an integral part of a system. It is used for solving reliability problems that often manifest due to complexities in design, manufacturing, operating environment and system maintenance. For safety-critical applications, using a model-based development process for complex systems might not always be ideal but it is equally important to establish the robustness of the solution. The information revolution has allowed data-driven methods to diffuse within this field to construct the requisite process (or system models) to cope with the so-called big data phenomenon. This is supported by large datasets that help machine-learning models achieve impressive accuracy. AI technologies are now being integrated into many PHM related applications including aerospace, automotive, medical robots and even autonomous weapon systems. However, with such rapid growth in complexity and connectivity, a systems’ behaviour is influenced in unforeseen ways by cyberattacks, human errors, working with incorrect or incomplete models and even adversarial phenomena. Many of these models depend on the training data and how well the data represents the test data. These issues require fine-tuning and even retraining the models when there is even a small change in operating conditions or equipment. Yet, there is still ambiguity associated with their implementation, even if the learning algorithms classify accordingly. Uncertainties can lie in any part of the AI-based PHM model, including in the requirements, assumptions, or even in the data used for training and validation. These factors lead to sub-optimal solutions with an open interpretation as to why the requirements have not been met. This warrants the need for achieving a level of robustness in the implemented PHM, which is a challenging task in a machine learning solution. This article aims to present a framework for testing the robustness of AI-based PHM. It reviews some key milestones achieved in the AI research community to deal with three particular issues relevant for AI-based PHM in safety-critical applications: robustness to model errors, robustness to unknown phenomena and empirical evaluation of robustness during deployment. To deal with model errors, many techniques from probabilistic inference and robust optimisation are often used to provide some robustness guarantee metric. In the case of unknown phenomena, techniques include anomaly detection methods, using causal models, the construction of ensembles and reinforcement learning. It elicits from the authors’ work on fault diagnostics and robust optimisation via machine learning techniques to offer guidelines to the PHM research community. Finally, challenges and future directions are also examined; on how to better cope with any uncertainties as they appear during the operating life of an asset.}
}
@article{SUN2021180,
title = {Learning hierarchical face representation to enhance HCI among medical robots},
journal = {Future Generation Computer Systems},
volume = {118},
pages = {180-186},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330314},
author = {Dianmin Sun and Honghua Zhao and Tao Song and Aiqin Liu and Jinling Cheng and Zhi Liu and Xin Zhao},
keywords = {Face recognition, Deep learning, Hierarchical neural network, Medical robot HCI},
abstract = {In this paper, we propose a hierarchical framework for face recognition by learning deep representation. In order to exploit key patches for face recognition, we separate the entire image into several patches including eyes, nose, and mouth. A binary facegrid is generated to indicate the accurate position of the key patches in face image. The patches are fed into the hierarchical framework to learn the deep representation of the image. We leverage the PCA and SVM method for face recognition. Our face representation can enhance many medical robot applications. Comprehensive experiments have demonstrated that our proposed method can effectively recognize real human faces from fake samples.}
}
@incollection{KEOGH20231045,
title = {Chapter 68 - Blockchain: an enabler for safe food in global supply networks},
editor = {Michael E. Knowles and Lucia E. Anelich and Alan R. Boobis and Bert Popping},
booktitle = {Present Knowledge in Food Safety},
publisher = {Academic Press},
pages = {1045-1066},
year = {2023},
isbn = {978-0-12-819470-6},
doi = {https://doi.org/10.1016/B978-0-12-819470-6.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194706000081},
author = {John G. Keogh and Abderahman Rejeb and Nida Khan and Khaldoon Zaid-Kaylani},
keywords = {Blockchain, food, supply chains, traceability, transparency, trust, food integrity},
abstract = {Recurring food safety and food fraud incidents have negatively impacted the food industry's credibility and consumer confidence. As a result, food safety management and food fraud risk have garnered significant attention. Food business operators are aware of the need to respond to consumer concerns regarding product quality and safety and shape consumer attitudes regarding food. In parallel, businesses with local, regional, or global food supply chains (FSCs) are continually evaluating the use of advanced technologies to improve supply chain efficiency and flexibility and reduce risk. In this regard, scholars have examined food safety issues and proposed blockchain technology. Blockchain is a foundational innovation that has the potential to radically transform FSCs and support the efficient management and operations of food industry stakeholders. In this chapter, we aim to expand the academic understanding of the full potential of the technology and the impediments to its adoption in the food industry by providing a comprehensive review and understanding of the literature's most prominent topics. As the chapter's primary sources of information, two blockchain use cases were selected, summarised, and analysed to highlight the practical relevance of adopting the technology in the FSC. The chapter's findings indicate that blockchain can improve FSC traceability, transparency and trust, collaboration and trade, and contributes to food safety management. However, the technology's benefits cannot be realised until technical, organisational, and regulatory issues are resolved.}
}
@article{LEE2021101428,
title = {Understanding digital transformation in advanced manufacturing and engineering: A bibliometric analysis, topic modeling and research trend discovery},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101428},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101428},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001804},
author = {Ching-Hung Lee and Chien-Liang Liu and Amy J.C. Trappey and John P.T. Mo and Kevin C. Desouza},
keywords = {Digital transformation, Advanced manufacturing and engineering, Bibliometric analysis, Topic modeling, Systematic review},
abstract = {Digital transformation (DT) is the process of combining digital technologies with sound business models to generate great value for enterprises. DT intertwines with customer requirements, domain knowledge, and theoretical and empirical insights for value propagations. Studies of DT are growing rapidly and heterogeneously, covering the aspects of product design, engineering, production, and life-cycle management due to the fast and market-driven industrial development under Industry 4.0. Our work addresses the challenge of understanding DT trends by presenting a machine learning (ML) approach for topic modeling to review and analyze advanced DT technology research and development. A systematic review process is developed based on the comprehensive DT in manufacturing systems and engineering literature (i.e., 99 articles). Six dominant topics are identified, namely smart factory, sustainability and product-service systems, construction digital transformation, public infrastructure-centric digital transformation, techno-centric digital transformation, and business model-centric digital transformation. The study also contributes to adopting and demonstrating the ML-based topic modeling for intelligent and systematic bibliometric analysis, particularly for unveiling advanced engineering research trends through domain literature.}
}
@article{CAO2021106004,
title = {An automated zizania quality grading method based on deep classification model},
journal = {Computers and Electronics in Agriculture},
volume = {183},
pages = {106004},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106004},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921000223},
author = {Jingjun Cao and Tan Sun and Wenrong Zhang and Ming Zhong and Bo Huang and Guomin Zhou and Xiujuan Chai},
keywords = {Zizania, Automatic grading, Convolutional neural network, Deep learning, Object classification},
abstract = {Zizania, one of aquatic vegetables, needs to be graded before entering market for assuring the product quality. However, it is time-consuming, tedious, labor-intensive, inaccurate and expensive to assess qualitatively and grade zizanias manually. This paper gives an effective solution to automatically classify fresh zizania into two categories, high quality and defective quality, by using the deep learnt features from the appearances. A new architecture of convolutional neural network, called LightNet, has been proposed and described. Specifically, it is composed of many compressed blocks, which is designed to reduce the computation complexity mainly by converting serial down-sampling and convolution operation to parallel structure. We evaluate the proposed architecture on the zizania image dataset collected by ourselves and integrate the algorithm in the automatic grading device. The experimental results show that the accuracy rate achieves 95.62% and the speed of inference quality is around 47 ms per zizania image. The proposed LightNet for automate classification has less parameters and lower computation complexity than popular networks, while maintaining the comparable accuracy in the task of grading zizanias. It obtains 99.31% accuracy in the task of grading apples. The result proves that it can be extended to other tasks about classification.}
}
@article{LO2021110164,
title = {Techno-economic analysis for biomass supply chain: A state-of-the-art review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {135},
pages = {110164},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.110164},
url = {https://www.sciencedirect.com/science/article/pii/S136403212030455X},
author = {Shirleen Lee Yuen Lo and Bing Shen How and Wei Dong Leong and Sin Yong Teng and Muhammad Akbar Rhamdhani and Jaka Sunarso},
keywords = {Biomass properties, Optimization, Supply chain risks, Supply chain uncertainties, Bioenergy},
abstract = {Given the increasing risk of climate change and depletion of non-renewable energy sources, countries around the world have been looking into energy profile diversification whereby biomass represents one of the most appealing alternatives for energy production feedstock. To attract interest and more investment from industry players into biomass-based industries, comprehensive techno-economic analysis has to be performed. In addition, various uncertainties related to supply chain such as biomass attainability, demand variation, and material price fluctuation, have to be considered in the evaluation to yield more accurate and reliable feasibility estimation. This review paper aims to: (i) provide an overview on the different types of methods or approaches used in the feasibility evaluation of biomass-based industries from the techno-economic point of view, and (ii) outline the supply chain uncertainties that should be incorporated into the evaluation model using a Malaysian case study to illustrate the impact of these uncertainties. Apart from that, some of the unquantifiable uncertainties and risks are critically reviewed in this paper. It was found that 78 % of the reviewed articles opted for mathematical modelling evaluation method with the majority leaning towards mathematical modelling with optimization (i.e., deterministic and stochastic optimization). Furthermore, only a minority had performed stochastic evaluation that incorporates biomass supply chain uncertainties. This review discussed six quantifiable uncertainties, include: (i) biomass availability, (ii) biomass quality, (iii) transportation cost, (iv) market demand, (v) fluctuation of pricing, and (vi) wages of workers.}
}
@article{KRAUS2021109,
title = {A valve closing body as a central sensory-utilizable component},
journal = {Procedia CIRP},
volume = {100},
pages = {109-114},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004765},
author = {Benjamin Kraus and Florian Schmitt and Kay-Eric Steffan and Eckhard Kirchner},
keywords = {Sensing machine elements (SME), Cyber-physical-system (CPS), Sensory-utilizable machine elements (SuME), Product development},
abstract = {Enhancing traditional machine elements by using their embodied physical effects as an additional sensorial function to create so called sensing machine elements (SME) has been gaining momentum as a novel research topic. Utilizing those machine elements has the potential to enable a wide digitization regarding the creation of cyber-physical systems (CPS) in all mechanical engineering fields, since the basic components of every machine are being enhanced. These basic machine elements are often located close to the point of interest and therefore enable the user to collect data directly within the process itself. This design idea for the location of sensors or sensory functions is called in-situ measurement. While the enhancement of machine elements to SMEs seems promising the question arises if not other components and their physical characteristics can be used as sensors themselves. Ideally, to minimize the distance between the desired target variable and the actual measurand, those sensory-utilizable components are integral components for the main function of the machine. One of those central components is the closing body of a valve. During the concept phase of a new valve a concept was chosen while also regarding the future possible integration of sensory functions. With this in mind it was possible to enhance the chosen concept doing a small tweak during the design phase with a sensory function, that detects the current position of the valves closing body, using its physical properties. The basic idea behind the concept and design phase as well as the gathered data from a first prototype is presented in this paper.}
}
@article{CLARKE2021100213,
title = {Appyters: Turning Jupyter Notebooks into data-driven web apps},
journal = {Patterns},
volume = {2},
number = {3},
pages = {100213},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921000234},
author = {Daniel J.B. Clarke and Minji Jeon and Daniel J. Stein and Nicole Moiseyev and Eryk Kropiwnicki and Charles Dai and Zhuorui Xie and Megan L. Wojciechowicz and Skylar Litz and Jason Hom and John Erol Evangelista and Lucas Goldman and Serena Zhang and Christine Yoon and Tahmid Ahamed and Samantha Bhuiyan and Minxuan Cheng and Julie Karam and Kathleen M. Jagodnik and Ingrid Shu and Alexander Lachmann and Sam Ayling and Sherry L. Jenkins and Avi Ma'ayan},
keywords = {workflow, notebooks, big data, data analysis, data visualization, RNA-seq, scRNA-seq, machine learning, TCGA, gene set enrichment analysis},
abstract = {Summary
Jupyter Notebooks have transformed the communication of data analysis pipelines by facilitating a modular structure that brings together code, markdown text, and interactive visualizations. Here, we extended Jupyter Notebooks to broaden their accessibility with Appyters. Appyters turn Jupyter Notebooks into fully functional standalone web-based bioinformatics applications. Appyters present to users an entry form enabling them to upload their data and set various parameters for a multitude of data analysis workflows. Once the form is filled, the Appyter executes the corresponding notebook in the cloud, producing the output without requiring the user to interact directly with the code. Appyters were used to create many bioinformatics web-based reusable workflows, including applications to build customized machine learning pipelines, analyze omics data, and produce publishable figures. These Appyters are served in the Appyters Catalog at https://appyters.maayanlab.cloud. In summary, Appyters enable the rapid development of interactive web-based bioinformatics applications.}
}
@article{NGOCLANHUYNH2021117193,
title = {Novel short-term solar radiation hybrid model: Long short-term memory network integrated with robust local mean decomposition},
journal = {Applied Energy},
volume = {298},
pages = {117193},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117193},
url = {https://www.sciencedirect.com/science/article/pii/S030626192100619X},
author = {Anh {Ngoc-Lan Huynh} and Ravinesh C. Deo and Mumtaz Ali and Shahab Abdulla and Nawin Raj},
keywords = {Short-term solar radiation prediction, Robust Local Mean Decomposition, Deep Learning},
abstract = {Data-intelligent algorithms tailored for short-term energy forecasting can generate meaningful information on the future variability of solar energy developments. Traditional forecasting methods find it relatively difficult to obtain a reliable solar energy monitoring system because of the inherent nonlinearities in solar radiation and the related atmospheric input variables to any forecasting system. This paper proposes a new artificial intelligence-based hybrid model by employing the robust version of local mean decomposition (RLMD) and Long Short-term Memory (LSTM) network denoted as RLMD-LSTM. The objective model (i.e., RLMD-LSTM) is built near real-time, half-hourly ground-based solar radiation dataset for the solar rich, metropolitan study sites in Vietnam with all of the forecasting results being benchmarked through classical modelling approaches (i.e., Support Vector Regression SVR, Long Short-term Memory LSTM, Multivariate Adaptive Regression Spline MARS, Persistence) as well as the other alternative hybrid methods (i.e., RLMD-MARS, RLMD-Persistence and RLMD-SVR). Verified by statistical metrics and visual infographics, the present results demonstrate that the proposed model can generate satisfactory predictions, outperforming several counterpart methods. The predictive performance is stable for all study sites that the root-mean-square error remained profoundly lower for RLMD-LSTM (19–20%) compared with RLMD-MARS (20–21%), RLMD-SVR (29–35%), RLMD- Persistence (29–51%), LSTM (25–48%), MARS (21–51%) and SVR (23–85%), Persistence (29–51%). The Legates and McCabe’s Index, yielding a value of approximately 0.7988–0.9256 for RLMD-LSTM compared with 0.765–0.8142, 0.4917–0.5711, 0.6900–0.7482, 0.6914–0.7646, 0.4349–0.7170 respectively, for the RLMD-MARS, RLMD-SVR, RLMD-Persistence, LSTM, MARS, SVR, Persistence models, also confirms the outstanding performance of RLMD-LSTM model. Accordingly, the study ascertains that the newly designed approach can be a potential candidate for real-time energy management, renewable energy integration into a power grid and other decisions to optimise the overall system's scheduling and performance.}
}
@article{FORSTMANN2021100001,
title = {NeuroImage: Reports: A new member of the NeuroImage family embracing negative findings, replication studies and registered reports},
journal = {Neuroimage: Reports},
volume = {1},
number = {1},
pages = {100001},
year = {2021},
issn = {2666-9560},
doi = {https://doi.org/10.1016/j.ynirp.2020.100001},
url = {https://www.sciencedirect.com/science/article/pii/S2666956020300015},
author = {Birte U. Forstmann and Christopher D. Chambers and Michael X. Cohen and Gesa Hartwigsen and Peter Kochunov and Dimitri {Van De Ville} and Chao-Gan Yan}
}
@article{GE2021100150,
title = {Semi-supervised data modeling and analytics in the process industry: Current research status and challenges},
journal = {IFAC Journal of Systems and Control},
volume = {16},
pages = {100150},
year = {2021},
issn = {2468-6018},
doi = {https://doi.org/10.1016/j.ifacsc.2021.100150},
url = {https://www.sciencedirect.com/science/article/pii/S2468601821000080},
author = {Zhiqiang Ge},
keywords = {Semi-supervised data, Data driven modeling, Machine learning, Process data analytics},
abstract = {Semi-supervised data are quite common in the process industry, which has caught much attention in recent years. The semi-supervised feature of process data not only has a great impact on data mining and analytics, but also matters in feature extraction and knowledge discovery in the process. In this paper, the framework of semi-supervised data modeling and applications is formulated for the process industry. First, the semi-supervised data structure is introduced, including the causes of semi-supervised data structure, the main feature of the semi-supervised data, and its effects on data modeling and applications in the process industry. Second, detailed research statuses on semi-supervised data modeling and applications in the process industry are illustrated, with introductions of some representative approaches. Third, several challenges and promising issues on modeling and application of semi-supervised data are discussed and highlighted for future research.}
}
@article{JAYAWARDENE2021100202,
title = {The role of data and information quality during disaster response decision-making},
journal = {Progress in Disaster Science},
volume = {12},
pages = {100202},
year = {2021},
issn = {2590-0617},
doi = {https://doi.org/10.1016/j.pdisas.2021.100202},
url = {https://www.sciencedirect.com/science/article/pii/S2590061721000624},
author = {Vimukthi Jayawardene and Thomas J. Huggins and Raj Prasanna and Bapon Fakhruddin},
keywords = {Disaster response, Decision making, Data and information quality},
abstract = {Massive amounts of data and information are exchanged during the response phase of disaster management. A large body of contemporary research has indicated that most of these data and information have severe quality related concerns, meaning that they may not be suitable for critical decision-making. The current paper addresses these issues by identifying how certain features of data and information quality function, to support specific, naturalistic decision-making processes during disaster response. These functions are used to revise and consolidate pre-existing definitions of data and information quality, for use in further disaster response research.}
}
@incollection{CROMPTON202183,
title = {Chapter 5 - Data Management from the DCS to the Historian},
editor = {Patrick Bangert},
booktitle = {Machine Learning and Data Science in the Oil and Gas Industry},
publisher = {Gulf Professional Publishing},
pages = {83-110},
year = {2021},
isbn = {978-0-12-820714-7},
doi = {https://doi.org/10.1016/B978-0-12-820714-7.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207147000054},
author = {Jim Crompton},
keywords = {data management, oil and gas industry, sensor data, digital operations, simulation, automation},
abstract = {The oil and gas industry produces great volumes of data on an ongoing basis. This data must be acquired, stored, curated, and made available properly for data science to be able to take place. This process starts at the sensor and continues via systems such as the control system and the historian. In this journey, information and operational technologies converge. Sensor data is acquired and transmitted through various systems to arrive at the control system, which then makes it available through various protocols for consumption by analysis software. Historians save the data and make it available to the human user as diagrams. Documents and simulation data can also be integrated into the picture. This complex landscape is discussed in this chapter.}
}
@article{MORLOCK2021542,
title = {Concept for Enabling Customer-oriented Data Analytics via Integration of Production Process Improvement Methods and Data Science Methods},
journal = {Procedia CIRP},
volume = {104},
pages = {542-546},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.091},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009896},
author = {Friedrich Morlock and Mario Boßlau},
keywords = {Industry 4.0, Data Analytics, Lean Management, Process Improvement},
abstract = {Production companies are facing the major challenge of digitization. New technical developments enable new optimization potentials in production that can be leveraged in a highly competitive market. Data analysis is a good example of this, allowing large amounts of data to be used to support production in optimization projects. For data analysis the CRISP-DM approach has become generally accepted in science and practice. This process model offers a good support for data science projects with useful data analysis methods and tools. In practice, however, it can often be observed that aspects such as customer-oriented project definition and the integration of process knowledge in data science projects are difficult to achieve. This paper will present a solution for that challenge. The combination of process optimization methods from the Lean Management and Six Sigma toolbox as well as project management methods for each phase of the CRISP-DM approach support data science projects to customize the project definition and integrate process knowledge.}
}
@article{YU2021100383,
title = {A node optimization model based on the spatiotemporal characteristics of the road network for urban traffic mobile crowd sensing},
journal = {Vehicular Communications},
volume = {31},
pages = {100383},
year = {2021},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2021.100383},
url = {https://www.sciencedirect.com/science/article/pii/S2214209621000528},
author = {Haiyang Yu and Jing Fang and Shuai Liu and Yilong Ren and Jian Lu},
keywords = {Urban traffic mobile crowd sensing, Spatiotemporal characteristics, Dynamic accessibility, Node selection, Utility maximization},
abstract = {Urban traffic mobile crowd sensing (Urban Traffic MCS) has emerged as a new effective paradigm of sensing and collecting data by means of vehicles equipped with various sensors in urban areas. In an Urban Traffic MCS system, the utility directly reflects the effectiveness of the sensing results, and it is essential to maximize the utility of the collected data. Some studies have shown that utility can be effectively improved by optimizing the selection of sensing nodes. However, most previous research has considered only the coverage and critical links of the road network while neglecting the spatiotemporal characteristics of the traffic flow, although the latter are essential for node selection optimization and significantly impact the utility of Urban Traffic MCS. Therefore, most existing methods are not suitable for Urban Traffic MCS systems. In this paper, a novel node optimization model based on the spatiotemporal characteristics of the road network is proposed. First, we introduce the Urban Traffic MCS system, and dynamic accessibility is introduced to describe the spatiotemporal characteristics of the whole road network. Then, the utility function for Urban Traffic MCS is redefined based on the effective coverage and dynamic accessibility to consider both the topological structure of the road network and the dynamic changes in traffic flow. On this basis, a node selection method with the aim of maximizing the utility of Urban Traffic MCS is proposed. Finally, the results of simulation experiments show that the node selection method in this paper can effectively achieve increased utility for an Urban Traffic MCS system.}
}
@article{SONG2021106521,
title = {The use of real-world data/evidence in regulatory submissions},
journal = {Contemporary Clinical Trials},
volume = {109},
pages = {106521},
year = {2021},
issn = {1551-7144},
doi = {https://doi.org/10.1016/j.cct.2021.106521},
url = {https://www.sciencedirect.com/science/article/pii/S1551714421002573},
author = {Fuyu Song and Chenxuan Zang and Xinyi Ma and Sifan Hu and Qiqing Sun and Shein-Chung Chow and Hongqiang Sun},
keywords = {Real-world data, Real-world evidence, Substantial evidence, Gap analysis, Fit-for-purpose, Regulatory submission},
abstract = {The 21st Century Cures Act passed by the United States (US) Congress in December 2016 requires the US Food and Drug Administration (FDA) shall establish a program to evaluate the potential use of real-world evidence (RWE) which is generated from real-world data (RWD) to (i) support approval of new indication for a drug approved under section 505 (c) and (ii) satisfy post-approval study requirements. RWE offers the opportunities to develop robust evidence using high-quality data and sophisticated methods for producing causal-effect estimates regardless randomization is feasible. In this article, we have demonstrated that the assessment of treatment effect (RWE) based on RWD could be biased due to the potential selection and information biases of RWD. Although fit-for-purpose RWE may meet regulatory standards under certain assumptions, it is not the same as substantial evidence (current regulatory standard in support of approval of regulatory submission). In practice, it is then suggested that when there are gaps between fit-for-purpose RWE and substantial evidence, we should make efforts to fill these gaps based on a comprehensive evaluation of the treatment effect. We also review two RWE examples to show some potential use of RWE in clinical studies.}
}
@article{CUNHA2021100403,
title = {A survey of privacy-preserving mechanisms for heterogeneous data types},
journal = {Computer Science Review},
volume = {41},
pages = {100403},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100403},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000435},
author = {Mariana Cunha and Ricardo Mendes and João P. Vilela},
keywords = {Privacy, Privacy taxonomy, Privacy-preserving mechanisms, Heterogeneous data types, Privacy tools},
abstract = {Due to the pervasiveness of always connected devices, large amounts of heterogeneous data are continuously being collected. Beyond the benefits that accrue for the users, there are private and sensitive information that is exposed. Therefore, Privacy-Preserving Mechanisms (PPMs) are crucial to protect users’ privacy. In this paper, we perform a thorough study of the state of the art on the following topics: heterogeneous data types, PPMs, and tools for privacy protection. Building from the achieved knowledge, we propose a privacy taxonomy that establishes a relation between different types of data and suitable PPMs for the characteristics of those data types. Moreover, we perform a systematic analysis of solutions for privacy protection, by presenting and comparing privacy tools. From the performed analysis, we identify open challenges and future directions, namely, in the development of novel PPMs.}
}
@article{FIAIDHI2021107187,
title = {Prognosis analysis of thick data: Clustering heart diseases risk groups case study},
journal = {Computers & Electrical Engineering},
volume = {92},
pages = {107187},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107187},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621001889},
author = {J. Fiaidhi and S. Mohammed},
keywords = {Thick Data, Prognosis Analysis, Fuzzy Clustering, Small Datasets, Healthcare Data, Risk Analysis, Machine Learning},
abstract = {Analyzing clinical data differs from other machine learning data analysis as most of the clinical data are relatively small requiring more qualitative techniques to bring focus to the context and then to predict important indicators like the patient risk in developing heart disease. The strength of qualitative analytics lies in data thickness as they can work on small samples and corpuses (“small data”). However, working with thick data analytics requires involving patient characteristics (e.g. socioeconomic status, family background, working conditions, social support, psycho-social characteristics, lifestyle risk factors, age group, gender and social capital) and their weights in a particular clinical practice. Therefore, the role of patient characteristics is not only a dominant factor in thick data analytics but it is also linked to predicting the prognosis of patient cases. A Fuzzy C-Means algorithm is presented as technique for prognostic predictions to identify risk groups associated with Cardiovascular Disease (CVD) conditions.}
}
@article{RUKANOVA2021101496,
title = {Identifying the value of data analytics in the context of government supervision: Insights from the customs domain},
journal = {Government Information Quarterly},
volume = {38},
number = {1},
pages = {101496},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2020.101496},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X20302756},
author = {Boriana Rukanova and Yao-Hua Tan and Micha Slegt and Marcel Molenhuis and Ben {van Rijnsoever} and Jonathan Migeotte and Mathieu L.M. Labare and Krunoslav Plecko and Bora Caglayan and Gavin Shorten and Otis {van der Meij} and Suzanne Post},
keywords = {Framework, Data analytics, Value, Government, Supervision, eCommerce, Capabilities, Collective capability},
abstract = {eCommerce, Brexit, new safety and security concerns are only a few examples of the challenges that government organisations, in particular customs administrations, face today when controlling goods crossing borders. To deal with the enormous volumes of trade customs administrations rely more and more on information technology (IT) and risk assessment, and are starting to explore the possibilities that data analytics (DA) can offer to support their supervision tasks. Driven by customs as our empirical domain, we explore the use of DA to support the supervision role of government. Although data analytics is considered to be a technological breakthrough, there is so far only a limited understanding of how governments can translate this potential into actual value and what are barriers and trade-offs that need to be overcome to lead to value realisation. The main question that we explore in this paper is: How to identify the value of DA in a government supervision context, and what are barriers and trade-offs to be considered and overcome in order to realise this value? Building on leading models from the information system (IS) literature, and by using case studies from the customs domain, we developed the Value of Data Analytics in Government Supervision (VDAGS) framework. The framework can help managers and policy-makers to gain a better understanding of the benefits and trade-offs of using DA when developing DA strategies or when embarking on new DA projects. Future research can examine the applicability of the VDAGS framework in other domains of government supervision.}
}
@article{CHOI2023148,
title = {Measuring taxi ridesharing effects and its spatiotemporal pattern in Seoul, Korea},
journal = {Travel Behaviour and Society},
volume = {30},
pages = {148-162},
year = {2023},
issn = {2214-367X},
doi = {https://doi.org/10.1016/j.tbs.2022.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214367X22001016},
author = {Junyong Choi and Youngchul Kim and Minchul Kwak and Minju Park and David Lee},
keywords = {Taxi ridesharing, Spatiotemporal pattern analysis, Mobility, Taxi data, Travel pattern},
abstract = {Most studies on dynamic ridesharing, which allows any driver to ferry separate passengers with overlapping routes in a single trip, have focused on developing efficient algorithms. While dynamic ridesharing demands extensive computing infrastructure, this paper suggests a simple taxi ridesharing approach that allows only passengers from certain taxi hotspots to share a ride with another who has a similar destination and measures the effects, spatiotemporal patterns and its benefits. Compared to dynamic ridesharing, the proposed method does not require dedicated driver fleets, networked communication system, or monopoly of all passenger information. We identify taxi pickup hotspots and analyze the spatiotemporal patterns of the simple ridesharing approach. The results show that 48 % of rides from hotspots could be shared, reducing the overall vehicle-km traveled by 1.2 km for each shared ride. We also find that spatiotemporal patterns of the ridesharing could represent urban characteristics. For example, places with high ridesharing potential and low saved-trip distances could imply low public transportation accessibility while areas with high shareability during working hours on both weekdays and weekends could represent public transportation hubs. The proposed method is expected to be useful to identify taxi stands that have high ridesharing opportunities. Policy makers can use our approach to support simple ridesharing scheme. Moreover, with the characterized taxi stands, such as longer saved-trip distance and nightlife peaks, the proposed method could be used as decision support tools for temporary allowed ridesharing. In addition, spatiotemporal patterns of the taxi stands would be used in designing public transportation systems.}
}
@article{LIU2021110601,
title = {A data mining-based framework for the identification of daily electricity usage patterns and anomaly detection in building electricity consumption data},
journal = {Energy and Buildings},
volume = {231},
pages = {110601},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.110601},
url = {https://www.sciencedirect.com/science/article/pii/S0378778820333879},
author = {Xue Liu and Yong Ding and Hao Tang and Feng Xiao},
keywords = {Building energy management, Time series clustering, Decision tree, Knowledge discovery, Electricity usage pattern, Data mining},
abstract = {With the development of advanced information techniques, ﻿smart energy meters have made a considerable amount of real-time electricity consumption data available. These data provide a promising way to understand energy usage patterns and improve building energy management. However, previous studies have paid more attention to methodologies for the identification of energy usage patterns and are limited in the interpretability and applications of the patterns. In this context, this paper proposes a general data mining-based framework that can extract typical electricity load patterns (TELPs) and discover insightful information hidden in the patterns. The framework integrates multiple data mining techniques and mainly consists of three phases: data preparation, identification of TELPs and knowledge discovery in the patterns. A new clustering method with a two-step clustering analysis is proposed to identify the TELPs at the individual building level. Before clustering, five statistical features that represent the shapes of electricity load profiles are first defined to reduce the dimensions of daily electricity load profiles. The first clustering step aims at detecting outliers of daily electricity load profiles (DELPs) by using the density-based spatial clustering application with noise (DBSCAN) algorithm clustering technique, which addresses the data quality issues for electricity consumption data derived from energy consumption monitoring platforms (ECMPs). The second clustering step aims at grouping similar DELPs by means of the k-means algorithm to extract TELPs. The effectiveness of the proposed clustering method is demonstrated by a comparison with two single-step clustering techniques. Furthermore, a classification and regression tree (CART) algorithm is employed to discover insightful knowledge on TELPs and improve the interpretability of clustering results, namely, to explain the relations between dynamic influencing factors related to electricity consumption and TELPs. The proposed framework is applied to analyze the time-series electricity consumption data of three practical office buildings in Chongqing, and its effectiveness has been confirmed. A potential application of discovered knowledge is presented: early fault detection of anomalous electricity load profiles. The proposed framework can provide building managers with an efficient way to understand the characteristics of building electricity usage patterns and detect anomalies therein.}
}
@article{AMOS2021100331,
title = {The NanoInformatics Knowledge Commons: Capturing spatial and temporal nanomaterial transformations in diverse systems},
journal = {NanoImpact},
volume = {23},
pages = {100331},
year = {2021},
issn = {2452-0748},
doi = {https://doi.org/10.1016/j.impact.2021.100331},
url = {https://www.sciencedirect.com/science/article/pii/S2452074821000409},
author = {Jaleesia D. Amos and Yuan Tian and Zhao Zhang and Greg V. Lowry and Mark R. Wiesner and Christine Ogilvie Hendren},
keywords = {Database, Nanoinformatics, Nanomaterials, Environmental nanotechnology, Transformations},
abstract = {The empirical necessity for integrating informatics throughout the experimental process has become a focal point of the nano-community as we work in parallel to converge efforts for making nano-data reproducible and accessible. The NanoInformatics Knowledge Commons (NIKC) Database was designed to capture the complex relationship between nanomaterials and their environments over time in the concept of an ‘Instance’. Our Instance Organizational Structure (IOS) was built to record metadata on nanomaterial transformations in an organizational structure permitting readily accessible data for broader scientific inquiry. By transforming published and on-going data into the IOS we are able to tell the full transformational journey of a nanomaterial within its experimental life cycle. The IOS structure has prepared curated data to be fully analyzed to uncover relationships between observable phenomenon and medium or nanomaterial characteristics. Essential to building the NIKC database and associated applications was incorporating the researcher's needs into every level of development. We started by centering the research question, the query, and the necessary data needed to support the question and query. The process used to create nanoinformatic tools informs usability and analytical capability. In this paper we present the NIKC database, our developmental process, and its curated contents. We also present the Collaboration Tool which was built to foster building new collaboration teams. Through these efforts we aim to: 1) elucidate the general principles that determine nanomaterial behavior in the environment; 2) identify metadata necessary to predict exposure potential and bio-uptake; and 3) identify key characterization assays that predict outcomes of interest.}
}
@article{WANG2021110929,
title = {Practical issues in implementing machine-learning models for building energy efficiency: Moving beyond obstacles},
journal = {Renewable and Sustainable Energy Reviews},
volume = {143},
pages = {110929},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.110929},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121002227},
author = {Zeyu Wang and Jian Liu and Yuanxin Zhang and Hongping Yuan and Ruixue Zhang and Ravi S. Srinivasan},
keywords = {Machine learning, Building energy modeling, Energy efficiency, Building energy management, Practical issues, Technology diffusion},
abstract = {Implementing machine-learning models in real applications is crucial to achieving intelligent building control and high energy efficiency. Over the past few decades, numerous studies have attempted to explore the application of machine-learning models to building energy efficiency. However, these studies have focused on analyzing the technical feasibility and superiority of machine learning algorithms for fitting building energy-related data and have not considered methods of implementing machine learning technology in building energy efficiency applications. Therefore, this review aims to summarize the current practical issues involved in applying machine-learning models to building energy efficiency by systematically analyzing existing research findings and limitations. The paper first reviews the application status of machine learning-based building energy efficiency research by analyzing the model implementation process and summarizing the main uses of the technology in the overall building energy management life cycle. The paper then elaborates on the causes of, influences on, and potential solutions for practical issues found in the implementation and promotion of machine learning-based building energy efficiency measures. Finally, this paper discusses valuable future machine learning-based building energy efficiency research directions with regard to technology opportunity discovery, data governance, feature engineering, generalizability test, technology diffusion, and knowledge sharing. This paper will provide building researchers and practitioners with a better understanding of the current application statuses of and potential research directions for machine learning models in building energy efficiency.}
}
@article{WANG2021629,
title = {GAPIT Version 3: Boosting Power and Accuracy for Genomic Association and Prediction},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {19},
number = {4},
pages = {629-640},
year = {2021},
note = {Bioinformatics Commons},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2021.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022921001777},
author = {Jiabo Wang and Zhiwu Zhang},
keywords = {GWAS, Genomic selection, Software, R, GAPIT},
abstract = {Genome-wide association study (GWAS) and genomic prediction/selection (GP/GS) are the two essential enterprises in genomic research. Due to the great magnitude and complexity of genomic and phenotypic data, analytical methods and their associated software packages are frequently advanced. GAPIT is a widely-used genomic association and prediction integrated tool as an R package. The first version was released to the public in 2012 with the implementation of the general linear model (GLM), mixed linear model (MLM), compressed MLM (CMLM), and genomic best linear unbiased prediction (gBLUP). The second version was released in 2016 with several new implementations, including enriched CMLM (ECMLM) and settlement of MLMs under progressively exclusive relationship (SUPER). All the GWAS methods are based on the single-locus test. For the first time, in the current release of GAPIT, version 3 implemented three multi-locus test methods, including multiple loci mixed model (MLMM), fixed and random model circulating probability unification (FarmCPU), and Bayesian-information and linkage-disequilibrium iteratively nested keyway (BLINK). Additionally, two GP/GS methods were implemented based on CMLM (named compressed BLUP; cBLUP) and SUPER (named SUPER BLUP; sBLUP). These new implementations not only boost statistical power for GWAS and prediction accuracy for GP/GS, but also improve computing speed and increase the capacity to analyze big genomic data. Here, we document the current upgrade of GAPIT by describing the selection of the recently developed methods, their implementations, and potential impact. All documents, including source code, user manual, demo data, and tutorials, are freely available at the GAPIT website (http://zzlab.net/GAPIT).}
}
@article{TENG2021110208,
title = {Recent advances on industrial data-driven energy savings: Digital twins and infrastructures},
journal = {Renewable and Sustainable Energy Reviews},
volume = {135},
pages = {110208},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.110208},
url = {https://www.sciencedirect.com/science/article/pii/S1364032120304974},
author = {Sin Yong Teng and Michal Touš and Wei Dong Leong and Bing Shen How and Hon Loong Lam and Vítězslav Máša},
keywords = {Digital twins, Data-driven energy savings, Artificial intelligence (AI), Blockchain, Internet of things (IoT), Cyber-physical production systems (CPPS)},
abstract = {Data-driven models for industrial energy savings heavily rely on sensor data, experimentation data and knowledge-based data. This work reveals that too much research attention was invested in making data-driven models, as supposed to ensuring the quality of industrial data. Furthermore, the true challenge within the Industry 4.0 is with data communication and infrastructure problems, not so significantly on developing modelling techniques. Current methods and data infrastructures for industrial energy savings were comprehensively reviewed to showcase the potential for a more accurate and effective digital twin-based infrastructure for the industry. With a few more development in enabling technologies such as 5G developments, Internet of Things (IoT) standardization, Artificial Intelligence (AI) and blockchain 3.0 utilization, it is but a matter of time that the industry will transition towards the digital twin-based approach. Global government efforts and policies are already inclining towards leveraging better industrial energy efficiencies and energy savings. This provides a promising future for the development of a digital twin-based energy-saving system in the industry. Foreseeing some potential challenges, this paper also discusses the importance of symbiosis between researchers and industrialists to transition from traditional industry towards a digital twin-based energy-saving industry. The novelty of this work is the current context of industrial energy savings was extended towards cutting-edge technologies for Industry 4.0. Furthermore, this work proposes to standardize and modularize industrial data infrastructure for smart energy savings. This work also serves as a concise guideline for researchers and industrialists who are looking to implement advanced energy-saving systems.}
}
@article{LEE202127,
title = {Strava Metro data for bicycle monitoring: a literature review},
journal = {Transport Reviews},
volume = {41},
number = {1},
pages = {27-47},
year = {2021},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2020.1798558},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722000435},
author = {Kyuhyun Lee and Ipek Nese Sener},
keywords = {Strava, bicycle, crowdsourced data, fitness tracking application, emerging travel data},
abstract = {ABSTRACT
Monitoring bicycle trips is no longer limited to traditional sources, such as travel surveys and counts. Strava, a popular fitness tracker, continuously collects human movement trajectories, and its commercial data service, Strava Metro, has enriched bicycle research opportunities over the last five years. Accrued knowledge from colleagues who have already utilised Strava Metro data can be valuable for those seeking expanded monitoring options. To convey such knowledge, this paper synthesises a data overview, extensive literature review on how the data have been applied to deal with drivers’ bicycle-related issues, and implications for future work. The review results indicate that Strava Metro data have the potential—although finite—to be used to identify various travel patterns, estimate travel demand, analyse route choice, control for exposure in crash models, and assess air pollution exposure. However, several challenges, such as the under-representativeness of the general population, bias towards and away from certain groups, and lack of demographic and trip details at the individual level, prevent researchers from depending entirely on the new data source. Cross-use with other sources and validation of reliability with official data could enhance the potentiality.}
}
@article{NEGROCALDUCH2021104507,
title = {Technological progress in electronic health record system optimization: Systematic review of systematic literature reviews},
journal = {International Journal of Medical Informatics},
volume = {152},
pages = {104507},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104507},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621001337},
author = {Elsa Negro-Calduch and Natasha Azzopardi-Muscat and Ramesh S. Krishnamurthy and David Novillo-Ortiz},
keywords = {Medical informatics, Electronic health records, eHealth, Artificial intelligence, Blockchain, Phenotyping, Deep learning, Natural language processing},
abstract = {Background
The recent, rapid development of digital technologies offers new possibilities for more efficient implementation of electronic health record (EHR) and personal health record (PHR) systems. A growing volume of healthcare data has been the hallmark of this digital transformation. The large healthcare datasets' complexity and their dynamic nature pose various challenges related to processing, analysis, storage, security, privacy, data exchange, and usability.
Materials and Methods
We performed a systematic review of systematic reviews to assess technological progress in EHR and PHR systems. We searched MEDLINE, Cochrane, Web of Science, and Scopus for systematic literature reviews on technological advancements that support EHR and PHR systems published between January 1, 2010, and October 06, 2020.
Results
The searches resulted in a total of 2,448 hits. Of these, we finally selected 23 systematic reviews. Most of the included papers dealt with information extraction tools and natural language processing technology (n = 10), followed by studies that assessed the use of blockchain technology in healthcare (n = 8). Other areas of digital technology research included EHR and PHR systems in austere settings (n = 1), de-identification methods (n = 1), visualization techniques (n = 1), communication tools within EHR and PHR systems (n = 1), and methodologies for defining Clinical Information Models that promoted EHRs and PHRs interoperability (n = 1).
Conclusions
Technological advancements can improve the efficiency in the implementation of EHR and PHR systems in numerous ways. Natural language processing techniques, either rule-based, machine-learning, or deep learning-based, can extract information from clinical narratives and other unstructured data locked in EHRs and PHRs, allowing secondary research (i.e., phenotyping). Moreover, EHRs and PHRs are expected to be the primary beneficiaries of the blockchain technology implementation on Health Information Systems. Governance regulations, lack of trust, poor scalability, security, privacy, low performance, and high cost remain the most critical challenges for implementing these technologies.}
}
@article{CABRERA2021105069,
title = {Future of dairy farming from the Dairy Brain perspective: Data integration, analytics, and applications},
journal = {International Dairy Journal},
volume = {121},
pages = {105069},
year = {2021},
issn = {0958-6946},
doi = {https://doi.org/10.1016/j.idairyj.2021.105069},
url = {https://www.sciencedirect.com/science/article/pii/S0958694621000972},
author = {Victor E. Cabrera and Liliana Fadul-Pacheco},
abstract = {Data integration is one of the biggest challenges the dairy industry faces nowadays due to increased number of technologies and data overflow at the farm. Here we review the current situation of precision dairy farming technologies that use integrated data and its application on the management decision process at the dairy farm. The most common data connections were those from activity monitors, dairy herd improvement records, herd management, and milking recordings. Algorithms used can be defined in general as artificial intelligence and machine learning approaches. Most of the 22 revised papers, research or review, demonstrated that applying different algorithms to integrated data provides additional and complementary insights to improve decision-making tools and therefore enhance economic, management and animal welfare, and hence the sustainability of dairy farms. All revised studies acknowledge the importance of live data integration to develop relevant decision support tools to improve decision making.}
}
@article{RAY2023116221,
title = {A holistic review on how artificial intelligence has redefined water treatment and seawater desalination processes},
journal = {Desalination},
volume = {546},
pages = {116221},
year = {2023},
issn = {0011-9164},
doi = {https://doi.org/10.1016/j.desal.2022.116221},
url = {https://www.sciencedirect.com/science/article/pii/S0011916422006762},
author = {Saikat Sinha Ray and Rohit Kumar Verma and Ashutosh Singh and Mahesh Ganesapillai and Young-Nam Kwon},
keywords = {Artificial intelligence (AI), Machine learning (ML), Artificial neural networks (ANNs), Desalination, Water treatment},
abstract = {In the modern era, deep learning (DL), and machine learning (ML), have emerged as potential technologies that are widely applied in the fields of science, engineering, and technology. These tools have been extensively used to optimize seawater desalination and water treatment processes to achieve efficient performance. Indeed, automation has played a key role in redefining the issues of water treatment and seawater desalination. Artificial intelligence (AI) has been developed as a versatile tool for processing data and optimizing smart water services while addressing the issues of monitoring, management, and labor costs. Recently, specific AI tools, such as artificial neural networks (ANNs) and genetic algorithms, have been implemented for self-monitoring and modeling applications in the field of water treatment and seawater desalination. In the present article, the application of AI in the water treatment and seawater desalination sectors is thoroughly reviewed. Additionally, conventional modeling approaches are compared with ANN modeling. Furthermore, the challenges and shortcomings are discussed, along with future prospects. Moreover, the applications of AI mechanisms in data processing, optimization, modeling, prediction, and decision-making during water treatment and seawater desalination processes are underscored. Finally, innovative trends in seawater desalination and water treatment with AI tools are summarized.}
}
@article{BARATA2021101624,
title = {The fourth industrial revolution of supply chains: A tertiary study},
journal = {Journal of Engineering and Technology Management},
volume = {60},
pages = {101624},
year = {2021},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2021.101624},
url = {https://www.sciencedirect.com/science/article/pii/S0923474821000138},
author = {João Barata},
keywords = {4SC, Supply Chain 4.0, Industry 4.0, Fourth Industrial Revolution, Tertiary study},
abstract = {This paper unfolds the ongoing fourth revolution of supply chains (4SC) and proposes guidelines for future research. The review of sixty-five literature reviews follows three stages: bibliometric analysis of Industry 4.0, its synergies with supply chain transformation, and state-of-the-art assessment. 4SC is a context-bound technological change driven by organizational and cultural priorities, aiming to create more sustainable networks to serve the customers and support responsible decisions in the supply lifecycle. The proposed framework can assist future literature reviews and digital transformation proposals for 4SC that need to frame their context and incorporate functions to endure change.}
}
@article{HALEEM202310,
title = {Management 4.0: Concept, applications and advancements},
journal = {Sustainable Operations and Computers},
volume = {4},
pages = {10-21},
year = {2023},
issn = {2666-4127},
doi = {https://doi.org/10.1016/j.susoc.2022.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666412722000277},
author = {Abid Haleem and Mohd Javaid and Ravi Pratap Singh and Rajiv Suman and Shahbaz Khan},
keywords = {Management 4.0, Technologies, Inspection, Controlling},
abstract = {Management 4.0 assists businesses in evolving, surviving and performing in the competitive and dynamic world. This fourth revolution uses advanced technologies like Artificial Intelligence (AI), Virtual Reality (VR), Internet of Things (IoT), Robotics, Holography, Additive Manufacturing etc., for the proper management systems. These technologies facilitate working personnel and make it more appealing to complete their duties efficiently and accurately. The main aim of this paper is to understand the concept of Management 4.0, its technologies and applications for proper management systems. As Management 4.0 enhances process control, the chance of human error is reduced, leading to increased efficiency. It enables rapid and intelligent decision-making, reduces costs, accelerates growth, and raises profitability. Management 4.0 technologies and advanced data analytics are helpful to make smart supply chain management well suited to fulfil industry 4.0. Thus, to overcome various obstacles and effectively deploy Management 4.0 technologies in manufacturing industries, top management must establish a clear asset performance management plan with the help of process engineers familiar with industrial system failure occurrences and what operators need to improve. Management 4.0 involves advanced technologies, system connectivity, data collection & analysis at the organisation level. Management 4.0 is expected to be a critical component in the long-term survival of any business, either manufacturing or service-providing organisations. This paper explores the development of Management 4.0 and its dimensions and transformations through Management 4.0 perspectives. Finally, the significant role of Management 4.0 for appropriate private management system in manufacturing industries are identified. Organisations require a system that seamlessly meets the company's expectations, consumers, investors, and other stakeholders to remain competitive, and Management 4.0 will enable this. Many businesses strive to integrate technologies and upskill their personnel to adapt to the new job duties and attract more workers with the necessary abilities.}
}
@incollection{2021vii,
title = {Contents},
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {vii-xii},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00015-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221327000152}
}
@article{YANG2021104887,
title = {A comparative analysis of eleven neural networks architectures for small datasets of lung images of COVID-19 patients toward improved clinical decisions},
journal = {Computers in Biology and Medicine},
volume = {139},
pages = {104887},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104887},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521006818},
author = {Yuan Yang and Lin Zhang and Mingyu Du and Jingyu Bo and Haolei Liu and Lei Ren and Xiaohe Li and M. Jamal Deen},
keywords = {Deep learning, Computed tomography, COVID-19, Image classification},
abstract = {The 2019 novel severe acute respiratory syndrome coronavirus 2-SARS-CoV2, commonly known as COVID-19, is a highly infectious disease that has endangered the health of many people around the world. COVID-19, which infects the lungs, is often diagnosed and managed using X-ray or computed tomography (CT) images. For such images, rapid and accurate classification and diagnosis can be performed using deep learning methods that are trained using existing neural network models. However, at present, there is no standardized method or uniform evaluation metric for image classification, which makes it difficult to compare the strengths and weaknesses of different neural network models. This paper used eleven well-known convolutional neural networks, including VGG-16, ResNet-18, ResNet-50, DenseNet-121, DenseNet-169, Inception-v3, Inception-v4, SqueezeNet, MobileNet, ShuffeNet, and EfficientNet-b0, to classify and distinguish COVID-19 and non-COVID-19 lung images. These eleven models were applied to different batch sizes and epoch cases, and their overall performance was compared and discussed. The results of this study can provide decision support in guiding research on processing and analyzing small medical datasets to understand which model choices can yield better outcomes in lung image classification, diagnosis, disease management and patient care.}
}
@article{YUAN2023103705,
title = {Research on the standardization model of data semantics in the knowledge graph construction of Oil&Gas industry},
journal = {Computer Standards & Interfaces},
volume = {84},
pages = {103705},
year = {2023},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2022.103705},
url = {https://www.sciencedirect.com/science/article/pii/S0920548922000721},
author = {Jingshu Yuan and Hongqi Li},
keywords = {Concept, MDR, Data semantic standardization, Knowledge Graph, Smart Field},
abstract = {At present, knowledge graphs in both general and vertical fields rarely consider the problem of data semantic standardization in the process of constructing from a global perspective, which brings trouble and loss to the sharing and interoperability of domain information. Therefore, it is very important to construct a set of standardized and semantically consistent knowledge organization model, and to propose a data semantics standardization methodology on this basis, so as to fundamentally solve this serious problem that hinders data sharing. This paper first proposes a model of data semantic standardization theoretically through the research on knowledge organization, concept triangle and concept logic and other theories, combined with ISO 1087 international standard, and proposes the important idea that the key issue of data semantic standardization is conceptual standardization as the core. Then, based on this theoretical model, a comprehensive study and combing of various vocabulary standards in the international Oil&Gas industry was carried out, and the Oil&Gas industry reference vocabulary standardization model (OIRVSM) used to guide the construction of data semantic standards in the oil and gas industry is proposed. Finally, under the guidance of the W3C best practice framework, combined with the proposed model, an operational algorithm for constructing standardized knowledge graph (ACSKG) in Oil&Gas industry and a construction example are given, which verifies the rationality and correctness of data semantic standardization model and vocabulary standard system model. The two models proposed in this paper are innovative. Although they are proposed in the context of Oil&Gas industry, they also have application and reference significance in other fields.}
}
@article{BAG2021107844,
title = {Industry 4.0 adoption and 10R advance manufacturing capabilities for sustainable development},
journal = {International Journal of Production Economics},
volume = {231},
pages = {107844},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2020.107844},
url = {https://www.sciencedirect.com/science/article/pii/S0925527320302103},
author = {Surajit Bag and Shivam Gupta and Sameer Kumar},
keywords = {Industry 4.0, Circular economy, Sustainable development, Practice based view, Dynamic capability view, Advanced manufacturing, I4.0 delivery system},
abstract = {Industry 4.0 technologies provide digital solutions for the automation of manufacturing. In circular economy-based models, the resources stay in the system as it experiences one of the 10 R (Refuse, Rethink, Reduce, Reuse, Repair, Refurbish, Remanufacture, Repurpose, Recycle, and Recover) processes. These 10 R processes require the development of advanced manufacturing capabilities; however, 10 R processes suffer from various challenges and can be effectively overcome through Industry 4.0 technological applications. Although literature has indicated the use of various Industry 4.0 technologies, little information is available about firms’ views on the degree of Industry 4.0 application in the 10 R based advanced manufacturing area and its ability to achieve sustainable development. The current study aspires to examine how great an effect Industry 4.0 adoption has on 10 R advanced manufacturing capabilities and its outcome on sustainable development under the moderating effect of an Industry 4.0 delivery system. Practice-based view and Dynamic capability view theories are used to conceptualise the theoretical model. The research team statistically validated the theoretical model considering 124 data points that were collected using an online survey with a structured questionnaire. The findings point out that the path degree of Industry 4.0 adoption and 10 R advanced manufacturing capabilities are statistically significant. 10 R advanced manufacturing capabilities are found to have a positive influence on sustainable development outcomes. Industry 4.0 delivery system has a moderating effect on the path degree of I4.0 implementation and 10 R advanced manufacturing capabilities. The study concludes with key take away points for managers.}
}
@article{HALEEM202171,
title = {Quality 4.0 technologies to enhance traditional Chinese medicine for overcoming healthcare challenges during COVID-19},
journal = {Digital Chinese Medicine},
volume = {4},
number = {2},
pages = {71-80},
year = {2021},
issn = {2589-3777},
doi = {https://doi.org/10.1016/j.dcmed.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2589377721000161},
author = {Abid Haleem and Mohd Javaid and Ravi Pratap Singh and Rajiv Suman},
keywords = {Quality 4.0, COVID-19, Digital healthcare, Traditional Chinese medicine (TCM), Industry 4.0, Quality revolution},
abstract = {The Quality 4.0 concept is derived from the industrial fourth revolution, i.e., Industry 4.0. Quality 4.0 is the future of quality, where new digital and disruptive technologies are used to maintain quality in organizations. It is also suitable for traditional Chinese medicine (TCM) to maintain quality. This quality revolution aims to improve industrial and service sectors’ quality by incorporating emerging technologies to connect physical systems with the natural world. The proposed digital philosophy can update and enhance the entire TCM treatment methodology to become more effective and attractive in the current competitive structure of the pharmaceutical and clinical industries. Thus, in healthcare, this revolution empowers quality treatment during the COVID-19 pandemic. There is a major requirement in healthcare to maintain the quality of medical tools, equipment, and treatment processes during a pandemic. Digital technologies can widely be used to provide innovative products and services with excellent quality for TCM. In this paper, we discuss the significant role of Quality 4.0 and how it can be used to maintain healthcare quality and fulfill challenges during the pandemic. Additionally, we discuss 10 significant applications of Quality 4.0 in healthcare during the COVID-19 pandemic. These technologies will provide unique benefits to maintain the quality of TCM throughout the treatment process. With Quality 4.0, quality can be maintained using innovative and advanced digital technologies.}
}
@article{LI2021,
title = {Entropy-based redundancy analysis and information screening},
journal = {Digital Communications and Networks},
year = {2021},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864821000985},
author = {Yang Li and Jiachen Yang and Jiabao Wen},
keywords = {Sampling, Quality assessment, Data mining, Low-shot, Few-shot},
abstract = {The ongoing data explosion introduced unprecedented challenges to the information security of communication networks. As images are one of the most commonly used information transmission carriers; therefore, their data redundancy analysis and screening are of great significance. However, most of the current research focus on the algorithm improvement of commonly used image datasets. Thus, we should consider an important question: Is there data redundancy in the open datasets? Considering the factors of model structures and data distribution to ensure the generalization, we conducted extensive experiments to compare the average accuracy based on few random data to the baseline accuracy based on all data. The results show serious data redundancy in the open datasets from different domains. For instance, with the aid of deep model, only 20% data can achieve more than 90% of the baseline accuracy. Further, we proposed a novel entropy-based information screening method, which outperforms the random sampling under many experimental conditions. In particular, considering 20% of data, for the shallow model, the improvement is approximately 10%, and for the deep model, the ratio to the baseline accuracy increases to greater than 95%. Moreover, this work can also serve as a new way of learning from a few valuable samples, compressing the size of existing datasets and guiding the construction of high-quality datasets in the future.}
}
@article{REIS2021107529,
title = {Data-centric process systems engineering: A push towards PSE 4.0},
journal = {Computers & Chemical Engineering},
volume = {155},
pages = {107529},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107529},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421003070},
author = {Marco S. Reis and Pedro M. Saraiva},
keywords = {Process systems engineering 4.0, Data-centric PSE, Data science, Industry 4.0, Artificial intelligence, Applied statistics},
abstract = {Process Systems Engineering (PSE) is now a mature field with a well-established body of knowledge, computational-oriented frameworks and methodologies designed and implemented for addressing chemical processes related problems spanning a wide range of scales in time and space. A common feature of many PSE approaches relies in their mostly deductive nature, based on a deep understanding of the underlying Chemical Engineering Science. Given the current data-intensive industrial and societal contexts, new sources of process or product information are now easily made available and should be exploited to complement and expand the classical PSE paradigm with inductive data-driven reasoning and knowledge discovery methodologies. In this article, based upon our over 25 years of research and teaching experience in the field, we discuss the scope and trends of this PSE evolution, refer to several relevant Data-Centric PSE approaches, and identify the main components, applications and future opportunities of this PSE 4.0 perspective.}
}
@article{JESSE20218,
title = {Data Strategy and Data Trust – Drivers for Business Development},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {8-12},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.409},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321018462},
author = {Norbert Jesse},
keywords = {Innovation Management, Intelligent Systems, Applications},
abstract = {Data is the new oil – which has often been heard since the end of the 90th. However, many companies took too long to exploit the new technological options for their business. It has been proven that the digital disruption led to seemingly stable and well-known brands disappearing from their market. Insufficient data competence is one of the reasons why companies failed in the escalating process of creative destruction. In this document we address three competence dimensions: data architecture, data preparation and the interchange of data. The competence in these fields is a precondition for a company’s decision-making process to lead successfully to profitability.}
}
@article{SIMONART202169,
title = {Epidemiologic evolution of common cutaneous infestations and arthropod bites: A Google Trends analysis},
journal = {JAAD International},
volume = {5},
pages = {69-75},
year = {2021},
issn = {2666-3287},
doi = {https://doi.org/10.1016/j.jdin.2021.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666328721000651},
author = {Thierry Simonart and Xuân-Lan {Lam Hoai} and Viviane {De Maertelaer}},
keywords = {bed bugs, head lice, infodemiology, pubic lice, scabies, ticks, Google Trends},
abstract = {Background
Common cutaneous infestations and arthropod bites are not reportable conditions in most countries. Their worldwide epidemiologic evolution and distribution are mostly unknown.
Objective
To explore the evolution and geographic distribution of common cutaneous infestations and arthropod bites through an analysis of Google Trends.
Methods
Search trends from 2004 through March 2021 for common cutaneous infestations and arthropod bites were extracted from Google Trends, quantified, and analyzed.
Results
Time series decomposition showed that total search term volume for pubic lice decreased worldwide over the study period, while the interest for ticks, pediculosis, insect bites, scabies, lice, and bed bugs increased (in increasing order). The interest for bed bugs was more pronounced in the former Union of Soviet Socialist Republics countries, interest for lice in Near East and Middle East countries, and interest for pubic lice in South American countries. Internet searches for bed bugs, insect bites, and ticks exhibited the highest seasonal patterns.
Limitations
Retrospective analysis limits interpretation.
Conclusion
Surveillance systems based on Google Trends may enhance the timeliness of traditional surveillance systems and suggest that, while most cutaneous infestations increase worldwide, pubic lice may be globally declining.}
}
@article{HUANG2021586,
title = {Toward a research framework to conceptualize data as a factor of production: The data marketplace perspective},
journal = {Fundamental Research},
volume = {1},
number = {5},
pages = {586-594},
year = {2021},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2021.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S2667325821001515},
author = {Lihua Huang and Yifan Dou and Yezheng Liu and Jinzhao Wang and Gang Chen and Xiaoyang Zhang and Runyin Wang},
keywords = {Data marketplace, Factor of production, Data lifecycle, Asset specificity},
abstract = {The widespread use of machine learning techniques and artificial intelligence algorithms has highlighted the strategic role of data. To acquire data for training algorithms and eventually empowering the digital transformation, data marketplaces are often required to support and coordinate cross-organizational data transactions. However, the prior industry practices have suggested that the transaction costs in the data marketplaces are severely high, and the supporting infrastructure is far from mature. This paper proposes a data attributes-affected data exchange (DADE) conceptual model to understand the challenges and directions for developing data marketplaces. Specifically, our model framework is built upon two dimensions, data lifecycle maturity and data asset specificity. Based on the DADE model, we propose four approaches for developing data marketplaces and discuss future research directions with an overview of computational methods as potential technical solutions.}
}
@article{GHANEM2021615,
title = {Comment on: High acquisition rate and internal validity in the Scandinavian Obesity Surgery Registry},
journal = {Surgery for Obesity and Related Diseases},
volume = {17},
number = {3},
pages = {615-617},
year = {2021},
issn = {1550-7289},
doi = {https://doi.org/10.1016/j.soard.2020.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1550728920306572},
author = {Omar M. Ghanem and Joseph N. Badaoui}
}
@article{LIM2021100013,
title = {State of data platforms for connected vehicles and infrastructures},
journal = {Communications in Transportation Research},
volume = {1},
pages = {100013},
year = {2021},
issn = {2772-4247},
doi = {https://doi.org/10.1016/j.commtr.2021.100013},
url = {https://www.sciencedirect.com/science/article/pii/S2772424721000135},
author = {Kai Li Lim and Jake Whitehead and Dongyao Jia and Zuduo Zheng},
keywords = {Connected mobility, Vehicular networks, Data platforms, Electric vehicles},
abstract = {The continuing expansion of connected and electro-mobility products and services has led to their ability to rapidly generate very large amounts of data, leading to a demand for effective data management solutions. This is further catalysed through the need for society to make informed policies and decisions that can properly support their emerging growth. While data systems and platforms exist, they are often proprietary, being only compatible to the products that they are designed for. Given the products and services generate energy and spatial-temporal data that can often correlate, a lack of interoperability between these systems would impede decision making, as data from each system must be considered independently. By studying currently available data platforms and frameworks, this paper weighs the problems that these products address, and identifies necessary gaps for a more cohesive platform to exist. This is performed through a top-down approach, whereby broader vehicle-to-everything approaches are first studied, before moving to the components that could comprise a data platform to integrate and ingest these various data feeds. Finally, potential design considerations for a data platform is presented, along with examples of application benefits that would enable users to make more informed and holistic decisions about current mobility options.}
}
@article{BABAR2021100992,
title = {Energy aware smart city management system using data analytics and Internet of Things},
journal = {Sustainable Energy Technologies and Assessments},
volume = {44},
pages = {100992},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.100992},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821000023},
author = {Muhammad Babar and Akmal Saeed Khattak and Mian Ahmad Jan and Muhammad Usman Tariq},
keywords = {Smart city, Internet of Things, Energy management, Data analytics},
abstract = {The rise of Internet of Things (IoT) concept founded the realization of a smart city. Energy management has lately turn out to be a vital concern for the services of a smart city as IoT devices consume massive energy constantly. This concern needs to be addressed to devise a practical method. Efficient energy utilization aims to promise smart city sustainability. Moreover, IoT devices produce enormous data that is required to be processed efficiently. In this article, a framework is proposed that assures the energy efficiency of IoT devices along with data analysis for cities. This article proposes a general design for smart city energy management that assures the energy efficiency of IoT devices along with data analysis. The proposed model includes three different components that are energy management, data processing, and service management. The energy management component is dependent on infrastructure optimization. The energy-efficient clustering, peak load shaving, optimized scheduling, and load balancing algorithms are integrated to achieve efficient energy management. The data processing is performed using distributed framework. The service management is performed using rules and thresholds. The experiments are performed using authentic datasets and the result highlights the efficiency of the proposed model.}
}
@article{MONEREOSANCHEZ2021118174,
title = {Quality control strategies for brain MRI segmentation and parcellation: Practical approaches and recommendations - insights from the Maastricht study},
journal = {NeuroImage},
volume = {237},
pages = {118174},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118174},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921004511},
author = {Jennifer Monereo-Sánchez and Joost J.A. {de Jong} and Gerhard S. Drenthen and Magdalena Beran and Walter H. Backes and Coen D.A. Stehouwer and Miranda T. Schram and David E.J. Linden and Jacobus F.A. Jansen},
keywords = {Brain segmentation, Cortical parcellation, FreeSurfer, Quality control, Manual editing, Outlier exclusion},
abstract = {Quality control of brain segmentation is a fundamental step to ensure data quality. Manual quality control strategies are the current gold standard, although these may be unfeasible for large neuroimaging samples. Several options for automated quality control have been proposed, providing potential time efficient and reproducible alternatives. However, those have never been compared side to side, which prevents consensus in the appropriate quality control strategy to use. This study aimed to elucidate the changes manual editing of brain segmentations produce in morphological estimates, and to analyze and compare the effects of different quality control strategies on the reduction of the measurement error. Structural brain MRI from 259 participants of The Maastricht Study were used. Morphological estimates were automatically extracted using FreeSurfer 6.0. Segmentations with inaccuracies were manually edited, and morphological estimates were compared before and after editing. In parallel, 12 quality control strategies were applied to the full sample. Those included: two manual strategies, in which images were visually inspected and either excluded or manually edited; five automated strategies, where outliers were excluded based on the tools “MRIQC” and “Qoala-T”, and the metrics “morphological global measures”, “Euler numbers” and “Contrast-to-Noise ratio”; and five semi-automated strategies, where the outliers detected through the mentioned tools and metrics were not excluded, but visually inspected and manually edited. In order to quantify the effects of each quality control strategy, the proportion of unexplained variance relative to the total variance was extracted after the application of each strategy, and the resulting differences compared. Manually editing brain surfaces produced particularly large changes in subcortical brain volumes and moderate changes in cortical surface area, thickness and hippocampal volumes. The performance of the quality control strategies depended on the morphological measure of interest. Overall, manual quality control strategies yielded the largest reduction in relative unexplained variance. The best performing automated alternatives were those based on Euler numbers and MRIQC scores. The exclusion of outliers based on global morphological measures produced an increase of relative unexplained variance. Manual quality control strategies are the most reliable solution for quality control of brain segmentation and parcellation. However, measures must be taken to prevent the subjectivity associated with these strategies. The detection of inaccurate segmentations based on Euler numbers or MRIQC provides a time efficient and reproducible alternative. The exclusion of outliers based on global morphological estimates must be avoided.}
}
@article{MARROQUINRIVERA2021110,
title = {Implementing a Redcap-based research data collection system for mental health},
journal = {Revista Colombiana de Psiquiatría (English ed.)},
volume = {50},
pages = {110-115},
year = {2021},
issn = {2530-3120},
doi = {https://doi.org/10.1016/j.rcpeng.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S2530312021000588},
author = {Arturo {Marroquin Rivera} and Juan Camilo Rosas-Romero and Sergio Mario Castro and Fernando Suárez-Obando and Jeny Aguilera-Cruz and Sophia Marie Bartels and Sena Park and William Chandler Torrey and Carlos Gómez-Restrepo},
keywords = {Data collection, REDCap, Mental health, Recolección de datos, REDCap, Salud mental},
abstract = {Background
The implementation of new technologies in medical research, such as novel big storage systems, has recently gained importance. Electronic data capture is a perfect example as it powerfully facilitates medical research. However, its implementation in resource-limited settings, where basic clinical resources, internet access, and human resources may be reduced might be a problem.
Methods
In this paper we described our approach for building a network architecture for data collection to achieve our objectives using a REDCap® tool in Colombia and provide guidance for data collection in similar settings.
Conclusions
REDCap is a feasible and efficient electronic data capture software to use in similar contexts to Colombia. The software facilitated the whole data management process and is a way to build research capacities in resourced-limited settings.
Resumen
Contexto
La implementación de nuevas tecnologías en la investigación médica, como los nuevos sistemas de gran almacenamiento de datos, recientemente han ganado importancia. El almacenamiento electrónico de datos es un ejemplo perfecto ya que facilita poderosamente la investigación médica. Sin embargo, su implementación en ambientes con recursos limitados, donde los recursos básicos clínicos, el acceso a internet y el recurso humano podrían ser reducidos, suponen un problema.
Métodos
En este artículo describimos nuestro acercamiento para construir una red arquitectónica para la recolección de datos, en aras de alcanzar nuestros objetivos mediante la utilización de la herramienta REDCap® en Colombia y proveer una guía para la recolección de datos en condiciones similares.
Conclusiones
REDCap es un software de almacenamiento electrónico de datos eficiente y encontramos que resulta posible su utilización en contextos similares a Colombia. Este software facilitó el proceso del manejo de los datos y es una manera de construir capacidades investigativas en contextos donde los recursos son limitados.}
}
@article{GOKALP2021527,
title = {Data-driven manufacturing: An assessment model for data science maturity},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {527-546},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001485},
author = {Mert Onuralp Gökalp and Ebru Gökalp and Kerem Kayabay and Altan Koçyiğit and P. Erhan Eren},
keywords = {Smart manufacturing, Industry 4.0, Maturity model, Data science, Maturity assessment, Process improvement},
abstract = {Today, data science presents immense opportunities by turning raw data into manufacturing intelligence in data-driven manufacturing that aims to improve operational efficiency and product quality together with reducing costs and risks. However, manufacturing firms face difficulties in managing their data science endeavors for reaping these potential benefits. Maturity models are developed to guide organizations by providing an extensive roadmap for improvement in certain areas. Therefore, this paper seeks to address this problem by proposing a theoretically grounded Data Science Maturity Model (DSMM) for manufacturing organizations to assess their existing strengths and weaknesses, perform a gap analysis, and draw a roadmap for continuous improvements in their progress towards data-driven manufacturing. DSMM comprises six maturity levels from “Not Performed” to” Innovating” and twenty-eight data science processes categorized under six headings: Organization, Strategy Management, Data Analytics, Data Governance, Technology Management, and Supporting. The applicability and usefulness of DSMM are validated through multiple case studies conducted in manufacturing organizations of various sizes, industries, and countries. The case study results indicate that DSMM is applicable in different settings and is able to reflect the organizations’ current data science maturity levels and provide significant insights to improve their data science capabilities.}
}
@incollection{2021295,
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {295-299},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221327000164}
}
@article{BELTRAMI2021127733,
title = {Industry 4.0 and sustainability: Towards conceptualization and theory},
journal = {Journal of Cleaner Production},
volume = {312},
pages = {127733},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.127733},
url = {https://www.sciencedirect.com/science/article/pii/S095965262101951X},
author = {Mirjam Beltrami and Guido Orzes and Joseph Sarkis and Marco Sartor},
keywords = {Industry 4.0, Fourth industrial revolution, Internet of things, Sustainability, Conceptual framework},
abstract = {Both Industry 4.0 and sustainability have gained momentum in the academic, managerial and policy debate. Despite the relevance of the topics, the relation between Industry 4.0 and sustainability – revealed by many authors – is still unclear; literature is fragmented. This paper seeks to overcome this limit by developing a systematic literature review of 117 peer-reviewed journal articles. After descriptive and content analyses, the work presents a conceptualization and theoretical framework. The paper contributes to both theory and practice by advancing current understanding of Industry 4.0 and sustainability, especially the impact of Industry 4.0 technologies on sustainability practices and performance.}
}
@article{DAI2021110480,
title = {Advanced battery management strategies for a sustainable energy future: Multilayer design concepts and research trends},
journal = {Renewable and Sustainable Energy Reviews},
volume = {138},
pages = {110480},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.110480},
url = {https://www.sciencedirect.com/science/article/pii/S1364032120307668},
author = {Haifeng Dai and Bo Jiang and Xiaosong Hu and Xianke Lin and Xuezhe Wei and Michael Pecht},
keywords = {Lithium-ion batteries, Battery management technologies, Multilayer design concepts, Safety and aging, Data and intelligence},
abstract = {Lithium-ion batteries are promising energy storage devices for electric vehicles and renewable energy systems. However, due to complex electrochemical processes, potential safety issues, and inherent poor durability of lithium-ion batteries, it is essential to monitor and manage batteries safely and efficiently. This study reviews the development of battery management systems during the past periods and introduces a multilayer design architecture for advanced battery management, which consists of three progressive layers. The foundation layer focuses on the system physical basis and theoretical principle, the algorithm layer aims at providing a comprehensive understanding of battery, and the application layer ensures a safe and efficient battery system through sufficient management. A comprehensive overview of each layer is presented from both academic and engineering perspectives. Future trends in research and development of next-generation battery management are discussed. Based on data and intelligence, the next-generation battery management will achieve better safety, performance, and interconnectivity.}
}
@article{GAN2021100845,
title = {Capturing the swarm intelligence in truckers: The foundation analysis for future swarm robotics in road freight},
journal = {Swarm and Evolutionary Computation},
volume = {62},
pages = {100845},
year = {2021},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2021.100845},
url = {https://www.sciencedirect.com/science/article/pii/S2210650221000067},
author = {Mi Gan and Qiujun Qian and Dandan Li and Yi Ai and Xiaobo Liu},
keywords = {Swarm Intelligence, Prediction, Trucker Trajectory, Machine learning},
abstract = {A group of individual truckers can be regarded as a swarm intelligence system without central management. With the development of autonomous driving technology, trucker groups will be replaced by driverless vehicles. At that point, a swarm of truckers will become a swarm robotics system. Therefore, considering the design and control of an efficient swarm robotics system, it is essential to investigate the properties and model the behaviors of a swarm of truckers in advance. In this study, we probe the characteristics of both individual truckers and a swarm of truckers using trajectory data of truckers. First, the trajectory data were map matched based on the geographic scale of cities and administrative regions. Then, the properties of the division of labor, pattern formation, and swarm synchronization were obtained through an analysis of the spatiotemporal distribution of radius of gyration, travel distance, and the number of visited places. Because predicting the next visit locations of individuals of a swarm is a measure for modeling swarm behaviors, the prediction model can be used to predict future swarm robotics (driverless trucks) behaviors. Thus, we apply several machine learning models to predict the next locations of truckers. The results show that there are common characteristics and routines embodied in the behavior of the truckers; the swarm shows consistency and regularity. Moreover, the peak predictability of the entire group reached 94%, indicating that our model can predict the behavior of groups and individuals. Our findings provide basis supporting to the future efficient swarm robotics system.}
}
@article{TEICHMANN2023102150,
title = {RegTech – Potential benefits and challenges for businesses},
journal = {Technology in Society},
volume = {72},
pages = {102150},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2022.102150},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X22002913},
author = {Fabian Teichmann and Sonia Boticiu and Bruno S. Sergi},
keywords = {RegTech, Benefits, Challenges, MiFID II, Companies, Compliance},
abstract = {The last Global Financial crisis in 2008 fuelled the need for regulation to avoid history repeating itself. New regulatory technologies (RegTech) are helping to transform how compliance issues are handled. This study aims to highlight the benefits of RegTech solutions for companies as well as the challenges of adopting these technologies. It also addresses the Markets in Financial Instruments Directive (MiFID) II legislation, which has led to an increase in the number of RegTech companies. Although these systems of technology offer compelling compliance tools, they also pose significant risks. For this reason, this study aims to draw attention to the opportunities and potential pitfalls associated with RegTech and propose recommendations for addressing the challenges related to implementing RegTech solutions.}
}
@article{MEI2021134,
title = {Effects of obstructive sleep apnoea severity on neurocognitive and brain white matter alterations in children according to sex: a tract-based spatial statistics study},
journal = {Sleep Medicine},
volume = {82},
pages = {134-143},
year = {2021},
issn = {1389-9457},
doi = {https://doi.org/10.1016/j.sleep.2020.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S1389945720303853},
author = {Lin Mei and Xiaodan Li and Guifei Zhou and Tingting Ji and Jun Chen and Zhifei Xu and Yun Peng and Yue Liu and Hongbin Li and Jie Zhang and Shengcai Wang and Yamei Zhang and Wentong Ge and Yongli Guo and Yue Qiu and Xinbei Jia and Jinghong Tian and Li Zheng and Jiangang Liu and Jun Tai and Xin Ni},
keywords = {Diffusion tensor imaging, Obstructive sleep apnoea, Children, Tract-based spatial statistics, White matter},
abstract = {Objectives
To investigate alterations in neurocognitive, attention, paediatric sleep questionnaire (PSQ) scores and whole brain white matter (WM) integrity between children with mild and severe obstructive sleep apnoea (OSA) according to sex and whether these changes are associated with OSA severity.
Methods
Fifty-seven children (36 males and 21 females) diagnosed with OSA were recruited for this study. Children of both sexes were divided into mild (male-MG, female-MG) and severe (male-SG, female-SG) groups according to OSA severity. Polysomnography (PSG), neurocognitive, attention and PSQ tests were compared between groups by one-way samples analysis of variance (ANOVA) F test. Diffusion tensor imaging (DTI) was scanned using a 3T GE MRI scanner and analysed by Tract-based Spatial Statistics (TBSS). Spearman correlation was calculated between DTI Eigenvalues and clinical characteristics.
Results
Compared to mild OSA patients, severe OSA patients presented greater severity of obstructive apnoea hypopnea index (OAHI), neurocognition, PSQ and attention tests in both male and female patients. Brain WM integrity in the male-SG, compared to the male-MG, demonstrated significantly reduced fractional anisotropy (FA) values in the right middle frontal gyrus and the right frontal sub-gyral regions and increased axial diffusivity (AD) values in the right inferior frontal gyrus, left parietal angular gyrus and sub-gyral regions, while no differences were found between the female-MG and female-SG. Alterations in male-SG brain regions were observably correlated with severity in male OSA patients.
Conclusions
The integrity of WM, which regulates autonomic, cognitive, and attention functions, is impaired in male, but not female, children with severe OSA.}
}
@article{LUO2021111224,
title = {An overview of data tools for representing and managing building information and performance data},
journal = {Renewable and Sustainable Energy Reviews},
volume = {147},
pages = {111224},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111224},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121005116},
author = {Na Luo and Marco Pritoni and Tianzhen Hong},
keywords = {Building information modeling, Ontology, Data schema, Metadata, Building performance data},
abstract = {Building information modeling (BIM) has been widely adopted for representing and exchanging building data across disciplines during building design and construction. However, BIM's use in the building operation phase is limited. With the increasing deployment of low-cost sensors and meters, as well as affordable digital storage and computing technologies, growing volumes of data have been collected from buildings, their energy services systems, and occupants. Such data are crucial to help decision makers understand what, how, and when energy is consumed in buildings—a critical step to improving building performance for energy efficiency, demand flexibility, and resilience. However, practical analyses and use of the collected data are very limited due to various reasons, including poor data quality, ad-hoc representation of data, and lack of data science skills. To unlock value from building data, there is a strong need for a toolchain to curate and represent building information and performance data in common standardized terminologies and schemas, to enable interoperability between tools and applications. This study selected and reviewed 24 data tools based on common use cases of data across the building life cycle, from design to construction, commissioning, operation, and retrofits. The selected data tools are grouped into three categories: (1) data dictionary or terminology, (2) data ontology and schemas, and (3) data platforms. The data are grouped into ten typologies covering most types of data collected in buildings. This study resulted in five main findings: (1) most data representation tools can represent their intended data typologies well, such as Green Button for smart meter data and Brick schema for metadata of sensors in buildings and HVAC systems, but none of the tools cover all ten types of data; (2) there is a need for data schemas to represent the basis of design data and metadata of occupant data; (3) standard terminologies such as those defined in BEDES are only adopted in a few data tools; (4) integrating data across various stages in the building life cycle remains a challenge; and (5) most data tools were developed and maintained by different parties for different purposes, their flexibility and interoperability can be improved to support broader use cases. Finally, recommendations for future research on building data tools are provided for the data and buildings community based on the FAIR principles to make data Findable, Accessible, Interoperable, and Reusable.}
}
@article{SCHROER2021526,
title = {A Systematic Literature Review on Applying CRISP-DM Process Model},
journal = {Procedia Computer Science},
volume = {181},
pages = {526-534},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.199},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002416},
author = {Christoph Schröer and Felix Kruse and Jorge Marx Gómez},
keywords = {CRISP-DM, Literature Review, Data Mining, Process Methodology, Deployment},
abstract = {CRISP-DM is the de-facto standard and an industry-independent process model for applying data mining projects. Twenty years after its release in 2000, we would like to provide a systematic literature review of recent studies published in IEEE, ScienceDirect and ACM about data mining use cases applying CRISP-DM. We give an overview of the research focus, current methodologies, best practices and possible gaps in conducting the six phases of CRISP-DM. The main findings are that CRISP-DM is still a de-factor standard in data mining, but there are challenges since the most studies do not foresee a deployment phase. The contribution of our paper is to identify best practices and process phases in which data mining analysts can be better supported. Further contribution is a template for structuring and releasing CRISP-DM studies.}
}
@article{YANG2021126442,
title = {Current advances and future challenges of AIoT applications in particulate matters (PM) monitoring and control},
journal = {Journal of Hazardous Materials},
volume = {419},
pages = {126442},
year = {2021},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2021.126442},
url = {https://www.sciencedirect.com/science/article/pii/S0304389421014072},
author = {Chao-Tung Yang and Ho-Wen Chen and En-Jui Chang and Endah Kristiani and Kieu Lan Phuong Nguyen and Jo-Shu Chang},
abstract = {Air pollution is at the center of pollution-control discussion due to the significant adverse health effects on individuals and the environment. Research has shown the association between unsafe environments and different sizes of particulate matter (PM), highlighting the importance of pollutant monitoring to mitigate its detrimental effect. By monitoring air quality with low-cost monitoring devices that collect massive observations, such as Air Box, a comprehensive collection of ground-level PM concentration is plausible due to the simplicity and low-cost, propelling applications in agriculture, aquaculture, and air quality, water resources, and disaster prevention. This paper aims to view IoT-based systems with low-cost microsensors at the sensor, network, and application levels, along with machine learning algorithms that improve sensor networks’ precision, providing better resolution. From the analysis at the three levels, we analyze current PM monitoring methods, including the use of sensors when collecting PM concentrations, demonstrate the use of IoT-based systems in PM monitoring and its challenges, and finally present the integration of AI and IoT (AIoT) in PM monitoring, indoor air quality control, and future directions. In addition, the inclusion of Taiwan as a site analysis was illustrated to show an example of AIoT in PM-control policy-making potential directions.}
}
@article{ZAHID2021104420,
title = {A systematic review of emerging information technologies for sustainable data-centric health-care},
journal = {International Journal of Medical Informatics},
volume = {149},
pages = {104420},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104420},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621000460},
author = {Arnob Zahid and Jennifer Kay Poulsen and Ravi Sharma and Stephen C. Wingreen},
keywords = {Emerging technologies, Data modelling, Data analytics, Data-centric health-care},
abstract = {Background
Of the Sustainable Development Goals (SDGs), the third presents the opportunity for a predictive universal digital healthcare ecosystem, capable of informing early warning, assisting in risk reduction and guiding management of national and global health risks. However, in reality, the existing technology infrastructure of digital healthcare systems is insufficient, failing to satisfy current and future data needs.
Objective
This paper systematically reviews emerging information technologies for data modelling and analytics that have potential to achieve Data-Centric Health-Care (DCHC) for the envisioned objective of sustainable healthcare. The goal of this review is to: 1) identify emerging information technologies with potential for data modelling and analytics, and 2) explore recent research of these technologies in DCHC.
Findings
A total of 1619 relevant papers have been identified and analysed in this review. Of these, 69 were probed deeply. Our analysis found that the extant research focused on elder care, rehabilitation, chronic diseases, and healthcare service delivery. Use-cases of the emerging information technologies included providing assistance, monitoring, self-care and self-management, diagnosis, risk prediction, well-being awareness, personalized healthcare, and qualitative and/or quantitative service enhancement. Limitations identified in the studies included vendor hardware specificity, issues with user interface and usability, inadequate features, interoperability, scalability, and compatibility, unjustifiable costs and insufficient evaluation in terms of validation.
Conclusion
Achievement of a predictive universal digital healthcare ecosystem in the current context is a challenge. State-of-the-art technologies demand user centric design, data privacy and protection measures, transparency, interoperability, scalability, and compatibility to achieve the SDG objective of sustainable healthcare by 2030.}
}
@article{ROSA2021106219,
title = {Using digital technologies in clinical trials: Current and future applications},
journal = {Contemporary Clinical Trials},
volume = {100},
pages = {106219},
year = {2021},
issn = {1551-7144},
doi = {https://doi.org/10.1016/j.cct.2020.106219},
url = {https://www.sciencedirect.com/science/article/pii/S1551714420302974},
author = {Carmen Rosa and Lisa A. Marsch and Erin L. Winstanley and Meg Brunner and Aimee N.C. Campbell},
keywords = {Clinical trials, Digital technology, Smartphones, Electronic health records, Artificial intelligence},
abstract = {In 2015, we provided an overview of the use of digital technologies in clinical trials, both as a methodological tool and as a mechanism to deliver interventions. At that time, there was limited guidance and limited use of digital technologies in clinical research. However, since then smartphones have become ubiquitous and digital health technologies have exploded. This paper provides an update to our earlier publication and an overview of how technology has been used in the past five years in clinical trials, providing examples with varying levels of technological integration and across different health conditions. Digital technology integration ranges from the incorporation of artificial intelligence in diagnostic devices to the use of real-world data (e.g., electronic health records) for study recruitment. Clinical trials can now be conducted entirely virtually, eliminating the need for in-person interaction. Much of the published research demonstrates how digital approaches can improve the design and implementation of clinical trials. While challenges remain, progress over the last five years is encouraging, and barriers can be overcome with careful planning.}
}
@article{LI2021104245,
title = {Review of tourism forecasting research with internet data},
journal = {Tourism Management},
volume = {83},
pages = {104245},
year = {2021},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2020.104245},
url = {https://www.sciencedirect.com/science/article/pii/S0261517720301710},
author = {Xin Li and Rob Law and Gang Xie and Shouyang Wang},
keywords = {Internet data, Tourism forecasting, Search engine, Social media, Systematic review},
abstract = {Internet techniques significantly influence the tourism industry and Internet data have been used widely used in tourism and hospitality research. However, reviews on the recent development of Internet data in tourism forecasting remain limited. This work reviews articles on tourism forecasting research with Internet data published in academic journals from 2012 to 2019. Then, the findings ae synthesized based on the following Internet data classifications: search engine, web traffic, social media, and multiple sources. Results show that among such classifications, search engine data are most widely incorporated into tourism forecasting. Time series and econometric forecasting models remain dominant, whereas artificial intelligence methods are still developing. For unstructured social media and multi-source data, methodological advancements in text mining, sentiment analysis, and social network analysis are required to transform data into time series for forecasting. Combined Internet data and forecasting models will help in improving forecasting accuracy further in future research.}
}
@incollection{PLOTKIN2021xvii,
title = {Introduction},
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {xvii-xxii},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00017-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221327000176},
author = {David Plotkin}
}
@article{CUI2021104726,
title = {Bidirectional cross-modality unsupervised domain adaptation using generative adversarial networks for cardiac image segmentation},
journal = {Computers in Biology and Medicine},
volume = {136},
pages = {104726},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104726},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521005205},
author = {Hengfei Cui and Chang Yuwen and Lei Jiang and Yong Xia and Yanning Zhang},
keywords = {Generative adversarial networks, Unsupervised domain adaptation, Cardiac segmentation, Self-attention mechanism, Knowledge distillation loss},
abstract = {Background
A novel Generative Adversarial Networks (GAN) based bidirectional cross-modality unsupervised domain adaptation (GBCUDA) framework is developed for cardiac image segmentation, which can effectively tackle the problem of network's segmentation performance degradation when adapting to the target domain without ground truth labels.
Method
GBCUDA uses GAN for image alignment, applies adversarial learning to extract image features, and gradually enhances the domain invariance of extracted features. The shared encoder performs an end-to-end learning task in which features that differ between the two domains complement each other. The self-attention mechanism is incorporated to the GAN network, which can generate details based on the prompts of all feature positions. Furthermore, spectrum normalization is implemented to stabilize the training of GAN, and knowledge distillation loss is introduced to process high-level feature-maps in order to better complete the cross-mode segmentation task.
Results
The effectiveness of our proposed unsupervised domain adaptation framework is tested over the Multi-Modality Whole Heart Segmentation (MM-WHS) Challenge 2017 dataset. The proposed method is able to improve the average Dice from 74.1% to 81.5% for the four cardiac substructures, and reduce the average symmetric surface distance (ASD) from 7.0 to 5.8 over CT images. For MRI images, our proposed framework trained on CT images gives the average Dice of 59.2% and reduces the average ASD from 5.7 to 4.9.
Conclusions
The evaluation results demonstrate our method's effectiveness on domain adaptation and the superiority to the current state-of-the-art domain adaptation methods.}
}