@article{ZAREJEDDI2022107476,
title = {Developing human biomonitoring as a 21st century toolbox within the European exposure science strategy 2020–2030},
journal = {Environment International},
volume = {168},
pages = {107476},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2022.107476},
url = {https://www.sciencedirect.com/science/article/pii/S0160412022004032},
author = {Maryam {Zare Jeddi} and Nancy B. Hopf and Henriqueta Louro and Susana Viegas and Karen S. Galea and Robert Pasanen-Kase and Tiina Santonen and Vicente Mustieles and Mariana F. Fernandez and Hans Verhagen and Stephanie K. Bopp and Jean Philippe Antignac and Arthur David and Hans Mol and Robert Barouki and Karine Audouze and Radu-Corneliu Duca and Peter Fantke and Paul Scheepers and Manosij Ghosh and An {Van Nieuwenhuyse} and Joana {Lobo Vicente} and Xenia Trier and Loïc Rambaud and Clémence Fillol and Sebastien Denys and André Conrad and Marike Kolossa-Gehring and Alicia Paini and Jon Arnot and Florian Schulze and Kate Jones and Ovnair Sepai and Imran Ali and Lorraine Brennan and Emilio Benfenati and Francesco Cubadda and Alberto Mantovani and Alena Bartonova and Alison Connolly and Jaroslav Slobodnik and Yuri {Bruinen de Bruin} and Jacob {van Klaveren} and Nicole Palmen and Hubert Dirven and Trine Husøy and Cathrine Thomsen and Ana Virgolino and Martin Röösli and Tim Gant and Natalie {von Goetz} and Jos Bessems},
keywords = {Human biomonitoring, Chemicals mixtures, Data governance, Zero Pollution Ambition, One substance-one assessment, Circular economy},
abstract = {Human biomonitoring (HBM) is a crucial approach for exposure assessment, as emphasised in the European Commission’s Chemicals Strategy for Sustainability (CSS). HBM can help to improve chemical policies in five major key areas: (1) assessing internal and aggregate exposure in different target populations; 2) assessing exposure to chemicals across life stages; (3) assessing combined exposure to multiple chemicals (mixtures); (4) bridging regulatory silos on aggregate exposure; and (5) enhancing the effectiveness of risk management measures. In this strategy paper we propose a vision and a strategy for the use of HBM in chemical regulations and public health policy in Europe and beyond. We outline six strategic objectives and a roadmap to further strengthen HBM approaches and increase their implementation in the regulatory risk assessment of chemicals to enhance our understanding of exposure and health impacts, enabling timely and targeted policy interventions and risk management. These strategic objectives are: 1) further development of sampling strategies and sample preparation; 2) further development of chemical-analytical HBM methods; 3) improving harmonisation throughout the HBM research life cycle; 4) further development of quality control / quality assurance throughout the HBM research life cycle; 5) obtain sustained funding and reinforcement by legislation; and 6) extend target-specific communication with scientists, policymakers, citizens and other stakeholders. HBM approaches are essential in risk assessment to address scientific, regulatory and societal challenges. HBM requires full and strong support from the scientific and regulatory domain to reach its full potential in public and occupational health assessment and in regulatory decision-making.}
}
@article{YANG2022104871,
title = {The framework of safety management on university laboratory},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {80},
pages = {104871},
year = {2022},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2022.104871},
url = {https://www.sciencedirect.com/science/article/pii/S0950423022001474},
author = {Jianfeng Yang and Shenqing Xuan and Yuanhao Hu and Xinyong Liu and Mingcheng Bian and Liangchao Chen and Siyun Lv and Pengchao Wang and Ru Li and Jianwen Zhang and Chi-Min Shu and Zhan Dou},
keywords = {University laboratory, Hazardous chemicals, safety management system, inherent safety process safety management},
abstract = {In recent years, with accidents in campus laboratories happening more frequently, and sporadically many personnel and property concerned are under serious threat. To address this issue, it has become a priority for universities in China to enhance safety management in the laboratory. To begin with, this study analyzed accidents that occurred in a university laboratory in the past ten years. The aim of this act was to conclude the problems in current safety management in the university laboratory. This was followed by exploring solutions and measures for those problems, making hazardous chemicals management a critical factor in campus laboratory safety management. With this idea, an assessment of hazardous chemicals of their holistic life-cycle on campus has been conducted, which realizes the information perception throughout its entire life-cycle on campus as well. Last but not least, a platform for safety and emergency management of hazardous chemicals has been built. This platform can act as a catalyst, effectively improving the regulatory levels of hazardous chemicals. Thus, the main responsibility for hazardous chemicals units can be further implemented. Through the construction of the safety management system for the full life-cycle of hazardous chemicals, the process of safety management of the university laboratory is fulfilled, and the inherent safety of the university laboratory is finally realized.}
}
@article{LI2022493,
title = {Emplacement ages of diamondiferous kimberlites in the Wafangdian District, North China Craton: New evidence from LA-ICP-MS U-Pb geochronology of andradite-rich garnet},
journal = {Gondwana Research},
volume = {109},
pages = {493-517},
year = {2022},
issn = {1342-937X},
doi = {https://doi.org/10.1016/j.gr.2022.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S1342937X22001617},
author = {Dongsheng Li and Zhongwei Wu and Xiaoming Sun and Song Shuai and Yu Fu and Dengfeng Li and Hongjun Chen and Yang Lu and Lubing Hong},
keywords = {Kimberlite, Andradite-rich garnet, LA-ICP-MS U-Pb dating, Wafangdian diamond deposit, North China Craton},
abstract = {This contribution presents new U-Pb geochronological data and attempts to elucidate the complex evolution history of various garnet types identified from two kimberlite pipes in the Wafangdian diamond mining district, southern Liaoning Province. These calcic garnets are dominated by andradite with relatively low proportions of schorlomite, grossular and pyrope. Abundant euhedral to subhedral, highly brecciated andradite phenocrysts hosted by LN30 “carbonatite-like” kimberlite samples yield a lower-intercept age of 459.3 ± 3.4 Ma, which is in excellent agreement with the previously reported phlogopite Ar-Ar (463.9 ± 0.9 Ma) and Rb-Sr ages (461.7 ± 4.8 Ma). Based on their trace element and C-O isotopic compositions of associated groundmass carbonate, we infer that these primary magmatic andradites probably originated from kimberlitic magmas. By comparison, three compositionally and texturally distinct groups of Ti-bearing andradites from LN42 hypabyssal kimberlites separately define three well-fitted regression lines with lower intercept ages at 581 ± 12 Ma, 414.9 ± 9.3 Ma and 292.0 ± 5.7 Ma, respectively. Relict andradite xenocrysts implies that ancient lower crust of the North China Craton (NCC) might have been affected by a significant but less-known tectonothermal event to varying degrees at ∼ 0.6 Ga. By contrast, fresh grains of magmatic Ti-andradites with chemical zoning produce a relatively young age of ∼ 415 Ma, which can still provide minimum age estimates for the most recent pulses of Paleozoic kimberlite magmatism in this study area. Noteworthily, a yet unrecognized local-scale hydrothermal alteration event at ∼ 292 Ma has been recorded in the texturally distinct population of secondary hydroandradites, whose age reported here for the first time is geologically meaningful. To sum up, this study further highlights andradite U-Pb dating as a potential robust geochronometer for constraining the late-stage evolution of kimberlite magmas as well as post emplacement hydrothermal alteration.}
}
@article{2022177,
title = {Diabetes mortality and trends before 25 years of age: an analysis of the Global Burden of Disease Study 2019},
journal = {The Lancet Diabetes & Endocrinology},
volume = {10},
number = {3},
pages = {177-192},
year = {2022},
issn = {2213-8587},
doi = {https://doi.org/10.1016/S2213-8587(21)00349-1},
url = {https://www.sciencedirect.com/science/article/pii/S2213858721003491},
author = {Ewerton Cousin and Bruce B Duncan and Caroline Stein and Kanyin Liane Ong and Theo Vos and Cristiana Abbafati and Mohsen Abbasi-Kangevari and Michael Abdelmasseh and Amir Abdoli and Rami Abd-Rabu and Hassan Abolhassani and Eman Abu-Gharbieh and Manfred Mario Kokou Accrombessi and Qorinah Estiningtyas Sakilah Adnani and Muhammad Sohail Afzal and Gina Agarwal and Krishna K Agrawaal and Marcela Agudelo-Botero and Bright Opoku Ahinkorah and Sajjad Ahmad and Tauseef Ahmad and Keivan Ahmadi and Sepideh Ahmadi and Ali Ahmadi and Ali Ahmed and Yusra {Ahmed Salih} and Wuraola Akande-Sholabi and Tayyaba Akram and Hanadi {Al Hamad} and Ziyad Al-Aly and Jacqueline Elizabeth Alcalde-Rabanal and Vahid Alipour and Syed Mohamed Aljunid and Rajaa M Al-Raddadi and Nelson Alvis-Guzman and Saeed Amini and Robert Ancuceanu and Tudorel Andrei and Catalina Liliana Andrei and Ranjit Mohan Anjana and Adnan Ansar and Ippazio Cosimo Antonazzo and Benny Antony and Anayochukwu Edward Anyasodor and Jalal Arabloo and Damian Arizmendi and Benedetta Armocida and Anton A Artamonov and Judie Arulappan and Zahra Aryan and Samaneh Asgari and Tahira Ashraf and Thomas Astell-Burt and Prince Atorkey and Maha Moh'd Wahbi Atout and Martin Amogre Ayanore and Ashish D Badiye and Atif Amin Baig and Mohan Bairwa and Jennifer L Baker and Ovidiu Constantin Baltatu and Palash Chandra Banik and Anthony Barnett and Mark Thomaz Ugliara Barone and Francesco Barone-Adesi and Amadou Barrow and Neeraj Bedi and Rebuma Belete and Uzma Iqbal Belgaumi and Arielle Wilder Bell and Derrick A Bennett and Isabela M Bensenor and David Beran and Akshaya Srikanth Bhagavathula and Sonu Bhaskar and Krittika Bhattacharyya and Vijayalakshmi S Bhojaraja and Ali Bijani and Boris Bikbov and Setognal Birara and Virginia Bodolica and Aime Bonny and Hermann Brenner and Nikolay Ivanovich Briko and Zahid A Butt and Florentino Luciano {Caetano dos Santos} and Luis Alberto Cámera and Ismael R Campos-Nonato and Yin Cao and Chao Cao and Ester Cerin and Promit Ananyo Chakraborty and Joht Singh Chandan and Vijay Kumar Chattu and Simiao Chen and Jee-Young Jasmine Choi and Sonali Gajanan Choudhari and Enayet Karim Chowdhury and Dinh-Toi Chu and Barbara Corso and Omid Dadras and Xiaochen Dai and Albertino Antonio Moura Damasceno and Lalit Dandona and Rakhi Dandona and Claudio Alberto Dávila-Cervantes and Jan-Walter {De Neve} and Edgar Denova-Gutiérrez and Deepak Dhamnetiya and Daniel Diaz and Sanam Ebtehaj and Hisham Atan Edinur and Sahar Eftekharzadeh and Iman {El Sayed} and Islam Y Elgendy and Muhammed Elhadi and Mohamed A Elmonem and Mohammed Faisaluddin and Umar Farooque and Xiaoqi Feng and Eduarda Fernandes and Florian Fischer and David Flood and Marisa Freitas and Peter Andras Gaal and Mohamed M Gad and Piyada Gaewkhiew and Lemma Getacher and Mansour Ghafourifard and Reza {Ghanei Gheshlagh} and Ahmad Ghashghaee and Nermin Ghith and Ghozali Ghozali and Paramjit Singh Gill and Ibrahim Abdelmageed Ginawi and Ekaterina Vladimirovna Glushkova and Mahaveer Golechha and Sameer Vali Gopalani and Rafael Alves Guimarães and Rajat Das Gupta and Rajeev Gupta and Vivek Kumar Gupta and Veer Bala Gupta and Sapna Gupta and Tesfa Dejenie Habtewold and Nima Hafezi-Nejad and Rabih Halwani and Asif Hanif and Graeme J Hankey and Shafiul Haque and Ahmed I Hasaballah and Syed Shahzad Hasan and Abdiwahab Hashi and Soheil Hassanipour and Simon I Hay and Khezar Hayat and Mohammad Heidari and Mohammad Bellal Hossain Hossain and Sahadat Hossain and Mostafa Hosseini and Soodabeh Hoveidamanesh and Junjie Huang and Ayesha Humayun and Rabia Hussain and Bing-Fang Hwang and Segun Emmanuel Ibitoye and Kevin S Ikuta and Leeberk Raja Inbaraj and Usman Iqbal and Md Shariful Islam and Sheikh Mohammed Shariful Islam and Rakibul M Islam and Nahlah Elkudssiah Ismail and Gaetano Isola and Ramaiah Itumalla and Masao Iwagami and Ihoghosa Osamuyi Iyamu and Mohammad Ali Jahani and Mihajlo Jakovljevic and Ranil Jayawardena and Ravi Prakash Jha and Oommen John and Jost B Jonas and Tamas Joo and Ali Kabir and Rohollah Kalhor and Ashwin Kamath and Tanuj Kanchan and Himal Kandel and Neeti Kapoor and Gbenga A Kayode and Sewnet Adem Kebede and Pedram Keshavarz and Mohammad Keykhaei and Yousef Saleh Khader and Himanshu Khajuria and Moien AB Khan and Md Nuruzzaman Khan and Maseer Khan and Amir M Khater and Tawfik Ahmed Muthafer Khoja and Jagdish Khubchandani and Min Seo Kim and Yun Jin Kim and Ruth W Kimokoti and Sezer Kisa and Adnan Kisa and Mika Kivimäki and Vladimir Andreevich Korshunov and Oleksii Korzh and Ai Koyanagi and Kewal Krishan and Barthelemy {Kuate Defo} and G Anil Kumar and Nithin Kumar and Dian Kusuma and Carlo {La Vecchia} and Ben Lacey and Anders O Larsson and Savita Lasrado and Wei-Chen Lee and Chiachi Bonnie Lee and Paul H Lee and Shaun Wen Huey Lee and Ming-Chieh Li and Stephen S Lim and Lee-Ling Lim and Giancarlo Lucchetti and Azeem Majeed and Ahmad Azam Malik and Borhan Mansouri and Lorenzo Giovanni Mantovani and Santi Martini and Prashant Mathur and Colm McAlinden and Nafiul Mehedi and Teferi Mekonnen and Ritesh G Menezes and Amanual Getnet Mersha and Junmei {Miao Jonasson} and Tomasz Miazgowski and Irmina Maria Michalek and Andreea Mirica and Erkin M Mirrakhimov and Agha Zeeshan Mirza and Prasanna Mithra and Abdollah Mohammadian-Hafshejani and Reza Mohammadpourhodki and Arif Mohammed and Ali H Mokdad and Mariam Molokhia and Lorenzo Monasta and Mohammad Ali Moni and Farhad Moradpour and Rahmatollah Moradzadeh and Ebrahim Mostafavi and Ulrich Otto Mueller and Christopher J L Murray and Ahmad Mustafa and Gabriele Nagel and Vinay Nangia and Atta Abbas Naqvi and Biswa Prakash Nayak and Javad Nazari and Rawlance Ndejjo and Ruxandra Irina Negoi and Sandhya {Neupane Kandel} and Cuong Tat Nguyen and Huong Lan Thi Nguyen and Jean Jacques Noubiap and Christoph Nowak and Bogdan Oancea and Oluwakemi Ololade Odukoya and Ayodipupo Sikiru Oguntade and Temitope T Ojo and Andrew T Olagunju and Obinna E Onwujekwe and Alberto Ortiz and Mayowa O Owolabi and Raffaele Palladino and Songhomitra Panda-Jonas and Seithikurippu R Pandi-Perumal and Shahina Pardhan and Tarang Parekh and Mojtaba Parvizi and Veincent Christian Filipino Pepito and Arokiasamy Perianayagam and Ionela-Roxana Petcu and Manju Pilania and Vivek Podder and Roman V Polibin and Maarten J Postma and Akila Prashant and Navid Rabiee and Mohammad Rabiee and Vafa Rahimi-Movaghar and Muhammad Aziz Rahman and Md. Mosfequr Rahman and Mosiur Rahman and Setyaningrum Rahmawaty and Nazanin Rajai and Pradhum Ram and Juwel Rana and Kamal Ranabhat and Priyanga Ranasinghe and Chythra R Rao and Satish Rao and Salman Rawaf and David Laith Rawaf and Lal Rawal and Andre M N Renzaho and Nima Rezaei and Aziz Rezapour and Seyed Mohammad Riahi and Daniela Ribeiro and Jefferson Antonio Buendia Rodriguez and Leonardo Roever and Peter Rohloff and Godfrey M Rwegerera and Paul MacDaragh Ryan and Maha Mohamed Saber-Ayad and Siamak Sabour and Basema Saddik and Sahar {Saeedi Moghaddam} and Amirhossein Sahebkar and Harihar Sahoo and KM Saif-Ur-Rahman and Hamideh Salimzadeh and Mehrnoosh Samaei and Juan Sanabria and Milena M Santric-Milicevic and Brijesh Sathian and Thirunavukkarasu Sathish and Markus P Schlaich and Abdul-Aziz Seidu and Mario Šekerija and Nachimuthu {Senthil Kumar} and Allen Seylani and Masood Ali Shaikh and Hina Shamshad and Md Shajedur Rahman Shawon and Sara Sheikhbahaei and Jeevan K Shetty and Rahman Shiri and K M Shivakumar and Kerem Shuval and Jasvinder A Singh and Ambrish Singh and Valentin Yurievich Skryabin and Anna Aleksandrovna Skryabina and Ahmad Sofi-Mahmudi and Amin Soheili and Jing Sun and Viktória Szerencsés and Miklós Szócska and Rafael Tabarés-Seisdedos and Hooman Tadbiri and Eyayou Girma Tadesse and Md. Tariqujjaman and Kavumpurathu Raman Thankappan and Rekha Thapar and Nihal Thomas and Binod Timalsina and Ruoyan Tobe-Gai and Marcello Tonelli and Marcos Roberto Tovani-Palone and Bach Xuan Tran and Jaya Prasad Tripathy and Lorainne {Tudor Car} and Biruk Shalmeno Tusa and Riaz Uddin and Era Upadhyay and Sahel {Valadan Tahbaz} and Pascual R Valdez and Tommi Juhani Vasankari and Madhur Verma and Victor E Villalobos-Daniel and Sergey Konstantinovitch Vladimirov and Bay Vo and Giang Thu Vu and Rade Vukovic and Yasir Waheed and Richard G Wamai and Andrea Werdecker and Nuwan Darshana Wickramasinghe and Andrea Sylvia Winkler and Befikadu Legesse Wubishet and Xiaoyue Xu and Suowen Xu and Seyed Hossein {Yahyazadeh Jabbari} and Hiroshi Yatsuya and Sanni Yaya and Taklo Simeneh Yazie Yazie and Siyan Yi and Naohiro Yonemoto and Ismaeel Yunusa and Siddhesh Zadey and Sojib Bin Zaman and Maryam Zamanian and Nelson Zamora and Mikhail Sergeevich Zastrozhin and Anasthasia Zastrozhina and Zhi-Jiang Zhang and Chenwen Zhong and Mohammad Zmaili and Alimuddin Zumla and Mohsen Naghavi and Maria Inês Schmidt},
abstract = {Summary
Background
Diabetes, particularly type 1 diabetes, at younger ages can be a largely preventable cause of death with the correct health care and services. We aimed to evaluate diabetes mortality and trends at ages younger than 25 years globally using data from the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2019.
Methods
We used estimates of GBD 2019 to calculate international diabetes mortality at ages younger than 25 years in 1990 and 2019. Data sources for causes of death were obtained from vital registration systems, verbal autopsies, and other surveillance systems for 1990–2019. We estimated death rates for each location using the GBD Cause of Death Ensemble model. We analysed the association of age-standardised death rates per 100 000 population with the Socio-demographic Index (SDI) and a measure of universal health coverage (UHC) and described the variability within SDI quintiles. We present estimates with their 95% uncertainty intervals.
Findings
In 2019, 16 300 (95% uncertainty interval 14 200 to 18 900) global deaths due to diabetes (type 1 and 2 combined) occurred in people younger than 25 years and 73·7% (68·3 to 77·4) were classified as due to type 1 diabetes. The age-standardised death rate was 0·50 (0·44 to 0·58) per 100 000 population, and 15 900 (97·5%) of these deaths occurred in low to high-middle SDI countries. The rate was 0·13 (0·12 to 0·14) per 100 000 population in the high SDI quintile, 0·60 (0·51 to 0·70) per 100 000 population in the low-middle SDI quintile, and 0·71 (0·60 to 0·86) per 100 000 population in the low SDI quintile. Within SDI quintiles, we observed large variability in rates across countries, in part explained by the extent of UHC (r2=0·62). From 1990 to 2019, age-standardised death rates decreased globally by 17·0% (−28·4 to −2·9) for all diabetes, and by 21·0% (–33·0 to −5·9) when considering only type 1 diabetes. However, the low SDI quintile had the lowest decline for both all diabetes (−13·6% [–28·4 to 3·4]) and for type 1 diabetes (−13·6% [–29·3 to 8·9]).
Interpretation
Decreasing diabetes mortality at ages younger than 25 years remains an important challenge, especially in low and low-middle SDI countries. Inadequate diagnosis and treatment of diabetes is likely to be major contributor to these early deaths, highlighting the urgent need to provide better access to insulin and basic diabetes education and care. This mortality metric, derived from readily available and frequently updated GBD data, can help to monitor preventable diabetes-related deaths over time globally, aligned with the UN's Sustainable Development Targets, and serve as an indicator of the adequacy of basic diabetes care for type 1 and type 2 diabetes across nations.
Funding
Bill & Melinda Gates Foundation.}
}
@article{KALOGIROU2022101716,
title = {Assessing and improving the National Interoperability Frameworks of European Union Member States: The case of Greece},
journal = {Government Information Quarterly},
volume = {39},
number = {3},
pages = {101716},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101716},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22000491},
author = {Victoria Kalogirou and Antonis Stasis and Yannis Charalabidis},
keywords = {Interoperability (IOP), European Interoperability Framework (EIF), Greek National Interoperability Framework (Greek NIF/eGIF), e-Government (e-Gov), Public Administration (PA), Once-Only Principle (OOP), Single Digital Gateway (SDG)},
abstract = {Interoperability (IOP) is the ability of a product or system – whose interfaces (APIs) are publicly documented – to connect to and operate with other products or systems, without restrictions. Interoperability further enables information and usable data to be properly exchanged and ensures the alignment of different business processes in critical sectors. In addition, is a prerequisite for transparent, domain-agnostic, and sustainable public sector digital services, where Public Administrations (PA) can efficiently interact across borders and domains by using common frameworks, standards, and processes for sharing information and data. The European Interoperability Framework (EIF) enables interoperability with guidelines for digital services. Therefore, the alignment with EIF becomes pivotal for the European Union (EU) countries since different regulations that facilitate and impose the implementation of European policies such as the Single Digital Gateway (SDG) regulation and the Once-Only Principle (OOP) consider the IOP a crucial technical and operational component for government digital services. This article proposes the update of the Greek NIF, with guidelines of EIF, OOP and other technological trends in conjunction with new legal and policy provisions. This proposed assessment methodology can be reused in other countries and can be further adapted for updating the EIF.}
}
@article{ACHIR2022103331,
title = {Service discovery and selection in IoT: A survey and a taxonomy},
journal = {Journal of Network and Computer Applications},
volume = {200},
pages = {103331},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103331},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521003167},
author = {Meriem Achir and Abdelkrim Abdelli and Lynda Mokdad and Jalel Benothman},
keywords = {Taxonomy, Service discovery, Service selection, IoT, QoS, QoE, Classification, Architecture, Object discovery},
abstract = {Recently, Internet has evolved into a new generation, called Internet of Things, thus enabling the connection between the physical and the digital worlds by creating an ubiquitous and self-organizing network. A huge number of smart objects are becoming now identifiable and addressable while being able to communicate with each other. Moreover, the integration of cloud infrastructures in the design of IoT, has moved this new trademark technology into a new dimension, enabling virtualisation and service provisioning. Billions of cloud services with different performance levels, requirements and functionalities are thus being offered in IoT, raising however the issues of their management, discovery and selection. In the literature, a considerable effort has been invested to address service discovery and selection in the context of IoT, despite the lack of standardization that meets the IoT requirements. In this paper, we propose an exhaustive taxonomy to classify service discovery approaches in the context of IoT, that we subsequently evaluate according to different aspects and criteria. Then, we discuss the gaps and advantages of each class of our taxonomy and locate the context and the requirements under which each can operate. Finally, we identify the challenges and future research directions in this domain.}
}
@article{MATTHESS2022100038,
title = {Supplier sustainability assessment in the age of Industry 4.0 – Insights from the electronics industry},
journal = {Cleaner Logistics and Supply Chain},
volume = {4},
pages = {100038},
year = {2022},
issn = {2772-3909},
doi = {https://doi.org/10.1016/j.clscn.2022.100038},
url = {https://www.sciencedirect.com/science/article/pii/S2772390922000117},
author = {Marcel Matthess and Stefanie Kunkel and Bing Xue and Grischa Beier},
keywords = {Supplier sustainability assessment, Sustainable supply chain management, Industry 4.0, Digital technologies, Supply chain transparency, Digitalization},
abstract = {Achieving transparency of the social and environmental impacts of industrial production poses significant obstacles for companies operating in complex global supply chains. They often do not possess sufficient information of other actors, especially at lower tiers in the supply chain. In recent years, data collection and information exchange in industry has been increasingly assisted by digital technologies, coining the term Industry 4.0. However, it remains largely unknown how companies try to foster transparency in their supply chains and how digital technologies are utilized for this purpose. In this study, we employ a qualitative, interview-based approach from both buyers’ and suppliers’ perspectives to investigate practices of supplier sustainability assessments in the electronics industry as well as their current and envisioned utilization of digital technologies. With regard to the exchange of sustainability-related information, we find that buying firms do not consistently check for the availability of digital interfaces to suppliers. Systematic and well-structured collection of such data is rare in suppliers, relying on manual self-assessments and lacking the means of automated data collection. This poses difficulties for buying firms to ensure validity of sustainability performance claims, highlighted by the fact that not all buying firms analyze suppliers’ self-assessments. To overcome such issues, ongoing industry-wide efforts of standardizing sustainability requirements should be extended to include strategic considerations of streamlining technology implementation to enhance data availability and validity.}
}
@incollection{ALSHORBAJI2022375,
title = {Chapter 16 - Health informatics in the Middle East and North Africa},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {375-397},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00029-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823413600029X},
author = {Najeeb Al-Shorbaji and Dari Alhuwail},
keywords = {Digital health, eHealth, Health systems, ICT, MENA, Middle East & North Africa},
abstract = {This chapter provides a background on the Middle East and North Africa as a diverse region from economic, social, political, and infrastructure points of view. This diversity has resulted into adoption of digital health in the countries of the Region based on health system’s structure and resources available. It provides highlights of the application of digital health in MENA countries ranging from the very simple appointment taking, to fully operational electronic health records systems, telemedicine services, and artificial intelligence. It provides an 8-point ‘Digital Health Innovations and Future Directions’ for a country, including commitment to develop and implement a national strategy and plan of digital health, ICT infrastructure, human resources development, funding, interoperability, priority setting, legal framework, and partnerships.}
}
@article{PARVEN2022103119,
title = {Impacts of disaster and land-use change on food security and adaptation: Evidence from the delta community in Bangladesh},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103119},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103119},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003387},
author = {Afshana Parven and Indrajit Pal and Apichon Witayangkurn and Malay Pramanik and Masahiko Nagai and Hiroyuki Miyazaki and Chanakan Wuthisakkaroon},
keywords = {Food security, Land-use change, Land-accretion, Socio-ecological system, Land ownership, Resilience etc},
abstract = {Climate-related disasters severely threaten the livelihoods and food security of millions of Bangladesh is living in deltaic areas. The study looked at shifting patterns of land use, the state of food security, and adaption mechanisms in pre-and post-disaster contexts to anticipate the future situation and its influence on livelihood. The study used Landsat 5 and 8 satellite images to evaluate land cover changes from 1990 to 2015. The study also used various interview tools to assess disaster impacts on land-use change, food security, and adaptive measures. In addition, logistic regression was used to determine land-use change factors and people's perceptions of disaster risk. Satellite image analysis in this study reveals significant positive changes in aquaculture and negative changes in fallow and agricultural land. The changes due to natural disasters considerably affect their socio-ecological system, family income, agriculture production, and out-migration in the study area. Land ownership has a substantial impact on food security. According to the study, disasters disrupt ecosystem services, agriculture production, and food security and impair people's capabilities. Human activity and land accretion are the major physical driving forces influencing land use and land cover changes. People's risk perceptions should be factored into disaster management planning at the local level; this could lead to a better understanding of the policy requirements for reducing food insecurity and poverty in deltaic Bangladesh, as well as strengthening community resilience.}
}
@article{KOVALCHUK2022104013,
title = {Three-stage intelligent support of clinical decision making for higher trust, validity, and explainability},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {104013},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104013},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000296},
author = {Sergey V. Kovalchuk and Georgy D. Kopanitsa and Ilia V. Derevitskii and Georgy A. Matveev and Daria A. Savitskaya},
keywords = {Clinical decision support, Predictive modeling, Interpretable machine learning, Personalized medicine, Machine learning, Diabetes mellitus},
abstract = {The paper presents a conceptual framework for building practically applicable clinical decision support systems (CDSSs) using data-driven (DD) predictive modelling. With the proposed framework we have tried to fill the gap between experimental CDSS implementations widely covered in the literature and solutions acceptable by physicians in daily practice. The framework is based on a three-stage approach where DD model definition is accomplished with practical norms referencing (scales, clinical recommendations, etc.) and explanation of the prediction results and recommendations. The approach is aimed at increasing the applicability of CDSSs based on DD models through better integration into decision context and higher explainability. The approach has been implemented in software solutions and tested within a case study in type 2 diabetes mellitus (T2DM) prediction, enabling us to improve known clinical scales (such as FINDRISK) while keeping the problem-specific reasoning interface similar to existing applications. A survey was performed to assess and investigate the acceptance level and provide insights on the influences of the introduced framework’s element on physicians’ behavior.}
}
@incollection{WEVILL202231,
title = {Chapter 2 - Relative performance of support vector machine, decision trees, and random forest classifiers for predicting production success in US unconventional shale plays},
editor = {Shuvajit Bhattacharya and Haibin Di},
booktitle = {Advances in Subsurface Data Analytics},
publisher = {Elsevier},
pages = {31-62},
year = {2022},
isbn = {978-0-12-822295-9},
doi = {https://doi.org/10.1016/B978-0-12-822295-9.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222959000078},
author = {Jessica Wevill and Alex Bromhead and Kate Evans and Jeffrey Yarus and Cédric M. John},
keywords = {Decision tree, Machine learning, Random forest, SGD-SVM, Shale plays},
abstract = {Unconventional shale reservoirs have revolutionized the energy industry. However, the prediction of production based on reservoir geology characterization has largely focused on sweet spot definition rather than on over-arching production trends across multiple plays. This study uses machine learning (ML) techniques to analyze the relationships between well log data and production success within seven North American shale plays. Three ML algorithms were evaluated: stochastic gradient descent kernel trained support-vector machine (SGD-SVM), decision tree (DT), and random forest (RF) classifier. Accuracy of predictions using the SGD-SVM and DT classifiers did not exceed 55%. A fine-tuned RF classifier is the most successful method at predicting well success based on normalized initial production, with an accuracy of 97%. To achieve this result, the RF is trained on the following input features: average play thickness, pore pressure, TVD, and resource concentration. The main factors impacting performance of our algorithm when trying to predict success in unconventional plays are previous understanding of heterogeneities in individual formations, and consistency of data availability across multiple wells. Despite challenges, ML and the RF method in particular show promising applications in the unconventional petroleum industry as a means to streamline production and data collection.}
}
@article{CHENG2022102398,
title = {How companies configure digital innovation attributes for business model innovation? A configurational view},
journal = {Technovation},
volume = {112},
pages = {102398},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2021.102398},
url = {https://www.sciencedirect.com/science/article/pii/S0166497221001796},
author = {Cong Cheng and Limin Wang},
keywords = {DI attributes, IT infrastructure Capability, Business model innovation, fsQCA},
abstract = {Digital innovation (DI) has garnered considerable attention across a broad array of literatures. However, empirical evidence about the impact of DI on business model innovation (BMI) is sparse with frameworks probably not fully capturing the complexities of DI attributes. We use the fuzzy-set approach by conducting qualitative comparative analysis (fsQCA) to explore the configurations of DI attributes and IT infrastructure capability that exist among the 167 manufacturing and service firms in China that undergoes different levels of BMI. Results reveal that DI attributes work in three pathways to promote BMI, including organization-oriented with emphasis on improvisation, product-oriented by focusing on user experience and value proposition, and product-organization complemented. When to additionally include IT infrastructure capability for deeper analysis, results demonstrate three more representative configurations among DI attributes and IT infrastructure capability to facilitate BMI: IT-enabled organization-product orchestration, IT-enabled product-organization orchestration, and IT-dominated orchestration. This research contributes to DI literature by uncovering the configurations among DI attributes in promoting BMI, and clarifying the complicated interacting effects between DI attributes and IT infrastructure capability to facilitate BMI.}
}
@article{MERCIER2022119438,
title = {Advances in human intracranial electroencephalography research, guidelines and good practices},
journal = {NeuroImage},
volume = {260},
pages = {119438},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119438},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922005559},
author = {Manuel R. Mercier and Anne-Sophie Dubarry and François Tadel and Pietro Avanzini and Nikolai Axmacher and Dillan Cellier and Maria Del Vecchio and Liberty S. Hamilton and Dora Hermes and Michael J. Kahana and Robert T. Knight and Anais Llorens and Pierre Megevand and Lucia Melloni and Kai J. Miller and Vitória Piai and Aina Puce and Nick F Ramsey and Caspar M. Schwiedrzik and Sydney E. Smith and Arjen Stolk and Nicole C. Swann and Mariska J Vansteensel and Bradley Voytek and Liang Wang and Jean-Philippe Lachaux and Robert Oostenveld},
keywords = {Intracranial recording in humans, Stereotactic electroencephalography, sEEG, Electrocorticogram, ECoG, Good research practice},
abstract = {Since the second half of the twentieth century, intracranial electroencephalography (iEEG), including both electrocorticography (ECoG) and stereo-electroencephalography (sEEG), has provided an intimate view into the human brain. At the interface between fundamental research and the clinic, iEEG provides both high temporal resolution and high spatial specificity but comes with constraints, such as the individual's tailored sparsity of electrode sampling. Over the years, researchers in neuroscience developed their practices to make the most of the iEEG approach. Here we offer a critical review of iEEG research practices in a didactic framework for newcomers, as well addressing issues encountered by proficient researchers. The scope is threefold: (i) review common practices in iEEG research, (ii) suggest potential guidelines for working with iEEG data and answer frequently asked questions based on the most widespread practices, and (iii) based on current neurophysiological knowledge and methodologies, pave the way to good practice standards in iEEG research. The organization of this paper follows the steps of iEEG data processing. The first section contextualizes iEEG data collection. The second section focuses on localization of intracranial electrodes. The third section highlights the main pre-processing steps. The fourth section presents iEEG signal analysis methods. The fifth section discusses statistical approaches. The sixth section draws some unique perspectives on iEEG research. Finally, to ensure a consistent nomenclature throughout the manuscript and to align with other guidelines, e.g., Brain Imaging Data Structure (BIDS) and the OHBM Committee on Best Practices in Data Analysis and Sharing (COBIDAS), we provide a glossary to disambiguate terms related to iEEG research.}
}
@article{QIN2022100326,
title = {A Segmented PageRank-Based Value Compensation Method for Personal Data in Alliance Blockchains},
journal = {Big Data Research},
volume = {30},
pages = {100326},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2022.100326},
url = {https://www.sciencedirect.com/science/article/pii/S221457962200020X},
author = {Chaoxia Qin and Bing Guo and Yun Zhang and Omar Cheikhrouhou and Yan Shen and Zhen Zhang and Hong Su},
keywords = {Alliance blockchains, Personal data, Value compensation, Markov model, Segmented PageRank (SPR)},
abstract = {Alliance blockchains provide a multi-party trusted data trading environment, promoting the development of the data trading market in which the value compensation for personal data is still a key issue. However, limited by the data format and content, traditional attempts on data value compensation cannot form a widely applicable solution. Therefore, we propose a universal value compensation method for personal data in alliance blockchains. The basic idea of this method is to evaluate the value weight of data based on the collaborative relationship of data value. First, we construct a Data Collaboration Markov Model (DCMM) to formalize the collaboration network of data value. Then, aiming at data collaboration networks with different structures, the corresponding Segmented PageRank (SPR) algorithm is proposed. SPR can universally evaluate the value weight of each data account without being subjected to the data format or content. Finally, we theoretically deduce that the time complexity and space complexity of SPR algorithm are respectively 1/K and 1/K2 taken by PageRank algorithm. Experiments show the feasibility and superior performance of SPR.}
}
@article{ZHANG2022120182,
title = {Grid-connected photovoltaic battery systems: A comprehensive review and perspectives},
journal = {Applied Energy},
volume = {328},
pages = {120182},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120182},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922014398},
author = {Yijie Zhang and Tao Ma and Hongxing Yang},
keywords = {Photovoltaic battery system, System sizing, Strategy improvement, Model predictive control, Demand-side management, PV curtailment},
abstract = {Due to the target of carbon neutrality and the current energy crisis in the world, green, flexible and low-cost distributed photovoltaic power generation is a promising trend. With battery energy storage to cushion the fluctuating and intermittent photovoltaic (PV) output, the photovoltaic battery (PVB) system has been getting increasing attention. This study is conducted to comprehensively review the PVB system studies with experimental and simulation studies, concerning mathematical modelling, system simulation, evaluation, capacity and operation strategy optimization. The system profitability, storage system choice and grid influence are also discussed, with the multi-objective optimization in large-scale systems with various participants expected as the future trend. In addition, several highlights of this topic are discussed in detail, including model predictive control, demand-side management, community energy storage system, peer-to-peer energy market scheme and mutual impacts on grid and system, emphasizing system flexibility, optimization complexity, large-scale simulation and the utility grid influences. Furthermore, some challenges and perspectives for future research are presented, with respect to DC distribution system revolution, large-scale vehicle to grid designs and impacts, multi-energy network operation and participation in the carbon transaction market. It is expected that this study could provide useful references and suggestions for researchers in this field of system design and power management of distributed solar PV.}
}
@article{GIORDANO2022111475,
title = {On the use of artificial intelligence to deal with privacy in IoT systems: A systematic literature review},
journal = {Journal of Systems and Software},
volume = {193},
pages = {111475},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111475},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001613},
author = {Giammaria Giordano and Fabio Palomba and Filomena Ferrucci},
keywords = {Data privacy, Artificial intelligence, Internet-of-Things, Software engineering for IoT},
abstract = {The Internet of Things (IoT) refers to a network of Internet-enabled devices that can make different operations, like sensing, communicating, and reacting to changes arising in the surrounding environment. Nowadays, the number of IoT devices is already higher than the world population. These devices operate by exchanging data between them, sometimes through an intermediate cloud infrastructure, and may be used to enable a wide variety of novel services that can potentially improve the quality of life of billions of people. Nonetheless, all that glitters is not gold: the increasing adoption of IoT comes with several privacy concerns due to the lack or loss of control over the sensitive data exchanged by these devices. This represents a key challenge for software engineering researchers attempting to address those privacy concerns by proposing (semi-)automated solutions to identify sources of privacy leaks. In this respect, a notable trend is represented by the adoption of smart solutions, that is, the definition of techniques based on artificial intelligence (AI) algorithms. This paper proposes a systematic literature review of the research in smart detection of privacy concerns in IoT devices. Following well-established guidelines, we identify 152 primary studies that we analyze under three main perspectives: (1) What are the privacy concerns addressed with AI-enabled techniques; (2) What are the algorithms employed and how they have been configured/validated; and (3) Which are the domains targeted by these techniques. The key results of the study identified six main tasks targeted through the use of artificial intelligence, like Malware Detection or Network Analysis. Support Vector Machine is the technique most frequently used in literature, however in many cases researchers do not explicitly indicate the domain where to use artificial intelligence algorithms. We conclude the paper by distilling several lessons learned and implications for software engineering researchers.}
}
@article{SHANKAR2022541,
title = {Digital marketing communication in global marketplaces: A review of extant research, future directions, and potential approaches},
journal = {International Journal of Research in Marketing},
volume = {39},
number = {2},
pages = {541-565},
year = {2022},
issn = {0167-8116},
doi = {https://doi.org/10.1016/j.ijresmar.2021.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167811621000720},
author = {Venkatesh Shankar and Dhruv Grewal and Sarang Sunder and Beth Fossen and Kay Peters and Amit Agarwal},
keywords = {Digital, Marketing communication, Global, International, Social media, Mobile, Internet, B2C, B2B, C2C, C2B},
abstract = {Digital marketing communication, that is, communication through digital or electronic media among businesses and consumers, is growing rapidly, especially during the COVID-19 era. We propose a framework for analyzing digital marketing communication along four major dyads, business-to-consumer (B2C), business-to-business (B2B), consumer-to-consumer (C2C), and consumer-to-business (C2B). We review and summarize, for researchers and practitioners, the literature during 2000–2021 in these dyads along four major components: goals; channels, media, and platforms; content; and responses. We find that extant research in digital marketing communication pertains mostly to a specific, national level rather than a global level, despite the porousness of national boundaries for digital marketing. We derive important insights, identify key research gaps and questions in each of the dyads along these dimensions. We suggest approaches to address these research questions under three major components: substantive issues, data, and methods. These approaches can offer the insights that managers need to better formulate digital marketing strategies in local and global contexts.}
}
@incollection{2022727,
title = {Index},
editor = {Thomas Mehner and Klement Tockner},
booktitle = {Encyclopedia of Inland Waters (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {727-768},
year = {2022},
isbn = {978-0-12-822041-2},
doi = {https://doi.org/10.1016/B978-0-12-819166-8.09973-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191668099734}
}
@article{DUC2022115927,
title = {An ensemble deep learning for automatic prediction of papillary thyroid carcinoma using fine needle aspiration cytology},
journal = {Expert Systems with Applications},
volume = {188},
pages = {115927},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115927},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421012811},
author = {Nguyen Thanh Duc and Yong-Moon Lee and Jae Hyun Park and Boreom Lee},
keywords = {Papillary thyroid carcinoma, Fine needle aspiration cytology, Computer-aided diagnosis, Deep CNN models, Ensemble learning, ThinPrep},
abstract = {Accurately cytopathological diagnosis of Papillary Thyroid Carcinoma (PTC) is of importance for reducing costs and increasing efficiency of treatments. In this paper, we pursue that goal by introducing artificial intelligence (AI) for automatic classification of malignant PTC cell clusters from Fine Needle Aspiration Cytology (FNAC) processed by ThinPrep. High-resolution cytological images obtained with a 40 × objective lens digital camera attached to an Olympus microscope were segmented into fragments and then divided into training, validation, and testing subsets. Fragments are non-overlapped patches containing only regions-of-interest that cover informative tissue structures for making proper diagnoses. Deep learning CNN models were pre-trained and fine-tuned on large-scale ImageNet domain before they were re-trained on cytology fragments. Moreover, we proposed a method to compute certainty of the patient-level prediction that undoubtedly provides additional evidence for reliability and confidence of the prediction. Results showed that the best classification performance on digital FNAC images achieved using DenseNet161, obtaining a mean accuracy of 0.9556 (p < 0.01), a mean sensitivity of 0.9734, and a mean specificity of 0.9405 on yet-to-be-seen test-set. Ensemble learning findings suggested combinations of AdaBoost classifier with multiple CNN models boosted predictive performances, up to 0.9971 accuracy. Moreover, stain normalization introduced by Reinhard increased the predictive ability, outperforming histogram specification, and Macenko methods. Presented findings demonstrate deep learning can integrate into computer-aided diagnosis systems to support cytopathologists in accurate diagnosis of PTC.}
}
@article{CHONG2022194,
title = {Bridging unlinkability and data utility: Privacy preserving data publication schemes for healthcare informatics},
journal = {Computer Communications},
volume = {191},
pages = {194-207},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S014036642200144X},
author = {Kah Meng Chong and Amizah Malip},
keywords = {Healthcare, Privacy, Utility, Anonymization, Unlinkability},
abstract = {Publishing patient data without revealing their sensitive information is one of the challenging research issues in the healthcare sector. Patient records contain useful information that is often released to healthcare industries and government institutions to support medical and census research. There are several existing privacy models in protecting healthcare data privacy, which are mainly built upon the anonymity of patients. In this paper, we incorporate unlinkability in the context of healthcare data publication, where two new privacy notions namely identity unlinkability and attribute unlinkability are introduced. We design two schemes using the proposed models to address identity disclosure and attribute disclosure problems in publishing healthcare data. Experimental results on real and synthetic datasets show that our schemes efficiently achieve data utility preservation and privacy protection simultaneously.}
}
@article{LANDOLSI2022674,
title = {Medical Named Entity Recognition using Surrounding Sequences Matching},
journal = {Procedia Computer Science},
volume = {207},
pages = {674-683},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.122},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010031},
author = {Mohamed Yassine Landolsi and Lotfi Ben Romdhane and Lobna Hlaoua},
keywords = {Medical text mining, named entity recognition, machine learning, information extraction, electronic medical records},
abstract = {Since the development of information technologies, there is a huge amount of electronic documents that was written by medical specialists and are rich of useful information needed to make critical decisions in several medical tasks. Thus, a doctor must have a big knowledge and he is responsible for every decision he takes for patients. In fact, the doctor should read, with full concentration, many electronic narrative documents to collect the necessary information. Unfortunately, it's too tiring to read all necessary information about drugs, diseases and patient due to the large amount of documents that are increasing every day. Consequently, so many medical errors can happen and even can cause fatalities. On the other hand, information extraction is such a good field that can handle this problem. One of the most important main task in this field is the Named Entity Recognition (NER) and its role was to identify the medical named entities, such as drug, disease or treatment, from an unstructured text written in natural language. However, in order to treat the narrative text, natural language tasks should be performed before NER. In our paper, we introduce a named entity recognition method, called NESSMa (Named Entity tagging by Surrounding Sequence Matching), and based on sequence tagging, it is able to annotate the words of a sentence using Bidirectional Long Short-Term Memory neural network with Conditional Random Field (BiLSTM-CRF) model. We pass the Bidirectional Encoder Representations from Transformers (BERT) word embedding as feature together with the Part of Speech (PoS) of the word and the cue sequence information. The cue sequence information indicates if a word belongs to a named entity surrounding sequence based on word edit distance. For that, we have automatically constructed a dictionary of named entities’ surrounding sequences for each entity type using a train set. As expected, experiments shows that adding the cue sequence information is able to improve the results according to F1-measure and outperform state-of-the-art methods.}
}
@incollection{BEJA202267,
title = {Chapter Two - Data services in ocean science with a focus on the biology∗},
editor = {Giuseppe Manzella and Antonio Novellino},
booktitle = {Ocean Science Data},
publisher = {Elsevier},
pages = {67-129},
year = {2022},
isbn = {978-0-12-823427-3},
doi = {https://doi.org/10.1016/B978-0-12-823427-3.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234273000062},
author = {Joana Beja and Leen Vandepitte and Abigail Benson and Anton {Van de Putte} and Dan Lear and Daphnis {De Pooter} and Gwenaëlle Moncoiffé and John Nicholls and Nina Wambiji and Patricia Miloslavich and Vasilis Gerovasileiou},
keywords = {Data challenges, Data services, Essential variables, Historical data, Indigenous knowledge, Marine biodiversity, Marine data management, Research data life cycle},
abstract = {Biological ocean science has a long history; it goes back millennia, whereas the related data services have emerged in the recent digital era of the past decades. To understand where we come from—and why data services are so important—we will start by taking you back to the rise in the study of marine biology—marine biodiversity—and its key players, before immersing ourselves in the data life cycle, past and present joint global initiatives, and systems that allow(ed) scientists to more easily access biological data, online services through some simple keyboard strokes, and the many challenges we still encounter on a daily basis when dealing with these types of data.}
}
@incollection{BRYANT202284,
title = {7.05 - Remote Sensing of Aeolian Processes},
editor = {John (Jack) F. Shroder},
booktitle = {Treatise on Geomorphology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {84-119},
year = {2022},
isbn = {978-0-12-818235-2},
doi = {https://doi.org/10.1016/B978-0-12-818234-5.00132-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128182345001322},
author = {Robert G. Bryant and Matthew C. Baddock},
keywords = {Aeolian processes, Bedform, Coastal dune, Dune morphology, Dunefield, Dust source, Geospatial data, Lidar, Remote camera, Remote sensing, Structure from motion, Unpiloted aerial vehicle},
abstract = {This review focuses on recent advances that have taken place in the use of remote sensing to observe aeolian processes, and to highlight recent approaches that have enabled and been employed to observe and quantify aeolian processes at a range of scales. As remote technologies continue to develop, the review emphasizes the significance that, in their different forms, these data are applicable across all scales at which aeolian processes operate. To address this, the review examines a range of space-borne, airborne and near-surface technologies.}
}
@article{LENFLE2022104455,
title = {Project-oriented agency and regeneration in socio-technical transition: Insights from the case of numerical weather prediction (1978–2015)},
journal = {Research Policy},
volume = {51},
number = {3},
pages = {104455},
year = {2022},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2021.104455},
url = {https://www.sciencedirect.com/science/article/pii/S004873332100247X},
author = {Sylvain Lenfle and Jonas Söderlund},
keywords = {Socio-technical transition, Regeneration, Multi-level perspective, Reverse salient, Project-oriented agency, Data assimilation, Weather forecasting},
abstract = {This paper analyzes the unfolding of socio-technical transition (STT) using the multi-level perspective (MLP) framework. It relies on an in-depth case study of the “quiet revolution” of numerical weather prediction. The study reveals how key actors targeted the reverse salient of data assimilation and thereby facilitated the transition toward a new “variational” regime. In so doing, the paper makes three contributions to the STT literature: (1) it identifies a new type of transition pathway, “regeneration,” in which the regime transforms itself from within, despite the lack of changes in landscape pressure, to overcome internal tensions; (2) it showcases “project-oriented agency” as the central mechanism of this transition, which allows the actors to join forces and cooperate to counteract the reverse salient; and (3) it proposes a process model of project-oriented agency that accounts for the role of the reverse salient in the regeneration pathway.}
}
@article{KOSEOGLU2022316,
title = {Relational bibliometrics for hospitality and tourism research: A best practice guide},
journal = {Journal of Hospitality and Tourism Management},
volume = {52},
pages = {316-330},
year = {2022},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2022.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S1447677022001243},
author = {Mehmet Ali Koseoglu and Melissa Yan {Yee Yick} and Brian King and Hasan Evrim Arici},
keywords = {Bibliometrics, Co-citation, Co-word, Co-authorship, Tourism, Hospitality},
abstract = {Noting the growing literature on relational bibliometrics and prevailing methodological challenges in hospitality and tourism research – inadequate bibliometric-focused structure and methodological transparency – this study contributes to knowledge about applicable analytical procedures. The authors uncover methodological issues by content analyzing 85 relational bibliometric articles published in 19 hospitality and tourism journals. The findings provide a basis for best practice recommendations. Four guiding principles are proposed for the scholarly deployment of relational bibliometrics, namely: (1) using multiple relational techniques to ensure a rich and comprehensive coverage of the pertinent field, (2) providing sufficient methodological disclosure, particularly language selection, data extraction from the applicable sampling, data cleaning and supplemental materials provided, (3) following a best practice work flow, including relational study methodologies, and (4) ensuring methodological adherence to three desired attributes – structured, comprehensive and transparent. The latter can potentially improve the thoroughness, clarity, and trustworthiness of future studies. The study concludes with a discussion of the findings, and a future research agenda which presents significant insights offering encouragement for bibliometric analyses, as well as acknowledging potential limitations.}
}
@article{WENDT2022103688,
title = {A multi-criteria CCUS screening evaluation of the Gulf of Mexico, USA},
journal = {International Journal of Greenhouse Gas Control},
volume = {118},
pages = {103688},
year = {2022},
issn = {1750-5836},
doi = {https://doi.org/10.1016/j.ijggc.2022.103688},
url = {https://www.sciencedirect.com/science/article/pii/S1750583622001062},
author = {Anna Wendt and Alana Sheriff and Chung Yan Shih and Derek Vikara and Tim Grant},
keywords = {Geologic carbon storage, Multi-criteria evaluation, Site screening, Gulf of Mexico, Outer continental shelf, Offshore CO storage},
abstract = {Continued research into reservoir characterization along with offshore carbon dioxide (CO2) transportation and infrastructure assets is needed to facilitate development of safe and successful carbon capture, utilization, and storage (CCUS) projects. This paper outlines a multi-criteria evaluation methodology that incorporates disparate sets of quantitative, spatially variable data into a decision-making framework for screening the Gulf of Mexico (GOM) outer continental shelf (OCS) for potentially viable CO2 storage and enhanced oil recovery (EOR) sites. Criteria categories include favorable geologic characteristics, logistics, and potential risks. Data compiled for 14 criteria from several publicly available geographic information system (GIS) layers was aggregated over 2559 spatially balanced points across the study area using the National Energy Technology Laboratory (NETL)-developed Cumulative Spatial Impact Layers™ (CSIL) GIS tool. Criteria are weighted by qualitative expert opinion relative to their perceived importance to given scenarios— the output of combined criteria values and weights enables regional CO2 storage suitability differentiation. The methodology considers both technical and non-technical factors impacting CCUS decision-making. The flexible methodology enables a systematic approach to regional ranking at high spatial resolution over a large study domain. Additionally, the framework enables high-grading of priority sites that warrant further characterization and follow-on analysis. Areas along the Louisiana coast and Mississippi River Delta consistently rank high for all scenarios largely a result of the favorable geology with the potential for stacked storage, as well as the density of existing pipelines and platforms, and proximity to several onshore CO2 sources. High-graded regions for the CO2 EOR-related scenarios are typically located further offshore towards the middle and edge of the OCS compared to higher priority regions for the geologic storage scenarios which fall closer to the Louisiana coastline.}
}
@article{MARCON202297,
title = {Capabilities supporting digital servitization: A multi-actor perspective},
journal = {Industrial Marketing Management},
volume = {103},
pages = {97-116},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019850122000487},
author = {Érico Marcon and Arthur Marcon and Néstor F. Ayala and Alejandro G. Frank and Vicky Story and Jamie Burton and Chris Raddats and Judy Zolkiewski},
keywords = {Digital transformation, Servitization, Digital servitization, Capabilities, Service actors},
abstract = {Digital transformation in business solutions is offering opportunities for servitization to become more digitalized. In this context, digital servitization requires the actors involved to perform new roles and develop new capabilities. Although servitization actor capabilities in the digital transformation context have been addressed in prior studies, the literature lacks a detailed understanding of how they operate according to different service types and different actor roles. Through a systematic literature review, our study aims to expound the capabilities required for digital servitization, for Base, Intermediate, and Advanced services, and analyze who of the main actors of the service triad (manufacturer, intermediaries, and customer) should own such capabilities. This analysis resulted in a final sample of 47 main articles addressing capabilities. We show how the structure of the service triad shifts the digital service provision based on the capabilities required by each actor. For instance, Base Services demand less capabilities, thus, intermediary actors play a less important role since they just execute services usually on behalf of a manufacturer in a more discrete capacity. For Intermediate Services, the intermediary actor becomes more important, with capabilities needed to deliver the digital solution. In Advanced Services, customers' relationships with manufacturers become stronger, as this actor reassumes a central role in the solution offer, and intermediaries move to a supporting role again. Our analysis offers propositions for future research on digital servitization and practical implications on the capabilities required.}
}
@article{MA2022103244,
title = {Quantitative assessment of essential tremor based on machine learning methods using wearable device},
journal = {Biomedical Signal Processing and Control},
volume = {71},
pages = {103244},
year = {2022},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.103244},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421008417},
author = {Chenbin Ma and Deyu Li and Longsheng Pan and Xuemei Li and Chunyu Yin and Ailing Li and Zhengbo Zhang and Rui Zong},
keywords = {Essential Tremor, Wearable Sensor, Rating of Severity, Machine Learning},
abstract = {Background
Essential tremor (ET) is a progressive neurological disorder with characteristic motor symptoms. Current clinical assessments are primarily based on expert consultation combined with reviewing patient complaints and physician expertise and diagnostic experience. Research on objective quantification through wearable sensor technology combined with machine learning methods has excellent potential for application. This study automatically rates the severity of symptoms in ET patients using wearable sensors in a standardized scenario.
Methods
This study relied on a rigorous clinical trial paradigm and a wearable device based on a nine-axis Inertial Measurement Unit (IMU) to collect a large amount of kinematic data from ET patients and obtain professional physician-supported scale scores. In this paper, hand tremor signals were comprehensively analyzed, and multiple kinematic features in the time and frequency domains were extracted. These features were used to explore different machine learning approaches to automatically and quantitatively assess the disease severity in ET patients.
Results
The optimized algorithm AdaBoost has a multi-classification F1 score of up to 97.33%, with 99.64% accuracy and 99.39% specificity, respectively. The model still has a better AUC for predicting a few classes and has the current optimal automatic ET symptom recognition performance.
Discussion
These results show that the proposed method is suitable for applying standardized laboratory tests to help clinicians automate the scoring of complex or early ET cases to aid decision-making and improve disease management efficiency.}
}
@article{PIAO20222,
title = {An ultra low-input method for global RNA structure probing uncovers Regnase-1-mediated regulation in macrophages},
journal = {Fundamental Research},
volume = {2},
number = {1},
pages = {2-13},
year = {2022},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2021.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S2667325821003113},
author = {Meiling Piao and Pan Li and Xiaomin Zeng and Xi-Wen Wang and Lan Kang and Jinsong Zhang and Yifan Wei and Shaojun Zhang and Lei Tang and Jianghui Zhu and Chun Kit Kwok and Xiaoyu Hu and Qiangfeng Cliff Zhang},
keywords = {RNA structure probing method, Low-input, RNA structure element, Macrophage, RNA structure, RNA structurome},
abstract = {To enable diverse functions and precise regulation, an RNA sequence often folds into complex yet distinct structures in different cellular states. Probing RNA in its native environment is essential to uncovering RNA structures of biological contexts. However, current methods generally require large amounts of input RNA and are challenging for physiologically relevant use. Here, we report smartSHAPE, a new RNA structure probing method that requires very low amounts of RNA input due to the largely reduced artefact of probing signals and increased efficiency of library construction. Using smartSHAPE, we showcased the profiling of the RNA structure landscape of mouse intestinal macrophages upon inflammation, and provided evidence that RNA conformational changes regulate immune responses. These results demonstrate that smartSHAPE can greatly expand the scope of RNA structure-based investigations in practical biological systems, and also provide a research paradigm for the study of post-transcriptional regulation.}
}
@article{WEI2022113233,
title = {Global satellite water classification data products over oceanic, coastal, and inland waters},
journal = {Remote Sensing of Environment},
volume = {282},
pages = {113233},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.113233},
url = {https://www.sciencedirect.com/science/article/pii/S003442572200339X},
author = {Jianwei Wei and Menghua Wang and Karlis Mikelsons and Lide Jiang and Susanne Kratzer and Zhongping Lee and Tim Moore and Heidi M. Sosik and Dimitry {Van der Zande}},
keywords = {Water class, Optical water type, Remote sensing reflectance, Spectral similarity, VIIRS, Hyperspectral},
abstract = {Satellites have generated extensive data of remote sensing reflectance spectra (Rrs(λ)) covering diverse water classes or types across global waters. Spectral classification of satellite Rrs(λ) data allows for the distinguishing and grouping of waters with characteristic bio-optical/biogeochemical features that may influence the productivity of a given water body. This study reports new satellite water class products (Level-2 and Level-3) from the Visible Infrared Imaging Radiometer Suite (VIIRS). We developed and implemented a hyperspectral scheme that accounts for the Rrs(λ) spectral shapes and globally resolves oceanic, coastal, and inland waters into 23 water classes. We characterized the light absorption and scattering coefficients, chlorophyll-a concentration, diffuse attenuation coefficient, and suspended particulate matter for individual water classes. It is shown that the water classes are separable by their distinct bio-optical and biogeochemical properties. Furthermore, validation result suggests that the VIIRS water class products are accurate globally. Finally, we examined the spatial and temporal variability of the water classes in case studies for a demonstration of applications. The water class data in open oceans reveal that the subtropical ocean gyres have experienced dramatic expansion over the last decade. In addition, the water class data appear to be a valuable (and qualitative) indicator for water quality in coastal and inland waters with compelling evidence. We stress that this new satellite product is an excellent addition to the aquatic science database, despite the need for continuous improvement toward perfection.}
}
@article{FATIMA2022101641,
title = {Integration of multi access edge computing with unmanned aerial vehicles: Current techniques, open issues and research directions},
journal = {Physical Communication},
volume = {52},
pages = {101641},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2022.101641},
url = {https://www.sciencedirect.com/science/article/pii/S1874490722000271},
author = {Nida Fatima and Paresh Saxena and Manik Gupta},
keywords = {Unmanned aerial vehicle, Multi-access edge computing, 5th generation wireless communication system (5G), Beyond 5G},
abstract = {During the last decade, research and development in the field of multi access edge computing (MEC) has rapidly risen to prominence. One of the factors propelling MEC’s evolution is the ability to deploy edge servers capable of providing both communication and computational services in close proximity to the mobile user terminal. MEC has been regarded as a potentially transformative technique for fifth-generation (5G) and beyond 5G (B5G) wireless communication systems, as well as a possible complement to traditional cloud computing. Additionally, unmanned aerial vehicles (UAVs) integrated with MEC will play a critical role by introducing an additional mobility based computational layer to provide more secure, efficient and faster services. UAV enabled MEC offers seamless connectivity, fulfilling the promise of 5G’s ubiquitous connectivity. Due to the enormous interest in UAV enabled MEC, there has been a tremendous increase in the number of published research articles in this domain; however, the research area still lacks a systematic study and categorization. We present a systematic literature review (SLR) on UAV enabled MEC, examining and analyzing data on the current state of the art using preferred reporting items for systematic reviews and meta-analyses (PRISMA) guidelines. To streamline our assessment, this study analyzes several research papers carefully selected through a multi-stage process satisfying the eligibility criteria defined in the paper. One of the SLR’s primary contributions is to broadly classify the research in the UAV enabled MEC domain into different categories including energy efficiency, resource allocation, security, architecture, and latency. We have identified key findings, technology, and pros and cons for the selected articles under each category. Additionally, we discuss the key open issues related to scalability and fairness, resource allocation and offloading optimization, service delivery with a focus on quality of experience (QoE) and quality of service (QoS), and standardization. Finally, we discuss several future research directions that would address the aforementioned issues and emerging use cases for UAV enabled MEC.}
}
@article{LI2022152935,
title = {Effects of warming, eutrophication and climate variability on acidification of the seasonally stratified North Yellow Sea over the past 40 years},
journal = {Science of The Total Environment},
volume = {815},
pages = {152935},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.152935},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722000249},
author = {Cheng-long Li and De-zhou Yang and Wei-dong Zhai},
keywords = {Coastal acidification, Aragonite saturation state, Pacific decadal oscillation, North Yellow Sea},
abstract = {The North Yellow Sea (NYS) is a productive marginal sea of the western North Pacific. In summer and autumn, CaCO3 saturation states beneath the seasonal thermocline in the NYS have frequently fallen below critical levels, indicating that marine calcifying organisms are under threat. To explore the long-term evolution of the acidification of the NYS, we reconstructed seasonal variations in subsurface aragonite saturation state (Ωarag) and pH during 1976–2017, using wintertime and summertime temperature, salinity, dissolved oxygen and pH data mainly from a quality-controlled oceanographic database. Over the past 40 years, the wintertime warming rate in the NYS was twice the rate of global ocean surface warming. Warming-induced decrease in CO2 solubility canceled out a part of the wintertime Ωarag decrease caused by atmospheric CO2 increase, and also had minor effect on pH changes in winter. Although the NYS is a semi-enclosed marginal sea, its interannual variations of wintertime temperature, salinity, pH and Ωarag were correlated to Pacific Decadal Oscillation with a lag of 2–3 years. Due to the eutrophication-induced enhancement of net community respiration beneath the seasonal thermocline, long-term declines of bottom-water Ωarag and pH in summer were substantially faster than the declines of assumed air-equilibrated Ωarag and pH in spring. Over the past 40 years, the amplitudes of seasonal variations of bottom-water Ωarag and pH from spring to summer/autumn have increased by 4–7 times. This amplification has pushed the NYS towards the critical threshold of net community CaCO3 dissolution at a pace faster than that forecast under scenarios of atmospheric CO2 increase. In summary, our results provide insights into the combined effects of ocean warming, eutrophication, atmospheric CO2 rise and climate variability on coastal hydrochemistry, explaining how the environmental stresses on local marine calcifying organisms and the benthic ecosystem increased over the past 40 years.}
}
@article{JIN2022171,
title = {Fusion of optical, radar and waveform LiDAR observations for land cover classification},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {187},
pages = {171-190},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622000752},
author = {Huiran Jin and Giorgos Mountrakis},
keywords = {Fusion, Land cover classification, Optical, SAR, Waveform LiDAR, Accuracy},
abstract = {Land cover is an integral component for characterizing anthropogenic activity and promoting sustainable land use. Mapping distribution and coverage of land cover at broad spatiotemporal scales largely relies on classification of remotely sensed data. Although recently multi-source data fusion has been playing an increasingly active role in land cover classification, our intensive review of current studies shows that the integration of optical, synthetic aperture radar (SAR) and light detection and ranging (LiDAR) observations has not been thoroughly evaluated. In this research, we bridged this gap by i) summarizing related fusion studies and assessing their reported accuracy improvements, and ii) conducting our own case study where for the first time fusion of optical, radar and waveform LiDAR observations and the associated improvements in classification accuracy are assessed using data collected by spaceborne or appropriately simulated platforms in the LiDAR case. Multitemporal Landsat-5/Thematic Mapper (TM) and Advanced Land Observing Satellite-1/ Phased Array type L-band SAR (ALOS-1/PALSAR) imagery acquired in the Central New York (CNY) region close to the collection of airborne waveform LVIS (Land, Vegetation, and Ice Sensor) data were examined. Classification was conducted using a random forest algorithm and different feature sets in terms of sensor and seasonality as input variables. Results indicate that the combined spectral, scattering and vertical structural information provided the maximum discriminative capability among different land cover types, giving rise to the highest overall accuracy of 83% (2–19% and 9–35% superior to the two-sensor and single-sensor scenarios with overall accuracies of 64–81% and 48–74%, respectively). Greater improvement was achieved when combining multitemporal Landsat images with LVIS-derived canopy height metrics as opposed to PALSAR features, suggesting that LVIS contributed more useful thematic information complementary to spectral data and beneficial to the classification task, especially for vegetation classes. With the Global Ecosystem Dynamics Investigation (GEDI), a recently launched LiDAR instrument of similar properties to the LVIS sensor now operating onboard the International Space Station (ISS), it is our hope that this research will act as a literature summary and offer guidelines for further applications of multi-date and multi-type remotely sensed data fusion for improved land cover classification.}
}
@article{LYU2022102850,
title = {Car restriction policies and housing markets},
journal = {Journal of Development Economics},
volume = {156},
pages = {102850},
year = {2022},
issn = {0304-3878},
doi = {https://doi.org/10.1016/j.jdeveco.2022.102850},
url = {https://www.sciencedirect.com/science/article/pii/S0304387822000268},
author = {Xueying Lyu},
keywords = {Car restriction policies, Housing prices, Capitalization, Wealth redistribution},
abstract = {This paper investigates the differential impacts of a unique car restriction policy – the car purchase lottery in Beijing – on the housing markets across locations within the city. I use a difference-in-differences approach to compare heterogeneous neighborhoods before and after the implementation of the policy. Housing prices experience a relative increase at locations closer to common destinations (employment centers: 0.7% per kilometer; primary schools: 3.3% per kilometer) and with better access to public transit (subways: 1.2% per kilometer; buses: 0.08% per line). These changes reflect capitalization of the car restriction policy and imply a large wealth redistribution as large as 6 years of average disposable income across homeowners. The results are relevant to policy, both in the context of unintended consequences and for efforts to develop offsetting measures.}
}
@article{ROCHERT2022101866,
title = {Caught in a networked collusion? Homogeneity in conspiracy-related discussion networks on YouTube},
journal = {Information Systems},
volume = {103},
pages = {101866},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101866},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000946},
author = {Daniel Röchert and German Neubaum and Björn Ross and Stefan Stieglitz},
keywords = {Machine learning, Social network analysis, YouTube, Conspiracy theories, Opinion- based homogeneity},
abstract = {In many instances, misinformation among the population manifests itself in the form of conspiracy theories. Services such as YouTube, which allow the publication of audiovisual material in juxtaposition with peer responses (e.g., comments), function as ideal forums to disseminate such conspiracy theories and reach a massive audience. While previous research provided initial evidence about the prevalence of conspiracy theories in social media, it remains unclear how online networks discussing conspiracist content are structured. Knowledge about the network structure, however, could indicate to what extent people discussing conspiracist ideas face the risk of becoming caught in homogeneous communication cocoons. This work presents an approach combining natural language processing and network analysis to measure opinion-based homogeneity of discussion networks of three conspiracy theories (Hollow Earth, Chemtrails, and New World Order) on YouTube. A classification model was used to identify conspiracy and counter-conspiracy videos and associated user-generated comments (N = 123,642), as well as the interconnections between them. Although classification accuracy varied between the investigated conspiracy theories, our results indicated that people who expressed a favorable stance toward the conspiracy theory tended to respond to content or interact with users that shared the same opinion. In contrast, for two out of three conspiracy theories, people who advocated against the theory in their comments were more willing to engage in cross-cutting interactions. Findings are interpreted in light of the widely discussed fragmentation of homogeneous online networks.}
}
@article{ZHANG2022201,
title = {Deep-learning generation of POI data with scene images},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {188},
pages = {201-219},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622000995},
author = {Jinbao Zhang and Xiaojuan Liu and Weilin Liao and Xia Li},
keywords = {Automatic POI-generation, Deep learning framework, Scene images, Visual-linguistic multimodal model},
abstract = {Point of interest (POI) is essential to urban scene understanding and location-based services. However, most of the POI data sets are collected manually on the spot, which is time-consuming and laborious. In this study, we propose a deep learning-based three-stage framework to automatically generate POI data sets from scene images by integrating instance segmentation, scene text recognition (STR), and multimodal technology. Firstly, we utilize an instance segmentation model to extract the region of interest (ROI) that contains POI text information from the scene images. Secondly, a STR method is used to locate and identify the text lines from the ROI. Thirdly, we develop a novel visual-linguistic multi-task classification model (VLMC) to classify ROIs and text lines through fusing text and image information. It is the first deep learning-based framework that allows generating POI information with different attributes (such as title, address, and tag) from the text lines of scene images and updating with high-performance models in the three-stage technique. In the experiments, we employ multiple STR data sets and annotated street view images for model training. The result shows that the deep learning-based framework can generate POI records from scene images with high accuracy (F1-score = 52.62%). Moreover, we find that the multi-modal VLMC model integrating the linguistic and visual embeddings has a higher accuracy in POI-generation than single-modal methods. We further use a trained framework to generate POI from Baidu Street View (BSV) images and Tencent Street View (TSV) images in Shenzhen, China, and ultimately obtain a long-term POI data set during 2013 – 2020 with 2,699,895 street view images. Of 815,616 records in the generated POI data set in 2020, 70.94% are covered by the existing Baidu POI data set of Shenzhen in 2013. This confirms the validity of the newly generated POI data set. These results demonstrate that the proposed deep-learning POI-generation framework and dataset can provide new insights for geographic data updating and urban scene understanding for fast growing cities. To facilitate future research, an implementation is made available at https://github.com/KampauCheung/scene-image-poi-generation.}
}
@article{GIOVANELLI2022101957,
title = {Data pre-processing pipeline generation for AutoETL},
journal = {Information Systems},
volume = {108},
pages = {101957},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101957},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001514},
author = {Joseph Giovanelli and Besim Bilalli and Alberto Abelló},
keywords = {Data pre-processing pipelines, Data analytics},
abstract = {Data pre-processing plays a key role in a data analytics process (e.g., applying a classification algorithm on a predictive task). It encompasses a broad range of activities that span from correcting errors to selecting the most relevant features for the analysis phase. There is no clear evidence, or rules defined, on how pre-processing transformations impact the final results of the analysis. The problem is exacerbated when transformations are combined into pre-processing pipeline prototypes. Data scientists cannot easily foresee the impact of pipeline prototypes and hence require a method to discriminate between them and find the most relevant ones (e.g., with highest positive impact) for their study at hand. Once found, these prototypes can be instantiated and optimized e.g., using Bayesian Optimization. In this work, we study the impact of transformations when chained together into prototypes, and the impact of transformations when instantiated via various operators. We develop and scrutinize a generic method that allows to generate pre-processing pipelines, as a step towards AutoETL. We make use of rules that enable the construction of prototypes (i.e., define the order of transformations), and rules that guide the instantiation of the transformations inside the prototypes (i.e., define the operator for each transformation). The optimization of our effective pipeline prototypes provide results that compared to an exhaustive search, get 90% of the predictive accuracy in the median, but with a time cost that is 24 times smaller.}
}
@incollection{CHIRBA2022265,
title = {Chapter 14 - FDA regulation of adipose cell use in clinical trials and clinical translation},
editor = {Lauren Kokai and Kacey Marra and J. Peter Rubin},
booktitle = {Scientific Principles of Adipose Stem Cells},
publisher = {Academic Press},
pages = {265-310},
year = {2022},
isbn = {978-0-12-819376-1},
doi = {https://doi.org/10.1016/B978-0-12-819376-1.00003-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128193761000032},
author = {Mary Ann Chirba and Veronica Morgan Jones and Patsy Simon and Adam J. Katz},
keywords = {Food and Drug Administration (FDA), Regulation, Adipose tissue, Adipose function, Cell therapy, Real-world evidence, Trial design},
abstract = {This chapter discusses United States Food and Drug Administration (FDA) regulation of adipose-derived stem cell therapies. It begins with the basic facts of what adipose is and what it does (while leaving more detailed discussions to other parts of this publication). It then summarizes the basics of law and regulation as applied to regenerative medicine and describes the FDA's overall risk-based framework for regulating cell therapies. This is followed by a closer examination of the FDA's specific regulation of adipose-derived cell and tissue products. The agency's missteps are identified, and their repercussions for the regulated as well as the regulator are considered. The chapter closes with a brief overview of recent developments in methods and mechanisms for obtaining approval of regenerative products and biologics in general, particularly with regard to real-world evidence and innovative trial design. While not limited to adipose products, this material is potentially relevant to those working in this area.}
}
@article{YUAN2022100217,
title = {Using traffic flow characteristics to predict real-time conflict risk: A novel method for trajectory data analysis},
journal = {Analytic Methods in Accident Research},
volume = {35},
pages = {100217},
year = {2022},
issn = {2213-6657},
doi = {https://doi.org/10.1016/j.amar.2022.100217},
url = {https://www.sciencedirect.com/science/article/pii/S2213665722000069},
author = {Chen Yuan and Ye Li and Helai Huang and Shiqi Wang and Zhenhao Sun and Yan Li},
keywords = {Real-time conflict risk, Heterogeneity, Random parameter, Machine learning},
abstract = {The real-time conflict prediction model using traffic flow characteristics is much less studied than the crash-based model. This study aims at exploring the relationship between conflicts and traffic flow features with the consideration of heterogeneity and developing predictive models to identify conflict-prone conditions in a real-time manner. The high-resolution trajectory data from the HighD dataset is used as empirical data. A novel method with the virtual detector approach for traffic feature extraction and a two-step framework is proposed for the trajectory data analysis. The framework consists of an exploratory study by random parameter logit model with heterogeneity in means and variances and a comparative study on several machine learning methods, including eXtreme Gradient Boosting (Boosting), Random Forest (Bagging), Support Vector Machine (Single-classifier), and Multilayer-Perceptron (Deep neural network). Results indicate that (1) traffic flow characteristics have significant impacts on the probability of conflict occurrence; (2) the statistical model considering mean heterogeneity outperforms the counterpart and lane differences variables are found to significantly impact the means of random parameters for both lane variables and lane differences variables; (3) eXtreme Gradient Boosting trained on an under-sampled dataset turns out to be the best model with the highest AUC of 0.871 and precision of 0.867, showing that re-sampling techniques can significantly improve the model performance. The proposed model is found to be sensitive to the conflict threshold. Sensitivity analysis on feature selection further confirms that the conflict risk prediction should consider both subject lane features and lane difference features, which verifies the consistency with exploratory analysis based on the statistical model. The consistency between statistical models and machine learning methods improves the interpretability of results for the latter one.}
}
@article{CASTANO2022101842,
title = {A knowledge-centered framework for exploration and retrieval of legal documents},
journal = {Information Systems},
volume = {106},
pages = {101842},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101842},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000788},
author = {Silvana Castano and Mattia Falduti and Alfio Ferrara and Stefano Montanelli},
keywords = {Legal knowledge model, Legal knowledge extraction, Legal document retrieval and exploration},
abstract = {Automated legal knowledge extraction systems are strongly demanded, to support annotation of legal documents as well as knowledge extraction from them, to provide useful and relevant suggestions to legal actors (e.g., judges, lawyers) for managing incoming new cases. In this paper, we propose CRIKE (CRIme Knowledge Extraction), a knowledge-based framework conceived to support legal knowledge extraction from a collection of legal documents, based on a reference legal ontology called LATO (Legal Abstract Term Ontology). We first introduce LATO-KM, the knowledge model of LATO where legal knowledge featuring documents in the collection is properly formalized as conceptual knowledge, in form of legal concepts and relationships, and terminological knowledge, in form of term-sets associated with legal concepts. Then, we present the bootstrapping cycle of CRIKE that aims to progressively enrich the terminological knowledge layer of LATO by extracting new terms from legal documents to be used for enriching the term-set associated with a corresponding legal concept. Finally, to evaluate the results obtained through CRIKE, we discuss experimental results on a real dataset of 180,000 court decisions of the State of Illinois taken from the Caselaw Access Project (CAP).}
}
@article{KANNISTO2022100253,
title = {Plant-wide interoperability and decoupled, data-driven process control with message bus communication},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100253},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100253},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000522},
author = {Petri Kannisto and David Hästbacka and Teresa Gutiérrez and Olli Suominen and Matti Vilkko and Peter Craamer},
keywords = {Systems integration, Service-oriented architecture, Industrial cyber–physical systems, Industry 4.0, Publish–subscribe communication pattern},
abstract = {Conventional industrial communication systems suffer from rigidness, inflexibility and lack of scalability. The environment is heterogeneous as the systems exchange data with a variety communication protocols, some of which are proprietary. This makes it laborious and expensive to reconfigure or upgrade the systems. As the solution, this article proposes a message-bus-based communication architecture to enable information exchange between systems regardless of their geographical location and position within the functional hierarchy of the plant. The architecture not only enables communication to cross the conventional physical borders but also provides scalability to growing data volumes and network sizes. As proofs of concept, the article presents a prototype in three environments: a copper smelter, a steel plant and a distillation column. The results suggest that the message-bus-based approach has potential to renew industrial communications, a core part of the fourth industrial revolution.}
}
@article{GOERLANDT2022189,
title = {The landscape of safety management systems research: A scientometric analysis},
journal = {Journal of Safety Science and Resilience},
volume = {3},
number = {3},
pages = {189-208},
year = {2022},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2022.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666449622000159},
author = {Floris Goerlandt and Jie Li and Genserik Reniers},
keywords = {Safety management system, Bibliometrics, Scientometrics, VOSviewer, CiteSpace},
abstract = {Safety management systems (SMSs) are widely applied across many industrial sectors, and a large body of literature has been published addressing their design, implementation, effectiveness, and associated challenges. This article presents a high-level analysis of the SMS research domain, guided by a set of questions addressing the contents, structure, and evolution the research domain, its dominant themes and focus topics, the key scientific domains and journals contributing to its development, and the key publications serving as an intellectual basis for SMS related research. The results show a rapidly increasing volume of research outputs and a shift from research based in North America and Europe to Asia and Australia. There is only a limited number of institutions enduringly contributing to the field, and there are relatively few stable research collaborations, with the number of Chinese institutions publishing SMS related research fast expanding in recent years. The domain is strongly interdisciplinary and embedded in applied domains of science, with industrial engineering the most contributing category, as well as categories focusing on the industrial application domains. A temporal evolution of the research activity in different application domains is apparent, with an initial focus on occupational health and safety, followed by process safety, patient safety, food safety, and construction safety. SMS research has a strong relation to safety culture and safety climate research, and while safety and risk management concepts and theories form an important knowledge base for most application domains, the dominant views on accident causation differ between these. Research on SMS in the food industry is relatively separated from the other application domains. Based on the findings, various future research directions are discussed.}
}
@article{YANG2022101802,
title = {Broadband internet and enterprise innovation},
journal = {China Economic Review},
volume = {74},
pages = {101802},
year = {2022},
issn = {1043-951X},
doi = {https://doi.org/10.1016/j.chieco.2022.101802},
url = {https://www.sciencedirect.com/science/article/pii/S1043951X22000608},
author = {Mengjun Yang and Shilin Zheng and Lin Zhou},
keywords = {Broadband internet, Innovation, Instrumental variable, Knowledge spillover, Financing constraints},
abstract = {Based on microdata from China's listed companies and macrodata for broadband internet access in prefecture-level cities, this paper explores the relationship between broadband internet and enterprise innovation. Using the change in market concentration caused by the North–South separation reform of China Telecom in 2002 as an instrumental variable, the results show that in general, a 1% increase in broadband internet access results in a 1.395% increase in the number of corporate patents. Specifically, the number of valid patents, patent citations and valid patent citations, reflecting patent quality, increases by 1.499%, 0.920% and 0.763%, respectively. The mechanistic analysis shows that broadband internet access contributes to increasing the number of R&D personnel and personal innovation efficiency, enhancing enterprises' willingness to innovate, and easing financing constraints. Further analysis suggests that broadband internet access mainly promotes invention patents rather than design patents. The innovation effect is more evident among high-tech, inventor-intensive, state-owned enterprises and enterprises located in the non-southeastern coastal region of China.}
}
@article{ROSTAMI2022412,
title = {Comparative sustainability study of energy storage technologies using data envelopment analysis},
journal = {Energy Storage Materials},
volume = {48},
pages = {412-438},
year = {2022},
issn = {2405-8297},
doi = {https://doi.org/10.1016/j.ensm.2022.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S2405829722001635},
author = {Fatemeh Rostami and Zoltán Kis and Rembrandt Koppelaar and Laureano Jiménez and Carlos Pozo},
keywords = {Data envelopment analysis (DEA), Energy storage, Hydrogen, Power systems flexibility, Sustainable energy},
abstract = {The transition to energy systems with a high share of renewable energy depends on the availability of technologies that can connect the physical distances or bridge the time differences between the energy supply and demand points. This study focuses on energy storage technologies due to their expected role in liberating the energy sector from fossil fuels and facilitating the penetration of intermittent renewable sources. The performance of 27 energy storage alternatives is compared considering sustainability aspects by means of data envelopment analysis. To this end, storage alternatives are first classified into two clusters: fast-response and long-term. The levelized cost of energy, energy and water consumption, global warming potential, and employment are common indicators considered for both clusters, while energy density is used only for fast-response technologies, where it plays a key role in technology selection. Flywheel reveals the highest efficiency between all the fast-response technologies, while green ammonia powered with solar energy ranks first for long-term energy storage. An uncertainty analysis is incorporated to discuss the reliability of the results. Overall, results obtained, and guidelines provided can be helpful for both decision-making and research and development purposes. For the former, we identify the most appealing energy storage options to be promoted, while for the latter, we report quantitative improvement targets that would make inefficient technologies competitive if attained. This contribution paves the way for more comprehensive studies in the context of energy storage by presenting a powerful framework for comparing options according to multiple sustainability indicators.}
}
@article{LIU2022106260,
title = {Rural typology dynamics and drivers in peripheral areas: A case of Northeast China},
journal = {Land Use Policy},
volume = {120},
pages = {106260},
year = {2022},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2022.106260},
url = {https://www.sciencedirect.com/science/article/pii/S0264837722002873},
author = {Jianzhi Liu and Yangang Fang and Ruru Wang and Cunming Zou},
keywords = {Rural typology, Rural change, Regional development, Peripheral areas, Eastern Europe},
abstract = {Socioeconomic developments in rural areas are characterized by heterogeneity and diversity rather than evolving along 'parallel linear paths'. To date, our understanding of rural heterogeneity and evolution remains limited, especially in institutionally transitional countries. Eastern Europe and Northeast China have both experienced economic recessions, industrial restructuring and urban shrinkage over the past decades and have become rust, peripheral and even stigmatized areas. However, the backdrop of rural evolution in Northeast China is different from that of Eastern Europe, since its institutional transition has been gradual rather than a 'shock therapy'. This paper proposes a conceptual framework for rural typology and uses cluster analysis and random forest to explore the rural typological evolution and drivers in Northeast China from 2000 to 2017. The results show that the modernizing grain agriculture leading type currently occupies the main position in Northeast China, while the urbanizing type and industry diversification type only account for 18.6 %. Regarding rural typological dynamics from 2000 to 2017, the modernizing grain agriculture leading type expanded and intensified in more peripheral areas with abundant arable land resources or high potential for arable land reclamation, driven by increasing national grain demand, widening regional economic disparities and division of labor, and agricultural policies. Meanwhile, the transformation mode from the modernizing grain agriculture leading type to the industry diversification type (5.81 %) developed in areas with relatively developed economies, proximity to markets, and dense populations. Finally, we discuss the similarities and differences in rural development between Northeast China and Eastern Europe and propose related policy implications for rural development.}
}
@article{BOGDANOVA2022190,
title = {Prediction of the Air Pollution by Geo-locations in Sofia},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {11},
pages = {190-195},
year = {2022},
note = {IFAC Workshop on Control for Smart Cities CSC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.08.071},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322011612},
author = {Boryana Bogdanova and Jr. Angel Marchev and Vladimir Zakov and Denis Stefanov and Kiril Genov},
keywords = {PM10 air pollution, citizen measurement network, bias correction, meteorological indicators, ARIMAX, random forest, feature engineering, next day prediction},
abstract = {We develop a general framework for analysis and prediction of air pollution in the city of Sofia, Bulgaria, as measured by the level of the PM10 air pollutant indicator. As a starting point in our analysis, we consider earlier findings documented in the literature. We focus on utilized methodology so as to support the process of defining proper predictive approach that is adopted in our methodological framework.}
}
@article{ABUTALIB2022102875,
title = {APT beaconing detection: A systematic review},
journal = {Computers & Security},
volume = {122},
pages = {102875},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102875},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822002693},
author = {Manar {Abu Talib} and Qassim Nasir and Ali {Bou Nassif} and Takua Mokhamed and Nafisa Ahmed and Bayan Mahfood},
keywords = {APT, Beaconing, Attack, Security breach, Detection, AI, C&C},
abstract = {Advanced Persistent Threat (APT) is a type of threat that has grabbed the attention of researchers, particularly in the industrial security field. APTs are cyber intrusions carried out by skilled and well-resourced adversaries who target specific information in high-profile organizations and governments, frequently as part of a multi-phase long-term operation. One of the phases of the APT process is the command-and-control (C&C) phase, also known as beaconing. Beaconing is an important part of an APT lifecycle, where the adversaries establish channels with the compromised hosts in the targeted system, allowing them to launch additional attacks. Detecting and predicting this stage is therefore a practical way to guard against APTs. This paper discusses the techniques and methods used to detect APTs and also specifically to identify beaconing, either during the APT lifecycle or not. In it, we determine various artificial intelligence algorithms used for detecting, analyzing and comparing characteristics of datasets and data sources used to implement these detection techniques. Moreover, we present the strengths and challenges of various APT/beaconing detection methods. Finally, this study outlines many cybersecurity vendor projects that have been created to identify APT or beaconing operations, categorized according to the detection approach utilized.}
}
@article{XIE2022382,
title = {Carbon price prediction considering climate change: A text-based framework},
journal = {Economic Analysis and Policy},
volume = {74},
pages = {382-401},
year = {2022},
issn = {0313-5926},
doi = {https://doi.org/10.1016/j.eap.2022.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0313592622000297},
author = {Qiwei Xie and Jingjing Hao and Jingyu Li and Xiaolong Zheng},
keywords = {Carbon price prediction, Text mining, Climate change, Long short-term memory (LSTM), Random forest (RF)},
abstract = {Carbon trading is a vital market mechanism to achieve carbon emission reduction. The accurate prediction of the carbon price is conducive to the effective management and decision-making of the carbon trading market. However, existing research on carbon price forecasting has ignored the impacts of multiple factors on the carbon price, especially climate change. This study proposes a text-based framework for carbon price prediction that considers the impact of climate change. Textual online news is innovatively employed to construct a climate-related variable. The information is combined with other variables affecting the carbon price to forecast the carbon price, using a long short-term memory network and random forest model. The results demonstrate that the prediction accuracy of the carbon price in the Hubei and Guangdong carbon markets is enhanced by adding the textual variable that measures climate change.}
}
@article{LO2022111357,
title = {Architectural patterns for the design of federated learning systems},
journal = {Journal of Systems and Software},
volume = {191},
pages = {111357},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111357},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000899},
author = {Sin Kit Lo and Qinghua Lu and Liming Zhu and Hye-Young Paik and Xiwei Xu and Chen Wang},
keywords = {Federated learning, Pattern, Software architecture, Machine learning, Artificial intelligence},
abstract = {Federated learning has received fast-growing interests from academia and industry to tackle the challenges of data hungriness and privacy in machine learning. A federated learning system can be viewed as a large-scale distributed system with different components and stakeholders as numerous client devices participate in federated learning. Designing a federated learning system requires software system design thinking apart from the machine learning knowledge. Although much effort has been put into federated learning from the machine learning technique aspects, the software architecture design concerns in building federated learning systems have been largely ignored. Therefore, in this paper, we present a collection of architectural patterns to deal with the design challenges of federated learning systems. Architectural patterns present reusable solutions to a commonly occurring problem within a given context during software architecture design. The presented patterns are based on the results of a systematic literature review and include three client management patterns, four model management patterns, three model training patterns, four model aggregation patterns, and one configuration pattern. The patterns are associated to the particular state transitions in a federated learning model lifecycle, serving as a guidance for effective use of the patterns in the design of federated learning systems.}
}
@incollection{MOHAGHEGH2022281,
title = {Chapter 11 - Application of artificial intelligence to computational fluid dynamics},
editor = {Shuvajit Bhattacharya and Haibin Di},
booktitle = {Advances in Subsurface Data Analytics},
publisher = {Elsevier},
pages = {281-352},
year = {2022},
isbn = {978-0-12-822295-9},
doi = {https://doi.org/10.1016/B978-0-12-822295-9.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222959000017},
author = {Shahab D. Mohaghegh and Ayodeji Aboaba and Yvon Martinez and Mehrdad Shahnam and Chris Guenther and Yong Liu},
keywords = {Artificial intelligence, CFD, Machine learning, Smart proxy},
abstract = {Smart proxy technology leverages the art and science of artificial intelligence and machine learning in order to build accurate and very fast proxy models for highly complex numerical simulation models. The main characteristics of the smart proxy modeling are (a) physics of the numerical simulation is not reduced or modified, (b) resolution of the numerical simulation is not reduced or modified, (c) detail cell results of the numerical simulation is replicated with high accuracy, and (d) deployment of the smart proxy to provide numerical simulation results for every cell will take few minutes using a laptop or desktop workstation. In this project, smart proxy technology is used to simulate the combustion of natural gas under various conditions such as varying natural gas composition and flow rate, inlet air flow rate and temperature, and outlet pressure in a high-pressure combustion facility (B6 combustor) with more than four million simulation cells.}
}
@article{CAI2022101086,
title = {Flood forecasting in urban reservoir using hybrid recurrent neural network},
journal = {Urban Climate},
volume = {42},
pages = {101086},
year = {2022},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2022.101086},
url = {https://www.sciencedirect.com/science/article/pii/S2212095522000049},
author = {Bo Cai and Yaoxiang Yu},
keywords = {Reservoir flood, Machine learning, Hybrid learning model, Real-time forecasting, Forecast system},
abstract = {Flood forecasting can provide accurate inferences and early warnings for flood control work during the flood season. Due to the variability of local rainfall and the complexity of geographic conditions, existing prediction methods were unable to accurately predict the flooding process in a particular basin. Additionally, the water level sensor generates a significant amount of noise in the inbound flow data during period measurement. To address these issues, this article proposes a real-time flood forecasting model, which is used to accurately predict flood trends and peak times in the flood period. The model uses a convolution kernel function to smooth out local noise and neighborhood values, minimizing the impact of non-stationary series on the training process while retaining the true evolution of the flood in the original data. In our model, we develop a time series attention mechanism that is used to apply various weights to time series input vectors, such as outflow flow and rainfall from upstream reservoirs, this mechanism also improves the accuracy of short-term series prediction. To obtain additional information about the output of the recurrent neural network layer, we also include a multivariate autoregressive integrated moving average module. This method can add a linear component to the output, allowing the prediction result to adapt to the input period's scale shift. This article develops matching models for interval and full basin floods based on the geographical characteristics of China's urban Reservoir and the river basin, thresholds are established based on the outflow from upstream reservoirs, which enables the flood forecasting system to dynamically adjust model parameters in response to the threshold, it also circumvents the scaling problem inherent in flood time series at various scales. We trained and predicted using 25 different types of floods in Ankang Reservoir from 2010 to 2020. Three on-site real-time forecasts of catastrophic flooding at the Ankang Reservoir were conducted in September 2021 to validate the model's accuracy. The algorithm's efficiency in forecasting flood inflows is demonstrated through comparisons to traditional hydrological models and other machine learning networks, and our model consistently forecasts the peak time and total flood volume with the least amount of error in the comparison algorithm.}
}
@incollection{DUBOIS2022335,
title = {Chapter Eleven - Use of GC×GC for the characterization of odours in forensic applications},
editor = {Chiara Emilia Irma Cordero},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {96},
pages = {335-365},
year = {2022},
booktitle = {Characterization of Odorant Patterns by Comprehensive Two-Dimensional Gas Chromatography},
issn = {0166-526X},
doi = {https://doi.org/10.1016/bs.coac.2021.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166526X2100101X},
author = {Lena M. Dubois and Gwen O'Sullivan and Pierre-Hugues Stefanuto and Court D. Sandau and Jean-François Focant},
keywords = {Forensic science, Odour analysis, Ignitable liquids, Fire debris investigation, VOCs, Volatolomics, Volatile organic compounds, GC×GC, Comprehensive two-dimensional gas chromatography},
abstract = {In forensic science, the emission of odours from objects or biological matrices is exploited for different purposes. For example, the monitoring of odours via biological or analytical detectors is used in thanatochemistry, the chemistry of death. The analysis of decomposition odour can be explored to support the localization of a missing body, a scenario encountered in urban search and rescue operations. A better understanding of the formation and evolution of decomposition odour is also of high interest to human remains detection canine handlers to improve training practices and chose appropriate training aids. Next to thanatochemistry, many other types of evidence evaluation benefit from the characterization of the volatile profile including the analysis of fire debris, chemical threat agents, explosives, and drugs. From a chemical point of view, an odour represents a complex mixture of gaseous molecules and its characterization demands for a powerful analytical technique. Especailly, in non-targeted analysis, the separation power provided by one-dimensional (1D) gas chromatography (GC) can be surpassed. Thus, a better insight is usually achieved using a multidimensional technique, such as comprehensive two-dimensional gas chromatography (GC×GC). This chapter focuses on scientific articles published between 2015 and 2020 reporting on the use of GC×GC for odour characterization in the context of forensic science. The main points are decomposition odour, volatolomic applications for profiling of human scent and illegal trade goods such as wildlife parts. Furthermore, the investigation of volatile traces of drugs and ignitable liquids in the context of arson investigations is addressed in detail. For each section, the length is proportional to the number of publications from the literature review.}
}
@article{BACKES2022102056,
title = {Lattice-based progressive author disambiguation},
journal = {Information Systems},
volume = {109},
pages = {102056},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2022.102056},
url = {https://www.sciencedirect.com/science/article/pii/S0306437922000473},
author = {Tobias Backes and Stefan Dietze},
keywords = {Author name disambiguation, Progressive entity resolution, Blocking, Formal concept analysis, Association rule learning},
abstract = {Different use cases have acknowledged the importance of author identities and the non-triviality of determining them. Author disambiguation (AD) is a special case of entity resolution resolving author mentions to actual real-world authors. Like in other entity resolution tasks, AD methods are strongly restricted by scale and person name conventions. So far, this has been addressed by static blocking methods which cannot adapt to such collection-dependent properties. We address this gap by presenting the first progressive method of author disambiguation. Progressive entity resolution tackles large-scale conflation problems by repeatedly increasing the number of pairs compared for potential equivalence. Our method uses lattice structures to model name inclusion in an adaptive and more efficient way than traditional blocking techniques based on alphabetical order or fixed-level generalization. Our work offers additional insights into the relationship between name-matching, different blocking schemes, blocking and clustering as well as cost and benefit. Using the Web of Science as large-scale annotated test data, we observe and compare our model’s performance over time and compare it with various configurations and baselines. Our approach consistently outperforms state-of-the-art blocking methods, underlining its contribution to the field of author disambiguation. Our approach offers a novel alternative for tackling ambiguity in entity resolution, which is a major challenge for many information systems.}
}
@article{YUAN2022105647,
title = {Safety barriers in the chemical process industries: A state-of-the-art review on their classification, assessment, and management},
journal = {Safety Science},
volume = {148},
pages = {105647},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105647},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521004872},
author = {Shuaiqi Yuan and Ming Yang and Genserik Reniers and Chao Chen and Jiansong Wu},
keywords = {Safety barrier, Barrier management, Barrier performance assessment, Process industry, Resilience},
abstract = {Barriers are used in various forms to assure the safety of chemical plants. A deep understanding of the literature related to safety barriers is essential to tackle the challenges in improving their design and management. This paper first provides an overview of the history of the development of the safety barrier concept. Subsequently, this paper elaborates a systematic review of the definition, classification, evaluation, performance assessment, and management of safety barriers in the chemical process industries. Based on the literature review, this study proposes a practical classification of safety barriers benefiting the identification of performance indicators and the collection of indicator-related data for safety barriers. The safety barrier functions are extended and illustrated by involving the resilience concept. Performance assessment criteria are proposed corresponding to the adaptability and recoverability of the safety barriers. Finally, the management of safety barriers is discussed. The roadmap for future studies to develop integrated management of safety and security barriers to ensure the resilience of chemical plants is suggested.}
}
@article{PAPANDREOU2022110321,
title = {Predicting VLCC fuel consumption with machine learning using operationally available sensor data},
journal = {Ocean Engineering},
volume = {243},
pages = {110321},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.110321},
url = {https://www.sciencedirect.com/science/article/pii/S002980182101622X},
author = {Christos Papandreou and Apostolos Ziakopoulos},
keywords = {Fuel oil consumption prediction, VLCC sensor Data, Machine learning, Polynomial regression, Artificial neural network, XGBoost regression},
abstract = {In the maritime industry, more accurate predictions of fuel oil consumption (FOC) could yield multidimensional results including more precise bunker calculations, emission reductions, more informed planning and limiting operational costs. However, models often require sophisticated data that may be partially unavailable to operators beforehand. The present research aims to develop accurate main engine FOC forecasting models that utilize exclusively data from sensors and simple weather data readily available in operational practice. Commonly available sensor data from a Very Large Crude Oil Carrier (VLCC) were used, comprising speed through water, relative wind direction, relative wind speed, mean draft, trim, days since last drydock and laden or ballast vessel state. Multivariate Polynomial Regression (MPR), Artificial Neural Networks (ANNs) and eXtreme Gradient Boosting (XGBoost) regression models were developed and evaluated based on their predictive accuracy for VLCC FOC. Results indicated that XGBoost had the best performance, yielding predictions within 5% of the true value more than 86% of the total cases, followed by MPR and ANN. In addition, accurate aggregate FOC forecasting was conducted with XGBoost for a laden voyage and a ballast voyage of a VLCC.}
}
@article{TRUONGHONG2022101490,
title = {Extracting structural components of concrete buildings from laser scanning point clouds from construction sites},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101490},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101490},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002391},
author = {L. Truong-Hong and Roderik Lindenbergh},
keywords = {Point cloud, Object extraction, Structure extraction, Segmentation, Inspection, Scan to BIM},
abstract = {In construction projects, inspection of structural components mostly relies on classical measurements obtained by measuring tapes, levelling, or total stations. With those methods, only a few points on the structure can be measured, and the resulting inspection may not fully reflect the actual, detailed condition of the complete object. Laser scanning is an emerging remote sensing technology to accurately and quickly capture surfaces of structures in high details. However, because of the complex, massive point cloud data acquired at a construction project, in practice, data processing is still manual work with computer aided programs. To improve upon current workflows, this paper proposes a method to automatically extract point clouds of individual surfaces of structural components of a concrete building, which subsequently can be used to inspect construction quality based on geometric information of the surfaces. The proposed method explores both spatial point cloud information and contextual knowledge of structures (e.g., orientation or shape) derived from building design specifications and practice. For extracting point clouds of surfaces of each structural component, the proposed method consists of 4 consecutive steps for extracting: (1) floors, ceiling slabs, and walls, (2) columns, and (3) primary and (4) secondary beams. Each step consists of two ingredients: (i) rough extracting the candidate points of the component and (ii) fine filtering of the surface points of the components via cell-based and voxel-based region growing segmentation (CRG and VRG) incorporating contextual knowledge of the structural members. Experimental tests on two different types of concrete buildings showed that the proposed method successfully extracts the structural elements, in which the completeness, correctness, and quality from the point-based evaluation are larger than 96.0%, 96.9%, and 92.0%, respectively. Moreover, the evaluation based on a shape similarity showed that the extracted floor, ceiling slab and wall overlap to the ground truth more than 92.5%.}
}
@incollection{BHAT20221,
title = {Chapter 1 - Emerging trends and sustainability challenges in the global agri-food sector},
editor = {Rajeev Bhat},
booktitle = {Future Foods},
publisher = {Academic Press},
pages = {1-21},
year = {2022},
isbn = {978-0-323-91001-9},
doi = {https://doi.org/10.1016/B978-0-323-91001-9.00041-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323910019000414},
author = {Rajeev Bhat},
keywords = {Sustainability challenges, Technological innovations, Digitalization, Green technologies, Circular economy, Designer foods, Underutilized resources, Climate change impacts, Food frauds, Gastronomy},
abstract = {The global agri-food system has progressively evolved and is experiencing precipitous transformations. Of late, technological innovations have substantially revolutionized the entire agri-food production and supply chain systems. And, this is irrespective of a regions socio-economic stature. In spite of this, still there are several recurring global sustainability challenges that requires to be tackled. In this chapter, some of the currently encountered sustainability challenges and coherent practical solutions/innovations offered in the agri-food sector are deliberated. In the future, major focus will revolve around designing appropriate models/frameworks to overcome the challenges, understanding the requirements of a resilient food system, sharing of evidence-based research/new knowledge with the consumers, educating local population, realigning and introducing new agri-food policies, and implementation of realistic solutions. In the coming decades, the prospects of the entire agri-food sector are affirmative, however, all the key players (farming community, unions, government and non-governmental organizations, environmental groups, suppliers and manufacturers, agri-food scientists/technologists, policy makers, risk managers, consumers, and the public) need to drive together to contribute for a successful sustainable future.}
}
@article{DEWULF2022693,
title = {Advances in the metrological traceability and performance of X-ray computed tomography},
journal = {CIRP Annals},
volume = {71},
number = {2},
pages = {693-716},
year = {2022},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2022.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007850622001275},
author = {Wim Dewulf and Harald Bosse and Simone Carmignato and Richard Leach},
keywords = {X-ray, Metrology, Traceability},
abstract = {X-ray computed tomography (XCT) is increasingly being used for evaluating quality and conformance of complex products, including assemblies and additively manufactured parts. The metrological performance and traceability of XCT nevertheless remains an important research area that is reviewed in this paper. The error sources influencing XCT measurement results are discussed, along with related qualification, calibration and optimization procedures. Moreover, progress on performance verification testing and on the determination of task-specific measurement uncertainty is covered. Results of interlaboratory comparisons are summarized and performance in various dimensional measurement fields is illustrated. Conclusions and an outlook for future research activities are also provided.}
}
@article{KONOVALENKO2022116208,
title = {Generating decision support for alarm processing in cold supply chains using a hybrid k-NN algorithm},
journal = {Expert Systems with Applications},
volume = {190},
pages = {116208},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116208},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015220},
author = {Iurii Konovalenko and André Ludwig},
keywords = {k-nearest neighbors, Fuzzy set, Recommendation, Decision Support, Pharmaceutical supply chain, Temperature deviation},
abstract = {Real-time temperature monitoring is necessary in cold pharmaceutical supply chains (SCs), where exposures to extreme temperatures can lead to product quality deterioration. Temperature alarms (TAs) triggered by the current rule-based systems still require lengthy examinations before a suitable corrective measure (CM) can be chosen. However, provision of additional information relevant to TAs can expedite the examination process. In the related areas of recommender systems and false alarm/anomaly detection, k-nearest neighbors (k-NN) algorithm has proven to be successful because of its interpretability and ease of use. However, in the context of TA processing, it may suffer from some inherent limitations (i.e., varying neighborhood radius, unreliable classifications in sparse and noisy regions, and blindness to natural class boundaries). To overcome these limitations, we propose a hybrid k-NN (Hk-NN) algorithm based on the principles of local similarity and neighborhood homogeneity. It incorporates a two-step voting procedure with an entropy-optimized k-NN radius, decision trees with k-constrained leaves, and nearest neighbor predictions. We investigate 16,525 comments by alarm personnel for TAs in a pharmaceutical SC and encode them in terms of deviation causes and CMs (target features). We use SC data on cargo location, SC phase, sensor role, and temperature characteristics as predictor features for TA similarity estimation. In eight experimental setups, Hk-NN consistently outperforms k-NN with an optimized k in terms of accuracy, balanced accuracy, macro-average precision, recall, and specificity. At the same time, Hk-NN refrains from predicting observations, for which k-NN’s accuracy is close to a random guess.}
}
@article{MISHRA2022133595,
title = {Internet of Things (IoT) adoption challenges in renewable energy: A case study from a developing economy},
journal = {Journal of Cleaner Production},
volume = {371},
pages = {133595},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133595},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622031742},
author = {Rahul Mishra and B. Koteswara Rao Naik and Rakesh D. Raut and Mukesh Kumar},
keywords = {IoT, Digitalization of renewable energy(DRE), Renewable energy (RE), Fuzzy-DEMATEL, Exploratory factor analysis (EFA), Energy transition},
abstract = {Digitalization of Renewable Energy (RE) systems will play a decisive role in integrating clean energy sources and optimizing energy use. While energy digitalization is mainly governed by technological progress, the societal acceptance of such emerging technologies is essential for a successful energy transition. This study aims to identify critical challenges to IoT-enabled renewable energy use through the lens of the consumers. It attempts to analyze the challenges to implementing IoT effectively in emerging economies like India. The present study employs Exploratory Factor Analysis (EFA) to classify 16 critical challenges into five main dimensions, using the responses obtained from 95 participants. Subsequently, the fuzzy decision-making trial and evaluation laboratory (DEMATEL) prioritizes the identified challenges and organizes them into cause-and-effect groups based on their interactions. The findings indicate that technological challenges constitute the most critical cause barrier that affects other category barriers. Besides technology, challenges reflecting the social infrastructure have emerged prominently in this study. Thereafter, the analysis suggests that modifications to the structure, procedures, and technology are required to create capabilities and enhance the compatibility of IoT with RE applications. Further, proactive governance and reorienting market design are highlighted to foster emerging technologies and develop a robust regulatory framework. The insights from this study are pertinent for policymakers, regulatory bodies, and practitioners seeking to capitalize on the immense potential of this sector and expedite energy transactivity.}
}
@article{ERYARSOY2022108,
title = {Assessing IoT challenges in supply chain: A comparative study before and during- COVID-19 using interval valued neutrosophic analytical hierarchy process},
journal = {Journal of Business Research},
volume = {147},
pages = {108-123},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322002703},
author = {Enes Eryarsoy and Huseyin Selcuk Kilic and Selim Zaim and Marzhan Doszhanova},
keywords = {Internet of Things, IoT, Industrial internet of things, IIoT, Industry 4.0, i4.0, Supply chain management, COVID-19, Analytical hierarchy process (AHP), Multi-criteria decision analysis (MCDA), Interval-valued neutrosophic sets},
abstract = {Although the Internet of Things (IoT) has spawned a new breed of smart factories within supply chains, the latest pandemic has ushered in unparalleled supply chain disturbances. Following the challenges identified in the literature, we interview top experts to evaluate the significance of these challenges. We apply a multi-criteria decision analysis (MCDA) tool, analytical hierarchy process (AHP) in combination with interval-valued neutrosophic numbers (IVN). The critical part of this research is that we also perform a comparative analysis by focusing on before- and during- the pandemic periods individually to better assess the impact of the latest pandemic on the IoT challenges. Our study also includes a comprehensive, systematic literature review to bring the readers up-to-date.}
}
@article{PLEKHANOV2022,
title = {Digital transformation: A review and research agenda},
journal = {European Management Journal},
year = {2022},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2022.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0263237322001219},
author = {Dmitry Plekhanov and Henrik Franke and Torbjørn H. Netland},
keywords = {Digital transformation, Digitalization, Digital ecosystems, Organizational change, Literature review},
abstract = {The ongoing and ubiquitous digital transformation challenges the raison d'être of firms and forces managers to rethink business strategies and operations and academics to reconsider related theories. To aid these efforts, we conduct a systematic review of research on firms' digital transformation, generating a database of 537 peer-reviewed academic articles and analyzing it using a novel multi-layered framework. The framework separates three layers: an organization's core activities, its peripheral activities, and its external environment. We find that firms that have come far in their transformations are more embedded in platform ecosystems with unclear business boundaries. Relatedly, we identify a tension between decentralizing versus centralizing power across organizational layers during a firm's digital transformation and how this dynamic affects corporate strategies and firms' internal and external boundaries.}
}
@article{MONTINI2022661,
title = {An IIoT Platform For Human-Aware Factory Digital Twins},
journal = {Procedia CIRP},
volume = {107},
pages = {661-667},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.042},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003262},
author = {Elias Montini and Vincenzo Cutrona and Niko Bonomi and Giuseppe Landolfi and Andrea Bettoni and Paolo Rocco and Emanuele Carpanzano},
keywords = {Human Digital Twin (HDT), Cyber Physical Systems (CPS), Industrial Internet of Things (IIoT), Reference Data Model, Industry 5.0},
abstract = {In the context of the Industry 4.0 approach, applications and solutions supporting monitoring, simulation, optimisation and decision-making in production systems are exponentially growing. These solutions are commonly built on digital twins, i.e., comprehensive, structured and effective digital representations of the production system and its entities, whose current status is constantly updated by the plugged data sources. The arising of the Industry 5.0 paradigm and the established key role of workers in manufacturing require new Digital Twins to represent also humans. In fact, as cognitive automation becomes more and more pervasive and its behaviour unintelligible to humans, it becomes essential for improving performance and well-being, at the same time, to model humans as data-driven agents and to represent their interaction with the factory systems. Currently, a standardised solution for creating Digital Twins is missing, forcing industrial solution architects to resort to ad-hoc implementations and models. These solutions lack re-usability, scalability and extensibility, preventing the introduction of a human digital representation in existent twins, so hindering the complete shift to the new Industry 5.0 paradigm. In this paper, such limitations are faced by introducing an extensible and flexible IIoT-industrial internet of things-based platform with a twofold benefit: on the one hand, to support the creation of customised data representations of production systems and their entities including humans; on the other hand, to provide a modular infrastructure, along with its interchangeable components, for easy digital twin instantiation and ramp-up. An implementation of the platform has been tested with different applications in a laboratory setting and released as a public resource. Finally, potential future applications of the proposed digital twin are discussed, highlighting its main benefits.}
}
@article{LEITGOB2022102805,
title = {Measurement invariance in the social sciences: Historical development, methodological challenges, state of the art, and future perspectives},
journal = {Social Science Research},
pages = {102805},
year = {2022},
issn = {0049-089X},
doi = {https://doi.org/10.1016/j.ssresearch.2022.102805},
url = {https://www.sciencedirect.com/science/article/pii/S0049089X22001168},
author = {Heinz Leitgöb and Daniel Seddig and Tihomir Asparouhov and Dorothée Behr and Eldad Davidov and Kim {De Roover} and Suzanne Jak and Katharina Meitinger and Natalja Menold and Bengt Muthén and Maksim Rudnev and Peter Schmidt and Rens {van de Schoot}},
keywords = {Comparative research, Measurement invariance, Item bias, Noninvariance detection, Multiple group confirmatory factor analysis, Scale construction},
abstract = {This review summarizes the current state of the art of statistical and (survey) methodological research on measurement (non)invariance, which is considered a core challenge for the comparative social sciences. After outlining the historical roots, conceptual details, and standard procedures for measurement invariance testing, the paper focuses in particular on the statistical developments that have been achieved in the last 10 years. These include Bayesian approximate measurement invariance, the alignment method, measurement invariance testing within the multilevel modeling framework, mixture multigroup factor analysis, the measurement invariance explorer, and the response shift-true change decomposition approach. Furthermore, the contribution of survey methodological research to the construction of invariant measurement instruments is explicitly addressed and highlighted, including the issues of design decisions, pretesting, scale adoption, and translation. The paper ends with an outlook on future research perspectives.}
}
@incollection{HOVENGA2022169,
title = {Chapter 8 - Health data standards’ limitations},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {169-207},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00015-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823413600015X},
author = {Evelyn Hovenga and Heather Grain},
keywords = {Ontology, Terminology, Data set, Electronic data processing, Big data, Data management, Linguistics},
abstract = {Data represent foundational assets of any healthcare delivery system. Clinical data form the basis of electronic communications from point of data collection to storage and archiving. Computers cannot handle ambivalence, hence the need for the widespread adoption of technical and terminology standards. Many domain ontologies and terminologies, developed to suit a variety of different purposes well before this digital era, are reviewed and examined to determine their usability within a digital ecosystem. The ontological data modelling approach was found to result in the highest degrees of expressivity and formalism available today. Resulting artefacts linked to standard value sets were found to be most comprehensive with their ability to best represent data for a lifetime support, patient safety, and electronic communication. Many issues and limitations, such as variations regarding design principles used, overlaps, and shortcomings, are identified and discussed. There is a need for a major globally led transformation.}
}
@article{GRUBEL2022101640,
title = {Dense Indoor Sensor Networks: Towards passively sensing human presence with LoRaWAN},
journal = {Pervasive and Mobile Computing},
volume = {84},
pages = {101640},
year = {2022},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2022.101640},
url = {https://www.sciencedirect.com/science/article/pii/S1574119222000700},
author = {Jascha Grübel and Tyler Thrash and Leonel Aguilar and Michal Gath-Morad and Didier Hélal and Robert W. Sumner and Christph Hölscher and Victor R. Schinazi},
keywords = {Dense Indoor Sensor Network, , Fused Twins, Human presence, Effective Signal Power},
abstract = {Sensors have become ubiquitous in buildings but are rarely connected to a network, and their potential to analyse the performance, use, and interaction with a building is not yet fully realised. In the coming years, we expect sensors in buildings to become part of the Internet of Things (IoT) and grow in numbers to form a Dense Indoor Sensor Network (DISN) that allows for unprecedented analysis of the performance, use, and interaction with buildings. Multiple technologies vie for leading this transformation. We explore Long Range Wide Area Network (LoRaWAN) as an alternative for creating indoor sensor networks that extends beyond its original long-distance communication purpose. For the present paper, we developed a DISN with 390 sensor nodes and four gateways and empirically evaluated its performance for two years. Our analysis of more than 86 million transmissions revealed that DISNs achieve a much lower distance coverage compared to estimations from previous research indicating that more gateways are required. In addition, the deployment of multiple gateways decreased the loss of transmissions due to environmental and network factors. Given the complexity of our system, we received few colliding concurrent messages, which demonstrates a gap between the projected requirements of LoRaWAN systems and the actual requirements of real-world applications given sufficient gateways. We also contribute to the modelling of transmissions with our comparison of attenuation models derived from multiple methodologies. Across all models, we find that robust coverage in an indoor environment can be maintained by placing a gateway every 30 m and every 5 floors. Finally, we also investigate the application of DISNs for the passive sensing and visualisation of human presence using a Digital Twin (DT) and a Fused Twins (FT) representation in Augmented Reality (AR). A passive sensing approach allows us to gather relevant data on human use of a building while still preserving privacy via the aggregation process. Immersive in situ visualisations in FT allow for new interactions and new forms of participation. We conclude that DISNs are already technologically feasible today and basing them on Low Power Wide Area Network (LPWAN) offers intriguing possibilities to reduce energy consumption, maintenance cost, and bandwidth use while also enabling new forms of human-building interaction.}
}
@article{KUEHNE2022,
title = {Causal analyses with target trial emulation for real-world evidence removed large self-inflicted biases: Systematic bias assessment of ovarian cancer treatment effectiveness},
journal = {Journal of Clinical Epidemiology},
year = {2022},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2022.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0895435622002463},
author = {Felicitas Kuehne and Marjan Arvandi and Lisa M. Hess and Douglas E. Faries and Raffaella Matteucci Gothe and Holger Gothe and Julie Beyrer and Alain Gustave Zeimet and Igor Stojkov and Nikolai Mühlberger and Willi Oberaigner and Christian Marth and Uwe Siebert},
keywords = {causal inference, comparative effectiveness, longitudinal data, electronic health records, target trial, inverse probability weighting},
abstract = {Background
Drawing causal conclusions from real-world data (RWD) poses methodological challenges and risk of bias. We aimed to systematically assess the type and impact of potential biases that may occur when analyzing RWD using the case of progressive ovarian cancer.
Methods
We retrospectively compared overall survival with and without second-line chemotherapy using electronic medical records. Potential biases were determined using directed acyclic graphs. We followed a stepwise analytic approach ranging from crude analysis and multivariable-adjusted Cox model up to a full causal analysis using a marginal-structural-Cox-model (MSCM) with replicates emulating a reference randomized controlled trial. To assess biases, we compared effect estimates (hazard ratios [HRs]) of each approach to the HR of the reference trial.
Results
The reference trial showed a HR for second-line versus delayed-therapy of 1.01 (95% confidence interval [95%CI]: 0.82-1.25). The corresponding HRs from the RWD analysis ranged from 0.51 for simple baseline adjustments to 1.41 (95%CI 1.22-1.64) accounting for immortal time bias with time-varying covariates. Causal trial emulation yielded a HR of 1.12 (95%CI: 0.96-1.28).
Conclusions
Our study, using ovarian cancer as an example, shows the importance of a thorough causal design and analysis if one is expecting RWD to emulate clinical trial results.}
}
@article{WANG2022108870,
title = {Data-attention-YOLO (DAY): A comprehensive framework for mesoscale eddy identification},
journal = {Pattern Recognition},
volume = {131},
pages = {108870},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108870},
url = {https://www.sciencedirect.com/science/article/pii/S003132032200351X},
author = {Xinning Wang and Xuegong Wang and Chong Li and Yuben Zhao and Peng Ren},
keywords = {Mesoscale eddy identification, Attention mechanism, Data-attention-based YOLO, One-stage detection},
abstract = {The accurate mesoscale eddy identification methods with deep learning framework depend on either single eddy characteristic from altimeter missions or multi-step eddy examination strategies, disregarding those indistinguishable features from multiple eddy data integration. In this article, we first propose a data-attention-based YOLO (DAY) to precisely recognize mesoscale eddies in the South China Sea (SCS), which can hierarchically unite multiple eddy attributes and efficiently predict eddies with one-step strategy involving detection and classification. It consists of two main components: heterogeneous eddy data integration module and dynamic attention detecting module for eddy identification. The data integration component empirically transforms the field of multi-source eddy data and propagates eddy labels through automatic labeling method, which sustains a good supply for our dynamic attention-base detecting network. To thoroughly identify mesoscale eddies based on spatio-temporal patterns, DAY efficiently learns the characteristics of mesoscale eddies with an improved one-step identification YOLO network. The comparative evaluation results demonstrate that DAY achieves 54% performance improvement over the state-of-the-art methods on single gray SLA data and outperforms two-stage detecting technique Faster R-CNN by 51%.}
}
@article{LI2022131944,
title = {Investigating the spatiotemporal changes and driving factors of nighttime light patterns in RCEP Countries based on remote sensed satellite images},
journal = {Journal of Cleaner Production},
volume = {359},
pages = {131944},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.131944},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622015530},
author = {Jie Li and Suling He and Jinliang Wang and Weifeng Ma and Hui Ye},
keywords = {Nighttime light (NTL), Carbon emission, Mann-Kendall test, Hurst analysis, The regional comprehensive economic partnership (RCEP)},
abstract = {The Regional Comprehensive Economic Partnership (RCEP) agreement signed in November 2020 is the world's largest and greatest potential free trade area. It was officially enforced on January 1, 2022, which attracted great worldwide attention. The RCEP advocates for the development of the low-carbon economy but lacks effective monitoring methods for socioeconomic conditions and carbon emissions. Nighttime light (NTL) images can objectively reflect socioeconomic status and carbon emission potential. However, most studies focused only on consistency correction for multi-source NTL data without deeply analyzing NTL dynamics. An in-depth analysis of the NTL change in the RCEP region is necessary and will strongly facilitate the strategic deployment of a low-carbon economy in RCEP countries. Sen's slope estimator, the Mann-Kendall trend test, the Mann-Kendall mutation test, and Hurst analysis were adopted to analyze the spatiotemporal changes of NTL in the past and future, and gray relational analysis was applied to explore driving factors. The results showed that (1) RCEP's NTL became brighter from 2000 to 2018, and the total NTL amount in 2018 was 3.27 times that in 2000. NTL in all countries except Japan showed an increasing trend to varying degrees, and this trend was more pronounced in developing countries. The maximum increase amount and growth rate were in China and Vietnam, respectively. (2) Areas where NTL showed an increasing trend in the past accounted for 76.67% of the NTL zones, and the areas where predicted to be brighter in the future accounted for 63.87%. These regions were generally clustered in developed urban zones. However, most Japanese cities have observed darkening trends. (3) The relational grade between NTL and all socioeconomic factors in the RECP region was greater than 0.6, and construction land expansion was the most direct and profound driver. Compared with that in developed countries, NTL in developing countries was more closely related to socioeconomic factors.}
}
@article{ALASSAF20222849,
title = {Improving Sentiment Analysis of Arabic Tweets by One-way ANOVA},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part A},
pages = {2849-2859},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820305176},
author = {Manar Alassaf and Ali Mustafa Qamar},
keywords = {Sentiment analysis, One-way ANOVA, Arabic tweets, Feature selection, Machine learning, High dimensionality},
abstract = {Social media is an indispensable necessity for modern life. As a result, it is full of people’s opinions, emotions, ideas, and attitudes, whether positive or negative. This abundance of views creates many opportunities for applying sentiment analysis to the education sector, which reflects how countries and cultures develop. In this research, a real-world Twitter dataset was collected, containing approximately 8144 tweets related to Qassim University, Saudi Arabia. The main aim of this experimental study was to explore the possibility of using a one-way analysis of variance (ANOVA) as a feature selection method to considerably reduce the number of features when classifying opinions conveyed through Arabic tweets. The primary motivation for this research was that no previous studies had examined one-way ANOVA comprehensively to tackle the curse of dimensionality and to enhance classification performance in sentiment analysis for Arabic tweets. Therefore, various experiments were conducted to investigate the effects of one-way ANOVA and to select important features concerning the performance of different supervised machine learning classifiers. Support Vector Machine and Naïve Bayes achieved the best results with one-way ANOVA as compared to the baseline experimental results in the collected dataset. Furthermore, the differences between all results have been statistically analyzed in this study. As further evidence, one-way ANOVA with Support Vector Machine represented an excellent combination across different Arabic benchmark datasets, with its results outperforming other studies.}
}
@article{WAGELE2022105,
title = {Towards a multisensor station for automated biodiversity monitoring},
journal = {Basic and Applied Ecology},
volume = {59},
pages = {105-138},
year = {2022},
issn = {1439-1791},
doi = {https://doi.org/10.1016/j.baae.2022.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1439179122000032},
author = {J.Wolfgang Wägele and Paul Bodesheim and Sarah J. Bourlat and Joachim Denzler and Michael Diepenbroek and Vera Fonseca and Karl-Heinz Frommolt and Matthias F. Geiger and Birgit Gemeinholzer and Frank Oliver Glöckner and Timm Haucke and Ameli Kirse and Alexander Kölpin and Ivaylo Kostadinov and Hjalmar S. Kühl and Frank Kurth and Mario Lasseck and Sascha Liedke and Florian Losch and Sandra Müller and Natalia Petrovskaya and Krzysztof Piotrowski and Bernd Radig and Christoph Scherber and Lukas Schoppmann and Jan Schulz and Volker Steinhage and Georg F. Tschan and Wolfgang Vautz and Domenico Velotto and Maximilian Weigend and Stefan Wildermann},
keywords = {Biodiversity monitoring, AMMOD, Bioacoustic monitoring, Visual monitoring, Computer vision, Metabarcoding, Volatile organic compounds, Pattern recognition, Computer science, Artificial intelligence},
abstract = {Rapid changes of the biosphere observed in recent years are caused by both small and large scale drivers, like shifts in temperature, transformations in land-use, or changes in the energy budget of systems. While the latter processes are easily quantifiable, documentation of the loss of biodiversity and community structure is more difficult. Changes in organismal abundance and diversity are barely documented. Censuses of species are usually fragmentary and inferred by often spatially, temporally and ecologically unsatisfactory simple species lists for individual study sites. Thus, detrimental global processes and their drivers often remain unrevealed. A major impediment to monitoring species diversity is the lack of human taxonomic expertise that is implicitly required for large-scale and fine-grained assessments. Another is the large amount of personnel and associated costs needed to cover large scales, or the inaccessibility of remote but nonetheless affected areas. To overcome these limitations we propose a network of Automated Multisensor stations for Monitoring of species Diversity (AMMODs) to pave the way for a new generation of biodiversity assessment centers. This network combines cutting-edge technologies with biodiversity informatics and expert systems that conserve expert knowledge. Each AMMOD station combines autonomous samplers for insects, pollen and spores, audio recorders for vocalizing animals, sensors for volatile organic compounds emitted by plants (pVOCs) and camera traps for mammals and small invertebrates. AMMODs are largely self-containing and have the ability to pre-process data (e.g. for noise filtering) prior to transmission to receiver stations for storage, integration and analyses. Installation on sites that are difficult to access require a sophisticated and challenging system design with optimum balance between power requirements, bandwidth for data transmission, required service, and operation under all environmental conditions for years. An important prerequisite for automated species identification are databases of DNA barcodes, animal sounds, for pVOCs, and images used as training data for automated species identification. AMMOD stations thus become a key component to advance the field of biodiversity monitoring for research and policy by delivering biodiversity data at an unprecedented spatial and temporal resolution.}
}
@article{CHENG2022101654,
title = {Eye gaze and visual attention as a window into leadership and followership: A review of empirical insights and future directions},
journal = {The Leadership Quarterly},
pages = {101654},
year = {2022},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2022.101654},
url = {https://www.sciencedirect.com/science/article/pii/S1048984322000571},
author = {Joey T. Cheng and Fabiola H. Gerpott and Alex J. Benson and Berno Bucker and Tom Foulsham and Tessa A.M. Lansu and Oliver Schülke and Keiko Tsuchiya},
keywords = {Eye gaze, Attention, Leadership, Followership, Group dynamics},
abstract = {Illuminating the nature of leadership and followership requires insights into not only how leaders and followers behave, but also the different cognitions that underpin these social relationships. We argue that the roots of leader and follower roles and status asymmetries often lie in basic mental processes such as attention and visual perception. To understand not only how but also why leaders’ and followers’ behavioral patterns vary, we focus here on underpinning attentional processes that often drive rank-based behaviors. Methodologically, this focus on basic attentional and perceptual processes lessens the reliance on self-report and questionnaire-based data, and expands our scientific understanding to actual, real-world leadership dynamics. Here, we review the available evidence indicating that leaders and followers differ in whether and how they receive, direct, and pay visual attention. Our review brings together diverse empirical evidence from organization science, primatology, and social, developmental, and cognitive psychology on eye gaze, attention, and status in adults, children, and non-human primates. Based on this review of the cross-disciplinary literature, we propose future directions and research questions that this attention-based approach can generate for illuminating the puzzle of leadership and followership.}
}
@article{DONG2022112947,
title = {Practical application of energy management strategy for hybrid electric vehicles based on intelligent and connected technologies: Development stages, challenges, and future trends},
journal = {Renewable and Sustainable Energy Reviews},
volume = {170},
pages = {112947},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112947},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122008280},
author = {Peng Dong and Junwei Zhao and Xuewu Liu and Jian Wu and Xiangyang Xu and Yanfang Liu and Shuhan Wang and Wei Guo},
keywords = {Hybrid electric vehicles(HEVs), Energy management strategies(EMSs), Development stages, Practical challenges, Implementation framework},
abstract = {The rapid development of intelligent and connected technologies is conducive to the efficient energy utilization of hybrid electric vehicles (HEVs). However, most energy management strategies (EMSs) with optimized, intelligent, and connected functions have not been directly applied to such vehicles because existing technical conditions cannot meet the theoretical requirements of complex EMSs. Therefore, based on the mapping relationship between the information decision-making ability and the energy management effect, this study is the first to propose four development stages of HEV energy management practical application as follows: energy management based on instantaneous driving cycles (Stage 1 or S1); energy management based on forward driving cycle prediction (Stage 2 or S2); energy management based on global driving cycle prediction (Stage 3 or S3); and energy management based on autonomous velocity planning (Stage 4 or S4). The key technologies of each development stage are not independent, i.e., they complement each other in the process of practical application development. Furthermore, realizing energy management practical applications not only requires novel algorithm models but also involves several challenges such as acquiring and processing multi-source information, predicting the vehicle power demand in the spatial–temporal domain during travel, and determining the vehicle control characteristics and ability. Finally, according to the development goals of energy management, this study proposes an implementation framework for HEV energy management in higher development stages, namely cooperative vehicle–edge–cloud for intelligent energy management, i.e., CVEC- IEM, which executes information decision tasks on different computing platforms and realizes interconnection and interaction to provide development directions and goals for the efficient utilization of energy and successful deployment of HEV practical applications.}
}
@incollection{BRIGHTPONTE2022265,
title = {Chapter 15 - Postmarket surveillance and regulatory considerations in reproductive and developmental toxicology: a Food and Drug Administration perspective},
editor = {Ramesh C. Gupta},
booktitle = {Reproductive and Developmental Toxicology (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {265-282},
year = {2022},
isbn = {978-0-323-89773-0},
doi = {https://doi.org/10.1016/B978-0-323-89773-0.00015-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323897730000151},
author = {Susan Bright-Ponte},
keywords = {Drugs, FDA, Pharmacovigilance, Safety, Surveillance, Vaccines},
abstract = {Pharmaceutical products for humans and animals may have toxic effects on human or animal reproductive and developmental processes. Various governmental agencies are charged with monitoring the safety of pharmaceuticals and biologics used to treat or prevent disease in humans and animals. Adverse event reporting systems and pharmacovigilance are important for monitoring the safety of medical products and in protecting public health. The US Food and Drug Administration (US FDA) is a regulatory, science-based federal agency responsible for protecting and promoting the public health through the monitoring and regulation of a number of items necessary for the health and well-being of consumers. This chapter will focus on the FDA's pharmacovigilance and surveillance activities as related to the regulation of marketed drugs and vaccines intended for use in humans, and drugs intended for use in animals. Examples of regulatory programs and issues specifically regarding potential reproductive and developmental toxicity of pharmaceutical products will be presented.}
}
@article{CHALE2022117936,
title = {Generating realistic cyber data for training and evaluating machine learning classifiers for network intrusion detection systems},
journal = {Expert Systems with Applications},
volume = {207},
pages = {117936},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117936},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422011757},
author = {Marc Chalé and Nathaniel D. Bastian},
keywords = {Generative machine learning, Generative adversarial network, Variational autoencoder, Synthetic data generation, Network intrusion detection},
abstract = {Cyberspace operations, in conjunction with artificial intelligence and machine learning enhanced cyberspace infrastructure, make it possible to connect sensors directly to shooters independent of human control. These technologies serve as the pivot around which cyber data from the military’s Internet of Battlefield Things, for example, will be turned into actionable insight and knowledge and, ultimately, an information advantage for the military. As such, network intrusion detection systems must detect, evaluate, and respond to malicious cyber traffic at machine speed. Generative adversarial networks and variational autoencoders are fit as generative models with labeled cyber data from a real military enterprise network. These generative models are used to create realistic, synthetic cyber data. A combination of real and synthetic cyber data sets are then used to train several machine learning models for network intrusion detection. Purely synthetic data is shown to be statistically similar to the real data. There is no statistically significant difference in the performance of classifiers trained with real data versus a combination of real and synthetic data; however, classifiers trained with only synthetic data underperformed. To avoid a decrease in intrusion detection performance, classifiers must be trained with at least 15% real data.}
}
@incollection{JEYAKUMAR2022347,
title = {Chapter 30 - A smart virtual vision system for health monitoring},
editor = {Valentina Emilia Balas and Oana Geman},
booktitle = {Biomedical Engineering Applications for People with Disabilities and the Elderly in the COVID-19 Pandemic and Beyond},
publisher = {Academic Press},
pages = {347-360},
year = {2022},
isbn = {978-0-323-85174-9},
doi = {https://doi.org/10.1016/B978-0-323-85174-9.00021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851749000212},
author = {Vijay Jeyakumar and K. Nirmala and R. Nithya},
keywords = {Assistive device, COVID-19, Health monitoring, Home monitoring, Intelligent system, Virtual vision system},
abstract = {One of the greatest challenges in regard to the elderly is the autonomous and healthy availability of appropriate beneficial and social functionalities to achieve the primary goal of prolonging independent living at home. In 2050, the forecast population of elderly people in India is about 300 million. The traditional kind of joint family in Indian culture has come under pressure due to family planning awareness, migration to cities, a lack of sustainability, and inadequate guidance from the elders. A virtual vision system (VVS) is a home-based monitoring device that captures the scene continuously to aid the user. The system helps the individual to navigate in a closed environment. The user can provide the system with commands, queries, and/or demands using free form natural language input to receive help. A wearable sensor that can track physiological parameters such as oxygen saturation, temperature, and pulse rate also can be integrated into the system. The risk of falling and potential injury is one reason for the elderly being placed in care facilities. The system also supports users by tracking their mobility, and helps to identify falls. The system autonomously patrols the user's environment without any user activity and checks if the user is well and has not suffered a fall. The VVS emphasis is on an application platform that incorporates distinct technical solutions such as biometric tools, remote doctor visits, emergency call and tracking systems, drug dispensers, and online shopping. In elderly care, continuous monitoring helps not only to focus on improving the senior's safety when a caregiver is not present, but also provides peace of mind to adult children who may be concerned about the welfare of their loved one. Such systems would be helpful for those in self-quarantine/isolation during this COVID-19 pandemic situation.}
}
@article{MELESSE20222743,
title = {Digital Twin for Inventory Planning of Fresh Produce},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2743-2748},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.134},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021474},
author = {Tsega Y. Melesse and Matteo Bollo and Valentina Di Pasquale and Stefano Riemma},
keywords = {Time-series, Digital Twin, Predictive Forecast, Inventory Planning, Fruits},
abstract = {The management of perishable food inventory demands special attention. Fruits quickly lose their freshness and perish if they are not consumed within a specified period. It is critical to develop a management tool based on the Internet of Things that can efficiently integrate all the dynamic data associated with various types of resources in real-time along the supply chain. This research is part of a comprehensive supply chain framework developed to analyze food bank logistics supply chain interactions. The study will mainly focus on the use of historical time-series data to create a digital twin that can anticipate future events. The digital twin framework was built based on the operational trend of the Italian food bank to strengthen the decision support system related to the fresh food inventory. The SAP Analytics Cloud was used to create a solution that would help the organization better satisfy consumer needs by reducing fruit waste in the inventory.}
}
@article{BEHERA2022685,
title = {Cognitive computing based ethical principles for improving organisational reputation: A B2B digital marketing perspective},
journal = {Journal of Business Research},
volume = {141},
pages = {685-701},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.11.070},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321008778},
author = {Rajat Kumar Behera and Pradip Kumar Bala and Nripendra P. Rana and Hatice Kizgin},
keywords = {Cognitive computing, B2B digital marketing, Organisational effectiveness, Organisational reputation, Ethics},
abstract = {Cognitive computing is ushering in the fourth industrial revolution through its promises of improved accuracy, scalability and personalisation. Therefore, business-to-business (B2B) organisations are wavering in the decision for adoption into their digital marketing initiatives. However, embracing moral rules and/or moral judgments in their digital marketing innovation can be challenging, since making mistakes could damage reputations. Therefore, this study applies the ethical principles of cognitive computing in B2B digital marketing business-centric ethical challenges. An integrated theoretical framework grounded on multidisciplinary studies is proposed. The primary data were collected from 300 respondents within B2B businesses. The results of this research led to the conclusion that good ethical practices are essential for the improvement of both organisational effectiveness and organisational reputation. Increased organisational reputation delivers a competitive edge in fast-growing marketplaces. B2B businesses need to look for proactive ways to achieve continuous improvement.}
}
@article{WANG20229,
title = {A comprehensive review for wind, solar, and electrical load forecasting methods},
journal = {Global Energy Interconnection},
volume = {5},
number = {1},
pages = {9-30},
year = {2022},
issn = {2096-5117},
doi = {https://doi.org/10.1016/j.gloei.2022.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096511722000226},
author = {Han Wang and Ning Zhang and Ershun Du and Jie Yan and Shuang Han and Yongqian Liu},
keywords = {Wind power, Solar power, Electrical load, Forecasting, Numerical Weather Prediction, Correlation},
abstract = {Wind power, solar power, and electrical load forecasting are essential works to ensure the safe and stable operation of the electric power system. With the increasing permeability of new energy and the rising demand response load, the uncertainty on the production and load sides are both increased, bringing new challenges to the forecasting work and putting forward higher requirements to the forecasting accuracy. Most review/survey papers focus on one specific forecasting object (wind, solar, or load), a few involve the above two or three objects, but the forecasting objects are surveyed separately. Some papers predict at least two kinds of objects simultaneously to cope with the increasing uncertainty at both production and load sides. However, there is no corresponding review at present. Hence, our study provides a comprehensive review of wind, solar, and electrical load forecasting methods. Furthermore, the survey of Numerical Weather Prediction wind speed/irradiance correction methods is also included in this manuscript. Challenges and future research directions are discussed at last.}
}
@article{OSTBERG202219,
title = {Domain Models and Data Modeling as Drivers for Data Management: The ASSISTANT Data Fabric Approach},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {19-24},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.362},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322016287},
author = {Per-Olov Östberg and Eduardo Vyhmeister and Gabriel G. Castañé and Bart Meyers and Johan {Van Noten}},
keywords = {Domain Models, Knowledge Graph, Data Modeling, Data Fabric, Data Base, Data Lake, AI, adaptive manufacturing},
abstract = {To develop AI-based models capable of governing or providing decision support to complex manufacturing environments, abstractions and mechanisms for unified management of data storage and processing capabilities are needed. Specifically, as such models tend to include and rely on detailed representations of systems, components, and tools with complex interactions, mechanisms for simplifying, integrating, and scaling management capabilities in the presence of complex data requirements (e.g., high volume, velocity, and diversity of data) are of particular interest. A data fabric is a system that provides a unified architecture for management and provisioning of data. In this work we present the background, design requirements, and high-level outline of the ASSISTANT data fabric - a flexible data management tool designed for use in adaptive manufacturing contexts. The paper outlines the implementation of the system with specific focus on the use of domain models and the data modeling approach used, as well as provides a generic use case structure reusable in many industrial contexts.}
}
@incollection{YANG2022245,
title = {Chapter 9 - MaaS system visualization},
editor = {Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
booktitle = {Big Data and Mobility as a Service},
publisher = {Elsevier},
pages = {245-263},
year = {2022},
isbn = {978-0-323-90169-7},
doi = {https://doi.org/10.1016/B978-0-323-90169-7.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901697000105},
author = {Chuang Yang and Renhe Jiang and Ryosuke Shibasaki},
keywords = {Mobility as a Service, Data visualization, Visual analytics, Intelligence transportation system, Spatial information science},
abstract = {With the MaaS system running, massive amounts of data are generated (such as GPS trajectory data of moving objects, daily delay data of metros, and ridership data). These data contain a considerable amount of information, knowledge, and insights, giving rise to the demand to perceive, understand, and utilize them, which visualization technology has native potential for such scenarios. In this paper, we summarize the existing visualization technologies of the current transportation system into a multi-view frame, covering the perspective of the three MaaS's leading actors, i.e., demanders of mobility, suppliers of transport services, and city managers. Three real-world application cases from different actor perspectives are introduced. Moreover, we recommend some practical open-source tools and libraries for MaaS system visualization and discuss some future challenges and directions of MaaS visualization.}
}
@article{VANDERSLOOT2022105716,
title = {Deepfakes: regulatory challenges for the synthetic society},
journal = {Computer Law & Security Review},
volume = {46},
pages = {105716},
year = {2022},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2022.105716},
url = {https://www.sciencedirect.com/science/article/pii/S0267364922000632},
author = {Bart {van der Sloot} and Yvette Wagensveld},
keywords = {Deepfake, synethetic media, post-truth era, Privacy, freedom of expression, rule of law, democracy, social equality, fake news, non-consensual fake porn},
abstract = {With the rise of deepfakes and synthetic media, the question as to what is real and what is not will become increasingly important and politized. Deepfakes can be used to spread fake news, influence elections, introduce highly realistic fake evidence in courts and make fake porno movies. Each of these applications potentially has a big impact on society, social relationships, democracy and the rule of law. The question this article shall assess is whether the current regulatory regime suffices to address these potential harms and if not, which additional rules and principles should be adopted. It will discuss several potential amendments to the privacy and data protection regime, limitations to the freedom of expression and ex ante rules on the distribution of use of deepfake-technologies.}
}
@article{ZHANG2022133,
title = {A fault diagnosis method for wind turbines with limited labeled data based on balanced joint adaptive network},
journal = {Neurocomputing},
volume = {481},
pages = {133-153},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.067},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222000868},
author = {Guangyao Zhang and Yanting Li and Wenbo Jiang and Lianjie Shu},
keywords = {Wind turbine, Fault diagnosis, Transfer learning, Domain adaptation, SCADA, MMD, LSTM},
abstract = {Traditional machine learning or deep learning relies on a sufficient amount of labeled data for the training of fault diagnosis models. However, for new wind farms, insufficient data and limited labels hinder the establishment of such models. In order to cope with these two challenges, we proposed a new domain adaptive method for wind turbine fault diagnosis: balanced joint adaptive network (BJAN), which can transfer wind turbine data from other wind farms to the target new wind farm. In this method, we proposed a new pseudo-label prediction method that combines the sub-domain majority voting and overall iterations (SMV-I) to label the unlabeled data. In addition, BJAN uses long short-term memory model (LSTM) to replace common convolutional neural network (CNN) as the feature extraction module to improve diagnosis efficiency. Moreover, we also proposed a new distributed adaptive distance for BJAN: balanced joint maximum mean discrepancy (BJMMD), which can balance the data of different states during the distributed adaptive process to improve diagnostic accuracy. Numerical experiments with real wind turbine data in three wind farms not only show that the proposed SMV-I has excellent pseudo-label prediction performance, but also verify that the proposed fault diagnosis model BJAN has higher diagnostic accuracy and efficiency.}
}
@article{PROUTSKOVA2022100735,
title = {The Jazz Ontology: A semantic model and large-scale RDF repositories for jazz},
journal = {Journal of Web Semantics},
volume = {74},
pages = {100735},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100735},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000245},
author = {Polina Proutskova and Daniel Wolff and György Fazekas and Klaus Frieler and Frank Höger and Olga Velichkina and Gabriel Solis and Tillman Weyde and Martin Pfleiderer and Hèlène Camille Crayencour and Geoffroy Peeters and Simon Dixon},
keywords = {Semantic modelling, Music, Metadata},
abstract = {Jazz is a musical tradition that is just over 100 years old; unlike in other Western musical traditions, improvisation plays a central role in jazz. Modelling the domain of jazz poses some ontological challenges due to specificities in musical content and performance practice, such as band lineup fluidity and importance of short melodic patterns for improvisation. This paper presents the Jazz Ontology – a semantic model that addresses these challenges. Additionally, the model also describes workflows for annotating recordings with melody transcriptions and for pattern search. The Jazz Ontology incorporates existing standards and ontologies such as FRBR and the Music Ontology. The ontology has been assessed by examining how well it supports describing and merging existing datasets and whether it facilitates novel discoveries in a music browsing application. The utility of the ontology is also demonstrated in a novel framework for managing jazz related music information. This involves the population of the Jazz Ontology with the metadata from large scale audio and bibliographic corpora (the Jazz Encyclopedia and the Jazz Discography). The resulting RDF datasets were merged and linked to existing Linked Open Data resources. These datasets are publicly available and are driving an online application that is being used by jazz researchers and music lovers for the systematic study of jazz.}
}
@article{ROBSON2022105118,
title = {Mining real-world high dimensional structured data in medicine and its use in decision support. Some different perspectives on unknowns, interdependency, and distinguishability},
journal = {Computers in Biology and Medicine},
volume = {141},
pages = {105118},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.105118},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521009124},
author = {Barry Robson and S. Boray and J. Weisman},
keywords = {Real world data, Assumptions, Approximations, Unknowns, Interdependency, Distinguishability, Coherence, Inference net, Bayes' rule, Bayes net, Hyperbolic Dirac net, Clinical decision support},
abstract = {Synopsis
There are many difficulties in extracting and using knowledge for medical analytic and predictive purposes from Real-World Data, even when the data is already well structured in the manner of a large spreadsheet. Preparative curation and standardization or “normalization” of such data involves a variety of chores but underlying them is an interrelated set of fundamental problems that can in part be dealt with automatically during the datamining and inference processes. These fundamental problems are reviewed here and illustrated and investigated with examples. They concern the treatment of unknowns, the need to avoid independency assumptions, and the appearance of entries that may not be fully distinguished from each other. Unknowns include errors detected as implausible (e.g., out of range) values that are subsequently converted to unknowns. These problems are further impacted by high dimensionality and problems of sparse data that inevitably arise from high-dimensional datamining even if the data is extensive. All these considerations are different aspects of incomplete information, though they also relate to problems that arise if care is not taken to avoid or ameliorate consequences of including the same information twice or more, or if misleading or inconsistent information is combined. This paper addresses these aspects from a slightly different perspective using the Q-UEL language and inference methods based on it by borrowing some ideas from the mathematics of quantum mechanics and information theory. It takes the view that detection and correction of probabilistic elements of knowledge subsequently used in inference need only involve testing and correction so that they satisfy certain extended notions of coherence between probabilities. This is by no means the only possible view, and it is explored here and later compared with a related notion of consistency.}
}
@article{PENG2022336,
title = {Global estimates of 500 m daily aerodynamic roughness length from MODIS data},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {183},
pages = {336-351},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621003130},
author = {Zhong Peng and Ronglin Tang and Yazhen Jiang and Meng Liu and Zhao-Liang Li},
keywords = {Aerodynamic roughness length, Machine learning, MODIS, Evapotranspiration},
abstract = {Aerodynamic roughness length (z0m) is a key parameter in the characterization of land surface turbulent heat fluxes and widely used in many surface and climate-related process models. The global products of time series of z0m at finer spatio-temporal resolution, however, have never been publicly available. Here we presented a practical method for global estimates of 500 m daily z0m with a combination of machine learning techniques, wind profile equation, observations from 273 sites and MODIS remote sensing data. Results showed that the random forest (RF) model outperformed the deep neural network (DNN) and convolutional neural network (CNN) models, and it could well reproduce the magnitude and temporal variability of daily z0m at almost all sites for all land cover types. In the validation of the RF-estimated daily z0m with the in-situ observations, the root mean square error (RMSE) varied between 0.02 m and 0.09 m, the mean absolute error (MAE) varied between 0.01 m and 0.05 m and the coefficient of determination (R2) was 1 for medium-to-high canopy shrublands, savannas and forests; for short-canopy croplands, grasslands and wetlands, the RMSE and MAE were 0.02 m and 0.01 m, respectively, and the R2 varied between 0.94 and 1. Compared to the Climate Forecast System Version 2 (CFSv2, 0.3°/monthly) and ECMWF Reanalysis v5 (ERA5, 0.25°/monthly) products in 2019, the RF-estimated z0m was found to have the similar global spatial pattern but significantly larger temporal variability, and it also showed a higher and lower global mean of z0m over forests and non-forests, respectively. The RF-estimated z0m displayed a higher temporal variability but a similar global spatial pattern of this variability compared to the CFSv2, whereas the ERA5 z0m product exhibited almost no temporal variability except for grasslands and croplands. This study is beneficial for improving the simulation of the momentum, water and energy transfer between land and atmosphere and helping boost the development of high-resolution land surface models and Earth system models.}
}
@article{DALIBOR2022111361,
title = {A Cross-Domain Systematic Mapping Study on Software Engineering for Digital Twins},
journal = {Journal of Systems and Software},
volume = {193},
pages = {111361},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111361},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000917},
author = {Manuela Dalibor and Nico Jansen and Bernhard Rumpe and David Schmalzing and Louis Wachtmeister and Manuel Wimmer and Andreas Wortmann},
keywords = {Software Engineering, Digital Twins, Manufacturing, Industry 4.0},
abstract = {Digital Twins are currently investigated as the technological backbone for providing an enhanced understanding and management of existing systems as well as for designing new systems in various domains, e.g., ranging from single manufacturing components such as sensors to large-scale systems such as smart cities. Given the diverse application domains of Digital Twins, it is not surprising that the characterization of the term Digital Twin, as well as the needs for developing and operating Digital Twins are multi-faceted. Providing a better understanding what the commonalities and differences of Digital Twins in different contexts are, may allow to build reusable support for developing, running, and managing Digital Twins by providing dedicated concepts, techniques, and tool support. In this paper, we aim to uncover the nature of Digital Twins based on a systematic mapping study which is not limited to a particular application domain or technological space. We systematically retrieved a set of 1471 unique publications of which 356 were selected for further investigation. In particular, we analyzed the types of research and contributions made for Digital Twins, the expected properties Digital Twins have to fulfill, how Digital Twins are realized and operated, as well as how Digital Twins are finally evaluated. Based on this analysis, we also contribute a novel feature model for Digital Twins from a software engineering perspective as well as several observations to further guide future software engineering research in this area.}
}