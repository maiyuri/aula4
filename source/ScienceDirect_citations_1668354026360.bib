@incollection{WANG2022238,
title = {1.10 - CyberGIS and Geospatial Data Science for Advancing Geomorphology},
editor = {John (Jack) F. Shroder},
booktitle = {Treatise on Geomorphology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {238-259},
year = {2022},
isbn = {978-0-12-818235-2},
doi = {https://doi.org/10.1016/B978-0-12-818234-5.00122-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818234500122X},
author = {Shaowen Wang and Michael P. Bishop and Zhe Zhang and Brennan W. Young and Zewei Xu},
keywords = {Artificial intelligence, CyberGIS, Deep learning, Geomorphology, Geospatial data science, Land cover science, LiDAR, Uncertainty},
abstract = {Theoretical and practical issues in geomorphology have not been adequately addressed due to a lack of formalization and digital representation of spatial and temporal concepts, given the limitations associated with modern-day geographic information systems (GIS). Rapid advancements in geospatial technologies have resulted in new sensors and large volumes of geospatial data that have yet to be fully exploited given a variety of computational issues. Computational limitations involving storage, preprocessing, analysis, and modeling pose significant problems for Earth scientists. Consequently, advanced cyberinfrastructure is required to address geospatial data-science issues involving communication, representation, computation, information production, decision-making, and geovisualization. We identify and discuss important aspects of exploiting advances in cyberinfrastructure that involve computational scalability, artificial intelligence, and uncertainty characterization and analysis for addressing issues in the Earth sciences. Such developments can be termed cyber geographic information science and systems (cyberGIS). We discuss this important topic by addressing the significant overlap of concepts in GIS and geomorphology that can be formalized, digitally represented, implemented, and evaluated with cyberGIS. We then introduce the fundamentals of cyberinfrastructure and cyberGIS, including a discussion of the utilization of artificial intelligence and deep learning. We finally provide one case study demonstrating operational cyberGIS capabilities.}
}
@incollection{ZHANG20221075,
title = {How Digital Twins are Propelling Metals Industry to Next Generation Decision-Making: A Practitioner’s View},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1075-1080},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50179-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596501792},
author = {Yale Zhang and Mitren Sukhram and Ian Cameron},
keywords = {Digital Twin, Analytics, Decision-making, Mining and Metals, Blast Furnace},
abstract = {The digital twin is a technology to digitally transform asset lifecycle in the metals industry, from improving project delivery to empowering operational intelligence toward next-generation decision-making. In this paper, Hatch’s digital twin framework is presented and demonstrated using a real-world blast furnace twin example, followed by development practice and lessons learned from our practice experience.}
}
@incollection{FUENTES2022125,
title = {Chapter 7 - Modern approaches to precision and digital viticulture},
editor = {J. Miguel Costa and Sofia Catarino and José M. Escalona and Piergiorgio Comuzzo},
booktitle = {Improving Sustainable Viticulture and Winemaking Practices},
publisher = {Academic Press},
pages = {125-145},
year = {2022},
isbn = {978-0-323-85150-3},
doi = {https://doi.org/10.1016/B978-0-323-85150-3.00015-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851503000153},
author = {Sigfredo Fuentes and Jorge Gago},
keywords = {Artificial Intelligence, Computer vision, Integrated viticulture, Machine and deep learning, Robotics, Satellite imagery, Unmanned aerial vehicles},
abstract = {New and emerging technologies could play a critical role in the viticulture and winemaking of the future. Climate change has threatened the status quo within the viticultural and wine industry due to increased ambient temperatures, the variability of precipitation, and the increase of climatic risks. These main threats are specifically related to the compression of phenological stages, earlier harvests, many of these within the hottest months producing a dual warming effect. Furthermore, the increase of climatic anomalies, such as floods, frosts, and bushfires, in number, intensity, and window of opportunity within the growing season directly impacts yield and grape and wine quality. The viticulture and winemaking of the future need to have a transformational process to be more predictive rather than only reactive by implementing disruptive technology supported by artificial intelligence.}
}
@article{LIN2022154640,
title = {Does environmental decentralization aggravate pollution emissions? Microscopic evidence from Chinese industrial enterprises},
journal = {Science of The Total Environment},
volume = {829},
pages = {154640},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.154640},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722017338},
author = {Boqiang Lin and Chongchong Xu},
keywords = {Environmental decentralization, Pollution emissions, Micro mechanism, Race to the bottom},
abstract = {Rational division of environmental management power among governments is a necessary institutional support for speeding up the realization of green development goals. Based on the combined microdata of China Industrial Enterprise Database and China Enterprise Pollution Database from 2000 to 2012, the effect of environmental decentralization on enterprise pollution emission is empirically examined in this research. Results show that Chinese-style environmental decentralization, especially environmental supervision decentralization and environmental monitoring decentralization, significantly aggravates the pollution emissions of enterprises. Moreover, the impact of environmental decentralization on enterprise pollution emissions has regional and enterprise ownership heterogeneity. The mechanism test results denote that the production scale effect, energy structure effect and pollution control effect are the micro mechanisms of environmental decentralization aggravating the pollution emission of enterprises. This research confirms the existence of “race to the bottom” among local governments in China and provides evidence support and beneficial enlightenment for the vertical reform of the environmental management system.}
}
@article{SERRANO2022100100,
title = {Verification and Validation for data marketplaces via a blockchain and smart contracts},
journal = {Blockchain: Research and Applications},
pages = {100100},
year = {2022},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2022.100100},
url = {https://www.sciencedirect.com/science/article/pii/S2096720922000410},
author = {Will Serrano},
keywords = {Data marketplace, Project information model, Smart cities, Smart buildings, Real estate, Distributed ledger technology, Blockchain, Smart contracts, Artificial intelligence},
abstract = {Actual challenges with data in physical infrastructure include 1) the adversity of its velocity based on access and retrieval, thus integration; 2) its value as its intrinsic quality; 3) its extensive volume with a limited variety in terms of systems and finally, 4) its veracity, as data can be modified to obtain an economical advantage. Physical infrastructure design based on agile project management and minimum viable products provides benefits against the traditional waterfall method. Agile supports an early return of investment that promotes circular re-investing while making the product more adaptable to variable social-economical environments. However, Agile also presents inherent issues due to its iterative approach. Furthermore, project information requires an efficient record of the aims, requirements, and governance not only for the investors, owners, or users, but also to keep evidence in future health & safety and other statutory compliance. In order to address these issues, this article presents a Validation and Verification (V&V) model for Data Marketplaces with a hierarchical process; each data V&V stage provides a layer of data abstraction, value-added services and authenticity based on Artificial Intelligence (AI). In addition, this proposed solution applies a Distributed Ledger Technology (DTL) for a decentralised approach where each user keeps and maintains the data within a ledger. The presented model is validated in real Data Marketplace applications: 1) live data for Newcastle urban observatory smart city project where data is collected from sensors embedded within the Smart city via APIs. 2) static data for University College London (UCL) – Real Estate – PEARL Project where different project users and stakeholders introduce data into a (Project Information Model) PIM.}
}
@article{XU2022114241,
title = {China Sponge City database development and urban runoff source control facility configuration comparison between China and the US},
journal = {Journal of Environmental Management},
volume = {304},
pages = {114241},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.114241},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721023033},
author = {Changqing Xu and Xinmei Shi and Mingyi Jia and Yu Han and Rongrong Zhang and Shakeel Ahmad and Haifeng Jia},
keywords = {Sponge City, Urban runoff source control facility, Database, Demand analysis, Comparison dimension},
abstract = {Urban runoff source control facilities (URSCFs) are important parts of Sponge City (SC) by controlling urban flooding, restoring eco-balance, and enhancing city resilience. To evaluate the performance of URSCF, one needs to summarize and analyze the past SC construction and operation data. Previous studies however are predominately engineering practice studies. There lacks localized reference datasets to quantitatively evaluate the performance and guide public policy development for SC. Therefore, it is imperative to develop a database, which would summarize data obtained through the already completed pilot sponge cities, and provide a reference for future URSCFs planning and construction. This study makes a zero to one breakthrough by establishing a SC database using New Orleans method. Then statistical results of facility type, size, and costs information for 30 pilot sponge cities have been summarized and analyzed. The URSCFs type distribution statistical results show that bioretention, permeable pavement, detention cell, grassed swale and constructed wetland are the top five most constructed facilities in China. The cost statistical results display that the range of facility cost collected is usually larger than the range given by the reference value, which may attribute to the variation in material cost, labor cost and design parameters in different cities. To check the similarities and differences of URSCFs parameters between China and the US. A configuration parameters comparison of URSCFs has been conducted. Bioretention is taken as an exampl. Comparison results show that factors such as climate type, geographical environment, and socio-economic conditions will affect the configuration parameters of URSCFs. The groundwater depth and designed rainfall intensity are mainly influenced by local climate and geographical conditions. Surface area is influenced by local socio-economic conditions. The thickness of the covering layer and drainage layer are not affected by geographic location. The service area ratio, water storage depth and planting soil layer thickness are significantly different between China and the US.}
}
@article{NEBELUNG2022548,
title = {Towards Real-Time Machining Tool Failure Forecast Approach for Smart Manufacturing Systems},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {548-553},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.251},
url = {https://www.sciencedirect.com/science/article/pii/S240589632200252X},
author = {Nicolas Nebelung and Mario D.S. {de Oliveira Santos} and Sofia T. Helena and Athon F.C.S. {de Moura Leite} and Matheus B. Canciglieri and Anderson L. Szejka},
keywords = {Industry 4.0, Smart System, Artificial Intelligence, Machine Learning, Ontology, Machine Failure Forecast},
abstract = {Industry 4.0 is characterized by a dynamic market that constantly looking for new methods to optimize and integrate manufacturing processes. In this context, Artificial Intelligence has gained prominence in problem-solving, such as failure prediction and decision making, thus improving product quality, and consequently bringing competitiveness to the company. Aiming to contribute to this scenario, this research develops a data treatment system that, from an intelligent tool and an interoperable ontological model, automates the prediction and detection of failures in machining machines lines. The system was developed for the prediction of faults in machining lines includes an artificial intelligence formed from prediction algorithms and inferences, it is possible to guarantee the correct treatment and communication of data at different stages of the process. For the experimental research, was used data collected from a machining line of a public dataset. The information is collected and classified by Artificial Intelligence that supports a decision system. The prediction of tool wear would enable the system to infer the type of problem that is causing this wear, a possible root cause, and the needed maintenance based on the ontological inference tool. By this classification of data, it is possible to achieve, through inferences, a reduction in the decision scope, bringing the possible problems caused by the incoming value. The semantic interoperability ensures correct data exchange and processing, which generates a more assertive view of production failures. The system may help companies to increase their productive process by helping them identify future failures in production if applied in a real scenario.}
}
@article{GE2022109054,
title = {Contrasting trends between peak photosynthesis timing and peak greenness timing across seven typical biomes in Northern Hemisphere mid-latitudes},
journal = {Agricultural and Forest Meteorology},
volume = {323},
pages = {109054},
year = {2022},
issn = {0168-1923},
doi = {https://doi.org/10.1016/j.agrformet.2022.109054},
url = {https://www.sciencedirect.com/science/article/pii/S016819232200243X},
author = {Zhongxi Ge and Jing Huang and Xufeng Wang and Xuguang Tang and Lei Fan and Yinjun Zhao and Mingguo Ma},
keywords = {Phenology, Climate change, FLUXNET2015, Potential PPT, Actual PPT},
abstract = {The peak photosynthesis timing (PPT) is a key factor that affects the seasonality of the terrestrial carbon uptake. Carbon phenology derived from gross primary production (GPP) has been used to validate the peak greenness timing (PGT) from satellite-based vegetation indices (VIs) in phenology research. However, PPT, derived from GPP, has not been comprehensively analyzed, especially taking different GPP estimates, fitting methods, and biomes into account. Moreover, whether or not the PPT trend is consistent with the reported PGT trend still unclear. We explored the above questions at widely used flux sites in Northern Hemisphere mid-latitudes and found that no significant differences in PPT derived from GPP using different carbon flux partitioning methods. Moreover, fitting methods performed well in grassland, cropland, wetland, and wood savannas compared with evergreen needleleaf forest, deciduous broadleaf forest, and mixed forest. Unexpectedly, we did not find an advancing trend in PPT derived from GPP compared with PGT from SPOT-VGT normalized difference vegetation index (NDVI). Our study suggests that the principle of the fitting method and physiological property of the biome should be taken into account when predicting PPT. More importantly, PGT is not a good proxy of the PPT. Therefore, PPT trends based on VIs should be viewed with caution. In general, this study is meaningful for better understanding photosynthesis and carbon cycling in the context of changing climate.}
}
@article{PANOVSKAGRIFFITHS2022126050,
title = {Modelling the impact of reopening schools in the UK in early 2021 in the presence of the alpha variant and with roll-out of vaccination against SARS-CoV-2},
journal = {Journal of Mathematical Analysis and Applications},
volume = {514},
number = {2},
pages = {126050},
year = {2022},
issn = {0022-247X},
doi = {https://doi.org/10.1016/j.jmaa.2022.126050},
url = {https://www.sciencedirect.com/science/article/pii/S0022247X22000646},
author = {J. Panovska-Griffiths and R.M. Stuart and C.C. Kerr and K. Rosenfield and D. Mistry and W. Waites and D.J. Klein and C. Bonell and R.M. Viner},
keywords = {COVID-19, National lockdown, Reopening schools and society, Mathematical modelling},
abstract = {Following the resurgence of the COVID-19 epidemic in the UK in late 2020 and the emergence of the alpha (also known as B117) variant of the SARS-CoV-2 virus, a third national lockdown was imposed from January 4, 2021. Following the decline of COVID-19 cases over the remainder of January 2021, the question of when and how to reopen schools became an increasingly pressing one in early 2021. This study models the impact of a partial national lockdown with social distancing measures enacted in communities and workplaces under different strategies of reopening schools from March 8, 2021 and compares it to the impact of continual full national lockdown remaining until April 19, 2021. We used our previously published agent-based model, Covasim, to model the emergence of the alpha variant over September 1, 2020 to January 31, 2021 in presence of Test, Trace and Isolate (TTI) strategies. We extended the model to incorporate the impacts of the roll-out of a two-dose vaccine against COVID-19, with 200,000 daily vaccine doses prioritised by age starting with people 75 years or older, assuming vaccination offers a 95% reduction in disease acquisition risk and a 30% reduction in transmission risk. We used the model, calibrated until January 25, 2021, to simulate the impact of a full national lockdown (FNL) with schools closed until April 19, 2021 versus four different partial national lockdown (PNL) scenarios with different elements of schooling open: 1) staggered PNL with primary schools and exam-entry years (years 11 and 13) returning on March 8, 2021 and the rest of the schools years on March 15, 2020; 2) full-return PNL with both primary and secondary schools returning on March 8, 2021; 3) primary-only PNL with primary schools and exam critical years (years 11 and 13) going back only on March 8, 2021 with the rest of the secondary schools back on April 19, 2021 and 4) part-rota PNL with both primary and secondary schools returning on March 8, 2021 with primary schools remaining open continuously but secondary schools on a two-weekly rota-system with years alternating between a fortnight of face-to-face and remote learning until April 19, 2021. Across all scenarios, we projected the number of new daily cases, cumulative deaths and effective reproduction number R until April 30, 2021. Our calibration across different scenarios is consistent with alpha variant being around 60% more transmissible than the wild type. We find that strict social distancing measures, i.e. national lockdowns, were essential in containing the spread of the virus and controlling hospitalisations and deaths during January and February 2021. We estimated that a national lockdown over January and February 2021 would reduce the number of cases by early March to levels similar to those seen in October 2020, with R also falling and remaining below 1 over this period. We estimated that infections would start to increase when schools reopened, but found that if other parts of society remain closed, this resurgence would not be sufficient to bring R above 1. Reopening primary schools and exam critical years only or having primary schools open continuously with secondary schools on rotas was estimated to lead to lower increases in cases and R than if all schools opened. Without an increase in vaccination above the levels seen in January and February, we estimate that R could have increased above 1 following the reopening of society, simulated here from April 19, 2021. Our findings suggest that stringent measures were integral in mitigating the increase in cases and bringing R below 1 over January and February 2021. We found that it was plausible that a PNL with schools partially open from March 8, 2021 and the rest of the society remaining closed until April 19, 2021 would keep R below 1, with some increase evident in infections compared to continual FNL until April 19, 2021. Reopening society in mid-April, without an increase in vaccination levels, could push R above 1 and induce a surge in infections, but the effect of vaccination may be able to control this in future depending on the transmission blocking properties of the vaccines.}
}
@article{ZHAO2022101786,
title = {Investment incentives and the relative demand for skilled labor: Evidence from accelerated depreciation policies in China},
journal = {China Economic Review},
volume = {73},
pages = {101786},
year = {2022},
issn = {1043-951X},
doi = {https://doi.org/10.1016/j.chieco.2022.101786},
url = {https://www.sciencedirect.com/science/article/pii/S1043951X2200044X},
author = {Lexin Zhao and Hongsheng Fang},
keywords = {Tax incentives, Accelerated depreciation, Relative demand for skilled labor, Financing constraints, Tax compliance},
abstract = {This study evaluates the effects of China's 2014 and 2015 accelerated depreciation policies on the relative demand of firms for skilled labor. We develop a simple model to explore how the policies affect the relative demand of firms for skilled labor and illustrate the roles of financing constraints and tax compliance in mediating the policy effects. We then employ a firm-level dataset from China's A-share listed companies and use a quasi-experimental design to examine the model predictions. We find that the policies significantly increase the relative demand of firms for skilled labor. The channels underlying the policy effects are that the policies generate additional cash flow for firms, stimulate investment and, thus, raise the demand of firms for skilled labor with the presence of capital–skill complementarity. We also find that the positive effects of the policies on the relative demand for skilled labor are primarily significant for firms with strong financing constraints and high tax compliance. Moreover, we document the positive effects of the policies on R&D investment, firm value added, productivity, workers' benefits, and corporate social responsibility performance, which further corroborate our main results.}
}
@article{HEIDARI2022104089,
title = {Applications of ML/DL in the management of smart cities and societies based on new trends in information technologies: A systematic literature review},
journal = {Sustainable Cities and Society},
volume = {85},
pages = {104089},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104089},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722004061},
author = {Arash Heidari and Nima Jafari Navimipour and Mehmet Unal},
keywords = {Smart cities, Sustainable city, Power management, Machine learning, City management, Deep learning, Review},
abstract = {The goal of managing smart cities and societies is to maximize the efficient use of finite resources while enhancing the quality of life. To establish a sustainable urban existence, smart cities use some new technologies such as the Internet of Things (IoT), Internet of Drones (IoD), and Internet of Vehicles (IoV). The created data by these technologies are submitted to analytics to obtain new information for increasing the smart societies and cities' efficiency and effectiveness. Also, smart traffic management, smart power, and energy management, city surveillance, smart buildings, and patient healthcare monitoring are the most common applications in smart cities. However, the Artificial intelligence (AI), Machine Learning (ML), and Deep Learning (DL) approach all hold a lot of promise for managing automated activities in smart cities. Therefore, we discuss different research issues and possible research paths in which the aforementioned techniques might help materialize the smart city notion. The goal of this research is to offer a better understanding of (1) the fundamentals of smart city and society management, (2) the most recent developments and breakthroughs in this field, (3) the benefits and drawbacks of existing methods, and (4) areas that require further investigation and consideration. IoT, cloud computing, edge computing, fog computing, IoD, IoV, and hybrid models are the seven key emerging developments in information technology that, in this paper, are considered to categorize the state-of-the-art techniques. The results indicate that the Conventional Neural Network (CNN) and Long Short-Term Memory (LSTM) are the most commonly used ML method in the publications. According to research, the majority of papers are about smart cities' power and energy management. Furthermore, most papers have concentrated on improving only one parameter, where the accuracy parameter obtains the most attention. In addition, Python is the most frequently used language, which was used in 69.8% of the papers.}
}
@article{LOUTFI2022473,
title = {A framework for evaluating the business deployability of digital footprint based models for consumer credit},
journal = {Journal of Business Research},
volume = {152},
pages = {473-486},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.07.057},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322006683},
author = {Ahmad Amine Loutfi},
keywords = {FinTech, Digital Finance, Alternative Data, Digital Footprint, Artificial Intelligence},
abstract = {Every time we interact with online digital services, we generate large amounts of data that reveal our shopping habits, social interactions, and much more. We refer to these data collectively as the user-generated digital footprint (UGDF). Today, there is growing interest in using UGDF data as an alternative to conventional financial data in building consumer credit models—UGDF models. Unfortunately, we also observe a hype where the models’ business deployability is reduced to simplistic technical metrics, namely, the model’s prediction accuracy. This study argues that this is a misleading oversimplification of the financial sector’s business realities as it ignores vital dimensions such as the model’s economic viability. Therefore, we develop a framework for evaluating the business deployability of UGDF models for consumer credit using a design science research methodology. The framework is composed of seven criteria: Data accessibility, data coverage, data timeliness, data authenticity, cost of deployment, interpretability, and compliance.}
}
@article{DREWIL2022100546,
title = {Air pollution prediction using LSTM deep learning and metaheuristics algorithms},
journal = {Measurement: Sensors},
volume = {24},
pages = {100546},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100546},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422001805},
author = {Ghufran Isam Drewil and Riyadh Jabbar Al-Bahadili},
keywords = {Deep learning, Long short-term memory (LSTM), Genetic algorithm (GA), Time series data, Air pollution},
abstract = {Air pollution is a leading cause of health concerns and climate change, one of humanity's most dangerous problems. This problem has been exacerbated by an overabundance of automobiles, industrial output pollution, transportation fuel consumption, and energy generation. As a result, air pollution forecasting has become vital. As a result of the large amount and variety of data acquired by air pollution monitoring stations, air pollution forecasting has become a popular topic, particularly when applying deep learning models of long short-term memory (LSTM). The ability of these models to learn long-term dependencies in air pollution data sets them apart. However, LSTM models using many other statistical and machine learning approaches may not offer adequate prediction results due to noisy data and improper hyperparameter settings. As a result, to define the pollution levels for a group of contaminants, an ideal representation of the LSTM is required. To address the problem of identifying the best hyperparameters for the LSTM model, In this paper, we propose a model based on the Genetic Algorithm (GA) algorithm as well as the long short-term memory (LSTM) deep learning algorithm. The model aims to find the best hyperparameters for LSTM and the pollution level for the next day using four types of pollutants PM10, PM2.5, CO, and NOX. The proposed model modified by optimization algorithms shows more accurate results with less experience and more speed than machine learning models and LSTM models.}
}
@article{THONGTHAMMACHART2022105447,
title = {Incorporating Light Gradient Boosting Machine to land use regression model for estimating NO2 and PM2.5 levels in Kansai region, Japan},
journal = {Environmental Modelling & Software},
volume = {155},
pages = {105447},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105447},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001530},
author = {Tin Thongthammachart and Shin Araki and Hikari Shimadera and Tomohito Matsuo and Akira Kondo},
keywords = {Light gradient boosting machine, Extreme gradient boosting, Random forests, Community multiscale air quality model, Land use regression, Air quality forecasting model},
abstract = {This study incorporates Light Gradient Boosting Machine (LightGBM) to a land use regression (LUR) model for estimating NO2 and PM2.5 levels. The predictions were compared with LUR-based machine learnings models of Extreme Gradient Boosting (XGBoost) and Random Forests (RF). Weather Research and Forecasting (WRF) model-simulated meteorological parameters, Community Multiscale Air Quality modeling system (CMAQ)-simulated NO2/PM2.5 concentrations, land use variables, and population data were used as predictor variables. The model performances were evaluated through spatial and temporal cross-validations (CV). The CV results indicated that the LightGBM model was moderately superior in NO2 and PM2.5 predictions compared to the RF and XGBoost models. Moreover, the LightGBM model had high performance in NO2 and PM2.5 predictions at high concentrations, which is essential for risk assessment. Our findings demonstrate that LightGBM can greatly improve the accuracy of NO2 and PM2.5 estimates.}
}
@article{SUN2022525,
title = {Effects of spatial scale of atmospheric reanalysis data on clear-sky surface radiation modeling in tropical climates: A case study for Singapore},
journal = {Solar Energy},
volume = {241},
pages = {525-537},
year = {2022},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X2200411X},
author = {Xixi Sun and Dazhi Yang and Christian A. Gueymard and Jamie M. Bright and Peng Wang},
keywords = {MERRA-2, Aerosol optical depth, Water vapor, Clear-sky radiation model, Inter-comparison, Spatial scale mismatch},
abstract = {Solar resource assessments most generally require atmospheric information, which is customarily acquired from gridded datasets. The spatial scale mismatch problem, i.e., the difference in spatial representativeness of gridded data and in situ measurements, therefore becomes relevant. This study examines how the gridded data used as inputs to clear-sky radiation models can affect their performance at urban scale. The tropical island of Singapore is selected for the case study. Aerosol optical depth at 550 nm (AOD550), Å ngström exponent (AE), and precipitable water (PW) from both the MERRA-2 reanalysis and ground-based stations (AERONET and SuomiNet) are collected between 2013–2020. Firstly, it is found that, relatively to the AERONET ground truth, the bias in MERRA-2’s AOD550 is more prominent than that in AE or PW. Next, the bias propagation from the gridded inputs (AOD550, AE, and PW) to clear-sky radiation predictions is explored using various models. The estimated clear-sky direct normal irradiance (DNIcs) is more sensitive to AOD550 variation than the clear-sky global horizontal irradiance (GHIcs). Six clear-sky radiation models, five of which accept MERRA-2 gridded inputs, are compared with each other, and with the in situ irradiance measurements recorded at 9 sites. The inter-model difference across Singapore is remarkably consistent because the whole island fits inside a single MERRA-2 grid cell. Under high-AOD550 situations, however, the inter-model deviation becomes large for both GHIcs and DNIcs. The conventional model-versus-measurement comparison shows that each model achieves very different site-to-site performance, largely because the spatially-averaged inputs cannot fully represent the micro-climatic variability. Relatively speaking, no clear-sky radiation model significantly outperforms its peers. The simple MAC2 model and the empirical (locally derived) Yang GHIcs-only model are recommended for Singapore.}
}
@article{WELLSANDT2022382,
title = {Hybrid-augmented intelligence in predictive maintenance with digital intelligent assistants},
journal = {Annual Reviews in Control},
volume = {53},
pages = {382-390},
year = {2022},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1367578822000165},
author = {Stefan Wellsandt and Konstantin Klein and Karl Hribernik and Marco Lewandowski and Alexandros Bousdekis and Gregoris Mentzas and Klaus-Dieter Thoben},
keywords = {Engineering applications of artificial intelligence, Predictive maintenance, Human-automation integration, Hybrid intelligence systems},
abstract = {Industrial maintenance strategies increasingly rely on artificial intelligence to predict asset conditions and prescribe maintenance actions. The related maintenance software and human maintenance actors can form a hybrid-augmented intelligence system where each side benefits from and enhances the other side's intelligence. This system requires optimized human-machine interfaces to help users express their knowledge and retrieve information from difficult-to-use software. Therefore, this article proposes a novel approach for maintenance experts and operators to interact with a predictive maintenance system through a digital intelligent assistant. This assistant is artificial intelligence (AI) that could help its users interact with the system via natural language and collect their feedback about the success of maintenance interventions. Implementing hybrid-augmented intelligence in a predictive maintenance system faces several technical, social, economic, organizational, and legal challenges. The benefits, limitations, and risks of hybrid-augmented intelligence must be clear to all employees to advocate its use. AI-focused change management and employee training could be techniques to address these challenges. The success of the proposed approach also relies on the continuous improvement of natural language understanding. Such a process will need conversation-driven development where actual interactions with the assistant provide accurate training data for language and dialog models. Future research has to be interdisciplinary and may cover the integration of explainable AI, suitable AI laws, operationalized trustworthy AI, efficient design for human-computer interaction, and natural language processing adapted to predictive maintenance.}
}
@article{MEYERS202213,
title = {Knowledge Graphs in Digital Twins for Manufacturing - Lessons Learned from an Industrial Case at Atlas Copco Airpower},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {13-18},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.361},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322016263},
author = {Bart Meyers and Johan {Van Noten} and Pieter Lietaert and Bavo Tielemans and Hristo Hristov and Davy Maes and Klaas Gadeyne},
keywords = {Intelligent manufacturing systems, Modeling of manufacturing operations, Quality assurance, maintenance},
abstract = {In this paper we introduce an architecture for a cognitive digital twin that uses an ontology-based knowledge graph to improve the analysis of manufacturing systems. The architecture is evaluated in a use case at Atlas Copco Airpower, revolving around an adaptive measurement strategy for quality control. We report on the requirements, outcomes, and the resulting challenges of using a cognitive digital twin, and provide a roadmap for future research.}
}
@article{CHOWDHURY2022100899,
title = {Unlocking the value of artificial intelligence in human resource management through AI capability framework},
journal = {Human Resource Management Review},
pages = {100899},
year = {2022},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2022.100899},
url = {https://www.sciencedirect.com/science/article/pii/S1053482222000079},
author = {Soumyadeb Chowdhury and Prasanta Dey and Sian Joel-Edgar and Sudeshna Bhattacharya and Oscar Rodriguez-Espindola and Amelie Abadie and Linh Truong},
keywords = {Artificial intelligence, Organisational resources, AI capability, Human resource management, Systematic review, AI-employee collaboration},
abstract = {Artificial Intelligence (AI) is increasingly adopted within Human Resource management (HRM) due to its potential to create value for consumers, employees, and organisations. However, recent studies have found that organisations are yet to experience the anticipated benefits from AI adoption, despite investing time, effort, and resources. The existing studies in HRM have examined the applications of AI, anticipated benefits, and its impact on human workforce and organisations. The aim of this paper is to systematically review the multi-disciplinary literature stemming from International Business, Information Management, Operations Management, General Management and HRM to provide a comprehensive and objective understanding of the organisational resources required to develop AI capability in HRM. Our findings show that organisations need to look beyond technical resources, and put their emphasis on developing non-technical ones such as human skills and competencies, leadership, team co-ordination, organisational culture and innovation mindset, governance strategy, and AI-employee integration strategies, to benefit from AI adoption. Based on these findings, we contribute five research propositions to advance AI scholarship in HRM. Theoretically, we identify the organisational resources necessary to achieve business benefits by proposing the AI capability framework, integrating resource-based view and knowledge-based view theories. From a practitioner’s standpoint, our framework offers a systematic way for the managers to objectively self-assess organisational readiness and develop strategies to adopt and implement AI-enabled practices and processes in HRM.}
}
@article{BJERREGAARD2022,
title = {Framing strategy under high complexity: Processes and practices of ongoing reframing in the becoming of strategy},
journal = {European Management Journal},
year = {2022},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2022.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0263237322000536},
author = {Toke Bjerregaard and Frederik Jeppesen},
keywords = {Framing, Strategy as practice, Complex context, Digital transformation, Ethnography},
abstract = {Framing is a key concept in research on how strategists legitimize and win support for strategic change by establishing a frame of reference for that change. This article advances research on strategy framing by showing how, under conditions of high complexity and uncertainty, strategists continuously reframe strategy in relation to shifting constellations of stakeholders. It presents the findings of an ethnographic study of strategizing in the highly complex context of the digital transformation journey of a global manufacturing firm. It shows how (re)framing practices are combined to iteratively shape strategy formation in ways that sustain strategic influence in the face of constant threats to legitimacy. By accounting for how (re)framing practices reach back and forth in time, the ethnographic findings refine the conventional understanding of how framing resources of past strategizing enter and reworked in present strategy work. Finally, the article contributes empirical insights into how information systems specialists, often marginalized as strategic actors, frame and pitch strategic projects to gain and exert influence in strategy formation processes.}
}
@article{YU2022102707,
title = {A data-driven perspective for sensing urban functional images: Place-based evidence in Hong Kong},
journal = {Habitat International},
volume = {130},
pages = {102707},
year = {2022},
issn = {0197-3975},
doi = {https://doi.org/10.1016/j.habitatint.2022.102707},
url = {https://www.sciencedirect.com/science/article/pii/S0197397522002041},
author = {Zidong Yu and Zhiyang Xiao and Xintao Liu},
keywords = {, , , , },
abstract = {Urban life involves a large variety of urban functions and human activities in a dense context due to the inherent nature of cities. Although technical frameworks have been previously proposed to understand urban functions and activities, there are limited studies that concern the individual places within a city and their detailed characteristics at a local scale. Using points of interest (POIs), we present a data-driven analytical framework to explore urban space containing urban functions and relevant activities by focusing on particular urban places. Urban functions are first extracted and induced by leveraging a latent Dirichlet allocation (LDA) topic modeling technique. We next evaluate the thematic functional differences among the selected places using the location quotient (LQ). Furthermore, tourist functions are assumed to occur in places within a city and carry broadly identifiable information; thus, tourist places are studied by comparing their perceptual experiences using the high-frequency keywords retrieved from tourist reviews on TripAdvisor. By adopting Hong Kong as our case study, the findings reveal considerable diversity of urban functions across different places, while each place displays the distinctive trait of urban functions. Tourist impressions reflected online are primarily consistent with the corresponding functional identities of these places but exhibit additional details associated with emotional and temporal aspects. This study uses a bottom-up assessment of local functions, and discusses their practical implications as related to city branding strategies.}
}
@article{WANG202259,
title = {Explainable AI techniques with application to NBA gameplay prediction},
journal = {Neurocomputing},
volume = {483},
pages = {59-71},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222001333},
author = {Yuanchen Wang and Weibo Liu and Xiaohui Liu},
keywords = {Data science, Explainable artificial intelligence, NBA, Clustering, Regression},
abstract = {In this paper, an explainable artificial intelligence (AI) technique is employed to analyze the match style and gameplay of the national basketball association (NBA). A descriptive analysis on the evolution of the NBA gameplay is conducted by using clustering and principal component analysis. Supervised-learning based AI models (including the random forest and the feed-forward neural network) are applied to produce accurate predictions on NBA outcomes at a season-by-season and a month-by-month basis. To evaluate the interpretability of the established AI models, an explainable AI algorithm is utilized to deduce and assess the precise reasoning behind the model prediction based on the local interpretable model-agnostic explanation method. To illustrate its application potential, the method is applied to the open-source NBA data from 1980 to 2019. Experimental results demonstrate the effectiveness of the introduced explainable AI algorithm on predicting NBA outcomes with interpretation.}
}
@article{ZOLBANIN2022103282,
title = {Data analytics for the sustainable use of resources in hospitals: Predicting the length of stay for patients with chronic diseases},
journal = {Information & Management},
volume = {59},
number = {5},
pages = {103282},
year = {2022},
note = {Big Data Analytics for Sustainability},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103282},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619301594},
author = {Hamed M. Zolbanin and Behrooz Davazdahemami and Dursun Delen and Amir Hassan Zadeh},
keywords = {Data analytics, Length of hospital stay, Deep learning, Temporal evaluation, Sustainability},
abstract = {Various factors are behind the forces that drive hospitals toward more sustainable operations. Hospitals contracting with Medicare, for instance, are reimbursed for the procedures performed, regardless of the number of days that patients stay in the hospital. This reimbursement structure has incentivized hospitals to use their resources (such as their beds) more efficiently to maximize revenues. One way hospitals can improve bed utilization is by predicting patients’ length of stay (LOS) at the time of admission, the benefits of which extend to employees, communities, and the patients themselves. In this paper, we employ a data analytics approach to develop and test a deep learning neural network to predict LOS for patients with chronic obstructive pulmonary disease (COPD) and pneumonia. The theoretical contribution of our effort is that it identifies variables related to patients’ prior admissions as important factors in the prediction of LOS in hospitals, thereby revising the current paradigm in which patients’ medical histories are rarely considered for the prediction of LOS. The methodological contributions of our work include the development of a data engineering methodology to augment the data sets, prediction of LOS as a numerical (rather than a binary) variable, temporal evaluation of the training and validation data sets, and a significant improvement in the accuracy of predicting LOS for COPD and pneumonia inpatients. Our evaluations show that variables related to patients’ previous admissions are the main driver of the deep network’s superior performance in predicting the LOS as a numerical variable. Using the assessment criteria introduced in prior studies (i.e., ±2 days and ±3 days tolerance), our models are able to predict the length of hospital stay with 86 % and 91 % accuracy for the COPD data set, and with 74 % and 85 % accuracy for the pneumonia data set. Hence, our effort could help hospitals serve a larger number of patients with a fixed amount of resources, thereby reducing their environmental footprint while increasing their revenue, as well as their patients’ satisfaction.}
}
@article{ABDELDAYEM2022149834,
title = {Viral outbreaks detection and surveillance using wastewater-based epidemiology, viral air sampling, and machine learning techniques: A comprehensive review and outlook},
journal = {Science of The Total Environment},
volume = {803},
pages = {149834},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.149834},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721049093},
author = {Omar M. Abdeldayem and Areeg M. Dabbish and Mahmoud M. Habashy and Mohamed K. Mostafa and Mohamed Elhefnawy and Lobna Amin and Eslam G. Al-Sakkari and Ahmed Ragab and Eldon R. Rene},
keywords = {SARS-CoV-2, COVID-19, Wastewater based-epidemiology, Viral air surveillance, Artificial intelligence, Artificial neural networks, Machine learning, Deep learning, Reinforcement Learning},
abstract = {A viral outbreak is a global challenge that affects public health and safety. The coronavirus disease 2019 (COVID-19) has been spreading globally, affecting millions of people worldwide, and led to significant loss of lives and deterioration of the global economy. The current adverse effects caused by the COVID-19 pandemic demands finding new detection methods for future viral outbreaks. The environment's transmission pathways include and are not limited to air, surface water, and wastewater environments. The wastewater surveillance, known as wastewater-based epidemiology (WBE), can potentially monitor viral outbreaks and provide a complementary clinical testing method. Another investigated outbreak surveillance technique that has not been yet implemented in a sufficient number of studies is the surveillance of Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2) in the air. Artificial intelligence (AI) and its related machine learning (ML) and deep learning (DL) technologies are currently emerging techniques for detecting viral outbreaks using global data. To date, there are no reports that illustrate the potential of using WBE with AI to detect viral outbreaks. This study investigates the transmission pathways of SARS-CoV-2 in the environment and provides current updates on the surveillance of viral outbreaks using WBE, viral air sampling, and AI. It also proposes a novel framework based on an ensemble of ML and DL algorithms to provide a beneficial supportive tool for decision-makers. The framework exploits available data from reliable sources to discover meaningful insights and knowledge that allows researchers and practitioners to build efficient methods and protocols that accurately monitor and detect viral outbreaks. The proposed framework could provide early detection of viruses, forecast risk maps and vulnerable areas, and estimate the number of infected citizens.}
}
@article{QUAIFE2022100648,
title = {Considering equity in priority setting using transmission models: Recommendations and data needs},
journal = {Epidemics},
volume = {41},
pages = {100648},
year = {2022},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2022.100648},
url = {https://www.sciencedirect.com/science/article/pii/S1755436522000883},
author = {M. Quaife and GF Medley and M. Jit and T. Drake and M. Asaria and P. {van Baal} and R. Baltussen and L. Bollinger and F. Bozzani and O. Brady and H. Broekhuizen and K. Chalkidou and Y.-L. Chi and DW Dowdy and S. Griffin and H. Haghparast-Bidgoli and T. Hallett and K. Hauck and TD Hollingsworth and CF McQuaid and NA Menzies and MW Merritt and A. Mirelman and A. Morton and FJ Ruiz and M. Siapka and J. Skordis and F. Tediosi and P. Walker and RG White and P. Winskill and A. Vassall and GB Gomez},
keywords = {Transmission modelling, Health economics, Equity, Cost-effectiveness analysis, Priority, Setting},
abstract = {Objectives
Disease transmission models are used in impact assessment and economic evaluations of infectious disease prevention and treatment strategies, prominently so in the COVID-19 response. These models rarely consider dimensions of equity relating to the differential health burden between individuals and groups. We describe concepts and approaches which are useful when considering equity in the priority setting process, and outline the technical choices concerning model structure, outputs, and data requirements needed to use transmission models in analyses of health equity.
Methods
We reviewed the literature on equity concepts and approaches to their application in economic evaluation and undertook a technical consultation on how equity can be incorporated in priority setting for infectious disease control. The technical consultation brought together health economists with an interest in equity-informative economic evaluation, ethicists specialising in public health, mathematical modellers from various disease backgrounds, and representatives of global health funding and technical assistance organisations, to formulate key areas of consensus and recommendations.
Results
We provide a series of recommendations for applying the Reference Case for Economic Evaluation in Global Health to infectious disease interventions, comprising guidance on 1) the specification of equity concepts; 2) choice of evaluation framework; 3) model structure; and 4) data needs. We present available conceptual and analytical choices, for example how correlation between different equity- and disease-relevant strata should be considered dependent on available data, and outline how assumptions and data limitations can be reported transparently by noting key factors for consideration.
Conclusions
Current developments in economic evaluations in global health provide a wide range of methodologies to incorporate equity into economic evaluations. Those employing infectious disease models need to use these frameworks more in priority setting to accurately represent health inequities. We provide guidance on the technical approaches to support this goal and ultimately, to achieve more equitable health policies.}
}
@article{CHENG2022960,
title = {Evaluation of opaque deep-learning solar power forecast models towards power-grid applications},
journal = {Renewable Energy},
volume = {198},
pages = {960-972},
year = {2022},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2022.08.054},
url = {https://www.sciencedirect.com/science/article/pii/S0960148122012204},
author = {Lilin Cheng and Haixiang Zang and Zhinong Wei and Fengchun Zhang and Guoqiang Sun},
keywords = {Solar power forecasting, Deep learning, Forecasting interpretability, Model evaluation, Solar photovoltaic, Integrated solar power system},
abstract = {Solar photovoltaic power plays a vital role in global renewable energy power generation, and an accurate solar power forecast can further promote applications in integrated power systems. Due to advanced artificial intelligence technologies, various deep-learning models have been developed with the benefits of improved prediction precision, but these models inevitably sacrifice their interpretability compared to linear methods. Since a 100% accurate forecast is impossible to achieve, an opaque black-box model will always raise doubts for the operators of renewable power-grids, especially when the prediction deviation may produce higher economic costs and even a system turbulence. Motivated by this, the present study summarizes the requirements of deep-learning solar power forecast models from the power-grid application perspective. Post-hoc evaluation and discussion are conducted to analyze the performances of a typical deep-learning benchmark model based on open-access dataset for solar forecasting. Based on the results, the aim of this study is to increase confidence of deep-learning-based intelligent models into the practical engineering utilization of solar power forecasting. The case studies indicate that some simple evaluation procedures can aid a better understanding of the factors that influence the performances of opaque models, and these procedures can help in the design methods for model modifications.}
}
@article{WANG2022244,
title = {Towards intelligent welding systems from a HCPS perspective: A technology framework and implementation roadmap},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {244-259},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001613},
author = {Baicun Wang and Yang Li and Theodor Freiheit},
keywords = {Smart manufacturing, Human-cyber-physical systems (HCPS), Human-centricity, Roadmap},
abstract = {A framework and implementation roadmap for an intelligent welding system (IWS) is proposed from the human-cyber-physical systems (HCPS) perspective of integrating cyber systems with humans and physical systems. Key technologies and system requirements for the framework are comprehensively analyzed. A technology network is proposed to explore the link between complex IWS technologies and application domain requirements. To investigate the practical extent of the state-of-the-art, an example enterprise illustrates the technologies necessary to implement an IWS and the representative technological gaps. Technologies required for IWS were evaluated for their maturity, benefit, and market availability using the Gartner hype-cycle and priority matrix methodology. Finally, an implementation roadmap with research suggestions towards IWS is recommended. This work can support enterprises or factories that want to implement intelligent welding into their operations by clarifying the development state of intelligent technologies and the priority by which further development should be pursued.}
}
@article{PILLERON2022346,
title = {International trends in cancer incidence in middle-aged and older adults in 44 countries},
journal = {Journal of Geriatric Oncology},
volume = {13},
number = {3},
pages = {346-355},
year = {2022},
issn = {1879-4068},
doi = {https://doi.org/10.1016/j.jgo.2021.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1879406821002575},
author = {Sophie Pilleron and Naser Alqurini and Jacques Ferlay and Kristen R. Haase and Michelle Hannan and Maryska Janssen-Heijnen and Kumud Kantilal and Kota Katanoda and Cindy Kenis and Grace Lu-Yao and Tomohiro Matsuda and Erna Navarrete and Nikita Nikita and Martine Puts and Fay J. Strohschein and Eva J.A. Morris},
keywords = {Older adults, Neoplasms, Incidence, Trends, Epidemiology, Population-based cancer registries},
abstract = {Objective
We examine international incidence trends of lung, colorectal, prostate, and breast cancers, as well as all cancers combined excluding non-melanoma skin cancer (NMSC) in adults aged 50 and older, over a fifteen-year period using data from 113 high quality population-based cancer registries included in the Cancer in Five Continents (CI5) series and NORDCAN.
Materials and methods
We calculated annual incidence rates between 1998 and 2012 for ages 50–64, 65–74, and 75+, by sex and both sexes combined. We estimated average annual percentage change (AAPC) in rates using quasi-Poisson regression models.
Results
From 1998 to 2012, incidence trends for all cancers (excluding NMSC) have increased in most countries across all age groups, with the greatest increase observed in adults aged 75+ in Ecuador (AAPC = +3%). Colorectal cancer incidence rates increased in the majority of countries, across all age groups. Lung cancer rates among females have increased but decreased for males. Prostate cancer rates have sharply increased in men aged 50–64 with AAPC between 5% and 15% in 24 countries, while decreasing in the 75+ age group in 21 countries, by up to −7% in Bahrain. Female breast cancer rates have increased across all age groups in most countries, especially in the 65–74 age group and in Asia with AAPC increasing to 7% in the Republic of Korea.
Conclusions
These findings assist with anticipating changing patterns and needs internationally. Due to the specific needs of older patients, it is urgent that cancer systems adapt to address their growing number.}
}
@article{ARIENTI2022209,
title = {The methodology of a “living” COVID-19 registry development in a clinical context},
journal = {Journal of Clinical Epidemiology},
volume = {142},
pages = {209-217},
year = {2022},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2021.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0895435621003747},
author = {Chiara Arienti and Silvia Campagnini and Lorenzo Brambilla and Chiara Fanciullacci and Stefano Giuseppe Lazzarini and Andrea Mannini and Michele Patrini and Maria Chiara Carrozza},
keywords = {COVID-19, Registry, Living systematic review, Real World Data, methodological study, rehabilitation},
abstract = {Objective
The aim of this study was to describe an innovative methodology of a registry development, constantly updated for the scientific assessment and analysis of the health status of the population with COVID-19.
Study Design and Setting
A methodological study design to develop a multi-site, Living COVID-19 Registry of COVID-19 patients admitted in Fondazione Don Gnocchi centres started in March 2020.
Results
The integration of the living systematic reviews and focus group methodologies led to a development of a registry which includes 520 fields filled in for 748 COVID-19 patients recruited from 17 Fondazione Don Gnocchi centres. The result is an evidence and experience-based registry, according to the evolution of a new pathology which was not known before outbreak of March 2020 and with the aim of building knowledge to provide a better quality of care for COVID-19 patients.
Conclusion
A Living COVID-19 Registry is an open, living and up to date access to large-scale patient-level data sets that could help identifying important factors and modulating variable for recognising risk profiles and predicting treatment success in COVID-19 patients hospitalized. This innovative methodology might be used for other registries, to be sure which the data collected is an appropriate means of accomplishing the scientific objectives planned.
Clinical trial registration number
not applicable}
}
@article{WANG2022109206,
title = {Privacy protection federated learning system based on blockchain and edge computing in mobile crowdsourcing},
journal = {Computer Networks},
volume = {215},
pages = {109206},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109206},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622002936},
author = {Weilong Wang and Yingjie Wang and Yan Huang and Chunxiao Mu and Zice Sun and Xiangrong Tong and Zhipeng Cai},
keywords = {Mobile crowdsourcing, Privacy protection, Blockchain, Edge computing, Federated learning, Localized Differential Privacy},
abstract = {With the rapid popularization and development of the Internet of Things (IoT) and 5G networks, mobile crowdsourcing (MCS) has become an indispensable part in today’s society. However, when task participants submit tasks, they are likely to expose their data privacy and location privacy. These privacy will be maliciously attacked and exploited by attackers (external attackers and untrusted third party). With the rapid increase of MCS data throughput, traditional cloud platforms can no longer meet the huge data processing needs. To solve these problems, this paper proposes an MCS federated learning system based on Blockchain and edge computing. This paper uses federated learning as the framework of the MCS system. The system protects data privacy and location privacy by using the Double local disturbance Localized Differential Privacy (DLD-LDP) proposed in this paper. Because the sensed data exists in multiple modalities (text, video, audio, etc.), this paper uses the Multi-modal Transformer (MulT) method to merge the multi-modal data before subsequent operations. To solve the problem that the third party is untrusted, we utilize Blockchain to distribute tasks and collect models in a distributed way. A reputation calculation method (Sig-RCU) is proposed to calculate the real-time reputation of task participants. Through conducting experiments on real data sets, the effectiveness and adaptation of the proposed DLD-LDP algorithm and Sig-RCU algorithm are verified.}
}
@article{JOHNSON2022105743,
title = {Metacognition for artificial intelligence system safety – An approach to safe and desired behavior},
journal = {Safety Science},
volume = {151},
pages = {105743},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105743},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522000832},
author = {Bonnie Johnson},
keywords = {Metacognition, Artificial intelligence systems, Machine learning, System safety, Complexity},
abstract = {Advances in computational thinking and data science have led to a new era of artificial intelligence systems being engineered to adapt to complex situations and develop actionable knowledge. These learning systems are meant to reliably understand the essence of a situation and construct critical decision recommendations to support autonomous and human–machine teaming operations. In parallel, the increasing volume, velocity, variety, veracity, value, and variability of data is confounding the complexity of these new systems – creating challenges in terms of their development and implementation. For artificial systems supporting critical decisions with higher consequences, safety has become an important concern. Methods are needed to avoid failure modes and ensure that only desired behavior is permitted. This paper discusses an approach that promotes self-awareness, or metacognition, within the artificial intelligence systems to understand their external and internal operational environments and use this knowledge to identify potential failures and enable self-healing and self-management for safe and desired behavior.}
}
@article{PUTHAL2022107754,
title = {Decision tree based user-centric security solution for critical IoT infrastructure},
journal = {Computers and Electrical Engineering},
volume = {99},
pages = {107754},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107754},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000623},
author = {Deepak Puthal and Stanly Wilson and Ashish Nanda and Ming Liu and Srinibas Swain and Biswa P.S. Sahoo and Kumar Yelamarthi and Prashant Pillai and Hesham El-Sayed and Mukesh Prasad},
keywords = {Internet of things, Cryptography, Security, Decision tree, Software defined perimeter},
abstract = {Data processing in real-time brings better business modeling and an intuitive plan of action. Internet of things (IoT), being a source of sensitive data collected and communicated through either public or private networks, requires better security from end to end to uphold integrity, quality, and acceptability of data. Designing an adaptive solution plays a vital role where IoT is deployed for the sensing-as-a-services in the critical infrastructure and near real-time decision making by deploying data analysis in the edge datacenters. Again, securing the system with user’s demand and device specifications is a challenging and open research problem. This paper proposed a decision tree based user-centric security approach named DecisionTSec that provides a secure channel for communication in IoT networks, combining edge datacenters in the network edges. Further, the proposed DecisionTSec is validated by experimenting with the real-time testbed for the system performance along with the theoretical security validation.}
}
@incollection{KALPANA2022225,
title = {13 - Data-driven machine learning: A new approach to process and utilize biomedical data},
editor = {Sudipta Roy and Lalit Mohan Goyal and Valentina E. Balas and Basant Agarwal and Mamta Mittal},
booktitle = {Predictive Modeling in Biomedical Data Mining and Analysis},
publisher = {Academic Press},
pages = {225-252},
year = {2022},
series = {Advanced Studies in Complex Systems: Theory and Applications},
isbn = {978-0-323-99864-2},
doi = {https://doi.org/10.1016/B978-0-323-99864-2.00017-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323998642000172},
author = { Kalpana and Aditya Srivastava and Shashank Jha},
keywords = {Data, Disease, Drug, Artificial intelligence, Machine learning, Cognitive computing, Vaccine, Bioinformatics, Artificial neural network},
abstract = {With new diseases like Covid-19 and preexisting challenges like the shortage of skilled personnel, the need for new advancements in healthcare is acutely felt. This also includes the development of precise and accurate diagnostic tools to ease the pressure on the medical personnel, simultaneously enhancing efficiency. Machine Learning (ML) and Artificial Intelligence (AI) have emerged as promising solutions, and are being explored extensively. Their core concept, Artificial Neural Networking (ANN), is a banal yet faithful replica of the natural brain, making complex computing and “learning” possible. Being a gold mine of biomedical data, the healthcare sector serves as an invaluable resource for the development of such tools. However, there are numerous hurdles along the path to the realization of the same. This chapter explores the development of ANN-based diagnostic tools, focusing more on the aforementioned challenges. A brief overview of the current scenarios and future prospects of Machine Learning in Biomedicine has also been discussed.}
}
@article{KOSNIK2022107610,
title = {Advancing exposure data analytics and repositories as part of the European Exposure Science Strategy 2020–2030},
journal = {Environment International},
volume = {170},
pages = {107610},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2022.107610},
url = {https://www.sciencedirect.com/science/article/pii/S0160412022005372},
author = {Marissa B. Kosnik and Stylianos Kephalopoulos and Amalia Muñoz and Nicolò Aurisano and Alberto Cusinato and Sani Dimitroulopoulou and Jaroslav Slobodnik and Jonathas {De Mello} and Maryam {Zare Jeddi} and Claudia Cascio and Andreas Ahrens and Yuri {Bruinen de Bruin} and Lothar Lieck and Peter Fantke},
keywords = {International Society of Exposure Science, Data analysis, Data management, Chemicals, Exposure assessment, Risk assessment},
abstract = {High-quality and comprehensive exposure-related data are critical for different decision contexts, including environmental and human health monitoring, and chemicals risk assessment and management. However, exposure-related data are currently scattered, frequently of unclear quality and structure, not readily accessible, and stored in various—partly overlapping—data repositories, leading to inefficient and ineffective data usage in Europe and globally. We propose strategic guidance for an integrated European exposure data production and management framework for use in science and policy, building on current and future data analysis and digitalization trends. We map the existing exposure data landscape to requirements for data analytics and repositories across European policies and regulations. We further identify needs and ways forward for improving data generation, sharing, and usage, and translate identified needs into an operational action plan for European and global advancement of exposure data for policies and regulations. Identified key areas of action are to develop consistent exposure data standards and terminology for data production and reporting, increase data transparency and availability, enhance data storage and related infrastructure, boost automation in data management, increase data integration, and advance tools for innovative data analysis. Improving and streamlining exposure data generation and uptake into science and policy is crucial for the European Chemicals Strategy for Sustainability and European Digital Strategy, in line with EU Data policies on data management and interoperability.}
}
@article{MERHI2022102545,
title = {An evaluation of the critical success factors impacting artificial intelligence implementation},
journal = {International Journal of Information Management},
pages = {102545},
year = {2022},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2022.102545},
url = {https://www.sciencedirect.com/science/article/pii/S0268401222000792},
author = {Mohammad I. Merhi},
keywords = {Artificial intelligence implementation, Critical factors, Analytics, Ethics, Analytical hierarchy process},
abstract = {AI systems offer organizations great benefits causing decision-makers to invest more in these systems. The advantages of AI cannot be achieved without successful implementation. Thus, it is crucial to recognize the factors impacting the successful implementation of AI. It is also important to assess and rank these factors by their importance to assist decision-makers in implementing these systems and increasing the success rate. Due to its importance, scholars called for studies to expand our knowledge in this critical area. This paper identifies, extracts, and assesses the most critical factors that influence the implementation of AI systems. This study identifies nineteen factors and categorizes them into four categories: organization, technology, process, and environment. The analytical hierarchy process is used to evaluate the factors and the categories. The analysis offers two types of results, at the category level and the level of the factors. The results indicate that technology is the most significant of the four categories. The results also suggest that ethics is the most crucial factor among all nineteen factors. The order of all factors and discussions of the implications of the findings for practice and research are presented in the paper.}
}
@article{FORYS2022435,
title = {Machine learning in house price analysis: regression models versus neural networks},
journal = {Procedia Computer Science},
volume = {207},
pages = {435-445},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.078},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009516},
author = {Iwona Foryś},
keywords = {machine learning, deep learning, neural networks, house value, regression models},
abstract = {The problem addressed in this paper is automatic house price determination using multiple regression models and machine learning. In the practice of real estate appraisal, discussions about automated valuation (AVM) are increasingly common. Resistance to modern machine learning methods stems from a low level of knowledge about these methods and, as a result, an unawareness to what extent and where they can support the classical process of estimating property value. The problem is much broader, because it affects many aspects of the use of machine learning (including neural networks) in the broader real estate market. The process of real estate management at each stage generates huge information resources, which are used at different levels and to different extents by entities operating in the real estate market. One of such professional groups are real estate appraisers, for whom intelligent systems for monitoring the market and providing necessary data are becoming increasingly common and sought after. In this context, a comparative study of two models: multiple regression and neural networks has been carried out. Both methods were used to determine house prices, based on the same set of input data, and in the next step the effects of using these models for an additional group of objects with known characteristics and transaction prices were compared. The multivariate regression model obtained in the study was of medium quality, but sufficient for the purpose of comparative analysis. In the case of neural networks, the highest quality model was not obtained for the study sample, despite normalizing the variables to the required interval (0;1). In both models, the prices for the control sample were overestimated in most cases. However, this does not deny the relevance of further research and attempts to teach neural networks using larger data sets, especially in the case of properties other than typical residential units or land for residential development. Machine learning methods can be extremely useful especially in the processes of general valuation. In these processes, large sets of properties are estimated in a short period of time and with the same methods.}
}
@article{QIAN2022109394,
title = {Research on deterioration evolution trend of primary loop piping in nuclear power plant based on fusion health index},
journal = {Annals of Nuclear Energy},
volume = {179},
pages = {109394},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109394},
url = {https://www.sciencedirect.com/science/article/pii/S030645492200425X},
author = {Hong Qian and Bangzhi Xu and Jun Zhang},
keywords = {Nuclear power plant, Piping deterioration, Health index, Improved Mahalanobis distance, Convolutional neural networks, Long short-term memory neural networks},
abstract = {The perennial operation of nuclear power primary loop piping leads to deterioration of the piping and the potential risk of leakage. At present, there is no reliable technology to detect the leakage directly. This paper proposes a method to obtain the piping deterioration evolution trend based on the analysis of sets of data that can reflect real state of the piping. The specific method is to quantitatively construct a fusion health index model based on the improved Mahalanobis distance, which integrates the analytic hierarchy process and the entropy weight method. A prediction model combining convolutional neural networks and long short-term memory neural networks is established to make the trend analysis and prediction. The experimental results show that the method can better reflect the actual health state of the piping and effectively predict the deterioration evolution trend, which provides a specific reference value for ensuring the safe and stable operation of equipment.}
}
@article{FOUNTAIN2022101645,
title = {The moon, the ghetto and artificial intelligence: Reducing systemic racism in computational algorithms},
journal = {Government Information Quarterly},
volume = {39},
number = {2},
pages = {101645},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101645},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000812},
author = {Jane E. Fountain},
keywords = {Digital government, Public management, Systemic racism, Discrimination, Artificial intelligence, Machine learning, Computational algorithms},
abstract = {Computational algorithms and automated decision making systems that include them offer potential to improve public policy and organizations. But computational algorithms based on biased data encode those biases into algorithms, models and their outputs. Systemic racism is institutionalized bias with respect to race, ethnicity and related attributes. Such bias is located in data that encode the results and outputs of decisions that have been discriminatory, in procedures and processes that may intentionally or unintentionally disadvantage people based on race, and in policies that may discriminate by race. Computational algorithms may exacerbate systemic racism if they are not designed, developed, and used–that is, enacted–with attention to identifying and remedying bias specific to race. Advancing social equity in digital governance requires systematic, ongoing efforts to assure that automated decision making systems, and their enactment in complex public organizational arrangements, are free from bias.}
}
@article{ARNDT2022118168,
title = {Making waves: Time for chemical surface water quality monitoring to catch up with its technical potential},
journal = {Water Research},
volume = {213},
pages = {118168},
year = {2022},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2022.118168},
url = {https://www.sciencedirect.com/science/article/pii/S0043135422001312},
author = {Julia Arndt and Julia S. Kirchner and Kevin S. Jewell and Michael P. Schluesener and Arne Wick and Thomas A. Ternes and Lars Duester},
keywords = {Surface water monitoring, Automated data processing, Online analysis, Real-time, Citizen science},
abstract = {A comprehensive real-time evaluation of the chemical status of surface water bodies is still utopian, but in our opinion, it is time to use the momentum delivered by recent advanced technical, infrastructural, and societal developments to get significantly closer. Procedures like inline and online analysis (in situ or in a bypass) with close to real-time analysis and data provision are already available in several industrial sectors. In contrast, atline and offline analysis involving manual sampling and time-decoupled analysis in the laboratory is still common practice in aqueous environmental monitoring. Automated tools for data analysis, verification, and evaluation are changing significantly, becoming more powerful with increasing degrees of automation and the introduction of self-learning systems. In addition, the amount of available data will most likely in near future be increased by societal awareness for water quality and by citizen science. In this analysis, we highlight the significant potential of surface water monitoring techniques, showcase “lighthouse” projects from different sectors, and pin-point gaps we must overcome to strike a path to the future of chemical monitoring of inland surface waters.}
}
@article{BARATA20223320,
title = {Interoperability standards for circular manufacturing in cyber-physical ecosystems: a survey},
journal = {Procedia Computer Science},
volume = {207},
pages = {3320-3329},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.390},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012807},
author = {João Barata and Alberto Cardoso and Jochen Haenisch and Mona Chaure},
keywords = {Interoperability, cyber-physical ecosystems, standards, KYKLOS 4.0},
abstract = {Cyber-physical ecosystems are among the most promising solutions to adopt circular manufacturing (CM), extending product lifecycles with customer-centric approaches that optimize energy and materials use in interconnected organizations. However, information sharing increases significantly, putting standardization at the top of the industry priorities. This paper reviews interoperability standards for circular manufacturing and devises initial guidelines for its identification and management in cyber-physical ecosystems. The study was conducted within the framework of KYKLOS 4.0, one of the innovative Horizon 2020 projects advancing the field. The results include (1) an overview of interoperability standards in manufacturing and (2) the proposal of a standards list to support circular practices. Our findings are relevant for identifying circularity requirements in cyber-physical ecosystems and defining an interoperability baseline. As more standards appear in the market, continuous interoperability assessment and cyber-physical structures for multi-standard scenarios must enter the manufacturers’ agenda.}
}
@article{WIBOWO2022484,
title = {Problem identification and intervention in the higher education data synchronization system in Indonesia},
journal = {Procedia Computer Science},
volume = {197},
pages = {484-494},
year = {2022},
note = {Sixth Information Systems International Conference (ISICO 2021)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.165},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921023899},
author = {Radityo Prasetianto Wibowo and Ika Nurkasanah and Rully Agus Hendrawan and Umi Laili Yuhana and Arif Wibisono and Nur Aini Lestari and Siti Aminatus Zehroh},
keywords = {PDDikti feeder, AKM completeness, pareto analysis, application of metadata management, data governance},
abstract = {IT Department identified problems in reporting student study activities (AKM) by interviewing stakeholders. Then the priorities of the problems are ineffective business process and technical matters determined by using Pareto Analysis. This study proposed two solutions, application of data governance to achieve an effective business process and application of metadata management to solve the technical matters. Both solutions were socialized, tested and applied among all stakeholders. The result was that the percentage of AKM completeness increased significantly from 84% to 100% and Data Reporting Team now can send data faster and reduce data transmission errors which decrease technical debt problem.}
}
@article{BADICA202242,
title = {Exploring the Usability of Process Mining in Smart City},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {11},
pages = {42-47},
year = {2022},
note = {IFAC Workshop on Control for Smart Cities CSC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.08.046},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322011351},
author = {Amelia Bădică and Costin Bădică and Ion Buligiu and Liviu-Ion Ciora},
keywords = {Optimization, Decision Making in Smart City Control, System Theory in Smart City Control, Cyber-Physical Systems},
abstract = {The aim of this paper is to explore the usability of Process Mining approaches in Smart City applications. Our research was triggered by the following three research questions, concerning the initial investigation of: i) the most researched Smart City problems using the methods of Process Mining; ii) the most utilized Process Mining methods in Smart City applications; iii) the most popular Smart City topics that were not approached yet from the perspective of Process Mining. This analysis will result in a set of challenges and opportunities of utilizing the most innovative Process Mining methods for solving emergent Smart City problems.}
}
@article{FOSHAMMER2022108573,
title = {Identification of aftermarket and legacy parts suitable for additive manufacturing: A knowledge management-based approach},
journal = {International Journal of Production Economics},
volume = {253},
pages = {108573},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2022.108573},
url = {https://www.sciencedirect.com/science/article/pii/S0925527322001657},
author = {Jeppe Foshammer and Peder Veng Søberg and Petri Helo and Iñigo Flores Ituarte},
keywords = {Additive manufacturing, 3D print, Part identification, Aftermarket parts, Legacy parts, Knowledge management},
abstract = {A research stream identifying aftermarket and legacy parts suitable for additive manufacturing (AM) has emerged in recent years. However, existing research reveals no golden standard for identifying suitable part candidates for AM and mainly combines preexisting methods that lack conceptual underpinnings. As a result, the identification approaches are not adjusted to organizations and are not completely operationalizable. Our first contribution is to investigate and map the existing literature from the perspective of knowledge management (KM). The second contribution is to develop and empirically investigate a combined part-identification approach in a defense sector case study. The part identification entailed an analytical hierarchy process (AHP), semi-structured interviews, and workshops. In the first run, we screened 35,000 existing aftermarket and legacy parts. Similar to previous research, the approach was not in sync with the organization. However, in contrast to previous research, we infuse part identification with KM theory by developing and testing a “Phase 0” assessment that ensures an operational fit between the approach and the organization. We tested Phase 0 and the knowledge management-based approach in a second run, which is the main contribution of this study. This paper contributes empirical research that moves beyond previous research by demonstrating how to overcome the present challenges of part identification and outlines how knowledge management-based part identification integrates with current operations and supply chains. The paper suggests avenues for future research related to AM; however, it also concerns Industry 4.0, lean improvement, and beyond, particularly from the perspective of KM.}
}
@article{DIFRANCO2022105076,
title = {Increasing the interoperability of snow/ice hyperspectral observations},
journal = {Computers & Geosciences},
volume = {162},
pages = {105076},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105076},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422000401},
author = {Sabina {Di Franco} and Roberto Salzano and Enrico Boldrini and Rosamaria Salvatori},
keywords = {Snow, Field spectroscopy, Metadata, Reflectance, Data model, Interoperability},
abstract = {This study aims to set up a metadata profile useful for preparing an interoperable dataset containing snow and ice hyperspectral measurements. The proposed Snow and Ice Spectral Library (SISpec) scheme was prepared for sharing a data collection focused on Antarctica, including 70 observations. Following the perspective to grant “open access” to such a dataset, we found a compromise between the ERC (European Research Council) guidelines, the FAIR (Findability, Accessibility, Interoperability, and Reuse) Data principles defined by the RDA (Research Data Alliance), and the GEO (Group on Earth Observation) Data Sharing Principles. The ISO (International Organization for Standardization) standard 19115 was chosen as the standard framework for describing SISpec. When the available metadata scheme was not sufficient or suitable, metadata extensions or new detailed metadata components were created to be compliant with the ISO 19115 standard. We also considered the INSPIRE (Infrastructure for Spatial Information in Europe) requirements and the result is a metadata model that can be useful to share SISpec metadata both in the European and international contexts. Particularly detailed metadata sections and elements were created for describing spectral signatures and microphysical snow parameters.}
}
@article{CARDONE202260,
title = {A fuzzy partition-based method to classify social messages assessing their emotional relevance},
journal = {Information Sciences},
volume = {594},
pages = {60-75},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S002002552200161X},
author = {Barbara Cardone and Ferdinando {Di Martino} and Sabrina Senatore},
keywords = {Fuzzy partition, Emotional categories, Classification, TF-IDF, Fuzzy linguistic labels},
abstract = {With the surge of the large volume of data availability, Machine Learning and mainly Deep Learning techniques are the leading solutions in classification and predictive tasks, targeted at data-efficient learning. These models learn by training on many diversified samples in a process that is computationally expensive or time-consuming. Moreover, in many real-world scenarios, the amount of available data for training is unsuitable, because it is unlabeled or covers only portions of the whole reference domain cases. This paper proposes an alternative approach for document classification that leverages the distribution of the data projected in the multi-dimensional feature space to assess the weight of features in the final classification. The approach does not rely on traditional iterative methods for classification but builds a relevance measure to assess the relevance/importance of the features describing the domain of interest. The idea is to harness this metric to select relevant features and then express the values calculated by these metrics in natural language by exploiting fuzzy variables and linguistic labels to make human comprehension more immediate. The approach has been employed for emotion extraction from social media messages. The novelty of this approach is twofold: first, the well-known TF-IDF measure was reinterpreted as a relevance measure of emotions discovered in text content. Then, the discovered emotion relevance was described by fuzzy linguistic labels, defined on an ad-hoc-designed fuzzy partition, to express the data classification in natural language, more suitable to human understanding.}
}
@article{CZVETKO2022117,
title = {Data-driven business process management-based development of Industry 4.0 solutions},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {36},
pages = {117-132},
year = {2022},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2021.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1755581721001929},
author = {Tímea Czvetkó and Alex Kummer and Tamás Ruppert and János Abonyi},
keywords = {Business process management (BPM), Business process redesign (BPR), Industry 4.0 (I4.0), Digital technology, Discrete event simulation},
abstract = {Business process management (BPM) supports the management and transformation of organizational operations. This paper provides a structured guideline for improving data-based process development within the BPM life cycle. We show how Industry 4.0-induced tools and models can be integrated within the BPM life cycle to achieve more efficient process excellence and evidence-based decision-making. The paper demonstrates how standards of machine learning (CRISP-ML(Q)), BPM, and tools of design science research can support the redesign phases of Industry 4.0 development. The proposed methodology is carried out on an assembly company, where the proposed improvement steps are investigated by simulation and evaluated by relevant key performance indicators.}
}
@article{AKTER202285,
title = {The future of marketing analytics in the sharing economy},
journal = {Industrial Marketing Management},
volume = {104},
pages = {85-100},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2022.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0019850122000815},
author = {Shahriar Akter and Umme Hani and Yogesh K. Dwivedi and Anuj Sharma},
keywords = {Marketing analytics capability, Sharing economy, Marketing agility, Marketing effectiveness, Market turbulence},
abstract = {The rise of sharing economy has accelerated the growth of marketing analytics to match demand and supply in industrial markets. However, the conceptualization of marketing analytics remains unclear in the sharing economy. Theorizing market turbulence as the dark side of the sharing economy, this study presents a marketing analytics capability model using dynamic capabilities and contingency theories to advance thought and practice in industrial marketing research. Using a thematic analysis and a survey-based empirical study on B2B cloud sharing platforms (n = 252), the findings present pattern identification, real-time solutions and data governance as the antecedents of marketing analytics capability with its holistic effects on marketing agility and marketing effectiveness. The empirical findings further support the mediating role of marketing agility and the moderating impact of market turbulence on marketing analytics-effectiveness and marketing agility-effectiveness chain. Overall, our results contribute toward a more nuanced understanding of the dark side of market turbulence on marketing analytics capability dynamics in the sharing economy.}
}
@article{LEE2022106595,
title = {SEMRES - A Triple Security Protected Blockchain Based Medical Record Exchange Structure},
journal = {Computer Methods and Programs in Biomedicine},
volume = {215},
pages = {106595},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106595},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721006696},
author = {Yen-Liang Lee and Hsiu-An Lee and Chien-Yeh Hsu and Hsin-Hua Kung and Hung-Wen Chiu},
keywords = {Blockchain, Security, EMR Protection, Encryption, Data exchange},
abstract = {Background and Objective
COVID-19, a serious infectious disease outbreak started in the end of 2019, has caused a strong impact on the overall medical system, which reflects the gap in the volume and capacity of medical services and highlights the importance of clinical data ex-change and application. The most important concerns of medical records in the medical field include data privacy, data correctness, and data security. By realizing these three goals, medical records can be made available to different hospital information systems to achieve the most complete medical care services. The privacy and protection of health data require detailed specification and usage requirements, which is particularly important for cross-agency data exchange.
Methods
This research is composed of three main modules. "Combined Encryption and Decryption Architecture", which includes the hybrid double encryption mechanism of AES and RSA, and encrypts medical records to produce "Secured Encrypted Medical Record". "Decentralize EMR Repository", which includes data decryption and an exchange mechanism. After a data transmission is completed, the content verification and data decryption process will be launched to confirm the correctness of the data and obtain the data. A blockchain architecture is used to store the hash value of the encrypted EMR, and completes the correctness verification of the EMR after transmission through the hash value.
Results
The results of this study provide an efficient triple encryption mechanism for electronic medical records. SEMRES ensures the correctness of data through the non-repudiation feature of a blockchain open ledger, and complete integrated information security protection and data verification architecture, in order that medical data can be exchanged, verified, and applied in different locations. After the patient receives medical services, the medical record is re-encrypted and verified and stored in the patient's medical record. The blockchain architecture is used to ensure the verification of non-repudiation of medical service, and finally to complete the payment for medical services.
Conclusions
The main aim of this study was to complete a security architecture for medical data, and develop a triple encryption authentication architecture to help data owners easily and securely share personal medical records with medical service personnel.}
}
@article{PENG2022101488,
title = {A collaborative design platform for new alloy material development},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101488},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101488},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002378},
author = {Gongzhuang Peng and Youzhao Sun and Qian Zhang and Quan Yang and Weiming Shen},
keywords = {New material development, Collaborative design platform, Mechanical performance prediction, Mamdani-type fuzzy modeling, Industrial internet of things (IIoT)},
abstract = {To overcome the shortcomings of the conventional trial and error mode for new material development, a full-process collaborative design platform for steel rolling is developed based on an industrial internet of things (IIoT) system in this study. Equipment, process and product entities are modeled in both the physical domain and the cyber domain. A systematic data-driven Mamdani-type fuzzy modeling methodology is proposed to map the relationship between material chemical compositions, organizational structures, process parameters and mechanical performances. The proposed methodology employs a random forest (RF) algorithm to select important parameters from mechanism models, simulation models and production process variables, utilizes a K-means algorithm to merge diverse steel grades into sub-clusters, and implements a multi-objective particle swarm optimization (MOPSO) algorithm to further improve the fuzzy model in terms of both the structure and the membership function parameters. A dataset of 3500 steel coils collected by the prototype platform built in a large hot rolling mill is used to evaluate the performance of the proposed approach. Experiment results show that the proposed methodology performs well in predicting the yield strength, tensile strength and elongation, with the coverage probability over 90% under 10% deviation and about 70% under 5% deviation on average.}
}
@article{SU2022730,
title = {Understanding the relationships between the development of the construction sector, carbon emissions, and economic growth in China: Supply-chain level analysis based on the structural production layer difference approach},
journal = {Sustainable Production and Consumption},
volume = {29},
pages = {730-743},
year = {2022},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2021.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S2352550921003316},
author = {Yuqi Su and Zijian Zou and Xiaoming Ma and Junping Ji},
keywords = {Construction sector, Input-output model, SDA, SPLD, Supply chain, CO emissions, Economic growth},
abstract = {Identifying the inter-sectoral relationships between the construction sector and other related sectors is crucial to reducing carbon emissions and promoting economic development. Non-competitive input–output models at comparable prices in 1992, 1997, 2002, 2007, 2012, and 2017 were used to conduct this study. The structural production layer difference (SPLD) method was employed in this study and complemented with the results of structural decomposition analysis to identify significant sectors affecting economic growth and carbon emissions in the construction sector. Please check editor name in valid PIT's and styled as title-footnote or misc-text according to JSS Overall, the carbon emissions and economic growth of China's construction sector in 2017 increased by 2.4 billion tonnes and 10,512.4 billion yuan, respectively, compared to 1992 at the 2000 price level. Important sectors influencing economic growth and carbon emissions in the construction sector are identified. Wholesale and retail trade sector (s31) and finance sector (s33) increased the economic growth of the construction sector without the additional emission of CO2, which is economically and environmentally sustainable. Smelting and processing of the metals (s15) and transport and storage (s29) sectors simultaneously restrained economic growth and inhibited carbon emissions in the construction sector. The production and distribution of the electric and heat power sector (s25) stimulates the growth of carbon emissions in the sector of smelting and processing of the metals (s15), reducing the carbon reduction effect of smelting and processing of the metals (s15) on the construction sector. The SPLD results revealed that the direct impact of other services sector (s35) and transport and storage (s29) on carbon emissions in the construction sector has not been highlighted by the structural decomposition analysis (SDA). In terms of economic added value, processing of petroleum coking and the nuclear fuel sector (s12) is a pivotal sector in the supply chain influencing the added value from the extraction of petroleum and natural gas sector (s3) to the construction sector according to the results of SPLD.}
}
@article{WU2022181,
title = {An ensemble of random decision trees with local differential privacy in edge computing},
journal = {Neurocomputing},
volume = {485},
pages = {181-195},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.01.145},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016313},
author = {Xiaotong Wu and Lianyong Qi and Jiaquan Gao and Genlin Ji and Xiaolong Xu},
keywords = {random decision tree, privacy preservation, data mining, local differential privacy, edge computing},
abstract = {Edge computing is an emerging computing paradigm, which offers a great opportunity to implement data mining-based services and applications for a large number of devices and sensors in Internet of Things. However, the new paradigm is faced with security and privacy challenges due to the diversity and the limited capability of edge components. In particular, data privacy is one of the most concerned problems for all the participants. In this paper, we propose a framework of privacy-preserving data mining based on private random decision trees in edge computing, which not only gives the strong privacy guarantee, but also provides a certain amount of data utility. Firstly, we design a preservation framework to implement private random decision trees satisfying local differential privacy. Secondly, we present the concrete implementations of algorithms and the corresponding task that each participant needs to undertake. Thirdly, we analyze the key factors to influence privacy and utility, including the allocation of data and privacy budget. Fourthly, we give the improved algorithms to further increase the utility with strong privacy preservation. Finally, extensive experiments demonstrate the good performance of our designed framework.}
}
@article{ABUMADI20222551,
title = {Key Research Challenges in Digital Twin Applications for Demanufacturing},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2551-2556},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.093},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021024},
author = {Farah A. Abumadi and Concetta Semeraro and Abdul Ghani Olabi and Michele Dassisti},
keywords = {Digital twin, Cyber-physical production system, Demanufacturing, Industry 4.0},
abstract = {Based on relevant studies, demanufacturing processes on end-of-life products may achieve around 70% material-saving, 60% energy-saving, and 50% cost-saving of the overall life-cycle cost. Despite these great benefits, many issues are still open in real-life applications, such as the unavailability of smart technology implementations that make more effective and efficient demanufacturing processes. The Digital twin application seems to be a promising tool to facilitate recycling, tracking, and managing such processes. This paper reviews the state-of-the-art research on digital twins for manufacturing and demanufacturing. A comprehensive analysis of the key challenges in applying digital twins for the demanufacturing process and potential solutions for the current challenges are provided.}
}
@article{LI2022103456,
title = {Multi-level video quality services and security guarantees based on compressive sensing in sensor-cloud system},
journal = {Journal of Network and Computer Applications},
volume = {205},
pages = {103456},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103456},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001072},
author = {Min Li and Di Xiao and Hui Huang and Bo Zhang},
keywords = {Compressive video sensing, Multi-level quality services, Multi-level security guarantees, Effective space utilization, The sensor-cloud system},
abstract = {The booming development of the Internet of Things has led to the emergence of novel application systems such as the sensor-cloud system. Numerous data (especially images and videos) are collected, processed, transmitted and stored via the cooperation between the sensor networks and the cloud computing every day, so there exist three main issues to be solved imminently in the sensor-cloud system. (1) Data security: how to ensure the security of required data from the perspective of cloud service users. (2) Effective space utilization: how to economize the cloud storage space as much as possible from the view of cloud service providers. (3) Multi-level quality services and security guarantees: different quality services and security guarantees should be considered for different levels of users, while maximizing financial benefits of sensor network providers and cloud service providers. In this paper, a novel scheme based on compressive sensing with the usage of the private cloud is proposed for three diverse levels of cloud service users to enjoy completely diverse video quality services and security guarantees in the sensor-cloud system. Theoretical analyses and experimental simulations show that the proposed scheme can balance the relationship among sensor network providers, cloud service providers and cloud service users well.}
}
@article{ZHAO2022255,
title = {Perspectives on nonstationary process monitoring in the era of industrial artificial intelligence},
journal = {Journal of Process Control},
volume = {116},
pages = {255-272},
year = {2022},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959152422001184},
author = {Chunhui Zhao},
keywords = {Industrial artificial intelligence, Nonstationary process monitoring, Machine learning, Cointegration analysis, Condition-driven, Semantic knowledge, Cloud–edge collaboration},
abstract = {The development of the Internet of Things, cloud computing, and artificial intelligence has given birth to industrial artificial intelligence (IAI) technology, which enables us to obtain fine perception and in-depth understanding capabilities for the operating conditions of industrial processes, and promotes the intelligent transformation of modern industrial production processes. At the same time, modern industry is facing diversified market demand instead of ultra-large-scale demand, resulting in typical variable conditions, which enhances the nonstationary characteristics of modern industry, and brings great challenges to the monitoring of industrial processes. In this regard, this paper analyzes the complex characteristics of nonstationary industrial operation, reveals the effects on operating condition monitoring, and summarizes the difficulties faced by varying condition monitoring. Furthermore, by reviewing the recent 30 years of development of data-driven methods for industrial process monitoring, we sorted out the evolution of nonstationary monitoring methods, and analyzed the features, advantages and disadvantages of the methods at different stages. In addition, by summarizing the existing related research methods by category, we hope to provide reference for monitoring methods of nonstationary process. Finally, combined with the development trend of industrial artificial intelligence technologies, some promising research directions are given in the field of nonstationary process monitoring.}
}
@article{HUANG2022111310,
title = {Image feature selection based on orthogonal ℓ2,0 norms},
journal = {Measurement},
volume = {199},
pages = {111310},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111310},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122005504},
author = {Guan-Yu Huang and Chiao-Yun Hung and Bo-Wei Chen},
keywords = {Feature selection, Image classification,  norms, Orthogonal constraints},
abstract = {This study presents a feature selection method based on orthogonal ℓ2,0-norms to reduce dimensions, especially for images, where correlated and redundant information is frequently present by nature. Recent ℓ2,0-norm methods have shown a way of discovering sparsity, but redundant features could still be selected in the process. In light of such, this study considers imposing an orthogonal constraint on sparsity, further limiting ℓ2,0 norms. To such an end, projection onto Stiefel manifolds is computed to satisfy the orthogonal constraint while ℓ2,0-norm regularization is computed via ℓp-box zero-one programming. Experiments on open datasets were carried out to evaluate the proposed method and different models, including ℓ1-, ℓ2,1-, and ℓ2,0-norm approaches. The experimental results showed that the mean accuracy and F1 scores of the proposed method were higher than those of the ℓ2,0-norm method without orthogonal constraints and those of the other baselines, subsequently proving the effectiveness of the proposed idea.}
}
@article{DAI2022354,
title = {Online quality inspection of resistance spot welding for automotive production lines},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {354-369},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000589},
author = {Wei Dai and Dayong Li and Yongjia Zheng and Dong Wang and Ding Tang and Huamiao Wang and Yinghong Peng},
keywords = {Quality inspection, Process stability, Deep learning, Low-rank and sparse decomposition, Channel attention mechanism},
abstract = {Reliable quality control of resistance spot welding (RSW) is a long-standing challenge, due to random disturbance on automotive production lines. In this paper, a quality evaluation framework is proposed based on dynamic resistance (DR) signals, aiming to accurately predict welding quality. The proposed framework integrates welding process stability with deep learning models. Given the uniform variation pattern of each weld with the same schedule, process stability can be determined based on the reference curve constructed by the low-rank and sparse decomposition method. Subsequently, a one-dimensional convolutional neural network (1DCNN) with channel attention mechanism is developed to further predict welding quality, which can perform channel-wise feature recalibration to enhance the classification performance. Extensive experiments substantiate that the proposed network yields a remarkable classification performance compared with typical algorithms on several RSW datasets collected on an actual production line. This study provides a valuable reference to achieve an intelligent online quality inspection system in the automotive manufacturing industry.}
}
@article{ALEO2022101846,
title = {SNAD transient miner: Finding missed transient events in ZTF DR4 using k-D trees},
journal = {New Astronomy},
volume = {96},
pages = {101846},
year = {2022},
issn = {1384-1076},
doi = {https://doi.org/10.1016/j.newast.2022.101846},
url = {https://www.sciencedirect.com/science/article/pii/S1384107622000574},
author = {P.D. Aleo and K.L. Malanchev and M.V. Pruzhinskaya and E.E.O. Ishida and E. Russeil and M.V. Kornilov and V.S. Korolev and S. Sreejith and A.A. Volnova and G.S. Narayan},
keywords = {Transient sources, Time domain astronomy, Supernovae, Active galactic nuclei},
abstract = {We report the automatic detection of 11 transients (7 possible supernovae and 4 active galactic nuclei candidates) within the Zwicky Transient Facility fourth data release (ZTF DR4), all of them observed in 2018 and absent from public catalogs. Among these, three were not part of the ZTF alert stream. Our transient mining strategy employs 41 physically motivated features extracted from both real light curves and four simulated light curve models (SN Ia, SN II, TDE, SLSN-I). These features are input to a k-D tree algorithm, from which we calculate the 15 nearest neighbors. After pre-processing and selection cuts, our dataset contained approximately a million objects among which we visually inspected the 105 closest neighbors from seven of our brightest, most well-sampled simulations, comprising 89 unique ZTF DR4 sources. Our result illustrates the potential of coherently incorporating domain knowledge and automatic learning algorithms, which is one of the guiding principles directing the SNAD team. It also demonstrates that the ZTF DR is a suitable testing ground for data mining algorithms aiming to prepare for the next generation of astronomical data.}
}
@article{WANG2022102572,
title = {Urban neighborhood socioeconomic status (SES) inference: A machine learning approach based on semantic and sentimental analysis of online housing advertisements},
journal = {Habitat International},
volume = {124},
pages = {102572},
year = {2022},
issn = {0197-3975},
doi = {https://doi.org/10.1016/j.habitatint.2022.102572},
url = {https://www.sciencedirect.com/science/article/pii/S0197397522000698},
author = {Lingqi Wang and Shenjing He and Shiliang Su and Yu Li and Lirong Hu and Guie Li},
keywords = {Neighborhood socioeconomic status, Area deprivation, Machine learning, Open data, Social inequalities, Online housing listings},
abstract = {Understanding the dynamic distribution of residents' socioeconomic status (SES) across neighborhoods within cities is essential for urban planning and policy-making aligning to the Sustainable Development Goals 2030. Whereas the promise in explicitly linking geographical features to SES has been highlighted fairly clear in previous works, scholars hold an eclectic attitude in their outlook, given the absence of theoretical ground, the heavy reliance on nontransparent proprietary data sources and the relatively coarse resolution predictions. Drawing on a case study of Hangzhou metropolitan in China, this paper aims to address these problems by demonstrating a novel approach to neighborhood SES inference based on online housing advertisements. We first revisit the theoretical debates on the linkage between neighborhood SES and online housing advertisements. Then, the Naïve Bayes classifier is employed to semantically identify the topics from online housing advertisements and the associated sentiments are quantified using the lexicon-based approach. Following that, seven commonly used machine learning algorithms are compared and utilized to infer the fine-grained neighborhood SES at residential quarters scale based on the housing attributes and extracted topics from online housing advertisements. Results show that machine learning algorithms vary with predictive ability and the tree-based algorithms are much more powerful in inferring neighborhood SES. More specifically, we distinguish 8 reliable features which not only present relative high importance estimated by all the machine learning algorithms but also exhibit great robustness in inferring neighborhood SES and show promising potential to being applied for unraveling social inequalities. We also observe noteworthy spatial heterogeneity in neighborhood SES across the research site. The demonstrated approach not only enables the policymakers to take stock of deprived neighborhoods in a timely manner, but also lays firm ground for framing contextualized strategies of urban governance. This study is among the first attempts to bridge the theoretical interpretation of housing attributes with the proxy indicator -based approach for fine-grained neighborhood SES measurement.}
}
@article{YANG2022102751,
title = {Social media data analytics for business decision making system to competitive analysis},
journal = {Information Processing & Management},
volume = {59},
number = {1},
pages = {102751},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102751},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002326},
author = {Jie Yang and Pishi Xiu and Lipeng Sun and Limeng Ying and Blaanand Muthu},
keywords = {Social media audit, Business intelligence, Business decision making, Competitive analysis, Data analytics},
abstract = {For the past few years, business intelligence has been a major field that uses data analysis to produce key information as part of business decision-making. Data collected from social media sites and blogs are analyzed to make business decisions, a process called social media analytics (SMA). This method, which goes beyond ordinary monitoring or a basic analysis of retweets, develops an in-depth insight into the social consumer. After reading the whole report, add the pertinent figures to the table. Add pertinent data from the Brand24 report to the table. During a social media audit, any followers, impressions, engagement, copy/traffic, and brand mentions are key parameters to analyze. For companies and research institutions, the great interest is to analyse and gain knowledge from user-produced data. These data contain useful knowledge, including customer perceptions feedback and product/service suggestions. Due to content saturation, social media's true meaning regarding business data is hardly ever found. Therefore, in this paper, the business decision making system (BDMS) has been proposed to develop business using social media data analytics. BDMS provides a clear understanding of the key principles, issues and functionality, and big social data developments. Besides, BDMS concentrates on marketing and describes an operational approach for obtaining valuable information from social data. BDMS performs a short and precise description of current use scenarios from the evidence, as per the help of decisions and investment opportunities companies get when using social data analytics. The experimental result shows that BDMS achieves the highest competitive results. With greater accuracy, system dependability, F-1 measurement, and deviation rate of 85.5%, the BDMS system guarantees 93.7%, 86.8%, and 7.0%.}
}
@article{PAN2022100924,
title = {An interdisciplinary review of AI and HRM: Challenges and future directions},
journal = {Human Resource Management Review},
pages = {100924},
year = {2022},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2022.100924},
url = {https://www.sciencedirect.com/science/article/pii/S1053482222000420},
author = {Yuan Pan and Fabian J. Froese},
keywords = {Artificial intelligence (AI), Systematic review, Theory, Method, Human resource management (HRM)},
abstract = {Artificial intelligence (AI) has the potential to change the future of human resource management (HRM). Scholars from different disciplines have contributed to the field of AI in HRM but with rather insufficient cross-fertilization, thus leading to a fragmented body of knowledge. In response, we conducted a systematic, interdisciplinary review of 184 articles to provide a comprehensive overview. We grouped prior research into four categories based on discipline: management and economics, computer science, engineering and operations, and others. The findings reveal that studies in different disciplines had different research foci and utilized different methods. While studies in the technical disciplines tended to focus on the development of AI for specific HRM functions, studies from the other disciplines tended to focus on the consequences of AI on HRM, jobs, and labor markets. Most studies in all categories were relatively weak in theoretical development. We therefore offer recommendations for interdisciplinary collaborations, propose a unified definition of AI, and provide implications for research and practice.}
}
@article{UYSAL2022100382,
title = {Machine learning-enabled healthcare information systems in view of Industrial Information Integration Engineering},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100382},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100382},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000504},
author = {Murat Pasa Uysal},
keywords = {Machine learning, Industrial Information Integration Engineering, Enterprise architecture, System architecture, Healthcare information system, Hospital Information System},
abstract = {Recent studies on Machine learning (ML) and its industrial applications report that ML-enabled systems may be at a high risk of failure or they can easily fall short of business objectives. Cutting-edge developments in this field have increased complexity and also brought new challenges for enterprise information integration. This situation can even get worse when considering the vital importance of ML-enabled healthcare information systems (HEIS). Therefore, the main argument of this paper is that we need to adopt the principles of Industrial Information Integration Engineering (IIIE) for the design, development, and deployment processes of ML-enabled systems. A mixed research paradigm is adopted, and therefore, this study is conducted by following the guidelines and principles of Action Research, Design Science Research, and IIIE. The contributions of this study are two-fold: (a) to draw researchers’ and practitioners’ attention to the integration problems of ML-enabled systems and discuss them in view of IIIE, and (b) to propose an enterprise integration architecture for ML-enabled HEIS of a university hospital, which is designed and developed by following the guidelines and principles of IIIE.}
}
@article{SAHA2022121768,
title = {The interplay of emerging technologies in pharmaceutical supply chain performance: An empirical investigation for the rise of Pharma 4.0},
journal = {Technological Forecasting and Social Change},
volume = {181},
pages = {121768},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.121768},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522002931},
author = {Esha Saha and Pradeep Rathore and Ratri Parida and Nripendra P. Rana},
keywords = {Pharma 4.0, Pharmaceutical supply chain, Emerging technologies, Empirical research},
abstract = {The impact and the relevance of the emerging technologies on the supply chains have attracted researchers and practitioners alike worldwide. Based on the resource-based view and organizational information processing theory, this study attempts to investigate how emerging technologies influence supply chain performance, particularly pharmaceutical supply chain in regards to the rise of Pharma 4.0. The pharmaceutical supply chain processes are considered as the mediators and the emerging technology adoption barriers are proposed as the moderators. The study is evaluated using a survey of pharmaceutical companies in India. The findings indicate that manufacturing, distribution and consumption processes in the supply chain mediate the effects of emerging technologies on supply chain performance; however, mediating effects are weakened due to the presence of intricate barriers. This study thereby empirically investigates the interplay between emerging technologies in pharmaceutical supply chain performance and provides managerial insights with a proposed research framework on how to incorporate and encourage Pharma 4.0 for achieving sustainability in the supply chains.}
}
@incollection{PROVA2022111,
title = {Chapter 10 - Big medical data analytics for diagnosis},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {111-124},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00013-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000133},
author = {Omanin Siddiqua Prova and Faiza Ahmed and Jafrin Sultana and Md. Ashrafuzzaman},
keywords = {Algorithms, Analytics, Big medical data, Diagnosis},
abstract = {Big Medical Data Analytics is an intricate process of extracting information of medical interest from a massive amount of data. Unstructured or structured heterogeneous datasets gleaned from various sources are analyzed with data analytics approaches (e.g., artificial intelligence, machine learning, data mining, etc.) for excavating useful diagnostic information to predict diseases and suitable treatment models. Intensive research is needed to scrutinize the impact of big medical data analytics to diagnose cardiovascular diseases, neurological disorders, and early diagnosis of chronic and genetic diseases. Utilizing this analytics process not only shortens the decision-making time for the caregivers but initiates the development of cost-effective treatment modules, algorithms, or software-based devices. In this chapter, how big medical data analytics can be incorporated with already facilitating diagnosing diseases is discussed, as well as some analytical tools are highlighted that have higher accuracy rates compared to conventional procedures in diagnosing diseases at an early stage.}
}
@article{BAS2022,
title = {An interpretable machine learning approach to understanding the impacts of attitudinal and ridesourcing factors on electric vehicle adoption},
journal = {Transportation Letters},
year = {2022},
issn = {1942-7867},
doi = {https://doi.org/10.1080/19427867.2021.2009098},
url = {https://www.sciencedirect.com/science/article/pii/S1942786722004623},
author = {Javier Bas and Zhenpeng Zou and Cinzia Cirillo},
keywords = {Electric vehicles, attitudes, ridesourcing, machine learning, local interpretable model-agnostic explanations (lime)},
abstract = {ABSTRACT
The global electric vehicle (EV) market has been experiencing an impressive growth in recent times. Understanding consumer preferences on this cleaner, more eco-friendly mobility option could help guide public policy toward accelerating EV adoption and sustainable transportation systems. Previous studies suggest the strong influence of individual and external factors on EV adoption decisions. In this study, we apply machine learning techniques on EV stated preference survey data to predict EV adoption using attitudinal factors, ridesourcing factors (e.g., frequency of Uber/Lyft rides), as well as underlying sociodemographic and vehicle factors. To overcome machine learning models’ low interpretability, we adopt the innovative Local Interpretable Model-Agnostic Explanations (LIME) method to elaborate each factor’s contribution to the predicting outcomes. Besides what was found in previous EV preference literature, we find that the frequent usage of ridesourcing, knowledge about EVs, and awareness of environmental protection are important factors in explaining high willingness of adopting EVs.}
}
@article{ZHOU2022122028,
title = {Can digital transformation alleviate corporate tax stickiness: The mediation effect of tax avoidance},
journal = {Technological Forecasting and Social Change},
volume = {184},
pages = {122028},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.122028},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522005492},
author = {Shuya Zhou and Peiyan Zhou and Hannah Ji},
keywords = {Digital transformation, Tax stickiness, Tax avoidance, Internal control, Tax enforcement, Environmental uncertainty},
abstract = {As the salient pain during the economic downturn, tax stickiness has been deeply troubling enterprises. Given the burgeoning academic interest in the role of digital transformation in corporate decision-making, we focus on its implications for tax management decisions. This study aims to test the association between digital transformation and tax stickiness and the mediating effect of tax avoidance. Theoretically, we use an information processing view to incorporate digital transformation into tax research. Empirically, we use the fixed effects model to verify our hypotheses based on the fine-grained panel data of Chinese publicly listed enterprises from 2007 to 2019. Results show that (a) digital transformation significantly reduces tax stickiness, (b) digital transformation alleviates tax stickiness by enhancing tax avoidance, and (c) the alleviating effect of digital transformation on tax stickiness is more prominent in companies with weak internal control and regions with low tax enforcement and high environmental uncertainty. Our findings hold after a range of robustness tests. Our results confirm the importance of digital transformation for tax management and provide new insights for enterprises to reduce tax stickiness through digital transformation. Our study also has important practical implications for business managers and policymakers.}
}
@article{BELLOMARINI2022407,
title = {Data science with Vadalog: Knowledge Graphs with machine learning and reasoning in practice},
journal = {Future Generation Computer Systems},
volume = {129},
pages = {407-422},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004179},
author = {Luigi Bellomarini and Ruslan R. Fayzrakhmanov and Georg Gottlob and Andrey Kravchenko and Eleonora Laurenza and Yavor Nenov and Stéphane Reissfelder and Emanuel Sallinger and Evgeny Sherkhonov and Sahar Vahdati and Lianlong Wu},
keywords = {Knowledge Graphs, Data science, Machine learning, Reasoning, Probabilistic reasoning},
abstract = {Following the recent successful examples of large technology companies, many modern enterprises seek to build Knowledge Graphs to provide a unified view of corporate knowledge, and to draw deep insights using machine learning and logical reasoning. There is currently a perceived disconnect between the traditional approaches for data science, typically based on machine learning and statistical modeling, and systems for reasoning with domain knowledge. In this paper, we demonstrate how to perform a broad spectrum of data science tasks in a unified Knowledge Graph environment. This includes data wrangling, complex logical and probabilistic reasoning, and machine learning. We base our work on the state-of-the-art Knowledge Graph Management System Vadalog, which delivers highly expressive and efficient logical reasoning and provides seamless integration with modern data science toolkits such as the Jupyter platform. We argue that this is a significant step forward towards practical, holistic data science workflows that combine machine learning and reasoning in data science.}
}
@article{HUANG2022107813,
title = {Cross-knowledge-graph entity alignment via relation prediction},
journal = {Knowledge-Based Systems},
volume = {240},
pages = {107813},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107813},
url = {https://www.sciencedirect.com/science/article/pii/S095070512101011X},
author = {Hongren Huang and Chen Li and Xutan Peng and Lifang He and Shu Guo and Hao Peng and Lihong Wang and Jianxin Li},
keywords = {Knowledge alignment, Anchor relation, Self-training, Data augmentation, Relation prediction},
abstract = {The entity alignment task aims to align entities corresponding to the same object in different KGs. The recent work focuses on applying knowledge embedding or graph neural networks to obtain entity embedding for entity alignment. However, there are two challenges encountered by these models: one is some models need to design hyper-parameter to balance embedding loss and alignment loss, the other is the limited training data size. In this paper, we propose a novel entity alignment framework named RpAlign (Relation prediction based cross-knowledge-graph entity Alignment) to address these two issues. Specifically, RpAlign transforms the entity alignment task to the KG completion task to solve and does not need to design any extra alignment component. Unlike the existing models that predict aligned entities by using entity vector distance, the RpAlign defines a new relation called ‘anchor’ for aligned entities, and it predicts new aligned entities based on the relational predictions between the entities. RpAlign employs several data augmentation and improved self-training techniques to mitigate the impact of the data limitation. We conduct experiments on two datasets, and the experimental results show that the RpAlign model significantly outperforms the current state-of-the-art models.}
}
@incollection{SIMONCELLI2022197,
title = {Chapter Four - A collaborative framework among data producers, managers, and users},
editor = {Giuseppe Manzella and Antonio Novellino},
booktitle = {Ocean Science Data},
publisher = {Elsevier},
pages = {197-280},
year = {2022},
isbn = {978-0-12-823427-3},
doi = {https://doi.org/10.1016/B978-0-12-823427-3.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234273000013},
author = {S. Simoncelli and Giuseppe M.R. Manzella and A. Storto and A. Pisano and M. Lipizer and A. Barth and V. Myroshnychenko and T. Boyer and C. Troupin and C. Coatanoan and A. Pititto and R. Schlitzer and Dick M.A. Schaap and S. Diggs},
keywords = {Data integration, Data management, Data products, Ocean decade, Ocean services, Quality elements},
abstract = {The needs of society and the emerging blue economy require access and integration of data and information for the construction of dedicated products. A “transparent and accessible ocean” is one of the key objectives of the Ocean Decade 2021–30. In this context, marine infrastructures become significant components of a global knowledge environment, enabling environmental assessment and providing the necessary data for scientifically valid actions to protect and restore ocean health, to use marine resources in a sustainable way. The data is collected, analyzed, organized, and used by people and their good use/reuse can be obtained with social practices, technological and physical agreements aimed at facilitating collaborative knowledge, decision-making, inference. The vision is a digital ocean data ecosystem made up of multiple, interoperable, and scalable components. The huge amount of data and the resulting products can drive the development of new knowledge as well as new applications and services. Predictive capabilities that derive from the digital ecosystem enable the implementation of services for real-time decision-making, multihazard warning systems, and advance marine space planning. The chapter develops following the progressive complexity and information content of products deriving from oceanic data: data cycle and data collections, data products, oceanic reanalysis. The chapter discusses the new challenges of data products and the complexity of deriving them.}
}
@article{CHEN2022108046,
title = {Multi-sourced sensing and support vector machine classification for effective detection of fire hazard in early stage},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {108046},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108046},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200307X},
author = {Siyuan Chen and Jinchang Ren and Yijun Yan and Meijun Sun and Fuyuan Hu and Huimin Zhao},
keywords = {Fire incident detection, Sensor fusion, Machine learning, Alarm systems, Fire safety},
abstract = {Accurate detection and early warning of fire hazard are crucial for reducing the associated damages. Due to the limitations of smoke-based detection mechanism, most commercial detectors fail to distinguish the smoke from dust and steam, leading to frequent false alarms and costly evacuation unnecessarily. To tackle this issue, we propose a fast and cost-effective indoor fire alarm system for real-time early fire detection under various scenarios, whilst significantly reducing the false alarms. Multimodal sensors are integrated to acquire the data of carbon monoxide, smoke, temperature and humidity, followed by effective data analysis and classification. For ease of embedded implementation, the support vector machine (SVM) is found to outperform the Random Forests (RF), K-means, and Artificial Neural Networks (ANN). On a public dataset and our own dataset, the proposed system performs promising, with the values of the precision, recall, and F1 of 99.8%, 99.6%, and 99.7%, respectively.}
}
@article{LI2022108487,
title = {A perspective survey on deep transfer learning for fault diagnosis in industrial scenarios: Theories, applications and challenges},
journal = {Mechanical Systems and Signal Processing},
volume = {167},
pages = {108487},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108487},
url = {https://www.sciencedirect.com/science/article/pii/S088832702100830X},
author = {Weihua Li and Ruyi Huang and Jipu Li and Yixiao Liao and Zhuyun Chen and Guolin He and Ruqiang Yan and Konstantinos Gryllias},
keywords = {Fault diagnosis, Deep learning, Transfer learning, Domain adaptation, Deep transfer learning},
abstract = {Deep Transfer Learning (DTL) is a new paradigm of machine learning, which can not only leverage the advantages of Deep Learning (DL) in feature representation, but also benefit from the superiority of Transfer Learning (TL) in knowledge transfer. As a result, DTL techniques can make DL-based fault diagnosis methods more reliable, robust and applicable, and they have been widely developed and investigated in the field of Intelligent Fault Diagnosis (IFD). Although several systematic and valuable review articles have been published on the topic of IFD, they summarized relevant research only from an algorithm perspective and overlooked practical applications in industry scenarios. Furthermore, a comprehensive review on DTL-based IFD methods is still lacking. From this insight, it is particularly important and more necessary to comprehensively survey the relevant publications of DTL-based IFD, which will help readers to conveniently understand the current state-of-the-art techniques and to quickly design an effective solution for solving IFD problems in practice. First, theoretical backgrounds of DTL are briefly introduced to present how the transfer learning techniques can be integrated with deep learning models. Then, major applications of DTL and their recent developments in the field of IFD are detailed and discussed. More importantly, suggestions on how to select DTL algorithms in practical applications, and some future challenges are shared. Finally, conclusions of this survey are given. At last, we have reason to believe that the works done in this article can provide convenience and inspiration for the researchers who want to devote their efforts in the progress and advance of IFD.}
}
@article{BERTL2022117464,
title = {A survey on AI and decision support systems in psychiatry – Uncovering a dilemma},
journal = {Expert Systems with Applications},
volume = {202},
pages = {117464},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117464},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422007965},
author = {Markus Bertl and Peeter Ross and Dirk Draheim},
keywords = {Medical information policy, Medical technology, Digital decision support system (DDSS), Clinical Decision Support Systems (CDSS), Artificial Intelligence (AI), Machine Learning (ML), Psychiatry},
abstract = {Every year, healthcare specialists collect more and more data about patients but struggle to use it to optimize disease prevention, diagnosis, or treatment processes. While a manual use of this medical data is virtually impossible considering the vast growth rate, automation with artificial intelligence (AI) and digital decision support systems (DDSSs) has still not yielded any large-scale success in healthcare. We aim to investigate possible obstacles, the trustworthiness based on potential biases, and the adoption of new technology by AI and DDSSs in psychiatry based on a systematic literature review. We screened 520 papers about AI or DDSSs in psychiatry. We added results from a literature screening of 65 articles about AI or DDSSs for post-traumatic stress disorder as one specific psychiatric disease to our research, given that literature possibly deviates from general decision support systems for psychiatry. Out of 80 articles, we extract algorithms, data collection method and sample size of the used training data, and testing process including accuracy metrics. The results show that sample sizes are small (median of 151.5), a focus on algorithm development without real-world interaction, and methodological shortcomings when it comes to the evaluation of DDSSs. Our survey concludes that DDSSs in psychiatry are not ready for the often-promised “AI revolution in healthcare”.}
}
@incollection{MUFTUOGLU202261,
title = {4 - Data sharing and privacy issues arising with COVID-19 data and applications},
editor = {Utku Kose and Deepak Gupta and Victor Hugo C. {de Albuquerque} and Ashish Khanna},
booktitle = {Data Science for COVID-19},
publisher = {Academic Press},
pages = {61-75},
year = {2022},
isbn = {978-0-323-90769-9},
doi = {https://doi.org/10.1016/B978-0-323-90769-9.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323907699000037},
author = {Z. Müftüoğlu and M.A. Kızrak and T. Yıldırım},
keywords = {COVID-19, Data privacy, Medical records sharing, Privacy metrics, Secure data sharing},
abstract = {The coronavirus disease 2019 (COVID-19) (2019-nCov), which was first detected in Wuhan/China in December 2019 and spread to the whole world in a short time, was explained as a new coronavirus by the World Health Organization on February 11, 2020. Countries are developing various strategies against the spread of epidemic threat. The main ones are to develop web-based or mobile applications to reduce the spread and economic damage of the epidemic by making use of COVID-19 datasets. It is seen that the existing applications developed within the framework of these expectations contain absolute location information (direct), relative location information (indirect), and characteristic data defining people. Even if these data mean a lot to the world's struggle with COVID-19, it is necessary to foresee the risks that may occur after the epidemic when the relations of the information are considered. In order to measure the privacy risk of this kind of applications containing personal data, privacy metrics have been defined in the literature. In this chapter, we give a perspective about the sharing and privacy of medical data within the scope of COVID-19. Within this context, privacy models, metrics, and approaches for selecting the appropriate model are described, in particular for COVID-19 applications, and we also propose a new metric with the entropy approach to metrics defined in the literature and effective in determining the privacy score.}
}
@article{ARMENIA2022102936,
title = {Anticipating human resilience and vulnerability on the path to 2030: What can we learn from COVID-19?},
journal = {Futures},
volume = {139},
pages = {102936},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2022.102936},
url = {https://www.sciencedirect.com/science/article/pii/S0016328722000362},
author = {Stefano Armenia and Steven Arquitt and Matteo Pedercini and Alessandro Pompei},
keywords = {Misperception of feedback and delays, Behavioural pattern awareness, Systems thinking, system dynamics, COVID-19, Climate change},
abstract = {The COVID-19 pandemic is causing unprecedented damage to our society and economy, globally impacting progress towards the SDGs. The integrated perspective that Agenda 2030 calls for is ever more important for understanding the vulnerability of our eco-socio-economic systems and for designing policies for enhanced resilience. Since the emergence of COVID-19, countries and international institutions have strengthened their monitoring systems to produce timely data on infections, fostering data-driven decision-making often without the support of systemic-based simulation models. Evidence from the initial phases of the pandemic indicates that countries that were able to implement effective policies before the number of cases grew large (e.g. Australia) managed to contain COVID-19 to a much greater extent than others. We argue that prior systemic knowledge of a phenomenon provides the essential information to correctly interpret data, develop a better understanding of the emerging behavioural patterns and potentially develop early qualitative awareness of how to react promptly in the early phases of destructive phenomena, eventually providing the ground for building more effective simulation models capable of better anticipating the effects of policies. This is even more important as, on its path to 2030, humanity will face other challenges of similar dynamic nature. Chief among these is Climate Change. In this paper, we show how a Systems Thinking and System Dynamics modelling approach is useful for developing a better understanding of these and other issues, and how systemic lessons learned from the COVID-19 case can help decision makers anticipate the destructive dynamics of Climate Change by improving perceptions of the potential impacts of reinforcing feedback and delays, ultimately leading to more timely interventions to achieve the SDGs and mitigate Climate Change risks.}
}
@article{ZHANG2022124919,
title = {Data augmentation for improving heating load prediction of heating substation based on TimeGAN},
journal = {Energy},
volume = {260},
pages = {124919},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124919},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222018205},
author = {Yunfei Zhang and Zhihua Zhou and Junwei Liu and Jianjuan Yuan},
keywords = {Heating load, Prediction model, High-quality data, TimeGAN},
abstract = {Heating load predictions serve as one of the fundamental tasks in heating operation management. Many studies have used data-driven methods to build prediction models, and the quantity and quality of training data are key factors affecting the model performance. However, for some special cases, such as new heating substation and the end period of heating with different load characteristics, sufficient and high-quality data cannot be provided for model training, resulting in low accuracy of the model. In this paper, TimeGAN is applied in the heating field for the first time to augment the data and improve the prediction accuracy of the model. Results show that the prediction error reduces by 50% and CV-RMSE can reach 0.0405 after using TimeGAN in the early period of heating, and the accuracy is highest when the synthetic data are three times of original data. For the mid and end period of heating, the prediction errors can also be reduced by 3%–8% compared with training on original data, and the data amount reaches 15,000–30000, the performance of the model reaches the best.}
}
@article{SANI20221526,
title = {Strategies for Achieving Pre-emptive Resilience in Military Supply Chains},
journal = {Procedia CIRP},
volume = {107},
pages = {1526-1532},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.186},
url = {https://www.sciencedirect.com/science/article/pii/S221282712200470X},
author = {S. Sani and D. Schaefer and J. Milisavljevic-Syed},
keywords = {Military supply chain, Supply chain disruptions, Pre-emptive resilience, Simulation, Mathematical modelling, Decision support, Digital twin},
abstract = {As technological advancement is rapidly evolving modern warfare, military supply chains are becoming more dynamic and complex with high vulnerability to unexpected disruptions. To increase their overall resilience against such unexpected disruptions, traditional approaches are no longer sufficient. To date, research on supply chain resilience has mainly focused on reactive responses and recovery strategies (post-disruption). Hence, the research gap addressed in this paper is that of identifying new and proactive strategies to enable pre-emptive resilience in military supply chains (pre-disruption). In this paper, the authors first provide a critical review of the pertinent literature and research conducted over the past 12 years. Following on from there, they identify new research directions for enabling pre-emptive resilience to aid military logistic planners in monitoring supply chains and strategic decision-making to maintain their resilience.}
}
@article{ZIMMERMANN202252,
title = {Identifying Sales-Influencing Touchpoints along the Omnichannel Customer Journey},
journal = {Procedia Computer Science},
volume = {196},
pages = {52-60},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.11.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022110},
author = {Robert Zimmermann and Wolfgang Weitzl and Andreas Auinger},
keywords = {Omnichannel, Touchpoints, Customer Journey, Sales},
abstract = {Retailers have started to integrate their online and offline channels in order to increase revenue by creating a superior customer experience. However, they lack an instrument with which to identify the touchpoints that are most influential for customer decision making. Therefore, this study introduces a novel, multi-method approach that utilizes combined data-collection and data-analyses procedures that help retailers to identify and meaningfully cluster relevant touchpoints along the customer journey. Results indicate, among others, that retailers can benefit from abandoning the classic, within-company perspective and cluster their touchpoints according to the customers’ perspective. Furthermore, our approach enables retailers to infer the most important sales-influencing touchpoints. Here, findings indicate that retailers should be selective in providing the right touchpoints for their customers, as some of them can have a direct or indirect negative impact on sales. Retailers can use these insights to support their touchpoint-selection and thus decision-making process through thought provoking impulses.}
}
@article{LOPEZGUAJARDO2022108671,
title = {Process intensification 4.0: A new approach for attaining new, sustainable and circular processes enabled by machine learning},
journal = {Chemical Engineering and Processing - Process Intensification},
volume = {180},
pages = {108671},
year = {2022},
issn = {0255-2701},
doi = {https://doi.org/10.1016/j.cep.2021.108671},
url = {https://www.sciencedirect.com/science/article/pii/S0255270121003597},
author = {Enrique A. López-Guajardo and Fernando Delgado-Licona and Alejandro J. Álvarez and Krishna D.P. Nigam and Alejandro Montesinos-Castellanos and Ruben Morales-Menendez},
keywords = {Process intensification, Industry 4.0, Process intensification 4.0, Machine learning, Circular Chemistry, Artificial Intelligence},
abstract = {This paper reviews system-level transformations converging into the next generation of Process Intensification strategies defined as PI4.0. Process Intensification 4.0 uses data-driven algorithms to understand other physical and chemical processes that improve equipment design, predictive control, and optimization. Following this, an overview of the use of Artificial Intelligence techniques, particularly Machine Learning for the acceleration of equipment design, process optimization, and streamlining, is presented. This work will highlight and discuss the emerging framework of the integration between Circular Chemistry, Industry 4.0, and Process Intensification and how the data obtained from this integration is at the core of the next generation of Process Intensification strategies. This is supported by a discussion of different cases that apply data-driven models enabled by Machine Learning as a mean to enhance an intensified system (product synthesis, equipment or methods).}
}
@article{MUNOZGAMA2022103994,
title = {Process mining for healthcare: Characteristics and challenges},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {103994},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.103994},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000107},
author = {Jorge Munoz-Gama and Niels Martin and Carlos Fernandez-Llatas and Owen A. Johnson and Marcos Sepúlveda and Emmanuel Helm and Victor Galvez-Yanjari and Eric Rojas and Antonio Martinez-Millana and Davide Aloini and Ilaria Angela Amantea and Robert Andrews and Michael Arias and Iris Beerepoot and Elisabetta Benevento and Andrea Burattin and Daniel Capurro and Josep Carmona and Marco Comuzzi and Benjamin Dalmas and Rene {de la Fuente} and Chiara {Di Francescomarino} and Claudio {Di Ciccio} and Roberto Gatta and Chiara Ghidini and Fernanda Gonzalez-Lopez and Gema Ibanez-Sanchez and Hilda B. Klasky and Angelina {Prima Kurniati} and Xixi Lu and Felix Mannhardt and Ronny Mans and Mar Marcos and Renata {Medeiros de Carvalho} and Marco Pegoraro and Simon K. Poon and Luise Pufahl and Hajo A. Reijers and Simon Remy and Stefanie Rinderle-Ma and Lucia Sacchi and Fernando Seoane and Minseok Song and Alessandro Stefanini and Emilio Sulis and Arthur H.M. {ter Hofstede} and Pieter J. Toussaint and Vicente Traver and Zoe Valero-Ramon and Inge van de Weerd and Wil M.P. {van der Aalst} and Rob Vanwersch and Mathias Weske and Moe Thandar Wynn and Francesca Zerbato},
keywords = {Process mining, Healthcare},
abstract = {Process mining techniques can be used to analyse business processes using the data logged during their execution. These techniques are leveraged in a wide range of domains, including healthcare, where it focuses mainly on the analysis of diagnostic, treatment, and organisational processes. Despite the huge amount of data generated in hospitals by staff and machinery involved in healthcare processes, there is no evidence of a systematic uptake of process mining beyond targeted case studies in a research context. When developing and using process mining in healthcare, distinguishing characteristics of healthcare processes such as their variability and patient-centred focus require targeted attention. Against this background, the Process-Oriented Data Science in Healthcare Alliance has been established to propagate the research and application of techniques targeting the data-driven improvement of healthcare processes. This paper, an initiative of the alliance, presents the distinguishing characteristics of the healthcare domain that need to be considered to successfully use process mining, as well as open challenges that need to be addressed by the community in the future.}
}
@article{ZHANG2022247,
title = {DeepBindBC: A practical deep learning method for identifying native-like protein-ligand complexes in virtual screening},
journal = {Methods},
volume = {205},
pages = {247-262},
year = {2022},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2022.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1046202322001633},
author = {Haiping Zhang and Tingting Zhang and Konda Mani Saravanan and Linbu Liao and Hao Wu and Haishan Zhang and Huiling Zhang and Yi Pan and Xuli Wu and Yanjie Wei},
keywords = {Native like protein-ligand, Drug virtual screening, ResNet, Deep learning, Human pancreatic alpha amylase inhibitor},
abstract = {Identifying native-like protein–ligand complexes (PLCs) from an abundance of docking decoys is critical for large-scale virtual drug screening in early-stage drug discovery lead searching efforts. Providing reliable prediction is still a challenge for most current affinity predicting models because of a lack of non-binding data during model training, lost critical physical–chemical features, and difficulties in learning abstract information with limited neural layers. In this work, we proposed a deep learning model, DeepBindBC, for classifying putative ligands as binding or non-binding. Our model incorporates information on non-binding interactions, making it more suitable for real applications. ResNet model architecture and more detailed atom type representation guarantee implicit features can be learned more accurately. Here, we show that DeepBindBC outperforms Autodock Vina, Pafnucy, and DLSCORE for three DUD.E testing sets. Moreover, DeepBindBC identified a novel human pancreatic α-amylase binder validated by a fluorescence spectral experiment (Ka = 1.0 × 105 M). Furthermore, DeepBindBC can be used as a core component of a hybrid virtual screening pipeline that incorporating many other complementary methods, such as DFCNN, Autodock Vina docking, and pocket molecular dynamics simulation. Additionally, an online web server based on the model is available at http://cbblab.siat.ac.cn/DeepBindBC/index.php for the user’s convenience. Our model and the web server provide alternative tools in the early steps of drug discovery by providing accurate identification of native-like PLCs.}
}
@article{AZHIN202279,
title = {Application of Multivariable Data Analysis in Mineral Processing},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {21},
pages = {79-84},
year = {2022},
note = {19th IFAC Symposium on Control, Optimization and Automation in Mining, Mineral and Metal Processing MMM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.247},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014793},
author = {Maryam Azhin and Robert Lopetinsky and John Stiksma and Faraz Amjad and Bardia Hassanzadeh and Siddhartha Tirumalaraju and Chowdary Meenavilli},
keywords = {Artificial Intelligence, Soft Sensor, Machine Learning, Hydrometalurgy, Metallurgy, Process Monitoring, Copper Boil},
abstract = {Data analysis and application of machine learning (ML) have demonstrated successful performance in various data rich industrial applications. Mineral processing and metallurgical operations are considered suitable for implementation of novel ML-based algorithms. The key operating performance and product outputs are usually obtained from the lab measurements and analyses that can be expensive, complex, and time consuming. Therefore, the development and application of a soft sensor and/or a state observer is a useful option to be considered due to their ability to provide the distribution of desired outputs in a continuous manner. In addition, the motivation to apply a soft sensor (a data-based model) is to provide guidance and/or information feedback to the operator in charge of making operational decisions. The soft sensor was developed at Sherritt's Metal Plant in Fort Saskatchewan as a nonlinear neural network model and it was based on two years of plant historical data. The model was also validated based on historical data, live testing, and additional sampling of process streams during simultaneous sampling campaign.}
}
@article{LI2022105720,
title = {Imbalanced nitrogen–phosphorus input alters soil organic carbon storage and mineralisation in a salt marsh},
journal = {CATENA},
volume = {208},
pages = {105720},
year = {2022},
issn = {0341-8162},
doi = {https://doi.org/10.1016/j.catena.2021.105720},
url = {https://www.sciencedirect.com/science/article/pii/S0341816221005786},
author = {Juanyong Li and Guangxuan Han and Guangmei Wang and Xiaoling Liu and Qiqi Zhang and Yawen Chen and Weimin Song and Wendi Qu and Xiaojing Chu and Peiguang Li},
keywords = {Imbalanced N and P input, SOC cycling, Microbial community structure, Salt marsh},
abstract = {A large imbalance in soil nitrogen (N) and phosphorus (P) inputs induced by anthropogenic activities is anticipated to profoundly influence soil carbon (C) budgets in salt marshes. In this study, we hypothesized that imbalances in the nitrogen–phosphorus (N–P) input would result in the nonlinear response of soil organic carbon (SOC) content, fractions and mineralization to the N–P input ratio. We applied three N–P input ratios (low (5:1), medium (15:1), high (45:1)) in a salt marsh of the Yellow River Delta (YRD) for four years (in which N added increased from 8.67 to 26.01 g N m−2 y−1 and P added decreased from 1.73 to 0.58 g P m−2 y−1) and quantified their impacts on SOC fractions and SOC mineralisation. The control treatment did not receive fertilization. The results showed that the N and P input led to overall increases in the availability of soil nutrients (i.e., inorganic N (IN) and available P (AP)), stimulation of plant biomass and changes of microbial community structure (i.e., γ- and δ-Proteobacteria and Acidobacteria). N and P input increased soil dissolved organic carbon (DOC) and decreased aromatic DOC components through improving N availability and stimulating plant growth. Notably, though, there may be a threshold N–P input ratio between 15:1 and 45:1 that, once crossed, triggers the loss of SOC. Appropriate increase in N availability induced by low and medium N-P input ratios would stimulate the SOC mineralization. However, excessive N-P input ratio would reduce SOC mineralization. Path analysis indicated that N–P input ratios dominantly regulate SOC mineralisation by changing soil DOC and microbial biomass (MBC)contents and microbial community structure. Thus, we speculate that the continuous increase in N input causes a growing N–P imbalance that reduces SOC stocks, despite a reduction in SOC mineralisation.}
}
@incollection{ZOHURI2022121,
title = {Chapter 5 - Mathematical modeling driven predication},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {121-163},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000052},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Data mining and data analytics, Forecasting and prediction, Modeling and mathematics},
abstract = {During the past decade, there has been a tremendous blast and progress in computation technology, and with it comes vast amounts of data in a variety of fields such as the economy, medicine, biology, banking services such as customer relation management and credit card fraud, finance, demographic population growth from a demographical point of view nationwide and worldwide, and the need for new lifestyles and growth in term of continuous renewable sources of energy and its production, as well as marketing are among the fields that can be mentioned. The challenge of understanding these data has led to the development of new tools such as predictive analytics in the field of statistics and spawned new areas such as data mining, machine learning, and bioinformatics to process these data and determine the integrity of their information for prediction analysis. Many of these tools have common underpinnings but are often expressed with different terminology. This chapter will summarize the important ideas in these areas in a common conceptual framework.}
}
@incollection{HOVENGA20221,
title = {Chapter 1 - Transforming health care},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {1-16},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00020-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234136000203},
author = {Evelyn Hovenga},
keywords = {Value proposition, Ecosystem, Government leadership, Innovation, Roadmap, Infrastructure},
abstract = {Revolutionary changes are needed to reform healthcare delivery systems globally to realise the vision of healthcare access for everyone, no matter their location, facilitated and enabled by effective national digital health ecosystems. This chapter introduces the reader to widely recognised drivers and desired future outcomes. Foundational principles adopted as the focus for this text are identified. This includes the need for sustained leadership, committed investments, effective governance, a national technical framework, and a description of the digital health ecosystem characteristics. Innovation blind spots that have the potential to undermine digital health transformation need to be identified, considered, and included as roadmap components. Principles to be adopted, some examples of existing foundations, and value proposition drivers are introduced, followed by a description of a digitally enabling health environment. The chapter concludes with the identification of key requirements for global and national action to be incorporated in a digital health roadmap which needs to consider numerous fragmented influencing factors. Six foundational concepts representing desired outcomes are introduced.}
}
@article{GROVER2022103639,
title = {A theoretical perspective on organizational culture and digitalization},
journal = {Information & Management},
volume = {59},
number = {4},
pages = {103639},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103639},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000519},
author = {Varun Grover and Shih-Lun Tseng and Wenxi Pu},
keywords = {Organizational culture, Digitalization, Digital culture},
abstract = {Digitalization has fundamentally changed organizational structures and processes and affects how people interact with each other, thereby impacting organizational culture. Given the pervasiveness of digitalization today, it is useful to study its profound effects on organizational culture through new theoretical lenses. In this paper, we offer a fresh perspective on organizational culture in the digital world. We accomplish this by integrating two competing perspectives and then leveraging the new perspective to identify digital cultural resources and propose a conceptualization of digital culture. We frame our conceptualization around the cultural resources for digitalization and describe four digital culture archetypes.}
}
@article{SALIM2022102786,
title = {Data analytics of social media 3.0: Privacy protection perspectives for integrating social media and Internet of Things (SM-IoT) systems},
journal = {Ad Hoc Networks},
volume = {128},
pages = {102786},
year = {2022},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.102786},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522000051},
author = {Sara Salim and Benjamin Turnbull and Nour Moustafa},
keywords = {Social media, Internet of Things (ioT), Data analytics, Privacy preservation},
abstract = {With the rapid evolution of web technologies, Web 3.0 aims to expand on current and emerging social media platforms such as Facebook, Twitter, and TikTok, and integrate emerging computing paradigms, including the Internet of Things (IoT), named social media 3.0. The combinations of these platforms in Web 3.0 promises consumers greater integration, interaction, and more seamless movement between physical spaces. However, ensuring the privacy of data across such systems is a potential challenge in this space. In this study, we propose a new privacy-preserving social media 3.0 framework that illustrates the interaction of SM and IoT services and estimates how this interaction could impact users’ behaviors. The framework consists of three main components. First, a new relational dataset, named SM-IoT, is designed to dynamically connect users with their IoT services and assist in processing data heterogeneity. Second, a data pre-processing module is employed for filtering heterogeneous data and providing a certain level of privacy preservation on the data. Third, data analytics using different statistical and machine/deep learning methods are applied to examine data complexity and identify users’ behaviors. The results revealed that our proposed framework can efficiently identify users’ behaviors from social media 3.0 data sources. The outcomes of comparing our SM-IoT dataset with two other well-known SM datasets, namely Pokec and Renren, as well as Activity Recognition with Ambient Sensing (ARAS) IoT dataset reveals the fidelity of our dataset to be used for future evaluations of privacy-preserving and machine learning-based decision-making techniques.}
}
@article{MIASAYEDAVA2022114283,
title = {Automated environmental compliance monitoring of rivers with IoT and open government data},
journal = {Journal of Environmental Management},
volume = {303},
pages = {114283},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.114283},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721023458},
author = {Lizaveta Miasayedava and Keegan McBride and Jeffrey Andrew Tuhtan},
keywords = {Environmental compliance monitoring, Environmental flows, Internet of things, Open government data},
abstract = {Environmental monitoring of rivers is a cornerstone of the European Union's Water Framework Directive. It requires the estimation and reporting of environmental flows in rivers whose characteristics vary widely across the EU member states. This variability has resulted in a fragmentation of estimation and reporting methods for environmental flows and is exhibited by the myriad of regulatory guidelines and estimation procedures. To standardise and systematically evaluate environmental flows at the pan-European scale, we propose to formalise the estimation procedures through automation by reusing existing river monitoring resources. In this work, we explore how sensor-generated hydrological open government data can be repurposed to automate the estimation and monitoring of river environmental flows. In contrast to existing environmental flows estimation methods, we propose a scalable IoT-based architecture and implement its cloud-layer web service. The major contribution of this work is the demonstration of an automated environmental flows system based on open river monitoring data routinely collected by national authorities. Moreover, the proposed system adds value to existing environmental monitoring data, reduces development and operational costs, facilitates streamlining of environmental compliance and allows for any authority with similar data to reuse or scale it with new data and methods. We critically discuss the opportunities and challenges associated with open government data, including its quality. Finally, we demonstrate the proposed system using the Estonian national river monitoring network and define further research directions.}
}
@article{GAO2022185,
title = {What can we learn from telematics car driving data: A survey},
journal = {Insurance: Mathematics and Economics},
volume = {104},
pages = {185-199},
year = {2022},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167668722000233},
author = {Guangyuan Gao and Shengwang Meng and Mario V. Wüthrich},
keywords = {Telematics car driving data, Heatmaps, Poisson regression models, Convolutional neural networks, Limited fluctuation credibility model},
abstract = {We give a survey on the field of telematics car driving data research in actuarial science. We describe and discuss telematics car driving data, we illustrate the difficulties of telematics data cleaning, and we highlight the transparency issue of telematics car driving data resulting in associated privacy concerns. Transparency of telematics data is demonstrated by aiming at correctly allocating different car driving trips to the right drivers. This is achieved rather successfully by a convolutional neural network that manages to discriminate different car drivers by their driving styles. In a last step, we describe two approaches of using telematics data for improving claims frequency prediction, one is based on telematics heatmaps and the other one on time series of individual trips, respectively.}
}
@article{YANG2022109092,
title = {A noise-aware fuzzy rough set approach for feature selection},
journal = {Knowledge-Based Systems},
volume = {250},
pages = {109092},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109092},
url = {https://www.sciencedirect.com/science/article/pii/S095070512200538X},
author = {Xiaoling Yang and Hongmei Chen and Tianrui Li and Chuan Luo},
keywords = {Fuzzy rough set, Robust feature selection, Noisy data, Density distribution, Dependency function},
abstract = {Feature selection has aroused extensive attention and aims at selecting features that are highly relevant to classification from raw datasets to improve the performance of a learning model. Fuzzy rough set theory is a powerful mathematical method for feature selection. The classical fuzzy rough set model is very sensitive to the noise while the noise samples in classification data often appear. In addition, fuzzy rough set theory does not fit well when the density distribution of the samples in the dataset varies greatly. Thus, it is of great significance to improve the robustness of fuzzy rough set models and its adaptability to data for feature selection. Inspired by these issues, we focus on the robust fuzzy rough set approach for feature selection. We first propose a robust fuzzy rough set model based on data distribution to achieve the purpose of anti-noise i.e., Noise-aware Fuzzy Rough Sets (NFRS) model. This model proposes a novel search mechanism, which weakens the sensitivity of the approximation operator to noise by considering the distribution of samples in the decision classes to weight the samples, further obtains three kinds of samples, i.e., intra-class samples, boundary samples, and outlier samples. Then, the degrees of relevance of the feature for class is defined by the dependency function based on the NFRS model to evaluate the significance of the feature subset. On this basis, an evaluation function about feature significance is constructed, which simultaneously considers the relevance and redundancy of a candidate feature provided for the selected subset and the remaining feature subset. A novel forward greedy search algorithm is presented to select a feature sequence. The selected features are subsequently evaluated with downstream classification tasks. Experimental using real-world datasets demonstrate the effectiveness of the proposed model and its superiority against comparison baseline methods.}
}
@incollection{RANGANATHANGANAKAMMAL2022221,
title = {Chapter 8 - Genomics technologies and bioinformatics in allergy and immunology},
editor = {Christopher Chang},
booktitle = {Allergic and Immunologic Diseases},
publisher = {Academic Press},
pages = {221-260},
year = {2022},
isbn = {978-0-323-95061-9},
doi = {https://doi.org/10.1016/B978-0-323-95061-9.00008-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323950619000084},
author = {Satishkumar {Ranganathan Ganakammal} and Ke Huang and Magdalena Walkiewicz and Sandhya Xirasagar},
keywords = {Clinical genomics, multiomics, molecular variants, informatics, big data, machine learning, FAIR guidelines},
abstract = {A detailed description of all aspects, approaches, and technologies and in bioinformatics is beyond the scope of this chapter. Instead, our goal is to help clinical researchers grasp key concepts and complexities of genomics and other omics technologies beginning with the experimental platforms used to produce these data, the strengths and weaknesses of these platforms, preprocessing and analysis of the data, and understanding the results and apply them to clinical research and patient care. Systems biology approaches which aim to deduce a holistic picture by integrating the data arising from these technologies are complex and can require significant investments in computation infrastructure and human resources. Any conclusions from these genomics studies should be supported by the phenotypic characteristics of the different disorders which are described in the accompanying chapters of this book. Different diseases have different levels of actionable genomic information that can be used directly for clinical care. The particular clinical practice setting, clinics, and medical offices versus hospitals with significant infrastructure and/or attached to academic institutions which are heavily invested in clinical genomics research with access to interdisciplinary staff including bioinformaticians and molecular geneticists, genetic counselors, etc., will have a significant influence on the choice of the platforms and downstream data analysis approaches. We hope that the information in this chapter will assist researchers in choosing the appropriate strategy for employing genomics and omics-based experimental platforms for the diseases that they treat.}
}
@article{KIM2022101547,
title = {Short-term prediction of particulate matter (PM10 and PM2.5) in Seoul, South Korea using tree-based machine learning algorithms},
journal = {Atmospheric Pollution Research},
volume = {13},
number = {10},
pages = {101547},
year = {2022},
issn = {1309-1042},
doi = {https://doi.org/10.1016/j.apr.2022.101547},
url = {https://www.sciencedirect.com/science/article/pii/S1309104222002288},
author = {Bu-Yo Kim and Yun-Kyu Lim and Joo Wan Cha},
keywords = {Particulate matter prediction, PM, PM, Tree-based machine learning, Air quality monitoring, Light gradient boosting algorithm},
abstract = {In this study, highly accurate particulate matter (PM10 and PM2.5) predictions were obtained using meteorological prediction data from the local data assimilation and prediction system (LDAPS) and tree-based machine learning (ML). The study area was Seoul, South Korea, and data from July 2018 to June 2021 as well as LDAPS 36-h predictions with 1-h intervals 4 times a day were used. The predicted PM values were then compared with the observed PM measurements to evaluate the prediction accuracy. The PM prediction performance of the Community Multi-Scale Air Quality (CMAQ)-based chemical transport model (CTM) was compared with that reported by this study. The experimental results report that, among tree-based ML algorithms, light gradient boosting (LGB) is the most suitable for PM prediction. The PM prediction results of the LGB algorithm for the hourly test data were: bias = −0.10 μg/m3, root mean square error (RMSE) = 13.15 μg/m3, and R2 = 0.86 for PM10 and bias = −0.02 μg/m3, RMSE = 7.48 μg/m3, and R2 = 0.83 for PM2.5, and for daily mean were: RMSE ≤1.16 μg/m3 and R2 = 0.996. The relative RMSE (%RMSE) is 21% lower than the results of the CTM model, and R2 is 0.20 higher. Even in the high PM concentration case prediction results, the algorithm showed good predictive performance with %RMSE = 8.91%–20.43% and R2 = 0.89–0.97. Therefore, in addition to the CTM, high-accuracy PM prediction results using ML can also be used for air quality monitoring and improvement.}
}
@article{NAGITTA20221084,
title = {Human-centered artificial intelligence for the public sector: The gate keeping role of the public procurement professional},
journal = {Procedia Computer Science},
volume = {200},
pages = {1084-1092},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.308},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003179},
author = {Pross Oluka Nagitta and Godfrey Mugurusi and Peter Adoko Obicci and Emmanuel Awuor},
keywords = {Human-centered artificial intelligence (AI), Explainable AI(XAI), public procurement, Ethical AI, Responsible AI, developing countries},
abstract = {The increasing deployment of artificial intelligence (AI) powered solutions for the public sector is hoped to change how developing countries deliver services in key sectors such as agriculture, healthcare, education, and social sectors. And yet AI has a high potential for abuse and creates risks, which if not managed and monitored will jeopardize respect and dignity of the most vulnerable in society. In this study, we argue for delineating public procurements’ role in the human-centred AI (HCAI) discourses, focusing on the developing countries. The study is based on an exploratory inquiry and gathered data among procurement practitioners in Uganda and Kenya, which have similar country procurement regimes: where traditional forms of competition in procurement apply compared to more recent pre-commercial procurement mechanisms that suit AI procurement. We found limited customization in AI technologies, a lack of developed governance frameworks, and little knowledge and distinction between AI procurement and other typical technology procurement processes. We proposed a framework, which in absence of good legal frameworks can allow procurement professionals to embed HCAI principles in AI procurement processes.}
}
@incollection{SCHNEIDER2022149,
title = {Chapter 8 - Machine learning: ML for eHealth systems},
editor = {Patrick Schneider and Fatos Xhafa},
booktitle = {Anomaly Detection and Complex Event Processing over IoT Data Streams},
publisher = {Academic Press},
pages = {149-191},
year = {2022},
isbn = {978-0-12-823818-9},
doi = {https://doi.org/10.1016/B978-0-12-823818-9.00019-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128238189000195},
author = {Patrick Schneider and Fatos Xhafa},
keywords = {Algorithms, Diagnostic systems, Ethics, safety, and equity, Learning problems, Learning techniques, Ethics, safety, privacy, accountability, and transparency, ML frameworks, Federated learning},
abstract = {Healthcare is at the dawn of a new era of intelligent systems and improved human relationships. The potential of artificial intelligence (AI) and machine learning (ML) technologies to support decision-making, optimize workflows, and free up quality human time is revolutionizing how people deliver and receive care. The success and performance of AI-based expert-level diagnostic systems have inspired unprecedented optimism. However, there are growing concerns about ethics, safety, and equity in the delivery of care. The lack of clarity about how it works and the resulting mistrust has negatively affected the relationship between AI and caregivers and recipients, preventing adoption. This chapter provides a general overview of the various AI learning areas and a detailed introduction to the area of federated learning, a key to the application of machine learning in the future vision of healthcare.}
}
@article{NI20221020,
title = {Socioeconomic inequalities in cancer incidence and access to health services among children and adolescents in China: a cross-sectional study},
journal = {The Lancet},
volume = {400},
number = {10357},
pages = {1020-1032},
year = {2022},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(22)01541-0},
url = {https://www.sciencedirect.com/science/article/pii/S0140673622015410},
author = {Xin Ni and Zhe Li and Xinping Li and Xiao Zhang and Guoliang Bai and Yingying Liu and Rongshou Zheng and Yawei Zhang and Xin Xu and Yuanhu Liu and Chenguang Jia and Huanmin Wang and Xiaoli Ma and Huyong Zheng and Yan Su and Ming Ge and Qi Zeng and Shengcai Wang and Junyang Zhao and Yueping Zeng and Guoshuang Feng and Yue Xi and Zhuo Deng and Yongli Guo and Zhuoyu Yang and Jinzhe Zhang},
abstract = {Summary
Background
Despite the substantial burden caused by childhood cancer globally, childhood cancer incidence obtained in a nationwide childhood cancer registry and the accessibility of relevant health services are still unknown in China. We comprehensively assessed the most up-to-date cancer incidence in Chinese children and adolescents, nationally, regionally, and in specific population subgroups, and also examined the association between cancer incidence and socioeconomic inequality in access to health services.
Methods
In this national cross-sectional study, we used data from the National Center for Pediatric Cancer Surveillance, the nationwide Hospital Quality Monitoring System, and public databases to cover 31 provinces, autonomous regions, and municipalities in mainland China. We estimated the incidence of cancer among children (aged 0–14 years) and adolescents (aged 15–19 years) in China through stratified proportional estimation. We classified regions by socioeconomic status using the human development index (HDI). Incidence rates of 12 main groups, 47 subgroups, and 81 subtypes of cancer were reported and compared by sex, age, and socioeconomic status, according to the third edition of the International Classification of Childhood Cancer. We also quantified the geographical and population density of paediatric oncologists, pathology workforce, diagnoses and treatment institutions of paediatric cancer, and paediatric beds. We used the Gini coefficient to assess equality in access to these four health service indicators. We also calculated the proportions of cross-regional patients among new cases in our surveillance system.
Findings
We estimated the incidence of cancer among children (aged 0–14 years) and adolescents (aged 15–19 years) in China from Jan 1, 2018, to Dec 31, 2020. An estimated 121 145 cancer cases were diagnosed among children and adolescents in China between 2018 and 2020, with world standard age-standardised incidence rates of 122·86 (95% CI 121·70–124·02) per million for children and 137·64 (136·08–139·20) per million for adolescents. Boys had a higher incidence rate of childhood cancer (133·18 for boys vs 111·21 for girls per million) but a lower incidence of adolescent cancer (133·92 for boys vs 141·79 for girls per million) than girls. Leukaemias (42·33 per million) were the most common cancer group in children, whereas malignant epithelial tumours and melanomas (30·39 per million) surpassed leukaemias (30·08 per million) in adolescents as the cancer with the highest incidence. The overall incidence rates ranged from 101·60 (100·67–102·51) per million in very low HDI regions to 138·21 (137·14–139·29) per million in high HDI regions, indicating a significant positive association between the incidence of childhood and adolescent cancer and regional socioeconomic status (p<0·0001). The incidence in girls showed larger variation (48·45% from the lowest to the highest) than boys (36·71% from lowest to highest) in different socioeconomic regions. The population and geographical densities of most health services also showed a significant positive correlation with HDI levels. In particular, the geographical density distribution (Gini coefficients of 0·32–0·47) had higher inequalities than population density distribution (Gini coefficients of 0·05–0·19). The overall proportion of cross-regional patients of childhood and adolescent cancer was 22·16%, and the highest proportion occurred in retinoblastoma (56·54%) and in low HDI regions (35·14%).
Interpretation
Our study showed that the burden of cancer in children and adolescents in China is much higher than previously nationally reported from 2000 to 2015. The distribution of the accessibility of health services, as a social determinant of health, might have a notable role in the socioeconomic inequalities in cancer incidence among Chinese children and adolescents. With regards to achieving the Sustainable Development Goals, policy approaches should prioritise increasing the accessibility of health services for early diagnosis to improve outcomes and subsequently reduce disease burdens, as well as narrowing the socioeconomic inequalities of childhood and adolescent cancer.
Funding
National Major Science and Technology Projects of China, National Natural Science Foundation of China, Chinese Academy of Engineering Consulting Research Project, Wu Jieping Medical Foundation, Beijing Municipal Administration of Hospitals Incubating Program.}
}
@article{BARCELLOS2022101961,
title = {Towards defining data interpretability in open data portals: Challenges and research opportunities},
journal = {Information Systems},
volume = {106},
pages = {101961},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101961},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001538},
author = {Raissa Barcellos and Flavia Bernardini and José Viterbo},
keywords = {Data interpretability, Open data portals},
abstract = {Open data portals are growing in scope, and the development of this initiative remains one of the main ways to help create new value for society and the economy. Citizens can use the open data made available on these portals to participate more effectively in democratic processes. For that, they have to be able to access, manipulate and interpret such data. Different authors present different definitions and perceptions about the meaning of data interpretability. Today there is no formal consensus on the concept of data interpretability. The goal of this work is to conceptualize what data interpretability is formal. For this, we carried out literature research to identify the definitions of data interpretability. In addition, we studied the information quality literature to identify the Non-Function Requirements that shape the concept of information quality. So, we aligned the interpretability characteristics with the NFR Framework characteristics to find a unique definition. We also conduct a qualitative analysis with experts in data analysis, e-government, and transparency to identify what these experts understand by interpretability. Based on these two studies, we defined interpretability through a model composed of 8 dimensions, each consisting of different characteristics, which must be guaranteed in the data interpretability process to interpret the data correctly. We understand that, for such characteristics to being guaranteed in the interpretability of open government data, it is necessary to have computational tools to support the user. Thus, we also surveyed which technologies and methods ensure each of the interpretability characteristics and pointed out which computational tools implement such technologies and methods. Finally, we analyzed three large open data portals to identify which characteristics are present in these portals, and we note that there are still several challenges to be handled in open government data portals.}
}
@article{SYU2022835,
title = {Usability and Usefulness of Circularity Indicators for Manufacturing Performance Management},
journal = {Procedia CIRP},
volume = {105},
pages = {835-840},
year = {2022},
note = {The 29th CIRP Conference on Life Cycle Engineering, April 4 – 6, 2022, Leuven, Belgium.},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.02.138},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122001391},
author = {Fu-Siang Syu and Adarsh Vasudevan and Mélanie Despeisse and Arpita Chari and Ebru Turanoglu Bekar and Maria M. Gonçalves and Marco A. Estrela},
keywords = {Circular economy, Circularity indicators, Sustainable manufacturing},
abstract = {Advances in industrial digitalization present many opportunities for process and product data exploitation in manufacturing, unlocking new systemic measures of performance beyond a single machine, process, facility area and even beyond the factory gates. However, existing data models and manufacturing systems’ performance measures are still focused on productivity, quality and delivery time, which could potentially lead to an accelerated linear economy. To shift to more circular industrial systems, we need to identify and assess circularity opportunities in ways that align the goals of sustainable and industrial development. In this study, micro-level circular indicators were reviewed, selected, analysed and tested in a manufacturing company to evaluate their usability and usefulness to guide process improvements. The aim is to enable circular and eco-efficient solutions towards sustainable production systems. Usability and usefulness of the indicators are essential to their integration into established environmental and operations management systems. The main contribution of this study is in the identification of key features making circularity indicators usable and useful from a manufacturer’s perspective. The conclusion also suggests directions for further research on tools and methods to support circular manufacturing.}
}
@article{DASILVA2022181,
title = {An ontological approach for modelling evolutionary knowledge of prognostic method selection},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {181-186},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.190},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001914},
author = {Márcio J. {da Silva} and Lynceo F. Braghirolli and Eike Broda and Hendrik Engbers and Enzo M. Frazzon and Carlos E. Pereira},
keywords = {Domain Ontology, Evolutionary Knowledge, Semantics Mapping, Industry 4.0 (I4.0)},
abstract = {The selection of appropriate predictive maintenance methods based on the current state of a manufacturing system, its machines, and its components is not an easy task due to the multitude of physical and virtual resources available. Moreover, the value of the prognostic information provided by a prognostic method, when applied to a given machine, depends on the system structure and the production and maintenance planning process. Therefore, it is necessary to consider the impact of such information on the system’s key performance indicators to assess the real benefits of each prognostic method. Based on knowledge from predictive maintenance approaches, manufacturing system simulation, and production and maintenance planning, an appropriate semantic model allows establishing a shared vocabulary and understanding among all these fields, along with a description of their relationship. Thus, this paper proposes an ontology of domain termed Ontological MetaMaintain (OntoMM), which specifies the description of concepts and their relationships to provide evolutionary knowledge on domain structure which further specializes in information flows. It is a novel semantic architecture in an ontology network with three modules that address automated prognostic method selection in an industrial environment.}
}
@article{HASHIGUCHI2022368,
title = {Fulfilling the Promise of Artificial Intelligence in the Health Sector: Let’s Get Real},
journal = {Value in Health},
volume = {25},
number = {3},
pages = {368-373},
year = {2022},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2021.11.1369},
url = {https://www.sciencedirect.com/science/article/pii/S1098301521032253},
author = {Tiago Cravo Oliveira Hashiguchi and Jillian Oderkirk and Luke Slawomirski},
keywords = {artificial intelligence, governance, machine learning, policy},
abstract = {Objectives
This study aimed to showcase the potential and key concerns and risks of artificial intelligence (AI) in the health sector, illustrating its application with current examples, and to provide policy guidance for the development, assessment, and adoption of AI technologies to advance policy objectives.
Methods
Nonsystematic scan and analysis of peer-reviewed and gray literature on AI in the health sector, focusing on key insights for policy and governance.
Results
The application of AI in the health sector is currently in the early stages. Most applications have not been scaled beyond the research setting. The use in real-world clinical settings is especially nascent, with more evidence in public health, biomedical research, and “back office” administration. Deploying AI in the health sector carries risks and hazards that must be managed proactively by policy makers. For AI to produce positive health and policy outcomes, 5 key areas for policy are proposed, including health data governance, operationalizing AI principles, flexible regulation, skills among health workers and patients, and strategic public investment.
Conclusions
AI is not a panacea, but a tool to address specific problems. Its successful development and adoption require data governance that ensures high-quality data are available and secure; relevant actors can access technical infrastructure and resources; regulatory frameworks promote trustworthy AI products; and health workers and patients have the information and skills to use AI products and services safely, effectively, and efficiently. All of this requires considerable investment and international collaboration.}
}
@article{WANG202255,
title = {A Knowledge-enriched Framework for Life Cycle Assessment in Manufacturing},
journal = {Procedia CIRP},
volume = {105},
pages = {55-60},
year = {2022},
note = {The 29th CIRP Conference on Life Cycle Engineering, April 4 – 6, 2022, Leuven, Belgium.},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122000105},
author = {Yanan Wang and Jiaqi Tao and Weipeng Liu and Tao Peng and Renzhong Tang and Qi Wu},
keywords = {knowledge graph, life cycle assessment, process, flow, life cycle inventory analysis, aluminum die casting},
abstract = {Even though carbon neutralization is promised by several countries, many developed tools for life cycle assessment (LCA) or sustainability performance is still not widely and easily used in industrial practices. A big gap exists between having the tools and using the tools, which is currently facilitated by human expertise. LCA practitioners should embrace and take advantage the rapid development of knowledge graph (KG) technology. In this paper, the important position and key methodologies of KG are presented in the context of LCA, latest advances are reviewed and presented. A framework of knowledge-enriched LCA is proposed which was illustrated with a LCA-oriented process-flow knowledge graph (PFKG) for inventory analysis. The knowledge acquisition and construction of PFKG was demonstrated with aluminum die casting, and on top of which a recommendation application of PFKG was discussed in a case of five-star feet of office chair. PFKG can facilitate non-experts in identifying the complex structured relationship between process and flow, and recommend possible life cycle inventory data. It is envisioned that merging with KG will boost engineering sustainability in a more practical manner.}
}
@article{NWANOSIKE2022104679,
title = {Potential applications and performance of machine learning techniques and algorithms in clinical practice: A systematic review},
journal = {International Journal of Medical Informatics},
volume = {159},
pages = {104679},
year = {2022},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104679},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621003051},
author = {Ezekwesiri Michael Nwanosike and Barbara R Conway and Hamid A Merchant and Syed Shahzad Hasan},
keywords = {Machine learning, Clinical studies, Electronic health records (EHRs), Clinical practice, Model deployment, AUROC, Prediction, COVID-19},
abstract = {Purpose
The advent of clinically adapted machine learning algorithms can solve numerous problems ranging from disease diagnosis and prognosis to therapy recommendations. This systematic review examines the performance of machine learning (ML) algorithms and evaluates the progress made to date towards their implementation in clinical practice.
Methods
Systematic searching of databases (PubMed, MEDLINE, Scopus, Google Scholar, Cochrane Library and WHO Covid-19 database) to identify original articles published between January 2011 and October 2021. Studies reporting ML techniques in clinical practice involving humans and ML algorithms with a performance metric were considered.
Results
Of 873 unique articles identified, 36 studies were eligible for inclusion. The XGBoost (extreme gradient boosting) algorithm showed the highest potential for clinical applications (n = 7 studies); this was followed jointly by random forest algorithm, logistic regression, and the support vector machine, respectively (n = 5 studies). Prediction of outcomes (n = 33), in particular Inflammatory diseases (n = 7) received the most attention followed by cancer and neuropsychiatric disorders (n = 5 for each) and Covid-19 (n = 4). Thirty-three out of the thirty-six included studies passed more than 50% of the selected quality assessment criteria in the TRIPOD checklist. In contrast, none of the studies could achieve an ideal overall bias rating of ‘low’ based on the PROBAST checklist. In contrast, only three studies showed evidence of the deployment of ML algorithm(s) in clinical practice.
Conclusions
ML is potentially a reliable tool for clinical decision support. Although advocated widely in clinical practice, work is still in progress to validate clinically adapted ML algorithms. Improving quality standards, transparency, and interpretability of ML models will further lower the barriers to acceptability.}
}
@article{LIN202233,
title = {A GAN-based method for time-dependent cloud workload generation},
journal = {Journal of Parallel and Distributed Computing},
volume = {168},
pages = {33-44},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S074373152200123X},
author = {Weiwei Lin and Kun Yao and Lan Zeng and Fagui Liu and Chun Shan and Xiaobin Hong},
keywords = {Cloud computing, Time-dependent workload generation, Generative adversarial networks, Deep learning},
abstract = {To design repeatable and comparable resource management policies for data centers, researchers mainly conduct experiments in the simulation environment, which requires large-scale workload traces to simulate real scenes. However, issues related to data collection, security and privacy hinder the public availability of cloud workload datasets. Though workload generation is a promising solution, due to the unpredictable time dependency, cloud workloads are difficult to model. In light of this, we propose a novel end-to-end model for time-dependent cloud workload generation using Generative Adversarial Networks, which adopts improved Temporal Convolution Networks and Spectral Normalization to capture the time dependency and stabilize the adversarial training. Experimental results on real cloud datasets demonstrate that our model can efficiently generate realistic workloads that fulfill the diversity, fidelity and usefulness. Further, we also propose a conditional GAN which is trained with labeled data and can generate specific kind of workloads according to the input.}
}
@article{CARUCCIO20221,
title = {A decision-support framework for data anonymization with application to machine learning processes},
journal = {Information Sciences},
volume = {613},
pages = {1-32},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522010490},
author = {Loredana Caruccio and Domenico Desiato and Giuseppe Polese and Genoveffa Tortora and Nicola Zannone},
keywords = {Privacy preserving machine learning, k-anonymity, Relaxed functional dependencies, Generalization strategies},
abstract = {The application of machine learning techniques to large and distributed data archives might result in the disclosure of sensitive information about the data subjects. Data often contain sensitive identifiable information, and even if these are protected, the excessive processing capabilities of current machine learning techniques might facilitate the identification of individuals, raising privacy concerns. To this end, we propose a decision-support framework for data anonymization, which relies on a novel approach that exploits data correlations, expressed in terms of relaxed functional dependencies (rfds) to identify data anonymization strategies providing suitable trade-offs between privacy and data utility. Moreover, we investigate how to generate anonymization strategies that leverage multiple data correlations simultaneously to increase the utility of anonymized datasets. In addition, our framework provides support in the selection of the anonymization strategy to apply by enabling an understanding of the trade-offs between privacy and data utility offered by the obtained strategies. Experiments on real-life datasets show that our approach achieves promising results in terms of data utility while guaranteeing the desired privacy level, and it allows data owners to select anonymization strategies balancing their privacy and data utility requirements.}
}