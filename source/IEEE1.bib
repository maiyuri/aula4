@ARTICLE{8935096,
author={Li, Mingda and Wang, Hongzhi and Li, Jianzhong},
journal={Big Data Mining and Analytics},
title={Mining conditional functional dependency rules on big data},
year={2020},
volume={3},
number={1},
pages={68-84},
abstract={Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.},
keywords={Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality},
doi={10.26599/BDMA.2019.9020019},
ISSN={2096-0654},
month={March},}
@ARTICLE{9523499,
author={Li, Xiaohan and Yu, Bowen and Feng, Guanyu and Wang, Haojie and Chen, Wenguang},
journal={Big Data Mining and Analytics},
title={LotusSQL: SQL engine for high-performance big data systems},
year={2021},
volume={4},
number={4},
pages={252-265},
abstract={In recent years, Apache Spark has become the de facto standard for big data processing. SparkSQL is a module offering support for relational analysis on Spark with Structured Query Language (SQL). SparkSQL provides convenient data processing interfaces. Despite its efficient optimizer, SparkSQL still suffers from the inefficiency of Spark resulting from Java virtual machine and the unnecessary data serialization and deserialization. Adopting native languages such as C++ could help to avoid such bottlenecks. Benefiting from a bare-metal runtime environment and template usage, systems with C++ interfaces usually achieve superior performance. However, the complexity of native languages also increases the required programming and debugging efforts. In this work, we present LotusSQL, an engine to provide SQL support for dataset abstraction on a native backend Lotus. We employ a convenient SQL processing framework to deal with frontend jobs. Advanced query optimization technologies are added to improve the quality of execution plans. Above the storage design and user interface of the compute engine, LotusSQL implements a set of structured dataset operations with high efficiency and integrates them with the frontend. Evaluation results show that LotusSQL achieves a speedup of up to 9× in certain queries and outperforms Spark SQL in a standard query benchmark by more than 2× on average.},
keywords={Structured Query Language;Optimization;Engines;C++ languages;Sparks;Big Data;Query processing;big data;C++;Structured Query Language (SQL);query optimization},
doi={10.26599/BDMA.2021.9020009},
ISSN={2096-0654},
month={Dec},}
@ARTICLE{7299603,
author={Immonen, Anne and Pääkkönen, Pekka and Ovaska, Eila},
journal={IEEE Access},
title={Evaluating the Quality of Social Media Data in Big Data Architecture},
year={2015},
volume={3},
number={},
pages={2028-2043},
abstract={The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.},
keywords={Big data;Social network services;Computer architecture;Meta data;Online services;architecture;big data;metadata;quality attribute;quality of data;Architecture;big data;metadata;quality attribute;quality of data},
doi={10.1109/ACCESS.2015.2490723},
ISSN={2169-3536},
month={},}
@ARTICLE{8787229,
author={Kumar, Sunil and Singh, Maninder},
journal={Big Data Mining and Analytics},
title={A novel clustering technique for efficient clustering of big data in Hadoop Ecosystem},
year={2019},
volume={2},
number={4},
pages={240-247},
abstract={Big data analytics and data mining are techniques used to analyze data and to extract hidden information. Traditional approaches to analysis and extraction do not work well for big data because this data is complex and of very high volume. A major data mining technique known as data clustering groups the data into clusters and makes it easy to extract information from these clusters. However, existing clustering algorithms, such as k-means and hierarchical, are not efficient as the quality of the clusters they produce is compromised. Therefore, there is a need to design an efficient and highly scalable clustering algorithm. In this paper, we put forward a new clustering algorithm called hybrid clustering in order to overcome the disadvantages of existing clustering algorithms. We compare the new hybrid algorithm with existing algorithms on the bases of precision, recall, F-measure, execution time, and accuracy of results. From the experimental results, it is clear that the proposed hybrid clustering algorithm is more accurate, and has better precision, recall, and F-measure values.},
keywords={Clustering algorithms;Big Data;Ocean temperature;Data mining;Meteorology;Partitioning algorithms;Temperature control;clustering;Hadoop;big data;k-means;hierarchical},
doi={10.26599/BDMA.2018.9020037},
ISSN={2096-0654},
month={Dec},}
@ARTICLE{8782595,
author={Wang, Songyun and Yuan, Jiabin and Li, Xin and Qian, Zhuzhong and Arena, Fabio and You, Ilsun},
journal={IEEE Access},
title={Active Data Replica Recovery for Quality-Assurance Big Data Analysis in IC-IoT},
year={2019},
volume={7},
number={},
pages={106997-107005},
abstract={QoS-aware big data analysis is critical in Information-Centric Internet of Things (IC-IoT) system to support various applications like smart city, smart grid, smart health, intelligent transportation systems, and so on. The employment of non-volatile memory (NVM) in cloud or edge system provides good opportunity to improve quality of data analysis tasks. However, we have to face the data recovery problem led by NVM failure due to the limited write endurance. In this paper, we investigate the data recovery problem for QoS guarantee and system robustness, followed by proposing a rarity-aware data recovery algorithm. The core idea is to establish the rarity indicator to evaluate the replica distribution and service requirement comprehensively. With this idea, we give the lost replicas with distinguishing priority and eliminate the unnecessary replicas. Then, the data replicas are recovered stage by stage to guarantee QoS and provide system robustness. From our extensive experiments and simulations, it is shown that the proposed algorithm has significant performance improvement on QoS and robustness than the traditional direct data recovery method. Besides, the algorithm gives an acceptable data recovery time.},
keywords={Nonvolatile memory;Quality of service;Data analysis;Data centers;Bandwidth;Big Data;Robustness;Big data analysis;data recovery;IC-IoT;NVM;QoS improvement},
doi={10.1109/ACCESS.2019.2932259},
ISSN={2169-3536},
month={},}
@ARTICLE{8822937,
author={Li, Xin and Fan, Xiaoping and Qu, Xilong and Sun, Guang and Yang, Chen and Zuo, Biao and Liao, Zhifang},
journal={IEEE Access},
title={Curriculum Reform in Big Data Education at Applied Technical Colleges and Universities in China},
year={2019},
volume={7},
number={},
pages={125511-125521},
abstract={With the boom in data science, big data education has received increasing attention from all kinds of colleges and universities in China, and many of them are in a rush to offer big data education. This paper first analyzes the major areas of big data capability training and the Chinese market needs for various kinds of data science talent. Then, it discusses the curriculum design process for the “Data Science & Big Data Technology” bachelor's degree program, and summarizes some detailed approaches to improving teaching experiments. Finally, this paper proposes a graduating student profile for big data education at applied technical colleges and universities in China. The authors' main ideas include that, at the applied technical colleges and universities, a) a suitable graduating student orientation should be determined as the big data talent needs are hierarchical; b) the redesigned curriculum in big data education should provide students more practical capabilities and knowledge; c) the teaching of the existing mainstream big data technologies and tools should be significant components in the syllabi of big data education.},
keywords={Data science;Data visualization;Economics;Training;Big Data applications;Applied Technical Colleges and Universities;big data education;curriculum reform},
doi={10.1109/ACCESS.2019.2939196},
ISSN={2169-3536},
month={},}
@ARTICLE{8950481,
author={Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu, Liang},
journal={Chinese Journal of Electrical Engineering},
title={A missing power data filling method based on improved random forest algorithm},
year={2019},
volume={5},
number={4},
pages={33-39},
abstract={Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.},
keywords={Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data mining;Big data cleaning;missing data filling;data preprocessing;random forest;data quality},
doi={10.23919/CJEE.2019.000025},
ISSN={2096-1529},
month={Dec},}
@ARTICLE{8667300,
author={Lee, Doyoung},
journal={IEEE Access},
title={Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea},
year={2019},
volume={7},
number={},
pages={36294-36299},
abstract={In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.},
keywords={Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical measurement;Big data;data quality;data traceability;metrology;standard reference data;uncertainty},
doi={10.1109/ACCESS.2019.2904286},
ISSN={2169-3536},
month={},}
@ARTICLE{9399411,
author={Li, Ming and Zeng, Leilei and Zhao, Le and Yang, Renlin and An, Dezhi and Fan, Haiju},
journal={IEEE Access},
title={Blockchain-Watermarking for Compressive Sensed Images},
year={2021},
volume={9},
number={},
pages={56457-56467},
abstract={With the application of multimedia big data, the problems such as information leakage and data tampering have emerged. The security of images which is one of the most typical multimedia has become a major problem facing the large-scale open network environment. This paper proposed a blockchain-watermarking scheme to protect the privacy, integrity and availability of compressed sensed images, which effectively combines multimedia watermarking, compressed sensing, Interplanetary File System (IPFS) and blockchain technologies. Based on the reliable authentication of watermarking, the confidentiality protection of compressed sensing, the secure storage of IPFS, and the decentralization and non-tamperability of blockchain, the all-round security protection of the image big data based on compressive sensing can be realized. Experiments show that the proposed scheme is effective and feasible.},
keywords={Watermarking;Blockchain;Compressed sensing;Big Data;Image coding;Privacy;Peer-to-peer computing;Blockchain;IPFS;compressed sensing;watermarking;data hiding},
doi={10.1109/ACCESS.2021.3072196},
ISSN={2169-3536},
month={},}
@ARTICLE{9259196,
author={Zhai, Guanlin and Yang, Yan and Wang, Heng and Du, Shengdong},
journal={Big Data Mining and Analytics},
title={Multi-attention fusion modeling for sentiment analysis of educational big data},
year={2020},
volume={3},
number={4},
pages={311-319},
abstract={As an important branch of natural language processing, sentiment analysis has received increasing attention. In teaching evaluation, sentiment analysis can help educators discover the true feelings of students about the course in a timely manner and adjust the teaching plan accurately and timely to improve the quality of education and teaching. Aiming at the inefficiency and heavy workload of college curriculum evaluation methods, a Multi-Attention Fusion Modeling (Multi-AFM) is proposed, which integrates global attention and local attention through gating unit control to generate a reasonable contextual representation and achieve improved classification results. Experimental results show that the Multi-AFM model performs better than the existing methods in the application of education and other fields.},
keywords={Two dimensional displays;Task analysis;Sentiment analysis;Data models;Data mining;Context modeling;Analytical models;educational big data;sentiment analysis;aspect-level;attention},
doi={10.26599/BDMA.2020.9020024},
ISSN={2096-0654},
month={Dec},}
@ARTICLE{9344686,
author={Zhang, Airong and Lv, Na},
journal={IEEE Access},
title={Research on the Impact of Big Data Capabilities on Government’s Smart Service Performance: Empirical Evidence From China},
year={2021},
volume={9},
number={},
pages={50523-50537},
abstract={The government of China seeks to improve e-government service quality and build a service-oriented government that citizens find satisfactory. To this end, big data is being used as a new tool of government service innovation. However, there is a lack of research on how big data affects the performance of government smart services. This article explores the influence mechanisms of government big data capabilities on the performance of smart service provision, utilizing the carding analysis of relevant literature, published both in China and abroad. To this end, a structural equation model was constructed. Using data from 289 valid questionnaires in Jiangsu, Shandong, Zhejiang, and other provinces and cities in China, the study tests internal mechanisms of big data capabilities and its effect on smart service performance. Following a new definition of government big data capability, the paper divides the capability into three dimensions: big data system capability, big data human capability and big data management capability. The main conclusions are as follows: (1) Big data management capability has a significant positive impact on big data human capability and big data system capability. (2) Big data system capability has a significant positive impact on big data human capability. (3) Big data system capability and big data management capability have a significant positive effect on smart service performance. (4) The impact of big data human capability on smart service performance is not however significant enough to bring about the improvements which the government seeks.},
keywords={Big Data;Government;Decision making;Data models;Mathematical model;Technological innovation;Information technology;Big data system capabilities;big data human capabilities;big data management capabilities;smart service performance;structural equation model},
doi={10.1109/ACCESS.2021.3056486},
ISSN={2169-3536},
month={},}
@ARTICLE{8809689,
author={Fiore, Sandro and Elia, Donatello and Pires, Carlos Eduardo and Mestre, Demetrio Gomes and Cappiello, Cinzia and Vitali, Monica and Andrade, Nazareno and Braz, Tarciso and Lezzi, Daniele and Moraes, Regina and Basso, Tania and Kozievitch, Nádia P. and Fonseca, Keiko Verônica Ono and Antunes, Nuno and Vieira, Marco and Palazzo, Cosimo and Blanquer, Ignacio and Meira, Wagner and Aloisio, Giovanni},
journal={IEEE Access},
title={An Integrated Big and Fast Data Analytics Platform for Smart Urban Transportation Management},
year={2019},
volume={7},
number={},
pages={117652-117677},
abstract={Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.},
keywords={Urban areas;Big Data;Data analysis;Transportation;Cloud computing;Data mining;Europe;Big data;cloud computing;data analytics;data privacy;data quality;distributed environment;public transport management;smart city},
doi={10.1109/ACCESS.2019.2936941},
ISSN={2169-3536},
month={},}
@ARTICLE{9139506,
author={Zhou, Shengyao and He, Jie and Yang, Hui and Chen, Donghua and Zhang, Runtong},
journal={IEEE Access},
title={Big Data-Driven Abnormal Behavior Detection in Healthcare Based on Association Rules},
year={2020},
volume={8},
number={},
pages={129002-129011},
abstract={Healthcare insurance frauds are causing millions of dollars of public healthcare fund losses around the world in various ways, which makes it very important to strengthen the management of medical insurance in order to guarantee the steady operation of medical insurance funds. Healthcare fraud detection methods can reduce the losses of healthcare insurance funds and improve medical quality. Existing fraud detection studies mostly focus on finding normal behavior patterns and treat those violating normal behavior patterns as fraudsters. However, fraudsters can often disguise themselves with some normal behaviors, such as some consistent behaviors when they seek medical treatments. To address these issues, we combined a MapReduce distributed computing model and association rule mining to propose a medical cluster behavior detection algorithm based on frequent pattern mining. It can detect certain consistent behaviors of patients in medical treatment activities. By analyzing 1.5 million medical claim records, we have verified the effectiveness of the method. Experiments show that this method has better performance than several benchmark methods.},
keywords={Insurance;Medical diagnostic imaging;Big Data;Data mining;Data models;Medical treatment;Big data;abnormal behavior;healthcare insurance;association rules},
doi={10.1109/ACCESS.2020.3009006},
ISSN={2169-3536},
month={},}
@ARTICLE{9852477,
author={Hart, Philip and He, Lijun and Wang, Tianyi and Kumar, Vijay S. and Aggour, Kareem and Subramanian, Arun and Yan, Weizhong},
journal={IEEE Open Access Journal of Power and Energy},
title={Application of Big Data Analytics and Machine Learning to Large-Scale Synchrophasor Datasets: Evaluation of Dataset ‘Machine Learning-Readiness’},
year={2022},
volume={9},
number={},
pages={386-397},
abstract={This manuscript presents a data quality analysis and holistic ‘machine learning-readiness’ evaluation of a representative set of large-scale, real-world phasor measurement unit (PMU) datasets provided under the United States Department of Energy-funded FOA 1861 research program. A major focus of this study is to understand the present-day suitability of large-scale, real-world synchrophasor datasets for application of commercially-available, off-the-shelf big data and supervised or semi-supervised machine learning (ML) tools and catalogue any major obstacles to their application. To this end, dataset quality is methodically examined through an interconnect-wide quantifications of basic bad data occurrences, a summary of several harder-to-detect data quality issues that can jeopardize successful application of machine learning, and an evaluation of the adequacy of event log labeling for supervised training of models used for online event classification. A global ‘six-point’ statistical analyses of several key dataset variables is demonstrated as a means by which to identify additional hard-to-detect data quality issues, also providing an example successful application of big data technology to extract insights regarding reasonable operational bounds of the US power system. Obstacles for application of commercial ML technologies are summarized, with a particular focus on supervised and semi-supervised ML. Lessons-learned are provided regarding challenges associated with present-day event labeling practices, large spatial scope of the dataset, and dataset anonymization. Finally, insight into efficacy of employed mitigation strategies are discussed, and recommendations for future work are made.},
keywords={Phasor measurement units;Data integrity;Big Data;Power systems;Machine learning;Training;Statistical analysis;Phasor measurement units;synchrophasor datasets;statistical analysis;data quality analysis;label quality;nonymized datasets;supervised machine learning;big data;wide area monitoring system},
doi={10.1109/OAJPE.2022.3197553},
ISSN={2687-7910},
month={},}
@ARTICLE{9475115,
author={Huang, Yongming and Liu, Shengheng and Zhang, Cheng and You, Xiaohu and Wu, Hequan},
journal={Intelligent and Converged Networks},
title={True-data testbed for 5G/B5G intelligent network},
year={2021},
volume={2},
number={2},
pages={133-149},
abstract={Future beyond fifth-generation (B5G) and sixth-generation (6G) mobile communications will shift from facilitating interpersonal communications to supporting internet of everything (IoE), where intelligent communications with full integration of big data and artificial intelligence (AI) will play an important role in improving network efficiency and provi di ng hi gh-quality servi ce. As a rapi d evolvi ng paradi gm, the AI-empowered mob i le communi cati ons demand large amounts of data acquired from real network environment for systematic test and verification. Hence, we build the world's first true-data testbed for 5G/B5G intelligent network (TTIN), which comprises 5G/B5G on-site experimental networks, data acquisition & data warehouse, and AI engine & network optimization. In the TTIN, true network data acquisition, storage, standardization, and analysis are available, which enable system-level online verification of B5G/6G-orientated key technologies and support data-driven network optimization through the closed-loop control mechanism. This paper elaborates on the system architecture and module design of TTIN. Detailed technical specifications and some of the established use cases are also showcased.},
keywords={Artificial intelligence;5G mobile communication;Optimization;Wireless communication;Engines;Data acquisition;Radio access networks;true-data testbed;wireless communication networks;artificial intelligence (AI);big data;internet of everything (IoE)},
doi={10.23919/ICN.2021.0002},
ISSN={2708-6240},
month={June},}
@ARTICLE{8361574,
author={Meng, Qianyu and Wang, Kun and He, Xiaoming and Guo, Minyi},
journal={Big Data Mining and Analytics},
title={QoE-driven big data management in pervasive edge computing environment},
year={2018},
volume={1},
number={3},
pages={222-233},
abstract={In the age of big data, services in the pervasive edge environment are expected to offer end-users better Quality-of-Experience (QoE) than that in a normal edge environment. However, the combined impact of the storage, delivery, and sensors used in various types of edge devices in this environment is producing volumes of high-dimensional big data that are increasingly pervasive and redundant. Therefore, enhancing the QoE has become a major challenge in high-dimensional big data in the pervasive edge computing environment. In this paper, to achieve high QoE, we propose a QoE model for evaluating the qualities of services in the pervasive edge computing environment. The QoE is related to the accuracy of high-dimensional big data and the transmission rate of this accurate data. To realize high accuracy of high-dimensional big data and the transmission of accurate data through out the pervasive edge computing environment, in this study we focused on the following two aspects. First, we formulate the issue as a high-dimensional big data management problem and test different transmission rates to acquire the best QoE. Then, with respect to accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on deep learning, which is suitable for high-dimensional big data analysis in the pervasive edge computing environment. Our simulation results reveal that our proposed algorithm can achieve high QoE performance.},
keywords={Big Data;Quality of experience;Edge computing;Quality of service;Computational modeling;Training;Streaming media;Quality-of-Experience (QoE); high-dimensional big data management; deep learning; pervasive edge},
doi={10.26599/BDMA.2018.9020020},
ISSN={2096-0654},
month={Sep.},}
@ARTICLE{9096305,
author={Nazir, Shah and Khan, Sulaiman and Khan, Habib Ullah and Ali, Shaukat and García-Magariño, Iván and Atan, Rodziah Binti and Nawaz, Muhammad},
journal={IEEE Access},
title={A Comprehensive Analysis of Healthcare Big Data Management, Analytics and Scientific Programming},
year={2020},
volume={8},
number={},
pages={95714-95733},
abstract={Healthcare systems are transformed digitally with the help of medical technology, information systems, electronic medical records, wearable and smart devices, and handheld devices. The advancement in the medical big data, along with the availability of new computational models in the field of healthcare, has enabled the caretakers and researchers to extract relevant information and visualize the healthcare big data in a new spectrum. The role of medical big data becomes a challenging task in the form of storage, required information retrieval within a limited time, cost efficient solutions in terms care, and many others. Early decision making based healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Scientific programming play a significant role to overcome the existing issues and future problems involved in the management of large scale data in healthcare, such as by assisting in the processing of huge data volumes, complex system modelling, and sourcing derivations from healthcare data and simulations. Therefore, to address this problem efficiently a detailed study and analysis of the available literature work is required to facilitate the doctors and practitioners for making the decisions in identifying the disease and suggest treatment accordingly. The peer reviewed reputed journals are selected for the accumulated of published research work during the period ranges from 2015 - 2019 (a portion of 2020 is also included). A total of 127 relevant articles (conference papers, journal papers, book section, and survey papers) are selected for the assessment and analysis purposes. The proposed research work organizes and summarizes the existing published research work based on the research questions defined and keywords identified for the search process. This analysis on the existence research work will help the doctors and practitioners to make more authentic decisions, which ultimately will help to use the study as evidence for treating patients and suggest medicines accordingly.},
keywords={Big Data;Diseases;Data mining;Medical diagnostic imaging;Data models;Healthcare;big data;big data management;big data analytics},
doi={10.1109/ACCESS.2020.2995572},
ISSN={2169-3536},
month={},}
@ARTICLE{8805062,
author={Nazir, Shah and Nawaz Khan, Muhammad and Anwar, Sajid and Adnan, Awais and Asadi, Shahla and Shahzad, Sara and Ali, Shaukat},
journal={IEEE Access},
title={Big Data Visualization in Cardiology—A Systematic Review and Future Directions},
year={2019},
volume={7},
number={},
pages={115945-115958},
abstract={The digital transformations and use of healthcare information system, electronic medical records, wearable technology, and smart devices are increasing with the passage of time. A variety of sources of big data in healthcare are available, such as biometric data, registration data, electronic health record, medical imaging, patient reported data, biomarker data, clinical data, and administrative data. Visualization of data is a key tool for producing images, diagrams, or animations to convey messages from the viewed insight. The role of cardiology in healthcare is obvious for living and life. The function of heart is the control of blood supply to the entire parts of the body. Recent speedy growth in healthcare and the development of computation in the field of cardiology enable researchers and practitioners to mine and visualize new insights from patient data. The role of visualization is to capture the important information from the data and to visualize it for the easiness of doctors and practitioners. To help the doctors and practitioners, the proposed study presents a detailed report of the existing literature on visualization of data in the field of cardiology. This report will support the doctors and practitioners in decision-making process and to make it easier. This detailed study will eventually summarize the results of the existing literature published related to visualization of data in the cardiology. This research uses the systematic literature protocol and the data was collected from the studies published during the year 2009 to 2018 (10 years). The proposed study selected 53 primary studies from different repositories according to the defined exclusion, inclusion, and quality criteria. The proposed study focused mainly on the research work been done on visualization of big data in the field of cardiology, presented a summary of the techniques used for visualization of data in cardiology, and highlight the benefits of visualizations in cardiology. The current research summarizes and organizes the available literature in the form of published materials related to big data visualization in cardiology. The proposed research will help the researchers to view the available research studies on the subject of medical big data in cardiology and then can ultimately be used as evidence in future research. The results of the proposed research show that there is an increase in articles published yearly wise and several studies exist related to medical big data in cardiology. The derivations from the studies are presented in the paper.},
keywords={Data visualization;Cardiology;Big Data;Medical services;Systematics;Protocols;Libraries;Big data;medical big data;visualization;healthcare;cardiology;systematic literature review},
doi={10.1109/ACCESS.2019.2936133},
ISSN={2169-3536},
month={},}
@ARTICLE{9454431,
author={Adnan, Kiran and Akbar, Rehan and Wang, Khor Siak},
journal={IEEE Access},
title={Development of Usability Enhancement Model for Unstructured Big Data Using SLR},
year={2021},
volume={9},
number={},
pages={87391-87409},
abstract={Unstructured text contains valuable information for a range of enterprise applications and informed decision making. Text analytics is used to extract valuable insights from unstructured big data. Among the most significant challenges of text analytics, quality and usability are critical in affecting the outcome of the analytical process. The enhancement in usability is important for the exploitation of unstructured data. Most of the existing literature focuses on the usability of structured data as compared to unstructured data whereas big data usability has been discussed merely in the context of its assessment. The existing approaches do not provide proper guidelines on the usability enhancement of unstructured data. In this study, a rigorous systematic literature review using PRISMA framework has been conducted to develop a model enhancing the usability of unstructured data bridging the research gap. The recent approaches and solutions for text analytics have been investigated thoroughly. The usability issues of unstructured text data and their consequences on data preparation for analytics have been identified. Defining the usability dimensions for unstructured big data, identification of the usability determinants, and developing a relationship between usability dimension and determinants to derive usability rules are the significant contributions of this research and are integrated to formulate the usability enhancement model. The proposed model is the major outcome of the research. It contributes to make unstructured data usable and facilitates the data preparation activities with more valuable data that eventually improve the analytical process.},
keywords={Usability;Big Data;Data models;Data mining;Bibliographies;Protocols;Systematics;Big data;data transformation;data usability;text data;unstructured data;usability enhancement},
doi={10.1109/ACCESS.2021.3089100},
ISSN={2169-3536},
month={},}
@ARTICLE{9162126,
author={Expósito, Roberto R. and Galego-Torreiro, Roi and González-Domínguez, Jorge},
journal={IEEE Access},
title={SeQual: Big Data Tool to Perform Quality Control and Data Preprocessing of Large NGS Datasets},
year={2020},
volume={8},
number={},
pages={146075-146084},
abstract={This paper presents SeQual, a scalable tool to efficiently perform quality control of large genomic datasets. Our tool currently supports more than 30 different operations (e.g., filtering, trimming, formatting) that can be applied to DNA/RNA reads in FASTQ/FASTA formats to improve subsequent downstream analyses, while providing a simple and user-friendly graphical interface for non-expert users. Furthermore, SeQual takes full advantage of Big Data technologies to process massive datasets on distributed-memory systems such as clusters by relying on the open-source Apache Spark cluster computing framework. Our scalable Spark-based implementation allows to reduce the runtime from more than three hours to less than 20 minutes when processing a paired-end dataset with 251 million reads per input file on an 8-node multi-core cluster.},
keywords={Tools;Quality control;Big Data;Bioinformatics;Sparks;Sequential analysis;Acceleration;Big data;next-generation sequencing (NGS);bioinformatics;quality control;apache spark},
doi={10.1109/ACCESS.2020.3015016},
ISSN={2169-3536},
month={},}
@ARTICLE{9126812,
author={Mehmood, Erum and Anees, Tayyaba},
journal={IEEE Access},
title={Challenges and Solutions for Processing Real-Time Big Data Stream: A Systematic Literature Review},
year={2020},
volume={8},
number={},
pages={119123-119143},
abstract={Contribution: Recently, real-time data warehousing (DWH) and big data streaming have become ubiquitous due to the fact that a number of business organizations are gearing up to gain competitive advantage. The capability of organizing big data in efficient manner to reach a business decision empowers data warehousing in terms of real-time stream processing. A systematic literature review for real-time stream processing systems is presented in this paper which rigorously look at the recent developments and challenges of real-time stream processing systems and can serve as a guide for the implementation of real-time stream processing framework for all shapes of data streams. Background: Published surveys and reviews either cover papers focusing on stream analysis in applications other than real-time DWH or focusing on extraction, transformation, loading (ETL) challenges for traditional DWH. This systematic review attempts to answer four specific research questions. Research Questions: 1)Which are the relevant publication channels for real-time stream processing research? 2) Which challenges have been faced during implementation of real-time stream processing? 3) Which approaches/tools have been reported to address challenges introduced at ETL stage while processing real-time stream for real-time DWH? 4) What evidence have been reported while addressing different challenges for processing real-time stream? Methodology: A systematic literature was conducted to compile studies related to publication channels targeting real-time stream processing/joins challenges and developments. Following a formal protocol, semi-automatic and manual searches were performed for work from 2011 to 2020 excluding research in traditional data warehousing. Of 679,547 papers selected for data extraction, 74 were retained after quality assessment. Findings: This systematic literature highlights implementation challenges along with developed approaches for real-time DWH and big data stream processing systems and provides their comparisons. This study found that there exists various algorithms for implementing real-time join processing at ETL stage for structured data whereas less work for un-structured data is found in this subject matter.},
keywords={Real-time systems;Big Data;Libraries;Systematics;Data mining;Bibliographies;Quality assessment;Real-time stream processing;big data streaming;structured/un-structured data;ETL;systematic literature review},
doi={10.1109/ACCESS.2020.3005268},
ISSN={2169-3536},
month={},}
@ARTICLE{8416666,
author={Choi, Hyunjin and Gim, Jangwon and Seo, Young-Duk and Baik, Doo-Kwon},
journal={IEEE Access},
title={VPL-Based Big Data Analysis System: UDAS},
year={2018},
volume={6},
number={},
pages={40883-40897},
abstract={Over the past five years, research on big data analysis has been actively conducted, and many services have been developed to find valuable data. However, low quality of raw data and data loss problem during data analysis make it difficult to perform accurate data analysis. With the enormous generation of both unstructured and structured data, refinement of data is becoming increasingly difficult. As a result, data refinement plays an important role in data analysis. In addition, as part of efforts to ensure research reproducibility, the importance of reuse of researcher data and research methods is increasing; however, the research on systems supporting such roles has not been conducted sufficiently. Therefore, in this paper, we propose a big data analysis system named the unified data analytics suite (UDAS) that focuses on data refinement. UDAS performs data refinement based on the big data platform and ensures the reusability and reproducibility of refinement and analysis through the visual programming language interface. It also recommends open source and visualization libraries to users for statistical analysis. The qualitative evaluation of UDAS using the functional evaluation factor of the big data analysis platform demonstrated that the average satisfaction of the users is significantly high.},
keywords={Data analysis;Tools;Data visualization;Big Data;Statistical analysis;Synthetic aperture sonar;Data analysis;data visualization;reproducibility of results;clouds;data refinement;R},
doi={10.1109/ACCESS.2018.2857845},
ISSN={2169-3536},
month={},}
@ARTICLE{7587347,
author={Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D. and Venugopalan, Janani and Hoffman, Ryan and Wang, May D.},
journal={IEEE Transactions on Biomedical Engineering},
title={–Omic and Electronic Health Record Big Data Analytics for Precision Medicine},
year={2017},
volume={64},
number={2},
pages={263-273},
abstract={Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of -omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present -omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Results: To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating -omic information into EHR. Conclusion: Big data analytics is able to address -omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of -omic and EHR data to improve healthcare outcome. It has long lasting societal impact.},
keywords={Big data;Bioinformatics;Genomics;Data mining;Medical services;DNA;Feature extraction;Big data analytics;bioinformatics;electronic health records (EHRs);health informatics;–omic data;precision medicine},
doi={10.1109/TBME.2016.2573285},
ISSN={1558-2531},
month={Feb},}
@ARTICLE{9496639,
author={Khan, Abudul Wahid and Khan, Maseeh Ullah and Khan, Javed Ali and Ahmad, Arshad and Khan, Khalil and Zamir, Muhammad and Kim, Wonjoon and Ijaz, Muhammad Fazal},
journal={IEEE Access},
title={Analyzing and Evaluating Critical Challenges and Practices for Software Vendor Organizations to Secure Big Data on Cloud Computing: An AHP-Based Systematic Approach},
year={2021},
volume={9},
number={},
pages={107309-107332},
abstract={Recently, its becomes easy to track down the data due to its availability in a large number. Although for data management, processing, and obtainability, cloud computing is considered a well-known approach for organizational development on the internet. Despite many advantages, cloud computing has still numerous security challenges that can affect the big-data usage on cloud computing. To find the security issues/challenges that are faced by software vendors’ organizations we conducted a systematic literature review (SLR) through which we have find out 103 relevant research publications by developing a search string that is inspired by the research questions. This relevant data was comprised from different databases e.g. Google Scholar, IEEE Explore, ScienceDirect, ACM Digital Library, and SpringerLink. Furthermore, for the detailed literature review, we have accomplished all the steps in SLR, for example, development of SLR protocol, Initials and final assortment of the relevant data, data extraction, data quality assessment, and data synthesis. We identified fifteen (15) critical security challenges which are: data secrecy, geographical data location, unauthorized data access, lack of control, lack of data management, network-level issues, data integrity, data recovery, lack of trust, data sharing, data availability, asset issues, legal amenabilities, lack of quality, and lack of consistency. Furthermore, sixty four (64) standard practices are identified for these critical security challenges using the proposed SLR that could help vendor organizations to overcome the security challenges for big data. The findings of our research study demonstrate the resemblances and divergences in the identified security challenges in different periods, continents, databases, and methods. The proposed SLR will also support software vendor organizations for securing big data on the cloud computing platforms. This paper has the following content: in Section II, we have describe the Literature review; in Section III, research methodology is specified; in Section IV, the findings of the SLR and the analysis of result are discussed; in Section V, the limitations of this research are given; in Section VI, we discussed our conclusions and future work.},
keywords={Cloud computing;Security;Big Data;Software;Organizations;Social networking (online);STEM;Security challenges;big data;cloud computing;SLR;vendor;SPSS},
doi={10.1109/ACCESS.2021.3100287},
ISSN={2169-3536},
month={},}
@ARTICLE{8537879,
author={Sakr, Sherif and Maamar, Zakaria and Awad, Ahmed and Benatallah, Boualem and Van Der Aalst, Wil M. P.},
journal={IEEE Access},
title={Business Process Analytics and Big Data Systems: A Roadmap to Bridge the Gap},
year={2018},
volume={6},
number={},
pages={77308-77320},
abstract={Business processes represent a cornerstone to the operation of any enterprise. They are the operational means for such organizations to fulfill their goals. Nowadays, enterprises are able to gather massive amounts of event data. These are generated as business processes are executed and stored in transaction logs, databases, e-mail correspondences, free form text on (enterprise) social media, and so on. Taping into these data, enterprises would like to weave data analytic techniques into their decision making capabilities. In recent years, the IT industry has witnessed significant advancements in the domain of Big Data analytics. Unfortunately, the business process management (BPM) community has not kept up to speed with such developments and often rely merely on traditional modeling-based approaches. New ways of effectively exploiting such data are not sufficiently used. In this paper, we advocate that a good understanding of the business process and Big Data worlds can play an effective role in improving the efficiency and the quality of various data-intensive business operations using a wide spectrum of emerging Big Data systems. Moreover, we coin the term process footprint as a wider notion of process data than that is currently perceived in the BPM community. A roadmap towards taking business process data intensive operations to the next level is shaped in this paper.},
keywords={Big Data;Task analysis;Data models;Organizations;Databases;Data mining;Business process analytics;Big Data systems;process data-intensive operations},
doi={10.1109/ACCESS.2018.2881759},
ISSN={2169-3536},
month={},}
@ARTICLE{8715359,
author={Ramzan, Shabana and Bajwa, Imran Sarwar and Ramzan, Bushra and Anwar, Waheed},
journal={IEEE Access},
title={Intelligent Data Engineering for Migration to NoSQL Based Secure Environments},
year={2019},
volume={7},
number={},
pages={69042-69057},
abstract={In an era of super computing, data is increasing exponentially requiring more proficiency from the available technologies of data storage, data processing, and analysis. Such continuous massive growth of structured and unstructured data is referred to as a “Big data”. The processing and storage of big data through a conventional technique is not possible. Due to improved proficiency of Big Data solution in handling data, such as NoSQL caused the developers in the previous decade to start preferring big data databases, such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology that is designed to provide scalability to support voluminous data, leading to the rise of NoSQL as the most viable database solution. These modern databases aim to overcome the limitations of relational databases such as unlimited scalability, high performance, data modeling, data distribution, and continuous availability. These days, the larger enterprises need to shift NoSQL databases due to their more flexible models. It is a great challenge for business organizations and enterprises to transform their existing databases to NoSQL databases considering heterogeneity and complexity in relational data. In addition, with the emergence of big data, data cleansing has become a great challenge. In this paper, we proposed an approach that has two modules: data transformation and data cleansing module. The first phase is the transformation of a relational database to Oracle NoSQL database through model transformation. The second phase provides data cleansing ability to improve data quality and prepare it for big data analytics. The experiments show the proposed approach successfully transforms the relational database to a big data database and improve data quality.},
keywords={Big Data;NoSQL databases;Transforms;Scalability;Servers;Tools;Relational databases;NoSQL;big data;data cleansing},
doi={10.1109/ACCESS.2019.2916912},
ISSN={2169-3536},
month={},}
@ARTICLE{9235580,
author={Sun, Maohua and Li, Yuangang},
journal={IEEE Access},
title={Eco-Environment Construction of English Teaching Using Artificial Intelligence Under Big Data Environment},
year={2020},
volume={8},
number={},
pages={193955-193965},
abstract={Application of big data and artificial intelligence has become one influence factor of English teaching, which have broken the balance of the teaching Eco-environment for English. In this article, the artificial intelligence and big data are introduced into English teaching to propose a new teaching Eco-environment construction method to meet the needs of the social development and international communication in English. In the proposed method, the characteristics of English teaching under big data environment are analyzed in detail. Then the big data technology is used to construct a new Eco-environment of English teaching to improve the teaching and learning quality. The data mining method is one of artificial intelligence methods, which is used to analyze the relationship of interdependence and mutual restriction among various factors in English teaching in order to build and implement a new Eco-environment with the information sharing, quality teaching and personalized learning of English. Finally, through the practical application of the constructed Eco-environment, the experiment results show that the proposed method can help students update their learning concepts, methods and contents of English, inspire their interest and initiative by comparing with some existed teaching methods, so as to improve their learning effects and application ability of English. Therefore, the constructed Eco-environment provides a new idea and direction for English teaching reform by application of big data and artificial intelligence.},
keywords={Education;Big Data;Data mining;Information technology;Symbiosis;Learning (artificial intelligence);Eco-environment;English teaching;big data;data mining;influence factor;comprehensive ability},
doi={10.1109/ACCESS.2020.3033068},
ISSN={2169-3536},
month={},}
@ARTICLE{8890933,
author={Yan, Yan and Zhang, Lianxiu and Sheng, Quan Z. and Wang, Bingqian and Gao, Xin and Cong, Yiming},
journal={IEEE Access},
title={Dynamic Release of Big Location Data Based on Adaptive Sampling and Differential Privacy},
year={2019},
volume={7},
number={},
pages={164962-164974},
abstract={Data releasing is a key part bridging between the collection of big data and their applications. Traditional methods release the static version of dataset or publish the snapshot with a fixed sampling interval, which cannot meet the dynamic query requirements and query precision for big data. Moreover, the quality of published data cannot reflect the characteristics of the dynamic changes of big data, which often leads to subsequent data analysis and mining errors. This paper proposes an adaptive sampling mechanism and privacy protection method for the release of big location data. In order to reflect the dynamic change of data in time, we design an adaptive sampling mechanism based on the proportional-integral-derivative (PID) controller according to the temporal and spatial correlation of the location data. To ensure the privacy of published data, we propose a heuristic quad-tree partitioning method as well as a corresponding privacy budget allocation strategy. Experiments and analysis prove that the adaptive sampling mechanism proposed in this paper can effectively track the trend of dynamic changes of data, and the designed differential privacy method can improve the accuracy of counting query and enhance the availability of published data under the premise of certain privacy intensity. The proposed methods can also be readily extended to other areas of big data release applications.},
keywords={Differential privacy;Publishing;Big Data;Data models;Vehicle dynamics;Big location data;privacy preserving data publishing;adaptive sampling;differential privacy;heuristic quad-tree partitioning},
doi={10.1109/ACCESS.2019.2951364},
ISSN={2169-3536},
month={},}
@ARTICLE{9409047,
author={Yahia, Nesrine Ben and Hlel, Jihen and Colomo-Palacios, Ricardo},
journal={IEEE Access},
title={From Big Data to Deep Data to Support People Analytics for Employee Attrition Prediction},
year={2021},
volume={9},
number={},
pages={60447-60458},
abstract={In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.},
keywords={Big Data;Organizations;Radio frequency;Predictive models;Support vector machines;Data models;Analytical models;Deep people analytics;employee attrition;retention;prediction;interpretation;policies recommendation},
doi={10.1109/ACCESS.2021.3074559},
ISSN={2169-3536},
month={},}
@ARTICLE{8641478,
author={Zhang, Meifan and Wang, Hongzhi and Li, Jianzhong and Gao, Hong},
journal={IEEE Access},
title={One-Pass Inconsistency Detection Algorithms for Big Data},
year={2019},
volume={7},
number={},
pages={22377-22394},
abstract={Data in the real world is often dirty. Inconsistency is an important kind of dirty data; before repairing inconsistency, we need to detect them first. The time complexities of the current inconsistency detection algorithms are super-linear to the size of data and not suitable for the big data. For the inconsistency detection of big data, we develop an algorithm that detects inconsistency within the one-pass scan of the data according to both the functional dependency (FD) and the conditional functional dependency (CFD) in our previous work. In this paper, we propose inconsistency detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are more expressive than FDs and CFDs. Developing the algorithm to detect the violation of DCs increases the applicability of our inconsistency detection algorithms. We compare the performance of our algorithm with the performance of implementing SQL queries in MySQL and BigQuery. The experimental results indicate the high efficiency of our algorithms.},
keywords={Big Data;Data integrity;Detection algorithms;Databases;Time complexity;Hazards;Business;Inconsistency detection;big data;one-pass algorithm;data quality;denial constraint},
doi={10.1109/ACCESS.2019.2898707},
ISSN={2169-3536},
month={},}
@ARTICLE{8561268,
author={Huang, Yuan and Zhao, Qiang and Zhou, Qianyu and Jiang, Wanchang},
journal={IEEE Access},
title={Air Quality Forecast Monitoring and Its Impact on Brain Health Based on Big Data and the Internet of Things},
year={2018},
volume={6},
number={},
pages={78678-78688},
abstract={Brain health quality pre-monitoring has become an urgent need, and this is a system of complex engineering. From the perspective of intelligent decision-making based on big data, the intelligent air index prediction is introduced, the popular classification algorithm is introduced, the hidden information of historical data is mined, and the brain health quality prediction is realized. The brain health quality monitoring system based on the Internet of Things is constructed, and the classification algorithm is used to realize real-time acquisition, intelligent processing of data. In order to improve the data processing speed and enhance the real-time performance of brain health quality prediction, this paper introduces cloud computing technology to accelerate data processing. In order to enable users to understand the air index, anytime and anywhere, it is also designed based on the problem of large historical data of air index and real-time data collection. The Android platform develops an air index forecast client.},
keywords={Monitoring;Big Data;Forecasting;Indexes;Cloud computing;Real-time systems;Brain modeling;Brain health quality;intelligent forecasting;air quality forecast monitoring;big data;Internet of Things},
doi={10.1109/ACCESS.2018.2885142},
ISSN={2169-3536},
month={},}
@ARTICLE{8412190,
author={El Kassabi, Hadeel T. and Serhani, Mohamed Adel and Dssouli, Rachida and Benatallah, Boualem},
journal={IEEE Access},
title={A Multi-Dimensional Trust Model for Processing Big Data Over Competing Clouds},
year={2018},
volume={6},
number={},
pages={39989-40007},
abstract={Cloud computing has emerged as a powerful paradigm for delivering data-intensive services over the Internet. Cloud computing has enabled the implementation and success of big data, a recent phenomenon handling huge data being generated from different sources. Competing clouds have made it challenging to select a cloud provider that guarantees quality of cloud service (QoCS). Also, cloud providers' claims of guaranteeing QoCS are exaggerated for marketing purposes; hence, they cannot often be trusted. Therefore, a comprehensive trust model is necessary to evaluate the QoCS prior to making any selection decision. In this paper, we propose a multi-dimensional trust model for big data workflow processing over different clouds. It evaluates the trustworthiness of cloud providers based on: the most up-to-date cloud resource capabilities, the reputation evidence measured by neighboring users, and a recorded personal history of experiences with the cloud provider. The ultimate goal is to ensure an efficient selection of trustworthiness cloud provider who eventually will guarantee high QoCS and fulfills key big data workflow requirements. Various experiments were conducted to validate our proposed model. The results show that our model captures the different components of trust, ensures high QoCS, and effectively adapts to the dynamic nature of the cloud.},
keywords={Cloud computing;Big Data;Computational modeling;Data models;Adaptation models;Task analysis;Big data;big data processing;cloud computing;cloud selection;trust model;quality of cloud services;service evaluation;community},
doi={10.1109/ACCESS.2018.2856623},
ISSN={2169-3536},
month={},}
@ARTICLE{8012376,
author={Xu, Xiaoya and Hua, Qingsong},
journal={IEEE Access},
title={Industrial Big Data Analysis in Smart Factory: Current Status and Research Strategies},
year={2017},
volume={5},
number={},
pages={17543-17551},
abstract={Under the background of cyber-physical systems and Industry 4.0, intelligent manufacturing has become an orientation and produced a revolutionary change. Compared with the traditional manufacturing environments, the intelligent manufacturing has the characteristics as highly correlated, deep integration, dynamic integration, and huge volume of data. Accordingly, it still faces various challenges. In this paper, we summarize and analyze the current research status in both domestic and aboard, including industrial big data collection, modeling of the intelligent product lines based on ontology, the predictive diagnosis based on industrial big data, group learning of product line equipment and the product line reconfiguration of intelligent manufacturing. Based on the research status and the problems, we propose the research strategies, including acquisition schemes of industrial big data under the environment of intelligent, ontology modeling and deduction method based intelligent product lines, predictive diagnostic methods on production lines based on deep neural network, deep learning among devices based on cloud supplements and 3-D selforganized reconfiguration mechanism based on the supplements of cloud. In our view, this paper will accelerate the implementation of smart factory.},
keywords={Manufacturing;Big Data;Ontologies;Production facilities;Cloud computing;Data models;Machine learning;Industrial big data;smart factory;data analysis;cyber-physical systems},
doi={10.1109/ACCESS.2017.2741105},
ISSN={2169-3536},
month={},}
@ARTICLE{8840830,
author={Nazir, Shah and Nawaz, Muhammad and Adnan, Awais and Shahzad, Sara and Asadi, Shahla},
journal={IEEE Access},
title={Big Data Features, Applications, and Analytics in Cardiology—A Systematic Literature Review},
year={2019},
volume={7},
number={},
pages={143742-143771},
abstract={In today's digital world the information surges with the widespread use of the internet and global communication systems. Healthcare systems are also facing digital transformations with the enhancement in the utilization of healthcare information systems, electronic records in medical, wearable, smart devices, handheld devices, and so on. A bulk of data is produced from these digital transformations. The recent increase in medical big data and the development of computational techniques in the field of cardiology enables researchers and practitioners to extract and visualize medical big data in a new spectrum. The role of medical big data in cardiology becomes a challenging task. Early decision making in cardiac healthcare system has massive potential for dropping the cost of care, refining quality of care, and reducing waste and error. Therefore, to facilitate this process a detailed report of the existing literature will be feasible to help the doctors and practitioners in decision making for the purpose of identifying and treating cardiac diseases. This detailed study will summarize results from the existing literature on big data in the field cardiac disease. This research uses the systematic literature protocol as presented by Kitchenham et al. The data was collected from the published materials from 2008 to 2018 as conference or journal publications, books, magazines and other online sources. 190 papers were included relying on the defined inclusion, exclusion, and checking the quality criteria. The current study helped to identify medical big data features, the application of medical big data, and the analytics of the big data in cardiology. The results of the proposed research shows that several studies exist that are associated to medical big data specifically to cardiology. This research summarizes and organizes the existing literature based on the defined keywords and research questions. The analysis will help doctors to make more authentic decisions, which ultimately will help to use the study as evidence for treating patients with heart related diseases.},
keywords={Big Data;Cardiology;Diseases;Protocols;Data visualization;Bibliographies;Big data;big data features;analytics in cardiology;healthcare;systematic literature review (SLR)},
doi={10.1109/ACCESS.2019.2941898},
ISSN={2169-3536},
month={},}
@ARTICLE{8946609,
author={Sarabia-Jácome, David and Palau, Carlos E. and Esteve, Manuel and Boronat, Fernando},
journal={IEEE Access},
title={Seaport Data Space for Improving Logistic Maritime Operations},
year={2020},
volume={8},
number={},
pages={4372-4382},
abstract={The maritime industry expects several improvements to efficiently manage the operation processes by introducing Industry 4.0 enabling technologies. Seaports are the most critical point in the maritime logistics chain because of its multimodal and complex nature. Consequently, coordinated communication among any seaport stakeholders is vital to improving their operations. Currently, Electronic Data Interchange (EDI) and Port Community Systems (PCS), as primary enablers of digital seaports, have demonstrated their limitations to interchange information on time, accurately, efficiently, and securely, causing high operation costs, low resource management, and low performance. For these reasons, this contribution presents the Seaport Data Space (SDS) based on the Industrial Data Space (IDS) reference architecture model to enable a secure data sharing space and promote an intelligent transport multimodal terminal. Each seaport stakeholders implements the IDS connector to take part in the SDS and share their data. On top of SDS, a Big Data architecture is integrated to manage the massive data shared in the SDS and extract useful information to improve the decision-making. The architecture has been evaluated by enabling a port authority and a container terminal to share its data with a shipping company. As a result, several Key Performance Indicators (KPIs) have been developed by using the Big Data architecture functionalities. The KPIs have been shown in a dashboard to allow easy interpretability of results for planning vessel operations. The SDS environment may improve the communication between stakeholders by reducing the transaction costs, enhancing the quality of information, and exhibiting effectiveness.},
keywords={Seaports;Big Data;Industries;Stakeholders;Logistics;Data models;Data mining;Analytics;big data;industry 4.0;industrial data spaces;Internet of Things;maritime;seaport;intelligent transport},
doi={10.1109/ACCESS.2019.2963283},
ISSN={2169-3536},
month={},}
@ARTICLE{8766970,
author={Aggarwal, Apeksha and Toshniwal, Durga},
journal={IEEE Access},
title={Frequent Pattern Mining on Time and Location Aware Air Quality Data},
year={2019},
volume={7},
number={},
pages={98921-98933},
abstract={With the advent of big data era, enormous volumes of data are generated every second. Varied data processing algorithms and architectures have been proposed in the past to achieve better execution of data mining algorithms. One such algorithm is extracting most frequently occurring patterns from the transactional database. Dependency of transactions on time and location further makes frequent itemset mining task more complex. The present work targets to identify and extract the frequent patterns from such time and location-aware transactional data. Primarily, the spatio-temporal dependency of air quality data is leveraged to find out frequently co-occurring pollutants over several locations of Delhi, the capital city of India. Varied approaches have been proposed in the past to extract frequent patterns efficiently, but this work suggests a generalized approach that can be applied to any numeric spatio-temporal transactional data, including air quality data. Furthermore, a comprehensive description of the algorithm along with a sample running example on air quality dataset is shown in this work. A detailed experimental evaluation is carried out on the synthetically generated datasets, benchmark datasets, and real world datasets. Furthermore, a comparison with spatio-temporal apriori as well as the other state-of-the-art non-apriori-based algorithms is shown. Results suggest that the proposed algorithm outperformed the existing approaches in terms of execution time of algorithm and memory resources.},
keywords={Data mining;Itemsets;Air quality;Data structures;Urban areas;Location awareness;Air quality;data mining;frequent;itemset;spatio-temporal},
doi={10.1109/ACCESS.2019.2930004},
ISSN={2169-3536},
month={},}
@ARTICLE{8993712,
author={Ullah, Hidayat and Wan, Wanggen and Haidery, Saqib Ali and Khan, Naimat Ullah and Ebrahimpour, Zeinab and Muzahid, A. A. M.},
journal={IEEE Access},
title={Spatiotemporal Patterns of Visitors in Urban Green Parks by Mining Social Media Big Data Based Upon WHO Reports},
year={2020},
volume={8},
number={},
pages={39197-39211},
abstract={Green parks in urban areas are believed to enhance the well-being of residents. The importance of green spaces to support health and fitness in urban areas has recently regained interest. Reports released in 2010-2016 by the World Health Organization (WHO) on urban planning, environment, and health stated that green spaces can have a positive impact on physical activity, social and mental well-being, enhance air quality and decrease noise exposure. We analyzed the number of check-ins in various parks of Shanghai by utilizing geotagged social media network check-in data. This article presents a descriptive study using social media data by obtaining the three-year comparison of spatial and temporal patterns of park visits to raise public awareness that green parks provide a healthy environment that can be beneficial for the well-being of urban citizens. We investigated the visitor spatiotemporal behavior in more than 115 green parks in 10 districts of Shanghai with approximately 250,000 check-ins. We examined 3 years of geotagged data and our main findings are: (i) the spatial and temporal variations of users in urban green parks (ii) the gender differences in space and time with relation to urban green parks. The main objective of this article is to present evident data for policymakers on the advantages of providing green spaces access to urban citizens and to facilitate cities with systematic approaches to provide green space access to improve the health of urban citizens.},
keywords={Green products;Urban areas;Social networking (online);Spatiotemporal phenomena;Data mining;Big Data;Air quality;Urban green parks;big data;social networks;spatiotemporal;KDE;data mining},
doi={10.1109/ACCESS.2020.2973177},
ISSN={2169-3536},
month={},}
@ARTICLE{8913543,
author={Li, Xiang and Zhang, Zijia},
journal={IEEE Access},
title={Research and Analysis for Real-Time Streaming Big Data Based on Controllable Clustering and Edge Computing Algorithm},
year={2019},
volume={7},
number={},
pages={171621-171632},
abstract={Aiming at the low efficiency, poor performance and weak stability of traditional clustering algorithms and the poor response to the processing of massive data in real time, a real-time streaming controllable clustering edge computing algorithm (SCCEC) is proposed. First, the data tuples that arrive in real time are pre-processed by coarse clustering, the number of clusters, and the position of the center point are determined, and a set formed by macro clusters having differences is formed. Secondly, the macro cluster set obtained by the coarse clustering is sampled, and then K-means parallel clustering is performed with the largest and smallest distances, thereby realizing fine clustering of data. Finally, the completely clustering algorithm and the edge-computing algorithm are combined to realize the clustering analysis under the edge-computing framework. The experimental results show that the proposed algorithm has the advantages of high efficiency, good quality, and strong stability. It can quickly obtain the global optimal solution, and deal with massive data with high real-time performance. It can be used for real-time streaming data aggregation under big data background.},
keywords={Clustering algorithms;Edge computing;Real-time systems;Big Data;Cloud computing;Internet of Things;Classification algorithms;Real-time streaming data;clustering;edge computing;algorithm},
doi={10.1109/ACCESS.2019.2955992},
ISSN={2169-3536},
month={},}
@ARTICLE{8664564,
author={Oh, Hyeontaek and Park, Sangdon and Lee, Gyu Myoung and Heo, Hwanjo and Choi, Jun Kyun},
journal={IEEE Access},
title={Personal Data Trading Scheme for Data Brokers in IoT Data Marketplaces},
year={2019},
volume={7},
number={},
pages={40120-40132},
abstract={With the widespread use of the Internet of Things, data-driven services take the lead of both online and off-line businesses. Especially, personal data draw heavy attention of service providers because of the usefulness in value-added services. With the emerging big-data technology, a data broker appears, which exploits and sells personal data about individuals to other third parties. Due to little transparency between providers and brokers/consumers, people think that the current ecosystem is not trustworthy, and new regulations with strengthening the rights of individuals were introduced. Therefore, people have an interest in their privacy valuation. In this sense, the willingness-to-sell (WTS) of providers becomes one of the important aspects for data brokers; however, conventional studies have mainly focused on the willingness-to-buy (WTB) of consumers. Therefore, this paper proposes an optimized trading model for data brokers who buy personal data with proper incentives based on the WTS, and they sell valuable information from the refined dataset by considering the WTB and the dataset quality. This paper shows that the proposed model has a global optimal point by the convex optimization technique and proposes a gradient ascent-based algorithm. Consequently, it shows that the proposed model is feasible even if the data brokers spend costs to gather personal data.},
keywords={Data models;Pricing;Data privacy;Sensors;Economics;Data integrity;Data brokers;profit maximization;willingness-to-buy;willingness-to-sell},
doi={10.1109/ACCESS.2019.2904248},
ISSN={2169-3536},
month={},}
@ARTICLE{9261414,
author={Duan, Gui-Jiang and Yan, Xin},
journal={IEEE Access},
title={A Real-Time Quality Control System Based on Manufacturing Process Data},
year={2020},
volume={8},
number={},
pages={208506-208517},
abstract={Quality prediction is one of the key links of quality control. Benefitting from the development of digital manufacturing, manufacturing process data have grown rapidly, which allows product quality predictions to be made based on a real-time manufacturing process. A real-time quality control system (RTQCS) based on manufacturing process data is presented in this paper. In this study, the relationship between the product real-time quality status and processing task process was established by analyzing the relationship between the product manufacturing resources and the quality status. The key quality characteristics of the product were identified by analyzing the similarity of the product quality characteristic variations in the manufacturing process based on the big data technology, and a quality-resource matrix was constructed. Based on the quality-resource matrix, the RTQCS was established by introducing an association-rule incremental-update algorithm. Finally, the RTQCS was applied in actual production, and the performance of RTQCS was verified by experiments. The experiments showed that the RTQCS can effectively guarantee the quality of product manufacturing and improve the manufacturing efficiency during production.},
keywords={Manufacturing processes;Production;Quality control;Product design;Real-time systems;Manufacturing;Quality assessment;Quality management;production control;prediction methods},
doi={10.1109/ACCESS.2020.3038394},
ISSN={2169-3536},
month={},}
@ARTICLE{7914196,
author={Ding, Xiaoou and Wang, Hongzhi and Gao, Yitong and Li, Jianzhong and Gao, Hong},
journal={Tsinghua Science and Technology},
title={Efficient currency determination algorithms for dynamic data},
year={2017},
volume={22},
number={3},
pages={227-242},
abstract={Data quality is an important aspect in data application and management, and currency is one of the major dimensions influencing its quality. In real applications, datasets timestamps are often incomplete and unavailable, or even absent. With the increasing requirements to update real-time data, existing methods can fail to adequately determine the currency of entities. In consideration of the velocity of big data, we propose a series of efficient algorithms for determining the currency of dynamic datasets, which we divide into two steps. In the preprocessing step, to better determine data currency and accelerate dataset updating, we propose the use of a topological graph of the processing order of the entity attributes. Then, we construct an Entity Query B-Tree (EQB-Tree) structure and an Entity Storage Dynamic Linked List (ES-DLL) to improve the querying and updating processes of both the data currency graph and currency scores. In the currency determination step, we propose definitions of the currency score and currency information for tuples referring to the same entity and use examples to discuss methods and algorithms for their computation. Based on our experimental results with both real and synthetic data, we verify that our methods can efficiently update data in the correct order of currency.},
keywords={Heuristic algorithms;Remuneration;Real-time systems;Databases;Big Data;data quality management; data currency; dynamic determining},
doi={10.23919/TST.2017.7914196},
ISSN={1007-0214},
month={June},}
@ARTICLE{9142149,
author={Yang, Shuhui and Yuan, Zimu and Li, Wei},
journal={Big Data Mining and Analytics},
title={Error data analytics on RSS range-based localization},
year={2020},
volume={3},
number={3},
pages={155-170},
abstract={The quality of measurement data is critical to the accuracy of both outdoor and indoor localization methods. Due to the inevitable measurement error, the analytics on the error data is critical to evaluate localization methods and to find the effective ones. For indoor localization, Received Signal Strength (RSS) is a convenient and low-cost measurement that has been adopted in many localization approaches. However, using RSS data for localization needs to solve a fundamental problem, that is, how accurate are these methods? The reason of the low accuracy of the current RSS-based localization methods is the oversimplified analysis on RSS measurement data. In this proposed work, we adopt a generalized measurement model to find optimal estimators whose estimated error is equal to the Cramér-Rao Lower Bound (CRLB). Through mathematical techniques, the key factors that affect the accuracy of RSS-based localization methods are revealed, and the analytics expression that discloses the proportional relationship between the localization accuracy and these factors is derived. The significance of our discovery has two folds: First, we present a general expression for localization error data analytics, which can explain and predict the accuracy of range-based localization algorithms; second, the further study on the general analytics expression and its minimum can be used to optimize current localization algorithms.},
keywords={Measurement errors;Measurement uncertainty;Data analysis;Loss measurement;Newton method;Covariance matrices;Current measurement;Cramér-Rao Lower Bound (CRLB);error data analytics;generalized least squares;Received Signal Strength (RSS)},
doi={10.26599/BDMA.2020.9020001},
ISSN={2096-0654},
month={Sep.},}
@ARTICLE{9389541,
author={Alfred, Rayner and Obit, Joe Henry and Chin, Christie Pei-Yee and Haviluddin, Haviluddin and Lim, Yuto},
journal={IEEE Access},
title={Towards Paddy Rice Smart Farming: A Review on Big Data, Machine Learning, and Rice Production Tasks},
year={2021},
volume={9},
number={},
pages={50358-50380},
abstract={Big Data (BD), Machine Learning (ML) and Internet of Things (IoT) are expected to have a large impact on Smart Farming and involve the whole supply chain, particularly for rice production. The increasing amount and variety of data captured and obtained by these emerging technologies in IoT offer the rice smart farming strategy new abilities to predict changes and identify opportunities. The quality of data collected from sensors greatly influences the performance of the modelling processes using ML algorithms. These three elements (e.g., BD, ML and IoT) have been used tremendously to improve all areas of rice production processes in agriculture, which transform traditional rice farming practices into a new era of rice smart farming or rice precision agriculture. In this paper, we perform a survey of the latest research on intelligent data processing technology applied in agriculture, particularly in rice production. We describe the data captured and elaborate role of machine learning algorithms in paddy rice smart agriculture, by analyzing the applications of machine learning in various scenarios, smart irrigation for paddy rice, predicting paddy rice yield estimation, monitoring paddy rice growth, monitoring paddy rice disease, assessing quality of paddy rice and paddy rice sample classification. This paper also presents a framework that maps the activities defined in rice smart farming, data used in data modelling and machine learning algorithms used for each activity defined in the production and post-production phases of paddy rice. Based on the proposed mapping framework, our conclusion is that an efficient and effective integration of all these three technologies is very crucial that transform traditional rice cultivation practices into a new perspective of intelligence in rice precision agriculture. Finally, this paper also summarizes all the challenges and technological trends towards the exploitation of multiple sources in the era of big data in agriculture.},
keywords={Agriculture;Digital agriculture;Machine learning;Machine learning algorithms;Market research;Big Data;Internet of Things;Rice production;big data analytics;Internet of Things;machine learning;smart farming;precision agriculture;agriculture supply chain},
doi={10.1109/ACCESS.2021.3069449},
ISSN={2169-3536},
month={},}
@ARTICLE{8824131,
author={Zhang, Caifeng and Ma, Rui and Sun, Shiwei and Li, Yujie and Wang, Yichuan and Yan, Zhijun},
journal={IEEE Access},
title={Optimizing the Electronic Health Records Through Big Data Analytics: A Knowledge-Based View},
year={2019},
volume={7},
number={},
pages={136223-136231},
abstract={Many hospitals are suffering from ineffective use of big data analytics with electronic health records (EHRs) to generate high quality insights for their clinical practices. Organizational learning has been a key role in improving the use of big data analytics with EHRs. Drawing on the knowledge-based view and big data lifecycle, we investigate how the three modes of knowledge can achieve meaningful use of big data analytics with EHRs. To test the associations in the proposed research model, we surveyed 580 nurses of a large hospital in China in 2019. Structural equation modelling was used to examine relationships between knowledge mode of EHRs and meaningful use of EHRs. The results reveal that know-what about EHRs utilization, know-how EHRs storage and utilization, and know-why storage and utilization can improve nurses' meaningful use of big data analytics with EHRs. This study contributes to the existing digital health and big data literature by exploring the proper adaptation of analytical tools to EHRs from the different knowledge mode in order to shape meaningful use of big data analytics with EHRs.},
keywords={Big Data;Hospitals;Organizations;Electronic medical records;Biomedical imaging;Big data analytics;electronic health records and impacts;knowledge-based view},
doi={10.1109/ACCESS.2019.2939158},
ISSN={2169-3536},
month={},}
@ARTICLE{9427143,
author={Zheng, Miaomiao and Zhang, Shanshan and Zhang, Yidan and Hu, Baozhong},
journal={IEEE Access},
title={Construct Food Safety Traceability System for People’s Health Under the Internet of Things and Big Data},
year={2021},
volume={9},
number={},
pages={70571-70583},
abstract={In the context of epidemic prevention and control, food safety monitoring, data analysis and food safety traceability have become more important. At the same time, the most important reason for food safety issues is incomplete, opaque, and asymmetric information. The most fundamental way to solve these problems is to do a good job of traceability, and establish a reasonable and reliable food safety traceability system. The traceability system is currently an important means to ensure food quality and safety and solve the crisis of trust between consumers and the market. Research on food safety traceability systems based on big data, artificial intelligence and the Internet of Things provides ideas and methods to solve the problems of low credibility and difficult data storage in the application of traditional traceability systems. Therefore, this research takes rice as an example and proposes a food safety traceability system based on RFID two-dimensional code technology and big data storage technology in the Internet of Things. This article applies RFID technology to the entire system by analyzing the requirements of the system, designing the system database and database tables, encoding the two-dimensional code and generating the design for information entry. Using RFID radio frequency technology and the data storage function in big data to obtain information in the food production process. Finally, the whole process of food production information can be traced through the design of dynamic query platform and mobile terminal. In this research, the food safety traceability system based on big data and the Internet of Things guarantees the integrity, reliability and safety of traceability information from a technical level. This is an effective solution for enhancing the credibility of traceability information, ensuring the integrity of information, and optimizing the data storage structure.},
keywords={Safety;Big Data;Internet of Things;Production;Radiofrequency identification;Python;Epidemics;Two-dimensional code technology;Internet of Things;big data;artificial intelligence;food safety traceability system},
doi={10.1109/ACCESS.2021.3078536},
ISSN={2169-3536},
month={},}
@ARTICLE{9884973,
author={Byabazaire, John and O’Hare, Gregory M.P. and Delaney, Declan T.},
journal={IEEE Sensors Journal},
title={End-to-End Data Quality Assessment Using Trust for Data Shared IoT Deployments},
year={2022},
volume={22},
number={20},
pages={19995-20009},
abstract={Continued development of communication technologies has led to widespread Internet-of-Things (IoT) integration into various domains, including health, manufacturing, automotive, and precision agriculture. This has further led to the increased sharing of data among such domains to foster innovation. Most of these IoT deployments, however, are based on heterogeneous, pervasive sensors, which can lead to quality issues in the recorded data. This can lead to sharing of inaccurate or inconsistent data. There is a significant need to assess the quality of the collected data, should it be shared with multiple application domains, as inconsistencies in the data could have financial or health ramifications. This article builds on the recent research on trust metrics and presents a framework to integrate such metrics into the IoT data cycle for real-time data quality assessment. Critically, this article adopts a mechanism to facilitate end-user parameterization of a trust metric tailoring its use in the framework. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd-sourced or other unreliable data collection techniques such as that in IoT. The article further discusses how the trust-based framework eliminates the requirement for a gold standard and provides visibility into data quality assessment throughout the big data model. To qualify the use of trust as a measure of quality, an experiment is conducted using data collected from an IoT deployment of sensors to measure air quality in which low-cost sensors were colocated with a gold standard reference sensor. The calculated trust metric is compared with two well-understood metrics for data quality, root mean square error (RMSE), and mean absolute error (MAE). A strong correlation between the trust metric and the comparison metrics shows that trust may be used as an indicative quality metric for data quality. The metric incorporates the additional benefit of its ability for use in low context scenarios, as opposed to RMSE and MAE, which require a reference for comparison.},
keywords={Data integrity;Measurement;Internet of Things;Big Data;Data models;Sensor phenomena and characterization;Quality assessment;Big data model;data quality;Internet of Things (IoT);machine learning;trust},
doi={10.1109/JSEN.2022.3203853},
ISSN={1558-1748},
month={Oct},}
@ARTICLE{8746175,
author={Saberi, Morteza and Hussain, Omar Khadeer and Chang, Elizabeth},
journal={IEEE Access},
title={Quality Management of Workers in an In-House Crowdsourcing-Based Framework for Deduplication of Organizations’ Databases},
year={2019},
volume={7},
number={},
pages={90715-90730},
abstract={While organizations in the current era of big data are generating massive volumes of data, they also need to ensure that its quality is maintained for it to be useful in decision-making purposes. The problem of dirty data plagues every organization. One aspect of dirty data is the presence of duplicate data records that negatively impact the organization's operations in many ways. Many existing approaches attempt to address this problem by using traditional data cleansing methods. In this paper, we address this problem by using an in-house crowdsourcing-based framework, namely, DedupCrowd. One of the main obstacles of crowdsourcing-based approaches is to monitor the performance of the crowd, by which the integrity of the whole process is maintained. In this paper, a statistical quality control-based technique is proposed to regulate the performance of the crowd. We apply our proposed framework in the context of a contact center, where the Customer Service Representatives are used as the crowd to assist in the process of deduplicating detection. By using comprehensive working examples, we show how the different modules of the DedupCrowd work not only to monitor the performance of the crowd but also to assist in duplicate detection.},
keywords={Crowdsourcing;Databases;Task analysis;Object recognition;Monitoring;Error analysis;Big Data;Quality management;quality control;data quality;duplicate detection;in-house crowdsourcing},
doi={10.1109/ACCESS.2019.2924979},
ISSN={2169-3536},
month={},}
@ARTICLE{9845398,
author={Meritxell, Gómez-Omella and Sierra, Basilio and Ferreiro, Susana},
journal={IEEE Access},
title={On the Evaluation, Management and Improvement of Data Quality in Streaming Time Series},
year={2022},
volume={10},
number={},
pages={81458-81475},
abstract={The Internet of Things (IoT) technologies plays a key role in the Fourth Industrial Revolution (Industry 4.0). This implies the digitisation of the industry and its services to improve productivity. To obtain the necessary information throughout the different processes, useful data streams are obtained to provide Artificial Intelligence and Big Data algorithms. However, strategic decision-making based on these algorithms may not be successful if they have been developed based on inadequate low-quality data. This research work proposes a set of metrics to measure Data Quality (DQ) in streaming time series, and implements and validates a set of techniques and tools that allow monitoring and improving the quality of the information. These techniques allow the early detection of problems that arise in relation to the quality of the data collected; and, in addition, they provide some mechanisms to solve these problems. Later, as part of the work, a use case related to industrial field is presented, where these techniques and tools have been deployed into a data management, monitoring and data analysis platform. This integration provides additional functionality to the platform, a Decision Support System (DSS) named DQ-REMAIN (Data Quality REport MAnagement and ImprovemeNt), for decision-making regarding the quality of data obtained from streaming time series.},
keywords={Measurement;Time series analysis;Data integrity;Decision support systems;Standards;Monitoring;Internet of Things;Data quality;streaming time series;decision support system},
doi={10.1109/ACCESS.2022.3195338},
ISSN={2169-3536},
month={},}
@ARTICLE{9658542,
author={Chen, Yanli and Zhou, Limengnan and Zhou, Yonghui and Chen, Yi and Hu, Shengbo and Dong, Zhicheng},
journal={IEEE Access},
title={Multiple Histograms Shifting-Based Video Data Hiding Using Compression Sensing},
year={2022},
volume={10},
number={},
pages={699-707},
abstract={With the development of multimedia editing technologies, the copyright protection has attacked more attentions. Reversible data hiding (RDH), in which the cover can be recovered losslessly, is an effect method to eliminate embedding distortions. As a typical RDH method, histogram shifting (HS) is used widely. Most existing RDH schemes based on HS usually build sharp histograms by predicting and sorting techniques. To make use of spatial correlations of multimedia, several RDH schemes based on multiple HS (MHS) are proposed to protect copyright, in which some rigid rules are used to build multiple histograms. Against images, videos have more spatial and temple correlations and it is easier to acquire sharper histograms. In this paper, a video MHS scheme based on compression sensing (CS) is proposed. As a linear sensing algorithm, CS can measure macroblock residuals by reducing corrections among pixels to acquire distinguishable macroblock features, while keeping their statistical characteristics immutable. By employing CS, macroblocks with similar characteristics cluster together to formulate multiple histograms. For each of these histograms, data embedding is implemented to reduce shifting distortions by expanding the outermost bins while other bins are unchanged. Experimental results show that the quality of most test videos in our scheme are higher than that in the state-of-art schemes.},
keywords={Histograms;Videos;Feature extraction;Distortion;Sensors;Data mining;Compression Sensing;multiple histograms shifting (MHS);reversible data hiding},
doi={10.1109/ACCESS.2021.3137398},
ISSN={2169-3536},
month={},}
@ARTICLE{8667817,
author={He, Zhenzhen and He, Yihai and Liu, Fengdi and Zhao, Yixiao},
journal={IEEE Access},
title={Big Data-Oriented Product Infant Failure Intelligent Root Cause Identification Using Associated Tree and Fuzzy DEA},
year={2019},
volume={7},
number={},
pages={34687-34698},
abstract={Infant failure analyzing is an effective approach to improve production quality continuously. The root causes of infant failure have always been a puzzle to manufacturers. To satisfy the increasing demand for the fuzzy root cause analysis of product infant failure in the era of big data, a novel root cause identification approach based on the associated tree and fuzzy data envelopment analysis (DEA) is presented for product infant failure. First, to decrease fuzziness with regard to the mechanism of infant failure, the associated tree is adapted to guide the analysis process for possible root causes based on axiomatic domain mapping. Second, considering the fuzzy mechanism and massive data, the fuzzy DEA technique is adopted to cluster all the potential factors of functional parameters, physical parameters, and process parameters from big data regarding product life cycle. Third, the ranking method of decision-making unit efficiency in fuzzy DEA is used to model and rank the weight of each node in the established associated tree of infant failure. Finally, a case study of root cause identification for a typical infant failure of the vibration and noise of a washing machine is presented to demonstrate the feasibility and validity of the proposed method.},
keywords={Big Data;Reliability;Manufacturing;Failure analysis;Analytical models;Production;Computational modeling;Infant failure;big data;root cause analysis;associated tree;fuzzy DEA},
doi={10.1109/ACCESS.2019.2904759},
ISSN={2169-3536},
month={},}
@ARTICLE{7298399,
author={Lu, Qinghua and Li, Zheng and Kihl, Maria and Zhu, Liming and Zhang, Weishan},
journal={IEEE Access},
title={CF4BDA: A Conceptual Framework for Big Data Analytics Applications in the Cloud},
year={2015},
volume={3},
number={},
pages={1944-1952},
abstract={Building big data analytics (BDA) applications in the cloud introduces inevitable challenges, such as loss of control and uncertainty. To address the existing challenges, numerous efforts have been made on BDA application engineering to optimize the quality of BDA applications in the cloud, such as performance and reliability. However, there is still a lack of systematic view on engineering BDA applications in the cloud. Therefore, in this paper, we present a conceptual framework named CF4BDA to analyze the existing work on BDA applications from two perspectives: 1) the lifecycle of BDA applications and 2) the objects involved in the context of BDA applications in the cloud. The framework can help researchers and practitioners identify the research opportunities in a structured way and guide implementing BDA applications in the cloud. We perform a preliminary evaluation of the usefulness of CF4BDA by applying it to analyze a set of representative studies.},
keywords={Big data;Data analystics;Software engineering;Cloud computing;Reliability engineering;Semantics;Context modeling;Big data analytics;Cloud computing;conceptual framework;software engineering;Big data analytics;cloud computing;conceptual framework;software engineering},
doi={10.1109/ACCESS.2015.2490085},
ISSN={2169-3536},
month={},}
@ARTICLE{8392685,
author={Yacchirema, Diana C. and Sarabia-JáCome, David and Palau, Carlos E. and Esteve, Manuel},
journal={IEEE Access},
title={A Smart System for Sleep Monitoring by Integrating IoT With Big Data Analytics},
year={2018},
volume={6},
number={},
pages={35988-36001},
abstract={Obtrusive sleep apnea (OSA) is one of the most important sleep disorders because it has a direct adverse impact on the quality of life. Intellectual deterioration, decreased psychomotor performance, behavior, and personality disorders are some of the consequences of OSA. Therefore, a real-time monitoring of this disorder is a critical need in healthcare solutions. There are several systems for OSA detection. Nevertheless, despite their promising results, these systems not guiding their treatment. For these reasons, this research presents an innovative system for both to detect and support of treatment of OSA of elderly people by monitoring multiple factors such as sleep environment, sleep status, physical activities, and physiological parameters as well as the use of open data available in smart cities. Our system architecture performs two types of processing. On the one hand, a pre-processing based on rules that enables the sending of real-time notifications to responsible for the care of elderly, in the event of an emergency situation. This pre-processing is essentially based on a fog computing approach implemented in a smart device operating at the edge of the network that additionally offers advanced interoperability services: technical, syntactic, and semantic. On the other hand, a batch data processing that enables a descriptive analysis that statistically details the behavior of the data and a predictive analysis for the development of services, such as predicting the least polluted place to perform outdoor activities. This processing uses big data tools on cloud computing. The performed experiments show a 93.3% of effectivity in the air quality index prediction to guide the OSA treatment. The system's performance has been evaluated in terms of latency. The achieved results clearly demonstrate that the pre-processing of data at the edge of the network improves the efficiency of the system.},
keywords={Sleep apnea;Monitoring;Biomedical monitoring;Real-time systems;Computer architecture;Big Data;Sensors;Internet-of-Things;big data;interoperability;sleep monitoring;health monitoring;open data;fog computing;cloud computing},
doi={10.1109/ACCESS.2018.2849822},
ISSN={2169-3536},
month={},}
@ARTICLE{8788596,
author={Wu, Di and Wang, Hao and Mohammed, Hadi and Seidu, Razak},
journal={IEEE Transactions on Sustainable Computing},
title={Quality Risk Analysis for Sustainable Smart Water Supply Using Data Perception},
year={2020},
volume={5},
number={3},
pages={377-388},
abstract={Constructing Sustainable Smart Water Supply systems are facing serious challenges all around the world with the fast expansion of modern cities. Water quality is influencing our life ubiquitously and prioritizing all the urban management. Traditional urban water quality control mostly focused on routine tests of quality indicators, which include physical, chemical, and biological groups. However, the inevitable delay for biological indicators has increased the health risk and leads to accidents such as massive infections in many big cities. In this paper, we first analyze the problem, technical challenges, and research questions. Then, we provide a possible solution by building a risk analysis framework for the urban water supply system. It takes indicator data we collected from industrial processes to perceive water quality changes, and further for risk detection. In order to provide explainable results, we propose an Adaptive Frequency Analysis (Adp-FA) method to resolve the data using indicators' frequency domain information for their inner relationships and individual prediction. We also investigate the scalability properties of this method from indicator, geography, and time domains. For the application, we select industrial quality data sets collected from a Norwegian project in four different urban water supply systems, as Oslo, Bergen, Strømmen, and Ålesund. We employ the proposed method to test spectrogram, prediction accuracy, and time consumption, comparing with classical Artificial Neural Network and Random Forest methods. The results show our method better perform in most of the aspects. It is feasible to support industrial water quality risk early warnings and further decision support.},
keywords={Water quality;Water resources;Lakes;Water pollution;Biology;Urban areas;Sensors;Sustainable water supply;water quality control;data perception;risk evaluation;frequency analysis;scalability},
doi={10.1109/TSUSC.2019.2929953},
ISSN={2377-3782},
month={July},}
@ARTICLE{9299499,
author={Liu, Jie and Cao, Yijia and Li, Yong and Guo, Yixiu and Deng, Wei},
journal={CSEE Journal of Power and Energy Systems},
title={A big data cleaning method based on improved CLOF and Random Forest for distribution network},
year={2020},
volume={},
number={},
pages={1-10},
abstract={In order to improve the data quality, the big data cleaning method of distribution network was studied in this paper. First, the Local Outlier Factor (LOF) algorithm based on DBSCAN clustering was used to detect outliers. However, due to the difficulty in determining the LOF threshold, a method of dynamically calculating the threshold based on the transformer districts and time was proposed. Besides, the LOF algorithm combines the statistical distribution method to reduce the "misjudgment rate". Aiming at the diversity and complexity of data missing forms in power big data, this paper improved the Random Forest imputation algorithm, which can be applied to various forms of missing data, especially the blocked missing data and even some horizontal or vertical data completely missing. The data in this paper were from real data of 44 transformer districts of a certain 10kV line in distribution network. Experimental results showed that outlier detection was accurate and suitable for any shape and multidimensional power big data. The improved Random Forest imputation algorithm was suitable for all missing forms, with higher imputation accuracy and better model stability. By comparing the network loss prediction between the data using this data cleaning method and the data removing outliers and missing values, it was found that the accuracy of network loss prediction had been improved by nearly 4 percentage points using the data cleaning method mentioned in this paper. Additionally, as the proportion of bad data increased, the difference between the prediction accuracy of cleaned data and that of uncleaned data was greater.},
keywords={Cleaning;Distribution networks;Big Data;Prediction algorithms;Clustering algorithms;Data models;Anomaly detection;Data cleaning;Outliers detection;missing data imputation;LOF;DBSCAN;Random Forest},
doi={10.17775/CSEEJPES.2020.04080},
ISSN={2096-0042},
month={},}
@ARTICLE{7912315,
author={Chen, Min and Hao, Yixue and Hwang, Kai and Wang, Lu and Wang, Lin},
journal={IEEE Access},
title={Disease Prediction by Machine Learning Over Big Data From Healthcare Communities},
year={2017},
volume={5},
number={},
pages={8869-8879},
abstract={With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013-2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.},
keywords={Diseases;Hospitals;Prediction algorithms;Machine learning algorithms;Big Data;Data models;Big data analytics;machine learning;healthcare},
doi={10.1109/ACCESS.2017.2694446},
ISSN={2169-3536},
month={},}
@ARTICLE{8374410,
author={Jang, Busik and Park, Sangdon and Lee, Joohyung and Hahn, Sang-Geun},
journal={IEEE Access},
title={Three Hierarchical Levels of Big-Data Market Model Over Multiple Data Sources for Internet of Things},
year={2018},
volume={6},
number={},
pages={31269-31280},
abstract={This paper proposes three hierarchical levels of a competitive big-data market model. We consider that a service provider gathers data from multiple data sources and provides valuable information from refined data as a service to its customers. Under our approach, a service provider determines optimal data procurement from multiple data sources within its budget constraint. The multiple data sources follow the service provider's action by independently submitting bidding prices to the service provider. Further, customers decide whether to subscribe or not based on the subscription fee, their willingness-to-pay, and the quality of the refined data. We study the economic benefits of such a market model by analyzing the hierarchical decision making procedures as a Stackelberg game. We show the existence and the uniqueness of the Nash equilibrium (NE), and the NE solution is given as a closed form. Finally, we reveal that the obtained unique equilibrium solution maximizes the payoff of all market participants.},
keywords={Data models;Games;Analytical models;Internet of Things;Data mining;Nash equilibrium;Numerical models;Data market;Internet of Things;stackelberg game;industrial informatics},
doi={10.1109/ACCESS.2018.2845105},
ISSN={2169-3536},
month={},}
@ARTICLE{7978034,
author={Xu, Xiaolong and Liu, Xinxin and Liu, Xiaoxiao and Sun, Yanfei},
journal={Journal of Systems Engineering and Electronics},
title={Truth finder algorithm based on entity attributes for data conflict solution},
year={2017},
volume={28},
number={3},
pages={617-626},
abstract={The Internet now is a large-scale platform with big data. Finding truth from a huge dataset has attracted extensive attention, which can maintain the quality of data collected by users and provide users with accurate and efficient data. However, current truth finder algorithms are unsatisfying, because of their low accuracy and complication. This paper proposes a truth finder algorithm based on entity attributes (TFAEA). Based on the iterative computation of source reliability and fact accuracy, TFAEA considers the interactive degree among facts and the degree of dependence among sources, to simplify the typical truth finder algorithms. In order to improve the accuracy of them, TFAEA combines the one-way text similarity and the factual conflict to calculate the mutual support degree among facts. Furthermore, TFAEA utilizes the symmetric saturation of data sources to calculate the degree of dependence among sources. The experimental results show that TFAEA is not only more stable, but also more accurate than the typical truth finder algorithms.},
keywords={Algorithm design and analysis;Reliability;Internet;Telecommunications;Big Data;Data models;truth finder;data reliability;entity attribute;data conflict},
doi={10.21629/JSEE.2017.03.21},
ISSN={1004-4132},
month={June},}
@ARTICLE{9906976,
author={Murshed, Belal Abdullah Hezam and Abawajy, Jemal and Mallappa, Suresha and Saif, Mufeed Ahmed Naji and Al-Ghuribi, Sumaia Mohammed and Ghanem, Fahd A.},
journal={IEEE Access},
title={Enhancing Big Social Media Data Quality for Use in Short-Text Topic Modeling},
year={2022},
volume={10},
number={},
pages={105328-105351},
abstract={With the emergence of microblogging platforms and social media applications, large amounts of user-generated data in the form of comments, reviews, and brief text messages are produced every day. Microblog data is typically of poor quality; hence improving the quality of the data is a significant scientific and practical challenge. In spite of the relevance of the problem, there has been not much work so far, especially in regard to microblog data quality for Short-Text Topic Modelling (STTM) purposes. This paper addresses this problem and proposes an approach called the Social Media Data Cleansing Model (SMDCM) to improve data quality for STTM. We evaluate SMDCM using six topic modelling methods, namely the Latent Dirichlet Allocation (LDA), Word-Network Topic Model (WNTM), Pseudo-document-based Topic Modelling (PTM), Biterm Topic Model (BTM), Global and Local word embedding-based Topic Modeling (GLTM), and Fuzzy Topic modelling (FTM). We used the Real-world Cyberbullying Twitter (RW-CB-Twitter) and the Cyberbullying Mendeley (CB-MNDLY) datasets in the evaluation. The results proved the efficiency of the GLTM and WNTM over the other STTM models when applying the SMDCM techniques, which achieved optimum topic coherence and high accuracy values on RW-CB-Twitter and CB-MNDLY datasets.},
keywords={Social networking (online);Data models;Blogs;Data integrity;Sentiment analysis;Information integrity;Big Data;Coherence;Social media;big data;microblogging platforms;topic modeling;data cleansing;data quality;topic coherence;purity},
doi={10.1109/ACCESS.2022.3211396},
ISSN={2169-3536},
month={},}
@ARTICLE{9663261,
author={Shao, Qixiang and Yu, Runlong and Zhao, Hongke and Liu, Chunli and Zhang, Mengyi and Song, Hongmei and Liu, Qi},
journal={Big Data Mining and Analytics},
title={Toward intelligent financial advisors for identifying potential clients: A multitask perspective},
year={2022},
volume={5},
number={1},
pages={64-78},
abstract={Intelligent Financial Advisors (IFAs) in online financial applications (apps) have brought new life to personal investment by providing appropriate and high-quality portfolios for users. In real-world scenarios, identifying potential clients is a crucial issue for IFAs, i.e., identifying users who are willing to purchase the portfolios. Thus, extracting useful information from various characteristics of users and further predicting their purchase inclination are urgent. However, two critical problems encountered in real practice make this prediction task challenging, i.e., sample selection bias and data sparsity. In this study, we formalize a potential conversion relationship, i.e., user ! activated user ! client and decompose this relationship into three related tasks. Then, we propose a Multitask Feature Extraction Model (MFEM), which can leverage useful information contained in these related tasks and learn them jointly, thereby solving the two problems simultaneously. In addition, we design a two-stage feature selection algorithm to select highly relevant user features efficiently and accurately from an incredibly huge number of user feature fields. Finally, we conduct extensive experiments on a real-world dataset provided by a famous fintech bank. Experimental results clearly demonstrate the effectiveness of MFEM.},
keywords={Task analysis;Feature extraction;Portfolios;Training;Investment;Amplitude modulation;Data mining;Intelligent Financial Advisor (IFA);potential client identification;MultiTask Learning (MTL);feature selection},
doi={10.26599/BDMA.2021.9020021},
ISSN={2096-0654},
month={March},}
@ARTICLE{8674747,
author={Liu, Bin and He, Songrui and He, Dongjian and Zhang, Yin and Guizani, Mohsen},
journal={IEEE Access},
title={A Spark-Based Parallel Fuzzy $c$ -Means Segmentation Algorithm for Agricultural Image Big Data},
year={2019},
volume={7},
number={},
pages={42169-42180},
abstract={With the explosive growth of image big data in the agriculture field, image segmentation algorithms are confronted with unprecedented challenges. As one of the most important images segmentation technologies, the fuzzy c-means (FCMs) algorithm has been widely used in the field of agricultural image segmentation as it provides simple computation and high-quality segmentation. However, due to its large amount of computation, the sequential FCM algorithm is too slow to finish the segmentation task within an acceptable time. This paper proposes a parallel FCM segmentation algorithm based on the distributed memory computing platform Apache Spark for agricultural image big data. The input image is first converted from the RGB color space to the lab color space and generates point cloud data. Then, point cloud data are partitioned and stored in different computing nodes, in which the membership degrees of pixel points to different cluster centers are calculated and the cluster centers are updated iteratively in a data-parallel form until the stopping condition is satisfied. Finally, point cloud data are restored after clustering for reconstructing the segmented image. On the Spark platform, the performance of the parallel FCMs algorithm is evaluated and reaches an average speedup of 12.54 on ten computing nodes. The experimental results show that the Spark-based parallel FCMs algorithm can obtain a significant increase in speedup, and the agricultural image testing set delivers a better performance improvement of 128% than the Hadoop-based approach. This paper indicates that the Spark-based parallel FCM algorithm provides faster speed of segmentation for agricultural image big data and has better scale-up and size-up rates.},
keywords={Image segmentation;Clustering algorithms;Cluster computing;Big Data;Partitioning algorithms;Sparks;Task analysis;Fuzzy C-means;image segmentation;image big data;Apache Spark;parallel algorithm},
doi={10.1109/ACCESS.2019.2907573},
ISSN={2169-3536},
month={},}
@ARTICLE{8319403,
author={Ali, Maqbool and Ali, Rahman and Khan, Wajahat Ali and Han, Soyeon Caren and Bang, Jaehun and Hur, Taeho and Kim, Dohyeong and Lee, Sungyoung and Kang, Byeong Ho},
journal={IEEE Access},
title={A Data-Driven Knowledge Acquisition System: An End-to-End Knowledge Engineering Process for Generating Production Rules},
year={2018},
volume={6},
number={},
pages={15587-15607},
abstract={Data-driven knowledge acquisition is one of the key research fields in data mining. Dealing with large amounts of data has received a lot of attention in the field recently, and a number of methodologies have been proposed to extract insights from data in an automated or semi-automated manner. However, these methodologies generally target a specific aspect of the data mining process, such as data acquisition, data preprocessing, or data classification. However, a comprehensive knowledge acquisition method is crucial to support the end-to-end knowledge engineering process. In this paper, we introduce a knowledge acquisition system that covers all major phases of the cross-industry standard process for data mining. Acknowledging the importance of an end-to-end knowledge engineering process, we designed and developed an easy-to-use data-driven knowledge acquisition tool (DDKAT). The major features of the DDKAT are: (1) a novel unified features scoring approach for data selection; (2) a user-friendly data processing interface to improve the quality of the raw data; (3) an appropriate decision tree algorithm selection approach to build a classification model; and (4) the generation of production rules from various decision tree classification models in an automated manner. Furthermore, two diabetes studies were performed to assess the value of the DDKAT in terms of user experience. A total of 19 experts were involved in the first study and 102 students in the artificial intelligence domain were involved in the second study. The results showed that the overall user experience of the DDKAT was positive in terms of its attractiveness, as well as its pragmatic and hedonic quality factors.},
keywords={Data mining;Feature extraction;Decision trees;Knowledge acquisition;Data models;Production;Knowledge engineering;data mining;features ranking;algorithm selection;decision tree;production rule;user experience},
doi={10.1109/ACCESS.2018.2817022},
ISSN={2169-3536},
month={},}
@ARTICLE{8951087,
author={Guleng, Siri and Wu, Celimuge and Liu, Zhi and Chen, Xianfu},
journal={IEEE Access},
title={Edge-Based V2X Communications With Big Data Intelligence},
year={2020},
volume={8},
number={},
pages={8603-8613},
abstract={Vehicular Internet-of-Things applications require an efficient Vehicle-to-Everything (V2X) communication scheme. However, it is particularly challenging to achieve a high throughput and low latency with limited wireless resources in highly dynamic vehicular networks. In this article, we propose a scheme that enhances V2V communications through integration of vehicle edge-based forwarding and learning-based edge selection policy optimization. The proposed scheme has three main characteristics. First, the Hierarchical edge-based preemptive route creation is introduced to create hierarchical edges and conduct efficient packet forwarding as well as route aggregation. Second, Two-stage learning is introduced to select efficient edge nodes using big data driven traffic prediction and reinforcement learning-based edge node selection. Third, Context-aware edge selection is employed to improve the performance of edge-based forwarding in various contexts. We use real traffic big data and realistic vehicular network simulations to evaluate the performance of the proposed scheme and show the advantage over other baseline approaches.},
keywords={Vehicle-to-everything;Big Data;Protocols;Edge computing;Wireless communication;Unicast;Quality of service;Edge computing;traffic big data;vehicular ad hoc networks;V2X communications},
doi={10.1109/ACCESS.2020.2964707},
ISSN={2169-3536},
month={},}
@ARTICLE{8449924,
author={Akila, T.H. and Paik, Incheon and Siriweera, S.},
journal={IEEE Access},
title={QoS-Aware Rule-Based Traffic-Efficient Multiobjective Service Selection in Big Data Space},
year={2018},
volume={6},
number={},
pages={48797-48814},
abstract={The number of Web services has increased dramatically during the last few years. This has resulted in an increase in the volume of candidate services for tasks in composition systems. This has led to growth in the variety of nonfunctional properties in service selection, resulting in uncertainty (veracity issues) among such properties, which has severely affected the NP-hard aspects of service selection. Despite this, consumers in many areas would like access to a variety of selection methods such as linear programming and dynamic programming techniques. An additional problem is that the composition length (the number of tasks) of the workflow has increased, with the incorporation of research domains such as data science. These trending composition issues are challenging the computational power of existing methods. Such concerns have opened the door to research involving Big Data space. We propose a flexible, distributed selection algorithm that facilitates heterogeneous-selection methods to satisfy multiobjective composition requirements rather than rigid, specific composition requirements. However, service-selection processes in a Big Data space will inevitably increase traffic congestion caused by the increased volume of internal communication, particularly external traffic, such as Zipf and Pareto phenomena, and internal traffic during shuffling. To address these concerns, we propose solutions for each case. Our experiments demonstrate that the proposed traffic-efficient multiobjective method is well behaved when selecting services in Big Data space.},
keywords={Quality of service;Big Data;Traffic congestion;Task analysis;Optimization;Heuristic algorithms;Web services;Big Data space;multiobjective service selection;QoS preferences;rule-based;traffic-efficient},
doi={10.1109/ACCESS.2018.2867633},
ISSN={2169-3536},
month={},}
@ARTICLE{9815858,
author={Lee, Sun-Yong and Connerton, Timothy Paul and Lee, Yeon-Woo and Kim, Daeyoung and Kim, Donghwan and Kim, Jin-Ho},
journal={IEEE Access},
title={Semi-GAN: An Improved GAN-Based Missing Data Imputation Method for the Semiconductor Industry},
year={2022},
volume={10},
number={},
pages={72328-72338},
abstract={Complete data are required for the operation, maintenance, and detection of faults in semiconductor equipment. Missing data occur frequently because of defects such as sensor, data storage, and communication faults, leading to reductions in yield, quality, and productivity. Although many attempts have been made to solve this problem in other fields, few studies have specifically addressed data imputation in the semiconductor industry. In this study, an improved generative adversarial network (GAN)-based missing data imputation for the semiconductor industry called Semi-GAN is proposed. This study introduces a machine learning approach for dealing with data imputation in the semiconductor industry. The proposed method was applied to real data and evaluated using traditional techniques. In particular, the proposed method showed excellent results compared to traditional attribution methods when all missing data ratios in the experiments were less than 20%. It was also observed to be superior when simple and repetitive patterns were omitted rather than repetitive but not simple patterns.},
keywords={Generative adversarial networks;Electronics industry;Data models;Training;Support vector machines;Random forests;Neurons;Data imputation;deep learning;fault classification and detection;generative adversarial networks;machine learning;missing data;semiconductor equipment},
doi={10.1109/ACCESS.2022.3188871},
ISSN={2169-3536},
month={},}
@ARTICLE{9115006,
author={Shao, Zengzhen and Sun, Hongxu and Wang, Xiao and Sun, Zhongzhi},
journal={IEEE Access},
title={An Optimized Mining Algorithm for Analyzing Students’ Learning Degree Based on Dynamic Data},
year={2020},
volume={8},
number={},
pages={113543-113556},
abstract={With the rapid development of educational informatization, it has enabled education to enter the era of big data. How to extract effective information from educational big data and realize adaptive personalized learning goals have become the current research hotspot. The traditional static data only analyzes the students' learning degree based on the students' final answer, but ignores the dynamic data in the process of answering questions, such as the modification and the time it answered on the question, which makes it difficult to fully and accurately mine the correlation between the massive data, so it turns from static data mining to dynamic data mining. The paper proposes an optimized mining algorithm for analyzing students' learning degree based on dynamic data. The algorithm first uses the optimized text classification technology to match the question texts to the knowledge points automatically, so as to improves the efficiency and quality. Then, it uses the subjective weighting method combined with the expert experience to generate the learning degree matrix of students on knowledge points based on dynamic data of the students' records. Finally, the DBSCAN clustering algorithm is used to cluster the personalized learning characteristics of students according to the learning degree matrix. The experimental result shows that the algorithm can deal with massive data automatically and effectively, and analyze the students' learning degree on knowledge points comprehensively and accurately, so as to classify students and realize personalized teaching.},
keywords={Classification algorithms;Heuristic algorithms;Support vector machines;Text categorization;Training;Data mining;dynamic data;students’ learning degree;subjective weighting method;clustering algorithm},
doi={10.1109/ACCESS.2020.3001749},
ISSN={2169-3536},
month={},}
@ARTICLE{8979364,
author={Zhang, Huiyan and Hu, Bo and Wang, Xiaoyi and Xu, Jiping and Wang, Li and Sun, Qian and Zhao, Zhiyao},
journal={IEEE Access},
title={An Action Dependent Heuristic Dynamic Programming Approach for Algal Bloom Prediction With Time-Varying Parameters},
year={2020},
volume={8},
number={},
pages={26235-26246},
abstract={Algal bloom is a nonlinear and time-varying process, which brings challenges for the accurate prediction. For the existing mechanism model of algae ignores the external key factors, we propose an algae growth model (AGM) optimized by action dependent heuristic dynamic programming (ADHDP). This model has the structure of information interaction with the outside, which can predict algal bloom with well adaptive ability. In this paper, chlorophyll-a concentration is used as the representative factor of algal bloom. We use ADHDP approach to map the external key factors to the time-varying parameters, so the AGM can be adjusted to realize the self-adaptive prediction with the changes in external environments. Compared with different prediction methods, the simulation result shows that the ADHDP-AGM prediction model can accurately predict the chlorophyll-a concentration under different data distributions. Moreover, the prediction process shows that the time-varying parameters in AGM conform to the evolution trend of chlorophyll-a concentration in fact, which further improves the interpretability of prediction model. It provides a new perspective for building a data-driven prediction model with clear physical significance, and makes the mechanism research and data science further fusion.},
keywords={Predictive models;Biological system modeling;Algae;Adaptation models;Mathematical model;Water quality;Data models;Algal bloom;action dependent heuristic dynamic programming;time-varying parameters identification;prediction},
doi={10.1109/ACCESS.2020.2971244},
ISSN={2169-3536},
month={},}
@ARTICLE{9104899,
author={Fiaidhi, Jinan},
journal={IEEE Access},
title={Envisioning Insight-Driven Learning Based on Thick Data Analytics With Focus on Healthcare},
year={2020},
volume={8},
number={},
pages={114998-115004},
abstract={Detecting and analyzing patient insights from social media enables healthcare givers to better understand what patients want and also to identify their pain points. Healthcare institutions cannot neglect the need to monitor and analyze popular social media outlets such as Twitter and Facebook. To have a study success, a healthcare giver needs to be able to engage with their patients and adapt to their preferences effectively. However, data-driven decision-making is no longer enough, as the best-in-class organizations struggle to realize tangible benefits from their data-driven analytics investments. Relying on simplistic textual analytics that use big data technologies to learn consumer/patient insights is no longer sufficient as most of these analytics utilize sort of bag-of-words counting algorithms. The majority of projects utilizing big data analytics have failed due to the obsession with metrics at the expense of capturing the customer's perspective data, as well as the failure in turning consumer insights into actions. Most of the consumer insights can be captured with qualitative research methods that work with small, even statistically insignificant, sample sizes. Employing qualitative analytics provide some kind of actionable intelligence which acquires understanding to broad questions about the consumer needs in tandem with analytical power. Generating insight, on one hand, requires sound techniques to measure consumers' engagement more precisely and offers depth analytics to the consumer data story. On the other hand, turning relevant insights into actions requires incorporating actionable intelligence across the business by verify hypotheses based on qualitative findings by using web analytics to see if these axioms apply to a large number of customers. The first component of our visionary approach is dedicated to identifying the relationships between constituents of the healthcare pain points as echoed by the social media conversation in terms of sociographic network where the elements composing these conversations are described as nodes and their interactions as links. In this part, conversation groups of nodes that are heavily connected will be identified representing what we call conversation communities. By identifying these conversation communities several consumer hidden insights can be inferred from using techniques such as visualizing conversation graphs relevant to given pain point, conversation learning from question answering, conversations summaries, conversation timelines, conversation anomalies and other conversation pattern learning techniques. These techniques will identify and learn the patient insights without forgetting from the context of conversation communities, are tagged as “thick data analytics”. Additionally machine learning methods can be used as assistive techniques to learn from the identified thick data and build models around identified thick data. With the use of transfer learning we also can fine tune these models with the arrival of new conversations. The author is currently experimenting with these seven insights driven learning methods described in this paper with massive geo-located Twitter data to infer the quality of care related to the current COVID-19 outbreak.},
keywords={Pain;Twitter;Tagging;Crawlers;Big Data;Patient insights;thick data;learning patient preferences;graph based algorithms;graph-based machine learning;graph-based transfer learning;Neo4j;twitter conversation communities},
doi={10.1109/ACCESS.2020.2995763},
ISSN={2169-3536},
month={},}
@ARTICLE{9159116,
author={Alkurd, Rawan and Abualhaol, Ibrahim Y. and Yanikomeroglu, Halim},
journal={IEEE Access},
title={Personalized Resource Allocation in Wireless Networks: An AI-Enabled and Big Data-Driven Multi-Objective Optimization},
year={2020},
volume={8},
number={},
pages={144592-144609},
abstract={The design and optimization of wireless networks have mostly been based on strong mathematical and theoretical modeling. Nonetheless, as novel applications emerge in the era of 5G and beyond, unprecedented levels of complexity will be encountered in the design and optimization of the network. As a result, the use of Artificial Intelligence (AI) is envisioned for wireless network design and optimization due to the flexibility and adaptability it offers in solving extremely complex problems in real-time. One of the main future applications of AI is enabling user-level personalization for numerous use cases. AI will revolutionize the way we interact with computers in which computers will be able to sense commands and emotions from humans in a non-intrusive manner, making the entire process transparent to users. By leveraging this capability, and accelerated by the advances in computing technologies, wireless networks can be redesigned to enable the personalization of network services to the user level in real-time. While current wireless networks are being optimized to achieve a predefined set of quality requirements, the personalization technology advocated in this article is supported by an intelligent big data-driven layer designed to micro-manage the scarce network resources. This layer provides the intelligence required to decide the necessary service quality that achieves the target satisfaction level for each user. Due to its dynamic and flexible design, personalized networks are expected to achieve unprecedented improvements in optimizing two contradicting objectives in wireless networks: saving resources and improving user satisfaction levels. This article presents some foundational background on the proposed network personalization technology and its enablers. Then, an AI-enabled big data-driven surrogate-assisted multi-objective optimization formulation is proposed and tested to illustrate the feasibility and prominence of this technology.},
keywords={Wireless networks;Quality of service;Optimization;Resource management;Real-time systems;Artificial intelligence;Complexity theory;Wireless network personalization;machine learning (ML);big data-driven;wireless networks;user satisfaction;quality-of-experience (QoE);deep learning (DL);artificial intelligence (AI);resource allocation;evolutionary multi-objective optimization (EMOO)},
doi={10.1109/ACCESS.2020.3014301},
ISSN={2169-3536},
month={},}
@ARTICLE{9826667,
author={Pan, Zhiwen and Li, Jiangtian and Chen, Yiqiang and Pacheco, Jesus and Dai, Lianjun and Zhang, Jun},
journal={International Journal of Crowd Science},
title={Knowledge discovery in sociological databases: An application on general society survey dataset},
year={2019},
volume={3},
number={3},
pages={315-332},
abstract={Purpose – The General Society Survey(GSS) is a kind of government-funded survey which aims at examining the Socio-economic status, quality of life, and structure of contemporary society. GSS data set is regarded as one of the authoritative source for the government and organization practitioners to make data-driven policies. The previous analytic approaches for GSS data set are designed by combining expert knowledges and simple statistics. By utilizing the emerging data mining algorithms, we proposed a comprehensive data management and data mining approach for GSS data sets. Design/methodology/approach – The approach are designed to be operated in a two-phase manner: a data management phase which can improve the quality of GSS data by performing attribute pre-processing and filter-based attribute selection; a data mining phase which can extract hidden knowledge from the data set by performing data mining analysis including prediction analysis, classification analysis, association analysis and clustering analysis. Findings – According to experimental evaluation results, the paper have the following findings: Performing attribute selection on GSS data set can increase the performance of both classification analysis and clustering analysis; all the data mining analysis can effectively extract hidden knowledge from the GSS data set; the knowledge generated by different data mining analysis can somehow cross-validate each other. Originality/value – By leveraging the power of data mining techniques, the proposed approach can explore knowledge in a fine-grained manner with minimum human interference. Experiments on Chinese General Social Survey data set are conducted at the end to evaluate the performance of our approach.},
keywords={Data analysis;Databases;Government;Data visualization;Interference;Filtering algorithms;Big Data;Data management;Data mining;Crowdsourced big data and analytics;Knowledge discovery},
doi={10.1108/IJCS-09-2019-0023},
ISSN={2398-7294},
month={October},}
@ARTICLE{8413084,
author={Peral, Jesús and Ferrández, Antonio and Gil, David and Muñoz-Terol, Rafael and Mora, Higinio},
journal={IEEE Access},
title={An Ontology-Oriented Architecture for Dealing With Heterogeneous Data Applied to Telemedicine Systems},
year={2018},
volume={6},
number={},
pages={41118-41138},
abstract={Current trends in medicine regarding issues of accessibility to and the quantity and quality of information and quality of service are very different compared to former decades. The current state requires new methods for addressing the challenge of dealing with enormous amounts of data present and growing on the Web and other heterogeneous data sources such as sensors and social networks and unstructured data, normally referred to as big data. Traditional approaches are not enough, at least on their own, although they were frequently used in hybrid architectures in the past. In this paper, we propose an architecture to process big data, including heterogeneous sources of information. We have defined an ontology-oriented architecture, where a core ontology has been used as a knowledge base and allows data integration of different heterogeneous sources. We have used natural language processing and artificial intelligence methods to process and mine data in the health sector to uncover the knowledge hidden in diverse data sources. Our approach has been applied to the field of personalized medicine (study, diagnosis, and treatment of diseases customized for each patient) and it has been used in a telemedicine system. A case study focused on diabetes is presented to prove the validity of the proposed model.},
keywords={Data mining;Medical services;Sensors;Unified modeling language;Medical diagnostic imaging;Ontologies;Telemedicine;Ontology-oriented architecture;heterogeneous data;health sector;artificial intelligence methods;personalized medicine;telemedicine system;diabetes’ treatment},
doi={10.1109/ACCESS.2018.2857499},
ISSN={2169-3536},
month={},}
@ARTICLE{9826632,
author={Pan, Zhiwen and Ji, Wen and Chen, Yiqiang and Dai, Lianjun and Zhang, Jun},
journal={International Journal of Crowd Science},
title={Anomaly data management and big data analytics: an application on disability datasets},
year={2018},
volume={2},
number={2},
pages={164-176},
abstract={Purpose – The disability datasets are the datasets that contain the information of disabled populations. By analyzing these datasets, professionals who work with disabled populations can have a better understanding of the inherent characteristics of the disabled populations, so that working plans and policies, which can effectively help the disabled populations, can be made accordingly. Design/methodology/approach – In this paper, the authors proposed a big data management and analytic approach for disability datasets. Findings – By using a set of data mining algorithms, the proposed approach can provide the following services. The data management scheme in the approach can improve the quality of disability data by estimating miss attribute values and detecting anomaly and low-quality data instances. The data mining scheme in the approach can explore useful patterns which reflect the correlation, association and interactional between the disability data attributes. Experiments based on real-world dataset are conducted at the end to prove the effectiveness of the approach. Originality/value – The proposed approach can enable data-driven decision-making for professionals who work with disabled populations.},
keywords={Correlation;Sociology;Employment;Decision making;Big Data;Data mining;Statistics;Decision support systems;Data analytics;anomaly data detection;Data management systems},
doi={10.1108/IJCS-09-2018-0020},
ISSN={2398-7294},
month={October},}
@ARTICLE{7882669,
author={Sharma, Shree Krishna and Wang, Xianbin},
journal={IEEE Access},
title={Live Data Analytics With Collaborative Edge and Cloud Processing in Wireless IoT Networks},
year={2017},
volume={5},
number={},
pages={4621-4635},
abstract={Recently, big data analytics has received important attention in a variety of application domains including business, finance, space science, healthcare, telecommunication and Internet of Things (IoT). Among these areas, IoT is considered as an important platform in bringing people, processes, data and things/objects together in order to enhance the quality of our everyday lives. However, the key challenges are how to effectively extract useful features from the massive amount of heterogeneous data generated by resource-constrained IoT devices in order to provide real-time information and feedback to the end-users, and how to utilize this data-aware intelligence in enhancing the performance of wireless IoT networks. Although there are parallel advances in cloud computing and edge computing for addressing some issues in data analytics, they have their own benefits and limitations. The convergence of these two computing paradigms, i.e., massive virtually shared pool of computing and storage resources from the cloud and real-time data processing by edge computing, could effectively enable live data analytics in wireless IoT networks. In this regard, we propose a novel framework for coordinated processing between edge and cloud computing/processing by integrating advantages from both the platforms. The proposed framework can exploit the network-wide knowledge and historical information available at the cloud center to guide edge computing units towards satisfying various performance requirements of heterogeneous wireless IoT networks. Starting with the main features, key enablers and the challenges of big data analytics, we provide various synergies and distinctions between cloud and edge processing. More importantly, we identify and describe the potential key enablers for the proposed edge-cloud collaborative framework, the associated key challenges and some interesting future research directions.},
keywords={Cloud computing;Wireless communication;Data analysis;Big Data;Wireless sensor networks;Edge computing;Distributed databases;Big data;data analytics;internet of things (IoT);cloud computing;edge computing;fog computing},
doi={10.1109/ACCESS.2017.2682640},
ISSN={2169-3536},
month={},}
@ARTICLE{8419685,
author={Sultan, Kashif and Ali, Hazrat and Zhang, Zhongshan},
journal={IEEE Access},
title={Call Detail Records Driven Anomaly Detection and Traffic Prediction in Mobile Cellular Networks},
year={2018},
volume={6},
number={},
pages={41728-41737},
abstract={Mobile networks possess information about the users as well as the network. Such information is useful for making the network end-to-end visible and intelligent. Big data analytics can efficiently analyze user and network information, unearth meaningful insights with the help of machine learning tools. Utilizing big data analytics and machine learning, this paper contributes in three ways. First, we utilize the call detail records data to detect anomalies in the network. For authentication and verification of anomalies, we use k-means clustering, an unsupervised machine learning algorithm. Through effective detection of anomalies, we can proceed to suitable design for resource distribution as well as fault detection and avoidance. Second, we prepare anomaly free data by removing anomalous activities and train a neural network model. By passing the anomaly and anomaly free data through this model, we observe the effect of anomalous activities in training of the model and also observe mean square error of the anomaly and anomaly free data. At last, we use an autoregressive integrated moving average model to predict future traffic for a user. Through simple visualization, we show that the anomaly free data better generalizes the learning models and performs better on prediction task.},
keywords={Big Data;Mobile handsets;Data analysis;Data models;Anomaly detection;Quality of service;Predictive models;Anomaly;call data records;data analytics},
doi={10.1109/ACCESS.2018.2859756},
ISSN={2169-3536},
month={},}
@ARTICLE{8066282,
author={Kotevska, Olivera and Kusne, A. Gilad and Samarov, Daniel V. and Lbath, Ahmed and Battou, Abdella},
journal={IEEE Access},
title={Dynamic Network Model for Smart City Data-Loss Resilience Case Study: City-to-City Network for Crime Analytics},
year={2017},
volume={5},
number={},
pages={20524-20535},
abstract={Today's cities generate tremendous amounts of data, thanks to a boom in affordable smart devices and sensors. The resulting big data creates opportunities to develop diverse sets of context-aware services and systems, ensuring smart city services are optimized to the dynamic city environment. Critical resources in these smart cities will be more rapidly deployed to regions in need, and those regions predicted to have an imminent or prospective need. For example, crime data analytics may be used to optimize the distribution of police, medical, and emergency services. However, as smart city services become dependent on data, they also become susceptible to disruptions in data streams, such as data loss due to signal quality reduction or due to power loss during data collection. This paper presents a dynamic network model for improving service resilience to data loss. The network model identifies statistically significant shared temporal trends across multivariate spatiotemporal data streams and utilizes these trends to improve data prediction performance in the case of data loss. Dynamics also allow the system to respond to changes in the data streams such as the loss or addition of new information flows. The network model is demonstrated by city-based crime rates reported in Montgomery County, MD, USA. A resilient network is developed utilizing shared temporal trends between cities to provide improved crime rate prediction and robustness to data loss, compared with the use of single city-based auto-regression. A maximum improvement in performance of 7.8 % for Silver Spring is found and an average improvement of 5.6 % among cities with high crime rates. The model also correctly identifies all the optimal network connections, according to prediction error minimization. City-to-city distance is designated as a predictor of shared temporal trends in crime and weather is shown to be a strong predictor of crime in Montgomery County.},
keywords={Smart cities;Resilience;Market research;Data models;Sensors;Predictive models;Adaptive algorithms;geospatial analysis;predictive models;statistical learning},
doi={10.1109/ACCESS.2017.2757841},
ISSN={2169-3536},
month={},}
@ARTICLE{7933943,
author={Islam, MD. Mofijul and Razzaque, MD. Abdur and Hassan, Mohammad Mehedi and Ismail, Walaa Nagy and Song, Biao},
journal={IEEE Access},
title={Mobile Cloud-Based Big Healthcare Data Processing in Smart Cities},
year={2017},
volume={5},
number={},
pages={11887-11899},
abstract={In recent years, the Smart City concept has become popular for its promise to improve the quality of life of urban citizens. The concept involves multiple disciplines, such as Smart health care, Smart transportation, and Smart community. Most services in Smart Cities, especially in the Smart healthcare domain, require the real-time sharing, processing, and analyzing of Big Healthcare Data for intelligent decision making. Therefore, a strong wireless and mobile communication infrastructure is necessary to connect and access Smart healthcare services, people, and sensors seamlessly, anywhere at any time. In this scenario, mobile cloud computing (MCC) can play a vital role by offloading Big Healthcare Data related tasks, such as sharing, processing, and analysis, from mobile applications to cloud resources, ensuring quality of service demands of end users. Such resource migration, which is also termed virtual machine (VM) migration, is effective in the Smart healthcare scenario in Smart Cities. In this paper, we propose an ant colony optimization-based joint VM migration model for a heterogeneous, MCC-based Smart Healthcare system in Smart City environment. In this model, the user’s mobility and provisioned VM resources in the cloud address the VM migration problem. We also present a thorough performance evaluation to investigate the effectiveness of our proposed model compared with the state-of-the-art approaches.},
keywords={Cloud computing;Medical services;Mobile communication;Smart cities;Servers;Real-time systems;Quality of service;Smart health care;smart city;big data;quality of service (QoS);virtual machine migration;ant colony optimization},
doi={10.1109/ACCESS.2017.2707439},
ISSN={2169-3536},
month={},}
@ARTICLE{9730861,
author={Zhang, Yu-Hang and Wen, Chang and Zhang, Min and Xie, Kai and He, Jian-Biao},
journal={IEEE Access},
title={Fast 3D Visualization of Massive Geological Data Based on Clustering Index Fusion},
year={2022},
volume={10},
number={},
pages={28821-28831},
abstract={With the development of $3D$ visualization technology, the amount of geological data information is increasing, and the interactive display of big data faces severe challenges. Because traditional volume rendering methods cannot entirely load large-scale data into the memory owing to hardware limitations, a visualization method based on variational deep embedding clustering fusion Hilbert R-tree is proposed to solve slow display and stuttering issues when rendering massive geological data. By constructing an efficient data index structure, deep clustering algorithms and space-filling curves can be integrated into the data structure to improve the indexing efficiency. In addition, this method combines time forecasting, data scheduling, and loading modules to improve the accuracy and real-time data display rate, thereby improving the stability of $3D$ visualization of large-scale geological data. This method uses real geological data as the experimental dataset, comparing and analyzing the existing index structure and time-series prediction method. The experimental results indicate that when comparing the index of the variational deep embedded clustering-Hilbert R-tree ( $VDEC-HRT$ ) with that of the K-means Hilbert R-tree ( $KHRT$ ), the time required is reduced by 55.67%, the viewpoint prediction correctness of the proposed method is improved by 22.7% compared with Lagrange interpolation algorithm. And the overall rendering performance and quality of the system achieve the expected results. Ours experiments prove the feasibility and effectiveness of the proposed scheme in the visualization of large-scale geological data.},
keywords={Data visualization;Rendering (computer graphics);Clustering algorithms;Prediction algorithms;Predictive models;Geology;Data models;3D visualization of massive data;deep learning;Hilbert R-tree;deep clustering;time-series forecasting;view frustum culling},
doi={10.1109/ACCESS.2022.3157823},
ISSN={2169-3536},
month={},}
@ARTICLE{8674542,
author={Santos, Joelson Antônio dos and Syed, Talat Iqbal and Naldi, Murilo C. and Campello, Ricardo J. G. B. and Sander, Joerg},
journal={IEEE Transactions on Big Data},
title={Hierarchical Density-Based Clustering Using MapReduce},
year={2021},
volume={7},
number={1},
pages={102-114},
abstract={Hierarchical density-based clustering is a powerful tool for exploratory data analysis, which can play an important role in the understanding and organization of datasets. However, its applicability to large datasets is limited because the computational complexity of hierarchical clustering methods has a quadratic lower bound in the number of objects to be clustered. MapReduce is a popular programming model to speed up data mining and machine learning algorithms operating on large, possibly distributed datasets. In the literature, there have been attempts to parallelize algorithms such as Single-Linkage, which in principle can also be extended to the broader scope of hierarchical density-based clustering, but hierarchical clustering algorithms are inherently difficult to parallelize with MapReduce. In this paper, we discuss why adapting previous approaches to parallelize Single-Linkage clustering using MapReduce leads to very inefficient solutions when one wants to compute density-based clustering hierarchies. Preliminarily, we discuss one such solution, which is based on an exact, yet very computationally demanding, random blocks parallelization scheme. To be able to efficiently apply hierarchical density-based clustering to large datasets using MapReduce, we then propose a different parallelization scheme that computes an approximate clustering hierarchy based on a much faster, recursive sampling approach. This approach is based on HDBSCAN*, the state-of-the-art hierarchical density-based clustering algorithm, combined with a data summarization technique called data bubbles. The proposed method is evaluated in terms of both runtime and quality of the approximation on a number of datasets, showing its effectiveness and scalability.},
keywords={Clustering algorithms;Partitioning algorithms;Programming;Data models;Machine learning algorithms;Big Data;Computational modeling;Density-based hierarchical clustering;MapReduce;big data},
doi={10.1109/TBDATA.2019.2907624},
ISSN={2332-7790},
month={March},}
@ARTICLE{9471831,
author={Zhang, Fujie and Yao, Degui and Zhang, Xiaofei and Hu, Zhouming and Zhu, Wenjun and Ju, Yun},
journal={IEEE Access},
title={Fault Judgment of Transmission Cable Based on Multi-Channel Data Fusion and Transfer Learning},
year={2021},
volume={9},
number={},
pages={98161-98168},
abstract={Non-intrusive transmission cable monitoring is the latest advanced measurement technology for smart grids. It only samples the voltage on a certain part of the transmission cable, and uses intelligent algorithms to identify the quality, which has obvious advantages of low construction and maintenance costs. This paper established a model based on multi-channel data fusion and transfer learning to classify the quality of transmission cable. First, we used the ANSYS Maxwell simulation platform to obtain ten kinds of specific fault data, which solved the time cost of manual labeling. Then, we performed multi-channel data fusion on the original data, which strengthened the expression of important features and was more conducive to the training of the model. Next, we used Depthwise Separable Convolution (DSC) to speed up the learning of the model, and improve the accuracy of the classification. Finally, we transferred the model trained with simulation data into the real scene, realized the transfer from multi classes to two classes, the effectiveness was proved in experiments. The accuracy of the model built in the article to classify the quality of the transmission cables is 98.1%.},
keywords={Data models;Communication cables;Transfer learning;Data integration;Convolution;Training;Analytical models;Quality of transmission cable;transfer learning;data fusion},
doi={10.1109/ACCESS.2021.3094231},
ISSN={2169-3536},
month={},}
@ARTICLE{9486501,
author={Sagan, Vasit and Maimaitijiang, Maitiniyazi and Paheding, Sidike and Bhadra, Sourav and Gosselin, Nichole and Burnette, Max and Demieville, Jeffrey and Hartling, Sean and LeBauer, David and Newcomb, Maria and Pauli, Duke and Peterson, Kyle T. and Shakoor, Nadia and Stylianou, Abby and Zender, Charles S. and Mockler, Todd C.},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={Data-Driven Artificial Intelligence for Calibration of Hyperspectral Big Data},
year={2022},
volume={60},
number={},
pages={1-20},
abstract={Near-earth hyperspectral big data present both huge opportunities and challenges for spurring developments in agriculture and high-throughput plant phenotyping and breeding. In this article, we present data-driven approaches to address the calibration challenges for utilizing near-earth hyperspectral data for agriculture. A data-driven, fully automated calibration workflow that includes a suite of robust algorithms for radiometric calibration, bidirectional reflectance distribution function (BRDF) correction and reflectance normalization, soil and shadow masking, and image quality assessments was developed. An empirical method that utilizes predetermined models between camera photon counts (digital numbers) and downwelling irradiance measurements for each spectral band was established to perform radiometric calibration. A kernel-driven semiempirical BRDF correction method based on the Ross Thick-Li Sparse (RTLS) model was used to normalize the data for both changes in solar elevation and sensor view angle differences attributed to pixel location within the field of view. Following rigorous radiometric and BRDF corrections, novel rule-based methods were developed to conduct automatic soil removal; and a newly proposed approach was used for image quality assessment; additionally, shadow masking and plot-level feature extraction were carried out. Our results show that the automated calibration, processing, storage, and analysis pipeline developed in this work can effectively handle massive amounts of hyperspectral data and address the urgent challenges related to the production of sustainable bioenergy and food crops, targeting methods to accelerate plant breeding for improving yield and biomass traits.},
keywords={Calibration;Hyperspectral imaging;Radiometry;Atmospheric measurements;Atmospheric modeling;Artificial intelligence;Cameras;Bidirectional reflectance distribution function (BRDF) correction;high-throughput phenotyping;image quality assessment;shadow compensation;soil removal},
doi={10.1109/TGRS.2021.3091409},
ISSN={1558-0644},
month={},}
@ARTICLE{9727160,
author={Bigdeli, Mohammad and Farahmand, Shahrokh and Abolhassani, Bahman and Nguyen, Ha H.},
journal={IEEE Access},
title={Globally Optimal Resource Allocation and Time Scheduling in Downlink Cognitive CRAN Favoring Big Data Requests},
year={2022},
volume={10},
number={},
pages={27504-27521},
abstract={This paper is concerned with a cognitive cloud radio access network (CRAN) with a special attention to efficient and reliable downlink transmission of big data for secondary users (SUs). Existing approaches either try to maximize the number of accepted SUs or the sum data rate of accepted SUs. The first approach unfairly favors users with small data requests, whereas the second approach allocates most resources to users with better channel conditions. In contrast, this paper develops a novel approach that favors big data requests while simultaneously maintaining a certain degree of fairness among SUs. To this end, we first introduce a novel objective function that allows us to jointly optimize deadline-aware time scheduling, spectrum allocation, SU selection, and remote radio head (RRH) allocation for SUs. Second, we demonstrate that finding the global optimum solution entails the enumeration of all colorful independent sets on a generalized interval graph, which is known to be NP-hard. Third, we propose a dynamic programming (DP) approach, which yields the global optimum solution at a reduced computational cost. Fourth, we analyze the complexity of the proposed DP approach and assess its performance against existing baseline algorithms. Simulation results reveal that our solution favors big data users while incurring only a small degradation in the fairness index. Our proposed solution is practical for small-to-medium size networks. Furthermore, it offers an optimum benchmark for any new sub-optimal low-complexity algorithm.},
keywords={Resource management;Big Data;Processor scheduling;Downlink;Interference;Delays;Clustering algorithms;Time scheduling;resource allocation;user selection;cloud radio access network (CRAN);big data transmission},
doi={10.1109/ACCESS.2022.3156584},
ISSN={2169-3536},
month={},}
@ARTICLE{8825796,
author={Jiang, Congfeng and Qiu, Yeliang and Gao, Honghao and Fan, Tiantian and Li, Kangkang and Wan, Jian},
journal={IEEE Access},
title={An Edge Computing Platform for Intelligent Operational Monitoring in Internet Data Centers},
year={2019},
volume={7},
number={},
pages={133375-133387},
abstract={The increasing demand for cloud-based services, such as big data analytics and online e-commerce, leads to rapid growth of large-scale internet data centers. In order to provide highly reliable, cost effective, and high quality cloud services, data centers are equipped with sensors to monitor the operational states of infrastructure hardware, such as servers, storage arrays, networking devices, and computer room air conditioning systems. However, such coarse grained monitoring cannot provide fine grained real time information for resource multiplexing and job scheduling. Moreover, the monitoring of node level power consumption plays an important role in the optimization of workload placement and energy efficiency in data centers. In this paper, we propose an edge computing platform for intelligent operational monitoring in data centers. The platform integrates wireless sensors and on-board built-in sensors to collect data during the operation and maintenance of data centers. Using logical functions, we divide the data center clusters into grids, and then deploy wireless sensors and edge servers in each grid. As such, data processing on edge servers can reduce the latency in data transmission to central clouds and thereby enhance the real time resource mapping decisions in data centers. In addition, the proposed platform also provides predictions of resource utilization, workload characteristics, and hardware health trends in data centers.},
keywords={Data centers;Monitoring;Sensors;Cloud computing;Servers;Edge computing;Energy consumption;Edge computing;data centers;monitoring;energy efficiency;intelligent operation},
doi={10.1109/ACCESS.2019.2939614},
ISSN={2169-3536},
month={},}
@ARTICLE{9354632,
author={Yang, Jingjian and Ma, Hongyan and Dou, Jiaming and Guo, Rong},
journal={IEEE Access},
title={Harmonic Characteristics Data-Driven THD Prediction Method for LEDs Using MEA-GRNN and Improved-AdaBoost Algorithm},
year={2021},
volume={9},
number={},
pages={31297-31308},
abstract={Light-emitting Diode (LED) lamps have been widely used due to versatility and energy efficiency. However, LEDs are nonlinear loads, the massive usage will inject harmonics into the lighting system, which has influenced the power quality. Total Harmonic Distortion (THD) is an important parameter to evaluate the power quality, but the prediction of THD for LEDs is a challenging task. This paper addresses this issue by designing harmonic characteristics detection experiment and using artificial intelligence algorithm. Firstly, LED lamps with different driving circuits were tested, the relevant data of each harmonic were sampled and analyzed. Then, a THD prediction method based on an improved AdaBoost algorithm is proposed. In this method, a Generalized Regression Neural Network (GRNN) model is established, and its parameters are optimized by Mind Evolution Algorithm (MEA) to improve the search ability of GRNN. On this basis, the AdaBoost algorithm is utilized to integrate multiple MEA-GRNN individuals to form a strong predictor, which improves the generalization ability of the model. To avoid the integration failure caused by improper selection of threshold value, a sigmoid adaptive factor is added to improve the accuracy of AdaBoost algorithm. Finally, the Ada-MEA-GRNN model is trained and simulated with the LED harmonic data collected by the experiment. The simulation results show that the prediction accuracy of the proposed method is better than BP and GRNN, which can reach 95.48%. Meanwhile, even if the input dimension is reduced, the error is still small.},
keywords={Harmonic analysis;Lighting;Predictive models;LED lamps;Prediction algorithms;Power quality;Neurons;LED lamps;THD prediction;ensemble learning;mind evolution algorithm (MEA);generalized regression neural network (GRNN)},
doi={10.1109/ACCESS.2021.3059483},
ISSN={2169-3536},
month={},}
@ARTICLE{8811507,
author={Tao, Chuanqi and Gao, Jerry and Wang, Tiexin},
journal={IEEE Access},
title={Testing and Quality Validation for AI Software–Perspectives, Issues, and Practices},
year={2019},
volume={7},
number={},
pages={120164-120175},
abstract={With the fast growth of artificial intelligence and big data computing technologies, more and more software service systems have been developed using diverse machine learning models and technologies to make business and intelligent decisions based on their multimedia input to achieve intelligent features, such as image recognition, recommendation, decision making, prediction, etc. Nevertheless, there are increasing quality problems resulting in erroneous testing costs in enterprises and businesses. Existing work seldom discusses how to perform testing and quality validation for AI software. This paper focuses on quality validation for AI software function features. The paper provides our understanding of AI software testing for new features and requirements. In addition, current AI software testing categories are presented and different testing approaches are discussed. Moreover, test quality assessment and criteria analysis are illustrated. Furthermore, a practical study on quality validation for an image recognition system is performed through a metamorphic testing method. Study results show the feasibility and effectiveness of the approach.},
keywords={Artificial intelligence;Software;Software testing;Data models;Quality assurance;Big Data;AI software quality validation;AI testing;testing AI software},
doi={10.1109/ACCESS.2019.2937107},
ISSN={2169-3536},
month={},}
@ARTICLE{8693792,
author={Chen, Yuantao and Wang, Jin and Chen, Xi and Zhu, Mingwei and Yang, Kai and Wang, Zhi and Xia, Runlong},
journal={IEEE Access},
title={Notice of Violation of IEEE Publication Principles: Single-Image Super-Resolution Algorithm Based on Structural Self-Similarity and Deformation Block Features},
year={2019},
volume={7},
number={},
pages={58791-58801},
abstract={To solve the problem of insufficient sample resources and poor noise immunity in single-image super-resolution (SR) restoration procedure, the paper has proposed the single-image SR algorithm based on structural self-similarity and deformation block features (SSDBF). First, the proposed method constructs a scale model, expands the search space as much as possible, and overcomes the shortcomings caused by the lack of a single-image SR training sample; Second, the limited internal dictionary size is increased by the geometric deformation of the sample block; Finally, in order to improve the anti-noise performance of the reconstructed picture, a group sparse learning dictionary is used to reconstruct the pending image. The experimental results show that, compared with state-of-the-art algorithms such as bicubic interpolation (BI), sparse coding (SC), deep recursive convolutional network (DRCN), multi-scale deep SR network (MDSR), super-resolution convolutional neural network (SRCNN) and second-order directional total generalized variation (DTGV). The SR images with more subjective visual effects and higher objective evaluation can be obtained through the proposed method. Compared with existing algorithms, the structural network converges more rapidly, the image edge and texture reconstruction effects are obviously improved, and the image quality evaluation, such as peak signal-noise ratio (PSNR), root mean square error (RMSE), and structural similarity (SSIM), are also superior and popular in image evaluation.},
keywords={},
doi={10.1109/ACCESS.2019.2911892},
ISSN={2169-3536},
month={},}
@ARTICLE{9644473,
author={Khan, Inam Ullah and Javeid, Nadeem and Taylor, C. James and Gamage, Kelum A. A. and Ma, Xiandong},
journal={IEEE Transactions on Smart Grid},
title={A Stacked Machine and Deep Learning-Based Approach for Analysing Electricity Theft in Smart Grids},
year={2022},
volume={13},
number={2},
pages={1633-1644},
abstract={The role of electricity theft detection (ETD) is critical to maintain cost-efficiency in smart grids. However, existing methods for theft detection can struggle to handle large electricity consumption datasets because of missing values, data variance and nonlinear data relationship problems, and there is a lack of integrated infrastructure for coordinating electricity load data analysis procedures. To help address these problems, a simple yet effective ETD model is developed. Three modules are combined into the proposed model. The first module deploys a combination of data imputation, outlier handling, normalization and class balancing algorithms, to enhance the time series characteristics and generate better quality data for improved training and learning by the classifiers. Three different machine learning (ML) methods, which are uncorrelated and skillful on the problem in different ways, are employed as the base learning model. Finally, a recently developed deep learning approach, namely a temporal convolutional network (TCN), is used to ensemble the outputs of the ML algorithms for improved classification accuracy. Experimental results confirm that the proposed framework yields a highly-accurate, robust classification performance, in comparison to other well-established machine and deep learning models and thus can be a practical tool for electricity theft detection in industrial applications.},
keywords={Support vector machines;Data models;Task analysis;Training;Radio frequency;Computational modeling;Smart grids;Electricity theft detection;big data;preprocessing;data classification;smart grid},
doi={10.1109/TSG.2021.3134018},
ISSN={1949-3061},
month={March},}
@ARTICLE{9174979,
author={Ceci, Michelangelo and Corizzo, Roberto and Japkowicz, Nathalie and Mignone, Paolo and Pio, Gianvito},
journal={IEEE Access},
title={ECHAD: Embedding-Based Change Detection From Multivariate Time Series in Smart Grids},
year={2020},
volume={8},
number={},
pages={156053-156066},
abstract={Smart grids are power grids where clients may actively participate in energy production, storage and distribution. Smart grid management raises several challenges, including the possible changes and evolutions in terms of energy consumption and production, that must be taken into account in order to properly regulate the energy distribution. In this context, machine learning methods can be fruitfully adopted to support the analysis and to predict the behavior of smart grids, by exploiting the large amount of streaming data generated by sensor networks. In this article, we propose a novel change detection method, called ECHAD (Embedding-based CHAnge Detection), that leverages embedding techniques, one-class learning, and a dynamic detection approach that incrementally updates the learned model to reflect the new data distribution. Our experiments show that ECHAD achieves optimal performances on synthetic data representing challenging scenarios. Moreover, a qualitative analysis of the results obtained on real data of a real power grid reveals the quality of the change detection of ECHAD. Specifically, a comparison with state-of-the-art approaches shows the ability of ECHAD in identifying additional relevant changes, not detected by competitors, avoiding false positive detections.},
keywords={Smart grids;Time series analysis;Task analysis;Feature extraction;Data models;Change detection algorithms;smart grids;one-class learning;neural networks;embedding},
doi={10.1109/ACCESS.2020.3019095},
ISSN={2169-3536},
month={},}
@ARTICLE{8944072,
author={Capizzi, Giacomo and Coco, Salvatore and Lo Sciuto, Grazia and Napoli, Christian and Hołubowski, Waldemar},
journal={IEEE Access},
title={An Entropy Evaluation Algorithm to Improve Transmission Efficiency of Compressed Data in Pervasive Healthcare Mobile Sensor Networks},
year={2020},
volume={8},
number={},
pages={4668-4678},
abstract={Data transmission is the most critical operation for mobile sensors networks in term of energy waste. Particularly in pervasive healthcare sensors network it is paramount to preserve the quality of service also by means of energy saving policies. Communication and data transmission are among the most critical operation for such devises in term of energy waste. In this paper we present a novel approach to increase battery life-span by means of shorter transmission due to data compression. On the other hand, since this latter operation has a non-neglectable energy cost, we developed a compression efficiency estimator based on the evaluation of the absolute and relative entropy. Such algorithm provides us with a fast mean for the evaluation of data compressibility. Since mobile wireless sensor networks are prone to battery discharge-related problems, such an evaluation can be used to improve the electrical efficiency of data communication. In facts the developed technique, due to its independence from the string or file length, is extremely robust both for small and big data files, as well as to evaluate whether or not to compress data before transmission. Since the proposed solution provides a quantitative analysis of the source's entropy and the related statistics, it has been implemented as a preprocessing step before transmission. A dynamic threshold defines whether or not to invoke a compression subroutine. Such a subroutine should be expected to greatly reduce the transmission length. On the other hand a data compression algorithm should be used only when the energy gain of the reduced transmission time is presumably greater than the energy used to run the compression software. In this paper we developed an automatic evaluation system in order to optimize the data transmission in mobile sensor networks, by compressing data only when this action is presumed to be energetically efficient. We tested the proposed algorithm by using the Canterbury Corpus as well as standard pictorial data as benchmark test. The implemented system has been proven to be time-inexpensive with respect to a compression algorithm. Finally the computational complexity of the proposed approach is virtually neglectable with respect to the compression and transmission routines themselves.},
keywords={Entropy;Data compression;Medical services;Data communication;Compression algorithms;Software algorithms;Batteries;Wireless sensor networks;data compression;entropy;quality of service;energy saving;quality prediction;differential information entropy},
doi={10.1109/ACCESS.2019.2962771},
ISSN={2169-3536},
month={},}
@ARTICLE{9467271,
author={Chen, Hongqian and Guan, Mengxi and Li, Hui},
journal={IEEE Access},
title={Air Quality Prediction Based on Integrated Dual LSTM Model},
year={2021},
volume={9},
number={},
pages={93285-93297},
abstract={Air quality prediction is an important reference for meteorological forecast and air controlling, but over fitting often occurs in prediction algorithms based on a single model. Aiming at the complexity of air quality prediction, a prediction method based on integrated dual LSTM (Long Short-Term Memory) model was proposed in this paper. Firstly, the Seq2Seq (Sequence to Sequence) technology is used to establish a single-factor prediction model which can obtain the predicted value of each component in air quality data, independently. Each component of air quality is regarded as time series data in the forecasting process. Then, the LSTM model with attention mechanism is used as the multi-factor prediction model. The influencing factors of air quality, like the data of neighboring stations and weather data, are considered in the model. Finally, XGBoosting (eXtreme Gradient Boosting) tree is used to integrate two models. The final prediction results can be obtained by accumulating the predicted values of the optimal subtree nodes. Through evaluation and analysis using five evaluation methods, the proposed method has better performance in terms of error and model expression power. Compared with other various models, the precision of prediction data has been greatly improved in our model.},
keywords={Air quality;Atmospheric modeling;Predictive models;Data models;Meteorology;Time series analysis;Forecasting;Air quality prediction;integrated dual model;LSTM model with attention mechanism;Seq2Seq technology;XGBoosting tree},
doi={10.1109/ACCESS.2021.3093430},
ISSN={2169-3536},
month={},}
@ARTICLE{9082667,
author={Ranjan, Navin and Bhandari, Sovit and Zhao, Hong Ping and Kim, Hoon and Khan, Pervez},
journal={IEEE Access},
title={City-Wide Traffic Congestion Prediction Based on CNN, LSTM and Transpose CNN},
year={2020},
volume={8},
number={},
pages={81606-81620},
abstract={Traffic congestion is a significant problem faced by large and growing cities that hurt the economy, commuters, and the environment. Forecasting the congestion level of a road network timely can prevent its formation and increase the efficiency and capacity of the road network. However, despite its importance, traffic congestion prediction is not a hot topic among the researcher and traffic engineers. It is due to the lack of high-quality city-wide traffic data and computationally efficient algorithms for traffic prediction. In this paper, we propose (i) an efficient and inexpensive city-wide data acquisition scheme by taking a snapshot of traffic congestion map from an open-source online web service; Seoul Transportation Operation and Information Service (TOPIS), and (ii) a hybrid neural network architecture formed by combing Convolutional Neural Network, Long Short-Term Memory, and Transpose Convolutional Neural Network to extract the spatial and temporal information from the input image to predict the network-wide congestion level. Our experiment shows that the proposed model can efficiently and effectively learn both spatial and temporal relationships for traffic congestion prediction. Our model outperforms two other deep neural networks (Auto-encoder and ConvLSTM) in terms of computational efficiency and prediction performance.},
keywords={Roads;Data models;Predictive models;Convolutional neural networks;Urban areas;Convolutional neural network;long short-term memory;partially convolutional neural network;spatiotemporal feature;traffic congestion forecasting;transport network},
doi={10.1109/ACCESS.2020.2991462},
ISSN={2169-3536},
month={},}
@ARTICLE{9110861,
author={Li, Mingwei and Li, Jinpeng and Wan, Shuangning and Chen, Hao and Liu, Chao},
journal={IEEE Access},
title={Causal Identification Based on Compressive Sensing of Air Pollutants Using Urban Big Data},
year={2020},
volume={8},
number={},
pages={109207-109216},
abstract={This study addresses the causal identification of air pollutants from surrounding cities affecting Beijing's air quality. A novel compressive sensing causality analysis (CS-Causality) method, which combines Granger causality analysis (GCA) and maximum correntropy criterion (MCC), is presented for efficient identification of the air pollutant causality between Beijing and surrounding cities. Firstly, taking the spatiotemporal correlation into consideration, the original data is mapped into low-dimensional space. Valid information is then obtained based on compressive sensing (CS), which can greatly reduce the dimensions of the data, thus decreasing the amount of data analysis required. Secondly, to analyze the causal relations, GCA, represented by the prediction from one time series to another, is extended to rule out “Non-Granger” causes of air pollutants in Beijing originating from its surrounding cities. Thirdly, the greatest impact on Beijing's air quality is confirmed based on MCC. Finally, the accuracy of these results is verified using the transfer entropy.},
keywords={Air pollution;Urban areas;Time series analysis;Compressed sensing;Correlation;Spatiotemporal phenomena;Granger causality analysis;maximum correntropy criterion;data compression;air pollutant},
doi={10.1109/ACCESS.2020.3000767},
ISSN={2169-3536},
month={},}
@ARTICLE{8478185,
author={Shen, Guojiang and Han, Xiao and Zhou, Junjie and Ruan, Zhongyuan and Pan, Qihong},
journal={IEEE Access},
title={Research on Intelligent Analysis and Depth Fusion of Multi-Source Traffic Data},
year={2018},
volume={6},
number={},
pages={59329-59335},
abstract={Intelligence transportation system (ITS) and vehicular networks have attracted the research community in the recent years which generate the “big data” in traffic. However, the collection and application of the big traffic data is limited by the privacy of people who generate data. Besides, data-driven-based ITS only needs information that could reflect one or more types of vehicles at specific intersections, sections, and road networks, rather than that of each individual vehicle. Overall, intelligent analysis and data fusion of multi-source traffic data play an important role to reduce the phenomenon of privacy disclosure and ensure the quality of data. As a result, a complete method of multi-source traffic data analyzing and processing is proposed in this paper, including the data analysis method based on the spatio-temporal regression model and the data fusion method using evidence theory based on the confidence tensor. Finally, the practical data is used to conform the ways proposed before. And not only do the results show that the implicit privacy information has been removed but also present a higher accuracy of the proceed data.},
keywords={Roads;Data analysis;Data privacy;Standards;Big Data;Data integration;Intelligent transportation system;multi-source traffic data;data analysis;data fusion;data privacy},
doi={10.1109/ACCESS.2018.2872805},
ISSN={2169-3536},
month={},}
@ARTICLE{9079499,
author={Iqbal, Naeem and Jamil, Faisal and Ahmad, Shabir and Kim, Dohyeun},
journal={IEEE Access},
title={Toward Effective Planning and Management Using Predictive Analytics Based on Rental Book Data of Academic Libraries},
year={2020},
volume={8},
number={},
pages={81978-81996},
abstract={Large scale data and predictive analytics are the most challenging tasks in the field of academic data mining. Academic libraries are a great source of information and knowledge to provide a wide range of services to meet end-user requirements. Due to the rapid changes in the educational environment and availability of huge library rental book data, it is required to utilize data mining and machine learning techniques in the context of the academic library to extract and analyze underlying knowledge from rental book data, which is important to facilitate library administration to drive better future decisions to improve and manage library resources effectively. These are the following resources, such as managing future demands of the library books, selection and arrangement of the books, operational efficiency, and also improve the quality of interaction between the library and end-users, etc. This work uses and analyzes a real dataset collected from the library of Jeju National University, the Republic of Korea. The dataset contains 2,211,413 rental book records including 173671 unique book records, 57203 unique number of the rental user, and 78 data parameters. In this paper, we propose a novel model to analyze and predict library rental book data to facilitates library administration in order to plan and manage library resources effectively and provide better services to end-users. The proposed model consists of two different modules; library data analysis and prediction modules. Firstly, we use data mining techniques to analyze and extract useful underlying patterns from library rental book data, which can lead to plan and manage library resources effectively. Secondly, a novel prediction model is proposed based on Deep Neural Network (DNN), Support Vector Regressor (SVR), and Random Forest (RF) to predict future usage of the academic libraries rental books. The performance results of the implemented regression models are evaluated in terms of MAE, MSE, and RMSE. In this paper, it is found that the DNN model performs significantly better than SVR and RF. The experimentation results show that the proposed model improves the future usage of library books to facilitate library administration to plan and manage library resources effectively. Based on the proposed model results, the academic library administration can easily plan and manage resources effectively to provide quality services to end-users.},
keywords={Libraries;Data mining;Data models;Predictive models;Analytical models;Predictive analytics;Machine learning;Data mining;academic libraries;big data;machine learning;predictive analysis},
doi={10.1109/ACCESS.2020.2990765},
ISSN={2169-3536},
month={},}
@ARTICLE{9508412,
author={Dou, Jiaming and Ma, Hongyan and Yang, Jingjian and Zhang, Yingda and Guo, Rong},
journal={IEEE Access},
title={An Improved Power Quality Evaluation for LED Lamp Based on G1-Entropy Method},
year={2021},
volume={9},
number={},
pages={111171-111180},
abstract={Nowadays, light emitting diode (LED) lamps have been widely utilized for lighting system due to its low-energy consumption. The harmonic emission standard is ignored by most of the manufacturers, high harmonic current will increase harmonic injection and cause fire risk. Existing research focuses on investigating harmonic emissions from several specific LED drivers, but a systematic evaluation approach is not given. The contribution of this paper proposed a LED harmonic evaluation in the management view, which can evaluate the harmonics of the LED lamps, accelerate the elimination of inferior LED lamps, and improve the power quality of distribution network. The evaluation approach combines G1 method and entropy method, which can make the weighting more scientific and rational. An evaluation model is established by collecting data, then the G1-entropy method is used to calculate the weights of harmonic characteristics in this model. Finally, we analyze and discuss the results, a specific evaluation approach is proposed, which can thoroughly and accurately represent the harmonic characteristics of LED lamps.},
keywords={Harmonic analysis;LED lamps;Power system harmonics;Lighting;Integrated circuit modeling;Buildings;Entropy;Power quality;evaluation approach;light-emitting diode lamp;sequential analysis method;entropy weighting method},
doi={10.1109/ACCESS.2021.3103052},
ISSN={2169-3536},
month={},}
@ARTICLE{9104998,
author={Yang, Qiming and Chao, Hongyang and Nguyen, Dan and Jiang, Steve},
journal={IEEE Access},
title={Mining Domain Knowledge: Improved Framework Towards Automatically Standardizing Anatomical Structure Nomenclature in Radiotherapy},
year={2020},
volume={8},
number={},
pages={105286-105300},
abstract={The automatic standardization of nomenclature for anatomical structures in radiotherapy (RT) clinical data is a critical prerequisite for data curation and data-driven research in the era of big data and artificial intelligence, but it is currently an unmet need. Existing methods either cannot handle cross-institutional datasets or suffer from heavy imbalance and poor-quality delineation in clinical RT datasets. To solve these problems, we propose an automated structure nomenclature standardization framework, 3D Non-local Network with Voting (3DNNV). This framework consists of an improved data processing strategy, namely, adaptive sampling and adaptive cropping (ASAC) with voting, and an optimized feature extraction module. The framework simulates clinicians' domain knowledge and recognition mechanisms to identify small-volume organs at risk (OARs) with heavily imbalanced data better than other methods. We used partial data from an open-source head-and-neck cancer dataset to train the model, then tested the model on three cross-institutional datasets to demonstrate its generalizability. 3DNNV outperformed the baseline model, achieving higher average true positive rates (TPR) over all categories on the three test datasets (+8.27%, +2.39%, and +5.53%, respectively). More importantly, the 3DNNV outperformed the baseline on the test dataset, 28.63% to 91.17%, in terms of F1 score for a small-volume OAR with only 9 training samples. The results show that 3DNNV can be applied to identify OARs, even error-prone ones. Furthermore, we discussed the limitations and applicability of the framework in practical scenarios. The framework we developed can assist in standardizing structure nomenclature to facilitate data-driven clinical research in cancer radiotherapy.},
keywords={Standardization;Semantics;Feature extraction;Task analysis;Data mining;Data processing;Data models;Nomenclature standardization;radiotherapy;deep learning;3D classification;voting},
doi={10.1109/ACCESS.2020.2999079},
ISSN={2169-3536},
month={},}
@ARTICLE{8423621,
author={Babar, Muhammad and Khan, Fazlullah and Iqbal, Waseem and Yahya, Abid and Arif, Fahim and Tan, Zhiyuan and Chuma, Joseph M.},
journal={IEEE Access},
title={A Secured Data Management Scheme for Smart Societies in Industrial Internet of Things Environment},
year={2018},
volume={6},
number={},
pages={43088-43099},
abstract={Smart societies have an increasing demand for quality-oriented services and infrastructure in an industrial Internet of Things (IIoT) paradigm. Smart urbanization faces numerous challenges. Among them, secured energy demand-side management (DSM) is of particular concern. The IIoT renders the industrial systems to malware, cyberattacks, and other security risks. The IIoT with the amalgamation of big data analytics can provide efficient solutions to such challenges. This paper proposes a secured and trusted multilayered DSM engine for a smart social society using IIoT-based big data analytics. The major objective is to provide a generic secured solution for smart societies in IIoT environment. The proposed engine uses a centralized approach to achieve optimum DSM over a home area network. To enhance the security of this engine, a payload-based authentication scheme is utilized that relies on a lightweight handshake mechanism. Our proposed method utilizes the lightweight features of the constrained application protocol to facilitate the clients in monitoring various resources residing over the server in an energy-efficient manner. In addition, data streams are processed using big data analytics with MapReduce parallel processing. The proposed authentication approach is evaluated using NetDuino Plus 2 boards that yield a lower connection overhead, memory consumption, response time, and a robust defense against various malicious attacks. On the other hand, our data processing approach is tested on reliable datasets using Apache Hadoop with Apache Spark to verify the proposed DMS engine. The test results reveal that the proposed architecture offers valuable insights into the smart social societies in the context of IIoT.},
keywords={Big Data;Internet of Things;Engines;Authentication;Protocols;Real-time systems;Demand side management;home area network;industrial Internet of Things;security;smart societies;trust},
doi={10.1109/ACCESS.2018.2861421},
ISSN={2169-3536},
month={},}
@ARTICLE{9739736,
author={Baoyu, Li and Guoxing, Li and Guiyu, Wang and Guofeng, Zhang and Man, Yang},
journal={IEEE Access},
title={Research on CART Model of Mass Concrete Temperature Prediction Based on Big Data Processing Technology},
year={2022},
volume={10},
number={},
pages={32845-32854},
abstract={Due to the influence of temperature changes or temperature gradients in the construction process of mass concrete, temperature cracks will occur in the concrete. In order to achieve a reasonable prediction of the temperature change of the mass concrete during the construction process and accurately obtain the temperature change trend, this paper attempts to construct a CART prediction model based on the big data processing technology based on the characteristics of the temperature change of the mass concrete. This paper introduces in detail how to use data processing methods such as outlier identification, missing value filling and random error elimination to improve data quality, as well as the method for constructing the CART prediction model, and combines engineering examples to demonstrate the feasibility of the model method. The results show that the model and method can better predict the temperature change of mass concrete. It has high prediction accuracy and can provide necessary guidance for practical engineering.},
keywords={Temperature sensors;Temperature distribution;Temperature measurement;Temperature control;Predictive models;Data models;Cooling;Mass concrete temperature;big data processing technology;CART prediction model},
doi={10.1109/ACCESS.2022.3161556},
ISSN={2169-3536},
month={},}
@ARTICLE{9364983,
author={Ma, Rui and Li, Jianqiang and Xing, Baohui and Zhao, Yuanyuan and Liu, Yuwen and Yan, Chao and Yin, Hang},
journal={IEEE Access},
title={A Novel Similar Player Clustering Method With Privacy Preservation for Sport Performance Evaluation in Cloud},
year={2021},
volume={9},
number={},
pages={37255-37261},
abstract={With the ever-increasing popularity of sports and health ideas, people are paying more attentions to gaining high-quality healthy life through various taking various sport items or exercises. Through observing and analyzing the past sport exercise score records, we can cluster the players into different categories, each of which share the same or similar sport preferences or performances. However, the sport exercise score records are often massive and often stored in different cloud platforms, which raise a big difficulty for time-efficient player clustering. Furthermore, the sport exercise score records are a kind of privacy for most players; therefore, it is often not rational or legal to release these sensitive data to the public for similar player clustering purpose. Considering the above two issues, we use SimHash, a kind of privacy-aware approximate neighbor search technique, for similar player clustering by analyzing the sport exercise score records distributed across different cloud platforms. Thus, we can realize privacy-aware similar player clustering through SimHash. At last, we provide a set of experiments to validate the advantages of our proposed privacy-aware similar player clustering algorithm. Reported experimental results show the effectiveness of our proposal in remedying the big data volume and privacy concerns in player clustering based on sport exercise score records.},
keywords={Sports;Privacy;Data privacy;Indexes;Collaborative filtering;Big Data;Encoding;Similar player clustering;sport exercise score records;SimHash;privacy;big data;cloud platform},
doi={10.1109/ACCESS.2021.3062735},
ISSN={2169-3536},
month={},}
@ARTICLE{9627153,
author={Fang, Na and Fang, Xianwen and Lu, Ke and Asare, Esther},
journal={IEEE Access},
title={Online Incremental Mining Based on Trusted Behavior Interval},
year={2021},
volume={9},
number={},
pages={158562-158573},
abstract={Incremental mining improves the quality of process mining by analyzing the differences between event logs and a reference model to obtain valuable information to update the reference model. Existing incremental mining methods focus on offline logs by setting thresholds for analysis, which limits process mining efforts by the domain knowledge, log completeness, and business completion time. Aiming at these problems, a real-time incremental mining algorithm based on the trusted behavior interval is proposed to analyze online event streams for updating the reference model. First, a clustering technique to analyze an existing reference model selects the core structure of the model and calculates the trusted behavior interval. Then, the behavioral and structural relationships between the online event streams and the reference model are analyzed to obtain a valid candidate set. Based on this set, an incremental update algorithm is proposed to optimize the model structure to achieve an online dynamic update of the reference model. The proposed algorithm is implemented in PM4PY and Scikit-learn frameworks; a reasonable number of clusters is determined using the elbow method and validated with artificial and real data. Experimental results show that the algorithm improves the efficiency of incremental mining and enhances the quality of the model with both complete and incomplete data.},
keywords={Data mining;Analytical models;Data models;Heuristic algorithms;Clustering algorithms;Business;Task analysis;Online incremental mining;trusted behavior interval;event stream;clustering;process mining},
doi={10.1109/ACCESS.2021.3130758},
ISSN={2169-3536},
month={},}
@ARTICLE{8880549,
author={Feng, Jie and Chen, Hongbin},
journal={IEEE Access},
title={Repairing Confident Information Coverage Holes for Big Data Collection in Large-Scale Heterogeneous Wireless Sensor Networks},
year={2019},
volume={7},
number={},
pages={155347-155360},
abstract={The quality of service (QoS) and lifetime of wireless sensor networks (WSNs) are severely degraded by coverage holes generated by random deployment or battery exhaustion of sensors. This work firstly introduces a novel confident information coverage CIC) model to dramatically reduce the density of sensor nodes and accurately detect confident information coverage holes (CICHs). Then, the problem of repairing confident information coverage holes (RCICHs) for big data collection in a large-scale heterogeneous WSN (LS-HWSN) which widely spreads over a geographic area with thousands of stationary sensor nodes and mobile sensor nodes is formulated, called as RCICH problem, which is to effectively repair CICHs considering that the transmitted data velocity of sensor nodes is different. Furthermore, we prove it to be NP-completeness. The target of the problem is to find a subset of mobile sensor nodes from all mobile sensor nodes while minimizing the amount of lost throughputs LTs) of all dispatched mobile sensor nodes or maximizing the amount of repairing transmission times (RTTs) for all dispatched mobile sensor nodes, with different objectives. Finally, based on the CIC model and the data-centric perspective, two heuristic schemes including a centralized dispatch scheme and a distributed dispatch scheme are proposed to effectively solve the RCICH problem. Simulation results show that the proposed schemes effectively repair CICHs while increasing the QoS and lifetime of the LS-HWSN with the topology control of a fan-shaped clustering (FSC) protocol.},
keywords={Wireless sensor networks;Sensor phenomena and characterization;Big Data;Energy consumption;Quality of service;Maintenance engineering;Large-scale heterogeneous wireless sensor networks;big data collection;confident information coverage;repairing confident information coverage holes},
doi={10.1109/ACCESS.2019.2949136},
ISSN={2169-3536},
month={},}
@ARTICLE{9006895,
author={Shin, Hyunkyung and Lee, Jaeho},
journal={IEEE Access},
title={Temporal Impulse of Traffic Accidents in South Korea},
year={2020},
volume={8},
number={},
pages={38380-38390},
abstract={With the emergence of urban computing technology, the development of smart cities has gained much attention as a means to improve citizens' quality of life. As traffic accidents constitute a major problem that affects the quality of life, an effective solution to address this problem can significantly increase the level of intelligence of smart cities. This paper presents the development of a mathematical model for accurate analysis of big data to promote the effectiveness of policy decisions, thereby largely advancing the intelligent transportation systems (ITS) of smart cities. Temporal impulse was designed as a novel and measurable quantity to analyze traffic accidents by identifying the hidden patterns, such as varying causes and diverging impacts of traffic accidents. Based on the big data produced by the South Korean National Police Agency, we analyzed traffic accidents over three years by applying the temporal impulse. The research results suggested that the temporal impulse not only helped in identifying the varying influence of weather and driver conditions but also facilitated the establishment of sophisticated policies in the implementation of smart cities with the use of urban computing technology. As presented in the section VII, our simulation outputs indicated that our temporal model was predictive within the parameter space comprising driver's dynamic behaviors, day of the week, and environmental factors including weather, road surface condition, and road type.},
keywords={Accidents;Smart cities;Big Data;Mathematical model;Meteorology;Roads;Automobiles;Big data;traffic accident data;intelligent transportation systems;smart city technology;stochastic process;time series;temporal impulse},
doi={10.1109/ACCESS.2020.2975529},
ISSN={2169-3536},
month={},}