@article{WANG2021101371,
title = {Crowdsourcing the perceived urban built environment via social media: The case of underutilized land},
journal = {Advanced Engineering Informatics},
volume = {50},
pages = {101371},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101371},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001245},
author = {Yan Wang and Shangde Gao and Nan Li and Siyu Yu},
keywords = {Built environment, Crowdsourcing, Social media, Urban analytics, Underutilized land},
abstract = {Crowdsourcing the public’s perceptions of the built environment in real time enables more responsive and agile infrastructure and land use planning. Social media has emerged to be an effective platform for citizens, engineers, and planners to communicate opinions and feelings transparently. However, a comprehensive terminological resource of the perceived built environment (BE) for consistent data collection and a specified analytical framework are still lacking, particularly for different underutilized land issues. To fill this knowledge gap, we demonstrate a BE-specific term construction and expansion method specifically for collecting Twitter data and propose a Geo-Topic-Sentiment analytical framework for retrieving and analyzing relevant tweets. We conduct a demonstrative study on un(der)utilized land-related BE terms across ten metropolitan statistical areas in the U.S. Findings reveal spatial variations in contents and sentiments about underutilized land environments, and more localized efforts may be required to address specific land use issues across different urban contexts. The research demonstrates Twitter as a useful platform in crowdsourcing perceived BE and sentiments at fine temporal and spatial scales in a timely manner. It contributes to engineering informatics by investigating the role of social media in environmental planning and proposing integrated domain-specific data analytic approaches for engineering practices.}
}
@incollection{WANG2021125,
title = {Chapter 4 - Battery state estimation methods},
editor = {Shunli Wang and Yongcun Fan and Daniel-Ioan Stroe and Carlos Fernandez and Chunmei Yu and Wen Cao and Zonghai Chen},
booktitle = {Battery System Modeling},
publisher = {Elsevier},
pages = {125-156},
year = {2021},
isbn = {978-0-323-90472-8},
doi = {https://doi.org/10.1016/B978-0-323-90472-8.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323904728000019},
author = {Shunli Wang and Yongcun Fan and Daniel-Ioan Stroe and Carlos Fernandez and Chunmei Yu and Wen Cao and Zonghai Chen},
keywords = {State of charge, State of energy, State of power, State of health, Remaining useful life, Influencing factors, Extended Kalman filtering, Support vector machine, Neural network, Particle filtering},
abstract = {The battery state estimation is a very important task in its management system. The state of charge represents the battery’s remaining energy ratio after a period of use or a long period of disuse, which can reflect the battery life or the battery remaining use time. As for the battery operation, the state parameter reflects its working conditions. The estimation methods are described for the battery state estimation of different working conditions. Before the battery state estimation, the definition of its state parameters is conducted, including state of charge, state of energy, state of power, state of health, and remaining useful life. After that, the main state influencing factors are analyzed as well as algorithm fusion and comparison. The parameter measurement technology is then introduced into the balancing control theory analysis and temperature adjustment. For the estimation method analysis, the foundational methods are analyzed in advance, including open-circuit voltage and ampere hour integral. The smart algorithms are introduced such as extended Kalman filtering, support vector machine, neural network, and particle filtering.}
}
@article{SERVA2023109391,
title = {Testing two NIRs instruments to predict chicken breast meat quality and exploiting machine learning approaches to discriminate among genotypes and presence of myopathies},
journal = {Food Control},
volume = {144},
pages = {109391},
year = {2023},
issn = {0956-7135},
doi = {https://doi.org/10.1016/j.foodcont.2022.109391},
url = {https://www.sciencedirect.com/science/article/pii/S0956713522005849},
author = {Lorenzo Serva and Giorgio Marchesini and Marco Cullere and Rebecca Ricci and Antonella {Dalle Zotte}},
keywords = {Chicken, Genotype, Myopathies, Meat quality, Near infra-red spectroscopy, Machine learning},
abstract = {To discriminate among three poultry meat types (hybrid broiler, hybrid broiler affected by breast myopathies, and slow-growing native breed), and to predict the proximate and the amino acid (AA) composition of breast meat, two NIRs (Near-Infrared) instruments operating between 850 and 2500 nm coupled with chemometric algorithms and Machine Learning (ML) approaches, were tested. The Partial Least Square Discriminant Analysis was performed for genotype identification, resulting in a Mathew Correlation Coefficient (MCC) ranging from 0.61 to 1.00, according to the spectra pretreatments and instrument adopted. The Partial Least Square Regression allowed reaching a high cross-validation determination coefficient (R2cv) for crude protein (0.98) and ether extract (0.99), while only three AA (aspartic acid, alanine and methionine) reached R2cv > 0.55. The latter predictions were successfully used to discriminate between genotypes using Factorial Discriminant Analysis, with an MCC ranging from 0.67 to 0.95. Overall, both tested NIRs instruments allowed to determine the chemical composition of fresh and freeze-dried chicken meat. In this sense, a significant improvement of NIRs data interpretability was achieved thanks to the use of ML algorithms, as it was possible to discriminate the chemical composition of meat depending on the genetic group and the presence of breast myopathies.}
}
@article{WANG2021311,
title = {Estimating daily full-coverage near surface O3, CO, and NO2 concentrations at a high spatial resolution over China based on S5P-TROPOMI and GEOS-FP},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {175},
pages = {311-325},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621000897},
author = {Yuan Wang and Qiangqiang Yuan and Tongwen Li and Liye Zhu and Liangpei Zhang},
keywords = {Full-coverage, Near surface concentrations, Air quality, S5P-TROPOMI, GEOS-FP, COVID-19},
abstract = {The Near Surface Concentrations (NSC) of O3, CO, and NO2 are crucial worldwide indicators of air quality. However, current frameworks devised for the estimation of the NSC of O3, CO, and NO2 have defects, such as coarse spatial resolution and large missing coverage. To address this issue, this study aims to estimate the daily (~13:30 local time) full-coverage NSC of O3, CO, and NO2 at a high spatial resolution (0.05° for O3 and NO2; 0.07° for CO) over China by using datasets from S5P-TROPOMI and GEOS-FP. In specific, the light gradient boosting machine is employed to train the estimation models. Validation results show that the NSC of O3, CO, and NO2 are well estimated, with the R2s of 0.91, 0.71, and 0.83 for the sample-based cross validation, respectively. Meanwhile, the proposed framework achieves a satisfactory performance in comparison to the latest related works, as reflected by the estimation accuracy and spatial resolution. As for the mapping, the estimated results show coherent spatial distribution and can accurately grasp the seasonal characteristics of each air pollutant. Finally, the estimated results are utilized to analyze the temporal variations of O3, CO, and NO2 during the COrona VIrus Disease 2019 (COVID-19) lockdown in China, which is an extend application for adopting the proposed framework in air quality monitoring. Results show that the estimated NSC of O3, CO, and NO2 in 2020 present significant variations during different periods of the COVID-19 lockdown in China compared to last year. In addition, the variations in the NSC of O3, CO, and NO2 during the COVID-19 lockdown in China possibly result from restrictions in the anthropogenic activities.}
}
@article{SOHRABPOUR2021120480,
title = {Export sales forecasting using artificial intelligence},
journal = {Technological Forecasting and Social Change},
volume = {163},
pages = {120480},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120480},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520313068},
author = {Vahid Sohrabpour and Pejvak Oghazi and Reza Toorajipour and Ali Nazarpour},
keywords = {Causal forecasting, Modeling, Export sales forecast, Genetic programming, Artificial intelligence},
abstract = {Sales forecasting is important in production and supply chain management. It affects firms’ planning, strategy, marketing, logistics, warehousing and resource management. While traditional time series forecasting methods prevail in research and practice, they have several limitations. Causal forecasting methods are capable of predicting future sales behavior based on relationships between variables and not just past behavior and trends. This research proposes a framework for modeling and forecasting export sales using Genetic Programming, which is an artificial intelligence technique derived from the model of biological evolution. Analyzing an empirical case of an export company, an export sales forecasting model is suggested. Moreover, a sales forecast for a period of six weeks is conducted, the output of which is compared with the real sales data. Finally, a variable sensitivity analysis is presented for the causal forecasting model.}
}
@article{FISCHER2021101689,
title = {On the composition of the long tail of business processes: Implications from a process mining study},
journal = {Information Systems},
volume = {97},
pages = {101689},
year = {2021},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101689},
url = {https://www.sciencedirect.com/science/article/pii/S030643792030137X},
author = {Marcus Fischer and Adrian Hofmann and Florian Imgrund and Christian Janiesch and Axel Winkelmann},
keywords = {Business Process Management, Long tail of business processes, Process mining, Process performance indicators},
abstract = {Digital transformation forces companies to rethink their processes to meet current customer needs. Business Process Management (BPM) can provide the means to structure and tackle this change. However, most approaches to BPM face restrictions on the number of processes they can optimize at a time due to complexity and resource restrictions. Investigating this shortcoming, the concept of the long tail of business processes suggests a hybrid approach that entails managing important processes centrally, while incrementally improving the majority of processes at their place of execution. This study scrutinizes this observation as well as corresponding implications. First, we define a system of indicators to automatically prioritize processes based on execution data. Second, we use process mining to analyze processes from multiple companies to investigate the distribution of process value in terms of their process variants. Third, we examine the characteristics of the process variants contained in the short head and the long tail to derive and justify recommendations for their management. Our results suggest that the assumption of a long-tailed distribution holds across companies and indicators and also applies to the overall improvement potential of processes and their variants. Across all cases, process variants in the long tail were characterized by fewer customer contacts, lower execution frequencies, and a larger number of involved stakeholders, making them suitable candidates for distributed improvement}
}
@incollection{CARLETTO20214407,
title = {Chapter 81 - Agricultural data collection to minimize measurement error and maximize coverage},
editor = {Christopher B. Barrett and David R. Just},
series = {Handbook of Agricultural Economics},
publisher = {Elsevier},
volume = {5},
pages = {4407-4480},
year = {2021},
booktitle = {Handbook of Agricultural Economics},
issn = {1574-0072},
doi = {https://doi.org/10.1016/bs.hesagr.2021.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S1574007221000086},
author = {Calogero Carletto and Andrew Dillon and Alberto Zezza},
keywords = {Agriculture, Measurement error, Sampling error, Survey design, Data collection},
abstract = {Advances in agricultural data production provide ever-increasing opportunities for pushing the research frontier in agricultural economics and designing better agricultural policy. As new technologies present opportunities to create new and integrated data sources, researchers face tradeoffs in survey design that may reduce measurement error or increase coverage. In this chapter, we first review the econometric and survey methodology literatures that focus on the sources of measurement error and coverage bias in agricultural data collection. Second, we provide examples of how agricultural data structure affects testable empirical models. Finally, we review the challenges and opportunities offered by technological innovation to meet old and new data demands and address key empirical questions, focusing on the scalable data innovations of greatest potential impact for empirical methods and research.}
}
@article{GASHI2021103505,
title = {Dealing with missing usage data in defect prediction: A case study of a welding supplier},
journal = {Computers in Industry},
volume = {132},
pages = {103505},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103505},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001123},
author = {Milot Gashi and Patrick Ofner and Helmut Ennsbrunner and Stefan Thalmann},
keywords = {Defect prediction, End-of-line testing, Welding industry, Predictive maintenance, Multi-component systems},
abstract = {End-of-line (EoL) testing is performed to determine product quality by ensuring reliable performance. Even though low-quality products may pass EoL testing, they have a high probability of failure over time. Analyzing product usage data can help to improve EoL testing in this regard. However, current approaches do not consider usage data for this purpose. The major challenge for manufacturers is that they do not have access to comprehensive usage data for their products because customers are unwilling to provide usage data. However, manufacturers obtain some usage data from their sales and service departments i.e., contextual data. In this paper, we introduce an alternative approach to improving EoL testing when usage data from customers are missing. We discuss whether it is possible to predict low-quality products from EoL testing data when only contextual information is available (i.e., historical service data and location data of shipped products). We find that a simple, duration-based product usage threshold is sufficient to separate products affected by the production process (low-quality products) from those affected primarily by usage and environmental factors (long-term influence). Low-quality products could only be predicted by combining EoL data and contextual data. Additionally, we identify frequent patterns of maintained components to tackle the challenge of having limited data and promote user acceptance of our predictive model. Finally, we demonstrate our approach by conducting a case study in the welding industry. Our approach can identify frequent component failures and improve product reliability in countries with varying environmental conditions and rates of product usage. We expect that our findings will improve EoL testing protocols in welding and other industries while improving defect prediction models in general.}
}
@article{AN2021104776,
title = {Deep convolutional neural network for automatic fault recognition from 3D seismic datasets},
journal = {Computers & Geosciences},
volume = {153},
pages = {104776},
year = {2021},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.104776},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421000807},
author = {Yu An and Jiulin Guo and Qing Ye and Conrad Childs and John Walsh and Ruihai Dong},
keywords = {Fault recognition, Seismic interpretation, Deep learning, Computer vision, Image processing},
abstract = {With the explosive growth in seismic data acquisition and the successful application of deep convolutional neural networks (DCNN) to various image processing tasks within multidisciplinary fields, many researchers have begun to research DCNN based automatic seismic interpretation techniques. Due to the vast number of parameters considered in deep neural networks, deep learning methods usually require a large amount of data for training. However, collecting a large number of expert interpretations is very time consuming, so related research usually uses synthetic datasets and ignores the practical problems of field datasets. In this paper, we open-source a multi-gigabyte expert-labelled field dataset in response to the challenge of accessing large-scale expert-labelled field datasets. We show that 2D fault recognition within this dataset is an image segmentation or edge detection problem in the computer vision field, that can be expressed as a pixel-level fault/non-fault binary classification. Both types of DCNNs are compared, and we propose a novel fault recognition workflow, which involves processing and screening of seismic images and labels, training DCNNs and automatic numerical evaluation. We have also demonstrated for three case study datasets that effective image augmentation methods can reduce the required labelled crosslines while maintaining satisfactory performance. Our experimental results show that our workflow not only outperforms two state-of-the-art DCNN solutions but also achieves performance comparable to humans on an expert labelled image dataset, even predicting subtle faults that an expert interpreter did not annotate. We suggest that the proposed workflow could reduce the fault interpretation life cycle from months to hours and improve the quality, and define the confidence, of fault interpretation results.}
}
@article{KANG2023118814,
title = {Hierarchical level fault detection and diagnosis of ship engine systems},
journal = {Expert Systems with Applications},
volume = {213},
pages = {118814},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118814},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422018322},
author = {Young-Jin Kang and Yoojeong Noh and Min-Sung Jang and Sunyoung Park and Ju-Tae Kim},
keywords = {Optimal hierarchical clustering and dimension reduction, Dynamic thresholds, Domain knowledge, Sensor error, Abnormality labeling, Duel fuel marine engine},
abstract = {As smart ships are developed, research into engine management through remote monitoring and support from data collected at onshore control centers is being conducted. However, ship engine data collected from various sensors have high dimensions, large measurement errors, few labeled data, and insufficient amounts and quality of data, making it difficult to determine the condition of engines with complex failure modes under various operating environments.This study proposes a hierarchical level fault detection and diagnosis (HL-FDD) method that combines domain knowledge of ship engines and advanced data analysis techniques. The developed method extracts key features of reduced dimensions from the original variables (sensors) through an optimal hierarchical clustering and dimension-reduction model, allowing a hierarchy divided into the top (the entire system combining all features), middle (subsystems and feature sets), and bottom (components and sensors) levels. The reduced key features are used to generate robust regression models and dynamic thresholds (prediction intervals) according to the engine load, and dynamic thresholds determine whether the engine’s condition is normal or abnormal. The dynamic thresholds are able to automatically label abnormal conditions of the engine. Once anomalies are detected at the top level, the proposed method can sequentially search for data on features belonging to the middle and bottom hierarchies for detailed fault diagnosis to determine which engine subsystem or component(s) caused the engine fault(s). Actual data collected by ship operators verify the proposed method’s efficiency, reliability, and accuracy.}
}
@incollection{2021xxxi,
title = {Author Biographies},
editor = {David Baker and Lucy Ellis},
booktitle = {Future Directions in Digital Information},
publisher = {Chandos Publishing},
pages = {xxxi-xxxix},
year = {2021},
series = {Chandos Digital Information Review},
isbn = {978-0-12-822144-0},
doi = {https://doi.org/10.1016/B978-0-12-822144-0.09989-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221440099894}
}
@article{ASHRAF2021114913,
title = {Strategic-level performance enhancement of a 660 MWe supercritical power plant and emissions reduction by AI approach},
journal = {Energy Conversion and Management},
volume = {250},
pages = {114913},
year = {2021},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2021.114913},
url = {https://www.sciencedirect.com/science/article/pii/S019689042101089X},
author = {Waqar Muhammad Ashraf and Ghulam Moeen Uddin and Syed Muhammad Arafat and Jaroslaw Krzywanski and Wang Xiaonan},
keywords = {Combustion power plant, Fuel management, GHG emission reduction, Artificial intelligence},
abstract = {Power plant heat rate is a plant level performance parameter that indicates the economy of power production, equipment’s safety, and availability. In this paper, seven operating parameters, including the performance indices of integrated energy devices and the environmental conditions are incorporated for modeling the power plant heat rate by Artificial Neural Network (ANN), Support Vector Machine (SVM), and automated machine learning (AutoML) approach. The parametric significance order is determined by ANN and SVM-based Monte Carlo analytics and other machine learning-driven algorithms. Subsequently, the best-performing model is selected based on the external validation test and deployed for knowledge mining purposes. The improvement in the power plant heat rate by the parametric adjustment is achieved and subsequently, up to 3.12 percentage point (pp) increase in the thermal efficiency of the power plant is confirmed. Moreover, the fuel savings corresponding to the improved power plant heat rate are also calculated at three power generation modes. Their equivalence to an annual reduction in emissions is quantified. It is estimated that the accumulated reduction in CO2, SO2, CH4, N2O, and Hg emissions, i.e., 288.2 kilo tons / year (kt/y), can be achieved under 3.15% improvement in the power plant heat rate, corresponding to 75% power generation mode.}
}
@article{SONG2021633,
title = {High-throughput phenotyping: Breaking through the bottleneck in future crop breeding},
journal = {The Crop Journal},
volume = {9},
number = {3},
pages = {633-645},
year = {2021},
note = {Rice as a model crop: genetics, genomics and breeding},
issn = {2214-5141},
doi = {https://doi.org/10.1016/j.cj.2021.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S2214514121000829},
author = {Peng Song and Jinglu Wang and Xinyu Guo and Wanneng Yang and Chunjiang Zhao},
keywords = {High-throughput phenotyping, Crop breeding, Crop phenomics, Phenotyping platform, Data analysis},
abstract = {With the rapid development of genetic analysis techniques and crop population size, phenotyping has become the bottleneck restricting crop breeding. Breaking through this bottleneck will require phenomics, defined as the accurate, high-throughput acquisition and analysis of multi-dimensional phenotypes during crop growth at organism-wide levels, ranging from cells to organs, individual plants, plots, and fields. Here we offer an overview of crop phenomics research from technological and platform viewpoints at various scales, including microscopic, ground-based, and aerial phenotyping and phenotypic data analysis. We describe recent applications of high-throughput phenotyping platforms for abiotic/biotic stress and yield assessment. Finally, we discuss current challenges and offer perspectives on future phenomics research.}
}
@article{ABDELRAHMAN2021110885,
title = {Data science for building energy efficiency: A comprehensive text-mining driven review of scientific literature},
journal = {Energy and Buildings},
volume = {242},
pages = {110885},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.110885},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821001699},
author = {Mahmoud M. Abdelrahman and Sicheng Zhan and Clayton Miller and Adrian Chong},
keywords = {Reference mining, Natural language processing, Data science, Built environment, Building energy efficiency, Word embeddings},
abstract = {The ever-changing data science landscape is fueling innovation in the built environment context by providing new and more effective means of converting large raw data sets into value for professionals in the design, construction and operations of buildings. The literature developed due to this convergence has rapidly increased in recent years, making it difficult for traditional review approaches to cover all related papers. Therefore, this paper applies a natural language processing (NLP) method to provide an exhaustive and quantitative review.Approximately 30,000 scientific publications were retrieved from the Elsevier API to extract the relationship between data sources, data science techniques, and building energy efficiency applications across the life cycle of buildings. The text-mining and NLP analysis reveals that data sciences techniques are applied more for operation phase applications such as fault detection and diagnosis (FDD), while being under-explored in design and commissioning phases. In addition, it is pointed out that more data science techniques that are to be investigated for various applications. For example, generative adversarial networks (GANs) has potential in facilitating parametric design; transfer learning is a promising path to promoting the application of optimal building operation;}
}
@incollection{LIN2021375,
title = {Chapter 14 - Machine learning and in silico methods},
editor = {Stavros Kassinos and Per Bäckman and Joy Conway and Anthony J. Hickey},
booktitle = {Inhaled Medicines},
publisher = {Academic Press},
pages = {375-390},
year = {2021},
isbn = {978-0-12-814974-4},
doi = {https://doi.org/10.1016/B978-0-12-814974-4.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128149744000134},
author = {Ching-Long Lin and Eric A. Hoffman and Stavros Kassinos},
keywords = {Computed tomography, Image registration, Airways, Lung, Asthma, COPD, Machine learning, Deep learning, Clustering, Clusters, Computational fluid and particle dynamics},
abstract = {This chapter reviews the techniques and strategies for identifying subpopulations (clusters) characterized by distinct lung features via machine learning and using cluster information to guide in silico computational fluid and particle dynamics (CFPD) analysis for the design of future inhaled drug delivery methods. We first review the collaborative efforts of collecting imaging, genetic, clinical and biological data sets for large cohorts of healthy, asthma and chronic obstructive pulmonary disease (COPD) subjects to investigate the heterogeneous nature of lung disease. We then focus on imaging-based phenotyping due to its quantitative nature that sensitively captures lung structural and functional alternations at both local (segmental/parenchymal) and global (lobar/lung) scales. Machine learning is then applied to identify imaging clusters for asthma and COPD patients. We select cluster archetypes to perform CFPD analysis and use CFPD-derived variables to interpret the link between cluster-specific alterations and particle depositions in the human lungs. Finally, we discuss the prospect of employing machine learning, physics-based learning and deep learning complementarily toward precision medicine.}
}
@incollection{KAPLAN2023191,
title = {Chapter 9 - Data handling: ethical principles, guidelines, and recommended practices},
editor = {David J. Cox and Noor Y. Syed and Matthew T. Brodhead and Shawn P. Quigley},
booktitle = {Research Ethics in Behavior Analysis},
publisher = {Academic Press},
pages = {191-214},
year = {2023},
isbn = {978-0-323-90969-3},
doi = {https://doi.org/10.1016/B978-0-323-90969-3.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323909693000062},
author = {Brent A. Kaplan and Shawn P. Gilroy and W. Brady DeHart and Jeremiah M. Brown and Mikahil N. Koffarnus},
keywords = {Data collection, Data storage, Data validation, Recommended practices, Research ethics},
abstract = {Ethical data handling practices are an important—yet often overlooked—aspect of clinical work and research conduct. In this chapter, we briefly describe legal considerations, types of data commonly encountered by behavior analysts (e.g., highly sensitive data, potentially sensitive data, not very sensitive data), and considerations for data collection (e.g., paper, electronic) and storage. We then identify strategies for data validation, analysis, and dissemination. Behavior analysts in clinical and research settings should consider how their organizational data handling practices reflect their ethical duty toward their clients' or research participants' right to confidentiality. Specifically, behavior analysts should arrange environments that increase the likelihood of compliance with ethical data handling practices. For example, research or clinical staff should be trained on the proper use of data collection tools, data should be stored using secure and redundant systems, and work-flow systems should be designed to ensure data validation. Finally, data analysis methods should be free of bias, reproducible, and shareable, enabling other scientists to recreate analyses using deidentified datasets.}
}
@article{DU2021113721,
title = {Effects of the joint prevention and control of atmospheric pollution policy on air pollutants-A quantitative analysis of Chinese policy texts},
journal = {Journal of Environmental Management},
volume = {300},
pages = {113721},
year = {2021},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.113721},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721017837},
author = {Huibin Du and Yaqian Guo and Zhongguo Lin and Yueming Qiu and Xiao Xiao},
keywords = {Joint prevention and control of atmospheric pollution policy, Policy quantification, Emission reduction, Policy effectiveness},
abstract = {Joint prevention and control of atmospheric pollution (JPCAP) policies play a vital role in alleviating regional pollution. Based on Latent Dirichlet Allocation (LDA) model, we construct two policy strength measures of effectiveness and number, and investigate the effects of policy strength on air pollutant emissions for four types of JPCAP policies. The results show that the effects of economic incentive policy tools and supporting policy tools on emission reduction deviate significantly from policy preferences. Economic incentive policy tools are the most effective in promoting emission reductions in SO2, NOx and dust, but their effectiveness are the lowest in reality. Supporting policy tools, with the highest strength, have little effect on emission reduction. Command-control policies and persuasion policies are both relatively high in quantity and effectiveness. In addition, policy strength plays a more important role in reducing air pollutants in key regions than in non-key regions. JPCAP policies have gradually changed from a single policy tool to multiple policy tools, and the government shifted its attention to improving the legal effectiveness of policies after 2015. Finally, we propose some policy implications to optimize JPCAP policies and address regional air pollution problem.}
}
@article{WEI2021191,
title = {Construction of super-resolution model of remote sensing image based on deep convolutional neural network},
journal = {Computer Communications},
volume = {178},
pages = {191-200},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002462},
author = {Zikang Wei and Yunqing Liu},
keywords = {Remote sensing image, Super resolution, Convolution algorithm, Cloud computing},
abstract = {With the continuous improvement of satellite remote sensing technology, using super-resolution image reconstruction technology to reconstruct remote sensing images has important application significance for social development. In the generator model proposed in this paper, the standard convolution layer in the residual network structure is replaced by empty convolution to improve the overall performance of the model while keeping the number of parameters unchanged and the receptive field of convolution at each stage unchanged. By analyzing the advantages of residual network, dense connection network, and cavity convolution in the field of image super resolution, an optimized super-resolution reconstruction model of GAN image with cavity convolution is constructed with dense connection block of cavity residue as a generator component. The cloud computing-based service model is introduced into the image reconstruction system, and the background management module is built through the cloud service system, which is responsible for model training, image transmission, image processing request and database reading. Through experimental analysis, it is proved that the whole automatic data processing from automatic matching data to processing data can be completed, and the performance is better than the traditional service mode, which can produce great economic benefits.}
}
@article{SAKKA2021101875,
title = {A profile-aware methodological framework for collaborative multidimensional modeling},
journal = {Data & Knowledge Engineering},
volume = {131-132},
pages = {101875},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101875},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000021},
author = {Amir Sakka and Sandro Bimonte and Stefano Rizzi and Lucile Sautot and François Pinet and Michela Bertolotto and Aurélien Besnard and Noura Rouillier},
keywords = {Data warehouse design, Collaborative systems, Quality dimensions},
abstract = {Multidimensional modeling, i.e., the design of cube schemata, has a key role in data warehouse (DW) projects, in self-service business intelligence, and in general to let users analyze data via the OLAP paradigm. Though an effective involvement of users in multidimensional modeling is crucial in these projects, not much has been said about how to establish a fruitful collaboration in projects involving numerous users with different skills, reputations, and degrees of authority. This issue is especially relevant in citizen science projects, where several volunteers can contribute their requirements despite not being formally-trained experts in the application domain. To fill this gap, we propose a framework for collaborative multidimensional modeling that can adapt itself to the profiles and skills of the actors involved. We first classify users depending on their authoritativeness, skills, and engagement in the project. Then, following this classification, we identify four possible methodological scenarios and propose a profile-aware methodology supported by two sets of quality attributes. Finally, we describe a Group Decision Support System that implements our methodological framework and present some experiments carried out on a real case study.}
}
@article{ABDULJABBAR2021102734,
title = {The role of micro-mobility in shaping sustainable cities: A systematic literature review},
journal = {Transportation Research Part D: Transport and Environment},
volume = {92},
pages = {102734},
year = {2021},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2021.102734},
url = {https://www.sciencedirect.com/science/article/pii/S1361920921000389},
author = {Rusul L. Abduljabbar and Sohani Liyanage and Hussein Dia},
keywords = {Micro-mobility, Smart cities, Systematic literature review, Sustainable transport, Bibliometric networks, Co-citation analysis},
abstract = {Micro-mobility is increasingly recognised as a promising mode of urban transport, particularly for its potential to reduce private vehicle use for short-distance travel. Despite valuable research contributions that represent fundamental knowledge on this topic, today’s body of research appears quite fragmented in relation to the role of micro-mobility as a transformative solution for meeting sustainability outcomes in urban environments. This paper consolidates knowledge on the topic, analyses past and on-going research developments, and provides future research directions by using a rigorous and auditable systematic literature review methodology. To achieve these objectives, the paper analysed 328 journal publications from the Scopus database covering the period between 2000 and 2020. A bibliographic analysis was used to identify relevant publications and explore the changing landscape of micro-mobility research. The study constructed and visualised the literature’s bibliometric networks through citations and co-citations analyses for authors, articles, journals and countries. The findings showed a consistent spike in recent research outputs covering the sustainability aspects of micro-mobility reflecting its importance as a low-carbon and transformative mode of urban transport. The co-citation analysis, in particular, helped to categorise the literature into four main research themes that address benefits, technology, policy and behavioural mode-choice categories where the majority of research has been focused during the analysis period. For each cluster, inductive reasoning is used to discuss the emerging trends, barriers as well as pathways to overcome challenges to wide-scale deployment. This article provides a balanced and objective summary of research evidence on the topic and serves as a reference point for further research on micro-mobility for sustainable cities.}
}
@article{LAN2021107522,
title = {Trade-off between carbon sequestration and water loss for vegetation greening in China},
journal = {Agriculture, Ecosystems & Environment},
volume = {319},
pages = {107522},
year = {2021},
issn = {0167-8809},
doi = {https://doi.org/10.1016/j.agee.2021.107522},
url = {https://www.sciencedirect.com/science/article/pii/S0167880921002267},
author = {Xin Lan and Zhiyong Liu and Xiaohong Chen and Kairong Lin and Linying Cheng},
keywords = {Land use, Forest, Cropland, GPP, NPP, Evapotranspiration, PT-JPL model},
abstract = {Land use management of forests and croplands mainly drives the vegetation greening in China. Vegetation greening strongly modulates the trade-off between carbon sequestration via photosynthesis and water loss from evapotranspiration (ET) at the terrestrial ecosystem (representing by ecosystem water use efficiency, WUE). The function of vegetation greening in terrestrial carbon sequestration is well known, but the impacts of water loss from ET caused by vegetation greening on WUE are often neglected. Here, the GIS-based Priestley-Taylor Jet Propulsion Laboratory model was established to evaluate ET in China from 2001 to 2015, incorporating vegetation dynamics as a key component. To quantify the net effect of the ET caused by vegetation greening on WUE, we compared two different simulation scenarios: actual vegetation greening scenario and simulated without vegetation greening scenario. The results show that forests and croplands mainly contribute to the growth in GPP and NPP in China with annual rates of 2.53 gC·m−2 yr−2 and 1.59 gC·m−2 yr−2 from 2001 to 2015, respectively. With the increase of terrestrial carbon sequestration, the ET under actual vegetation greening scenario was generated 6.78 mm·yr−1 more than that under simulated without vegetation greening scenario. But as a result of the negative impacts of vegetation physiological effect (elevated CO2 concentration and the decreased VPD) on ET, values of ET under two different scenarios all exhibited a decline trend from 2001 to 2015 with rates of − 2.04% and − 3.63%, respectively. Consequently, although the WUE under two different scenarios exhibited increased trends (6.44%, actual vegetation dynamics scenario; 10.74%, simulated without vegetation greening scenario), the ET caused by vegetation greening led to an obvious divergence between the WUE under two different scenarios. For better understanding the impacts of human activities on carbon and water cycles at the terrestrial ecosystem, it is necessary to take the water loss from ET caused by vegetation greening into consideration, which is crucial for enhancing the sustainability of future vegetation-related projects.}
}
@article{ZHAO2021545,
title = {A polygenic methylation prediction model associated with response to chemotherapy in epithelial ovarian cancer},
journal = {Molecular Therapy - Oncolytics},
volume = {20},
pages = {545-555},
year = {2021},
issn = {2372-7705},
doi = {https://doi.org/10.1016/j.omto.2021.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S2372770521000309},
author = {Lanbo Zhao and Sijia Ma and Linconghua Wang and Yiran Wang and Xue Feng and Dongxin Liang and Lu Han and Min Li and Qiling Li},
keywords = {ovarian cancer, bioinformatics, DNA methylation, chemotherapy response, prediction model, AGR2, HSPA2, ACAT2},
abstract = {To identify potential aberrantly differentially methylated genes (DMGs) correlated with chemotherapy response (CR) and establish a polygenic methylation prediction model of CR in epithelial ovarian cancer (EOC), we accessed 177 (47 chemo-sensitive and 130 chemo-resistant) samples corresponding to three DNA-methylation microarray datasets from the Gene Expression Omnibus and 306 (290 chemo-sensitive and 16 chemo-resistant) samples from The Cancer Genome Atlas (TCGA) database. DMGs associated with chemotherapy sensitivity and chemotherapy resistance were identified by several packages of R software. Pathway enrichment and protein-protein interaction (PPI) network analyses were constructed by Metascape software. The key genes containing mRNA expressions associated with methylation levels were validated from the expression dataset by the GEO2R platform. The determination of the prognostic significance of key genes was performed by the Kaplan-Meier plotter database. The key genes-based polygenic methylation prediction model was established by binary logistic regression. Among accessed 483 samples, 457 (182 hypermethylated and 275 hypomethylated) DMGs correlated with chemo resistance. Twenty-nine hub genes were identified and further validated. Three genes, anterior gradient 2 (AGR2), heat shock-related 70-kDa protein 2 (HSPA2), and acetyltransferase 2 (ACAT2), showed a significantly negative correlation between their methylation levels and mRNA expressions, which also corresponded to prognostic significance. A polygenic methylation prediction model (0.5253 cutoff value) was established and validated with 0.659 sensitivity and 0.911 specificity.}
}
@article{ARUNTHAVANATHAN2021107197,
title = {An analysis of process fault diagnosis methods from safety perspectives},
journal = {Computers & Chemical Engineering},
volume = {145},
pages = {107197},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.107197},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420312400},
author = {Rajeevan Arunthavanathan and Faisal Khan and Salim Ahmed and Syed Imtiaz},
keywords = {Process safety, Process risk management, Process failure analysis, Process fault diagnosis, Process automation},
abstract = {Industry 4.0 provides substantial opportunities to ensure a safer environment through online monitoring, early detection of faults, and preventing the faults to failures transitions. Decision making is an important step in abnormal situation management. Assigning risk based on the consequences may provide additional information for abnormal situation management decisions to prevent the accident before it occurs. This paper analyzes the interconnections between the three essential aspects of process safety: fault detection and diagnosis (FDD), risk assessment (RA), and abnormal situation management (ASM) in the context of the current and next generation of process systems. The authors present their thoughts on research directions in process safety in Industry 4.0. This article aims to serve as a road map for the next generation of process safety research to enable safer and sustainable process operations and development.}
}
@article{KANN2021916,
title = {Artificial intelligence for clinical oncology},
journal = {Cancer Cell},
volume = {39},
number = {7},
pages = {916-927},
year = {2021},
issn = {1535-6108},
doi = {https://doi.org/10.1016/j.ccell.2021.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1535610821002105},
author = {Benjamin H. Kann and Ahmed Hosny and Hugo J.W.L. Aerts},
keywords = {artificial intelligence, clinical oncology, precision medicine, care pathway, clinical translation},
abstract = {Summary
Clinical oncology is experiencing rapid growth in data that are collected to enhance cancer care. With recent advances in the field of artificial intelligence (AI), there is now a computational basis to integrate and synthesize this growing body of multi-dimensional data, deduce patterns, and predict outcomes to improve shared patient and clinician decision making. While there is high potential, significant challenges remain. In this perspective, we propose a pathway of clinical cancer care touchpoints for narrow-task AI applications and review a selection of applications. We describe the challenges faced in the clinical translation of AI and propose solutions. We also suggest paths forward in weaving AI into individualized patient care, with an emphasis on clinical validity, utility, and usability. By illuminating these issues in the context of current AI applications for clinical oncology, we hope to help advance meaningful investigations that will ultimately translate to real-world clinical use.}
}
@article{PURCELL2023100094,
title = {Digital Twins in Agriculture: A State-of-the-art review},
journal = {Smart Agricultural Technology},
volume = {3},
pages = {100094},
year = {2023},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2022.100094},
url = {https://www.sciencedirect.com/science/article/pii/S2772375522000594},
author = {Warren Purcell and Thomas Neubauer},
keywords = {Digital Twin, Agriculture},
abstract = {The Digital Twin enables the distinctions between state sensing, entity understanding and physical automation to be eliminated, through high-fidelity modelling and bi-directional data streams. The concept of real-time virtual representation places the Digital Twin in a unique position to enable digitization in agriculture. The union of data, modelling and what-if simulation can provide an approach to overcome current limitations in decision-making support and automation, across a diverse range of agricultural enterprises. This paper conducts a Systematic Literature Review of Digital Twins in agriculture, identifying current trends and open questions with the goal of increasing awareness and understanding of the Digital Twin and its possibilities.}
}
@article{SADHANA2021255,
title = {AI-based Power Screening Solution for SARS-CoV2 Infection: A Sociodemographic Survey and COVID-19 Cough Detector},
journal = {Procedia Computer Science},
volume = {194},
pages = {255-271},
year = {2021},
note = {18th International Learning & Technology Conference 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.081},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921021220},
author = {S Sadhana and S Pandiarajan and E Sivaraman and D Daniel},
keywords = {Artificial Intelligence, Coronavirus (SARS –CoV2), Machine Learning, Pathomorphological Variations, Power Screening Solutions},
abstract = {Globally, the confirmed coronavirus (SARS-CoV2) cases are being increasing day by day. Coronavirus (COVID-19) causes an acute infection in the respiratory tract that started spreading in late 2019. Huge datasets of SARS-CoV2 patients can be incorporated and analyzed by machine learning strategies for understanding the pattern of pathological spread and helps to analyze the accuracy and speed of novel therapeutic methodologies, also detect the susceptible people depends on their physiological and genetic aspects. To identify the possible cases faster and rapidly, we propose the Artificial Intelligence (AI) power screening solution for SARS- CoV2 infection that can be deployable through the mobile application. It collects the details of the travel history, symptoms, common signs, gender, age and diagnosis of the cough sound. To examine the sharpness of pathomorphological variations in respiratory tracts induced by SARS-CoV2, that compared to other respiratory illnesses to address this issue. To overcome the shortage of SARS-CoV2 datasets, we apply the transfer learning technique. Multipronged mediator for risk-averse Artificial Intelligence Architecture is induced for minimizing the false diagnosis of risk-stemming from the problem of complex dimensionality. This proposed application provides early detection and prior screening for SARS-CoV2 cases. Huge data points can be processed through AI framework that can examine the users and classify them into “Probably COVID”, “Probably not COVID” and “Result indeterminate”.}
}
@incollection{EMINAGA2021309,
title = {Chapter 16 - Prospect and adversity of artificial intelligence in urology},
editor = {Lei Xing and Maryellen L. Giger and James K. Min},
booktitle = {Artificial Intelligence in Medicine},
publisher = {Academic Press},
pages = {309-337},
year = {2021},
isbn = {978-0-12-821259-2},
doi = {https://doi.org/10.1016/B978-0-12-821259-2.00016-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212592000168},
author = {Okyaz Eminaga and Joseph C. Liao},
keywords = {Urology, artificial intelligence, MRI, CT, urine analyses, AI-based solution, prostate cancer, bladder cancer, kidney cancer, ultrasound, diagnostic imaging, prediction models},
abstract = {The emergence of artificial intelligence (AI) has opened a new avenue for tackling existing challenges in clinical routine. This chapter will briefly introduce potential applications of AI in urology and focus on its benefits and barriers in solving real clinical problems. First, the introduction section will generally discuss AI and existing data resources. Then, the chapter will explain the potential application of AI in urological endoscopy, urine, stone and andrology, imaging and the robotic surgery. Further, this chapter will briefly discuss some tools of risk predictions for urological cancer. Finally, the author will discuss the potential future direction of AI in urology.}
}
@incollection{ORLOWSKI202187,
title = {Chapter 3 - Specification of architecture layers and reference models of Internet of Things systems},
editor = {Cezary Orlowski},
booktitle = {Management of IOT Open Data Projects in Smart Cities},
publisher = {Academic Press},
pages = {87-126},
year = {2021},
isbn = {978-0-12-818779-1},
doi = {https://doi.org/10.1016/B978-0-12-818779-1.00003-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128187791000031},
author = {Cezary Orlowski},
keywords = {IoT networks, sensors, communication layers and protocols, data collection},
abstract = {While the first chapter introduced open data and Smart Cities, the second chapter presented the environment for generating open data, in this chapter we discuss the layers of architecture of the Internet of Things systems. Characteristics of microcontrollers and sensors are presented in detail, and communication protocols are discussed. The purpose of this chapter is to familiarize the reader with the description of basic internet devices and methods of their communication.}
}
@article{ISLAM2021103008,
title = {Context-aware scheduling in Fog computing: A survey, taxonomy, challenges and future directions},
journal = {Journal of Network and Computer Applications},
volume = {180},
pages = {103008},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103008},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521000357},
author = {Mir Salim Ul Islam and Ashok Kumar and Yu-Chen Hu},
keywords = {Fog computing, Context-awareness, Scheduling, Resource management, Resource estimation, Resource provisioning, Contextual information},
abstract = {Fog computing extends Cloud-based facilities and stays in the vicinity of the end-users to provide an attractive solution to a diverse range of latency-sensitive applications. The applications are becoming more sophisticated, context-aware, and computation-intensive due to varying situational and environmental conditions in order to meet the ever-increasing users’ demands. Further, resource heterogeneity, dynamic nature, resource limitations, and unpredictability of the Fog environment make scheduling of application tasks while satisfying Quality of Service (QoS) requirements a challenging job. To overcome these issues various scheduling strategies have been proposed considering contextual information of different entities involved in Fog computing. This survey represents a comprehensive literature analysis pertaining to context-aware scheduling in Fog computing. It provides detailed comparison of existing scheduling approaches based on important factors such as context-aware parameters, case studies, performance metrics, and evaluation tools along with advantages and limitations. It also presents detailed taxonomy, performance metrics, and context-aware parameter analysis. Further, it list several issues and challenges. This study will aid the research community in exploring future research directions and essential aspects of scheduling approaches using different types of contextual information.}
}
@incollection{ASSMANN2023397,
title = {Chapter 15 - Transcriptomics to devise human health and disease},
editor = {Mohammad {Ajmal Ali} and Joongku Lee},
booktitle = {Transcriptome Profiling},
publisher = {Academic Press},
pages = {397-417},
year = {2023},
isbn = {978-0-323-91810-7},
doi = {https://doi.org/10.1016/B978-0-323-91810-7.00016-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918107000169},
author = {Taís Silveira Assmann and Daisy Crispim and Fermín Milagro and J. Alfredo Martínez},
keywords = {Omics, transcriptomics, human health},
abstract = {Advances in high-throughput techniques, often referred to as omics-based technologies, have accelerated research and have contributed for a better genetic and biological characterization of complex diseases. Current “Omics” research has mainly been focused on genes (genomics), messenger-RNA (mRNA) and noncoding (nc) RNAs (transcriptomics), epigenetic factors (epigenomics), proteins (proteomics), metabolites (metabolomics), lipids (lipidomics), and gut microbiota (microbiomics) signatures associated with different phenotypes and diseases. Different platforms and bioinformatics tools have been developed to explore the massive amount of data generated by research using omics-based approaches embedded in the Systems Biology theoretical and computational framework. This chapter will be focused on transcriptomics, presenting an overview of this category of “omics,” discussing the techniques used for this analysis, its main applications, and the integration of transcriptome data with other “omics” using system biology approaches.}
}
@article{LI2021100483,
title = {Robust Gaussian process regression based on iterative trimming},
journal = {Astronomy and Computing},
volume = {36},
pages = {100483},
year = {2021},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2021.100483},
url = {https://www.sciencedirect.com/science/article/pii/S2213133721000378},
author = {Zhao-Zhou Li and Lu Li and Zhengyi Shao},
keywords = {Gaussian process, Robust regression, Outlier detection, Ridge line, Star clusters},
abstract = {The Gaussian process (GP) regression can be severely biased when the data are contaminated by outliers. This paper presents a new robust GP regression algorithm that iteratively trims the most extreme data points. While the new algorithm retains the attractive properties of the standard GP as a nonparametric and flexible regression method, it can greatly improve the model accuracy for contaminated data even in the presence of extreme or abundant outliers. It is also easier to implement compared with previous robust GP variants that rely on approximate inference. Applied to a wide range of experiments with different contamination levels, the proposed method significantly outperforms the standard GP and the popular robust GP variant with the Student-t likelihood in most test cases. In addition, as a practical example in the astrophysical study, we show that this method can precisely determine the main-sequence ridge line in the color–magnitude diagram of star clusters.}
}
@article{PARK2021115691,
title = {The risk of hip fractures in individuals over 50 years old with prediabetes and type 2 diabetes – A longitudinal nationwide population-based study},
journal = {Bone},
volume = {142},
pages = {115691},
year = {2021},
issn = {8756-3282},
doi = {https://doi.org/10.1016/j.bone.2020.115691},
url = {https://www.sciencedirect.com/science/article/pii/S8756328220304713},
author = {Ho Youn Park and Kyoungdo Han and Youngwoo Kim and Yoon Hwan Kim and Yoo Joon Sur},
keywords = {Diabetes mellitus, Prediabetic state, Diabetic complications, Hip fractures, Risk assessment, Cohort studies},
abstract = {Background
The present study aimed to investigate the association between type 2 diabetes mellitus (T2DM) and hip fractures using a large-scale nationwide population-based cohort that is representative of the Republic of Korea. We determined the risks of hip fractures in individuals with prediabetes and T2DM with different diabetes durations, and compared them with the risks of hip fractures in individuals without T2DM.
Methods
A total of 5,761,785 subjects over 50 years old who underwent the National Health Insurance Service medical checkup in 2009–2010 were included. Subjects were classified into 5 groups based on the diabetes status; Normal, Prediabetes, Newly-diagnosed T2DM, T2DM less than 5 years, and T2DM more than 5 years. They were followed from the date of the medical checkup to the end of 2016. The endpoint was a new development of hip fracture during follow-up. The hazard ratios (HRs) and 95% confidence intervals (CIs) of hip fractures for each group were analyzed using Cox proportional hazard regression models after adjusting for age, sex, smoking, alcohol drinking, regular exercise, body mass index, hypertension, dyslipidemia, and chronic kidney disease.
Results
The HRs of hip fractures were 1 in the Normal group, 1.032 (95% CI: 1.009, 1.056) in the Prediabetes group, 1.168 (95% CI: 1.113, 1.225) in the Newly-diagnosed T2DM2, 1.543 (95% CI: 1.495, 1.592) in the T2DM less than 5 years and 2.105 (95% CI: 2.054, 2.157) in the T2DM more than 5 years. The secular trend of the HRs of hip fractures according to the duration of T2DM was statistically significant (P < .001). Subgroup analyses also showed the same increasing pattern of the HRs of hip fractures according to the duration of T2DM in both sexes and all age groups (50–64 years, 65–74 years, over 75 years).
Conclusions
In summary, this large-scale, retrospective, longitudinal, nationwide population-based cohort study of 5,761,785 subjects demonstrated that the risks of hip fractures started to increase in prediabetes and was associated linearly with the duration of T2DM. The secular trend of risks of hip fractures according to the duration of T2DM was consistent in both sexes and all age groups.}
}
@article{CZETANY2021111376,
title = {Development of electricity consumption profiles of residential buildings based on smart meter data clustering},
journal = {Energy and Buildings},
volume = {252},
pages = {111376},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111376},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821006605},
author = {László Czétány and Viktória Vámos and Miklós Horváth and Zsuzsa Szalay and Adrián Mota-Babiloni and Zsófia Deme-Bélafi and Tamás Csoknyai},
keywords = {Electricity consumption profile, Smart meter, Data clustering, K-means, Fuzzy k-means, Hierarchical, Residential buildings},
abstract = {In the present research, a high-resolution, detailed electric load dataset was assessed, collected by smart meters from nearly a thousand households in Hungary, many of them single-family houses. The objective was to evaluate this database in detail to determine energy consumption profiles from time series of daily and annual electric load. After representativity check of dataset daily and annual energy consumption profiles were developed, applying three different clustering methods (k-means, fuzzy k-means, agglomerative hierarchical) and three different cluster validity indexes (elbow method, silhouette method, Dunn index) in MATLAB environment. The best clustering method for our examination proved to be the k-means clustering technique. Analyses were carried out to identify different consumer groups, as well as to clarify the impact of specific parameters such as meter type in the housing unit (e.g. peak, off-peak meter), day of the week (e.g. weekend, weekday), seasonality, geographical location, settlement type and housing type (single-family house, flat, age class of the building). Furthermore, four electric user profile types were proposed, which can be used for building energy demand simulation, summer heat load and winter heating demand calculation.}
}
@article{BOCHIE2021103213,
title = {A survey on deep learning for challenged networks: Applications and trends},
journal = {Journal of Network and Computer Applications},
volume = {194},
pages = {103213},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103213},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002149},
author = {Kaylani Bochie and Mateus S. Gilbert and Luana Gantert and Mariana S.M. Barbosa and Dianne S.V. Medeiros and Miguel Elias M. Campista},
keywords = {Challenged networks, Internet of Things, Sensor networks, Industrial networks, Wireless mobile networks, Vehicular networks, Deep learning, Machine learning},
abstract = {Computer networks are dealing with growing complexity, given the ever-increasing volume of data produced by all sorts of network nodes. Performance improvements are a non-stop ambition and require tuning fine-grained details of the system operation. Analyzing such data deluge, however, is not straightforward and sometimes not supported by the system. There are often problems regarding scalability and the predisposition of the involved nodes to understand and transfer the data. This issue is at least partially circumvented by knowledge acquisition from past experiences, which is a characteristic of the herein called “challenged networks”. The addition of intelligence in these scenarios is fundamental to extract linear and non-linear relationships from the data collected by multiple sources. This is undoubtedly an invitation to machine learning and, more particularly, to deep learning. This paper identifies five different challenged networks: IoT and sensor, mobile, industrial, and vehicular networks as typical scenarios that may have multiple and heterogeneous data sources and face obstacles concerning connectivity. As a consequence, deep learning solutions can contribute to system performance by adding intelligence and the ability to interpret data. We start the paper by providing an overview of deep learning, further explaining this approach’s benefits over the cited scenarios. We propose a workflow based on our observations of deep learning applications over challenged networks, and based on it, we strive to survey the literature on deep-learning-based solutions at an application-oriented level using the PRISMA methodology. Afterward, we also discuss new deep learning techniques that show enormous potential for further improvements as well as transversal issues, such as security. Finally, we provide lessons learned raising trends linking all surveyed papers to deep learning approaches. We are confident that the proposed paper contributes to the state of the art and can be a piece of inspiration for beginners and also for enthusiasts on advanced networking research.}
}
@incollection{SCHMIDT2021283,
title = {Computational Toxicology},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {283-300},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11534-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012801238311534X},
author = {Friedemann Schmidt},
keywords = {Applicability domain, Artificial intelligence, Classification, Computational toxicology, Data mining, Descriptor, Expert system, In silico, In silico profiling, Machine learning, Off target toxicity, QSAR, Rule-based, SAR, Toxicology},
abstract = {Powerful computer systems and growing amounts of curated data have rocketed the implementation of computational disciplines in all fields of drug discovery sciences, including toxicology. Computational toxicology feeds from the needs of researchers, manufacturers, regulators and patients to fully characterize the safety profiles of drugs to avoid the exposure of patients at risk. In this article, we introduce basic concepts of computational toxicology and discuss algorithms and tools that are widely employed to complement experimental laboratory work. Since more than two decades, lab work and particularly animal experimentation have found more and more counterparts by in-silico simulation and computational prediction. These methods are employed to inform scientists early in the drug discovery process, fill data gaps, explain causalities and ultimately reduce experimentation and cycle times. Since computational toxicology is applied throughout the full drug discovery value chain, from research to clinic, it has given rise to using an extraordinarily broad kit of tools employing data mining, curation of structural alerts, molecular fragment methods, quantitative structure-property relationships and machine learning. While the use of data-driven technologies is exploding, and multiple competing applications become available for predictive modeling, their underlying training datasets for individual endpoints may still be small—too small to allow for global applicability. Care must be taken therefore to ensure that predictive methods have been properly validated and are fit-for-purpose. We discuss such practice, as well as model interpretability and how computational methods can contribute to generate hypotheses and support the progression of new molecules.}
}
@article{CAI2021128792,
title = {The need for urban form data in spatial modeling of urban carbon emissions in China: A critical review},
journal = {Journal of Cleaner Production},
volume = {319},
pages = {128792},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128792},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621029905},
author = {Meng Cai and Yuan Shi and Chao Ren and Takahiro Yoshida and Yoshiki Yamagata and Chao Ding and Nan Zhou},
keywords = {Urban carbon emissions, Spatial modeling, Systematic review, Urban form, China},
abstract = {Cities produce over 70% of global carbon emissions and are thus crucial in driving climate change. Urban carbon emissions may continue to increase especially in those less-developed countries and regions which are still under rapid urban development. Policymakers need to find ways to effectively control and reduce carbon emissions. Thus, spatial modeling methods to map and predict urban carbon emissions have been developed to meet these needs. This paper examines the progress of the spatial modeling of carbon emissions and the relationship between urban form and carbon emissions in China by reviewing more than 100 peer-reviewed journal articles in the Scopus database. The latest prediction methods and techniques are described in the paper. Their advantages and limitations are then discussed. Urban forms have a significant influence on carbon emissions and have been applied in spatial modeling studies in other countries. However, this review has identified the lack of urban form data and high-resolution inventories from existing studies in China. Future developments in the spatial modeling in China should therefore have a fine spatial resolution and incorporate open and high-quality urban form data, including urban morphology and land use/land cover.}
}
@article{FURNESS2021129127,
title = {Building the ‘Bio-factory’: A bibliometric analysis of circular economies and Life Cycle Sustainability Assessment in wastewater treatment},
journal = {Journal of Cleaner Production},
volume = {323},
pages = {129127},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129127},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621033163},
author = {Madeline Furness and Ricardo Bello-Mendoza and Jonatan Dassonvalle and Rolando Chamy-Maggi},
keywords = {Life cycle sustainability assessment, Resource recovery, Wastewater treatment, Circular economies, Biofactory},
abstract = {The “Biofactory” is a circular economy-based concept for wastewater treatment that improves water quality, promotes efficient use of materials and energy, while also recovering resources and decreasing both emissions and costs. Due to socio-economic bottlenecks, such as typical high costs and low public acceptance of novel resource recovery scenarios in wastewater treatment, realizing the Biofactory goals becomes a difficult task. Decision makers are currently unable to appreciate the environmental, social, and economic benefits of the Biofactory, as most decision-making tools focus on mainly technical and economic aspects. This is the first review is to use bibliometric analysis of publication trends in life cycle-based modelling for circular economies in wastewater treatment across the globe, focusing on Life Cycle Assessment (LCA), Life Cycle Costing (LCC) and Social Life Cycle Assessment (SLCA). The integration of LCA, LCC and SLCA for the development of a Life Cycle Sustainability Assessment (LCSA) decision making framework is recommended, while practical implications for the methodological development of this tool are compiled. The goal and scope of LCSA for the Biofactory must explore multi-product functional units for the recovery of different resources from wastewater, where system boundaries must include techno-environmental, social and economic systems together. Internal loops, feedbacks loops, avoided products and co-product allocation methodologies within the integrated system boundaries must be considered. Innovation is required for building LCA, LCC and SLCA inventories, where life cycle data management is important for implementing the Biofactory into the future.}
}
@incollection{FAKA2021199,
title = {Chapter 10 - Environmental sensing: a review of approaches using GPS/GNSS},
editor = {George p. Petropoulos and Prashant K. Srivastava},
booktitle = {GPS and GNSS Technology in Geosciences},
publisher = {Elsevier},
pages = {199-220},
year = {2021},
isbn = {978-0-12-818617-6},
doi = {https://doi.org/10.1016/B978-0-12-818617-6.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186176000135},
author = {Antigoni Faka and Konstantinos Tserpes and Christos Chalkias},
keywords = {Air pollution, Crowdsourcing, Environmental monitoring, Noise pollution, Smartphones},
abstract = {The condition of the environment is a critical factor for the quality of life and the well-being of humans. Environmental assessment ensures foreseeing and address of potential complications at an early stage in environmental planning and management. The recent technological advancements in spatiotemporal monitoring of environmental features provide to the scientific community the proper material for efficient and accurate assessment of the environment. This chapter aims to review the role of Global Positioning System (GPS)/Global Navigation Satellite System (GNSS) technologies in environmental sensing. The presentation focuses on the technologies and approaches followed in terms of data collection, analysis, and visualization and demonstrates the major categories of environmental sensing applications. The findings reveal a constantly gaining momentum of mobile monitoring and participatory systems. The adoption of GPS/GNSS enhancement techniques, including crowdsourcing methods, would assist environmental policymaking to establish targets and prioritize planned actions.}
}
@article{ALKHEDER2021120269,
title = {Taxi Ride sharing in Kuwait: Econ-enviro study},
journal = {Energy},
volume = {225},
pages = {120269},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.120269},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221005181},
author = {Sharaf AlKheder},
keywords = {Traffic, Ridesharing, Passengers, Fuel consumption, Gas emissions},
abstract = {Traffic congestion had been the most important and sensitive problem facing Kuwait roads for decades. The objectives of this study were to: optimize the traffic flow on road networks by implementing shared rides, reduce vehicle emissions and minimize the ride cost. The concept concentrated on sharing the journeys starting and ending around the same place and time. The effect of ridesharing was studied according to the data collected for 3 months (July 2018, October 2018 and January 2019) from a taxi company. The data was then filtered so that the start point of all trips was South Surra. Three different scenarios were created: single passenger, two passengers, and more than two passengers. Java NetBeans was used to calculate the total distance for each month and the cost for each trip after applying the concept. “My driving” was used to obtain the fuel consumption to calculate the gas emissions for the three scenarios;CO, NOx and HC emissions. For the trip distance analysis, the results for July, October, and January decreased by 0.84%, 0.45%, and 1.25%, respectively. For gas emissions and by comparing the first scenario with the second one, the CO, NOx and HC emissions were reduced by 9.072%, 9.069%, and 9.074%, respectively. Finally, by comparing the first scenario with the third one, CO, NOx and HC emissions were lowered by 0.116%, 0.08%, and 0.108%, respectively.}
}
@article{KRAU2021150,
title = {Digital Manufacturing for Smart Small Satellites Systems},
journal = {Procedia Computer Science},
volume = {180},
pages = {150-161},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.138},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921001769},
author = {Markus Krauß and Florian Leutert and Markus R. Scholz and Michael Fritscher and Robin Heß and Christian Lilge and Klaus Schilling},
keywords = {digital manufacturing, demonstrator factory, new space, augmented reality user interfaces, human-robot collaboration, predictive maintenance, telemaintenance, quality of service, mobile robotics, intelligent material flow},
abstract = {The term Industry 4.0 – manufacturing with exploitation of digital technologies – comprises several challenging topics, among others intuitive machine programming, advanced maintenance-enabling technologies as well as flexible logistics. This paper gives an overview on recent research and development of our institute (Zentrum für Telematik, ZfT) in this field, the results of which are combined in an Industry 4.0 demonstration factory for the assembly of small satellites systems. We present a total of six tools to be used in such advanced manufacturing systems and their individual advantages. Based on our experience with industrial project partners, customers and visitors of our demonstration factory as well as on the evaluations of the jurors of several awards, we give a qualitative estimate of the effort required to port the individual tools to new production environments. Finally, utilizing our in-house expertise in the New-Space and Industry-4.0 sectors, we give an insight into the benefits achievable using digital manufacturing for small satellite assembly.}
}
@article{WANG2023103013,
title = {A node trust evaluation method of vehicle-road-cloud collaborative system based on federated learning},
journal = {Ad Hoc Networks},
volume = {138},
pages = {103013},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.103013},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522001858},
author = {Denghui Wang and Yuping Yi and Shan Yan and Na Wan and Junhui Zhao},
keywords = {Federated learning, Trust evaluation, Vehicle-road-cloud collaboration system},
abstract = {As the vehicle-road-cloud collaboration system develops rapidly, it is accompanied by serious information security problems while solving the data transmission issues. For constructing secure transmission of data, trust is recommended as a relevant way to accomplish network security; that is, developing a trust model that can be used by sensor nodes to determine the reliability of another node is crucial. However, the heterogeneity of the network has different functional requirements for trust evaluation, and the openness of the network makes the nodes more vulnerable to attacks. Therefore, the research of trust evaluation model in the vehicle-road-cloud collaborative system is facing greater challenges than the traditional network. In this paper, a trust evaluation scheme of the vehicle-road-cloud collaborative system based on Federated Learning (FLT) is proposed. A hierarchical trust evaluation model is designed, and the complex model is simplified to an orderly hierarchical structure by using hierarchical analysis. The trust indexes of different layers are evaluated, and the influencing factors among different nodes are comprehensively considered. Combined with federated learning, it solves the problem of finding the most reliable route and realizes personalization at the level of equipment, data, and model. For the purpose of alleviating the heterogeneity and obtaining a high-quality personalized model for each device, trust values can be adaptively updated as changes in the topology of the network occur in real-time. The simulation findings demonstrate that, as compared to previous schemes, the energy consumption is lowered by 35%, and the accuracy is raised by 45% while maintaining trust stability.}
}
@article{CARTWRIGHT2021120,
title = {Managing relationships on social media in business-to-business organisations},
journal = {Journal of Business Research},
volume = {125},
pages = {120-134},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320307815},
author = {Severina Cartwright and Iain Davies and Chris Archer-Brown},
keywords = {Social media, B2B marketing, Relationship marketing, Business networks},
abstract = {Social media (SM) constitutes a valuable source of market intelligence, characterised by great ease and efficiency of interactions between networked partners, and by facilitation of individual expressions of self and brand engagement. Thus, SM can enable interaction, collaboration, and networking, thereby strengthening the relationships between actors within networks. Nonetheless, research into B2B organisations ́ usage of SM for relationship management remains limited, fragmented and lacking strategic direction. To expand the current state of theory, we draw upon twelve case studies of SM management concerning tactics for acquiring new and potential relationships, building a reputation online, and engaging with business partners. Results show four distinct engagement strategies that organisations tend to employ when implementing SM marketing strategies. The four strategies provide an insight into current approaches to SM marketing within B2B organisations, and how organisations are structuring themselves and managing their resources to respond to this opportunity.}
}
@article{SINGH2021100489,
title = {Quantifying COVID-19 enforced global changes in atmospheric pollutants using cloud computing based remote sensing},
journal = {Remote Sensing Applications: Society and Environment},
volume = {22},
pages = {100489},
year = {2021},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2021.100489},
url = {https://www.sciencedirect.com/science/article/pii/S2352938521000252},
author = {Manmeet Singh and Bhupendra Bahadur Singh and Raunaq Singh and Badimela Upendra and Rupinder Kaur and Sukhpal Singh Gill and Mriganka Sekhar Biswas},
keywords = {COVID19, Google earth engine, PM, NO, AOD, Tropospheric ozone, Cloud computing},
abstract = {Global lockdowns in response to the COVID-19 pandemic have led to changes in the anthropogenic activities resulting in perceivable air quality improvements. Although several recent studies have analyzed these changes over different regions of the globe, these analyses have been constrained due to the usage of station based data which is mostly limited up to the metropolitan cities. Also the quantifiable changes have been reported only for the developed and developing regions leaving the poor economies (e.g. Africa) due to the shortage of in-situ data. Using a comprehensive set of high spatiotemporal resolution satellites and merged products of air pollutants, we analyze the air quality across the globe and quantify the improvement resulting from the suppressed anthropogenic activity during the lockdowns. In particular, we focus on megacities, capitals and cities with high standards of living to make the quantitative assessment. Our results offer valuable insights into the spatial distribution of changes in the air pollutants due to COVID-19 enforced lockdowns. Statistically significant reductions are observed over megacities with mean reduction by 19.74%, 7.38% and 49.9% in nitrogen dioxide (NO2), aerosol optical depth (AOD) and PM2.5 concentrations. Google Earth Engine empowered cloud computing based remote sensing is used and the results provide a testbed for climate sensitivity experiments and validation of chemistry-climate models. Additionally, Google Earth Engine based apps have been developed to visualize the changes in a real-time fashion.}
}
@article{MEI20212358,
title = {Web resources facilitate drug discovery in treatment of COVID-19},
journal = {Drug Discovery Today},
volume = {26},
number = {10},
pages = {2358-2366},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2021.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S135964462100204X},
author = {Long-Can Mei and Yin Jin and Zheng Wang and Ge-Fei Hao and Guang-Fu Yang},
keywords = {Bioinformatics, SARS-CoV-2, Sequence and structure, Drug design, Vaccines, Monoclonal antibodies},
abstract = {The infectious disease Coronavirus 2019 (COVID-19) continues to cause a global pandemic and, thus, the need for effective therapeutics remains urgent. Global research targeting COVID-19 treatments has produced numerous therapy-related data and established data repositories. However, these data are disseminated throughout the literature and web resources, which could lead to a reduction in the levels of their use. In this review, we introduce resource repositories for the development of COVID-19 therapeutics, from the genome and proteome to antiviral drugs, vaccines, and monoclonal antibodies. We briefly describe the data and usage, and how they advance research for therapies. Finally, we discuss the opportunities and challenges to preventing the pandemic from developing further.}
}
@article{WANG2021325,
title = {Early event detection in a deep-learning driven quality prediction model for ultrasonic welding},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {325-336},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001333},
author = {Baicun Wang and Yang Li and Ying Luo and Xingyu Li and Theodor Freiheit},
keywords = {Ultrasonic welding, Quality prediction, Deep-learning, Long short-term memory, Event detection},
abstract = {A goal in ultrasonic welding (USW) process monitoring is to accurately predict quality outcomes based on monitored signals. However, in most cases, knowing only that the USW process has failed is insufficient. Modern process automation should assess signal information and intercede to rectify process problems. Identification of when a process signal deviates from an acceptable final quality outcome, i.e., the time at which an abnormal event starts, facilitates control action or root cause analysis to bring it back to compliance. A long short-term memory (LSTM) recurrent neural network is proposed to monitor USW and other time-series signals and identify this point. This deep neural network is trained to classify quality outcomes from continuous signals. The process monitoring signals and their sampling time are divided into finite segments as input to this network. The time segment at which the process signal first converges to the final quality class prediction is identified using cross-entropy of the classification probabilities. This procedure is demonstrated using USW quality monitoring algorithms and robot motion failure detection. The examples show an LSTM network not only provides high accuracy for USW quality prediction, but also that the time of classification convergence is consistent with variance observed in USW weld quality factors. Moreover, classification convergence time was shown to be associated to specific robot motion failures, useful as input to adaptive learning. This work realizes deep-learning driven quality prediction and early event detection for quality classification problems, and provides the information necessary for adaptive control algorithms.}
}
@article{PIAO2021102651,
title = {Privacy preserving in blockchain-based government data sharing: A Service-On-Chain (SOC) approach},
journal = {Information Processing & Management},
volume = {58},
number = {5},
pages = {102651},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102651},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321001400},
author = {Chunhui Piao and Yurong Hao and Jiaqi Yan and Xuehong Jiang},
keywords = {Consortium blockchain, Government data sharing, Privacy preserving, Smart contract},
abstract = {Sharing government data is of great significance to social development, but insecure and inappropriate sharing may lead to privacy breaches. Data sharing in consortium blockchain has provided a promising direction for efficient privacy preserving government data sharing. However, since government data is not owned by a single person or any government employers, it is hard to attribute participants’ responsibility for motivating data sharing behavior in blockchain systems. Furthermore, the scope and scale of government data to be shared among departments is unclear, as the purposes for retrieving shared data are dynamically changing. In order to solve these problems, we propose a Service-On-Chain (or simply, SOC) approach. The SOC approach can effectively identify different departments’ data retrieving requirements while efficiently sharing government data with trustworthiness in data content and controllability in data ownership. In particular, we utilize smart contracts to provide an onchain service to define data sharing agreements between government departments, which can identify ambiguous data retrieving requirements and formalize the process logic. We apply the SOC approach to a real world scenario and demonstrate that the SOC approach provides a feasible solution for secure and efficient sharing of data among government departments.}
}
@article{SUN2021103372,
title = {Improvement of PM2.5 and O3 forecasting by integration of 3D numerical simulation with deep learning techniques},
journal = {Sustainable Cities and Society},
volume = {75},
pages = {103372},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103372},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721006466},
author = {Haochen Sun and Jimmy C.H. Fung and Yiang Chen and Wanying Chen and Zhenning Li and Yeqi Huang and Changqing Lin and Mingyun Hu and Xingcheng Lu},
keywords = {Deep learning, Community Multiscale Air Quality model, Spatial correction, PM, O},
abstract = {Air pollution is a major impediment to the sustainable development of cities and society. Governed by emission characteristics and meteorological conditions, the formation and destruction of fine particulate matter (PM2.5) and ozone (O3) are complicated, and accurate predictions of the concentrations of these two major secondary atmospheric pollutants remain challenging. In this study, by combining meteorological and air pollutant data from ground observations and the Weather Research and Forecasting (WRF)-Community Multiscale Air Quality (CMAQ) model simulations, a deep learning model structure based on long short-term memory layers (LSTM) was developed and applied to predict the PM2.5 and O3 concentrations in the future 48 h period. The forecasting improvement was extended to the whole Greater Bay Area by introducing a spatial correction (SC) method to the CMAQ simulation results. Compared with the original CMAQ forecast, the new method gained a 26% reduction in mean absolute error (MAE) and a 33% reduction in root mean square error (RMSE), respectively, in terms of PM2.5; it also achieved a 40% reduction in MAE and a 34% reduction in RMSE in terms of O3. SC method, applied to the whole GBA region, also reduced the overall MAE and RMSE by 10% and 17% in terms of PM2.5 and by 31% and 25% in terms of O3, respectively. Using an AI approach, our study provides new perspectives for further improving air quality forecasting from both temporal and spatial perspectives, thus increasing the smartness and resilience of the cities and promoting environmentally sustainable development in the area.}
}
@incollection{BIBAULT2021361,
title = {Chapter 18 - Artificial intelligence in oncology},
editor = {Lei Xing and Maryellen L. Giger and James K. Min},
booktitle = {Artificial Intelligence in Medicine},
publisher = {Academic Press},
pages = {361-381},
year = {2021},
isbn = {978-0-12-821259-2},
doi = {https://doi.org/10.1016/B978-0-12-821259-2.00018-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212592000181},
author = {Jean-Emmanuel Bibault and Anita Burgun and Laure Fournier and André Dekker and Philippe Lambin},
keywords = {Oncology, cancer, artificial intelligence, deep learning, machine learning, prediction},
abstract = {Medical decisions can rely on a very large number of parameters, but it is traditionally considered that our cognitive capacity can only integrate up to five factors in order to take a decision. Oncologists will need to combine vast amount of clinical, biological, and imaging data to achieve state-of-the-art treatments. Data science and artificial intelligence (AI) will have an important role in the generation of models to predict outcome and guide treatments. A new paradigm of data-driven decision-making, reusing routine health-care data to provide decision support is emerging. This chapter explores the studies published in imaging, medical and radiation oncology and explains the technical challenges that need to be addressed before AI can be routinely used to treat cancer patients.}
}
@article{RAIJADA2021113857,
title = {Integration of personalized drug delivery systems into digital health},
journal = {Advanced Drug Delivery Reviews},
volume = {176},
pages = {113857},
year = {2021},
issn = {0169-409X},
doi = {https://doi.org/10.1016/j.addr.2021.113857},
url = {https://www.sciencedirect.com/science/article/pii/S0169409X21002490},
author = {Dhara Raijada and Katarzyna Wac and Emanuel Greisen and Jukka Rantanen and Natalja Genina},
keywords = {Personalized medicine, Pharmaceutical supply chain, Digital therapeutics, Smartphone, Internet of Things (IoT), 2D barcodes, Traceability, Anti-counterfeiting, Track and trace, Unique identifiers},
abstract = {Personalized drug delivery systems (PDDS), implying the patient-tailored dose, dosage form, frequency of administration and drug release kinetics, and digital health platforms for diagnosis and treatment monitoring, patient adherence, and traceability of drug products, are emerging scientific areas. Both fields are advancing at a fast pace. However, despite the strong complementary nature of these disciplines, there are only a few successful examples of merging these areas. Therefore, it is important and timely to combine PDDS with an increasing number of high-end digital health solutions to create an interactive feedback loop between the actual needs of each patient and the drug products. This review provides an overview of advanced design solutions for new products such as interactive personalized treatment that would interconnect the pharmaceutical and digital worlds. Furthermore, we discuss the recent advancements in the pharmaceutical supply chain (PSC) management and related limitations of the current mass production model. We summarize the current state of the art and envision future directions and potential development areas.}
}
@article{HUMAN2023103796,
title = {A design framework for a system of digital twins and services},
journal = {Computers in Industry},
volume = {144},
pages = {103796},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103796},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522001920},
author = {C. Human and A.H. Basson and K. Kruger},
keywords = {Design framework, Complex system, Digital twin, Aggregation, Service-oriented architecture},
abstract = {Digital twins represent physical systems in virtual space to enable knowledge intensive tasks. For complex physical systems, effective digital integration can often be achieved by means of a system of aggregated digital twins coupled with a services network. Designing such a system of digital twins and services involves a daunting array of options and considerations. This paper presents a design framework that facilitates systematic, effective decisions when designing a system of digital twins to integrate the data from a complex physical system. The design framework is arranged in six steps: 1) needs and constraints analysis, 2) physical system decomposition, 3) services allocation, 4) performance and quality considerations, 5) implementation considerations and 6) verification and validation. The design framework works with a general reference architecture that combines a digital twin aggregation hierarchy with a service-oriented architecture. This reference architecture allows for the separation of concerns, computational load distribution, incremental development and modular software design. The paper considers the merits of the design framework using two illustrative case studies, i.e. a heliostat field and a smart city. The case studies indicate that the design framework can be applied to a wide range of complex systems and can improve the effectiveness of the design process.}
}
@article{RAJKUMARREDDY2021107334,
title = {Developing a blockchain framework for the automotive supply chain: A systematic review},
journal = {Computers & Industrial Engineering},
volume = {157},
pages = {107334},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107334},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221002382},
author = {Kotha {Raj Kumar Reddy} and Angappa Gunasekaran and P. Kalpana and V. {Raja Sreedharan} and S {Arvind Kumar}},
keywords = {Automotive supply chain, Blockchain, Systematic literature review, VUCA world},
abstract = {As world is affected by demand volatility; process uncertainty; supply chain complexity and information ambiguity forming a VUCA world. To manage this scenario, industries are adopting emerging technologies for business excellence and one among them is Blockchain. Blockchain technology (BCT) is a distributed ledger technology (DLT) that stores transactional records in a tamper-proof and immutable way; it is a promising solution for incorporating transparency and traceability in traditional ecosystem. As automotive industries are facing a Volatile environment, Uncertain schedules & information; Complex supply chain networks, and Ambiguous decisions that cripples the automotive supply chain (ASC). Therefore, BCT can be used to address issues related to ASC in VUCA world. Keeping this in mind, study reported a systematic literature review (SLR) of BCT applications in ASC. More than seventy research papers were reviewed based on different BCT characteristics and applications. Through content analysis, study explored how to link supply chain visibility, information transparency with BCT for an efficient ASC in VUCA world. Moreover, a BCT implementation framework is proposed for ASC, to provide a decision-making approach for practitioners in VUCA world.}
}
@article{LI2021105961,
title = {Evolution characteristics and displacement forecasting model of landslides with stair-step sliding surface along the Xiangxi River, three Gorges Reservoir region, China},
journal = {Engineering Geology},
volume = {283},
pages = {105961},
year = {2021},
issn = {0013-7952},
doi = {https://doi.org/10.1016/j.enggeo.2020.105961},
url = {https://www.sciencedirect.com/science/article/pii/S0013795220318585},
author = {Changdong Li and Robert E. Criss and Zhiyong Fu and Jingjing Long and Qinwen Tan},
keywords = {Reservoir landslide, Evolution process, Multi-step sliding surface, Displacement forecasting model, Lower reaches of Xiangxi River},
abstract = {Five large and many small landslides are developed in Jurassic strata along the lower reaches of Xiangxi River, where interbedded weak and hard bedrock layers foster the development of landslides with a “stair-step” sliding surface. The paper investigates the evolution characteristics of these landslides and presents a novel forecasting model for their displacements. The distribution characteristics and behavior of landslides developed along Xiangxi River is revealed by the database of landslides in the larger Zigui basin, of which this area is part. Most landslides occur at rather low elevations of <300 m and in areas of moderate rainfall. The geological evolution of landslides in the Xiangxi River valley can be divided into four stages, beginning with anticline formation, followed by valley incision, then by weathering and erosion, and culminating in formation of the colluvial landslides. The accumulative displacement curves of landslides with a stair-step sliding surface in Xiangxi River region also present obvious, step-like characteristics. A novel GA-CEEMD-RF algorithm was developed to predict the displacement of these stair-step landslides, which helps to define the combination of induced factors and weak stableness of prediction results using a single displacement prediction model and the multi-field monitoring data.}
}
@incollection{CHANAL2021111,
title = {Chapter 7 - Security and privacy in the internet of things: computational intelligent techniques-based approaches},
editor = {Siddhartha Bhattacharyya and Paramartha Dutta and Debabrata Samanta and Anirban Mukherjee and Indrajit Pan},
booktitle = {Recent Trends in Computational Intelligence Enabled Research},
publisher = {Academic Press},
pages = {111-127},
year = {2021},
isbn = {978-0-12-822844-9},
doi = {https://doi.org/10.1016/B978-0-12-822844-9.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228449000098},
author = {Poornima M. Chanal and Mahabaleshwar S. Kakkasageri and Sunil Kumar S. Manvi},
keywords = {Internet of things, security, privacy, authentication, integrity, confidentiality, availability, computational intelligence},
abstract = {The Internet of Things (IoT) is a network of universally interconnected devices via the Internet. The IoT requires the interconnection between billions or trillions of intelligent objects. IoT devices (nodes) are capable of capturing, preserving, analyzing, and sharing data about themselves and their physical world. Security and privacy are the major challenges in the implementation of IoT technology. Major privacy aspects in the IoT are stealing data, monitoring, and tracking, etc. Authentication, integrity, and confidentiality are major concerns for privacy and security preservation in the IoT. Computational intelligence (CI) focuses on the design and development of intelligent algorithms to solve real-time problems with minimum cost. The main goal of CI is to supplement natural and artificial intelligence to produce human-required competitive results. Computational intelligent mechanisms for providing privacy and security in the IoT include quantum cryptography, artificial intelligence, neural networks, natural computational techniques, bio-inspired computational techniques, fuzzy logic techniques, genetic algorithms, intelligent multiagents, etc.}
}
@article{DIAS2021104134,
title = {Criteria for selecting apps: Debating the perceptions of young children, parents and industry stakeholders},
journal = {Computers & Education},
volume = {165},
pages = {104134},
year = {2021},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2021.104134},
url = {https://www.sciencedirect.com/science/article/pii/S0360131521000117},
author = {Patrícia Dias and Rita Brito},
keywords = {Young children, Mobile media, Apps, Parents, Stakeholders},
abstract = {It is indisputable that young children are exposed to digital media since birth and start using them very early. This fuels debate that engages scholars and researchers, industry and brands, policymakers, and parents. Our study aimed to contrast these different perspectives, adding the view of children, who are frequently left out of this debate. Using an exploratory qualitative approach, we conducted interviews with children under 8 years old and their parents in 81 families, and with 17 expert stakeholders in different fields. We focused on their perceptions and practices regarding digital media, and specifically on how they assess and select apps, concluding that parents value safety and learning, children enjoy entertainment, and stakeholders highlight the importance of a good user experience.}
}
@article{SONG2021102656,
title = {Self-help cognitive behavioral therapy application for COVID-19-related mental health problems: A longitudinal trial},
journal = {Asian Journal of Psychiatry},
volume = {60},
pages = {102656},
year = {2021},
issn = {1876-2018},
doi = {https://doi.org/10.1016/j.ajp.2021.102656},
url = {https://www.sciencedirect.com/science/article/pii/S187620182100112X},
author = {Jiaqi Song and Ronghuan Jiang and Nan Chen and Wei Qu and Dan Liu and Meng Zhang and Hongzhen Fan and Yanli Zhao and Shuping Tan},
keywords = {Cognitive behavioral therapy, Depression, Anxiety, Insomnia, COVID-19},
abstract = {Background and aim
Recently, the availability and usefulness of mobile self-help mental health applications have increased, but few applications deal with COVID-19-related psychological problems. This study explored the intervention efficacy of a mobile application on addressing psychological problems related to COVID-19.
Methods
A longitudinal control trial involving 129 Chinese participants with depression symptoms was conducted through the mobile application “Care for Your Mental Health and Sleep during COVID-19” (CMSC) based on WeChat. Participants were divided into two groups: mobile internet cognitive behavioral therapy (MiCBT) and wait-list. The primary outcome was improvement in depression symptoms. Secondary outcomes included improvement in anxiety and insomnia. The MiCBT group received three self-help CBT intervention sessions in one week via CMSC.
Results
The MiCBT group showed significant improvement in depression and insomnia (allP < 0.05) compared with the wait-list group. Although both groups showed significant improvement in anxiety at the intervention’s end, compared with the wait-list group, the MiCBT group had no significant advantage. Correlation analysis showed that improvement in depression and anxiety had a significant positive association with education level. Changes in insomnia were significantly negatively correlated with anxiety of COVID-19 at the baseline. CMSC was considered helpful (n=68, 81.9 %) and enjoyable (n=54, 65.9 %) in relieving depression and insomnia during the COVID-19 outbreak.
Conclusions
CMSC is verified to be effective and convenient for improving COVID-19-related depression and insomnia symptoms. A large study with sufficient evidence is required to determine its continuous effect on reducing mental health problems during the pandemic.}
}
@article{CALAFIORE2021101539,
title = {A geographic data science framework for the functional and contextual analysis of human dynamics within global cities},
journal = {Computers, Environment and Urban Systems},
volume = {85},
pages = {101539},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101539},
url = {https://www.sciencedirect.com/science/article/pii/S0198971520302726},
author = {Alessia Calafiore and Gregory Palmer and Sam Comber and Daniel Arribas-Bel and Alex Singleton},
keywords = {Foursquare, Geographic data science, Urban analytics},
abstract = {This study develops a Geographic Data Science framework that transforms the Foursquare check-in locations and user origin-destination flows data into knowledge about the emerging forms and characteristics of cities' neighbourhoods. We employ a longitudinal mobility dataset describing human interactions with Foursquare venues in ten global cities: Chicago, Istanbul, Jakarta, London, Los Angeles, New York, Paris, Seoul, Singapore, Tokyo. This social media data provides spatio-temporally referenced digital traces left by human use of urban environments, giving us access to the intangible aspects of urban life, such as people behaviours and preferences. Our framework capitalizes on these new data sources, bringing about a novel Geographic Data Science and human-centered methodological approach. Combining network science – a study area with great promise for the analysis of cities and their structure – with geospatial analysis methods, we model cities as a series of global urban networks. Through a spatially weighted community detection algorithm, we uncover functional neighbourhoods for the ten global cities. Each neighbourhood is linked to hyper-local characterisations of their built environment for the Foursquare venues that compose them, and complemented with a range of measures describing their diversity, morphology and mobility. This information is used in a clustering exercise that uncovers a set of four functional neighbourhood types. Our results enable the profiling and comparison of functional neighbourhoods, based on human dynamics and their contexts, across the sample of global cities. The framework is portable to other geographic contexts where interaction data are available to bind different localities into functional agglomerations, and provide insight into their contextual and human dynamics.}
}
@article{DAVID2021106630,
title = {Towards a comprehensive characterisation of the human internal chemical exposome: Challenges and perspectives},
journal = {Environment International},
volume = {156},
pages = {106630},
year = {2021},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2021.106630},
url = {https://www.sciencedirect.com/science/article/pii/S0160412021002555},
author = {Arthur David and Jade Chaker and Elliott J. Price and Vincent Bessonneau and Andrew J. Chetwynd and Chiara M. Vitale and Jana Klánová and Douglas I. Walker and Jean-Philippe Antignac and Robert Barouki and Gary W. Miller},
keywords = {Exposome, High-Resolution Mass Spectrometry, Internal chemical exposome, Non-targeted analysis, Suspect screening, EWAS},
abstract = {The holistic characterisation of the human internal chemical exposome using high-resolution mass spectrometry (HRMS) would be a step forward to investigate the environmental ætiology of chronic diseases with an unprecedented precision. HRMS-based methods are currently operational to reproducibly profile thousands of endogenous metabolites as well as externally-derived chemicals and their biotransformation products in a large number of biological samples from human cohorts. These approaches provide a solid ground for the discovery of unrecognised biomarkers of exposure and metabolic effects associated with many chronic diseases. Nevertheless, some limitations remain and have to be overcome so that chemical exposomics can provide unbiased detection of chemical exposures affecting disease susceptibility in epidemiological studies. Some of these limitations include (i) the lack of versatility of analytical techniques to capture the wide diversity of chemicals; (ii) the lack of analytical sensitivity that prevents the detection of exogenous (and endogenous) chemicals occurring at (ultra) trace levels from restricted sample amounts, and (iii) the lack of automation of the annotation/identification process. In this article, we discuss a number of technological and methodological limitations hindering applications of HRMS-based methods and propose initial steps to push towards a more comprehensive characterisation of the internal chemical exposome. We also discuss other challenges including the need for harmonisation and the difficulty inherent in assessing the dynamic nature of the internal chemical exposome, as well as the need for establishing a strong international collaboration, high level networking, and sustainable research infrastructure. A great amount of research, technological development and innovative bio-informatics tools are still needed to profile and characterise the “invisible” (not profiled), “hidden” (not detected) and “dark” (not annotated) components of the internal chemical exposome and concerted efforts across numerous research fields are paramount.}
}
@article{KARAGIANNIDIS2021108616,
title = {Data-driven modelling of ship propulsion and the effect of data pre-processing on the prediction of ship fuel consumption and speed loss},
journal = {Ocean Engineering},
volume = {222},
pages = {108616},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.108616},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821000512},
author = {Pavlos Karagiannidis and Nikos Themelis},
keywords = {Performance monitoring, Artificial neural network, Data processing, Propulsion modeling, Predictive analytics},
abstract = {Data-driven models for ship propulsion are presented while the effect of data pre-processing techniques is extensively examined. In this study, a large, automatically collected with high sampling frequency data set is exploited for training models that estimate the required shaft power or main engine fuel consumption of a container ship sailing under arbitrary conditions. Emphasis is given to the statistical evaluation and pre-processing of the data and two algorithms are presented for this scope. Additionally, state-of-the-art techniques for training and optimizing Feed-Forward Neural Networks (FNNs) are applied. The results indicate that with a delicate filtering and preparation stage it is possible to significantly increase the model's accuracy. Therefore, increase the prediction ability and awareness regarding the ship's hull and propeller actual condition. Furthermore, such models could be employed in studies targeting at the improvement of ship's operational energy efficiency.}
}
@incollection{2021399,
title = {Index},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {399-462},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10828-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717108280}
}
@article{ZHONG2021107767,
title = {Characteristics of vegetation response to drought in the CONUS based on long-term remote sensing and meteorological data},
journal = {Ecological Indicators},
volume = {127},
pages = {107767},
year = {2021},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2021.107767},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X21004325},
author = {Shaobo Zhong and Ziheng Sun and Liping Di},
keywords = {Drought, Standardized precipitation evapotranspiration index (SPEI), Vegetation condition index (VCI), Vegetation response, CONUS},
abstract = {Drought is one of the billion-dollar natural disasters and hard to trace and measure. In recent years drought monitoring becomes much easier with remote sensing. However, it is still difficult to pin vegetation variances on drought because of the delay of the caused vegetation stress. To assess vegetative drought, it is important to first understand the relationship between meteorological condition and vegetation condition, and measure the vegetation responses to meteorological drought. It would be very helpful for effective early warning about agricultural drought. This study uses the CONUS as the study area, and utilizes remote sensing products such as NDVI/VCI (normalized difference vegetation index/vegetation condition index) and SPI/SPEI (standardized precipitation index/standardized precipitation evapotranspiration index) to give a comparative evaluation to the vegetation’s drought response. The used vegetation products and meteorological data are ensured to be consistent. The scale and lag of vegetation response to drought for various vegetation types and aridity levels were thoroughly investigated. The results show that: The AVHRR and MODIS NDVI series pairs and the meteorological drought index series pairs (SPEI and SPI) have fairly good consistencies. Among them, 69.5% and 84% have rho (correlation coefficient) values greater than 0.8 respectively. For the NDVI series pairs, the maximum rho value is 0.98, the minimum rho value is −0.47, and the mean rho value is 0.79, which are 0.97, 0.68, and 0.87 respectively for the meteorological index series pairs. Compared to rho values of the meteorological index series pairs, the rho values of the NDVI series pairs have more outliers indicating instability. The correlation between SPEI and VCI significantly relies on time lags and has high spatial inhomogeneity. 1- and 2-month lags of SPEI have more significant positive correlation with VCI in Arid, Semi-Arid and Dry sub-humid areas of central west CONUS with less precipitation and lower temperature. For various time scales (time scale is SPEI reference range), the most significant positive correlation between SPEI and VCI happens in the time scales of 6- to 12-month in summer, the time scales of 3- and 6-month in spring and autumn, and the time scales of 2- and 3-month in winter regardless of time lags. Despite of the different vegetation types and aridity levels, the maximum correlations between SPEI and VCI are observed in Hyper arid regions in January, Arid regions in April, Semi-arid regions in July, and Dry sub-humid regions in October. Shrub has prominent responses in January and April, grass responses appear in July and October, and Evergreen forest shows least responses in all seasons. The results add more insights of the connection between vegetation and climate, and guide the development of new technology leveraging remote sensing data for vegetation drought monitoring and early-warning. The results are also helpful to provide important references for studying large-scale physiological and phenological properties of the vegetation under different climate conditions.}
}
@article{TANG2021142007,
title = {Robustness analysis of storm water quality modelling with LID infrastructures from natural event-based field monitoring},
journal = {Science of The Total Environment},
volume = {753},
pages = {142007},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.142007},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720355364},
author = {Sijie Tang and Jiping Jiang and Yi Zheng and Yi Hong and Eun-Sung Chung and Asaad Y. Shamseldin and Yan Wei and Xiuheng Wang},
keywords = {Robustness analysis, Stormwater quality, SWMM, Sponge city, Nash-Sutcliffe efficiency coefficient (NSE)},
abstract = {Sponge city construction (SCC) in China, as a new concept and a practical application of low-impact development (LID), is gaining wide popularity. Modelling tools are widely used to evaluate the ecological benefits of SCC in stormwater pollution mitigation. However, the understanding of the robustness of water quality modelling with different LID design options is still limited due to the paucity of water quality data as well as the high cost of water quality data collection and model calibration. This study develops a new concept of ‘robustness’ measured by model calibration performances. It combines an automatic calibration technique with intensive field monitoring data to perform the robustness analysis of storm water quality modelling using the SWMM (Storm Water Management Model). One of the national pilot areas of SCC, Fenghuang Cheng, in Shenzhen, China, is selected as the study area. Five water quality variables (COD, NH3-N, TN, TP, and SS) and 13 types of LID/non-LID infrastructures are simulated using 37 rainfall events. The results show that the model performance is satisfactory for different water quality variables and LID types. Water quality modelling of greenbelts and rain gardens has the best performance, while the models of barrels and green roofs are not as robust as those of the other LID types. In urban runoff, three water quality parameters, namely, SS, TN and COD, are better captured by the SWMM models than NH3-N and TP. The modelling performance tends to be better under heavy rain and significant pollutant concentrations, denoting a potentially more stable and reliable design of infrastructures. This study helps to improve the current understanding of the feasibility and robustness of using the SWMM model in sponge city design.}
}
@article{CHEN2021112348,
title = {Eddy morphology: Egg-like shape, overall spinning, and oceanographic implications},
journal = {Remote Sensing of Environment},
volume = {257},
pages = {112348},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112348},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721000663},
author = {Ge Chen and Jie Yang and Guiyan Han},
keywords = {Oceanic eddy, Morphology, Altimeter},
abstract = {Systematic tracking of individual eddies during their entire lifetimes marks a significant milestone in satellite oceanography over the first two decades of this century. Assuming that the geometry and properties of an oceanic eddy are orientation-sensitive, an angularly aligned composite analysis of over 40 million eddy-focused surface topographic “snapshots” obtained by merging tandem altimeter data from January 1993 through January 2019 reveals that oceanic vortices appear to have a characteristic surface shape of “egg” rather than circle or ellipse as previously understood. Consequently, a second-order moment in eddy morphology is revealed which leads to a ~ 10 km departure from a standard ellipse in terms of major axis. Furthermore, the sharp poles of oceanic eddies exhibit two quasi-orthogonal modes of orientation: primarily meridional and secondarily zonal. The additional submesoscale asymmetry in eddy shape is confirmed by a consistent anisotropy in a normalized eddy-centric velocity field derived from over 25 thousand drifters. The high-order moments in terms of geometric asymmetry and dynamic anisotropy associated with mesoscale eddies (which are found to be statistically significant at 99% level) may have profound geophysical and biological impacts on energy transport, substance entrainment, as well as ecosystem dynamics in the ocean. In particular, it is demonstrated that some of the previously identified patterns in eddy properties obtained by non-rotated normalization may be notably biased due to the ignorance of existing eddy orientation.}
}
@article{CHIOU2021100042,
title = {Travel pattern analytics driven by cellular signaling data},
journal = {Asian Transport Studies},
volume = {7},
pages = {100042},
year = {2021},
issn = {2185-5560},
doi = {https://doi.org/10.1016/j.eastsj.2021.100042},
url = {https://www.sciencedirect.com/science/article/pii/S2185556021000109},
author = {Yu-Chiun Chiou and Chih-Wei Hsieh},
keywords = {Cellular signaling data, Trip identification, Trip chaining pattern},
abstract = {Abstract
Cellular signaling data (CSD) could be one of the primary data sources for transportation planning and demand forecasting as a result of the rapid growth in triangulation and location techniques. Utilizing CSD to analyze travel patterns requires a meticulous process to overcome data oscillation and trajectory discontinuities. This study analyzes CSD and develops analytical models to enhance the applicability of CSD in transportation planning. For this study, we invite 30 volunteers to participate in a 30-day travel diary survey to collect data. In addition, based on CSD, we develop analytical algorithms to generate travel trajectories and analyze travel patterns, including the home and work location, trip chaining, and trip purpose of the user. Comparing the model results against the diary travel survey data indicates 84% accuracy for trip estimation and 89% accuracy for trip purpose identification, suggesting that the applicability of the proposed algorithm is satisfactory.}
}
@article{LI2021103955,
title = {Applications of distributed ledger technology (DLT) and Blockchain-enabled smart contracts in construction},
journal = {Automation in Construction},
volume = {132},
pages = {103955},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103955},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521004064},
author = {Jennifer Li and Mohamad Kassem},
keywords = {Blockchain, Building Information Modelling (BIM), Construction sector, Distributed ledger technology (DLT), Smart contracts, Systematic review, Thematic analysis},
abstract = {The contribution of distributed ledger technology (DLT) (e.g. blockchain) and smart contracts to the digitalisation and digital transformation of the construction sector is nascent but rapidly gaining traction. ‘Systematic reviews’ of DLT and smart contract applications that are specific to the construction sector are missing. This paper performs an extensive systematic review of 153 DLT and smart contract papers specific to the design, construction and operation of built assets. The protocols and processes of a systematic review were adopted to ensure full transparency, accountability, reproducibility, and updateability of the results. Through thematic analysis, we identify eight distinct themes of applications for DLT and smart contracts in construction: information management, payments, procurement, supply chain management, regulations and compliance, construction management and delivery, dispute resolution, and technological systems. Each theme identified was analysed to understand current capabilities, applications, and future developments. A cross-themes discussion revealed that DLT and smart contracts are ‘supplementary’ technologies that are used in combination with other technologies (e.g. BIM, IoT, cloud computing) as part of ‘technological systems’ that need to co-evolve in order to enable the themes' applications identified. Research into DLT and smart contracts in construction is rapidly moving from theoretical insights and frameworks into developing proofs-of-concept studies (27 studies) and testing them in case studies (20 studies). The next stage of research involving wider academic communities and industry-wide engagement is expected to begin uncovering the anticipated benefits of DLT and smart contracts through investments into technological systems and testing in real-world pilot studies. The discussion of the themes identified from technology, policy, process, and society perspectives exposed the need for an extended socio-technical approach to the solution in order to deliver the necessary change and impact from the adoption of DLT and smart contracts at speed and scale. The results of this systematic review provide a noteworthy reference point for academics, practitioners and policy makers interested in the future development of DLT and smart contract applications in construction.}
}
@article{LIU2021112364,
title = {Production of global daily seamless data cubes and quantification of global land cover change from 1985 to 2020 - iMap World 1.0},
journal = {Remote Sensing of Environment},
volume = {258},
pages = {112364},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112364},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721000821},
author = {Han Liu and Peng Gong and Jie Wang and Xi Wang and Grant Ning and Bing Xu},
keywords = {Global land cover mapping, Seamless data cube, Daily, Analysis ready data, Cloud computing, Intelligent mapping, Amazon web Services},
abstract = {Longer time high-resolution, high-frequency, consistent, and more detailed land cover data are urgently needed in order to achieve sustainable development goals on food security, high-quality habitat construction, biodiversity conservation and planetary health, and for the understanding, simulation and management of the Earth system. However, due to technological constraints, it is difficult to provide simultaneously high spatial resolution, high temporal frequency, and high quality observation data. Existing mapping solutions are limited by traditional remotely sensed data, that have shorter observation periods, poor spatio-temporal consistency and comparability. Therefore, a new mapping paradigm is needed. This paper develops a framework for intelligent mapping (iMap) of land cover based on state-of-the-art technologies such as cloud computing, artificial intelligence, virtual constellations, and spatio-temporal reconstruction and fusion. Under this framework, we built an automated, serverless, end-to-end data production chain and parallel mapping system based on Amazon Web Services (AWS) and produced the first 30 m global daily seamless data cubes (SDC), and annual to seasonal land cover maps for 1985–2020. The SDC was produced through a multi-source spatio-temporal data reconstruction and fusion workflow based on Landsat, MODIS, and AVHRR virtual constellations. Independent validation results show that the relative mean error of the SDC is less than 2.14%. As analysis ready data (ARD), it can lay a foundation for high-precision quantitative remote sensing information extraction. From this SDC, we produced 36-year long, 30 m resolution global land cover map data set by combining strategies of sample migration, machine learning, and spatio-temporal adjustment. The average overall accuracy of our annual land cover maps over multiple periods of time is 80% for level 1 classification and over 73% for level 2 classification (29 and 33 classes). Based on an objective validation sample consisting of FLUXNET sites, our map accuracy is 10% higher than that of existing global land cover datasets including Globeland30. Our results show that the average global land cover change rate is 0.36%/yr. Global forest decreased by 1.47 million km2 from 38.44 million km2, cropland increased by 0.84 million km2 from 12.49 million km2 and impervious surface increased by 0.48 million km2 from 0.57 million km2 during 1985– 2020.}
}
@article{ZHU2021106519,
title = {DNN-based seabed classification using differently weighted MBES multifeatures},
journal = {Marine Geology},
volume = {438},
pages = {106519},
year = {2021},
issn = {0025-3227},
doi = {https://doi.org/10.1016/j.margeo.2021.106519},
url = {https://www.sciencedirect.com/science/article/pii/S0025322721001018},
author = {Zhengren Zhu and Xiaodong Cui and Kai Zhang and Bo Ai and Bo Shi and Fanlin Yang},
keywords = {MBES, Backscatter mosaic, Angular response, Seabed sediment classification, Deep neural networks, Weight coefficient},
abstract = {Seabed sediment classification has significance for the utilization of marine resources and marine scientific research. Currently, the multibeam echo sounder (MBES) is increasingly becoming the tool of choice for large-scale seabed sediment classification. To further explore the technology of seabed sediment classification, this paper proposes a new classification method. In addition to backscatter mosaic, the method also integrates three other different types of features, including texture features of backscatter mosaic, MBES bathymetry features, and backscatter angular response (AR) features, which are given different weights in the classification process. First, geographically weighted regression (GWR) analysis is performed between different types of features and seabed sediment types, and the normalized coefficient of determination (R2) is employed as the weight coefficient for the different types of features. Second, the backscatter mosaic is combined with features from different types to predict the seabed sediment types using a deep neural network (DNN) classifier. Third, the classification residuals of the features from these three different types are acquired through the above classification results. Last, the classification residuals of features from different types are added to the classification results of the backscatter mosaic according to the weights, thereby achieving seabed sediment classification based on MBES multifeatures with different weights. The results show that the overall classification accuracy of the seabed sediments can be significantly improved from 88.98%/85.14% to 93.43% when using the DNN classification model based on MBES multifeatures with different weights compared with the other two models (DNN classification model based on MBES multifeatures with equal weights and DNN classification model based on principal component analysis (PCA) dimensionality reduction). The kappa coefficient can also be significantly improved from approximately 0.85/0.80 to 0.91. Via analysis, the proposed method can reasonably assign the weights of the different features and take advantage of integrating MBES multifeatures for seabed sediment classification. This approach also provides an important reference for future research on seabed sediment classification.}
}
@article{HAN202126,
title = {A Bayesian LSTM model to evaluate the effects of air pollution control regulations in Beijing, China},
journal = {Environmental Science & Policy},
volume = {115},
pages = {26-34},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2020.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1462901120313538},
author = {Yang Han and Jacqueline CK Lam and Victor OK Li and David Reiner},
keywords = {Air pollution control regulations, Effects of regulatory interventions, Bayesian LSTM, Propensity score, Counterfactual analysis, Causal inference},
abstract = {Rapid socio-economic development and urbanization have resulted in serious deterioration in air-quality in many world cities, including Beijing, China. This study attempts to examine the effectiveness of air pollution control regulations implemented in Beijing during 2008–2019 through a data-driven regulatory intervention analysis. Our proposed Bayesian deep learning model utilizes proxy data including Aerosol Optical Depth (AOD) and meteorology as well as socio-economic data, while accounting for confounding effects via propensity score estimation. Our results show that air pollution control regulatory measures implemented in China and Beijing during 2008–2019 reduced PM2.5 pollution in Beijing by 11 % on average. After the introduction of Action Plan for Clean Air in China and Beijing in late 2013, as compared to the hypothetical PM2.5 concentration (without any regulatory interventions), the estimated PM2.5 reduction increased dramatically from 15 % in 2015 to 44 % in 2018. Our results suggest that Beijing’s air quality has improved gradually over the past decade, though the annual PM2.5 pollution still exceeds the WHO threshold. In this regard, the air pollution control regulations introduced in Beijing and China tend to become more effective after 2015, suggesting a 2-year time lag before the stringent air pollution control regulations starting from 2013 takes any strong positive effects. Moreover, as compared to the air pollution control regulations introduced before 2013, newly introduced policy-making governance, which couples the policy-makings of the local jurisdictions with that of the central government, and the new policy measures that tackle the vested interests of the local stakeholders in Beijing and its nearby cities, alongside with the stringent local and national air pollution control regulations and plans, should help reduce air pollution and promote healthy living in Beijing over the longer term.}
}
@article{HUSAK2021517,
title = {Predictive methods in cyber defense: Current experience and research challenges},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {517-530},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20329836},
author = {Martin Husák and Václav Bartoš and Pavol Sokol and Andrej Gajdoš},
keywords = {Cybersecurity, Prediction, Forecasting, Data mining, Machine learning, Time series},
abstract = {Predictive analysis allows next-generation cyber defense that is more proactive than current approaches based on intrusion detection. In this paper, we discuss various aspects of predictive methods in cyber defense and illustrate them on three examples of recent approaches. The first approach uses data mining to extract frequent attack scenarios and uses them to project ongoing cyberattacks. The second approach uses a dynamic network entity reputation score to predict malicious actors. The third approach uses time series analysis to forecast attack rates in the network. This paper presents a unique evaluation of the three distinct methods in a common environment of an intrusion detection alert sharing platform, which allows for a comparison of the approaches and illustrates the capabilities of predictive analysis for current and future research and cybersecurity operations. Our experiments show that all three methods achieved a sufficient technology readiness level for experimental deployment in an operational setting with promising accuracy and usability. Namely prediction and projection methods, despite their differences, are highly usable for predictive blacklisting, the first provides a more detailed output, and the second is more extensible. Network security situation forecasting is lightweight and displays very high accuracy, but does not provide details on predicted events.}
}
@article{ZHAO2021102913,
title = {Impact of data processing on deriving micro-mobility patterns from vehicle availability data},
journal = {Transportation Research Part D: Transport and Environment},
volume = {97},
pages = {102913},
year = {2021},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2021.102913},
url = {https://www.sciencedirect.com/science/article/pii/S1361920921002121},
author = {Pengxiang Zhao and He Haitao and Aoyong Li and Ali Mansourian},
keywords = {Micro-mobility, E-scooter sharing, Data processing, Data sampling, Spatio-temporal patterns, Vehicle availability data, GPS, Trip identification},
abstract = {Vehicle availability data is emerging as a potential data source for micro-mobility research and applications. However, there is not yet research that systematically evaluates or validates the processing of this emerging mobility data. To fill this gap, we propose a generally applicable data processing framework and validate its related algorithms. The framework exploits micro-mobility vehicle availability data to identify individual trips and derive aggregate patterns by evaluating a range of temporal, spatial, and statistical mobility descriptors. The impact of data processing is systematically and rigorously investigated by applying the proposed framework with a case study dataset from Zurich, Switzerland. Our results demonstrate that the sampling rate used when collecting vehicle availability data has a significant and intricate impact on the derived micro-mobility patterns. This research calls for more attention to investigate various issues with emerging mobility data processing to ensure its validity for transportation research and practices.}
}
@article{MINBAEVA2021100820,
title = {Disrupted HR?},
journal = {Human Resource Management Review},
volume = {31},
number = {4},
pages = {100820},
year = {2021},
note = {Navigating the shifting landscapes of HRM},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2020.100820},
url = {https://www.sciencedirect.com/science/article/pii/S1053482220300930},
author = {Dana Minbaeva},
keywords = {Strategic HR, Disruption, Flexible workforce, Digitalization, Analytics},
abstract = {In this paper, I discuss possible avenues for future research aimed at bridging the research-practice gap on the topic of disruptions in human resources (HR). I focus on three global mega-trends—the flexible workforce, the digitalization of business models, and artificial intelligence and machine learning—and examine their influence on the field of human resource management (HRM) in general and in the context of the COVID-19 pandemic. I discuss why HRM research has overlooked potential paradigm-shifting possibilities that could ultimately equip HR practitioners with the knowledge needed to respond to disruptions caused by these mega-trends.}
}
@article{VANSANTEN2021264,
title = {Microbial natural product databases: moving forward in the multi-omics era},
journal = {Natural Product Reports},
volume = {38},
number = {1},
pages = {264-278},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d0np00053a},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822003555},
author = {Jeffrey A. {van Santen} and Satria A. Kautsar and Marnix H. Medema and Roger G. Linington},
abstract = {Covering: 2010–2020 The digital revolution is driving significant changes in how people store, distribute, and use information. With the advent of new technologies around linked data, machine learning and large-scale network inference, the natural products research field is beginning to embrace real-time sharing and large-scale analysis of digitized experimental data. Databases play a key role in this, as they allow systematic annotation and storage of data for both basic and advanced applications. The quality of the content, structure, and accessibility of these databases all contribute to their usefulness for the scientific community in practice. This review covers the development of databases relevant for microbial natural product discovery during the past decade (2010–2020), including repositories of chemical structures/properties, metabolomics, and genomic data (biosynthetic gene clusters). It provides an overview of the most important databases and their functionalities, highlights some early meta-analyses using such databases, and discusses basic principles to enable widespread interoperability between databases. Furthermore, it points out conceptual and practical challenges in the curation and usage of natural products databases. Finally, the review closes with a discussion of key action points required for the field moving forward, not only for database developers but for any scientist active in the field.}
}
@article{BUYUKOZKAN2021108309,
title = {Evaluating Blockchain requirements for effective digital supply chain management},
journal = {International Journal of Production Economics},
volume = {242},
pages = {108309},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108309},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321002851},
author = {Gülçin Büyüközkan and Gizem Tüfekçi and Deniz Uztürk},
keywords = {Blockchain, Digitalization, Supply chain, House of quality, Intuitionistic fuzzy sets, Incomplete preferences},
abstract = {Collapsing traditional supply chain (SC) nodes into digitally interconnected networks creates several opportunities for efficiency, cost savings, and SC traceability. Blockchain is one of the pioneering transformative 4.0 technologies and is seen as the backbone of digital SCs (DSCs). Several attempts have been made to create explorative designs or models, however, forming a strategic Blockchain vision remains a challenge. This paper specifies those customer needs and design requirements that should be prioritized with a systematic approach for effective Blockchain integration into SCs. The proposed functional methodology provides a practical roadmap for practitioners. It is based on the House of Quality (HoQ) method, with its customer-focused design aspect, utilizing a group decision making (GDM) approach with an incomplete intuitionistic fuzzy relation (IIFR) extension. The GDM approach is employed to overcome the biases of decision making, while IIFRs deal with different or complementary focus centers in qualitative data. The usefulness of this methodology is tested with an application, and the results are validated by experts in the field. The results indicate that a Blockchain-based DSC (BC-DSC) is expected to deliver continuing financial benefit, efficient use of time, and the ability to support. Automation, effective coordination, and conformity are identified as highly critical design requirements. GDM and IIFR are combined for the first time in the literature. The originality of this paper comes from its generation of critical factors geared to obtaining an effective BC-DSC structure and the implementation of HoQ as an efficient tool for design.}
}
@article{TIEDE2021112163,
title = {Investigating ESA Sentinel-2 products' systematic cloud cover overestimation in very high altitude areas},
journal = {Remote Sensing of Environment},
volume = {252},
pages = {112163},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.112163},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720305368},
author = {Dirk Tiede and Martin Sudmanns and Hannah Augustin and Andrea Baraldi},
keywords = {Sentinel-2 cloud mask, Cloud cover overestimation, Big EO data image selection, Geographical bias},
abstract = {Cloud detection in optical remote sensing imagery is crucial because undetected clouds can produce misleading results in analyses. Almost all optical remote sensing data access portals rely to some degree on a cloud cover filter. Here we show that cirrus as well as opaque cloud cover in Sentinel-2 Level-1C (L1C) and Level-2A (L2A) imagery is systematically and significantly overestimated in very high altitude areas (e.g. Himalayas, Andes). We argue that this systematic bias is created by applying simple thresholds to single bands instead of using a multi-band spectral signature in the cloud detection process. This results in a lot of “hidden” data for very high altitude areas when each image's estimated cloud cover is used as an automated selection criterion for analysis (e.g. global analyses, cloud-free mosaic production). We show geographic locations exemplifying this overestimation, and compare the L1C and L2A cloud masks produced by ESA to cloud masks generated by an expert system that uses comprehensive spectral signatures, showing that reliable cloud estimations are possible in very high altitudes. Based on this comparison, we argue for changes to L1C and L2A cloud detection algorithms in order to improve initial querying and selection of big EO data, where reliable yet automated quality indicators are necessary to handle an overwhelming data volume and velocity. Our contribution raises awareness of potential bias when pre-selecting images based on reported cloud cover in very high altitude areas for researchers and users of Sentinel-2 imagery in the environmental domain.}
}
@article{HOFFMANN2021102367,
title = {Improving the evidence base: A methodological review of the quantitative climate migration literature},
journal = {Global Environmental Change},
volume = {71},
pages = {102367},
year = {2021},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2021.102367},
url = {https://www.sciencedirect.com/science/article/pii/S0959378021001461},
author = {Roman Hoffmann and Barbora Šedová and Kira Vinke},
keywords = {Climate migration, Climate change, Systematic review, Meta-analysis, Methodology, Methods},
abstract = {The question whether and how climatic factors influence human migration has gained both academic and public interest in the past years. Based on two meta-analyses, this paper systematically reviews the quantitative empirical literature on climate-related migration from a methodological perspective. In total, information from 127 original micro- and macro-level studies is analyzed to assess how different concepts, research designs, and analytical methods shape our understanding of climate migration. We provide an overview of common methodological approaches and present evidence on their potential implications for the estimation of climatic impacts. We identify five key challenges, which relate to the i) measurement of migration and ii) climatic events, iii) the integration and aggregation of data, iv) the identification of causal relationships, and v) the exploration of contextual influences and mechanisms. Advances in research and modelling are discussed together with best practice cases to provide guidance to researchers studying the climate-migration nexus. We recommend for future empirical studies to employ approaches that are of relevance for and reflect local contexts, ensuring high levels of comparability and transparency.}
}
@article{KOUTSOUDIS20211,
title = {Multispectral aerial imagery-based 3D digitisation, segmentation and annotation of large scale urban areas of significant cultural value},
journal = {Journal of Cultural Heritage},
volume = {49},
pages = {1-9},
year = {2021},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1296207421000650},
author = {Anestis Koutsoudis and George Ioannakis and Petros Pistofidis and Fotis Arnaoutoglou and Nikolaos Kazakis and George Pavlidis and Chistodoulos Chamzas and Nestor Tsirliganis},
keywords = {multispectral, machine learning, 3D digitisation, 3D segmentation, annotation, structure from motion, urban, architecture, disaster management},
abstract = {Disaster risk management of movable and immovable cultural heritage is a highly significant research topic. In this work, we present a pipeline for 3D digitisation, segmentation and annotation of large scale urban areas in order to produce data that can be exploited in disaster management simulators (e.g fire spreading, crowd movement, firefighting training, evacuation planning, etc.). We have selected the old town of Xanthi (Greece) as a challenging case study. We developed a custom multispectral camera to be carried by a commercial drone. Using the structure from motion / multiview stereo (SFM/MVS) approach, we produced a 3D model of the urban area covering 0.5km2 that is followed by a multilayer texture map which carries information from visible and near-infrared regions of the electromagnetic spectrum. We developed a set of machine learning approaches based on logistic regression, support vector machines and artificial neural networks that allow 3D model segmentation by exploiting not only morphological and structural features but also the multispectral behaviour of different material surfaces. We objectively evaluate the performance of the proposed segmentation approaches on six significant material-based classes (cobbled-roads granite kilns, building walls, ceramic roof-tiles, low-vegetation, high-vegetation and metal surfaces) that are used in simulating fire propagation and crowd movement. The experiments revealed that the segmentation accuracy can be enhanced by taking into consideration surface material multispectral properties as well as morphological features. A Web-based multi-user annotation tool complements our proposed pipeline by enabling further 3D model segmentation, fine tuning and semantics annotation (e.g. usage-based building classification and evacuation priorities, escape paths and gathering points).}
}
@article{KRAJSIC202139,
title = {Semi-Supervised Anomaly Detection in Business Process Event Data using Self-Attention based Classification},
journal = {Procedia Computer Science},
volume = {192},
pages = {39-48},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921014927},
author = {Philippe Krajsic and Bogdan Franczyk},
keywords = {anomaly detection, business process, classification, event data, process mining},
abstract = {The analysis of business processes has become increasingly important in recent years, not least due to the emergence of analysis tools that enable data-centric views of processes and thus provide increasingly operational support for process flows. In this work, a semi-supervised classification model is presented that takes into account different developments in deep learning (e.g., deep generative models), time series analysis (e.g., long short-term memory) and sequence processing (e.g., attention mechanism) and combines them in one approach. The results of the experimental implementation of the classification model show that it is able to filter activity-related and time-related anomalies from the event data and outperform existing approaches in its classification accuracy (F1 score). The classification model achieves an F1 score of up to 93%.}
}
@article{ZHANG2021116641,
title = {Spatiotemporal wind field prediction based on physics-informed deep learning and LIDAR measurements},
journal = {Applied Energy},
volume = {288},
pages = {116641},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116641},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921001732},
author = {Jincheng Zhang and Xiaowei Zhao},
keywords = {Deep learning, LIDAR measurements, Physics-informed neural networks, Wind field prediction},
abstract = {Spatiotemporal wind field information is of great interest in wind industry e.g. for wind resource assessment and wind turbine/farm monitoring & control. However, its measurement is not feasible because only sparse point measurements are available with the current sensor technology such as LIDAR. This work fills the gap by developing a method that can achieve spatiotemporal wind field predictions by combining LIDAR measurements and flow physics. Specifically, a deep neural network is constructed and the Navier–Stokes equations, which provide a good description of atmospheric flows, are incorporated in the deep neural network by employing the physics-informed deep learning technique. The training of this physics-incorporated deep learning model only requires the sparse LIDAR measurement data while the spatiotemporal wind field in the whole domain (which cannot be measured) can be predicted after training. This study, which can discover complex wind patterns that do not present in the training dataset, is totally distinct from previous machine learning based wind prediction studies which treat machine learning models as “black-box” and require the corresponding input and target values to learn complex relations. The numerical results on the prediction of the wind field in front of a wind turbine show that the proposed method predicts the spatiotemporal flow velocity (including both downwind and crosswind components) in the whole domain very well for a wide range of scenarios (including various measurement noises, resolutions, LIDAR look directions, and turbulence levels), which is promising given that only line-of-sight wind speed measurements at sparse locations are used.}
}
@article{RODRIGUEZ2021106442,
title = {IoT-Agro: A smart farming system to Colombian coffee farms},
journal = {Computers and Electronics in Agriculture},
volume = {190},
pages = {106442},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106442},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921004592},
author = {Jhonn Pablo Rodríguez and Ana Isabel Montoya-Munoz and Carlos Rodriguez-Pabon and Javier Hoyos and Juan Carlos Corrales},
keywords = {Smart Farming, Internet of Things, Data Analytics, Outlier Detection, BPMN, Coffee Farm},
abstract = {Currently, the adoption of smart technologies for sustainable farming systems creates a distinct competitive edge for farmers, extension services, agri-business, and policy-makers. However, selecting the most appropriate technologies from a wide range of options is never an easy job. In this context, several authors consider Smart Farming as the best solution. However, they fall short in providing more information to recommend the most appropriate IoT technology, the options to manage the IoT infrastructure, and the services to crop management plans and crop production estimation. This paper implements a Smart Farming System based on a three-layered architecture (Agriculture Perception, Edge Computing, and Data Analytics). In the Agriculture Perception Layer, we evaluated Omicron, Libelium, and Intel technologies under criteria such as the price, the number of inputs for sensor connection, communication protocols, portability, battery life, and harvesting energy system photovoltaic panel. We evaluated edge-based management mechanisms in the Edge Layer to provide data reliability, focusing on outlier detection and treatment using Machine Learning and Interpolation algorithms. We recommend the Isolation Forest algorithm for classifying outliers in the monthly temperature dataset (99% of precision) and the Cubic Spline technique for effectively replacing the data classified as outliers (RMSE lower than 0.085). In the Data Analytics Layer, we evaluated different machine learning algorithms to estimate coffee production. The results show that the measured error values of the XGBOOST algorithm keep the values lower than the other models (RMSE 0.008, MAE 0.032, and RSE 0.585). The www.iot-agro.com platform offers farmer services such as weather variables monitoring, coffee production estimating, and IoT infrastructure setting. Finally, stakeholders, researchers, and engineers validated our Smart Farming Solution through a Colombian coffee farm case study. The test evaluated the usability, the straightforward interpretation of data, and the look feel of the web application.}
}
@article{PRZYBYLAKASPEREK20213560,
title = {Stop Criterion in Building Decision Trees with Bagging Method for Dispersed Data},
journal = {Procedia Computer Science},
volume = {192},
pages = {3560-3569},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.129},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921018688},
author = {Małgorzata Przybyła-Kasperek and Samuel Aning},
keywords = {Ensemble of classifiers, Dispersed data, Stop criterion, Bagging method, Classification trees, Independent data sources},
abstract = {This article discusses issues related to decision making based on applying decision trees and bagging methods on dispersed knowledge. In dispersed knowledge, local decision tables possess data independently in fragments. In this study, sub-tables are further generated with bagging method for each local table, based on which the decision trees are built. These decision trees classify the test object, and a probability vector is defined over the decision classes for each local table. For each vector, decision classes with the maximum value of the coordinates are selected and final joint decisions for all local tables are made by majority voting. Quality of decision making has been observed to increase when bagging method as an ensemble method is combined with decision trees on independent dispersed data. An important criterion in building a decision tree is to know when to stop growing the tree (stop splitting). That is, at what minimum number of objects on a working node do we stop building the tree to ensure the best decision results. The contribution of the paper is to observe the influence a stop criterion (expressed in the number of objects in the node) for decision trees used in conjunction with bagging method on independent data sources. It can be concluded that in dispersed data set, the stop split criteria does not influence the classification quality much. The statistical significance of the difference in the mean classification error values was confirmed only for a very high stop criterion (0.1× number of objects in training set) and for a very low stop criterion (equal to two). There is no significant statistical difference in the classification quality obtained for the stop criterion values: 4, 6, 8 and 10. An interesting remark is that for some dispersed data sets, in the case of smaller number of local tables and larger number of bootstrap samples, better quality of classification is obtained for a small number of objects in the stop criterion (mostly for two objects). Only, at a significant increase in the minimum number of objects at which growth of trees is stopped is quality of classification affected. However, the gain in reducing the complexity for trees that we get when using the larger values of stop criterion is significant.}
}
@article{NOMURA2021446,
title = {Pain Management in Clinical Practice Research Using Electronic Health Records},
journal = {Pain Management Nursing},
volume = {22},
number = {4},
pages = {446-454},
year = {2021},
issn = {1524-9042},
doi = {https://doi.org/10.1016/j.pmn.2021.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S152490422100031X},
author = {Aline Tsuma Gaedke Nomura and Lisiane Pruinelli and Luciana Nabinger Menna Barreto and Murilo dos Santos Graeff and Elizabeth A. Swanson and Thamiris Silveira and Miriam de Abreu Almeida},
abstract = {Background: The use of electronic health record (EHR) systems encourages and facilitates the use of data for the development and surveillance of quality indicators, including pain management. Aim: to conduct an integrative review on pain management research using data extracted from EHR in order to synthesize and analyze the following elements: pain management (assessments, interventions, and outcomes) and study results with potential clinical implications, data source, clinical sample characteristics, and method description. Design: An integrative review of the literature was undertaken to identify exemplars of scientific research studies that explore pain management using data from EHR, using Cooper’s framework. Results: Our search of 1,061 records from PubMed, Scopus, and Cinahl was narrowed down to 28 eligible articles to be analyzed. Conclusion: Results of this integrative review will make a critical contribution, assisting others in developing research proposals and sound research methods, as well as providing an overview of such studies over the past 10 years. Through this review it is therefore possible to guide new research on clinical pain management using EHR.}
}
@article{WU2021275,
title = {Machine learning-based predictive control using noisy data: evaluating performance and robustness via a large-scale process simulator},
journal = {Chemical Engineering Research and Design},
volume = {168},
pages = {275-287},
year = {2021},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2021.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0263876221000551},
author = {Zhe Wu and Junwei Luo and David Rincon and Panagiotis D. Christofides},
keywords = {Machine learning, Long short-term memory neural networks, Noisy data, Model predictive control, Nonlinear systems, Chemical processes},
abstract = {Machine learning modeling of chemical processes using noisy data is a practically challenging task due to the occurrence of overfitting during learning. In this work, we propose a dropout method and a co-teaching learning algorithm that develop long short-term memory (LSTM) neural networks to capture the ground truth (i.e., underlying process dynamics) from noisy data. To evaluate the performance and robustness of the proposed modeling approaches, we consider an industrial chemical reactor example and use a large-scale process simulator, Aspen Plus Dynamics that does not employ assumptions on reactor properties typically made in the derivation of first-principles models, to generate process operational data that are corrupted by sensor noise which is determined using industrial data. The dropout method is first utilized to reduce the overfitting of LSTM models to noisy data. Then, another approach termed co-teaching method is used to train LSTM models with additional noise-free data generated from simulations of the reactor first-principles model that employs several standard modeling assumptions not made in the Aspen model. Through open-loop and closed-loop simulations, we demonstrate the improvement of model prediction accuracy and of the open- and closed-loop performances under model predictive controllers using dropout and co-teaching LSTM neural network models compared to the LSTM model developed from the standard training process from the noisy data.}
}
@article{HASAN2021100799,
title = {Missing value imputation affects the performance of machine learning: A review and analysis of the literature (2010–2021)},
journal = {Informatics in Medicine Unlocked},
volume = {27},
pages = {100799},
year = {2021},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2021.100799},
url = {https://www.sciencedirect.com/science/article/pii/S2352914821002653},
author = {Md. Kamrul Hasan and Md. Ashraful Alam and Shidhartho Roy and Aishwariya Dutta and Md. Tasnim Jawad and Sunanda Das},
keywords = {Incomplete datasets, Imputation methods and evaluations, Machine learning classifiers and evaluations, PRISMA technique},
abstract = {Recently, numerous studies have been conducted on Missing Value Imputation (MVI), intending the primary solution scheme for the datasets containing one or more missing attribute’s values. The incorporation of MVI reinforces the Machine Learning (ML) models’ performance and necessitates a systematic review of MVI methodologies employed for different tasks and datasets. It will aid beginners as guidance towards composing an effective ML-based decision-making system in various fields of applications. This article aims to conduct a rigorous review and analysis of the state-of-the-art MVI methods in the literature published in the last decade. Altogether, 191 articles, published from 2010 to August 2021, are selected for review using the well-known Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) technique. We summarize those articles with relevant definitions, theories, and analyses to provide essential information for building a precise decision-making framework. In addition, the evaluation metrics employed for MVI methods and ML-based classification models are also discussed and explored. Remarkably, the trends for the MVI method and its evaluation are also scrutinized from the last twelve years’ data. To come up with the conclusion, several ML-based pipelines, where the MVI schemes are incorporated for performance enhancement, are investigated and reviewed for many different datasets. In the end, informative observations and recommendations are addressed for future research directions and trends in related fields of interest.}
}
@article{ZHAO2021103465,
title = {Effect of an agency’s resources on the implementation of open government data},
journal = {Information & Management},
volume = {58},
number = {4},
pages = {103465},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103465},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621000392},
author = {Yupan Zhao and Bo Fan},
keywords = {OGD, Resource-based theory, Resource allocation, Influencing factors},
abstract = {Open government data (OGD) exhibits substantial political, economic, cultural, and social values that have gained considerable attention globally. Based on the investigation and analysis of the OGD practice, this study raises the research question, “Why do considerable differences exist in the degree of OGD implementation among different agencies under the same local government?” Our study takes resource-based theory as theoretical foundation to explore the factors that affect OGD implementation of constituent agencies within the same local government. A questionnaire survey is conducted to analyze the effect of factors including technical capacity, organizational awareness, organizational arrangement, and rules and regulations on OGD implementation. Results show that the technical capacity, organizational arrangement, and rules and regulations of government agencies have a direct positive effect on OGD implementation. Notably, rules and regulations moderate the relationship between technical capacity and OGD implementation. Besides, the matching degree of technical capacity and other organizational factors in a government agency exerts a positive influence on OGD implementation. Finally, our study proposes policy suggestions that emphasize the direction and focus for OGD implementation.}
}
@article{DIVAN2021106871,
title = {Metadata-based measurements transmission verified by a Merkle Tree},
journal = {Knowledge-Based Systems},
volume = {219},
pages = {106871},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106871},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001349},
author = {Mario José Diván and María Laura Sánchez-Reynoso},
keywords = {Measurement, Metadata-guided transmission, Brief data message, Integrity record, Data streaming},
abstract = {The Data Stream Processing Strategy (DSPS) is focused on the automatization of measurement projects based on a measurement framework. The measurement adapter (MA) is an architecture component located on mobile devices aims to integrate heterogeneous data sources (i.e., sensors). The Gathering Function (GF) is the component responsible for interacting and receiving measures from the MAs, and it resides on the Stream Processing Engine (SPE). MA and GF share the project definition based on a measurement framework to foster data interoperability, while MA regulates the frequency, size, and route related to data transmission. As contributions (i) The brief data message is introduced to optimize the data transmission keeping immutable the hierarchical data organization based on the project definition, and (ii) The integrity record for mobile and SPE environments is described based on a Merkle Tree. This allows optimizing each data transaction, incorporating a historical integrity record both MA and SPE. The proposals and simulations have been implemented on the cincamimis, cincamipd, mair, and pabmmcommons libraries, which are freely available on GitHub under the terms of the Apache 2.0 licence. Four simulations are explained to detail how to measures were obtained. Interesting results show that the brief data message consumes 17.50 KB to transmit 1000 measures (2.4 times smaller than JSON), while a message with 200 measures could be generated and compressed using GZIP in 25.12 ms (2.43 times faster than JSON). 196 KB is required to keep 17 min of the integrity history in a MA, being created in 4.85 ms.}
}
@article{BECK2021117441,
title = {White matter microstructure across the adult lifespan: A mixed longitudinal and cross-sectional study using advanced diffusion models and brain-age prediction},
journal = {NeuroImage},
volume = {224},
pages = {117441},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117441},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920309265},
author = {Dani Beck and Ann-Marie G. {de Lange} and Ivan I. Maximov and Geneviève Richard and Ole A. Andreassen and Jan E. Nordvik and Lars T. Westlye},
keywords = {Ageing, White matter, Multi-shell, Longitudinal,, Diffusion, Brain age},
abstract = {The macro- and microstructural architecture of human brain white matter undergoes substantial alterations throughout development and ageing. Most of our understanding of the spatial and temporal characteristics of these lifespan adaptations come from magnetic resonance imaging (MRI), including diffusion MRI (dMRI), which enables visualisation and quantification of brain white matter with unprecedented sensitivity and detail. However, with some notable exceptions, previous studies have relied on cross-sectional designs, limited age ranges, and diffusion tensor imaging (DTI) based on conventional single-shell dMRI. In this mixed cross-sectional and longitudinal study (mean interval: 15.2 months) including 702 multi-shell dMRI datasets, we combined complementary dMRI models to investigate age trajectories in healthy individuals aged 18 to 94 years (57.12% women). Using linear mixed effect models and machine learning based brain age prediction, we assessed the age-dependence of diffusion metrics, and compared the age prediction accuracy of six different diffusion models, including diffusion tensor (DTI) and kurtosis imaging (DKI), neurite orientation dispersion and density imaging (NODDI), restriction spectrum imaging (RSI), spherical mean technique multi-compartment (SMT-mc), and white matter tract integrity (WMTI). The results showed that the age slopes for conventional DTI metrics (fractional anisotropy [FA], mean diffusivity [MD], axial diffusivity [AD], radial diffusivity [RD]) were largely consistent with previous research, and that the highest performing advanced dMRI models showed comparable age prediction accuracy to conventional DTI. Linear mixed effects models and Wilk's theorem analysis showed that the ‘FA fine’ metric of the RSI model and ‘orientation dispersion’ (OD) metric of the NODDI model showed the highest sensitivity to age. The results indicate that advanced diffusion models (DKI, NODDI, RSI, SMT mc, WMTI) provide sensitive measures of age-related microstructural changes of white matter in the brain that complement and extend the contribution of conventional DTI.}
}
@article{VANEM2021103158,
title = {Data-driven state of health modelling—A review of state of the art and reflections on applications for maritime battery systems},
journal = {Journal of Energy Storage},
volume = {43},
pages = {103158},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.103158},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21008598},
author = {Erik Vanem and Clara Bertinelli Salucci and Azzeddine Bakdi and Øystein Å sheim Alnes},
keywords = {Battery state of health, Degradation modelling, Capacity, Maritime battery systems, Data-driven modelling},
abstract = {Battery systems are becoming an increasingly attractive alternative for powering ocean going ships, and the number of fully electric or hybrid ships relying on battery power for propulsion and manoeuvring is growing. In order to ensure the safety of such electric ships, it is of paramount importance to monitor the available energy that can be stored in the batteries, and classification societies typically require that the state of health of the batteries can be verified by independent tests — annual capacity tests. However, this paper discusses data-driven state of health modelling for maritime battery systems based on operational sensor data collected from the batteries as an alternative approach. Thus, this paper presents a comprehensive review of different data-driven approaches to state of health modelling, and aims at giving an overview of current state of the art. More than 300 papers have been reviewed, most of which are referred to in this paper. Moreover, some reflections and discussions on what types of approaches can be suitable for modelling and independent verification of state of health for maritime battery systems are presented.}
}
@article{WANG2021192,
title = {Quantifying the dynamics between environmental information disclosure and firms’ financial performance using functional data analysis},
journal = {Sustainable Production and Consumption},
volume = {28},
pages = {192-205},
year = {2021},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2021.03.026},
url = {https://www.sciencedirect.com/science/article/pii/S2352550921000993},
author = {Deqing Wang and Xuemei Li and Sihua Tian and Lingyun He and Yan Xu and Xu Wang},
keywords = {Environmental information disclosure, Firm financial performance, Data smoothing, Functional regression analysis, Moderating effect},
abstract = {Environmental information disclosure (EID) is an important way for firms to communicate to the government and the public to fulfill their environmental protection responsibilities. Essentially, the dynamic impacts of firms’ activities on the ecological environment are evolving continuously. We aim to introduce functional data analysis (FDA) for exploring the dynamics in the relationship between environmental information disclosure (EID) and firms’ financial performance. Based on continuous curves smoothed from 75 Chinese listed firms of pollution-intensive industries, this study examined the dynamic effect and its structural break of EID on firms' financial performance. Furthermore, moderating effects of public attention, government subsidy and ratio of profits to total cost were tested within a functional framework. The results revealed that the positive effect of EID on firms' financial performance is constantly significant, whereas there existed a structural break in 2015 due to the implementation of new Environmental Protection Law. Moreover, the positive moderating effect of the ratio of profits to total cost is significant only before 2015, while both the main effect and moderating effect of government subsidy are not significant. Surprisingly, although the main effects of public attention are not significant, its positive moderating effect is statistically significant. We contributed in introducing FDA as a useful toolkit for quantifying the time-dynamics in ecological economics, and our findings could offer guidance for stakeholders seeking to improve EID.}
}
@article{NIU2021102276,
title = {Incentive alignment for blockchain adoption in medicine supply chains},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {152},
pages = {102276},
year = {2021},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2021.102276},
url = {https://www.sciencedirect.com/science/article/pii/S136655452100051X},
author = {Baozhuang Niu and Jian Dong and Yaoqi Liu},
keywords = {Blockchain technology, Social goods, Supply chain competition, Information sharing},
abstract = {In recent years, blockchain technology has been increasingly adopted in OTC medicine supply chains, enabling customers to track the entire process from raw material purchasing to finished medicine distribution. This improves the brand image and hence expands the market. With the use of blockchain, information transparency can be achieved because data are stored immutably and safely in a distributed database that is accessible by all supply chain members. However, will the incentives for supply chain members to participate in blockchain for larger-scale demand come at the cost of information disclosure? In this paper, to investigate the supply chain members’ incentive alignment opportunities towards the adoption of blockchain technology, we consider a two-stage supply chain comprising two medicine manufacturers and a common retailer that has more accurate demand information than the manufacturers have. We find that, interestingly, the retailer has incentives to participate in blockchain when the manufacturers’ competition is mild and the demand variance is low. We further investigate the impact of blockchain on total surplus and customer surplus and find that the adoption of blockchain always benefits customers and society; therefore, blockchain can be particularly useful for social goods such as OTC medicine.}
}
@article{ZHANG2021128801,
title = {Satellite-based ground PM2.5 estimation using a gradient boosting decision tree},
journal = {Chemosphere},
volume = {268},
pages = {128801},
year = {2021},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2020.128801},
url = {https://www.sciencedirect.com/science/article/pii/S0045653520329994},
author = {Tianning Zhang and Weihuan He and Hui Zheng and Yaoping Cui and Hongquan Song and Shenglei Fu},
keywords = {Aerosol optical depth, Air pollution, Machine learning, MODIS, Particulate matter},
abstract = {Fine particulate matter with an aerodynamic diameter less than 2.5 μm (PM2.5) is one of the major air pollutants risks to human health worldwide. Satellite-based aerosol optical depth (AOD) products are an effective metric for acquiring PM2.5 information, featuring broad coverage and high resolution, which compensate for the sparse and uneven distribution of existing monitoring stations. In this study, a gradient boosting decision tree (GBDT) model for estimating ground PM2.5 concentration directly from AOD products across China in 2017, integrating human activities and various natural variables was proposed. The GBDT model performed well in estimating temporal variability and spatial contrasts in daily PM2.5 concentrations, with relatively high fitted model (10-fold cross-validation) coefficients of determination of 0.98 (0.81), low root mean square errors of 3.82 (11.57) μg/m3, and mean absolute error of 1.44 (7.45) μg/m3. Seasonal examinations revealed that summer had the cleanest air with the highest estimation accuracies, whereas winter had the most polluted air with the lowest estimation accuracies. The model successfully captured the PM2.5 distribution pattern across China in 2017, showing high levels in southwest Xinjiang, the North China Plain, and the Sichuan Basin, especially in winter. Compared with other models, the GBDT model showed the highest performance in the estimation of PM2.5 with a 3-km resolution. This algorithm can be adopted to improve the accuracy of PM2.5 estimation with higher spatial resolution, especially in summer. In general, this study provided a potential method of improving the accuracy of satellite-based ground PM2.5 estimation.}
}
@article{MALANDRI2021103341,
title = {MEET-LM: A method for embeddings evaluation for taxonomic data in the labour market},
journal = {Computers in Industry},
volume = {124},
pages = {103341},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103341},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305753},
author = {Lorenzo Malandri and Fabio Mercorio and Mario Mezzanzanica and Navid Nobani},
keywords = {Embeddings evaluation, Taxonomies, Semantic hierarchies, Labour market, ICT},
abstract = {Taxonomies are the mainstay of the semantic web as they aim at organising knowledge in concepts linked by IS-A relationships. However, keeping such hierarchies updated and able to represent the domain from which they have been drawn is still a time-consuming, costly and error prone activity. Here, word embeddings have proven to be effective in catching lexicon and semantic similarities to enrich taxonomies from text data. This, in turn, would require to evaluate the generated embeddings to estimate the extent to which they encode the semantic similarity derived from the hierarchy itself. In this paper, we propose and implement MEET-LM, a methodology that aims at generating and evaluating embeddings from a text corpus preserving the co-hyponymy relations synthesised from a domain-specific taxonomy. We apply MEET-LM to a real-life dataset of 2M+ vacancies related to ICT-jobs, framed within the research activities of an EU project that collects millions of Online Job Vacancies and classifies them within the European standard hierarchy ESCO. To show MEET-LM is useful in practice, we also trained a neural network to classify co-hyponym relations using the selected embeddings as features. Our experiments reach 99.4% of accuracy and 86.5% of f1-score.}
}
@article{SCHREIBER2021120894,
title = {Application of data-driven methods for energy system modelling demonstrated on an adaptive cooling supply system},
journal = {Energy},
volume = {230},
pages = {120894},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.120894},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221011427},
author = {Thomas Schreiber and Christoph Netsch and Sören Eschweiler and Tianyuan Wang and Thomas Storek and Marc Baranski and Dirk Müller},
keywords = {Machine-learning, Supervised learning, Building automation and control, Data-driven modelling, Optimal control},
abstract = {The efficient and sustainable operation of building energy systems is playing an increasingly important role in most industrialized countries. At the same time, building energy systems are becoming increasingly complex; fault-free and optimal operation, under dynamic boundary conditions, is becoming more and more challenging. There are many approaches in research to address the optimal control problem of building energy systems, such as Rule-based Control, Model Predictive Control, or Adaptive Control. However, most methods rely on models of the system dynamics with high prediction accuracies. This is especially the case in Model Predictive Control, where the model is part of a continuously executed optimization problem; but models are also required when it comes to the optimal design of Rule-based Controllers, the safe pre-training of Adaptive Controllers, or model-based fault detection. A limiting factor for the manual development of physical models, for building energy systems, are the low monetary incentives for engineering services, due to the low energy prices in most countries. In addition, the creation of such models is time-consuming and error-prone, even for domain experts. Another weakness is that changes in the system dynamics are not automatically adapted within the models. These challenges are contrasted by an increasing availability of monitoring-data and computational power in recent years; with machine-learning algorithms, these resources are used in numerous application areas to achieve very promising results. Machine-learning methods can help to obtain data-driven, self-calibrating models, which can be learned from monitoring-data. In this paper, we apply methods for automated data-driven model generation. We demonstrate how machine-learning algorithms together with structured hyper-parameter tuning can be used to model individual subsystems as well as a complete energy supply system. To represent the dynamics of the supply system, it is first decomposed into simple functional relationships, which are aggregated into the overall system after training of the comparatively simple subsystem models. We evaluate the accuracy of the data-driven subsystem models using established metrics for the evaluation of regression models, namely the R2-score and the RMSE. The considered system is integrated into a district cooling network and consists of two compression chillers and an ice storage unit. Our investigations show that the dynamics of the subsystems can be learned with high accuracies, depending on the operation mode and the selected features. The prediction of the power demand of the compression chillers is learned with R2-scores between 0.94 and 0.99 and RMSE values between 2.02 kW and 3.51 kW. Also, the prediction of the percentage of ice formation within the ice storage is learned accurately with a R2-score of 1 and RMSE values between 0.08 % and 0.72 %. The dynamics of the aggregated system also show plausible behavior and can thus be used in future work. This work is part of an ongoing research project with the aim to optimize the operation of the entire campus cooling energy supply system. Our results show that, if detailed monitoring-data are available, data-driven modelling represents a viable alternative to the labor-intensive physical modelling approach. Furthermore, we emphasize the importance of structured hyper-parameter tuning, discuss the specifics of different machine-learning algorithms, and elaborate on possible future developments in this research area.}
}
@article{BOSSA2021100190,
title = {FAIRification of nanosafety data to improve applicability of (Q)SAR approaches: A case study on in vitro Comet assay genotoxicity data},
journal = {Computational Toxicology},
volume = {20},
pages = {100190},
year = {2021},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2021.100190},
url = {https://www.sciencedirect.com/science/article/pii/S2468111321000384},
author = {Cecilia Bossa and Cristina Andreoli and Martine Bakker and Flavia Barone and Isabella {De Angelis} and Nina Jeliazkova and Penny Nymark and Chiara Laura Battistelli},
keywords = {Nanomaterials, FAIR principles, (Q)SAR approaches, Nanosafety data, Genotoxicity,  Comet assay},
abstract = {(Quantitative) structure-activity relationship ([Q]SAR) methodologies are widely applied to predict the (eco)toxicological effects of chemicals, and their use is envisaged in different regulatory frameworks for filling data gaps of untested substances. However, their application to the risk assessment of nanomaterials is still limited, also due to the scarcity of large and curated experimental datasets. Despite a great amount of nanosafety data having been produced over the last decade in international collaborative initiatives, their interpretation, integration and reuse has been hampered by several obstacles, such as poorly described (meta)data, non-standard terminology, lack of harmonized reporting formats and criteria. Recently, the FAIR (Findable, Accessible, Interoperable, and Reusable) principles have been established to guide the scientific community in good data management and stewardship. The EU H2020 Gov4Nano project, together with other international projects and initiatives, is addressing the challenge of improving nanosafety data FAIRness, for maximizing their availability, understanding, exchange and ultimately their reuse. These efforts are largely supported by the creation of a common Nanosafety Data Interface, which connects a row of project-specific databases applying the eNanoMapper data model. A wide variety of experimental data relating to characterization and effects of nanomaterials are stored in the database; however, the methods, protocols and parameters driving their generation are not fully mature. This article reports the progress of an ongoing case study in the Gov4nano project on the reuse of in vitro Comet genotoxicity data, focusing on the issues and challenges encountered in their FAIRification through the eNanoMapper data model. The case study is part of an iterative process in which the FAIRification of data supports the understanding of the phenomena underlying their generation and, ultimately, improves their reusability.}
}
@article{RANJAN2021960,
title = {Ocular artifact elimination from electroencephalography signals: A systematic review},
journal = {Biocybernetics and Biomedical Engineering},
volume = {41},
number = {3},
pages = {960-996},
year = {2021},
issn = {0208-5216},
doi = {https://doi.org/10.1016/j.bbe.2021.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0208521621000838},
author = {Rakesh Ranjan and Bikash {Chandra Sahana} and Ashish {Kumar Bhandari}},
keywords = {EEG signal, Ocular artifact, Artifact removal techniques, Hybrid method, Performance evaluation, Brain-computer interface},
abstract = {Electroencephalography (EEG) is the signal of intrigue that has immense application in the clinical diagnosis of various neurological, psychiatric, psychological, psychophysiological, and neurocognitive disorders. It is significantly crucial in neural communication, brain-computer interface, and other practical tasks. EEG signal is exceptionally susceptible to artifacts, which are external noise signals originated from non-cerebral regions. The interference of artifacts in EEG signals can potentially affect the original recorded EEG signal quality and pattern. Therefore, artifact removal from EEG signal is critically important before applying it to a specific task for accurate outcomes. Researchers have proposed numerous techniques to remove various artifacts present in the contaminated EEG signal. However, neither optimum method nor criterion stands standard for endorsement of clinically recorded EEG signals. Therefore, the research related to artifact elimination from EEG signal is challenging and perplexing task. This paper attempts to give an extensive outline of the advancement in methodologies to eliminate one of the most common artifacts, i.e., ocular artifact. It is anticipated that the study will enlighten the researchers on all the existing ocular artifact elimination techniques with a validated simulation model on the recorded EEG signal. In future advancements, Standard norms in artifact elimination techniques are expected to diminish the neurologist's load by substantiating the clinical diagnosis after gaining correct information from artifact-free EEG signals.}
}
@incollection{2021361,
title = {Index},
editor = {Guy J-P. Schumann},
booktitle = {Earth Observation for Flood Applications},
publisher = {Elsevier},
pages = {361-370},
year = {2021},
series = {Earth Observation},
isbn = {978-0-12-819412-6},
doi = {https://doi.org/10.1016/B978-0-12-819412-6.00021-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194126000213}
}
@article{GRUNZE2021178,
title = {“Apples and pears are similar, but still different things.” Bipolar disorder and schizophrenia- discrete disorders or just dimensions ?},
journal = {Journal of Affective Disorders},
volume = {290},
pages = {178-187},
year = {2021},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2021.04.064},
url = {https://www.sciencedirect.com/science/article/pii/S0165032721003967},
author = {Heinz Grunze and Marcelo Cetkovich-Bakmas},
keywords = {Bipolar disorder, Schizophrenia, Dichotomy, Phenomenology, Neurobiology, Computational psychiatry},
abstract = {Starting with the dichotomous view of Kraepelin, schizophrenia and bipolar disorder have traditionally been considered as separate entities. More recent, this taxonomic view of illnesses has been challenged and a continuum psychosis has been postulated based on genetic and neurobiological findings suggestive of a large overlap between disorders. In this paper we will review clinical and experimental data from genetics, morphology, phenomenology and illness progression demonstrating what makes schizophrenia and bipolar disorder different conditions, challenging the idea of the obsolescence of the categorical approach. However, perhaps it is also time to move beyond DSM and search for more refined clinical descriptions that could uncover clinical invariants matching better with molecular data. In the future, computational psychiatry employing artificial intelligence and machine learning might provide us a tool to overcome the gap between clinical descriptions (phenomenology) and neurobiology.}
}
@article{LI2021105093,
title = {An overview of scientometric mapping for the safety science community: Methods, tools, and framework},
journal = {Safety Science},
volume = {134},
pages = {105093},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.105093},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520304902},
author = {Jie Li and Floris Goerlandt and Genserik Reniers},
keywords = {Scientometrics, Bibliometrics, Safety science, Mapping knowledge domains, Science mapping},
abstract = {Scientometrics analysis is increasingly applied across scientific domains to gain quantitative insights in the development of research on particular (sub-)domains of scientific inquiry. By visualizing metrics containing quantitative information about such a domain, scientometric mapping allows researchers to gain insights in aspects thereof. Methods have been developed to answer specific research questions, focusing e.g. on collaboration networks, thematic research clusters, historic evolution patterns, and trends in topics addressed. Several articles applying scientometric mapping to safety-related topics have been published. In context of the Special Issue ‘Mapping Safety Science – Reviewing Safety Research’, this article first reviews these, and subsequently provides an overview of key concepts, methods, and tools for scientometric mapping. Data sources and freely available tools are introduced, focusing on which research questions these are suited to answer. A brief tutorial-style description of a scientometrics research process is provided, guiding researchers new to this method how to engage with it. Finally, a discussion on best practices in scientometric mapping research is made, focusing on how to obtain reliable and valid results, and how to use the scientometric maps to gain meaningful insights. It is hoped that this work can advance the application of scientometric research within the safety science community.}
}
@article{DUPLESSIS2021116395,
title = {Short-term solar power forecasting: Investigating the ability of deep learning models to capture low-level utility-scale Photovoltaic system behaviour},
journal = {Applied Energy},
volume = {285},
pages = {116395},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116395},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920317657},
author = {A.A. {du Plessis} and J.M. Strauss and A.J. Rix},
keywords = {Short-term power forecasting, Machine learning, Deep learning, Photovoltaic, Long Short-Term Memory, Gated Recurrent Unit},
abstract = {Photovoltaic (PV) system power supply is characteristically intermittent. Therefore, PV forecasting is crucial for decision makers responsible for electrical grid stability. With forecast models traditionally trained as macro-level solutions, where a single model emulates the entire PV system, there is uncertainty regarding the ability of these macro-level models to capture the low-level power output dynamics of large multi-megawatt PV systems. Instead, an aggregated inverter-level forecasting methodology is proposed to obtain an enhanced forecasting accuracy. These macro-level and inverter-level forecasting methodologies are implemented with state-of-the-art deep learning based Feedforward neural network, Long Short-Term Memory and Gated Recurrent Unit recurrent neural network models. Results are generated for a real-world scenario, with multi-step forecasts delivered 1–6 h ahead for a 75 MW rated PV system. To ensure the scalability of the proposed methodology, a unique inverter-clustering technique is presented, which reduces the effort of optimising multiple low-level forecast models. A heuristic process of systematic hyperparameter optimisation is also proposed, which serves to guide future forecasting practitioners towards unbiased model development. From the deterministic and probabilistic confidence interval evaluations, overall results demonstrate a marginal increase in forecasting accuracy from the proposed aggregated inverter-level forecasts. The best performing macro-level model obtained Mean Absolute Percentage Error (MAPE) values ranging between 1.42%–8.13% for all weather types and forecast horisons. In comparison, the equivalent inverter-level forecasts delivered MAPE values ranging from 1.27%–8.29%. Finally, it is concluded that deep learning based macro-level forecast models have a sufficient ability to capture low-level PV system behaviour.}
}
@article{SAKOR2023100760,
title = {Knowledge4COVID-19: A semantic-based approach for constructing a COVID-19 related knowledge graph from various sources and analyzing treatments’ toxicities},
journal = {Journal of Web Semantics},
volume = {75},
pages = {100760},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100760},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000440},
author = {Ahmad Sakor and Samaneh Jozashoori and Emetis Niazmand and Ariam Rivas and Konstantinos Bougiatiotis and Fotis Aisopos and Enrique Iglesias and Philipp D. Rohde and Trupti Padiya and Anastasia Krithara and Georgios Paliouras and Maria-Esther Vidal},
keywords = {Knowledge graphs, COVID-19, Drug–drug interactions},
abstract = {In this paper, we present Knowledge4COVID-19, a framework that aims to showcase the power of integrating disparate sources of knowledge to discover adverse drug effects caused by drug–drug interactions among COVID-19 treatments and pre-existing condition drugs. Initially, we focus on constructing the Knowledge4COVID-19 knowledge graph (KG) from the declarative definition of mapping rules using the RDF Mapping Language. Since valuable information about drug treatments, drug–drug interactions, and side effects is present in textual descriptions in scientific databases (e.g., DrugBank) or in scientific literature (e.g., the CORD-19, the Covid-19 Open Research Dataset), the Knowledge4COVID-19 framework implements Natural Language Processing. The Knowledge4COVID-19 framework extracts relevant entities and predicates that enable the fine-grained description of COVID-19 treatments and the potential adverse events that may occur when these treatments are combined with treatments of common comorbidities, e.g., hypertension, diabetes, or asthma. Moreover, on top of the KG, several techniques for the discovery and prediction of interactions and potential adverse effects of drugs have been developed with the aim of suggesting more accurate treatments for treating the virus. We provide services to traverse the KG and visualize the effects that a group of drugs may have on a treatment outcome. Knowledge4COVID-19 was part of the Pan-European hackathon#EUvsVirus in April 2020 and is publicly available as a resource through a GitHub repository and a DOI.}
}
@incollection{AWASTHI2021455,
title = {20 - Metagenomics: A computational approach in emergence of novel applications},
editor = {Maulin P. Shah and Susana Rodriguez-Couto},
booktitle = {Wastewater Treatment Reactors},
publisher = {Elsevier},
pages = {455-482},
year = {2021},
isbn = {978-0-12-823991-9},
doi = {https://doi.org/10.1016/B978-0-12-823991-9.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239919000046},
author = {Shruti Awasthi and Shubha Dwivedi and Naveen Dwivedi},
keywords = {Metagenomics, sequencing, computational approach, environment, ecosystem, statistical approaches},
abstract = {Metagenomics is a one of the fastest developing fields of research nowadays. It is a molecular tool used to study the genetic material obtained from environmental samples without culturing. Through culture-independent genomic sequencing, the metagenomics approach offers an exceptional analysis of the various microbial worlds in different environments, such as soil, air, ocean, and other water bodies, human and animal body sites, and many others. In the study of sequencing, metagenomics allows researcher to determine directly the collection of genes which are present in an environmental sample that are totally unknown organisms. It has been difficult to identify them, but now they can be analyzed by conducting biochemical tests and focusing on how they interact in the environment. Metagenomics data are growing at a fast pace so we need new infrastructure, methods, and technology so that this huge data can be analyzed and predicted; overcoming this problem with a computational approach can be a boon. A large volume of data could be managed by using a computational environment. This method collects, processes, and extracts useful biological information from samples and complex datasets, but genomics sequences need the integration of these computational methods. This chapter aims to focus on the detailed study of novel computational approach in metagenomics data analysis and the wide applications of sequencing methods.}
}
@article{RENGARAJAN2021120560,
title = {Strategy tools in dynamic environments – An expert-panel study},
journal = {Technological Forecasting and Social Change},
volume = {165},
pages = {120560},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120560},
url = {https://www.sciencedirect.com/science/article/pii/S004016252031386X},
author = {Srinath Rengarajan and Roger Moser and Gopalakrishnan Narayanamurthy},
keywords = {Strategy tools, Strategy frameworks, Dynamic environments, Information processing, Contextual fit},
abstract = {Strategy tools and frameworks are crucial for managers to navigate their business environment and formulate strategies. Extant research has focused on the characteristics, dimensions, applications, and impact of traditional tools. However, there are questions regarding the suitability of these tools to the increasingly dynamic environments faced by strategy practitioners characterized by blurring industry boundaries, uncertainty, and ambiguity. Using an expert-panel approach, we address this research gap by investigating how strategy experts from practice and academia assess established strategy tools in dynamic environment. We identify the characteristics of strategy tools that experts value in such contexts and which can inform future development of context-specific strategy tools. Additionally, we also investigate why experts select and apply specific tools and how they combine these tools. Our findings further allow us to explore the difference in perspectives of strategy scholars and practitioners, which is necessary to reconcile the gap between strategy theory and practice. Finally, we discuss implications of the study for strategy and management research, education, and practice.}
}