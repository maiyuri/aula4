@article{10.14778/3352063.3352116,
author = {Nargesian, Fatemeh and Zhu, Erkang and Miller, Ren\'{e}e J. and Pu, Ken Q. and Arocena, Patricia C.},
title = {Data Lake Management: Challenges and Opportunities},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352116},
doi = {10.14778/3352063.3352116},
abstract = {The ubiquity of data lakes has created fascinating new challenges for data management research. In this tutorial, we review the state-of-the-art in data management for data lakes. We consider how data lakes are introducing new problems including dataset discovery and how they are changing the requirements for classic problems including data extraction, data cleaning, data integration, data versioning, and metadata management.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1986–1989},
numpages = {4}
}

@article{10.1145/3450751,
author = {Zhou, Ke and Song, Jingkuan},
title = {Introduction to the Special Issue on Learning-Based Support for Data Science Applications},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2691-1922},
url = {https://doi.org/10.1145/3450751},
doi = {10.1145/3450751},
journal = {ACM/IMS Trans. Data Sci.},
month = {apr},
articleno = {9},
numpages = {1}
}

@inproceedings{10.1145/3368756.3369005,
author = {Bentalha, Badr and Hmioui, Aziz and Alla, Lhoussaine},
title = {The Digitalization of the Supply Chain Management of Service Companies: A Prospective Approach},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369005},
doi = {10.1145/3368756.3369005},
abstract = {Supply Chain Management (SCM) was born and developed first in an industrial context. In the field of services, little research has addressed the issue of the company's SCM. According to [1] "service logistics is an approach that stabilizes and guarantees the continuity of flows: it is then oriented more towards the service provided than towards reducing traffic costs". The SCM of services is of increasing interest to companies facing strong competition, market globalization and rapid changes in information and communication technologies. This evolution has led to a rapid integration of new digital practices in this field.So, how is the digitalization of the SCM of service companies looking today and what will be the future trends? On the one hand, with the help of the literature review, we seek to identify the concept of the SCM in services and its specificities, then that of digitization of the SCM and its organizational dimension. On the other hand, we are attempting a prospective approach to the current practices and digitalization prospects of the service company's SCM.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {29},
numpages = {8},
keywords = {supply chain, prospective approach, digital, service company, SCM},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@inproceedings{10.1145/3463531.3463536,
author = {Wan, Xinxin},
title = {A Study on the Current Development of Artificial Intelligence in Education Industry in China},
year = {2021},
isbn = {9781450389662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463531.3463536},
doi = {10.1145/3463531.3463536},
abstract = {This article first explained the definition of AI in education (AIEd) and reported findings regarding the current development of AIEd industry in the Chinese context. The research design is a context-specific case study using the supply and demand theoretical framework. From a demand-side perspective, the author made an in-depth analysis of the specific AI applications employed in different educational scenarios, including the automated speaking assessment system, the content-based image retrieval system, adaptive learning system, AI-supported classrooms, and AI-assisted campus safety system. For the supply analysis of the AIEd industry, this article summarized key AIEd industry chains and technologies currently widely used in China, obtaining the industry market scale through data collected from different sources. In addition, the iFLYTEK company, as a typical enterprise in the AIEd industry, was taken as a medium to conduct a case analysis. The employment of various AI applications in smart classrooms, smart exams, and smart terminals were comprehensively discussed. In a nutshell, this article discussed the development status and future trends of Chinese AIEd industry, with an aim to offer suggestions and implications for education practitioners.},
booktitle = {2021 7th International Conference on Education and Training Technologies},
pages = {28–35},
numpages = {8},
keywords = {AI Education, Oral Assessment, Adaptive Learning, Smart Classroom, Education Informatization},
location = {Macau, China},
series = {ICETT 2021}
}

@article{10.1145/2334184.2334188,
author = {Churchill, Elizabeth F.},
title = {From Data Divination to Data-Aware Design},
year = {2012},
issue_date = {September + October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {1072-5520},
url = {https://doi.org/10.1145/2334184.2334188},
doi = {10.1145/2334184.2334188},
journal = {Interactions},
month = {sep},
pages = {10–13},
numpages = {4}
}

@inproceedings{10.1145/3447548.3470799,
author = {Lee, Jae-Gil and Roh, Yuji and Song, Hwanjun and Whang, Steven Euijong},
title = {Machine Learning Robustness, Fairness, and Their Convergence},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470799},
doi = {10.1145/3447548.3470799},
abstract = {Responsible AI becomes critical where robustness and fairness must be satisfied together. Traditionally, the two topics have been studied by different communities for different applications. Robust training is designed for noisy or poisoned data where image data is typically considered. In comparison, fair training primarily deals with biased data where structured data is typically considered. Nevertheless, robust training and fair training are fundamentally similar in considering that both of them aim at fixing the inherent flaws of real-world data. In this tutorial, we first cover state-of-the-art robust training techniques where most of the research is on combating various label noises. In particular, we cover label noise modeling, robust training approaches, and real-world noisy data sets. Then, proceeding to the related fairness literature, we discuss pre-processing, in-processing, and post-processing unfairness mitigation techniques, depending on whether the mitigation occurs before, during, or after the model training. Finally, we cover the recent trend emerged to combine robust and fair training in two flavors: the former is to make the fair training more robust (i.e., robust fair training), and the latter is to consider robustness and fairness as two equals to incorporate them into a holistic framework. This tutorial is indeed timely and novel because the convergence of the two topics is increasingly common, but yet to be addressed in tutorials. The tutors have extensive experience publishing papers in top-tier machine learning and data mining venues and developing machine learning platforms.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4046–4047},
numpages = {2},
keywords = {machine learning, robustness, convergence, fairness},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.5555/2693848.2694146,
author = {Wu, Xinghao and Qiao, Fei and Poon, Kwok},
title = {Cloud Manufacturing Application in Semiconductor Industry},
year = {2014},
publisher = {IEEE Press},
abstract = {This paper aims to shed some light on how the concept of cloud manufacturing has been applied to the semiconductor manufacturing operations. It starts with describing the challenges to the semiconductor manufacturing due to evolving of outsourcing business model in global context, then discusses the different forms of cloud manufacturing and proposes the semiconductor industry oriented architecture for cloud manufacturing. Serus is used as a case study to share how the cloud manufacturing has created the values for the customer and its outsourced suppliers in the semiconductor industry.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {2376–2383},
numpages = {8},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/3424311.3424326,
author = {Wang, Lei and Wang, Yang},
title = {Application of Machine Learning for Process Control in Semiconductor Manufacturing},
year = {2020},
isbn = {9781450377348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424311.3424326},
doi = {10.1145/3424311.3424326},
abstract = {In this article, the authors attempt to describe the core quality inspection during semiconductor manufacturing in terms of production efficiency and yield. Special focus is therefore given to photolithography, which is the most critical step for the fabrication of wafer patterns in front-end processes. Further, machine learning approaches are demonstrated and their applicability in semiconductor manufacturing industry is discussed. Also, a technical concept regarding virtual metrology for advanced process control in semiconductor production is introduced as a potential utilization case. Finally, current status and future trends in technology as well as application are summarized based on authors' perspective in the concluding section.},
booktitle = {Proceedings of the 2020 International Conference on Internet Computing for Science and Engineering},
pages = {109–111},
numpages = {3},
keywords = {Semiconductor manufacturing, Machine learning, Virtual metrology, Data analytics, Advanced process control},
location = {Male, Maldives},
series = {ICICSE '20}
}

@inproceedings{10.1145/3447548.3470814,
author = {Zhou, Zirui and Chu, Lingyang and Liu, Changxin and Wang, Lanjun and Pei, Jian and Zhang, Yong},
title = {Towards Fair Federated Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470814},
doi = {10.1145/3447548.3470814},
abstract = {Federated learning has become increasingly popular as it facilitates collaborative training of machine learning models among multiple clients while preserving their data privacy. In practice, one major challenge for federated learning is to achieve fairness in collaboration among the participating clients, because different clients' contributions to a model are usually far from equal due to various reasons. Besides, as machine learning models are deployed in more and more important applications, how to achieve model fairness, that is, to ensure that a trained model has no discrimination against sensitive attributes, has become another critical desiderata for federated learning. In this tutorial, we discuss formulations and methods such that collaborative fairness, model fairness, and privacy can be fully respected in federated learning. We review the existing efforts and the latest progress, and discuss a series of potential directions.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4100–4101},
numpages = {2},
keywords = {distributed learning, model fairness, data privacy, collaborative fairness, data leakage, federated learning},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/2729104.2729134,
author = {Kokkinakos, Panagiotis and Koutras, Costas and Markaki, Ourania and Koussouris, Sotirios and Trutnev, Dmitrii and Glikman, Yuri},
title = {Assessing Governmental Policies' Impact through Prosperity Indicators and Open Data},
year = {2014},
isbn = {9781450334013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2729104.2729134},
doi = {10.1145/2729104.2729134},
abstract = {The aim of this paper is to provide an overview of (the theory and practice of) prosperity indicators for assessing the impact of governmental policies and the data sources associated to their calculation, touching also on the broad theme of Open Data which opens up new horizons for the calculation and exploitation of Social Indicators. Following a quick overview of the basics of prosperity indicators, their basic methodological principles and their typology, a presentation of the Policy Compass project approach and the description of its pilot application in St. Petersburg are provided, which are tackling the above mentioned issue with the provision of a powerful ICT platform.},
booktitle = {Proceedings of the 2014 Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {70–74},
numpages = {5},
keywords = {Prosperity Indicators, Policy Making, Fuzzy Cognitive Maps, Policy Impact Evaluation, Open Data},
location = {St. Petersburg, Russian Federation},
series = {EGOSE '14}
}

@inproceedings{10.1145/2884781.2884783,
author = {Kim, Miryung and Zimmermann, Thomas and DeLine, Robert and Begel, Andrew},
title = {The Emerging Role of Data Scientists on Software Development Teams},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884783},
doi = {10.1145/2884781.2884783},
abstract = {Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {96–107},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3479162.3479167,
author = {Suaprae, Phanintorn and Nilsook, Prachyanun and Wannapiroon, Panita},
title = {System Framework of Intelligent Consulting Systems with Intellectual Technology},
year = {2021},
isbn = {9781450390071},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479162.3479167},
doi = {10.1145/3479162.3479167},
abstract = {The purposes of this research were: 1) Analyze factors affecting the student retention of higher education students, 2) Develop intelligent consulting system models with intellectual technology for the student retention of higher education students, 3) Design intelligent consulting system architecture with intellectual technology for the student retention of higher education students, 4) Develop intelligent consulting systems with intellectual technology for the student retention of higher education students, and 5) Study the results of intelligent consultation systems with intellectual technology for the student retention of higher education students. An intelligent counseling system with intellectual technology for the student retention of higher education students is a system that can reduce students' mid-exit rates and increase student retention rates. The research has synthesized analysis of factors that affect Student retention applied to Cognitive technology, machine learning can provide accurate student retention forecasts. Counselors can know before students drop out.},
booktitle = {Proceedings of the 9th International Conference on Computer and Communications Management},
pages = {31–36},
numpages = {6},
location = {Singapore, Singapore},
series = {ICCCM '21}
}

@article{10.1145/3356773.3356810,
author = {Xie, Xiaoyuan and Poon, Pak-Lok and Pullum, Laura L.},
title = {Workshop Summary: 2019 IEEE / ACM Fourth International Workshop on Metamorphic Testing (MET 2019)},
year = {2020},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3356773.3356810},
doi = {10.1145/3356773.3356810},
abstract = {MET is a relatively new workshop on metamorphic testing for academic researchers and industry practitioners. The first international workshop on MET (MET 2016) was co-located with the 38th International Conference on Software Engineering (ICSE 2016) in Austin TX, USA on May 16, 2016. Since then the workshop has become an annual event. This paper reports on the fourth International Workshop on Metamorphic Testing (MET 2019) held in Montr\'{e}al, Canada on May 26, 2019, as part of the 41st International Conference on Software Engineering (ICSE 2019). We first outline the aims of the workshop, followed by a discussion of its keynote speech and technical program.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {56–59},
numpages = {4},
keywords = {software verification and validation, software testing, software engineering, metamorphic testing}
}

@article{10.1145/2883611,
author = {Cao, Tien-Dung and Pham, Tran-Vu and Vu, Quang-Hieu and Truong, Hong-Linh and Le, Duc-Hung and Dustdar, Schahram},
title = {MARSA: A Marketplace for Realtime Human Sensing Data},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/2883611},
doi = {10.1145/2883611},
abstract = {This article introduces a dynamic cloud-based marketplace of near-realtime human sensing data (MARSA) for different stakeholders to sell and buy near-realtime data. MARSA is designed for environments where information technology (IT) infrastructures are not well developed but the need to gather and sell near-realtime data is great. To this end, we present techniques for selecting data types and managing data contracts based on different cost models, quality of data, and data rights. We design our MARSA platform by leveraging different data transferring solutions to enable an open and scalable communication mechanism between sellers (data providers) and buyers (data consumers). To evaluate MARSA, we carry out several experiments with the near-realtime transportation data provided by people in Ho Chi Minh City, Vietnam, and simulated scenarios in multicloud environments.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {16},
numpages = {21},
keywords = {platform, Internet of Things, cost model, data contract}
}

@inproceedings{10.1145/3394486.3406473,
author = {Pei, Jian},
title = {Data Pricing -- From Economics to Data Science},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406473},
doi = {10.1145/3394486.3406473},
abstract = {Data are invaluable. How can we assess the value of data objectively and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, data management, data mining, electronic commerce, and marketing. In this tutorial, we present a unified and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing, review the development and evolution of pricing models, and compare the proposals of marketplaces of data. We cover both digital products, such as ebooks and MP3 music, and data products, such as data sets, data queries and machine learning models. We also connect data pricing with the highly related areas, such as cloud service pricing, privacy pricing, and decentralized privacy preserving infrastructure like blockchains.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3553–3554},
numpages = {2},
keywords = {subscription, digital products, revenue maximization, data products, privacy, data pricing, fairness, bundling, information goods, auctions, arbitrage, trustfulness},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3159652.3160594,
author = {Lin, Yu-Ru and Castillo, Carlos and Yin, Jie},
title = {The 5th International Workshop on Social Web for Disaster Management(SWDM'18): Collective Sensing, Trust, and Resilience in Global Crises},
year = {2018},
isbn = {9781450355810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159652.3160594},
doi = {10.1145/3159652.3160594},
abstract = {During large-scale emergencies such as natural and man-made disasters, a massive amount of information is posted by the public in social media. Collecting, aggregating, and presenting this information to stakeholders can be extremely challenging, particularly if an understanding of the "big picture»» is sought. This international workshop, the fifth in the series, is a key venue for researchers and practitioners to discuss research challenges and technical issues around the usage of social media in disaster management. Workshop»s website: https://sites.google.com/site/swdm2018/},
booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
pages = {791–792},
numpages = {2},
keywords = {social media, emergency management, disaster response},
location = {Marina Del Rey, CA, USA},
series = {WSDM '18}
}

@inproceedings{10.1145/3047273.3047386,
author = {Matheus, Ricardo and Janssen, Marijn},
title = {How to Become a Smart City? Balancing Ambidexterity in Smart Cities},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047386},
doi = {10.1145/3047273.3047386},
abstract = {Most cities have limited resources to become a smart city. Yet some cities have been more successful than others in becoming a smart city. This raises the questions why were some cities able to become smart, whereas other were not able to do so? This research is aimed at identifying factors influencing the shift towards becoming a smart city. In this way insight is gained into factors that governments can influence to become a smart city. First, Literature was reviewed to identify dimensions and factors enabling or impeding the process of becoming a smart city. These factors were used to compare two similar type of case studies. The cases took different paths to become a smart city and had different levels of success. This enabled us to identify factors influencing the move towards smart cities. The results reveal that existing infrastructures should be used and extended in such a way that they can facilitate a variety of different applications. Synergy from legacy systems can avoid extra expenditures. Having such an infrastructure in place facilitates the development of new organizational models. These models are developed outside the existing organization structure to avoid hinder from existing practices and organizational structures. This finding suggests that smart cities focussed on structural ambidexterity innovate quicker.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {405–413},
numpages = {9},
keywords = {innovation, smart cities, exploitation, ambidexterity, transformation, exploration, e-government},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3556384.3556409,
author = {Feng, Langrong and Xiao, Zhongming},
title = {Marine Traffic State Recognition: Review and Prospect},
year = {2022},
isbn = {9781450396912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3556384.3556409},
doi = {10.1145/3556384.3556409},
abstract = {With the increasing complexity of marine traffic environment, the traditional management method of manually and subjectively identifying traffic status can not meet the needs of current marine traffic management. The Real-time traffic flow data were used to automatically identify the channel traffic state and understand the maritime traffic mode is the development direction of maritime traffic management mode in the future. This paper objectively evaluates the existing research on maritime traffic state recognition at home and abroad, compares it with the research status of road traffic from three aspects: channel congestion, real-time traffic state recognition and traffic complexity. It is pointed out that traffic state recognition has formed a research system to meet different traffic requirements in road traffic, in the follow-up study of maritime traffic state recognition should further combine the road research results with the characteristics of maritime traffic, and further discuss the three key problems of maritime traffic state recognition: data processing, evaluation index selection and discrimination method selection. Finally, the prospect of marine traffic state recognition is expected.},
booktitle = {Proceedings of the 2022 5th International Conference on Signal Processing and Machine Learning},
pages = {159–165},
numpages = {7},
location = {Dalian, China},
series = {SPML '22}
}

@inproceedings{10.1145/3041021.3055510,
author = {Lehmann, Jens and Auer, S\"{o}ren and Capadisli, Sarven and Janowicz, Krzysztof and Bizer, Christian and Heath, Tom and Hogan, Aidan and Berners-Lee, Tim},
title = {LDOW2017: 10th Workshop on Linked Data on the Web},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3055510},
doi = {10.1145/3041021.3055510},
abstract = {The 10th Linked Data on the Web workshop (LDOW2017) was held in Perth, Western Australia on April 3, 2017, co-located with the 26th International World Wide Web Conference (WWW2017). In its 10th anniversary edition, the LDOW workshop aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data on the Web as well as mining knowledge from said data.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1679–1680},
numpages = {2},
keywords = {semantic web, linked data},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/2912160.2912180,
author = {Netten, Niels and Bargh, Mortaza S. and van den Braak, Susan and Choenni, Sunil and Leeuw, Frans},
title = {On Enabling Smart Government: A Legal Logistics Framework for Future Criminal Justice Systems},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912180},
doi = {10.1145/2912160.2912180},
abstract = {While in business and private settings the disruptive impact of advanced information communication technology (ICT) have already been felt, the legal sector is now starting to face great disruptions due to such ICTs. Bits and pieces of innovations in the legal sector have been emerging for some time, affecting the performance of core functions and the legitimacy of public institutions.In this paper, we present our framework for enabling the smart government vision, particularly for the case of criminal justice systems, by unifying different isolated ICT-based solutions. Our framework, coined as Legal Logistics, supports the well-functioning of a legal system in order to streamline the innovations in these legal systems. The framework targets the exploitation of all relevant data generated by the ICT-based solutions. As will be illustrated for the Dutch criminal justice system, the framework may be used to integrate different ICT-based innovations and to gain insights about the well-functioning of the system. Furthermore, Legal Logistics can be regarded as a roadmap towards a smart and open justice.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {293–302},
numpages = {10},
keywords = {effectivity, legal design, smart governance, and penal law, Law enforcement, efficiency, open justice},
location = {Shanghai, China},
series = {dg.o '16}
}

@article{10.1145/2380776.2380789,
author = {Maz\'{o}n, Jose-Norberto and Garrig\'{o}s, Irene and Daniel, Florian and Castellanos, Malu},
title = {Report of the International Workshop on Business Intelligence and the Web: BEWEB 2011},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2380776.2380789},
doi = {10.1145/2380776.2380789},
abstract = {The 2nd International Workshop on Business intelligencE and the WEB (BEWEB) was co-located with the EDBT/ICDT 2011 Joint Conference in Uppsala (Sweden) on March 25, 2011. BEWEB intends to be an international forum for researchers and practitioners to exchange ideas on how to leverage the huge amount of data that is available on the Web in BI applications and on how to apply Web engineering methods and techniques to the design of BI applications. This report summarizes the 2011 edition of BEWEB.},
journal = {SIGMOD Rec.},
month = {oct},
pages = {51–53},
numpages = {3}
}

@inproceedings{10.1145/3450508.3464558,
author = {Bednarz, Tomasz and Hughes, Rowan T. and Mathews, Alex and Chen, Dawei and Zhu, Liming and Filonik, Daniel},
title = {Visual Analytics for Large Networks: Theory, Art and Practice},
year = {2021},
isbn = {9781450383615},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450508.3464558},
doi = {10.1145/3450508.3464558},
booktitle = {ACM SIGGRAPH 2021 Courses},
articleno = {15},
numpages = {213},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@inproceedings{10.1145/3447548.3470825,
author = {Zalmout, Nasser and Zhang, Chenwei and Li, Xian and Liang, Yan and Dong, Xin Luna},
title = {All You Need to Know to Build a Product Knowledge Graph},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3470825},
doi = {10.1145/3447548.3470825},
abstract = {Knowledge graphs have been pivotal in supporting downstream applications like search, recommendation, and question answering, among others. Therefore, knowledge graphs have naturally become key enabling technologies in e-Commerce platforms. Developing a high coverage product knowledge graph is more challenging than generic knowledge graphs. The highly specific and complex domain, the sparsity of training data, along with the dynamic taxonomies and product types, can constrain the resulting knowledge graphs. In this tutorial we present best practices and ML innovations in industry towards building a scalable product knowledge graph. Contributions in this domain benefit from the general literature in areas including information extraction and data mining, tailored to address the specific characteristics of e-Commerce platforms.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4090–4091},
numpages = {2},
keywords = {taxonomy, data cleaning, information extraction, knowledge graphs},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3085228.3085255,
author = {Parycek, P. and Pereira, G. Viale},
title = {Drivers of Smart Governance: Towards to Evidence-Based Policy-Making},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085255},
doi = {10.1145/3085228.3085255},
abstract = {This paper presents the preliminary framework proposed by the authors for drivers of Smart Governance. The research question of this study is: What are the drivers for Smart Governance to achieve evidence-based policy-making? The framework suggests that in order to create a smart governance model, data governance and collaborative governance are the main drivers. These pillars are supported by legal framework, normative factors, principles and values, methods, data assets or human resources, and IT infrastructure. These aspects will guide a real time evaluation process in all levels of the policy cycle, towards to the implementation of evidence-based policies.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {564–565},
numpages = {2},
keywords = {Decision-making, Data Governance, Evaluation, Collaborative Governance},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@article{10.1145/3403976,
author = {Hoffmann, Leah},
title = {Seeing Light at the End of the Cybersecurity Tunnel},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3403976},
doi = {10.1145/3403976},
abstract = {After decades of cybersecurity research, Elisa Bertino remains optimistic.},
journal = {Commun. ACM},
month = {jul},
pages = {104–ff},
numpages = {2}
}

@inproceedings{10.1145/3469213.3470424,
author = {Yan, Feng},
title = {A Building Integrated Control Platform Oriented Towards Intelligent Building},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470424},
doi = {10.1145/3469213.3470424},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {217},
numpages = {10},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@article{10.5555/3344081.3344085,
author = {Li, Xinming and Talburt, John R. and Li, Ting and Liu, Xiangwen},
title = {Scoring Matrix Combined with Machine Learning for Heterogeneously Structured Entity Resolution},
year = {2019},
issue_date = {April 2019},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {34},
number = {7},
issn = {1937-4771},
abstract = {This paper describes how machine learning works with "coring matrix", which is designed for measuring the similarity between heterogeneously structured references, to get a better performance in Entity Resolution (ER). In the scoring matrix, each entity reference is tokenized and all pairs of tokens between the references are scored by a similarity scoring function such as the Levenshtein edit distance. In so doing, a similarity score vector can measure the similarity between references. With the similarity score vector, machine learning is used to make the linking decision. Our experiments show that machine learning based on score vector outperforms TF-IDF and FuzzyWuzzy benchmarks. One possible explanation is that a similarity score vector conveys much more information than a single similarity score. Random forest and neural network even get better performance with raw score vector input than with the statistic characteristic input.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {38–45},
numpages = {8}
}

@inproceedings{10.1145/3503928.3503944,
author = {Wu, Ji and Zhou, Ming and Xu, Min and Zhang, Jin and Wu, Yue and Zha, Weiwei and Zhang, Chengping},
title = {Design and Research of IoT Management Architecture for Power Grid Enterprises Based on Digital Transformation: Application of IoT in Power Grid Enterprises According to Enterprise Architecture Method},
year = {2022},
isbn = {9781450385220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503928.3503944},
doi = {10.1145/3503928.3503944},
abstract = {Internet of things technology, as the core technology in digital transformation, helps enterprises in digital transformation to carry out comprehensive perception, intelligent management and secure transmission. Following the information architecture of State Grid Corporation of China and combined with the business objectives and Strategies of electric power company, carry out the differentiated design of power Internet of things architecture, put forward the improvement direction of power Internet of things architecture, clarify the application scenario and future evolution route of power Internet of things business, and ensure the realization of technology.},
booktitle = {2021 the 6th International Conference on Information Systems Engineering},
pages = {84–88},
numpages = {5},
keywords = {Digital transformation, Intelligent IoT management system, IoT},
location = {Shanghai, China},
series = {ICISE 2021}
}

@inbook{10.1145/3310205.3310207,
title = {Introduction},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310207},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3029387.3029421,
author = {Anjum, Shahid W.},
title = {Risk Magnification Framework for Clouds Computing Architects in Business Intelligence},
year = {2017},
isbn = {9781450348034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3029387.3029421},
doi = {10.1145/3029387.3029421},
abstract = {IT infrastructure and applications in enterprise systems started with traditional client-server architecture and have gone through key paradigm shifts in infrastructure, software, enterprise, and service architectures to current age of cloud computing and internet of everything. Using strengths-weaknesses-opportunities-threats and analytical hierarchy process of multi-criteria decision making frameworks together, various aspects of cloud computing characteristics related to opportunities, benefits, costs, value and risks can be understood in a more detailed way and can be ranked. This article has combined these two frameworks for the ranking of various business intelligence architects for cloud computing by using 'business automation with sustainable hedging for information risks' framework for cloud computing from conservative perspective where risk relevancy attracts the prime focus. The results have shown that moving operational business intelligence is the best business intelligence architecture for cloud computing as its strengths are more than inherent risks as has become evident by using this approach.},
booktitle = {Proceedings of the 5th International Conference on Information and Education Technology},
pages = {140–144},
numpages = {5},
keywords = {A'WOT Analysis, Business Intelligence Architecture, Cloud Computing, IT Risk Management, Multi Criteria Decision Making},
location = {Tokyo, Japan},
series = {ICIET '17}
}

@article{10.1145/2536669.2536682,
author = {Zhou, Xiaofang and Sadiq, Shazia},
title = {Data Centric Research at the University of Queensland},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2536669.2536682},
doi = {10.1145/2536669.2536682},
journal = {SIGMOD Rec.},
month = {oct},
pages = {63–68},
numpages = {6}
}

@inproceedings{10.1145/3366625.3369437,
author = {Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad},
title = {An In-Depth Look of BFT Consensus in Blockchain: Challenges and Opportunities},
year = {2019},
isbn = {9781450370400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366625.3369437},
doi = {10.1145/3366625.3369437},
abstract = {Since the introduction of Bitcoin---the first wide-spread application driven by blockchains---the interest of the public and private sector in blockchains has skyrocketed. At the core of this interest are the ways in which blockchains can be used to improve data management, e.g., by enabling federated data management via decentralization, resilience against failure and malicious actors via replication and consensus, and strong data provenance via a secured immutable ledger.In practice, high-performance blockchains for data management are usually built in permissioned environments in which the participants are vetted and can be identified. In this setting, blockchains are typically powered by Byzantine fault-tolerant consensus protocols. These consensus protocols are used to provide full replication among all honest blockchain participants by enforcing an unique order of processing incoming requests among the participants.In this tutorial, we take an in-depth look at Byzantine fault-tolerant consensus. First, we take a look at the theory behind replicated computing and consensus. Then, we delve into how common consensus protocols operate. Finally, we take a look at current developments and briefly look at our vision moving forward.},
booktitle = {Proceedings of the 20th International Middleware Conference Tutorials},
pages = {6–10},
numpages = {5},
location = {Davis, CA, USA},
series = {Middleware '19}
}

@inproceedings{10.1145/3428690.3429172,
author = {Khadivizand, Sam and Beheshti, Amin and Sobhanmanesh, Fariborz and Sheng, Quan Z. and Istanbouli, Elias and Wood, Steven and Pezaro, Damon},
title = {Towards Intelligent Feature Engineering for Risk-Based Customer Segmentation in Banking},
year = {2021},
isbn = {9781450389242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428690.3429172},
doi = {10.1145/3428690.3429172},
abstract = {Business Processes, i.e., a set of coordinated tasks and activities to achieve a business goal, and their continuous improvements are key to the operation of any organization. In banking, business processes are increasingly dynamic as various technologies have made dynamic processes more prevalent. For example, customer segmentation, i.e., the process of grouping related customers based on common activities and behaviors, could be a data-driven and knowledge-intensive process. In this paper, we present an intelligent data-driven pipeline composed of a set of processing elements to move customers' data from one system to another, transforming the data into the contextualized data and knowledge along the way. The goal is to present a novel intelligent customer segmentation process which automates the feature engineering, i.e., the process of using (banking) domain knowledge to extract features from raw data via data mining techniques, in the banking domain. We adopt a typical scenario for analyzing customer transaction records, to highlight how the presented approach can significantly improve the quality of risk-based customer segmentation in the absence of feature engineering.},
booktitle = {Proceedings of the 18th International Conference on Advances in Mobile Computing &amp; Multimedia},
pages = {74–83},
numpages = {10},
keywords = {risk-based customer segmentation, business process, feature engineering, banking processes},
location = {Chiang Mai, Thailand},
series = {MoMM '20}
}

@inproceedings{10.1145/3503823.3503900,
author = {Stakoulas, Konstantinos and Georgiou, Konstantinos and Mittas, Nikolaos and Angelis, Lefteris},
title = {An Analysis of User Profiles from Covid-19 Questions in Stack Overflow},
year = {2022},
isbn = {9781450395557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503823.3503900},
doi = {10.1145/3503823.3503900},
abstract = {The COVID-19 pandemic brought many changes in society, with one of the most important being an explosion of software development concerning technological solutions for combatting its crippling effects. In this global crisis, many software enthusiasts, combined with seasoned developers and specialists turned their attention to Questions and Answers platforms such as Stack Overflow to expand their knowledge and ask questions regarding their COVID-19 related solutions. This paper examines the different characteristics of these users, dividing them into Newcomers and Oldcomers and pinpoints popularity differences, scientific and technological backgrounds by analyzing key technologies, as well as the role of gender in their participation.},
booktitle = {25th Pan-Hellenic Conference on Informatics},
pages = {419–424},
numpages = {6},
location = {Volos, Greece},
series = {PCI 2021}
}

@inproceedings{10.5555/2873003.2873011,
author = {Barhak, Jacob},
title = {Modeling Clinical Data from Publications},
year = {2015},
isbn = {9781510801028},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Medical data is becoming increasingly available. Access to such data is generally restricted and researchers cannot access it easily. On the other hand, clinical trial data is freely available and published without restriction for access to the public at the summary level. With proper analysis, it is possible to extract valuable conclusions from such data. This paper will review new methods to look at such public data and will discuss possible future trends.},
booktitle = {Proceedings of the Symposium on Modeling and Simulation in Medicine},
pages = {47–52},
numpages = {6},
keywords = {high performance computing, clinical trial, Monte-Carlo, reference modeling, disease modeling, publications},
location = {Alexandria, Virginia},
series = {MSM '15}
}

@article{10.1145/3368091,
author = {Douglas, David M.},
title = {Should Researchers Use Data from Security Breaches?},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3368091},
doi = {10.1145/3368091},
abstract = {Evaluating the arguments for and against using digital data derived from security breaches.},
journal = {Commun. ACM},
month = {nov},
pages = {22–24},
numpages = {3}
}

@inproceedings{10.1145/3543434.3543445,
author = {Valle-Cruz, David and Garc\'{\i}a-Contreras, Rigoberto and Mu\~{n}oz-Ch\'{a}vez, J. Patricia},
title = {Mind the Gap: Towards an Understanding of Government Decision-Making Based on Artificial Intelligence},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543434.3543445},
doi = {10.1145/3543434.3543445},
abstract = {Decision-making has become more critical for organizations in the 21st century. The citizens’ countless needs and the emerging problems (internal and external) faced by governments increase the complexity of government decisions worldwide. The research question guiding this attempt is: How is government decision-making grounded on artificial intelligence (AI)? Based on the PRISMA approach and empirical analysis of some international cases are adopted. The authors analyze different organizational and environmental factors, the objectives, benefits, and risks of AI-supported decision-making. The findings show an increasing interest in the research on government decision-making based on AI. Finally, there is the potential of AI to support decision-making for the benefit of citizens and public value generation, collaboratively between governments, industry, and society. Future work will further analyze AI-based decision-making in government in depth.},
booktitle = {DG.O 2022: The 23rd Annual International Conference on Digital Government Research},
pages = {226–234},
numpages = {9},
keywords = {Public Sector, Risks, Artificial Intelligence, Benefits, Goals, Government Decision-Making, Organizational and environmental},
location = {Virtual Event, Republic of Korea},
series = {dg.o 2022}
}

@inproceedings{10.1145/3401025.3404099,
author = {Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad},
title = {Blockchain Consensus Unraveled: Virtues and Limitations},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3404099},
doi = {10.1145/3401025.3404099},
abstract = {Since the introduction of Bitcoin---the first wide-spread application driven by blockchains---the interest of the public and private sector in blockchains has skyrocketed. At the core of this interest are the ways in which blockchains can be used to improve data management, e.g., by enabling federated data management via decentralization, resilience against failure and malicious actors via replication and consensus, and strong data provenance via a secured immutable ledger.In practice, high-performance blockchains for data management are usually built in permissioned environments in which the participants are vetted and can be identified. In this setting, blockchains are typically powered by Byzantine fault-tolerant consensus protocols. These consensus protocols are used to provide full replication among all honest blockchain participants by enforcing an unique order of processing incoming requests among the participants.In this tutorial, we take an in-depth look at Byzantine fault-tolerant consensus. First, we take a look at the theory behind replicated computing and consensus. Then, we delve into how common consensus protocols operate. Finally, we take a look at current developments and briefly look at our vision moving forward.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {218–221},
numpages = {4},
keywords = {consensus, cluster-sending, permissioned blockchains, geo-scale, sharding, resilient transaction processing, byzantine learning},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.1145/3267305.3267694,
author = {Han, Yang and Li, Victor O.K. and Lam, Jacqueline C.K. and Lu, Zhiyi},
title = {UMeAir: Predicting Momentary Happiness Towards Air Quality via Machine Learning},
year = {2018},
isbn = {9781450359665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267305.3267694},
doi = {10.1145/3267305.3267694},
abstract = {Subjective well-being (SWB) refers to people's subjective evaluation of their own quality of life. Previous studies show that environmental pollution, such as air pollution, has generated significant negative impacts on one's SWB. However, such works are often constrained by the lack of appropriate representation of SWB specifically related to air quality. In this study, we develop UMeAir, which collects one's real-time SWB, specifically, one's momentary happiness at a given air quality, pre-processes input data and detects outliers via Isolation Forests, trains and selects the best model via Support Vector Machine and Random Forests, and predicts the momentary happiness towards any air quality one experienced. Unlike traditional representation of air quality by pollution concentration/Air Pollution Index, UMeAir intends to represent air quality in a more user-comprehensible way, by connecting the air quality experienced at a particular time and location with the corresponding momentary happiness perceived towards the air. The higher the momentary happiness, the better the air quality one experienced. Our work is the first attempt to predict momentary happiness towards air quality in real-time, with the development of the-first-of-its-kind UMeAir Happiness Index (HAPI) towards air quality via machine learning.},
booktitle = {Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers},
pages = {702–705},
numpages = {4},
keywords = {Short-term happiness, Machine learning, Air quality, Subjective well-being prediction, Data interpretability},
location = {Singapore, Singapore},
series = {UbiComp '18}
}

@inproceedings{10.1145/3080546.3080550,
author = {Frey, Remo Manuel and Hardjono, Thomas and Smith, Christian and Erhardt, Keeley and Pentland, Alex 'Sandy'},
title = {Secure Sharing of Geospatial Wildlife Data},
year = {2017},
isbn = {9781450350471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3080546.3080550},
doi = {10.1145/3080546.3080550},
abstract = {Modern tracking technologies enables new ways for data mining in the wild. It allows wildlife monitoring centers to permanently collect geospatial data in a non-intrusive manner in real-time and at low cost. Unfortunately, wildlife data is exposed to crime and there is already a first reported case of 'cyber-poaching'. Based on stolen geospatial data, poachers can easily track and kill animals. As a result, cautious monitoring centers limited data access for research and public use. This means that the data cannot fully exploit its potential. We propose a novel solution to overcome the security problem. It allows monitoring centers to securely answer questions from the research community and to provide aggregated data to the public while the raw data is protected against unauthorized third parties. This data service can also be monetized. Several new applications are conceivable, such as a mobile app for preventing conflicts between human and wildlife or for engaging people in wildlife donation. Besides presenting the solution and potential use cases, the intention of present article is to start a discussion about the need for data protection and privacy in the animal world.},
booktitle = {Proceedings of the Fourth International ACM Workshop on Managing and Mining Enriched Geo-Spatial Data},
articleno = {5},
numpages = {6},
keywords = {wildlife, GPS, species protection, hunting, geospatial, animal, crime, data sharing, security, blockchain, cyber-poaching, privacy},
location = {Chicago, Illinois},
series = {GeoRich '17}
}

@inproceedings{10.1109/ICSE-Companion.2019.00023,
author = {Dang, Yingnong and Lin, Qingwei and Huang, Peng},
title = {AIOps: Real-World Challenges and Research Innovations},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00023},
doi = {10.1109/ICSE-Companion.2019.00023},
abstract = {AIOps is about empowering software and service engineers (e.g., developers, program managers, support engineers, site reliability engineers) to efficiently and effectively build and operate online services and applications at scale with artificial intelligence (AI) and machine learning (ML) techniques. AIOps can help improve service quality and customer satisfaction, boost engineering productivity, and reduce operational cost. In this technical briefing, we first summarize the real-world challenges in building AIOps solutions based on our practice and experience in Microsoft. We then propose a roadmap of AIOps related research directions, and share a few successful AIOps solutions we have built for Microsoft service products.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {4–5},
numpages = {2},
keywords = {AIOps, software analytics, DevOps},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3335150,
author = {Hastings, Justine S. and Howison, Mark and Lawless, Ted and Ucles, John and White, Preston},
title = {Unlocking Data to Improve Public Policy},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3335150},
doi = {10.1145/3335150},
abstract = {When properly secured, anonymized, and optimized for research, administrative data can be put to work to help government programs better serve those in need.},
journal = {Commun. ACM},
month = {sep},
pages = {48–53},
numpages = {6}
}

@article{10.1145/3377391.3377398,
author = {Winslett, Marianne and Braganholo, Vanessa},
title = {Michael Franklin Speaks Out on Data Science},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3377391.3377398},
doi = {10.1145/3377391.3377398},
abstract = {Welcome to ACM SIGMOD Record series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we're at the 2017 SIGMOD and PODS conference in Chicago. I have here with me Mike Franklin, who is the chair of the Computer Science department at the University of Chicago. Before that, for many years, Mike was a professor at Berkeley where he also served as a chair of the Computer Science division. Mike was a co-founder and director of the Algorithms, Machines, and People Lab, better known as the AMPLab. He is an ACM fellow, a two-time winner of the SIGMOD Ten Year Test of Time Award, and a founder of the successful startup, Truviso. Mike's Ph.D. is from the University of Wisconsin Madison. So, Mike, welcome!},
journal = {SIGMOD Rec.},
month = {dec},
pages = {29–35},
numpages = {7}
}

@article{10.1145/2435221.2435222,
author = {Talburt, John R.},
title = {SPECIAL ISSUE ON ENTITY RESOLUTION Overview: The Criticality of Entity Resolution in Data and Information Quality},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2435221.2435222},
doi = {10.1145/2435221.2435222},
journal = {J. Data and Information Quality},
month = {mar},
articleno = {6},
numpages = {2}
}

@article{10.1145/2805789.2805795,
author = {Calyam, Prasad and Swany, Martin},
title = {Research Challenges in Future Multi-Domain Network Performance Measurement and Monitoring},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/2805789.2805795},
doi = {10.1145/2805789.2805795},
abstract = {The perfSONAR-based Multi-domain Network Performance Measurement and Monitoring Workshop was held on February 20-21, 2014 in Arlington, VA. The goal of the workshop was to review the state of the perfSONAR effort and catalyze future directions by cross-fertilizing ideas, and distilling common themes among the diverse perfSONAR stakeholders that include: network operators and managers, end-users and network researchers. The timing and organization for the second workshop is significant because there are an increasing number of groups within NSF supported data-intensive computing and networking programs that are dealing with measurement, monitoring and troubleshooting of multi-domain issues. These groups are forming explicit measurement federations using perfSONAR to address a wide range of issues. In addition, the emergence and wide-adoption of new paradigms such as software-defined networking are taking shape to aid in traffic management needs of scientific communities and network operators. Consequently, there are new challenges that need to be addressed for extensible and programmable instrumentation, measurement data analysis, visualization and middleware security features in perfSONAR. This report summarizes the workshop efforts to bring together diverse groups for delivering targeted short/long talks, sharing latest advances, and identifying gaps that exist in the community for solving end-to-end performance problems in an effective, scalable fashion.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jul},
pages = {29–34},
numpages = {6},
keywords = {research challenges, next-generation measurement infrastructures, future multi-domain network monitoring}
}

@inproceedings{10.1145/3132300.3132305,
author = {Yatim, Ir. Fazilah Mat and Majid, Zulkepli and Amerudin, Shahabuddin},
title = {Locating Success Within A Geographic Information System},
year = {2017},
isbn = {9781450352895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132300.3132305},
doi = {10.1145/3132300.3132305},
abstract = {Tenaga Nasional Berhad (TNB) being one of the largest utilities in the Southeast Asia has embarked on enriching their GIS solutions suite for its business operations. A distribution station was chosen as a pilot project to run the business processes using GIS. The successful implementation is to be measured through its impact on the station day-today operations. Key success factors (KSF) were defined and will be measured with reference to component of GIS. The outcome of the measurement will guide the implementation of GIS nation-wide in TNB Distribution, Malaysia. This paper is aimed at providing insights for utilities who are keen in identifying those success factors and methodology of measuring the success of the GIS implementation.},
booktitle = {Proceedings of the International Conference on Imaging, Signal Processing and Communication},
pages = {138–142},
numpages = {5},
keywords = {GIS-Geographic Information System, KSF-Key Success Factors, TNB-Tenaga Nasional Berhad},
location = {Penang, Malaysia},
series = {ICISPC 2017}
}

@inproceedings{10.1145/3494193.3494250,
author = {Schuch de Azambuja, Luiza},
title = {Drivers and Barriers for the Development of Smart Sustainable Cities: A Systematic Literature Review},
year = {2022},
isbn = {9781450390118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494193.3494250},
doi = {10.1145/3494193.3494250},
abstract = {The term Smart Sustainable City (SSC) has been gaining popularity due to the growth of initiatives to address urban problems towards sustainable development. SSC can be considered as a combination of sustainable city and smart city, and some variance between the concepts may be expected. As this is a modern term, the literature falls short of studies presenting factors that hinder and/or facilitate the complex phenomenon of SSC development. Therefore, this paper aims to analyse scientific studies to identify aspects that influence the progress of smart sustainable cities. The methodological approach undertaken was a systematic literature review that included 169 papers. The results offer a comprehensive list of 57 drivers and 63 barriers, classified according to five main dimensions of a smart sustainable city, which are the three sustainability pillars (society, environment, and economy), combined to governance, and urban infrastructure. The findings revealed ‘governance’ as the most significant domain for SSC development, and multistakeholder engagement as one of the main challenges. This study shows that SSC is not a research field itself, but an interdisciplinary concept, contributing to academics, government, and policymakers for eradicating potential interferences in the development of smart and sustainable cities.},
booktitle = {14th International Conference on Theory and Practice of Electronic Governance},
pages = {422–428},
numpages = {7},
keywords = {sustainable city, enablers, challenges, smart city},
location = {Athens, Greece},
series = {ICEGOV 2021}
}

@inproceedings{10.1145/3371238.3371269,
author = {Pan, Zhiwen and Zhao, Shuangye and Pacheco, Jesus and Zhang, Yuxin and Song, Xiaofan and Chen, Yiqiang and Dai, Lianjun and Zhang, Jun},
title = {Comprehensive Data Management and Analytics for General Society Survey Dataset},
year = {2019},
isbn = {9781450376402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371238.3371269},
doi = {10.1145/3371238.3371269},
abstract = {The General Society Survey(GSS) is a kind of government-funded survey which aims at examining the Socio-economic status, quality of life, and structure of contemporary society. GSS dataset is regarded as one of the authoritative source for the government and organization practitioners to make data-driven policies. The previous analytic approaches for GSS dataset are designed by combining expert knowledges and simple statistics. In this paper, we proposed a comprehensive data management and data mining approach for GSS datasets. The approach is designed to be operated in a two-phase manner: a data management phase which can improve the quality of GSS data by performing attribute preprocessing and filter-based attribute selection; a data mining phase which can extract hidden knowledges from the dataset by performing data mining analysis including prediction analysis, classification analysis, association analysis and clustering analysis. By leveraging the power of data mining techniques, our proposed approach can explore knowledges in a fine-grained manner with minimum human interference. Experiments on Chinese General Social Survey dataset are conducted at the end to evaluate the performance of our approach.},
booktitle = {Proceedings of the 4th International Conference on Crowd Science and Engineering},
pages = {195–203},
numpages = {9},
keywords = {Decision support systems, Data mining, Society survey, Data management, Knowledge discovery},
location = {Jinan, China},
series = {ICCSE'19}
}

@article{10.1145/3529098,
author = {Chen, Rongli and Chen, Xiaozhong and Wang, Lei and Li, Jianxin},
title = {The Core Industry Manufacturing Process of Electronics Assembly Based on Smart Manufacturing},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2158-656X},
url = {https://doi.org/10.1145/3529098},
doi = {10.1145/3529098},
abstract = {This research takes a case study approach to show the development of a diverse adoption and product strategy distinct from the core manufacturing industry process. It explains the development status in all aspects of smart manufacturing, via the example of ceramic circuit board manufacturing and electronic assembly, and outlines future smart manufacturing plans and processes. This research proposed two experiments using Artificial Intelligence and deep learning are used to demonstrate the problems and solutions regarding methods in manufacturing and factory facilities, respectively. In the first experiment, a Bayesian network inference is used to find the cause of the problem of metal residues between electronic circuits through key process and quality correlations. In the second experiment, a Convolutional Neural Network (CNN) is used to identify false defects that were over-inspected during Automatic Optical Inspection. This improves the manufacturing process by enhancing yield rate and reducing cost. The contributions of the study in circuit board production. Smart manufacturing, with the application of a Bayesian network to an IoT setup, has addressed the problem of residue and redundant conductors on the edge of the ceramic circuit board pattern, and has improved and prevented leakage and high-frequency interference. CNN and deep learning were used to improve the accuracy of the AOI system, reduce the current manual review ratio, save labour costs and provide defect classification as a reference for pre-process improvement.},
note = {Just Accepted},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {aug},
keywords = {Bayesian network, Smart manufacturing, Neural network, Artificial intelligence, Industry manufacturing process}
}

@inproceedings{10.1145/3396956.3396975,
author = {van Donge, W. and Bharosa, N. and Janssen, M. F. W. H. A.},
title = {Future Government Data Strategies: Data-Driven Enterprise or Data Steward? Exploring Definitions and Challenges for the Government as Data Enterprise},
year = {2020},
isbn = {9781450387910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396956.3396975},
doi = {10.1145/3396956.3396975},
abstract = {Comparable to the concept of a data(-driven) enterprise, the concept of a ‘government as data (-driven) enterprise’ is gaining popularity as a data strategy. However, what it implies is unclear. The objective of this paper is to clarify the concept of the government as data (-driven) enterprise, and identify the challenges and drivers that shape future data strategies. Drawing on literature review and expert interviews, this paper provides a rich understanding of the challenges for developing sound future government data strategies. Our analysis shows that two contrary data strategies dominate the debate. On the one hand is the data-driven enterprise strategy that focusses on collecting and using data to improve or enrich government processes and services (internal orientation). On the other hand, respondents point to the urgent need for governments to take on data stewardship, so other parties can use data to develop value for society (external orientation). Since these data strategies are not mutually exclusive, some government agencies will attempt to combine them, which is very difficult to pull off. Nonetheless, both strategies demand a more data minded culture. Moreover, the successful implementation of either strategy requires mature data governance – something most organisations still need to master. This research contributes by providing more depth to these strategies. The main challenge for policy makers is to decide on which strategy best fits their agency's roles and responsibilities and develop a shared roadmap with the external actors while at the same time mature on data governance.},
booktitle = {The 21st Annual International Conference on Digital Government Research},
pages = {196–204},
numpages = {9},
keywords = {data governance, e-government, data enterprise, data stewardship, Data-driven government},
location = {Seoul, Republic of Korea},
series = {dg.o '20}
}

@inproceedings{10.1145/3027385.3027414,
author = {Hoel, Tore and Griffiths, Dai and Chen, Weiqin},
title = {The Influence of Data Protection and Privacy Frameworks on the Design of Learning Analytics Systems},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027414},
doi = {10.1145/3027385.3027414},
abstract = {Learning analytics open up a complex landscape of privacy and policy issues, which, in turn, influence how learning analytics systems and practices are designed. Research and development is governed by regulations for data storage and management, and by research ethics. Consequently, when moving solutions out the research labs implementers meet constraints defined in national laws and justified in privacy frameworks. This paper explores how the OECD, APEC and EU privacy frameworks seek to regulate data privacy, with significant implications for the discourse of learning, and ultimately, an impact on the design of tools, architectures and practices that now are on the drawing board. A detailed list of requirements for learning analytics systems is developed, based on the new legal requirements defined in the European General Data Protection Regulation, which from 2018 will be enforced as European law. The paper also gives an initial account of how the privacy discourse in Europe, Japan, South-Korea and China is developing and reflects upon the possible impact of the different privacy frameworks on the design of LA privacy solutions in these countries. This research contributes to knowledge of how concerns about privacy and data protection related to educational data can drive a discourse on new approaches to privacy engineering based on the principles of Privacy by Design. For the LAK community, this study represents the first attempt to conceptualise the issues of privacy and learning analytics in a cross-cultural context. The paper concludes with a plan to follow up this research on privacy policies and learning analytics systems development with a new international study.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {243–252},
numpages = {10},
keywords = {privacy frameworks, data protection by design, privacy by design, data protection by default, data protection, personal information, learning analytics systems design, learning analytics process requirements, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@article{10.14778/3415478.3415565,
author = {Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad},
title = {Building High Throughput Permissioned Blockchain Fabrics: Challenges and Opportunities},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415565},
doi = {10.14778/3415478.3415565},
abstract = {Since the introduction of Bitcoin---the first widespread application driven by blockchains---the interest in the design of blockchain-based applications has increased tremendously. At the core of these applications are consensus protocols that securely replicate client requests among all replicas, even if some replicas are Byzantine faulty. Unfortunately, these consensus protocols typically have low throughput, and this lack of performance is often cited as the reason for the slow wider adoption of blockchain technology. Consequently, many works focus on designing more efficient consensus protocols to increase throughput of consensus.We believe that this focus on consensus protocols only explains part of the story. To investigate this belief, we raise a simple question: Can a well-crafted system using a classical consensus protocol outperform systems using modern protocols? In this tutorial, we answer this question by diving deep into the design of blockchain systems. Further, we take an in-depth look at the theory behind consensus, which can help users select the protocol that best-fits their requirements. Finally, we share our vision of high-throughput blockchain systems that operate at large scales.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3441–3444},
numpages = {4}
}

@inproceedings{10.1145/3530019.3531346,
author = {Jiang, Shanshan and R\ae{}der, Truls Bakkejord},
title = {Experience on Using ArchiMate Models for Modelling Blockchain-Enhanced Value Chains},
year = {2022},
isbn = {9781450396134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530019.3531346},
doi = {10.1145/3530019.3531346},
abstract = {Blockchain is an emerging disruptive technology with a great potential to impact business value creation. Yet it is challenging to understand how blockchain can be utilised to improve enterprises’ performance and value creation. A formalised conceptual and enterprise modelling may help bridge the communication gap between domain experts and blockchain system designers. The goal of this study is to explore how can modelling facilitate blockchain-enhanced value chain design from a motivation viewpoint. We report our experience from modelling a blockchain-enhanced seafood supply chain using ArchiMate motivation and strategy models, where stakeholders have diverse concerns and goals. Preliminary results have indicated that such a modelling approach facilitates the common understanding, and the decision and prioritisation of the business strategies, as well as the identification of the blockchain benefits for the enterprises. It may help stakeholders along the value chain to align their business strategies and priorities, and can be an effective tool for value co-creation and enhancing the collaboration and communication among stakeholders and with blockchain application developers.},
booktitle = {Proceedings of the International Conference on Evaluation and Assessment in Software Engineering 2022},
pages = {375–382},
numpages = {8},
keywords = {Blockchain, ArchiMate, Goal, Strategy, Enterprise modelling, Motivation},
location = {Gothenburg, Sweden},
series = {EASE '22}
}

@inproceedings{10.1145/3292500.3332296,
author = {Dong, Xin Luna and Rekatsinas, Theodoros},
title = {Data Integration and Machine Learning: A Natural Synergy},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3332296},
doi = {10.1145/3292500.3332296},
abstract = {As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3193–3194},
numpages = {2},
keywords = {data fusion, data integration, schema mapping, data cleaning, entity linkage},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1145/3340286,
author = {Wong, Ka-Chun and Zhang, Jiao and Yan, Shankai and Li, Xiangtao and Lin, Qiuzhen and Kwong, Sam and Liang, Cheng},
title = {DNA Sequencing Technologies: Sequencing Data Protocols and Bioinformatics Tools},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3340286},
doi = {10.1145/3340286},
abstract = {The recent advances in DNA sequencing technology, from first-generation sequencing (FGS) to third-generation sequencing (TGS), have constantly transformed the genome research landscape. Its data throughput is unprecedented and severalfold as compared with past technologies. DNA sequencing technologies generate sequencing data that are big, sparse, and heterogeneous. This results in the rapid development of various data protocols and bioinformatics tools for handling sequencing data.In this review, a historical snapshot of DNA sequencing is taken with an emphasis on data manipulation and tools. The technological history of DNA sequencing is described and reviewed in thorough detail. To manipulate the sequencing data generated, different data protocols are introduced and reviewed. In particular, data compression methods are highlighted and discussed to provide readers a practical perspective in the real-world setting. A large variety of bioinformatics tools are also reviewed to help readers extract the most from their sequencing data in different aspects, such as sequencing quality control, genomic visualization, single-nucleotide variant calling, INDEL calling, structural variation calling, and integrative analysis. Toward the end of the article, we critically discuss the existing DNA sequencing technologies for their pitfalls and potential solutions.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {98},
numpages = {30},
keywords = {software, computational biology, history, bioinformatics, tools, DNA sequencing, third-generation sequencing (TGS), technology, data protocols}
}

@inproceedings{10.1145/3445969.3450427,
author = {Empl, Philip and Pernul, G\"{u}nther},
title = {A Flexible Security Analytics Service for the Industrial IoT},
year = {2021},
isbn = {9781450383196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445969.3450427},
doi = {10.1145/3445969.3450427},
abstract = {In Cloud Computing, the cloud serves as a central data hub for the Industrial Internet of Things' (IIoT) data and is deployed in diverse application fields, e.g., Smart Grid or Smart Manufacturing. Therefore, the aggregated and contextualized data is bundled in a central data hub, bringing tremendous cybersecurity advantages. Given the threat landscape in IIoT systems, especially SMEs (small and medium-sized enterprises) need to be prepared regarding their cybersecurity, react quickly, and strengthen their overall cybersecurity. For instance, with the application of machine learning algorithms, security-related data can be analyzed predictively in order to be able to ward off a potential attack at an early stage. Since modern reference architectures for IIoT systems, such as RAMI 4.0 or IIRA, consider cybersecurity approaches on a high level and SMEs lack financial funds and knowledge, this paper conceptualizes a security analytics service used as a security add-on to these reference architectures. Thus, this paper conceptualizes a flexible security analytics service that implements security capabilities with flexible analytical techniques that fit specific SMEs' needs. The security analytics service is also evaluated with a real-world use case.},
booktitle = {Proceedings of the 2021 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
pages = {23–32},
numpages = {10},
keywords = {security as a service, industrial IoT, security analytics},
location = {Virtual Event, USA},
series = {SAT-CPS '21}
}

@inproceedings{10.1145/2883851.2883893,
author = {Drachsler, Hendrik and Greller, Wolfgang},
title = {Privacy and Analytics: It's a DELICATE Issue a Checklist for Trusted Learning Analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883893},
doi = {10.1145/2883851.2883893},
abstract = {The widespread adoption of Learning Analytics (LA) and Educational Data Mining (EDM) has somewhat stagnated recently, and in some prominent cases even been reversed following concerns by governments, stakeholders and civil rights groups about privacy and ethics applied to the handling of personal data. In this ongoing discussion, fears and realities are often indistinguishably mixed up, leading to an atmosphere of uncertainty among potential beneficiaries of Learning Analytics, as well as hesitations among institutional managers who aim to innovate their institution's learning support by implementing data and analytics with a view on improving student success. In this paper, we try to get to the heart of the matter, by analysing the most common views and the propositions made by the LA community to solve them. We conclude the paper with an eight-point checklist named DELICATE that can be applied by researchers, policy makers and institutional managers to facilitate a trusted implementation of Learning Analytics.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {89–98},
numpages = {10},
keywords = {privacy, legal aspects, trust, data management, educational data mining, ethics, learning analytics, implementation},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2684200.2684336,
author = {Hu, Bo and Rodrigues, Eduarda Mendes and Viel, Emeric},
title = {Capri: Programmable Analytics for Linked Data},
year = {2014},
isbn = {9781450330015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684200.2684336},
doi = {10.1145/2684200.2684336},
abstract = {Link Data (LD) initiative has fundamentally changed the way how data are published, distributed, and consumed. It advocates data transparency and accessibility to fulfill the Web of Data vision. Thus far, tens of billions of data items have been made publicly available in machine-understandable forms (e.g. RDF). The sheer size of LD data, however, has not resulted in a significant increase of data consumption and thus a self-sustainable consumption-driven publication. We contend that this is primarily due to the lack of tooling for exploiting LD. A new programming paradigm is necessary to simplify and encourage value-add LD data utilisation.This paper reports an on-going project towards programmable Linked Open Data. We propose to tap into a distributed computing environment underpinning the popular statistical toolkit R. Where possible, native R operators and functions are used in our approach so as to lower the learning curve for experienced data scientists.We believe a report to the relevant community at this stage can help us to collect critical requirements before moving into the next stage of development. The crux of our future work lies in comprehensive and extensive evaluations, in terms of, but not limited to, system performance, system stability, system scalability, programming productivity and user experience.},
booktitle = {Proceedings of the 16th International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {217–223},
numpages = {7},
keywords = {RDF, R, Linked Data},
location = {Hanoi, Viet Nam},
series = {iiWAS '14}
}

@inproceedings{10.1145/3428502.3428576,
author = {Hanbal, Rajesh Dinesh and Prakash, Amit and Srinivasan, Janaki},
title = {Who Drives Data in Data-Driven Governance? The Politics of Data Production in India's Livelihood Program},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428576},
doi = {10.1145/3428502.3428576},
abstract = {The increased digitisation of government information systems, as well as emerging data analytics and visualization techniques, have led lately to a surge in interest in the role of data in governance and development. The latest buzzwords in governance now include data-driven governance, data-for-development, evidence-based policy-making, and open government data. However, not much attention has been paid to understand the process of the production of data in government information systems. Our findings are based on six months of an ethnographic study of India's livelihood program- Mahatma Gandhi National Rural Employment Guarantee Act (MGNREGA) in a rural district of Karnataka. We argue that the practice of data production is carefully managed and controlled by local power elites providing an illusion of transparency in a digital information system. Understanding and recognizing the political nature of data production can help in better evaluation of development interventions, policy-making as well as in the design of more just information systems.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {485–493},
numpages = {9},
keywords = {data justice, Data production, Open government data},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/3510455.3512779,
author = {Dey, Sangeeta and Lee, Seok-Won},
title = {Are We Training with the Right Data? Evaluating Collective Confidence in Training Data Using Dempster Shafer Theory},
year = {2022},
isbn = {9781450392242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510455.3512779},
doi = {10.1145/3510455.3512779},
abstract = {The latest trend of incorporating various data-centric machine learning (ML) models in software-intensive systems has posed new challenges in the quality assurance practice of software engineering, especially in a high-risk environment. ML experts are now focusing on explaining ML models to assure the safe behavior of ML-based systems. However, not enough attention has been paid to explain the inherent uncertainty of the training data. The current practice of ML-based system engineering lacks transparency in the systematic fitness assessment process of the training data before engaging in the rigorous ML model training. We propose a method of assessing the collective confidence in the quality of a training dataset by using Dempster Shafer theory and its modified combination rule (Yager's rule). With the example of training datasets for pedestrian detection of autonomous vehicles, we demonstrate how the proposed approach can be used by the stakeholders with diverse expertise to combine their beliefs in the quality arguments and evidences about the data. Our results open up a scope of future research on data requirements engineering that can facilitate evidence-based data assurance for ML-based safety-critical systems.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {11–15},
numpages = {5},
keywords = {safety, Dempster Shafer theory, data uncertainty, machine learning},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-NIER '22}
}

@inproceedings{10.1145/3289402.3289526,
author = {Dahbi, Kawtar Younsi and Lamharhar, Hind and Chiadmi, Dalila},
title = {Exploring Dimensions Influencing the Usage of Open Government Data Portals},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289526},
doi = {10.1145/3289402.3289526},
abstract = {Governments are considered as one of the major producers of data. Opening up and publishing this Big Government Data in national portals have significant impact on fostering innovation, improving transparency, public accountability and collaboration. Thus, the expected benefits are hindered by several factors that influence the usage of Open Government Data portals, exploring and investigating these factors is the first step to propose an evaluation approach for OGD portals and promote their usage. In this work, we identified a set of evaluation dimensions that affect OGD portal's usage and fulfillment of users' needs and requirements. According to the identified dimensions, we propose an evaluation of two national OGD portals},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {26},
numpages = {6},
keywords = {Open Government Data portals, Evaluation, usage, Open Government Data},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1145/3378393.3402248,
author = {Ramanujapuram, Arun and Malemarpuram, Charan Kumar},
title = {Enabling Sustainable Behaviors of Data Recording and Use in Low-Resource Supply Chains},
year = {2020},
isbn = {9781450371292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378393.3402248},
doi = {10.1145/3378393.3402248},
abstract = {Public services, such as public health supply chains, in low- and middle-income countries can be characterized as low-resource environments, where both infrastructure and human capacity are limited. There is no strong culture of data recording or use, with ad hoc reporting practices, poor planning and lack of coordination. All these lead to poor supply chain performance, thereby restricting access to medicines, and eventually resulting in poorer health and mortality.We describe the ground-up design of Logistimo SCM, a supply chain management software, offered as a service, that has enabled a transformative change in public health supply chains, leading to improved performance. Our approach is rooted in bottom-up empowerment of the human value chain, based on the principle that higher self-efficacy amongst health workers and managers can lead to sustained changes in data recording and use behaviors. This is achieved through a service that optimizes data collection effort, maximizes supervisory bandwidth, promotes proactive and collaborative operations, and enables frictionless performance recognition. We describe the guiding principles of inclusive software service design and four mechanisms that enable the appropriate conditions for stimulating a behavior of data recording and use. We demonstrate their effectiveness in achieving good supply chain performance through case studies in India and Africa. The principles and methods discussed here are generic and can be applied to any low-resource environment.},
booktitle = {Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies},
pages = {65–75},
numpages = {11},
keywords = {low-resource environment, public health, supply chain, Data collection, culture of data use, data use behavior},
location = {Ecuador},
series = {COMPASS '20}
}

@article{10.14778/3415478.3415504,
author = {Wang, Chen and Huang, Xiangdong and Qiao, Jialin and Jiang, Tian and Rui, Lei and Zhang, Jinrui and Kang, Rong and Feinauer, Julian and McGrail, Kevin A. and Wang, Peng and Luo, Diaohan and Yuan, Jun and Wang, Jianmin and Sun, Jiaguang},
title = {Apache IoTDB: Time-Series Database for Internet of Things},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415504},
doi = {10.14778/3415478.3415504},
abstract = {The amount of time-series data that is generated has exploded due to the growing popularity of Internet of Things (IoT) devices and applications. These applications require efficient management of the time-series data on both the edge and cloud side that support high throughput ingestion, low latency query and advanced time series analysis. In this demonstration, we present Apache IoTDB managing time-series data to enable new classes of IoT applications. IoTDB has both edge and cloud versions, provides an optimized columnar file format for efficient time-series data storage, and time-series database with high ingestion rate, low latency queries and data analysis support. It is specially optimized for time-series oriented operations like aggregations query, down-sampling and sub-sequence similarity search. An edge-to-cloud time-series data management application is chosen to demonstrate how IoTDB handles time-series data in real-time and supports advanced analytics by integrating with Hadoop and Spark. An end-to-end IoT data management solution is shown by integrating IoTDB with PLC4x, Calcite, and Grafana.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {2901–2904},
numpages = {4}
}

@inproceedings{10.1145/3410886.3410913,
author = {Kritzinger, A.K. and Calitz, A.P. and Westraadt, L.},
title = {Data Wrangling for South African Smart City Crime Data},
year = {2020},
isbn = {9781450388474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410886.3410913},
doi = {10.1145/3410886.3410913},
abstract = {South Africa (S.A.) is currently facing economic and social challenges that could benefit from the implementation of international smart city guidelines. Crucial to transforming a city into a smart city is the collection and access to reliable data. One of the main problems experienced by S.A. cities is the limited access to data, resulting from a traditionally fragmented approach to data collection, sharing and use. Crime-related data is one of the most commonly collected datasets in smart cities. In S.A., crime data is predominantly collected by the S.A. Police Services (SAPS) and security companies. While the latter are not readily available for public use, SAPS crime data is consolidated and disseminated at the national level. Initial data exploration, however, shows that temporal, spatial and structural inconsistencies in the data limits the usefulness of available crime data. In this study, the inconsistencies in SAPS crime data are summarised, and standard data wrangling techniques are implemented and evaluated to clean the data. The study proposes a data wrangling model for S.A. crime data. Furthermore, this study will further developments that could benefit S.A. cities in general as they transform into smart cities.},
booktitle = {Conference of the South African Institute of Computer Scientists and Information Technologists 2020},
pages = {198–209},
numpages = {12},
keywords = {Data Cleaning, Open Data, Data Wrangling, Smart City Data},
location = {Cape Town, South Africa},
series = {SAICSIT '20}
}

@inproceedings{10.1145/3486640.3491391,
author = {Devarakonda, Ranjeet and Guntupally, Kavya and Thornton, Michele and Wei, Yaxing and Singh, Debjani and Lunga, Dalton},
title = {FAIR Interfaces for Geospatial Scientific Data Searches},
year = {2021},
isbn = {9781450391238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486640.3491391},
doi = {10.1145/3486640.3491391},
abstract = {Several factors must be considered in designing a highly accurate, reliable, scalable, and user-friendly geospatial data search interfaces. This paper examines four critical questions that ought to be considered during design phase: (1) Is the search interface or API that provides the search capability useable by both humans and machines? (2) Are the results consistent and reliable? (3) Is the output response format free to use, community-defined, and non-propriety? (4) Does the API clearly state the usage clauses? This paper discusses how certain data repositories at the US Department of Energy's Oak Ridge National Laboratory apply FAIR data principles to enable geospatial searches and address the above-mentioned questions.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data},
pages = {1–4},
numpages = {4},
keywords = {ARM Data Center, ORNL DAAC, FAIR data principle for scientific data, Geospatial search interfaces},
location = {Beijing, China},
series = {GeoSearch'21}
}

@inproceedings{10.1145/3457784.3457820,
author = {Al-Khowarizmi, Al-Khowarizmi and Lubis, Muharman and Ridho Lubis, Arif and Fauzi, Fauzi and Ramadhan Nasution, Ilham},
title = {Model of Business Intelligence Applied the Principle of Cooperative Society in the Business Forums},
year = {2021},
isbn = {9781450388825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457784.3457820},
doi = {10.1145/3457784.3457820},
abstract = {Business forums are activities between individuals and organizations that carry out the transactions on online media or within applications, which spread across countries. Along with the development of information technology towards business intelligence (BI), the business processes carried out in the business forum are modeled specifically in order to create an effort and attempt to follow the indicator and criteria from the industrial revolution 4.0. In this paper, a model is designed to combine three type of principles, namely the business forum, BI and the cooperative principle. Actually, cooperatives have been long abandoned since the existence of conventional and Islamic banking concept but it has kinship principle to divide the profits based on the size of the contribution given. Meanwhile, BI model is designed to obtain a formula from the cooperative principle, namely the residual income from operations where the transaction process is successfully implemented through the application to allocate a portion of the profits to the members based on the specified percent.},
booktitle = {2021 10th International Conference on Software and Computer Applications},
pages = {224–228},
numpages = {5},
keywords = {Business Intelligence, Cooperative Society, Business Forum, Model and Simulation},
location = {Kuala Lumpur, Malaysia},
series = {ICSCA 2021}
}

@inproceedings{10.1109/JCDL.2019.00088,
author = {Yu, Qianqian and Zhang, Jianyong and Qian, Li and Dong, Zhipeng and Huang, Yongwen and Jianhua, Liu},
title = {Practice of Constructing Name Authority Database Based on Multi-Source Data Integration},
year = {2020},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00088},
doi = {10.1109/JCDL.2019.00088},
abstract = {Name authority is a common issue in digital library. This paper mainly summarizes the practice of constructing name authority database based on multi-source data in NSTL. Firstly, we load, integrate different source data and convert them into unified structure. Then, we extract scientific entities and relationships from these data, according to metadata model. For different entities, we use different disambiguation rules and algorithms. As a result, we have constructed author name authority database, institution authority database, journal authority database, and fund authority database. Compared with Incites, taking six institutions name authority data as a sample, the result shows that the average accuracy can reach 86.8%.1},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {398–399},
numpages = {2},
keywords = {name disambiguation, multi-source, name authority, NSTL},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@article{10.1145/3466160,
author = {Sambasivan, Nithya},
title = {Seeing like a Dataset from the Global South},
year = {2021},
issue_date = {July - August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1072-5520},
url = {https://doi.org/10.1145/3466160},
doi = {10.1145/3466160},
abstract = {This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor},
journal = {Interactions},
month = {jun},
pages = {76–78},
numpages = {3}
}

@inproceedings{10.1145/1963192.1963325,
author = {Baeza-Yates, Ricardo and Masan\`{e}s, Julien and Spaniol, Marc},
title = {The 1st Temporal Web Analytics Workshop (TWAW)},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963325},
doi = {10.1145/1963192.1963325},
abstract = {The objective of the 1st Temporal Web Analytics Workshop (TWAW) is to provide a venue for researchers of all domains (IE/IR, Web mining etc.) where the temporal dimension opens up an entirely new range of challenges and possibilities. The workshop's ambition is to help shaping a community of interest on the research challenges and possibilities resulting from the introduction of the time dimension in Web analysis. The maturity of the Web, the emergence of large scale repositories of Web material, makes this very timely and a growing sets of research and services (recorded future1, truthy2 launched just in the last months) are emerging that have this focus in common. Having a dedicated workshop will help, we believe, to take a rich and cross-domain approach to this new research challenge with a strong focus on the temporal dimension.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {307–308},
numpages = {2},
keywords = {distributed data analytics, web scale data analytics, temporal web analytics},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/3265757.3265766,
author = {Grillenberger, Andreas and Romeike, Ralf},
title = {Developing a Theoretically Founded Data Literacy Competency Model},
year = {2018},
isbn = {9781450365888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265757.3265766},
doi = {10.1145/3265757.3265766},
abstract = {Today, data is everywhere: Our digitalized world depends on enormous amounts of data that are captured by and about everyone and considered a valuable resource. Not only in everyday life, but also in science, the relevance of data has clearly increased in recent years: Nowadays, data-driven research is often considered a new research paradigm. Thus, there is general agreement that basic competencies regarding gathering, storing, processing and visualizing data, often summarized under the term data literacy, are necessary for every scientist today. Moreover, data literacy is generally important for everyone, as it is essential for understanding how the modern world works. Yet, at the moment data literacy is hardly considered in CS teaching at schools. To allow deeper insight into this field and to structure related competencies, in this work we develop a competency model of data literacy by theoretically deriving central content and process areas of data literacy from existing empirical work, keeping a school education perspective in mind. The resulting competency model is contrasted to other approaches describing data literacy competencies from different perspectives. The practical value of this work is emphasized by giving insight into an exemplary lesson sequence fostering data literacy competencies.},
booktitle = {Proceedings of the 13th Workshop in Primary and Secondary Computing Education},
articleno = {9},
numpages = {10},
keywords = {CS education, data management, competency model, data literacy, data, data science},
location = {Potsdam, Germany},
series = {WiPSCE '18}
}

@inproceedings{10.1145/2908131.2908172,
author = {Weller, Katrin and Kinder-Kurlanda, Katharina E.},
title = {A Manifesto for Data Sharing in Social Media Research},
year = {2016},
isbn = {9781450342087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908131.2908172},
doi = {10.1145/2908131.2908172},
abstract = {More and more researchers want to share research data collected from social media to allow for reproducibility and comparability of results. With this paper we want to encourage them to pursue this aim -- despite initial obstacles that they may face. Sharing can occur in various, more or less formal ways. We provide background information that allows researchers to make a decision about whether, how and where to share depending on their specific situation (data, platform, targeted user group, research topic etc.). Ethical, legal and methodological considerations are important for making this decision. Based on these three dimensions we develop a framework for social media sharing that can act as a first set of guidelines to help social media researchers make practical decisions for their own projects. In the long run, different stakeholders should join forces to enable better practices for data sharing for social media researchers. This paper is intended as our call to action for the broader research community to advance current practices of data sharing in the future.},
booktitle = {Proceedings of the 8th ACM Conference on Web Science},
pages = {166–172},
numpages = {7},
keywords = {archiving, methodology, social media, data protection, legal issues, reproducibility, data sharing, data archives, privacy},
location = {Hannover, Germany},
series = {WebSci '16}
}

@article{10.1145/2070736.2070750,
author = {Badia, Antonio and Lemire, Daniel},
title = {A Call to Arms: Revisiting Database Design},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2070736.2070750},
doi = {10.1145/2070736.2070750},
journal = {SIGMOD Rec.},
month = {nov},
pages = {61–69},
numpages = {9}
}

@article{10.1145/3154525,
author = {Fathy, Yasmin and Barnaghi, Payam and Tafazolli, Rahim},
title = {Large-Scale Indexing, Discovery, and Ranking for the Internet of Things (IoT)},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3154525},
doi = {10.1145/3154525},
abstract = {Network-enabled sensing and actuation devices are key enablers to connect real-world objects to the cyber world. The Internet of Things (IoT) consists of the network-enabled devices and communication technologies that allow connectivity and integration of physical objects (Things) into the digital world (Internet). Enormous amounts of dynamic IoT data are collected from Internet-connected devices. IoT data are usually multi-variant streams that are heterogeneous, sporadic, multi-modal, and spatio-temporal. IoT data can be disseminated with different granularities and have diverse structures, types, and qualities. Dealing with the data deluge from heterogeneous IoT resources and services imposes new challenges on indexing, discovery, and ranking mechanisms that will allow building applications that require on-line access and retrieval of ad-hoc IoT data. However, the existing IoT data indexing and discovery approaches are complex or centralised, which hinders their scalability. The primary objective of this article is to provide a holistic overview of the state-of-the-art on indexing, discovery, and ranking of IoT data. The article aims to pave the way for researchers to design, develop, implement, and evaluate techniques and approaches for on-line large-scale distributed IoT applications and services.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {29},
numpages = {53},
keywords = {large-scale data, Internet of things (IoT), ranking, indexing, discovery, wireless sensor network (WSN)}
}

@article{10.1145/3517189,
author = {Suhail, Sabah and Hussain, Rasheed and Jurdak, Raja and Oracevic, Alma and Salah, Khaled and Hong, Choong Seon and Matulevi\v{c}ius, Raimundas},
title = {Blockchain-Based Digital Twins: Research Trends, Issues, and Future Challenges},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {11s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3517189},
doi = {10.1145/3517189},
abstract = {Industrial processes rely on sensory data for decision-making processes, risk assessment, and performance evaluation. Extracting actionable insights from the collected data calls for an infrastructure that can ensure the dissemination of trustworthy data. For the physical data to be trustworthy, it needs to be cross validated through multiple sensor sources with overlapping fields of view. Cross-validated data can then be stored on the blockchain, to maintain its integrity and trustworthiness. Once trustworthy data is recorded on the blockchain, product lifecycle events can be fed into data-driven systems for process monitoring, diagnostics, and optimized control. In this regard, digital twins (DTs) can be leveraged to draw intelligent conclusions from data by identifying the faults and recommending precautionary measures ahead of critical events. Empowering DTs with blockchain in industrial use cases targets key challenges of disparate data repositories, untrustworthy data dissemination, and the need for predictive maintenance. In this survey, while highlighting the key benefits of using blockchain-based DTs, we present a comprehensive review of the state-of-the-art research results for blockchain-based DTs. Based on the current research trends, we discuss a trustworthy blockchain-based DTs framework. We also highlight the role of artificial intelligence in blockchain-based DTs. Furthermore, we discuss the current and future research and deployment challenges of blockchain-supported DTs that require further investigation.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {240},
numpages = {34},
keywords = {industrial control systems (ICSs), Artificial intelligence (AI), digital twins (DTs), Industry 4.0, cyber-physical systems (CPSs), Internet of Things (IoT), blockchain}
}

@inproceedings{10.1145/3548608.3559245,
author = {Chen, Jun and Song, Binghu and Li, Xiaopeng and Feng, Liang and Zhai, Yujia},
title = {Research on Intelligent Management and Control Platform of Power Material Storage Based on Web},
year = {2022},
isbn = {9781450397179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548608.3559245},
doi = {10.1145/3548608.3559245},
abstract = {In view of the current problems of time-consuming and poor effect of intelligent management and control of power material warehousing, this paper puts forward the research method of intelligent management and control platform of power material warehousing based on Web. First, combined with web technology, this paper constructs the ERP management model of power material warehousing, improves the ERP management function of power material intelligent warehousing, optimizes the information processing algorithm of power material warehousing, and optimizes the structure and function of the platform. The processing steps of the intelligent management and control platform for power material warehousing are simplified. Finally, the experiment proves that the intelligent management and control platform for power material warehousing based on Web can realize the management of massive warehousing information more quickly in the actual application process, and its running time is shortened by more than 4h, which has high practicability.},
booktitle = {Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics},
pages = {479–483},
numpages = {5},
location = {Nanjing, China},
series = {ICCIR '22}
}

@article{10.1145/3516515,
author = {Sambasivan, Nithya},
title = {All Equation, No Human: The Myopia of AI Models},
year = {2022},
issue_date = {March - April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1072-5520},
url = {https://doi.org/10.1145/3516515},
doi = {10.1145/3516515},
abstract = {This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor},
journal = {Interactions},
month = {feb},
pages = {78–80},
numpages = {3}
}

@inproceedings{10.1145/3176349.3176901,
author = {Bogers, Toine and G\"{a}de, Maria and Freund, Luanne and Hall, Mark and Koolen, Marijn and Petras, Vivien and Skov, Mette},
title = {Workshop on Barriers to Interactive IR Resources Re-Use},
year = {2018},
isbn = {9781450349253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176349.3176901},
doi = {10.1145/3176349.3176901},
abstract = {The goal of this workshop is to serve as a starting point for a community-driven effort to design and implement a platform for the collection, organization, maintenance, and sharing of resources for IIR experimentation. As in all scientific endeavors, progress in IIR research is contingent on the ability to build on previous ideas, approaches, and resources. However, we believe there to be a number of barriers to reproducibility and re-use of resources in IIR research: the fragmentary nature of how the community»s resources are organized, the lack of awareness of their existence, documentation and organization of the resources, the nature of the typical research publication cycle, and the effort required to make such resources available. We believe that an online platform dedicated to the collection and organization of IIR resources could be a promising way of overcoming these barriers. The workshop therefore aims to serve both as a brainstorming opportunity about the shape this iRepository should take, as well as a way of building support in the community for its implementation.},
booktitle = {Proceedings of the 2018 Conference on Human Information Interaction &amp; Retrieval},
pages = {382–385},
numpages = {4},
keywords = {evaluation, intertactive information retrieval, repository},
location = {New Brunswick, NJ, USA},
series = {CHIIR '18}
}

@inproceedings{10.1145/3299819.3299850,
author = {Li, Ying and Zhang, AiMin and Zhang, Xinman and Wu, Zhihui},
title = {A Data Lake Architecture for Monitoring and Diagnosis System of Power Grid},
year = {2018},
isbn = {9781450366236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299819.3299850},
doi = {10.1145/3299819.3299850},
abstract = {In this paper, a data lake architecture is proposed for a class of monitoring and diagnostic systems applied to power grid. The differences between data lake and data warehouse is studied to make an informed decision on how to manage a huge amount of data. To adapt to the characteristics and performances of historical data and real-time data of power grid equipment, a monitoring and diagnosis system based on data lake storage architecture is designed. The application of the framework indicates the applicability and effectiveness of data lake architecture.},
booktitle = {Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference},
pages = {192–198},
numpages = {7},
keywords = {Data Pond, Monitoring And Diagnostic, Power Grid, Data Lake},
location = {Tokyo, Japan},
series = {AICCC '18}
}

@inproceedings{10.1145/3522664.3528603,
author = {Husom, Erik Johannes and Tverdal, Simeon and Goknil, Arda and Sen, Sagar},
title = {UDAVA: An Unsupervised Learning Pipeline for Sensor Data Validation in Manufacturing},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528603},
doi = {10.1145/3522664.3528603},
abstract = {Manufacturing has enabled the mechanized mass production of the same (or similar) products by replacing craftsmen with assembly lines of machines. The quality of each product in an assembly line greatly hinges on continual observation and error compensation during machining using sensors that measure quantities such as position and torque of a cutting tool and vibrations due to possible imperfections in the cutting tool and raw material. Patterns observed in sensor data from a (near-)optimal production cycle should ideally recur in subsequent production cycles with minimal deviation. Manually labeling and comparing such patterns is an insurmountable task due to the massive amount of streaming data that can be generated from a production process. We present UDAVA, an unsupervised machine learning pipeline that automatically discovers process behavior patterns in sensor data for a reference production cycle. UDAVA performs clustering of reduced dimensionality summary statistics of raw sensor data to enable high-speed clustering of dense time-series data. It deploys the model as a service to verify batch data from subsequent production cycles to detect recurring behavior patterns and quantify deviation from the reference behavior. We have evaluated UDAVA from an AI Engineering perspective using two industrial case studies.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {159–169},
numpages = {11},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/2957276.2957280,
author = {Muller, Michael and Guha, Shion and Baumer, Eric P.S. and Mimno, David and Shami, N. Sadat},
title = {Machine Learning and Grounded Theory Method: Convergence, Divergence, and Combination},
year = {2016},
isbn = {9781450342766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957276.2957280},
doi = {10.1145/2957276.2957280},
abstract = {Grounded Theory Method (GTM) and Machine Learning (ML) are often considered to be quite different. In this note, we explore unexpected convergences between these methods. We propose new research directions that can further clarify the relationships between these methods, and that can use those relationships to strengthen our ability to describe our phenomena and develop stronger hybrid theories.},
booktitle = {Proceedings of the 19th International Conference on Supporting Group Work},
pages = {3–8},
numpages = {6},
keywords = {supervised learning, axial coding, machine learning, grounded theory, coding families, unsupervised learning},
location = {Sanibel Island, Florida, USA},
series = {GROUP '16}
}

@inproceedings{10.1145/3297280.3297354,
author = {Satti, Fahad Ahmed and Khan, Wajahat Ali and Lee, Ganghun and Khattak, Asad Masood and Lee, Sungyoung},
title = {Resolving Data Interoperability in Ubiquitous Health Profile Using Semi-Structured Storage and Processing},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297354},
doi = {10.1145/3297280.3297354},
abstract = {Advancements in the field of healthcare information management have led to the development of a plethora of software, medical devices and standards. As a consequence, the rapid growth in quantity and quality of medical data has compounded the problem of heterogeneity; thereby decreasing the effectiveness and increasing the cost of diagnostics, treatment and follow-up. However, this problem can be resolved by using a semi-structured data storage and processing engine, which can extract semantic value from a large volume of patient data, produced by a variety of data sources, at variable rates and conforming to different abstraction levels. Going beyond the traditional relational model and by re-purposing state-of-the-art tools and technologies, we present, the Ubiquitous Health Profile (UHPr), which enables a semantic solution to the data interoperability problem, in the domain of healthcare1.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {762–770},
numpages = {9},
keywords = {text tagging, ACM proceedings},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3267305.3274762,
author = {Budde, Matthias and Riedel, Till},
title = {Challenges in Capturing and Analyzing High Resolution Urban Air Quality Data},
year = {2018},
isbn = {9781450359665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267305.3274762},
doi = {10.1145/3267305.3274762},
abstract = {Classic measurement grids with their static and expensive infrastructure are unfit to realize modern air quality monitoring needs, such as source appointment, pollution tracking or the assessment of personal exposure. Fine grained air quality assessment (both in time and space) is the future. Different approaches, ranging from measurement with low-cost sensors over advanced modeling and remote sensing to combinations of these have been proposed. This position paper summarizes our previous contributions in this field and lists what we see as open challenges for future research.},
booktitle = {Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers},
pages = {1162–1165},
numpages = {4},
keywords = {Air quality, sensing, PM2.5, particulate matter, PM10, challenges, urban air},
location = {Singapore, Singapore},
series = {UbiComp '18}
}

@inproceedings{10.1145/3495018.3495486,
author = {Liu, Ying and Yu, Wei and Xiao, Suhong},
title = {Digital Protection of Nanfeng Nuo Mask Based on AR Technology},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495486},
doi = {10.1145/3495018.3495486},
abstract = {The sculpture of Nanfeng Nuo mask originated in the Han Dynasty, developed in the Tang and Song Dynasties, and prospered in the Ming and Qing Dynasties. The carving art has been passed down to this day. The sculptures of Nanfeng Nuo masks are famous for their simple and profound, vivid shapes and delicate techniques. This article first stated that under the background of AR technology, in terms of digital protection mode, limited by ideological understanding and technology, the current digital protection of Chinese traditional culture is still at the stage of digital information collection and preservation. How to enrich and perfect the existing digital protection mode with new digital technology is an urgent problem to be solved in the new era. Taking the Nanfeng Nuo mask as an example, this research analyzes the inheritance dilemma of the Nanfeng Nuo mask wood carving and the modern and innovative protection model through the reading of historical documents, field inspections of existing wood carvings, survey visits to the protection status, and understanding of digital technology. Through the fuzzy KNN algorithm and AR compared to the database, the various databases are related to form a complete protection system; in the "live inheritance protection mode", AR technology is proposed as the basic technology, and AR image acquisition technology and AR display technology are proposed. , AR human-computer interaction technology and digital protection mode are combined, and then a digital protection platform based on AR technology is designed to achieve the realization of the live inheritance mode by the way audiences participate in the use of the digital protection platform. Experimental research results show that people in their 30s to 40s may take a long time to receive and learn when facing a new interactive operating system, but now due to the popularity of social software such as WeChat, the 30 to 40 age group 24.4% of the population can also perform basic operations, which provides a prerequisite for the use of AR technology to digitize the Nanfeng Nuo mask.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1792–1796},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@article{10.1145/3185504,
author = {Liu, Jinwei and Shen, Haiying and Narman, Husnu S. and Chung, Wingyan and Lin, Zongfang},
title = {A Survey of Mobile Crowdsensing Techniques: A Critical Component for The Internet of Things},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3185504},
doi = {10.1145/3185504},
abstract = {Mobile crowdsensing serves as a critical building block for emerging Internet of Things (IoT) applications. However, the sensing devices continuously generate a large amount of data, which consumes much resources (e.g., bandwidth, energy, and storage) and may sacrifice the Quality-of-Service (QoS) of applications. Prior work has demonstrated that there is significant redundancy in the content of the sensed data. By judiciously reducing redundant data, data size and load can be significantly reduced, thereby reducing resource cost and facilitating the timely delivery of unique, probably critical information and enhancing QoS. This article presents a survey of existing works on mobile crowdsensing strategies with an emphasis on reducing resource cost and achieving high QoS. We start by introducing the motivation for this survey and present the necessary background of crowdsensing and IoT. We then present various mobile crowdsensing strategies and discuss their strengths and limitations. Finally, we discuss future research directions for mobile crowdsensing for IoT. The survey addresses a broad range of techniques, methods, models, systems, and applications related to mobile crowdsensing and IoT. Our goal is not only to analyze and compare the strategies proposed in prior works, but also to discuss their applicability toward the IoT and provide guidance on future research directions for mobile crowdsensing.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {jun},
articleno = {18},
numpages = {26},
keywords = {cost-effectiveness, quality of service, redundancy elimination, Mobile crowdsensing, Internet of Things}
}

@inproceedings{10.1145/3333165.3333168,
author = {Lamghari, Zineb and Radgui, Maryam and Saidi, Rajaa and Rahmani, Moulay Driss},
title = {Passage Challenges from Data-Intensive System to Knowledge-Intensive System Related to Process Mining Field},
year = {2019},
isbn = {9781450360890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333165.3333168},
doi = {10.1145/3333165.3333168},
abstract = {Process mining has emerged as a research field that focuses on the analysis of processes using event data. Process mining is a current topic that reveals several challenges, the most important of which have defined in the Process Mining Manifesto [1]. However, none of the published works have mentioned the progress of process challenges from data-intensive system to knowledge-intensive system related to process mining field. Therefore, the objective of this paper is to provide researchers with the recent challenges emerged during the passage from data-intensive system to knowledge-intensive system.},
booktitle = {Proceedings of the ArabWIC 6th Annual International Conference Research Track},
articleno = {3},
numpages = {6},
keywords = {Business Process Management, Adaptive Case Management, Process mining challenges, Process Mining, Data-intensive, Knowledge-intensive},
location = {Rabat, Morocco},
series = {ArabWIC 2019}
}

@article{10.1145/3532090,
author = {Wu, Jimmy Ming-Tai and Teng, Qian and Huda, Shamsul and Chen, Yeh-Cheng and Chen, Chien-Ming},
title = {A Privacy Frequent Itemsets Mining Framework for Collaboration in IoT Using Federated Learning},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1550-4859},
url = {https://doi.org/10.1145/3532090},
doi = {10.1145/3532090},
abstract = {Rapid advancement of industrial internet of things (IoT) technology has changed the supply chain network to an open system to meet the high demand for individualized products and provide better customer experiences. However the open-system supply chain has forced many small and midsize enterprises (SMEs) to adopt vertical integration by being divided into smaller companies with a distinctive business for each SME but a central alliance to produce a range of products and gain competencies. Therefore, existing models do not guarantee the protection of data privacy of individual SMEs. Moreover, especially for the IoT environment, collecting data in a secure way and revealing valuable knowledge in an IoT network is difficult. How to share data in a secure framework is of paramount importance in the internet of behavior field. In this article, a privacy-preserving data-mining framework is proposed for joint-venture industrial collaborative activities by combining federated learning and a ”pre-large concept” of data-mining techniques. The novelty of the proposed approach is that, while mining high-utility itemsets from multiple datasets, it does not require direct data sharing. In the proposed method, the federated-learning framework can learn from aggregated learning parameters without scanning all data from different sets. The pre-large concept in this approach reduces the amount of scanning into different datasets. Thus, the approach makes it possible to train federated learning more quickly while protecting the privacy of individual data owners. The approach has been tested on real industrial datasets in a collaborative environment. Extensive experimental results show that the approach achieves high accuracy compared with conventional data-mining techniques while preserving the privacy of datasets.},
note = {Just Accepted},
journal = {ACM Trans. Sen. Netw.},
month = {may},
keywords = {privacy protection, Internet of Behaviour, Frequent itemset mining, federated learning, pre-large concept, industrial collaborative data mining}
}

@inproceedings{10.1145/3544109.3544134,
author = {Zhang, Shaochen and Qu, Youyang and Wang, Peng},
title = {Design of Cloud Computing Data Center Security System Based on Virtualization Environment},
year = {2022},
isbn = {9781450395786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544109.3544134},
doi = {10.1145/3544109.3544134},
abstract = {In order to improve the security of cloud computing data center in the virtualized environment, a security system design of cloud computing data center is proposed based on the virtualization environment. Firstly, the security architecture of cloud computing data center is constructed, and the security of data center is evaluated. By optimizing the system equipment structure and operation steps, the security performance of cloud computing data center can be improved. The experimental results show that the design method of cloud computing data center security architecture based on Virtualization environment has high precision, good practical effect and fully meets the research requirements.},
booktitle = {Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {137–145},
numpages = {9},
location = {Dalian, China},
series = {IPEC '22}
}

@article{10.14778/2536274.2536300,
author = {Okcan, Alper and Riedewald, Mirek and Panda, Biswanath and Fink, Daniel},
title = {Scolopax: Exploratory Analysis of Scientific Data},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536274.2536300},
doi = {10.14778/2536274.2536300},
abstract = {The formulation of hypotheses based on patterns found in data is an essential component of scientific discovery. As larger and richer data sets become available, new scalable and user-friendly tools for scientific discovery through data analysis are needed. We demonstrate Scolopax, which explores the idea of a search engine for hypotheses. It has an intuitive user interface that supports sophisticated queries. Scolopax can explore a huge space of possible hypotheses, returning a ranked list of those that best match the user preferences. To scale to large and complex data sets, Scolopax relies on parallel data management and mining techniques. These include model training, efficient model summary generation, and novel parallel join techniques that together with traditional approaches such as clustering manipulate massive model-summary collections to find the most interesting hypotheses. This demonstration of Scolopax uses a real observational data set, provided by the Cornell Lab of Ornithology. It contains more than 3.3 million bird sightings reported by citizen scientists and has almost 2500 attributes. Conference attendees have the opportunity to make novel discoveries in this data set, ranging from identifying variables that strongly affect bird populations in specific regions to detecting more sophisticated patterns such as habitat competition and migration.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1298–1301},
numpages = {4}
}

@inproceedings{10.1145/2666158.2666183,
author = {Nakuc\c{c}i, Emona and Theodorou, Vasileios and Jovanovic, Petar and Abell\'{o}, Alberto},
title = {Bijoux: Data Generator for Evaluating ETL Process Quality},
year = {2014},
isbn = {9781450309998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666158.2666183},
doi = {10.1145/2666158.2666183},
abstract = {Obtaining the right set of data for evaluating the fulfillment of different quality standards in the extract-transform-load (ETL) process design is rather challenging. First, the real data might be out of reach due to different privacy constraints, while providing a synthetic set of data is known as a labor-intensive task that needs to take various combinations of process parameters into account. Additionally, having a single dataset usually does not represent the evolution of data throughout the complete process lifespan, hence missing the plethora of possible test cases. To facilitate such demanding task, in this paper we propose an automatic data generator (i.e., Bijoux). Starting from a given ETL process model, Bijoux extracts the semantics of data transformations, analyzes the constraints they imply over data, and automatically generates testing datasets. At the same time, it considers different dataset and transformation characteristics (e.g., size, distribution, selectivity, etc.) in order to cover a variety of test scenarios. We report our experimental findings showing the effectiveness and scalability of our approach.},
booktitle = {Proceedings of the 17th International Workshop on Data Warehousing and OLAP},
pages = {23–32},
numpages = {10},
keywords = {data generator, ETL, process quality},
location = {Shanghai, China},
series = {DOLAP '14}
}

@inproceedings{10.1145/3306446.3340821,
author = {Saddiqa, Mubashrah and Rasmussen, Lise and Magnussen, Rikke and Larsen, Birger and Pedersen, Jens Myrup},
title = {Bringing Open Data into Danish Schools and Its Potential Impact on School Pupils},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340821},
doi = {10.1145/3306446.3340821},
abstract = {Private and public institutions are using open and public data to provide better services, which increases the impact of open data on daily life. With the advancement of technology, it becomes also important to equip our younger generation with the essential skills for future challenges. In order to bring up a generation equipped with 21st century skills, open data could facilitate educational processes at school level as an educational resource. Open data could acts as a key resource to enhance the understanding of data through critical thinking and ethical vision among the youth and school pupils. To bring open data into schools, it is important to know the teacher's perspective on open data literacy and its possible impact on pupils. As a research contribution, we answered these questions through a Danish public school teacher's survey where we interviewed 10 Danish public school teachers of grade 5-7th and analyzed their views about the impact of open data on pupils' learning development. After analyzing Copenhagen city's open data, we identified four open data educational themes that could facilitate different subjects, e.g. geography, mathematics, basic science and social science. The survey includes interviews, open discussions, questionnaires and an experiment with the grade 7th pupils, where we test the pupils' understanding with open data. The survey concluded that open data cannot only empower pupils to understand real facts about their local areas, improve civics awareness and develop digital and data skills, but also enable them to come up with the ideas to improve their communities.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {9},
numpages = {10},
keywords = {school pupils, educational resource, educational themes, open data, impact},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@article{10.1145/3360000,
author = {Andersen, Kim Normann and Lee, Jungwoo and Henriksen, Helle Zinner},
title = {Digital Sclerosis? Wind of Change for Government and the Employees},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2691-199X},
url = {https://doi.org/10.1145/3360000},
doi = {10.1145/3360000},
abstract = {Contrasting the political ambitions on the next generation of government, the uptake of technology can lead to digital sclerosis characterized by stiffening of the governmental processes, failure to respond to changes in demand, and lowering innovation feedback from workers. In this conceptual article, we outline three early warnings of digital sclerosis: decreased bargaining and discretion power of governmental workers, enhanced agility and ability at shifting and extended proximities, and panopticonization. To respond proactively and take preventive care initiatives, policy makers and systems developers need to be sensitized about the digital sclerosis, prepare the technology, and design intelligent augmentations in a flexible and agile approach.},
journal = {Digit. Gov.: Res. Pract.},
month = {feb},
articleno = {9},
numpages = {14},
keywords = {e-Government, changing nature of work, work, digitalization, future work, digital sclerosis, workplace, public sector}
}

@article{10.1145/3423923,
author = {Tentori, Monica and Ziviani, Artur and Muchaluat-Saade, D\'{e}bora C. and Favela, Jesus},
title = {Digital Healthcare in Latin America: The Case of Brazil and Mexico},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3423923},
doi = {10.1145/3423923},
journal = {Commun. ACM},
month = {oct},
pages = {72–77},
numpages = {6}
}

@inproceedings{10.1145/3405962.3405983,
author = {Stefanidis, Dimosthenis and Christodoulou, Chrysovalantis and Symeonidis, Moysis and Pallis, George and Dikaiakos, Marios and Pouis, Loukas and Orphanou, Kalia and Lampathaki, Fenareti and Alexandrou, Dimitrios},
title = {The ICARUS Ontology: A General Aviation Ontology Developed Using a Multi-Layer Approach},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405983},
doi = {10.1145/3405962.3405983},
abstract = {The management of aviation data is a great challenge in the aviation industry, as they are complex and can be derived from heterogeneous data sources. To handle this challenge, ontologies can be applied to facilitate the modelling of the data across multiple data sources. This paper presents an aviation domain ontology, the ICARUS ontology, which aims at facilitating the semantic description and integration of information resources that represent the various assets of the ICARUS platform and their use. To present the functionality and usability of the proposed ontology, we present the results of querying the ontology using SPARQL queries through three use case scenarios. As shown from the evaluation, the ICARUS ontology enables the integration and reasoning over multiple sources of heterogeneous aviation-related data, the semantic description of metadata produced by ICARUS, and their storage in a knowledge-base which is dynamically updated and provides access to its contents via SPARQL queries.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {21–32},
numpages = {12},
keywords = {datasets, queries, services, aviation, ontology},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3421766.3421800,
author = {Wang, Deli},
title = {Research on Bank Marketing Behavior Based on Machine Learning},
year = {2020},
isbn = {9781450375535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421766.3421800},
doi = {10.1145/3421766.3421800},
abstract = {At present, under the background that data mining technology is becoming more mature and widely used in various fields, and due to the advent of the customer-oriented era and increased competition from banks, data mining technology is being widely used in the field of banking and finance to determine the target customer group And promote bank sales. Therefore, based on the Bank Marketing data in the UCI Machine Learning Repository database, this article uses the C5.0 algorithm to classify customers on the clementine experimental platform, and proposes corresponding suggestions for bank marketing based on the classification results.This article first explores and understands the Bank Marketing data set, and describes the distribution of the customer background in the data set. The quality of the data set was further explored, and the outliers and outliers were corrected by replacing them with normal data that were closest to the outliers or extreme values.This paper further selects the optimal feature variable. First, use the Filter node to filter the unimportant variables of the classification, and further select one of the more relevant variables to reduce the redundancy of the variables. The final variables are: previous, age, duration, outcome, contact, housing, job, loan, marital, education.Secondly, this paper uses sampling nodes to perform undersampling to balance the data set. On this basis, the C5.0 algorithm is used to establish a classification model and optimize parameters, and finally obtain eight classification rules. Based on this, suggestions are provided for target group determination.Finally, this article introduces the remaining four classification algorithms: C&amp;T, QUEST, CHAID, Neural Networks, and compares the C5.0 algorithm with the four classification algorithms based on the balanced data set. It is concluded that several algorithms have certain differences and the overall prediction accuracy is good.This article combines data mining theory with practical problems of banking business, and establishes a bank target customer classification model based on C5.0 algorithm. The obtained classification rules can effectively help banks to divide customer groups and take targeted measures to improve marketing efficiency.},
booktitle = {Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {150–154},
numpages = {5},
keywords = {Customer segmentation, Classification algorithm, C5.0 algorithm, Bank Direct Sales Project},
location = {Manchester, United Kingdom},
series = {AIAM2020}
}

@inproceedings{10.1145/3482632.3487461,
author = {Tang, Xinzhong and Zhuang, Bing and Yao, Ying and Dong, Xuesong},
title = {Research on High-Reliability Intelligent-Sensing Health Service Support Platform and Key Technologies Based on Biometrics and Blockchain Security Technology},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3487461},
doi = {10.1145/3482632.3487461},
abstract = {A new type of high-reliability intelligent-sensing health service support platform and its key technologies are introduced in this article. By the technologies of automatic data collection, perceptual data removal, abnormal data detection, perceptual heterogeneous data identification, it is realizable to collect, analyze and process the health data of users. In order to solve the security problem in the process of data transmission, biometric identification and blockchain are used to realize the high-reliability transmission. At the end of the paper, a high-reliability intelligent-sensing health service support platform is built. And the implementation and service support process are expounded, indicating that the platform has high practical value.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {2514–2518},
numpages = {5},
keywords = {blockchain, High-reliability, health services, deep learning, biological characteristics},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3531072.3535324,
author = {Hellerstein, Joseph M. and Parameswaran, Aditya G.},
title = {Piloting Data Engineering at Berkeley},
year = {2022},
isbn = {9781450393508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531072.3535324},
doi = {10.1145/3531072.3535324},
abstract = {In the Spring of 2021, we launched a pilot edition of a new Data Engineering course at Berkeley, targeted at our burgeoning Data Science major. We discuss aspects of the design of our first offering of the course, focusing on fluency of data models, languages and transformation tasks.},
booktitle = {1st International Workshop on Data Systems Education},
pages = {38–43},
numpages = {6},
location = {Philadelphia, PA, USA},
series = {DataEd '22}
}

@inproceedings{10.5555/3370272.3370329,
author = {Alexander, Rohan and Lyons, Kelly and Alexopoulos, Michelle and Austin, Lisa},
title = {Workshop on Barriers to Data Science Adoption: Why Existing Frameworks Aren't Working},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {Data science is an interdisciplinary scientific approach that provides methods to understand and solve problems in an evidence-based manner, using data and experience. Despite the clear benefits from adoption, many firms face challenges, be that legal, organisational, or business practices, when seeking to implement and embed data science within an existing framework. In this workshop, panel and audience members drew on their experiences to elaborate on the challenges encountered when attempting to deploying data science within existing frameworks. Panel and audience members were drawn from business, academia, and think-tanks. For discussion purposes the challenges were grouped within three themes: regulatory; investment; and workforce.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {384–385},
numpages = {2},
keywords = {data science adoption, challenges, legal, organisational, business practices},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@inproceedings{10.1145/3482632.3484010,
author = {Yang, Rui},
title = {Statistics and Mining Analysis of Lightning Monitoring Data in Power Grid Based on Classical Metrology Model},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484010},
doi = {10.1145/3482632.3484010},
abstract = {Lightning, also known as lightning, is a strong catastrophic discharge phenomenon between clouds and between clouds and the ground in the process of atmospheric convection. There are two types: cloud flash (between clouds) and ground flash (between clouds and the earth). Lightning location system is a system that uses telemetry technology to monitor lightning activities in full-automatic, large-area, high-precision, continuous and real-time. By analyzing lightning location data collected for a long time, lightning accident points can be quickly located, the distribution of regional lightning activities can be counted, the development trend can be analyzed, and early warning can be carried out, which can provide reference for lightning protection research of ground buildings, thus reducing the harm to human activities. In this paper, the grid method is used to store and query lightning data based on the classical measurement model. Taking the lightning protection technology of transmission network as an example, the method and application of statistics and mining of lightning monitoring data in power grid are studied.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1649–1653},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3401025.3406443,
author = {Baban, Philsy},
title = {Pre-Processing and Data Validation in IoT Data Streams},
year = {2020},
isbn = {9781450380287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401025.3406443},
doi = {10.1145/3401025.3406443},
abstract = {In the last few years, distributed stream processing engines have been on the rise due to their crucial impacts on real-time data processing with guaranteed low latency in several application domains such as financial markets, surveillance systems, manufacturing, smart cities, etc. Stream processing engines are run-time libraries to process data streams without knowing the lower level streaming mechanics. Apache Storm, Apache Flink, Apache Spark, Kafka Streams and Hazelcast Jet are some of the popular stream processing engines. Nowadays, critical systems like energy systems, are interconnected and automated. As a result, these systems are vulnerable to cyber-attacks. In real-world applications, the sensing values come from sensor devices contains missing values, redundant data, data outliers, manipulated data, data failures, etc. Therefore, our system must be resilient to these conditions. In this paper, we present an approach to check if there is any above mentioned conditions by pre-processing data streams using a stream processing engine like Apache Flink which will be updated as a library in future. Then, the pre-processed streams are forwarded to other stream processing engines like Apache Kafka for real stream processing. As a result, data validation, data consistency and integrity for a resilient system can be accomplished before initiating the actual stream processing.},
booktitle = {Proceedings of the 14th ACM International Conference on Distributed and Event-Based Systems},
pages = {226–229},
numpages = {4},
keywords = {stream processing, resiliency, data validation, data pre-processing},
location = {Montreal, Quebec, Canada},
series = {DEBS '20}
}

@inproceedings{10.1145/3078564.3078572,
author = {Song, Zekun and Zhang, Lvyang and Liu, Tao and Chen, Ying},
title = {Ranking Learning Algorithm of Information Retrieval Based on WeChat Public Numbers},
year = {2017},
isbn = {9781450352109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078564.3078572},
doi = {10.1145/3078564.3078572},
abstract = {On the basis of obtaining the data of mass WeChat public1, in order to improve the operational efficiency and quality of WeChat public number. On the basis of the retrieval technology, the quality evaluation model of WeChat public number was established. A sort learning algorithm based on model retrieval is proposed. Use the vector space technology based on the weight of the entry position to retrieve the contents of WeChat public number, and then use the WeChat public number quality evaluation model to sort. The retrieved articles sorted data to recommend to the operator, so that the operator can be faster and more efficient to find their hope to find high quality WeChat number of public articles.},
booktitle = {Proceedings of the 6th International Conference on Information Engineering},
articleno = {4},
numpages = {5},
keywords = {WeChatpublic number, Meta data model, Recommendation system, Rank learning algorithm},
location = {Dalian Liaoning, China},
series = {ICIE '17}
}

@inproceedings{10.1145/3197026.3200209,
author = {Klein, Martin and Xie, Zhiwu and Fox, Edward A.},
title = {Web Archiving and Digital Libraries (WADL)},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3200209},
doi = {10.1145/3197026.3200209},
abstract = {The 2018 edition of the Workshop on Web Archiving and Digital Libraries (WADL) will explore the integration of Web archiving and digital libraries. The workshop aims at addressing aspects covering the entire life cycle of digital resources and will also explore areas such as community building and ethical questions around web archiving.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {425–426},
numpages = {2},
keywords = {web archiving, digital preservation, community building},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@inproceedings{10.1145/3132218.3132241,
author = {Beek, Wouter and Fern\'{a}ndez, Javier D. and Verborgh, Ruben},
title = {LOD-a-Lot: A Single-File Enabler for Data Science},
year = {2017},
isbn = {9781450352963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132218.3132241},
doi = {10.1145/3132218.3132241},
abstract = {Many data scientists make use of Linked Open Data (LOD) as a huge interconnected knowledge base represented in RDF. However, the distributed nature of the information and the lack of a scalable approach to manage and consume such Big Semantic Data makes it difficult and expensive to conduct large-scale studies. As a consequence, most scientists restrict their analyses to one or two datasets (often DBpedia) that contain at most hundreds of millions of triples. LOD-a-lot is a dataset that integrates a large portion (over 28 billion triples) of the LOD Cloud into a single ready-to-consume file that can be easily downloaded, shared and queried with a small memory footprint. This paper shows there exists a wide collection of Data Science use cases that can be performed over such a LOD-a-lot file. For these use cases LOD-a-lot significantly reduces the cost and complexity of conducting Data Science.},
booktitle = {Proceedings of the 13th International Conference on Semantic Systems},
pages = {181–184},
numpages = {4},
location = {Amsterdam, Netherlands},
series = {Semantics2017}
}

@inproceedings{10.1145/3047273.3047327,
author = {Zeleti, Fatemeh Ahmadi and Ojo, Adegboyega},
title = {An Ontology for Open Government Data Business Model},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047327},
doi = {10.1145/3047273.3047327},
abstract = {Despite the existence of number of well-known conceptualization in e-Business and e-Commerce, there have been no efforts so far to develop a detailed, comprehensive conceptualization for business model. Current business literature is replete with fragmented conceptualizations, which only partially describe aspects of a business model. In addition, the existing conceptualizations do not explicitly support the emerging phenomenon of open government data -- an increasingly valuable economic and strategic resource. Consequently, no comprehensive, formal, executable open government data business model ontology exists, that could be directly leveraged to facilitate the design, development of an operational open data business model. This paper bridges this gap by providing a parsimonious yet sufficiently detailed, conceptualization and formal ontology of open government data business model for open data-driven organizations. Following the design science approach, we developed the ontology as a 'design artefact' and validate the ontology by using it to describe an open data business model of an open data-driven organization.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {195–203},
numpages = {9},
keywords = {e-Commerce ontology, Open government data, and business model ontology, formal conceptualization, open data-driven organization, e-Business ontology, open data business model},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@article{10.1145/2826686.2826692,
author = {Resch, Bernd and Blaschke, Thomas},
title = {Fusing Human and Technical Sensor Data: Concepts and Challenges},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/2826686.2826692},
doi = {10.1145/2826686.2826692},
abstract = {As geo-sensor webs have not grown as quickly as expected, new, alternative data sources have to be found for near real-time analysis in areas like emergency management, environmental monitoring, public health, or urban planning. This paper assesses the ability of human sensors, i.e., user-generated observations in a wide range of social networks, the mobile phone network, or micro-blogs, to complement geo-sensor networks. We clearly delineate the concepts of People as Sensors, Collective Sensing and Citizen Science. Furthermore, we point out current challenges in fusing data from technical and human sensors, and sketch future research areas in this field.},
journal = {SIGSPATIAL Special},
month = {sep},
pages = {29–35},
numpages = {7}
}

@article{10.1145/3376915,
author = {De Aguiar, Erikson J\'{u}lio and Fai\c{c}al, Bruno S. and Krishnamachari, Bhaskar and Ueyama, J\'{o}},
title = {A Survey of Blockchain-Based Strategies for Healthcare},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3376915},
doi = {10.1145/3376915},
abstract = {Blockchain technology has been gaining visibility owing to its ability to enhance the security, reliability, and robustness of distributed systems. Several areas have benefited from research based on this technology, such as finance, remote sensing, data analysis, and healthcare. Data immutability, privacy, transparency, decentralization, and distributed ledgers are the main features that make blockchain an attractive technology. However, healthcare records that contain confidential patient data make this system very complicated because there is a risk of a privacy breach. This study aims to address research into the applications of the blockchain healthcare area. It sets out by discussing the management of medical information, as well as the sharing of medical records, image sharing, and log management. We also discuss papers that intersect with other areas, such as the Internet of Things, the management of information, tracking of drugs along their supply chain, and aspects of security and privacy. As we are aware that there are other surveys of blockchain in healthcare, we analyze and compare both the positive and negative aspects of their papers. Finally, we seek to examine the concepts of blockchain in the medical area, by assessing their benefits and drawbacks and thus giving guidance to other researchers in the area. Additionally, we summarize the methods used in healthcare per application area and show their pros and cons.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {27},
numpages = {27},
keywords = {distributed ledger technology, healthcare, medical, Distributed systems, survey, blockchain}
}

@inproceedings{10.1145/3341162.3347758,
author = {Manea, Vlad and Berrocal, Allan and De Masi, Alexandre and M\o{}ller, Naja Holten and Wac, Katarzyna and Bayer, Hannah and Lehmann, Sune and Ashley, Euan},
title = {LDC '19: International Workshop on Longitudinal Data Collection in Human Subject Studies},
year = {2019},
isbn = {9781450368698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341162.3347758},
doi = {10.1145/3341162.3347758},
abstract = {Individuals increasingly use mobile, wearable, and ubiquitous devices capable of unobtrusive collection of vast amounts of scientifically rich personal data over long periods (months to years), and in the context of their daily life. However, numerous human and technological factors challenge longitudinal data collection, often limiting research studies to very short data collection periods (days to weeks), spawning recruitment biases, and affecting participant retention over time. This workshop is designed to bring together researchers involved in longitudinal data collection studies to foster an insightful exchange of ideas, experiences, and discoveries to improve the studies' reliability, validity, and perceived meaning of longitudinal mobile, wearable, and ubiquitous data collection for the participants.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {878–881},
numpages = {4},
keywords = {panel technique, mobile devices, longitudinal studies, human subject studies, in situ, human sensing},
location = {London, United Kingdom},
series = {UbiComp/ISWC '19 Adjunct}
}

@inproceedings{10.1145/2910896.2926734,
author = {Cabanac, Guillaume and Chandrasekaran, Muthu Kumar and Frommholz, Ingo and Jaidka, Kokil and Kan, Min-Yen and Mayr, Philipp and Wolfram, Dietmar},
title = {Joint Workshop on Bibliometric-Enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016)},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2926734},
doi = {10.1145/2910896.2926734},
abstract = {The large scale of scholarly publications poses a challenge for scholars in information-seeking and sensemaking. Bibliometric, information retrieval~(IR), text mining and NLP techniques could help in these activities, but are not yet widely used in digital libraries. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric and recommendation techniques which can advance the state-of-the-art in scholarly document understanding, analysis and retrieval at scale.},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {299–300},
numpages = {2},
keywords = {bibliometrics, digital libraries, information retrieval, natural language processing, text mining},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.1145/3527049.3527050,
author = {Kochinev, Yury and Antysheva, Elena and Alpysbayev, Kaisar},
title = {A System of Financial Indicators for Assessing the Risk of Material Misstatement of Accounting Information in the Context of a Continuous Online Audit},
year = {2022},
isbn = {9781450386944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527049.3527050},
doi = {10.1145/3527049.3527050},
abstract = {In the time of digitalization of the economy, the audit of financial reporting is bound to shift from periodic inspections (annual, semi-annual, quarterly) to continuous monitoring of financial information. Continuous monitoring ensures that the control results of activities are produced simultaneously or within the shortest period possible after discovering the relevant events. The risk of possible misstatement of financial statements in the process of this monitoring is assessed using analytical procedures. The source data for them are the values of financial indicators of the audited entity selected by the auditor. Analytical procedures are aimed at obtaining audit evidence and carried out through a number of actions taken to find out, study, analyze and assess the correlations between the financial-economic and other performance indicators of the organization in order to discover non-standard phenomena as well as facts and the causes of these discrepancies. The main purpose of the article was to develop a system of financial indicators, the continuous monitoring of which would allow for assessing the risk of material misstatement in real time. The methodological basis of the research is determined by the following fundamental techniques and principles: analysis, synthesis, attributive and proportional analogies, abstractions, descriptive generalizations, formulation and confirmation of working analytical hypotheses, economic/mathematical and mathematical/statistical methods. The paper analyzes the financial indicators, suggested for these purposes by a number of authors, and highlights the lack of substantiation of the former. The relationships between possible financial indicators and the items of financial statements are studied and a set of indicators has been formed. Their growth rate, if monitored in the online auditing process, makes it possible to assess the risk of material misstatement of accounting information and react accordingly.},
booktitle = {Proceedings of the 3rd International Scientific Conference on Innovations in Digital Economy},
pages = {402–407},
numpages = {6},
keywords = {Risk of material misstatement, Continuous online auditing, Financial indicators, Analytical procedures, Digitalization of the economy},
location = {Saint - Petersburg, Russian Federation},
series = {SPBPU IDE '21}
}

@article{10.1007/s00778-017-0486-1,
author = {Herschel, Melanie and Diestelk\"{a}mper, Ralf and Ben Lahmar, Houssem},
title = {A Survey on Provenance: What for? What Form? What From?},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0486-1},
doi = {10.1007/s00778-017-0486-1},
abstract = {Provenance refers to any information describing the production process of an end product, which can be anything from a piece of digital data to a physical object. While this survey focuses on the former type of end product, this definition still leaves room for many different interpretations of and approaches to provenance. These are typically motivated by different application domains for provenance (e.g., accountability, reproducibility, process debugging) and varying technical requirements such as runtime, scalability, or privacy. As a result, we observe a wide variety of provenance types and provenance-generating methods. This survey provides an overview of the research field of provenance, focusing on what provenance is used for (what for?), what types of provenance have been defined and captured for the different applications (what form?), and which resources and system requirements impact the choice of deploying a particular provenance solution (what from?). For each of these three key questions, we provide a classification and review the state of the art for each class. We conclude with a summary and possible future research challenges.},
journal = {The VLDB Journal},
month = {dec},
pages = {881–906},
numpages = {26},
keywords = {Provenance applications, Data provenance, Survey, Provenance types, Provenance capture, Provenance requirements, Workflow provenance}
}

@inproceedings{10.1145/3373477.3373499,
author = {Tian, Bing and Lv, Shuqing and Yin, Qilin and Li, Ning and Zhang, Yue and Liu, Ziyan},
title = {Real-Time Dynamic Data Desensitization Method Based on Data Stream},
year = {2020},
isbn = {9781450372916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373477.3373499},
doi = {10.1145/3373477.3373499},
abstract = {With the rapid development of the data mining industry, the value hidden in the massive data has been discovered, but at the same time it has also raised concerns about privacy leakage, leakage of sensitive data and other issues. These problems have also become numerous studies. Among the methods for solving these problems, data desensitization technology has been widely adopted for its outstanding performance. However, with the increasing scale of data and the increasing dimension of data, the traditional desensitization method for static data can no longer meet the requirements of various industries in today's environment to protect sensitive data. In the face of ever-changing data sets of scale and dimension, static desensitization technology relies on artificially designated desensitization rules to grasp the massive data, and it is difficult to control the loss of data connotation. In response to these problems, this paper proposes a real-time dynamic desensitization method based on data flow, and combines the data anonymization mechanism to optimize the data desensitization strategy. Experiments show that this method can efficiently and stably perform real-time desensitization of stream data, and can save more information to support data mining in the next steps.},
booktitle = {Proceedings of the International Conference on Advanced Information Science and System},
articleno = {22},
numpages = {6},
keywords = {data desensitization, stream data, dynamic desensitization},
location = {Singapore, Singapore},
series = {AISS '19}
}

@inbook{10.1145/3310205.3310206,
title = {Preface},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310206},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/2987491.2987521,
author = {de Jager, Tiaan and Brown, Irwin},
title = {A Descriptive Categorized Typology of Requisite Skills for Business Intelligence Professionals},
year = {2016},
isbn = {9781450348058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987491.2987521},
doi = {10.1145/2987491.2987521},
abstract = {Business Intelligence (BI) is regarded by executives as a critical practice to adopt and invest in. The purpose of this research is to develop a categorized typology of skills required by BI professionals. A review of extant literature resulted in the identification of twenty three skills. The research aimed to validate these skills, and add additional skills to this typology based on the experiences of BI professionals within industry. These experiences were captured through interviews. Skills were then categorized by identifying commonalities across them. No additional skills were identified by the interviewed participants. A categorized typology of skills was developed which grouped the initial twenty three skills into seven higher order categories. The seven categories of skills were identified as: (1) Prepare data for subject matter expert (SME), analyst or other external party for further analysis; (2) Apply simulation modelling, statistical techniques and provide business insight; (3) Manage stakeholders and project and operational tasks; (4) Develop strategic long term BI roadmap that links to corporate strategy; (5) Understand business processes in order to effectively extract user requirements; (6) Design and code sustainable solutions; (7) Absorb and distribute knowledge.},
booktitle = {Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists},
articleno = {14},
numpages = {10},
keywords = {IS Profession, Analytics, IT Skills, Business Intelligence, Typology},
location = {Johannesburg, South Africa},
series = {SAICSIT '16}
}

@inproceedings{10.1145/3339252.3342112,
author = {Schaberreiter, Thomas and Kupfersberger, Veronika and Rantos, Konstantinos and Spyros, Arnolnt and Papanikolaou, Alexandros and Ilioudis, Christos and Quirchmayr, Gerald},
title = {A Quantitative Evaluation of Trust in the Quality of Cyber Threat Intelligence Sources},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3342112},
doi = {10.1145/3339252.3342112},
abstract = {Threat intelligence sharing has become a cornerstone of cooperative and collaborative cybersecurity. Sources providing such data have become more widespread in recent years, ranging from public entities (driven by legislatorial changes) to commercial companies and open communities that provide threat intelligence in order to help organisations and individuals to better understand and assess the cyber threat landscape putting their systems at risk. Tool support to automatically process this information is emerging concurrently. It has been observed that the quality of information received by the sources varies significantly and that in order to assess the quality of a threat intelligence source it is not sufficient to only consider qualitative indications of the source itself, but it is necessary to monitor the data provided by the source continuously to be able to draw conclusions about the quality of information provided by a source. In this paper, we propose a methodology for evaluating cyber threat information sources based on quantitative parameters. The methodology aims to facilitate trust establishment to threat intelligence sources, based on a weighted evaluation method that allows each entity to adapt it to its own needs and priorities. The approach facilitates automated tools utilising threat intelligence, since information to be considered can be prioritised based on which source is trusted the most at the time the intelligence arrives.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {83},
numpages = {10},
keywords = {quality parameters, trust indicators, Cooperative and collaborative cybersecurity, cyber threat intelligence source evaluation, cyber threat information sharing},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inproceedings{10.1145/3236024.3236056,
author = {Wang, Ying and Wen, Ming and Liu, Zhenwei and Wu, Rongxin and Wang, Rui and Yang, Bo and Yu, Hai and Zhu, Zhiliang and Cheung, Shing-Chi},
title = {Do the Dependency Conflicts in My Project Matter?},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236056},
doi = {10.1145/3236024.3236056},
abstract = {Intensive dependencies of a Java project on third-party libraries can easily lead to the presence of multiple library or class versions on its classpath. When this happens, JVM will load one version and shadows the others. Dependency conflict (DC) issues occur when the loaded version fails to cover a required feature (e.g., method) referenced by the project, thus causing runtime exceptions. However, the warnings of duplicate classes or libraries detected by existing build tools such as Maven can be benign since not all instances of duplication will induce runtime exceptions, and hence are often ignored by developers. In this paper, we conducted an empirical study on real-world DC issues collected from large open source projects. We studied the manifestation and fixing patterns of DC issues. Based on our findings, we designed Decca, an automated detection tool that assesses DC issues' severity and filters out the benign ones. Our evaluation results on 30 projects show that Decca achieves a precision of 0.923 and recall of 0.766 in detecting high-severity DC issues. Decca also detected new DC issues in these projects. Subsequently, 20 DC bug reports were filed, and 11 of them were confirmed by developers. Issues in 6 reports were fixed with our suggested patches.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {319–330},
numpages = {12},
keywords = {static analysis, Empirical study, third party library},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1007/s00778-019-00588-3,
author = {Qin, Xuedi and Luo, Yuyu and Tang, Nan and Li, Guoliang},
title = {Making Data Visualization More Efficient and Effective: A Survey},
year = {2019},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-019-00588-3},
doi = {10.1007/s00778-019-00588-3},
abstract = {Data visualization is crucial in today’s data-driven business world, which has been widely used for helping decision making that is closely related to major revenues of many industrial companies. However, due to the high demand of data processing w.r.t. the volume, velocity, and veracity of data, there is an emerging need for database experts to help for efficient and effective data visualization. In response to this demand, this article surveys techniques that make data visualization more efficient and effective. (1) Visualization specifications define how the users can specify their requirements for generating visualizations. (2) Efficient approaches for data visualization process the data and a given visualization specification, which then produce visualizations with the primary target to be efficient and scalable at an interactive speed. (3) Data visualization recommendation is to auto-complete an incomplete specification, or to discover more interesting visualizations based on a reference visualization.},
journal = {The VLDB Journal},
month = {nov},
pages = {93–117},
numpages = {25},
keywords = {Data visualization recommendation, Efficient data visualization, Data visualization, Visualization languages}
}

@inproceedings{10.1145/3532213.3532252,
author = {Ming Tang, Chun and Tao, Peng and Li, Yan},
title = {Design and Generalization of Enterprise Knowledge Graph},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532252},
doi = {10.1145/3532213.3532252},
abstract = {Knowledge Graph is widely used in artificial intelligence fields such as intelligent search, intelligent recommendation and intelligent question answering, and EKG (Enterprise Knowledge Graph) is an important foundation for enterprises to build intelligent platforms. This paper proposes the design idea of constructing EKG based on the five dimensions of human, financial, material, time and information. First, the EKG framework is built by applying component microservices, pre-construction and business orchestration; Mining and analyzing prediction methods, and finally designing an intelligent visual EKG with auxiliary decision-making functions.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {259–264},
numpages = {6},
keywords = {knowledge generalization, Enterprise knowledge graph, intelligent visualization, component microservice},
location = {Tianjin, China},
series = {ICCAI '22}
}

@article{10.1145/3448888,
author = {Koesten, Laura and Simperl, Elena},
title = {UX of Data: Making Data Available Doesn't Make It Usable},
year = {2021},
issue_date = {March - April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1072-5520},
url = {https://doi.org/10.1145/3448888},
doi = {10.1145/3448888},
abstract = {This forum provides a space to engage with the challenges of designing for intelligent algorithmic experiences. We invite articles that tackle the tensions between research and practice when integrating AI and UX design. We welcome interdisciplinary debate, artful critique, forward-looking research, case studies of AI in practice, and speculative design explorations. --- Juho Kim and Henriette Cramer, Editors},
journal = {Interactions},
month = {mar},
pages = {97–99},
numpages = {3}
}

@article{10.1145/3292384.3292389,
author = {Al-Jaroodi, Jameela and Mohamed, Nader and Jawhar, Imad},
title = {A Service-Oriented Middleware Framework for Manufacturing Industry 4.0},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
url = {https://doi.org/10.1145/3292384.3292389},
doi = {10.1145/3292384.3292389},
abstract = {The advantages of the Internet of things (IoT) initiated the vision of Industry 4.0 in Europe and smart manufacturing in USA. Both visions aim to implement the smart factory to achieve similar objectives by utilizing new technologies. These technologies include cloud computing, fog computing, cyber-physical systems (CPS), and data analytics. Together they help automate and autonomize the manufacturing processes and controls to optimize the productivity, reliability, quality, cost-effeteness, and safety of these processes. While both visions are promising, developing and operating Industry 4.0 applications are extremely challenging. This is due to the complexity of the manufacturing processes as well as their management, controls, and integration dynamics. This paper introduces Man4Ware, a service-oriented middleware for Industry 4.0. Man4Ware can help facilitate the development and operations of cloud and fog-integrated smart manufacturing applications. Man4Ware offers many advantages through service level interfaces to enable easy utilization of new technologies and integration of different services to relax many of the challenges facing the development and operations of such applications1.},
journal = {SIGBED Rev.},
month = {nov},
pages = {29–36},
numpages = {8},
keywords = {cyber-physical systems, middleware, IoT, fog computing, cloud computing, smart manufacturing, industry 4.0}
}

@article{10.1145/3543508,
author = {Opdahl, Andreas L. and Al-Moslmi, Tareq and Dang-Nguyen, Duc-Tien and Gallofr\'{e} Oca\~{n}a, Marc and Tessem, Bj\o{}rnar and Veres, Csaba},
title = {Semantic Knowledge Graphs for the News: A Review},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3543508},
doi = {10.1145/3543508},
abstract = {ICT platforms for news production, distribution, and consumption must exploit the ever-growing availability of digital data. These data originate from different sources and in different formats; they arrive at different velocities and in different volumes. Semantic knowledge graphs (KGs) is an established technique for integrating such heterogeneous information. It is therefore well-aligned with the needs of news producers and distributors, and it is likely to become increasingly important for the news industry. This paper reviews the research on using semantic knowledge graphs for production, distribution, and consumption of news. The purpose is to present an overview of the field; to investigate what it means; and to suggest opportunities and needs for further research and development.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {jun},
keywords = {Semantic Web, Literature Review, Ontology, Linked Open Data, News Consumption, Semantic Technologies, News Production, Knowledge Graphs, News, News Distribution, Journalism, Linked Data}
}

@inproceedings{10.1145/3384544.3384596,
author = {Valachamy, Mageshwari and Sahibuddin, Shamsul and Ahmad, Noor Azurati and Bakar, Nur Azaliah Abu},
title = {Geospatial Data Sharing: Preliminary Studies on Issues and Challenges in Natural Disaster Management},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384596},
doi = {10.1145/3384544.3384596},
abstract = {The rapid development of information technology has led to the demand for the latest, precise and easy to understand data. Data especially geospatial data is becoming increasingly crucial in all types of planning and decision making. Geospatial data sharing can be categorized into different disciplines such as public safety, disaster management, transportation, traffic control, tracking, health, environment, natural resources, mining, agriculture, utilities and many more. Whether as a way of distribution or retrieval of data, geospatial data has become an essential component of government GIS operations. Despite the prominence of this activity and its centrality to the day-to-day function of many government systems, the geospatial data sharing is still given less attention in the field of natural disaster management. Preliminary information is gathered from Literature Reviews (LR) and unstructured interviews with experts to seek information in depth. Thirteen (13) issues and challenges of geospatial data sharing in Malaysia Public Sector (MPS) for natural disaster management have been identified.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {51–56},
numpages = {6},
keywords = {Spatial Data Infrastructure, Issues and Challenges, Geospatial data, Spatial data, Sharing},
location = {Langkawi, Malaysia},
series = {ICSCA 2020}
}

@inproceedings{10.5555/2814058.3252433,
author = {Siqueira, Sean W. M. and Carvalho, Sergio T.},
title = {Session Details: Main Track - Management, Governance, and Government},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
location = {Goiania, Goias, Brazil},
series = {SBSI 2015}
}

@article{10.1145/1978542.1978562,
author = {Chaudhuri, Surajit and Dayal, Umeshwar and Narasayya, Vivek},
title = {An Overview of Business Intelligence Technology},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/1978542.1978562},
doi = {10.1145/1978542.1978562},
abstract = {BI technologies are essential to running today's businesses and this technology is going through sea changes.},
journal = {Commun. ACM},
month = {aug},
pages = {88–98},
numpages = {11}
}

@inproceedings{10.1145/3384772.3385138,
author = {H. Gyldenkaerne, Christopher and From, Gustav and M\o{}nsted, Troels and Simonsen, Jesper},
title = {PD and The Challenge of AI in Health-Care},
year = {2020},
isbn = {9781450376068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384772.3385138},
doi = {10.1145/3384772.3385138},
abstract = {In its promise to contribute to considerable cost savings and improved patient care through efficient analysis of the tremendous amount of data stored in electronic health records (EHR), there is currently a strong push for the proliferation of artificial intelligence (AI) in health-care. We identify, through a study of AI being used to predict patient no-show’s, that for the AI to gain full potential there lies a need to balance the introduction of AI with a proper focus on the patients and the clinicians’ interests. We call for a Participatory Design (PD) approach to understand and reconfigure the socio-technical setup in health-care, especially where AI is being used on EHR data that are manually being submitted by health-care personnel.},
booktitle = {Proceedings of the 16th Participatory Design Conference 2020 - Participation(s) Otherwise - Volume 2},
pages = {26–29},
numpages = {4},
keywords = {Primary- and Secondary Use data, precision medicine, Electronic Health Record data, Participatory Design, Artificial Intelligence},
location = {Manizales, Colombia},
series = {PDC '20}
}

@inbook{10.1145/3310205.3310215,
title = {References},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310215},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3297280.3297477,
author = {Maamar, Zakaria and Baker, Thar and Faci, Noura and Ugljanin, Emir and Khafajiy, Mohammed Al and Bur\'{e}gio, Vanilson},
title = {Towards a Seamless Coordination of Cloud and Fog: Illustration through the Internet-of-Things},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297477},
doi = {10.1145/3297280.3297477},
abstract = {With the increasing popularity of the Internet-of-Things (IoT), organizations are revisiting their practices as well as adopting new ones so they can deal with an ever-growing amount of sensed and actuated data that IoT-compliant things generate. Some of these practices are about the use of cloud and/or fog computing. The former promotes "anything-as-a-service" and the latter promotes "process data next to where it is located". Generally presented as competing models, this paper discusses how cloud and fog could work hand-in-hand through a seamless coordination of their respective "duties". This coordination stresses out the importance of defining where the data of things should be sent (either cloud, fog, or cloud&amp;fog concurrently) and in what order (either cloud then fog, fog then cloud, or fog&amp;cloud concurrently). Applications' concerns with data such as latency, sensitivity, and freshness dictate both the appropriate recipients and the appropriate orders. For validation purposes, a healthcare-driven IoT application along with an in-house testbed, that features real sensors and fog and cloud platforms, have permitted to carry out different experiments that demonstrate the technical feasibility of the coordination model.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2008–2015},
numpages = {8},
keywords = {healthcare, fog, internet-of-things, coordination, cloud},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1145/3209581,
author = {Baeza-Yates, Ricardo},
title = {Bias on the Web},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3209581},
doi = {10.1145/3209581},
abstract = {Bias in Web data and use taints the algorithms behind Web-based applications, delivering equally biased results.},
journal = {Commun. ACM},
month = {may},
pages = {54–61},
numpages = {8}
}

@inproceedings{10.1145/3371425.3371459,
author = {Tan, Wenan and Jiang, Zihui},
title = {A Novel Experience-Based Incentive Mechanism for Mobile Crowdsensing System},
year = {2019},
isbn = {9781450376334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371425.3371459},
doi = {10.1145/3371425.3371459},
abstract = {While sensor networks have been pervasively deployed in the real world, more and more mobile crowdsensing (MCS) applications have come into realization to collaboratively detect events and collect data. This paper aims to design a novel incentive mechanism to achieve good services for mobile crowdsensing applications. Responding to insufficient participants, we propose a novel Experience-Based incentive mechanism using Reverse Auction (EBRA). Additionally, it can also guarantee fair competition while maximizing the total profit of the service platform. Through strictly proving, our proposed EBRA incentive mechanism satisfies four properties: computational efficiency, individual rationality, profitability, and truthfulness. The extensive simulations show that the proposed EBRA method has a better performance over 20% than other benchmark mechanisms.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing},
articleno = {70},
numpages = {6},
keywords = {incentive mechanism, fairness competition, mobile crowdsensing, sensor network},
location = {Sanya, China},
series = {AIIPCC '19}
}

@inproceedings{10.1145/3472163.3472173,
author = {Zouari, Firas and Kabachi, Nadia and Boukadi, Khouloud and Ghedira Guegan, Chirine},
title = {Data Management in the Data Lake: A Systematic Mapping},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472173},
doi = {10.1145/3472163.3472173},
abstract = {The computer science community is paying more and more attention to data due to its crucial role in performing analysis and prediction. Researchers have proposed many data containers such as files, databases, data warehouses, cloud systems, and recently data lakes in the last decade. The latter enables holding data in its native format, making it suitable for performing massive data prediction, particularly for real-time application development. Although data lake is well adopted in the computer science industry, its acceptance by the research community is still in its infancy stage. This paper sheds light on existing works for performing analysis and predictions on data placed in data lakes. Our study reveals the necessary data management steps, which need to be followed in a decision process, and the requirements to be respected, namely curation, quality evaluation, privacy-preservation, and prediction. This study aims to categorize and analyze proposals related to each step mentioned above.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {280–284},
numpages = {5},
keywords = {Systematic mapping, Data lake, Data management},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@article{10.1145/2935752,
author = {Morstatter, Fred and Liu, Huan},
title = {Replacing Mechanical Turkers? Challenges in the Evaluation of Models with Semantic Properties},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2935752},
doi = {10.1145/2935752},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {15},
numpages = {4},
keywords = {automation, evaluation, crowdsourcing, Artificial intelligence, data mining}
}

@inproceedings{10.1145/3360322.3360836,
author = {Baasch, Gaby and Wicikowski, Adam and Faure, Ga\"{e}lle and Evins, Ralph},
title = {Comparing Gray Box Methods to Derive Building Properties from Smart Thermostat Data},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360836},
doi = {10.1145/3360322.3360836},
abstract = {The development of quantitative techniques for determining the amount of heat lost through the building envelope is essential for targeted retrofits. This type of evaluation is traditionally a resource intensive process that involves onsite appraisal and in-situ measurements. In order to build more efficient and scalable methods for retrofit analysis, new sources of data could be used. Smart thermostat data, for example, provide a valuable resource, however they often lack detailed information about the building characteristics and energy loads. This paper presents and compares three methods for assessing heating characteristics of households using a dataset that does not contain heating power. The three methods are based on: (1) balance point plots, (2) the extraction of indoor temperature decay curves, and (3) the classic differential equation for indoor temperature. These methods all take a gray box approach in which physics-based and machine learning models are combined. The dataset used for this study consists of over 4,000 houses in Ontario and New York. The three methods are applied to each building and the resulting data is analyzed to determine whether the results are statistically sound. It is found that there is a positive linear correlation between characteristics derived for each method, although there is uncertainty about absolute values. This result indicates that the methods can be used to ascertain relative values for the thermal characteristics of a building. The methods suggested in this paper may therefore be used to filter heating profiles to target potential retrofit measures or other stock-level decisions.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {223–232},
numpages = {10},
keywords = {gray box models, Buildings, thermal characteristics, smart thermostats},
location = {New York, NY, USA},
series = {BuildSys '19}
}

@inproceedings{10.1145/3277593.3277642,
author = {Truong, Hong-Linh},
title = {Dynamic IoT Data, Protocol, and Middleware Interoperability with Resource Slice Concepts and Tools: Tutorial},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277642},
doi = {10.1145/3277593.3277642},
abstract = {Dealing with interoperability in the IoT domain is a complex matter that requires various techniques for tackling data, protocol and middleware interoperability. We cannot solve IoT interoperability problems by just developing (new) software components and (semantic) data models. In this tutorial, we will present interoperability techniques for complex IoT Cloud applications by leveraging dynamic solutions of provisioning and reconfiguring of IoT data processing pipelines, protocol bridges, IoT middleware and cloud services. First, the tutorial will examine cross-layered, cross-system inter-operability issues and present a DevOps IoT Interoperability approach for defining metadata, selecting resources and software artifacts, and provisioning and connecting resources to create various potential solutions for IoT Cloud interoperability using resource slice concepts. Second, the tutorial will present techniques for dynamically provisioning data pipelines, middleware services, protocol adapters and custom solutions to address cross-layered, cross-system interoperability for IoT Cloud applications. Such solutions also allow dynamic reconfiguration of resources to add/remove interoperability support. We will present the concepts and techniques with hands-on examples using our research tools rsiHub and IoTCloudSamples.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {48},
numpages = {4},
keywords = {resource slice, cloud computing, IoT interoperability},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/2661829.2663539,
author = {Alonso, Omar and Kamps, Jaap and Karlgren, Jussi},
title = {Seventh Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'14): CIKM 2014 Workshop},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2663539},
doi = {10.1145/2661829.2663539},
abstract = {There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. The goal of the ESAIR'14 workshop remains to advance the general research agenda on this core problem, with an explicit focus on one of the most challenging aspects to address in the coming years. The main remaining challenge is on the user's side - the potential of rich document annotations can only be realized if matched by more articulate queries exploiting these powerful retrieval cues - and a more dynamic approach is emerging by exploiting new forms of query autosuggest. How can the query suggestion paradigm be used to encourage searcher to articulate longer queries, with concepts and relations linking their statement of request to existing semantic models? How do entity results and social network data in "graph search" change the classic division between searchers and information and lead to extreme personalization - are you the query? How to leverage transaction logs and recommendation, and how adaptive should we make the system? What are the privacy ramifications and the UX aspects - how to not creep out users?},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {2094–2095},
numpages = {2},
keywords = {graph search, query suggest, semantic annotation},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/3487553.3524214,
author = {Gao, Liming and Liao, Dongliang and Li, Gongfu and Xu, Jin and Zhuo, Hankz Hankui},
title = {Semantic IR Fused Heterogeneous Graph Model in Tag-Based Video Search},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524214},
doi = {10.1145/3487553.3524214},
abstract = {With the rapid growth of video resources on the Internet, text-video retrieval has become a common requirement. Scholars handled text-video retrieval tasks with two-broad-category: concept-based methods and neural semantics match networks. Besides deep neural semantics matching models, some scholars mined queries and videos relationships from click-graphs, which express the users’ implicit judgments on relevance relations. However, bad generalization of click-based or concept-based models hardly capture semantic information from short queries, which stunt existing methods to fully utilize the methods to enhance the IR performance. In this paper, we propose a framework ETHGS to combine the abilities of concept-based, click-based and semantic-based models in IR and publish a new video retrieval dataset QVT from a real-world video search engine. In ETHGS, we make use of tags (i.e. concept) to construct a heterogeneous graph to alleviate the sparsity of click-through data. And we also overcome the problem of long-tailed query representation without graph information by fusing tag embeddings to represent queries. ETHGS leverages semantic embeddings to review deviant semantic information from graph nodes information. Finally, we evaluate our model ETHGS on the QVT.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {94–98},
numpages = {5},
keywords = {text tagging, gaze detection, datasets, neural networks},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1007/s00778-017-0477-2,
author = {Ali, Syed Muhammad and Wrembel, Robert},
title = {From Conceptual Design to Performance Optimization of ETL Workflows: Current State of Research and Open Problems},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0477-2},
doi = {10.1007/s00778-017-0477-2},
abstract = {In this paper, we discuss the state of the art and current trends in designing and optimizing ETL workflows. We explain the existing techniques for: (1) constructing a conceptual and a logical model of an ETL workflow, (2) its corresponding physical implementation, and (3) its optimization, illustrated by examples. The discussed techniques are analyzed w.r.t. their advantages, disadvantages, and challenges in the context of metrics such as autonomous behavior, support for quality metrics, and support for ETL activities as user-defined functions. We draw conclusions on still open research and technological issues in the field of ETL. Finally, we propose a theoretical ETL framework for ETL optimization.},
journal = {The VLDB Journal},
month = {dec},
pages = {777–801},
numpages = {25},
keywords = {ETL workflow, ETL conceptual design, ETL optimization, ETL logical design, ETL physical implementation}
}

@inproceedings{10.1145/2815833.2816944,
author = {Rizzo, Giuseppe and Corcho, Oscar and Troncy, Rapha\"{e}l and Plu, Julien and Hermida, Juan Carlos Ballesteros and Assaf, Ahmad},
title = {The 3cixty Knowledge Base for Expo Milano 2015: Enabling Visitors to Explore the City},
year = {2015},
isbn = {9781450338493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815833.2816944},
doi = {10.1145/2815833.2816944},
abstract = {In this paper, we present the 3cixty Knowledge Base, which collects and harmonizes descriptions of events, places, transportation facilities and user-generated data such as reviews of the city and Expo site of Milan. This knowledge base is used by a set of web and mobile applications to guide Expo Milano 2015 visitors in the city and in the exhibit, allowing them to find places, satellite events and transportation facilities around Milan. As of July 24th, 2015 the knowledge base contains 18665 unique events, 225821 unique places, 94789 reviews, and 9343 transportation facilities, collected from several static, near- and real time local and global data providers, including Expo Milano 2015 official services and numerous social media platforms. The ontologies used as a backbone for structuring the knowledge base follow a rigorous development method where the design principle has generally been to re-use existing ontologies when they exist. We think that the lessons learned from this development will be useful for similar endeavors in other cities or large events around the world with a similar ecosystem of data provisioning services.},
booktitle = {Proceedings of the 8th International Conference on Knowledge Capture},
articleno = {18},
numpages = {4},
keywords = {Data Integration, Data Reconciliation, 3cixty, Smart City, Expo 2015, Knowledge base},
location = {Palisades, NY, USA},
series = {K-CAP 2015}
}

@article{10.1145/3191750,
author = {Lin, Yuxiang and Dong, Wei and Chen, Yuan},
title = {Calibrating Low-Cost Sensors by a Two-Phase Learning Approach for Urban Air Quality Measurement},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191750},
doi = {10.1145/3191750},
abstract = {Urban air quality information, e.g., PM2.5 concentration, is of great importance to both the government and society. Recently, there is a growing interest in developing low-cost sensors, installed on moving vehicles, for fine-grained air quality measurement. However, low-cost mobile sensors typically suffer from low accuracy and thus need careful calibration to preserve a high measurement quality. In this paper, we propose a two-phase data calibration method consisting of a linear part and a nonlinear part. We use MLS (multiple least square) to train the linear part, and use RF (random forest) to train the nonlinear part. We propose an automatic feature selection algorithm based on AIC (Akaike information criterion) for the linear model, which helps avoid overfitting due to the inclusion of inappropriate features. We evaluate our method extensively. Results show that our method outperforms existing approaches, achieving an overall accuracy improvement of 16.4% in terms of PM2.5 levels compared with state-of-the-art approach.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {18},
numpages = {18},
keywords = {Low-cost sensors, Sensor calibration, Air quality, Mobile sensor network}
}

@article{10.1145/3446373,
author = {Li, Xi and Wang, Zehua and Leung, Victor C. M. and Ji, Hong and Liu, Yiming and Zhang, Heli},
title = {Blockchain-Empowered Data-Driven Networks: A Survey and Outlook},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3446373},
doi = {10.1145/3446373},
abstract = {The paths leading to future networks are pointing towards a data-driven paradigm to better cater to the explosive growth of mobile services as well as the increasing heterogeneity of mobile devices, many of which generate and consume large volumes and variety of data. These paths are also hampered by significant challenges in terms of security, privacy, services provisioning, and network management. Blockchain, which is a technology for building distributed ledgers that provide an immutable log of transactions recorded in a distributed network, has become prominent recently as the underlying technology of cryptocurrencies and is revolutionizing data storage and processing in computer network systems. For future data-driven networks (DDNs), blockchain is considered as a promising solution to enable the secure storage, sharing, and analytics of data, privacy protection for users, robust, trustworthy network control, and decentralized routing and resource managements. However, many important challenges and open issues remain to be addressed before blockchain can be deployed widely to enable future DDNs. In this article, we present a survey on the existing research works on the application of blockchain technologies in computer networks and identify challenges and potential solutions in the applications of blockchains in future DDNs. We identify application scenarios in which future blockchain-empowered DDNs could improve the efficiency and security, and generally the effectiveness of network services.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {58},
numpages = {38},
keywords = {blockchain, blockchain-empowered data-driven networks, networking technologies, Data-driven networks}
}

@article{10.1145/3085580,
author = {Razzaque, M. A. and Hira, Muta Tah and Dira, Mukta},
title = {QoS in Body Area Networks: A Survey},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3085580},
doi = {10.1145/3085580},
abstract = {Body Area Networks (BANs) are becoming increasingly popular and have shown great potential in real-time monitoring of the human body. With the promise of being cost-effective and unobtrusive and facilitating continuous monitoring, BANs have attracted a wide range of monitoring applications, including medical and healthcare, sports, and rehabilitation systems. Most of these applications are real time and life critical and require a strict guarantee of Quality of Service (QoS) in terms of timeliness, reliability, and so on. Recently, there has been a number of proposals describing diverse approaches or frameworks to achieve QoS in BANs (i.e., for different layers or tiers and different protocols). This survey put these individual efforts into perspective and presents a more holistic view of the area. In this regard, this article identifies a set of QoS requirements for BAN applications and shows how these requirements are linked in a three-tier BAN system and presents a comprehensive review of the existing proposals against those requirements. In addition, open research issues, challenges, and future research directions in achieving these QoS in BANs are highlighted.},
journal = {ACM Trans. Sen. Netw.},
month = {aug},
articleno = {25},
numpages = {46},
keywords = {cloud computing, QoS, Body area networks, medical care, healthcare}
}

@inbook{10.1145/3310205.3310214,
title = {Conclusion and Future Thoughts},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310214},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3283207.3283210,
author = {Jilani, Musfira and Corcoran, Padraig and Bertolotto, Michela},
title = {A Multi-Layer CRF Based Methodology for Improving Crowdsourced Street Semantics},
year = {2018},
isbn = {9781450360371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3283207.3283210},
doi = {10.1145/3283207.3283210},
abstract = {This paper presents an intuitive and novel method for improving the semantic quality of streets in crowdsourced maps. Two factors negatively affecting the quality are incorrect and ambiguous semantics. Toward overcoming these, a multi-layer CRF based model is proposed that performs a simultaneous hierarchical classification of streets into fine-grained (crowdsourced; therefore, rich but ambiguous) and coarse-grained (familiar and standard) semantics. Inference is performed using Lazy Flipper algorithm which is fast for street network consisting of several hundred thousand streets. The model achieves a classification accuracy of 61% for fine-grained classification and 77% for coarse-grained classification respectively.},
booktitle = {Proceedings of the 11th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
pages = {29–38},
numpages = {10},
keywords = {Street Networks, Semantics, Conditional Random Fields, OpenStreetMap, Hierarchical Classification},
location = {Seattle, WA, USA},
series = {IWCTS'18}
}

@inproceedings{10.1145/2938503.2938519,
author = {Caruccio, Loredana and Deufemia, Vincenzo and Polese, Giuseppe},
title = {On the Discovery of Relaxed Functional Dependencies},
year = {2016},
isbn = {9781450341189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938503.2938519},
doi = {10.1145/2938503.2938519},
abstract = {Functional dependencies (fds) express important relationships among data, which can be used for several goals, including schema normalization and data cleansing. However, to solve several issues in emerging application domains, such as the identification of data inconsistencies or patterns of semantically related data, it has been necessary to relax the fd definition through the introduction of approximations in data comparison and/or validity. Moreover, while fds were originally specified at design time, with the availability of massive data and computational power many algorithms have been devised to automatically discover them from data, including algorithms for discovering some types of relaxed fds. In this paper we present a technique that exploits lattice-based algorithms for the discovery of fds from data, in order to detect relaxed fds. Moreover, we introduce an algorithm to determine a proper distance threshold for a given relaxed fd holding over the entire database.},
booktitle = {Proceedings of the 20th International Database Engineering &amp; Applications Symposium},
pages = {53–61},
numpages = {9},
keywords = {functional dependency, Database integration, discovery algorithm, approximate match},
location = {Montreal, QC, Canada},
series = {IDEAS '16}
}

@inproceedings{10.1145/3522664.3528604,
author = {Singhal, Anmol and Anish, Preethu Rose and Sonar, Pratik and Ghaisas, Smita S},
title = {Data is about Detail: An Empirical Investigation for Software Systems with NLP at Core},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528604},
doi = {10.1145/3522664.3528604},
abstract = {Businesses continue to operate under increasingly complex demands such as ever-evolving regulatory landscape, personalization requirements from software apps, and stricter governance with respect to security and privacy. In response to these challenges, large enterprises have been emphasizing automation across a wide range, starting with business processes all the way to customer experience. As AI continues to be a core component of software systems being developed, data assumes a predominant role. AI-centric software systems of industrial scale need large amounts of training data, that in our experience, has introduced several challenges. In this paper, through an empirical study based on interviews with AI practitioners, we present current challenges that need to be addressed in 'data requirements' of Software Systems with NLP at the Core (SSNLPCore). We further discuss the impact of the challenges and techniques currently employed by practitioners for addressing them. Our findings reveal that a focus on details pertaining to data is required early into the project lifecycle, which include aspects such as how we may select, process, and annotate data. This can ensure that the AI component is effective in meeting business goals of software systems.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {145–156},
numpages = {12},
keywords = {data requirements, empirical study, natural language processing},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3077136.3082060,
author = {Deng, Alex and Dmitriev, Pavel and Gupta, Somit and Kohavi, Ron and Raff, Paul and Vermeer, Lukas},
title = {A/B Testing at Scale: Accelerating Software Innovation},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3082060},
doi = {10.1145/3077136.3082060},
abstract = {The Internet provides developers of connected software, including web sites, applications, and devices, an unprecedented opportunity to accelerate innovation by evaluating ideas quickly and accurately using controlled experiments, also known as A/B tests. From front-end user-interface changes to backend algorithms, from search engines (e.g., Google, Bing, Yahoo!) to retailers (e.g., Amazon, eBay, Etsy) to social networking services (e.g., Facebook, LinkedIn, Twitter) to travel services (e.g., Expedia, Airbnb, Booking.com) to many startups, online controlled experiments are now utilized to make data-driven decisions at a wide range of companies. While the theory of a controlled experiment is simple, and dates back to Sir Ronald A. Fisher's experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s, the deployment and evaluation of online controlled experiments at scale (100's of concurrently running experiments) across variety of web sites, mobile apps, and desktop applications presents many pitfalls and new research challenges. In this tutorial we will give an introduction to A/B testing, share key lessons learned from scaling experimentation at Bing to thousands of experiments per year, present real examples, and outline promising directions for future work. The tutorial will go beyond applications of A/B testing in information retrieval and will also discuss on practical and research challenges arising in experimentation on web sites and mobile and desktop apps. Our goal in this tutorial is to teach attendees how to scale experimentation for their teams, products, and companies, leading to better data-driven decisions. We also want to inspire more academic research in the relatively new and rapidly evolving field of online controlled experimentation.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1395–1397},
numpages = {3},
keywords = {experimentation, a/b testing},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/2523429.2523458,
author = {Nyk\"{a}nen, Ossi},
title = {Datamap Visualization Technique for Interactively Visualizing Large Datasets},
year = {2013},
isbn = {9781450319928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523429.2523458},
doi = {10.1145/2523429.2523458},
abstract = {This article describes the novel datamap visualization technique which enables visualizing large datasets interactively and fairly, inspired by geographic maps and microscopes. The main contributions include introducing the datamap metaphor and datamap visualization architecture, specifying efficient methods of approximate rendering, and illustrating the basic concepts in terms of example applications.},
booktitle = {Proceedings of International Conference on Making Sense of Converging Media},
pages = {52–58},
numpages = {7},
keywords = {Datamap visualization, Interactive data exploration and discovery, Data and knowledge visualization, Visualization techniques and methodologies},
location = {Tampere, Finland},
series = {AcademicMindTrek '13}
}

@inproceedings{10.1145/3141880.3141886,
author = {Grillenberger, Andreas and Romeike, Ralf},
title = {Key Concepts of Data Management: An Empirical Approach},
year = {2017},
isbn = {9781450353014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141880.3141886},
doi = {10.1145/3141880.3141886},
abstract = {When preparing new topics for teaching, it is important to identify their central aspects. Sets of fundamental ideas, great principles or big ideas have already been described for several parts of computer science. Yet, existing catalogs of ideas, principles and concepts of computer science only consider the field data management marginally. However, we assume that several concepts of data management are fundamental to CS and, despite the significant changes in this field in recent years, have long-term relevance. In order to provide a comprehensive overview of the key concepts of data management and to bring relevant parts of this field to school, we describe and use an empirical approach to determine such central aspects systematically. This results in a model of key concepts of data management. On the basis of examples, we show how the model can be interpreted and used in different contexts and settings.},
booktitle = {Proceedings of the 17th Koli Calling International Conference on Computing Education Research},
pages = {30–39},
numpages = {10},
keywords = {mechanics, CS education, principles, practices, core technologies, key concepts, data management, model},
location = {Koli, Finland},
series = {Koli Calling '17}
}

@article{10.1145/2676566,
author = {MacLean, Diana Lynn},
title = {Gathering People to Gather Data},
year = {2014},
issue_date = {Winter 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1528-4972},
url = {https://doi.org/10.1145/2676566},
doi = {10.1145/2676566},
abstract = {An interview with Paul Wicks, Vice President of Innovation at PatientsLikeMe, a patient network and real-time research platform.},
journal = {XRDS},
month = {dec},
pages = {18–22},
numpages = {5}
}

@inproceedings{10.1145/3485768.3485770,
author = {Guo, Yuanyuan},
title = {Data Protection Measures in E-Society: Policy Implications of British Data Protection Act to China},
year = {2021},
isbn = {9781450390156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485768.3485770},
doi = {10.1145/3485768.3485770},
booktitle = {2021 5th International Conference on E-Society, E-Education and E-Technology},
pages = {171–176},
numpages = {6},
keywords = {E-society, Private information, Public policy, Data protection},
location = {Taipei, Taiwan},
series = {ICSET 2021}
}

@inproceedings{10.1145/3210586.3210587,
author = {Baker, Karen S. and Karasti, Helena},
title = {Data Care and Its Politics: Designing for Local Collective Data Management as a Neglected Thing},
year = {2018},
isbn = {9781450363716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210586.3210587},
doi = {10.1145/3210586.3210587},
abstract = {In this paper, we think with Puig de la Bellacasa's 'matters of care' about how to support data care and its politics. We use the notion to reflect on participatory design activities in two recent case studies of local collective data management in ecological research. We ask "How to design for data care?" and "How to account for the politics of data care in design?" Articulation of data care together with ethically and politically significant data issues in design, reveals in these cases the invisible labors of care by local data advocates and a 'partnering designer'. With digital data work in the sciences increasing and data infrastructures for research under development at a variety of large scales, the local level is often considered merely a recipient of services rather than an active participant in design of data practices and infrastructures. We identify local collective data management as a 'neglected thing' in infrastructure planning and speculate on how things could be different in the data landscape.},
booktitle = {Proceedings of the 15th Participatory Design Conference: Full Papers - Volume 1},
articleno = {10},
numpages = {12},
keywords = {participatory design, science and technology studies, politics, matters of care, local collective data management, information management, partnering designer, infrastructuring, information infrastructure, data care},
location = {Hasselt and Genk, Belgium},
series = {PDC '18}
}

@article{10.14778/3352063.3352066,
author = {Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Li, Zijue and Li, Jianzhong and Gao, Hong},
title = {Cleanits: A Data Cleaning System for Industrial Time Series},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352066},
doi = {10.14778/3352063.3352066},
abstract = {The great amount of time series generated by machines has enormous value in intelligent industry. Knowledge can be discovered from high-quality time series, and used for production optimization and anomaly detection in industry. However, the original sensors data always contain many errors. This requires a sophisticated cleaning strategy and a well-designed system for industrial data cleaning. Motivated by this, we introduce Cleanits, a system for industrial time series cleaning. It implements an integrated cleaning strategy for detecting and repairing three kinds of errors in industrial time series. We develop reliable data cleaning algorithms, considering features of both industrial time series and domain knowledge. We demonstrate Cleanits with two real datasets from power plants. The system detects and repairs multiple dirty data precisely, and improves the quality of industrial time series effectively. Cleanits has a friendly interface for users, and result visualization along with logs are available during each cleaning process.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1786–1789},
numpages = {4}
}

@inproceedings{10.1145/3487664.3487783,
author = {Diamantini, Claudia and Potena, Domenico and Storti, Emanuele},
title = {A Semantic Data Lake Model for Analytic Query-Driven Discovery},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487783},
doi = {10.1145/3487664.3487783},
abstract = {Data Lake (DL) architectures have recently emerged as an effective solution to the problem of data analytics with big, highly heterogeneous, and quickly changing data sources. However, novel challenges arise too, including how to make sense of disparate raw data and how to identify the sources that satisfy a data need. In the paper, we introduce a semantic model for a Data Lake aimed to support data discovery and integration in data analytics scenarios. By formally modeling indicators of interest, their computation formulas, and dimensions of analysis in a knowledge graph, and by seamlessly mapping them to relevant source metadata, the framework is suited for identifying the sources and the required transformation steps according to the analytical request.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {183–186},
numpages = {4},
keywords = {Data Lake, ontology, indicator, query-driven discovery},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3485314.3485323,
author = {Su, Hang and He, Qian and Guo, Biao},
title = {KPI Anomaly Detection Method for Data Center AIOps Based on GRU-GAN},
year = {2022},
isbn = {9781450384957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485314.3485323},
doi = {10.1145/3485314.3485323},
abstract = {The system architecture and application services of the data center are becoming increasingly large. To ensure the stable operation of the systems and businesses carried by the data center, the operations engineer needs to collect and monitor the generating KPIs during the operation of the systems and services. Traditional KPI anomaly detection methods are faced with the challenges of the huge amount of KPIs and constantly changing data characteristics, which are gradually no longer suitable for highly dynamic systems and services. With the popularity of artificial intelligence algorithms, machine learning and deep learning methods have also begun to be applied in operation and maintenance scenarios, that is the emergence of Artificial Intelligence for IT Operations (AIOps). KPI anomaly detection is the underlying core technology of AIOps. This paper proposes a hybrid model based on GRU-GAN (GGAN) for KPI anomaly detection in data center AIOps. The Gated Recurrent Unit (GRU) network is selected as the generator and discriminator of Generative adversarial network (GAN) in this model, which get the time correlation and data distribution of KPI through the adversarial training between the generator and the discriminator to make use of the reconstruction ability of the generator and the discriminant ability of the discriminator at the same time. At the anomaly detection stage, the anomaly score is formed by integrating reconstruction difference and discrimination loss to complete the anomaly detection task. Experimental results show that the proposed method can more accurately capture the variable data characteristics of KPI compared with the traditional KPI anomaly detection method and the general unsupervised method, as well as achieve better performance in the KPI anomaly detection task.},
booktitle = {2021 10th International Conference on Internet Computing for Science and Engineering},
pages = {23–29},
numpages = {7},
keywords = {GRU, GAN, AIOps, Data Center, KPI Anomaly Detection},
location = {Guilin, China},
series = {ICICSE 2021}
}

@inproceedings{10.1145/3465336.3475107,
author = {Nurmikko-Fuller, Terhi and Pickering, Paul},
title = {Reductio Ad Absurdum?: From Analogue Hypertext to Digital Humanities},
year = {2021},
isbn = {9781450385510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465336.3475107},
doi = {10.1145/3465336.3475107},
abstract = {In this paper we report on a complex and complete archive of historical primary sources that map the political landscape of the anglophone world in the mid-to late 1800s. The ruthless pragmatism applied to the construction of the initial Humanities dataset resulted in an analogue equivalent of a hypertext system, which has already resulted in published academic books and articles. Here, we describe the processes of a current project, which consists of the translation of this analogue information aggregation system into a graph database using Linked Data and semantic Web technologies.},
booktitle = {Proceedings of the 32nd ACM Conference on Hypertext and Social Media},
pages = {245–250},
numpages = {6},
keywords = {hypertext, linked data, australian history, information aggregation, political history},
location = {Virtual Event, USA},
series = {HT '21}
}

@inproceedings{10.1145/3085228.3085283,
author = {Brolch\'{a}in, Niall \'{O} and Porwol, Lukasz and Ojo, Adegboyega and Wagner, Tilman and Lopez, Eva Tamara and Karstens, Eric},
title = {Extending Open Data Platforms with Storytelling Features},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085283},
doi = {10.1145/3085228.3085283},
abstract = {1Research into Data-Driven Storytelling using Open Data has led to considerable discussion into many possible futures for storytelling and journalism in a Data-Driven world, in particular, into the Open Data directives framed by various governments across the globe as a means of facilitating governments, transparency enabled citizens and journalists to get more insights into government actions and enable deeper and easier monitoring of governments' work. While progress in the development of Open Data platforms (usually funded by national and local governments) has been significant, it is only now that we are beginning to see the emergence of more practical and more applied use of Open Data platforms. Previous works have highlighted the potential for storytelling using Open Data as a source of information for journalistic stories. Nevertheless, there is a paucity of studies into Open Data platform affordances to support Data-Driven Storytelling. In this paper, we elaborate on existing Open Data platforms in terms of support for storytelling and analyse feedback from stakeholder focus groups, to discover what methods and tools can introduce or facilitate the storytelling capabilities of Open Data platforms.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {48–53},
numpages = {6},
keywords = {Journalism, YDS Platform, Data-Driven Storytelling, Usable Open Data Platform, Data-Driven Journalism, Open Data},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@inproceedings{10.1145/2721956.2721968,
author = {Leibold, Christian F. and Spies, Marcus},
title = {Towards a Pattern Language for Cognitive Systems Integration},
year = {2014},
isbn = {9781450334167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2721956.2721968},
doi = {10.1145/2721956.2721968},
abstract = {This paper discusses the influence of recent advances in cognitive computing systems on enterprise software architecture and design/development. Specifically, building on key features and capabilities of cognitive computing systems, we propose a new schema of enterprise application integration patterns in the tradition of the design pattern literature. Our schema has three groups of patterns addressing essential scoping, security and service integration issues related to cognitive components in enterprise architecture. While some patterns are modifications or refinements of known Enterprise Application Integration patterns, some of them are new and require dedicated consideration by enterprise architects and software designers.},
booktitle = {Proceedings of the 19th European Conference on Pattern Languages of Programs},
articleno = {17},
numpages = {9},
keywords = {scope identification, enterprise integration pattern, pattern language, social and ethical impact, cognitive systems, requirements engineering},
location = {Irsee, Germany},
series = {EuroPLoP '14}
}

@inproceedings{10.1145/3093338.3104170,
author = {Pugmire, David and Bozda\u{g}, Ebru and Lefebvre, Matthieu and Tromp, Jeroen and Komatitsch, Dmitri and Peter, Daniel and Podhorszki, Norbert and Hill, Judith},
title = {Pillars of the Mantle: Imaging the Interior of the Earth with Adjoint Tomography},
year = {2017},
isbn = {9781450352727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093338.3104170},
doi = {10.1145/3093338.3104170},
abstract = {In this work, we investigate global seismic tomographic models obtained by spectral-element simulations of seismic wave propagation and adjoint methods. Global crustal and mantle models are obtained based on an iterative conjugate-gradient type of optimization scheme. Forward and adjoint seismic wave propagation simulations, which result in synthetic seismic data to make measurements and data sensitivity kernels to compute gradient for model updates, respectively, are performed by the SPECFEM3D_GLOBE package [1] [2] at the Oak Ridge Leadership Computing Facility (OLCF) to study the structure of the Earth at unprecedented levels. Using advances in solver techniques that run on the GPUs on Titan at the OLCF, scientists are able to perform large-scale seismic inverse modeling and imaging. Using seismic data from global and regional networks from global CMT earthquakes, scientists are using SPECFEM3D_GLOBE to understand the structure of the mantle layer of the Earth. Visualization of the generated data sets provide an effective way to understand the computed wave perturbations which define the structure of mantle in the Earth.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
articleno = {75},
numpages = {4},
location = {New Orleans, LA, USA},
series = {PEARC17}
}

@inproceedings{10.5555/3017447.3017493,
author = {Bishop, Bradley Wade and Hank, Carolyn},
title = {Data Curation Profiling of Biocollections},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {In the contexts of the data deluge and open data, scientists studying biodiversity benefit from online access to global datasets of existing vouchered biological and paleontological collections. Using biocollections collected over time across the world allows for the advancement of scientific knowledge concerning evolution in process as well as species poleward migrations, an indicator of climate change. This study's purpose was to validate and expand the Data Curation Profiles (DCP) to digital biocollections and inform a DCP framework for worldwide biota. Ten biocollection producers, curating various types of specimens affiliated with the project building the United States' national biodiversity infrastructure, were interviewed using the DCP questionnaire. Results indicate there is extreme diversity in the curation of biocollections and additional DCP questions should be added to reflect the complicated approaches to biological data curation. Although discipline specific metadata creation tools, standards, and practices enable long-term sustainability of the U.S. digitization effort, some scientists would benefit from further clarification and guidance on the information needs of consumers beyond designated communities of expert users, and the long-term preservation of biocollections.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology},
articleno = {46},
numpages = {9},
keywords = {biocollections, biology, data curation, data provenance, data curation profiles},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@article{10.1145/2641383.2641390,
author = {Agosti, Maristella and Fuhr, Norbert and Toms, Elaine and Vakkari, Pertti},
title = {Evaluation Methodologies in Information Retrieval Dagstuhl Seminar 13441},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/2641383.2641390},
doi = {10.1145/2641383.2641390},
journal = {SIGIR Forum},
month = {jun},
pages = {36–41},
numpages = {6}
}

@inproceedings{10.1145/3465481.3470055,
author = {P\"{o}hn, Daniela and Seeber, Sebastian and Hanauer, Tanja and Ziegler, Jule A. and Schmitz, David},
title = {Towards Improving Identity and Access Management with the IdMSecMan Process Framework},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470055},
doi = {10.1145/3465481.3470055},
abstract = {In today’s networks, administrative access to Linux servers is commonly managed by Privileged Access Management (PAM). It is not only important to monitor these privileged accounts, but also to control segregation of duty and detect keys as well as accounts that potentially bypass PAM. Unprohibited access can become a business risk. In order to improve the security in a controlled manner, we establish IdMSecMan, a security management process tailored for identity and access management (IAM). Security management processes typically use the Deming Cycle or an adaption for continuous improvements of products, services, or processes within the network infrastructure. We adjust a security management process with visualization for IAM, which also shifts the focus from typical assets to the attacker. With the controlled cycles, the maturity of IAM is measured and can continually advance. This paper presents and applies the work in progress IdMSecMan to a motivating scenario in the field of Linux server. We evaluate our approach in a controlled test environment with first steps to roll it out in our data center. Last but not least, we discuss challenges and future work.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {89},
numpages = {10},
keywords = {Server, Security Management, Identity Management, Security},
location = {Vienna, Austria},
series = {ARES 21}
}

@inproceedings{10.5555/3522802.3522903,
author = {Hunker, Joachim and Wuttke, Alexander and Scheidler, Anne Antonia and Rabe, Markus},
title = {A Farming-for-Mining-Framework to Gain Knowledge in Supply Chains},
year = {2022},
publisher = {IEEE Press},
abstract = {Gaining knowledge from a given data basis is a complex challenge. One of the frequently used methods in the context of a supply chain (SC) is knowledge discovery in databases (KDD). For a purposeful and successful knowledge discovery, valid and preprocessed input data are necessary. Besides preprocessing collected observational data, simulation can be used to generate a data basis as an input for the knowledge discovery process. The process of using a simulation model as a data generator is called data farming. This paper investigates the link between data farming and data mining. We developed a Farming-for-Mining-Framework, where we highlight requirements of knowledge discovery techniques and derive how the simulation model for data generation can be configured accordingly, e.g., to meet the required data accuracy. We suggest that this is a promising approach and is worth further research attention.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {130},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3340964.3340975,
author = {Xu, Jianqiu and Lu, Hua and G\"{u}ting, Ralf Hartmut},
title = {Understanding Human Mobility: A Multi-Modal and Intelligent Moving Objects Database},
year = {2019},
isbn = {9781450362801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340964.3340975},
doi = {10.1145/3340964.3340975},
abstract = {The research field of moving objects has been quite active in the past 20 years. The recording of position data becomes easy and huge amounts of mobile data are collected. Moving objects databases represent time-dependent objects and support queries with spatial and temporal constraints. In this paper we provide the vision of a multi-model and intelligent moving objects database. The goal is to enhance the data management of moving objects by providing extensive data models for different applications and fusing artificial intelligence techniques. Toward this goal, we propose how to develop corresponding modules and integrate them into the system to achieve the next-generation moving objects database.},
booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
pages = {222–225},
numpages = {4},
location = {Vienna, Austria},
series = {SSTD '19}
}

@inproceedings{10.1145/3209281.3209297,
author = {Yoon, Sang-Pil and Joo, Moon-Ho and Kwon, Hun-Yeong},
title = {Role of Law as a Guardian of the Right to Use Public Sector Information: Case Study of Korean Government},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209297},
doi = {10.1145/3209281.3209297},
abstract = {With data revolution, data is emerging as a new raw material. As the importance of data has increased, interest in the availability of public sector information (PSI) has also grown. PSI created in the public sector comprises public attributes and directly impacts national administration and citizen's lives.Korea has almost the highest level of Information and Communication Technology (ICT) infrastructure and considerable data through government-led policies. As a result of such policies, Korea has demonstrated excellent results in the United Nation's e-government survey, ITU's ICT development index, and OECD's public data openness index. Paradoxically, however, this history and experience is a stumbling block to a new era. PSI, which is a basic resource for realizing the value of openness, sharing, cooperation, and communication, should be actively managed and opened by government to provide support for reuse in the sense that the government is its main producer and manager. However, no matter how good the quality of PSI through data management is and how excellent policies and institutions are established, if the private sector cannot actively use it, it is useless. What is the role of government and law in the context of changing the way data is managed and blurring sectoral boundaries?This paper aims to propose core challenges by analyzing the case of Korea in order to derive a basis for discussions to coordinate public and private cooperation and legal relations in the process. To begin with, we analyze the changes in the management environment of data and PSI and identify the role of government and law in responding to changes in the legal rights. Then, we discuss how Korea responds to change, examines related policies by function and discussions on the data law, which seem to have the greatest effect on government's role, and suggests essential tasks to change its role accordingly.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {83},
numpages = {10},
keywords = {public data, reuse of PSI, openness, legal right management, public sector information, public-private cooperation, data management, the right to know},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3303772.3303821,
author = {Mitra, Ritayan and Chavan, Pankaj},
title = {DEBE Feedback for Large Lecture Classroom Analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303821},
doi = {10.1145/3303772.3303821},
abstract = {Learning Analytics (LA) research has demonstrated the potential of LA in detecting and monitoring cognitive-affective parameters and improving student success. But most of it has been applied to online and computerized learning environments whereas physical classrooms have largely remained outside the scope of such research. This paper attempts to bridge that gap by proposing a student feedback model in which they report on the difficult/easy and engaging/boring aspects of their lecture. We outline the pedagogical affordances of an aggregated time-series of such data and discuss it within the context of LA research.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {426–430},
numpages = {5},
keywords = {mobile application, Large lectures, live feedback, learning analytics, quantified self},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2912160.2912206,
author = {Li, Hongqin and Zhai, Jun},
title = {Constructing Investment Open Data of Chinese Listed Companies Based on Linked Data},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912206},
doi = {10.1145/2912160.2912206},
abstract = {Linked Data can provide the data according to user's demand, promote the availability of half structured and unstructured data on the network, improve the interoperability of open data. With the development of Linked Data, Linked Enterprise Data becomes a research hotspot. This article draw lessons from foreign investment Linked Data research of listed companies, selected the listed company information and XBRL reports from Shanghai stock exchange and Shenzhen stock exchange, industry information from the China securities regulatory commission and daily stock price from Flush as data source, built investment open data of Chinese listed companies based on Linked Data. This work could promote the internationalization of the Chinese data and the commercial use of open government data, lay the foundation for the global data ecological system, at the same time prepare for the challenge of Shanghai-Hong Kong Stock Connect and Shenzhen-Hong Kong Stock Connect even the challenge of international investment.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {475–480},
numpages = {6},
keywords = {Open Data, Linked Enterprise Data, XBRL, Linked Data},
location = {Shanghai, China},
series = {dg.o '16}
}

@article{10.14778/3484224.3484233,
author = {Ammerlaan, Remmelt and Antonius, Gilbert and Friedman, Marc and Hossain, H M Sajjad and Jindal, Alekh and Orenberg, Peter and Patel, Hiren and Qiao, Shi and Ramani, Vijay and Rosenblatt, Lucas and Roy, Abhishek and Shaffer, Irene and Srinivasan, Soundarajan and Weimer, Markus},
title = {PerfGuard: Deploying ML-for-Systems without Performance Regressions, Almost!},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3484224.3484233},
doi = {10.14778/3484224.3484233},
abstract = {Modern data processing systems require optimization at massive scale, and using machine learning to optimize these systems (ML-for-systems) has shown promising results. Unfortunately, ML-for-systems is subject to over generalizations that do not capture the large variety of workload patterns, and tend to augment the performance of certain subsets in the workload while regressing performance for others. In this paper, we introduce a performance safeguard system, called PerfGuard, that designs pre-production experiments for deploying ML-for-systems. Instead of searching the entire space of query plans (a well-known, intractable problem), we focus on query plan deltas (a significantly smaller space). PerfGuard formalizes these differences, and correlates plan deltas to important feedback signals, like execution cost. We describe the deep learning architecture and the end-to-end pipeline in PerfGuard that could be used with general relational databases. We show that this architecture improves on baseline models, and that our pipeline identifies key query plan components as major contributors to plan disparity. Offline experimentation shows PerfGuard as a promising approach, with many opportunities for future improvement.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {3362–3375},
numpages = {14}
}

@inproceedings{10.1145/3316782.3322785,
author = {Haescher, Marian and Matthies, Denys J. C. and Krause, Silvio and Bieber, Gerald},
title = {Presenting a Data Imputation Concept to Support the Continuous Assessment of Human Vital Data and Activities},
year = {2019},
isbn = {9781450362320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316782.3322785},
doi = {10.1145/3316782.3322785},
abstract = {Data acquisition of mobile tracking devices often suffers from invalid and non-continuous input data streams. This issue especially occurs with current wearables tracking the user's activity and vital data. Typical reasons include the short battery life and the fact that the body-worn tracking device may be doffed. Other reasons, such as technical issues, can corrupt the data and render it unusable. In this paper, we introduce a data imputation concept which complements and thus fixes incomplete datasets by using a new merging approach that is particularly suitable for assessing activities and vital data. Our technique enables the dataset to become coherent and comprehensive so that it is ready for further analysis. In contrast to previous approaches, our technique enables the controlled creation of continuous data sets that also contain information on the level of uncertainty for possible reconversions, approximations, or later analysis.},
booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {587–592},
numpages = {6},
keywords = {data fusion, controlled data creation, smartwatch, coherent database, sensor fusion, accelerometer, mobile device, data imputation},
location = {Rhodes, Greece},
series = {PETRA '19}
}

@inproceedings{10.1145/3487075.3487147,
author = {Zhang, Jiapeng and Zhuang, Cunbo and Liu, Jianhua and Yuan, Kun and Zhang, Jin and Liu, Juan},
title = {Digital Twin-Based Three-Dimensional Visual and Global Monitoring of Assembly Shop-Floor},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487147},
doi = {10.1145/3487075.3487147},
abstract = {Aiming at the requirements of rapid response and production efficiency improvement in the assembly shop-floor, a three-dimensional (3D) visual and global monitoring method for the assembly shop-floor based on the digital twin is proposed. The paper analyzes the monitoring objectives, objects, and methods of assembly shop-floor, and constructs a global monitoring framework of digital twin-based assembly shop-floor. Then, three key technologies of realizing global monitoring shop-floor are described: current-time data perception and collection, current-time information-driven digital twin generation, and state monitoring and optimization based on digital twin. Finally, a global monitoring prototype system is designed and developed to verify the effectiveness of the proposed method.},
booktitle = {The 5th International Conference on Computer Science and Application Engineering},
articleno = {72},
numpages = {7},
keywords = {Assembly shop-floor, 3D visual monitoring, Global monitoring, Digital twin},
location = {Sanya, China},
series = {CSAE 2021}
}

@proceedings{10.1145/3543106,
title = {ICEMC '22: Proceedings of the 2022 International Conference on E-Business and Mobile Commerce},
year = {2022},
isbn = {9781450397162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seoul, Republic of Korea}
}

@article{10.1145/3503780.3503792,
author = {Kondylakis, Haridimos and Stefanidis, Kostas and Rao, Praveen},
title = {Report on the Third International Workshop on Semantic Web Meets Health Data Management (SWH 2020)},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3503780.3503792},
doi = {10.1145/3503780.3503792},
abstract = {Creating a holistic view of patient data comes with many challenges but also brings many benefits for disease prediction, prevention, diagnosis, and treatment. Especially in the COVID-19 era, this is more important than ever before. The third International Workshop on Semantic Web Meets Health Data Management (SWH) was aimed at bringing together an interdisciplinary audience who was interested in the fields of Semantic Web, data management, and health informatics. The workshop goal was to discuss the challenges in healthcare data management and to propose new solutions for the next generation of data-driven healthcare systems. In this article, we summarize the outcomes of the workshop, and we present a number of key observations and research directions that emerged from presentations.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {32–35},
numpages = {4}
}

@inproceedings{10.1145/3512826.3512840,
author = {Yang, Xiaohui and Guo, Chenxi and Ren, Huan and Dong, Ming},
title = {Research on Anomaly Detection Method of Online Monitoring Data of Dissolved Gas in Transformer Oil},
year = {2022},
isbn = {9781450395489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512826.3512840},
doi = {10.1145/3512826.3512840},
abstract = {Dissolved gases in oil analysis has been a significant conventional condition detection method for condition evaluation for power transformers. But false data and wrong data do exist in DGA on-line monitoring system, which often lead to misjudgment. To handle this problem, the monitoring system often uses a threshold method based on data distribution statistics to determine the authenticity of the data. However, it is difficult to grasp the rules of the data distribution in advance, resulting in the problem of generally low detection rate of abnormal data. In this paper, according to the time series characteristics of on-line monitoring data of DGA, an abnormal data detection method based on condensed hierarchical clustering is proposed. First, the sliding time window is used to preprocess a variety of oil gas monitoring data to obtain a time series set of monitoring data, and comprehensively apply statistical indicators to classify them and establish Typical time series map; on this basis, the agglomerated hierarchical clustering model is used to perform similarity clustering on the distance between different characteristic data points and the typical abnormal map to determine the abnormal type of the monitoring data. The verification of the application of actual monitoring data shows that this method can detect data anomalies in the online monitoring data stream and determine its type in real time.},
booktitle = {2022 The 3rd International Conference on Artificial Intelligence in Electronics Engineering},
pages = {65–69},
numpages = {5},
keywords = {On-line monitoring, Time series, Hierarchical agglomerative cluster, Anomaly detection, Dissolved gases in oil},
location = {Bangkok, Thailand},
series = {AIEE 2022}
}

@article{10.1145/3349629,
author = {Verma, Neeta and Dawar, Savita},
title = {Digital Transformation in the Indian Government},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3349629},
doi = {10.1145/3349629},
journal = {Commun. ACM},
month = {oct},
pages = {50–53},
numpages = {4}
}

@article{10.1145/2590989.2591002,
author = {Pedersen, Torben Bach and Lehner, Wolfgang},
title = {Report on the Second International Workshop on Energy Data Management (EnDM 2013)},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2590989.2591002},
doi = {10.1145/2590989.2591002},
journal = {SIGMOD Rec.},
month = {feb},
pages = {70–72},
numpages = {3}
}

@inproceedings{10.1145/2912160.2912179,
author = {Hu, Yanhua and Bai, Xianyang and Sun, Shuyang},
title = {Readiness Assessment of Open Government Data Programs: A Case of Shenzhen},
year = {2016},
isbn = {9781450343398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912160.2912179},
doi = {10.1145/2912160.2912179},
abstract = {More and more cities in China are implementing various open government data initiatives for improving their governance. Little research, however, has been done in evaluating the readiness of individual governments in pursuing such initiatives. This paper presents a case study of the readiness assessment on the adoption of open data programs in Shenzhen based on the open data readiness assessment framework of the World Bank. The result shows that there are several issues including developing an action plan, providing privacy and ownership solutions, designating a unified administration, and implementing consistent data management policies and standards that need to be adequately addressed for the effective adoption of the open data program in the city.},
booktitle = {Proceedings of the 17th International Digital Government Research Conference on Digital Government Research},
pages = {97–103},
numpages = {7},
keywords = {Open data, Open government data, Readiness assessment},
location = {Shanghai, China},
series = {dg.o '16}
}

@inproceedings{10.1145/2851613.2851757,
author = {Vilela, J\'{e}ssyka and Gon\c{c}alves, Enyo and Holanda, Ana and Figueiredo, Bruno and Castro, Jaelson},
title = {Retrospective, Relevance, and Trends of SAC Requirements Engineering Track},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851757},
doi = {10.1145/2851613.2851757},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: This study aims to investigate how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 79 papers over the 8 previous SAC RE-Track editions, which were analyzed and discussed.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1264–1269},
numpages = {6},
keywords = {symposium on applied computing, relevance, scoping study, SAC, retrospective, systematic mapping study, requirements engineering, trends},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3414274.3414509,
author = {Liu, Xingchen and Zhao, Boyu and Qian, Haotian and Liu, Yuhang},
title = {Multidimensional Data Mining on the Early Scientific Talents of China},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414509},
doi = {10.1145/3414274.3414509},
abstract = {The cultivation of high-level talents in either scientific or engineering domain is of significant importance to the development of a country. From the perspective of data science, this paper takes the group of academicians of the Chinese Academy of Sciences and the Chinese Academy of Engineering as an example to explore the method of cultivating high-end talents. Through multidimensional data analysis, it is of great significance to explore the spatial pattern and time evolution characteristics of the first generation of top natural science talents in China, to deepen the understanding of the growth and education laws of talents, optimize top-level design, and implement targeted scientific policies. It is found that Chinese culture and Chinese native universities have made significant contributions for the early scientific talents of China, and the analysis method of data science can be used to facilitate the education innovation.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {239–246},
numpages = {8},
keywords = {Cultivation of high-level talents, Education, Data-mining},
location = {Xiamen, China},
series = {DSIT 2020}
}

@inproceedings{10.1145/2935663.2935664,
author = {Zhu, Chengang and Cheng, Guang and Guo, Xiaojun and Wang, Yuxiang},
title = {RBAS: A Real-Time User Behavior Analysis System for Internet TV in Cloud Computing},
year = {2016},
isbn = {9781450341813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2935663.2935664},
doi = {10.1145/2935663.2935664},
abstract = {The characteristic of Internet TV user behavior is quite essential for designers to optimize resource schedule and improve user experience. With the rapid development of Internet, both Internet TV users and STB (set top boxes) models are booming. This brings a large amount of behavior data which requires matching computing and storage resource to process. Therefore, scalable Internet TV user behavior analysis becomes more difficult. As a solution, cloud computing framework such as Hive is emerged. But limited by performance, it's not an appropriate choice for interactive analysis or real-time data exploration. In this paper, we present a real-time Internet TV user behavior analysis system with advantages of high concurrency, low latency and good transportability. Firstly, we design an event capture scheme, consisted of agents embedded in STBs and capture server clusters, to capture every manipulation performed by users. Secondly, we develop a SQL-on-Hadoop engine with distributed transactional management to decrease the response time. The engine has excellent query performance and ability to interactively query various data sources in different Hadoop formats. Lastly, we evaluate RBAS in a commercial Internet TV platform of 16 million registered users. The results show that, with a 32-node cluster, the system can effectively process 10.2 TB of behavior data every day, which is about 40x faster than original Hive-based system.},
booktitle = {Proceedings of the 11th International Conference on Future Internet Technologies},
pages = {36–42},
numpages = {7},
keywords = {Internet TV, cloud computing, User behavior analysis, SQL-on-Hadoop},
location = {Nanjing, China},
series = {CFI '16}
}

@inproceedings{10.1145/3211954.3211955,
author = {Seabolt, Ed and Kandogan, Eser and Roth, Mary},
title = {Contextual Intelligence for Unified Data Governance},
year = {2018},
isbn = {9781450358514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211954.3211955},
doi = {10.1145/3211954.3211955},
abstract = {Current data governance techniques are very labor-intensive, as teams of data stewards typically rely on best practices to transform business policies into governance rules. As data plays an increasingly key role in today's data-driven enterprises, current approaches do not scale to the complexity and variety present in the data ecosystem of an enterprise as an increasing number of data requirements, use cases, applications, tools and systems come into play. We believe techniques from artificial intelligence and machine learning have potential to improve discoverability, quality and compliance in data governance. In this paper, we propose a framework for 'contextual intelligence', where we argue for (1) collecting and integrating contextual metadata from variety of sources to establish a trusted unified repository of contextual data use across users and applications, and (2) applying machine learning and artificial intelligence techniques over this rich contextual metadata to improve discoverability, quality and compliance in governance practices. We propose an architecture that unifies governance across several systems, with a graph serving as a core repository of contextual metadata, accurately representing data usage across the enterprise and facilitating machine learning, We demonstrate how our approach can enable ML-based recommendations in support of governance best practices.},
booktitle = {Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {2},
numpages = {9},
keywords = {Graph, Context, Data Governance, Analytics},
location = {Houston, TX, USA},
series = {aiDM'18}
}

@article{10.1145/3446679,
author = {Celes, Clayson and Boukerche, Azzedine and Loureiro, Antonio A. F.},
title = {Mobility Trace Analysis for Intelligent Vehicular Networks: Methods, Models, and Applications},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3446679},
doi = {10.1145/3446679},
abstract = {Intelligent vehicular networks emerge as a promising technology to provide efficient data communication in transportation systems and smart cities. At the same time, the popularization of devices with attached sensors has allowed the obtaining of a large volume of data with spatiotemporal information from different entities. In this sense, we are faced with a large volume of vehicular mobility traces being recorded. Those traces provide unprecedented opportunities to understand the dynamics of vehicular mobility and provide data-driven solutions. In this article, we give an overview of the main publicly available vehicular mobility traces; then, we present the main issues for preprocessing these traces. Also, we present the methods used to characterize and model mobility data. Finally, we review existing proposals that apply the hidden knowledge extracted from the mobility trace for vehicular networks. This article provides a survey on studies that use vehicular mobility traces and provides a guideline for the proposition of data-driven solutions in the domain of vehicular networks. Moreover, we discuss open research problems and give some directions to undertake them.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {49},
numpages = {38},
keywords = {data mining, data analysis, routing, mobility, vanet, Vehicular networks, topology, survey}
}

@inproceedings{10.1145/3512850.3512861,
author = {Mei, Songzhu and Liu, Cong and Wang, Qinglin and Su, Huayou},
title = {Model Provenance Management in MLOps Pipeline},
year = {2022},
isbn = {9781450395717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512850.3512861},
doi = {10.1145/3512850.3512861},
booktitle = {2022 The 8th International Conference on Computing and Data Engineering},
pages = {45–50},
numpages = {6},
keywords = {Machine learning engineering, MLOps, Artificial Intelligence, Model provenance},
location = {Bangkok, Thailand},
series = {ICCDE 2022}
}

@article{10.1145/3131780,
author = {Lukyanenko, Roman and Samuel, Binny M.},
title = {Are All Classes Created Equal? Increasing Precision of Conceptual Modeling Grammars},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/3131780},
doi = {10.1145/3131780},
abstract = {Recent decade has seen a dramatic change in the information systems landscape that alters the ways we design and interact with information technologies, including such developments as the rise of business analytics, user-generated content, and NoSQL databases, to name just a few. These changes challenge conceptual modeling research to offer innovative solutions tailored to these environments. Conceptual models typically represent classes (categories, kinds) of objects rather than concrete specific objects, making the class construct a critical medium for capturing domain semantics. While representation of classes may differ between grammars, a common design assumption is what we term different semantics same syntax (D3S). Under D3S, all classes are depicted using the same syntactic symbols. Following recent findings in psychology, we introduce a novel assumption semantics-contingent syntax (SCS) whereby syntactic representations of classes in conceptual models may differ based on their semantic meaning. We propose a core SCS design principle and five guidelines pertinent for conceptual modeling. We believe SCS carries profound implications for theory and practice of conceptual modeling as it seeks to better support modern information environments.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {sep},
articleno = {14},
numpages = {15},
keywords = {Conceptual modeling, database design}
}

@inproceedings{10.1109/MSR.2019.00031,
author = {Ma, Yuxing and Bogart, Chris and Amreen, Sadika and Zaretzki, Russell and Mockus, Audris},
title = {World of Code: An Infrastructure for Mining the Universe of Open Source VCS Data},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00031},
doi = {10.1109/MSR.2019.00031},
abstract = {Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flows? To answer such questions we a) create a very large and frequently updated collection of version control data for FLOSS projects named World of Code (WoC) and b) provide basic tools for conducting research that depends on measuring interdependencies among all FLOSS projects. Our current WoC implementation is capable of being updated on a monthly basis and contains over 12B git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {143–154},
numpages = {12},
keywords = {software mining, software supply chain, software ecosystem},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@proceedings{10.1145/3545897,
title = {ICIEB '22: Proceedings of the 2022 3rd International Conference on Internet and E-Business},
year = {2022},
isbn = {9781450397322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Madrid, Spain}
}

@article{10.1145/3488717,
author = {Stoyanovich, Julia and Abiteboul, Serge and Howe, Bill and Jagadish, H. V. and Schelter, Sebastian},
title = {Responsible Data Management},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3488717},
doi = {10.1145/3488717},
abstract = {Perspectives on the role and responsibility of the data-management research community in designing, developing, using, and overseeing automated decision systems.},
journal = {Commun. ACM},
month = {may},
pages = {64–74},
numpages = {11}
}

@inproceedings{10.1145/3284103.3284114,
author = {Wang, Rongxiao and Chen, Bin and Wang, Yiduo and Zhu, Zhengqiu and Ma, Liang and Qiu, Xiaogang},
title = {The Air Contaminant Dispersion Prediction by the Integration of the Neural Network and AermodSystem},
year = {2018},
isbn = {9781450360449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284103.3284114},
doi = {10.1145/3284103.3284114},
abstract = {Air pollution caused by industrial production has become a serious problem for public health. This challenging problem promotes the development of the research in the air contaminant dispersion (ADS) prediction, for the management of the emission and leak accident. However, conventional ADS models can hardly meet the requirement of both accuracy and efficiency. The data model, like the artificial neural network (ANN) provides a feasible way of forecasting the dispersion with high accuracy and efficiency. However, the construction of the ANN for prediction needs plenty of data, which is impractical to obtain in most emission cases. To address this problem, an ADS simulation software AermodSystem is applied to build the simulated dispersion scenarios and provide synthetic dataset for the model training and test. Based on the synthetic data set, the ANN prediction model is established, and evaluated on the test set, as well as the Gaussian model. Further, these two models are served as the forward dispersion model and combined with the Particle Swarm Optimization (PSO) for source estimation. The results verify the effectiveness of the proposed model and indicate that the ANN together with the AermodSystem as the data generator is feasible in the air contaminant dispersion forecast and the source estimation of a particular case.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience},
articleno = {11},
numpages = {5},
keywords = {AermodSystem, Atmospheric dispersion prediction, Source estimation, Artificial neural network, Particle Swarm Optimization},
location = {Seattle, WA, USA},
series = {Safety and Resilience'18}
}

@inproceedings{10.1145/3510003.3510057,
author = {Biswas, Sumon and Wardat, Mohammad and Rajan, Hridesh},
title = {The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines in Theory, in-the-Small, and in-the-Large},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510057},
doi = {10.1145/3510003.3510057},
abstract = {Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large. Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2091–2103},
numpages = {13},
keywords = {descriptive, predictive, data science pipelines, data science processes},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3458027,
author = {Bromander, Siri and Swimmer, Morton and Muller, Lilly Pijnenburg and J\o{}sang, Audun and Eian, Martin and Skj\o{}tskift, Geir and Borg, Fredrik},
title = {Investigating Sharing of Cyber Threat Intelligence and Proposing A New Data Model for Enabling Automation in Knowledge Representation and Exchange},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2692-1626},
url = {https://doi.org/10.1145/3458027},
doi = {10.1145/3458027},
abstract = {For a strong, collective defense in the digital domain, we need to produce, consume, analyze, and share cyber threat intelligence. With an increasing amount of available information, we need automation to ensure adequate efficiency. We present the results from a questionnaire investigating the use of standards and standardization and how practitioners share and use cyber threat intelligence (CTI). We propose a strict data model for CTI that enables consumption of all relevant data, data validation, and analysis of consumed content. The main contribution of this article is insight into how CTI is shared and used by practitioners, and the strictness of the data model that enforces input of information and enables automation and deduction of new knowledge.},
journal = {Digital Threats},
month = {oct},
articleno = {6},
numpages = {22},
keywords = {security, ontology, Cyber threat intelligence, knowledge graph}
}

@article{10.1145/1462571.1462573,
author = {Agrawal, Rakesh and Ailamaki, Anastasia and Bernstein, Philip A. and Brewer, Eric A. and Carey, Michael J. and Chaudhuri, Surajit and Doan, AnHai and Florescu, Daniela and Franklin, Michael J. and Garcia-Molina, Hector and Gehrke, Johannes and Gruenwald, Le and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Korth, Hank F. and Kossmann, Donald and Madden, Samuel and Magoulas, Roger and Ooi, Beng Chin and O'Reilly, Tim and Ramakrishnan, Raghu and Sarawagi, Sunita and Stonebraker, Michael and Szalay, Alexander S. and Weikum, Gerhard},
title = {The Claremont Report on Database Research},
year = {2008},
issue_date = {September 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/1462571.1462573},
doi = {10.1145/1462571.1462573},
abstract = {In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds. We also report on discussions of the community's growth, including suggestions for changes in community processes to move the research agenda forward, and to enhance impact on a broader audience.},
journal = {SIGMOD Rec.},
month = {sep},
pages = {9–19},
numpages = {11}
}

@inproceedings{10.1145/2345396.2345413,
author = {Mehta, R. Vasanth Kumar and Sankarasubramaniam, B. and Rajalakshmi, S.},
title = {An Algorithm for Fuzzy-Based Sentence-Level Document Clustering for Micro-Level Contradiction Analysis},
year = {2012},
isbn = {9781450311960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2345396.2345413},
doi = {10.1145/2345396.2345413},
abstract = {Contradiction Analysis is one of the popular text-mining operations in which a document whose content is contradictory to the theme of a set of documents is identified. It is a means to identifying Outlier documents that do not confirm to the overall sense conveyed by other documents. Most of the existing techniques perform document-level comparisons, ignoring the sentence-level semantics, often leading to loss of vital information. Applications in domains like Defence and Healthcare require high levels of accuracy and identification of micro-level contradictions are vital. In this paper, we propose an algorithm for identifying contradictory documents using sentence-level clustering technique along with an optimization feature. A novel visualization scheme is also suggested to present the results to an end-user.},
booktitle = {Proceedings of the International Conference on Advances in Computing, Communications and Informatics},
pages = {102–105},
numpages = {4},
keywords = {information-retrieval, document clustering, contradiction analysis},
location = {Chennai, India},
series = {ICACCI '12}
}

@article{10.1145/2756547,
author = {Shekhar, Shashi and Feiner, Steven K. and Aref, Walid G.},
title = {Spatial Computing},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/2756547},
doi = {10.1145/2756547},
abstract = {Knowing where you are in space and time promises a deeper understanding of neighbors, ecosystems, and the environment.},
journal = {Commun. ACM},
month = {dec},
pages = {72–81},
numpages = {10}
}

@inproceedings{10.1145/3368089.3417039,
author = {Nguyen-Duc, Anh and Abrahamsson, Pekka},
title = {Continuous Experimentation on Artificial Intelligence Software: A Research Agenda},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417039},
doi = {10.1145/3368089.3417039},
abstract = {Moving from experiments to industrial level AI software development requires a shift from understanding AI/ ML model attributes as a standalone experiment to know-how integrating and operating AI models in a large-scale software system. It is a growing demand for adopting state-of-the-art software engineering paradigms into AI development, so that the development efforts can be aligned with business strategies in a lean and fast-paced manner. We describe AI development as an “unknown unknown” problem where both business needs and AI models evolve over time. We describe a holistic view of an iterative, continuous approach to develop industrial AI software basing on business goals, requirements and Minimum Viable Products. From this, five areas of challenges are presented with the focus on experimentation. In the end, we propose a research agenda with seven questions for future studies.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1513–1516},
numpages = {4},
keywords = {AI system, Industrial Artificial Intelligence, AI software, Continuous Experimentation},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1109/TNET.2019.2944984,
author = {Wu, Haiqin and Wang, Liangmin and Xue, Guoliang and Tang, Jian and Yang, Dejun},
title = {Enabling Data Trustworthiness and User Privacy in Mobile Crowdsensing},
year = {2019},
issue_date = {Dec. 2019},
publisher = {IEEE Press},
volume = {27},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2944984},
doi = {10.1109/TNET.2019.2944984},
abstract = {Ubiquitous mobile devices with rich sensors and advanced communication capabilities have given rise to mobile crowdsensing systems. The diverse reliabilities of mobile users and the openness of sensing paradigms raise concerns for data trustworthiness, user privacy, and incentive provision. Instead of considering these issues as isolated modules in most existing researches, we comprehensively capture both conflict and inner-relationship among them. In this paper, we propose a holistic solution for trustworthy and privacy-aware mobile crowdsensing with no need of a trusted third party. Specifically, leveraging cryptographic technologies, we devise a series of protocols to enable benign users to request tasks, contribute their data, and earn rewards anonymously without any data linkability. Meanwhile, an anonymous trust/reputation model is seamlessly integrated into our scheme, which acts as reference for our fair incentive design, and provides evidence to detect malicious users who degrade the data trustworthiness. Particularly, we first propose the idea of limiting the number of issued pseudonyms which serves to efficiently tackle the anonymity abuse issue. Security analysis demonstrates that our proposed scheme achieves stronger security with resilience against possible collusion attacks. Extensive simulations are presented which demonstrate the efficiency and practicality of our scheme.},
journal = {IEEE/ACM Trans. Netw.},
month = {dec},
pages = {2294–2307},
numpages = {14}
}

@inproceedings{10.1145/3170427.3170632,
author = {Thelisson, Eva and Sharma, Kshitij and Salam, Hanan and Dignum, Virginia},
title = {The General Data Protection Regulation: An Opportunity for the HCI Community?},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3170632},
doi = {10.1145/3170427.3170632},
abstract = {With HCI, researchers conduct studies in interdisciplinary projects involving massive volume of data, artificial intelligence and machine learning capabilities. Awareness of the responsibility is emerging as a key concern for the HCI community.This Community will be impacted by the General Data Protection Regulation (GDPR) [5], that will enter into force on the 25th of May 2018. From that date, each data controller and data processor will face an increase of its legal obligations (in particular its accountability) under certain conditions.The GDPR encourages the adoption of Soft Law mechanisms, approved by the national competent authority on data protection, to demonstrate the compliance to the Regulation. Approved Guidelines, Codes of Conducts, Labeling, Marks and Seals dedicated to data protection, as well as certification mechanisms are some of the options proposed by the GDPR.There may be discrepancies between the realities of HCI fieldwork and the formal process of obtaining Soft Law approval by Competent Authorities dedicated to data protection. Given these issues, it is important for researchers to reflect on legal and ethical encounters in HCI research as a community.This workshop will provide a forum for researchers to share experiences about Soft Law they have put in place to increase Trust, Transparency and Accountability among the shareholders. These discussions will be used to develop a white paper of practical Soft Law mechanisms (certification, labeling, marks, seals...) emerging in HCI research with the aim to demonstrate that the GDPR may be an opportunity for the HCI community.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {privacy, responsible innovation, quality standards, soft law, codes of conduct, design, labeling},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.5555/2346696.2346710,
author = {Wang, BingQiang and See, Simon},
title = {Accelerate High Throughput Analysis for Genome Sequencing with GPU},
year = {2012},
isbn = {9781450316446},
publisher = {A*STAR Computational Resource Centre},
address = {SGP},
booktitle = {Proceedings of the ATIP/A*CRC Workshop on Accelerator Technologies for High-Performance Computing: Does Asia Lead the Way?},
articleno = {11},
numpages = {46},
location = {Buona Vista, Singapore},
series = {ATIP '12}
}

@article{10.1145/2590989.2590995,
author = {Naumann, Felix},
title = {Data Profiling Revisited},
year = {2014},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2590989.2590995},
doi = {10.1145/2590989.2590995},
abstract = {Data profiling comprises a broad range of methods to efficiently analyze a given data set. In a typical scenario, which mirrors the capabilities of commercial data profiling tools, tables of a relational database are scanned to derive metadata, such as data types and value patterns, completeness and uniqueness of columns, keys and foreign keys, and occasionally functional dependencies and association rules. Individual research projects have proposed several additional profiling tasks, such as the discovery of inclusion dependencies or conditional functional dependencies.Data profiling deserves a fresh look for two reasons: First, the area itself is neither established nor defined in any principled way, despite significant research activity on individual parts in the past. Second, more and more data beyond the traditional relational databases are being created and beg to be profiled. The article proposes new research directions and challenges, including interactive and incremental profiling and profiling heterogeneous and non-relational data.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {40–49},
numpages = {10}
}

@inproceedings{10.1145/3486635.3491073,
author = {Rao, Jinmeng and Gao, Song and Zhu, Xiaojin},
title = {VTSV: A Privacy-Preserving Vehicle Trajectory Simulation and Visualization Platform Using Deep Reinforcement Learning},
year = {2021},
isbn = {9781450391207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486635.3491073},
doi = {10.1145/3486635.3491073},
abstract = {Trajectory data is among the most sensitive data and the society increasingly raises privacy concerns. In this demo paper, we present a privacy-preserving Vehicle Trajectory Simulation and Visualization (VTSV) web platform (demo video: https://youtu.be/NY5L4bu2kTU), which automatically generates navigation routes between given pairs of origins and destinations and employs a deep reinforcement learning model to simulate vehicle trajectories with customized driving behaviors such as normal driving, overspeed, aggressive acceleration, and aggressive turning. The simulated vehicle trajectory data contain high-sample-rate of attributes including GPS location, speed, acceleration, and steering angle, and such data are visualized in VTSV using streetscape.gl, an autonomous driving data visualization framework. Location privacy protection methods such as origin-destination geomasking and trajectory k-anonymity are integrated into the platform to support privacy-preserving trajectory data generation and publication. We design two application scenarios to demonstrate how VTSV performs location privacy protection and customize driving behavior, respectively. The demonstration shows that VTSV is able to mitigate data privacy, sparsity, and imbalance sampling issues, which offers new insights into driving trajectory simulation and GeoAI-powered privacy-preserving data publication.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
pages = {43–46},
numpages = {4},
keywords = {transportation, vehicle trajectory, data visualization, privacy protection, reinforcement learning},
location = {Beijing, China},
series = {GEOAI '21}
}

@inproceedings{10.1145/3381991.3395603,
author = {Momen, Nurul and Bock, Sven and Fritsch, Lothar},
title = {Accept - Maybe - Decline: Introducing Partial Consent for the Permission-Based Access Control Model of Android},
year = {2020},
isbn = {9781450375689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3381991.3395603},
doi = {10.1145/3381991.3395603},
abstract = {The consent to personal data sharing is an integral part of modern access control models on smart devices. This paper examines the possibility of registering conditional consent which could potentially increase trust in data sharing. We introduce an indecisive state of consenting to policies that will enable consumers to evaluate data services before fully committing to their data sharing policies. We address technical, regulatory, social, individual and economic perspectives for inclusion of partial consent within an access control mechanism. Then, we look into the possibilities to integrate it within the access control model of Android by introducing an additional button in the interface--Maybe. This article also presents a design for such implementation and demonstrates feasibility by showcasing a prototype built on Android platform. Our effort is exploratory and aims to shed light on the probable research direction.},
booktitle = {Proceedings of the 25th ACM Symposium on Access Control Models and Technologies},
pages = {71–80},
numpages = {10},
keywords = {partial consent, access control, privacy, data protection},
location = {Barcelona, Spain},
series = {SACMAT '20}
}

@inproceedings{10.1145/3328519.3329133,
author = {Rezig, El Kindi and Ouzzani, Mourad and Elmagarmid, Ahmed K. and Aref, Walid G. and Stonebraker, Michael},
title = {Towards an End-to-End Human-Centric Data Cleaning Framework},
year = {2019},
isbn = {9781450367912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328519.3329133},
doi = {10.1145/3328519.3329133},
abstract = {Data Cleaning refers to the process of detecting and fixing errors in the data. Human involvement is instrumental at several stages of this process such as providing rules or validating computed repairs. There is a plethora of data cleaning algorithms addressing a wide range of data errors (e.g., detecting duplicates, violations of integrity constraints, and missing values). Many of these algorithms involve a human in the loop, however, this latter is usually coupled to the underlying cleaning algorithms. In a real data cleaning pipeline, several data cleaning operations are performed using different tools. A high-level reasoning on these tools, when combined to repair the data, has the potential to unlock useful use cases to involve humans in the cleaning process. Additionally, we believe there is an opportunity to benefit from recent advances in active learning methods to minimize the effort humans have to spend to verify data items produced by tools or humans. There is currently no end-to-end data cleaning framework that systematically involves humans in the cleaning pipeline regardless of the underlying cleaning algorithms. In this paper, we present opportunities that this framework could offer, and highlight key challenges that need to be addressed to realize this vision. We present a design vision and discuss scenarios that motivate the need for this framework to judiciously assist humans in the cleaning process.},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {1},
numpages = {7},
location = {Amsterdam, Netherlands},
series = {HILDA'19}
}

@inproceedings{10.1145/3207677.3277983,
author = {Li, Pei and Dai, Chaofan and Wang, Wenqian},
title = {Inconsistent Data Detection Based on Maximum Dependency Set},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3277983},
doi = {10.1145/3207677.3277983},
abstract = {For1 the incomplete detection of inconsistent data by conditional functional dependency(CFDs), this paper proposes a dependency lifting algorithm (DLA) based on maximum dependency set (MDS), which detects inconsistent data in data set by acquiring recessive conditional functional dependencies (RCFDs) in CFDs. Presenting the dynamic domain adjustment, setting forward and backward pointers of numerical change to improve the enumeration process in original algorithm, the applicability of the algorithm to the continuous attributes is raised too. Then, this paper provides the algorithm flow and pseudo code of dynamic domain adjustment and the DLA, analyses the convergence and time complexity of them. Finally, the validity of the DLA is verified by comparing the detection accuracy and time-cost.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {63},
numpages = {8},
keywords = {dynamic domain adjustment, inconsistent data, maximum dependency set (MDS), Conditional functional dependency (CFDs)},
location = {Hohhot, China},
series = {CSAE '18}
}

@article{10.1145/3398020,
author = {Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv},
title = {Orchestrating the Development Lifecycle of Machine Learning-Based IoT Applications: A Taxonomy and Survey},
year = {2020},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3398020},
doi = {10.1145/3398020},
abstract = {Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {82},
numpages = {47},
keywords = {orchestration, IoT, machine learning, deep learning}
}

@article{10.1145/3552490.3552504,
author = {Fekete, Alan D. and R\"{o}hm, Uwe},
title = {Teaching about Data and Databases: Why, What, How?},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3552490.3552504},
doi = {10.1145/3552490.3552504},
abstract = {The panel on data(base) education at VLDB2021 [13] drew attention to important challenges in choosing how database classes are constructed for students in a world where data is being used in novel and impactful settings. This paper aims to present one view of a process for making these pedagogy decisions. We don't aim to present a best-possible design of the subject, rather we want to illuminate the space of possibilities, to encourage reasoned choices rather than simply teaching the subject as it was previously offered, or spending time on the latest innovations without considering the "opportunity cost" of doing so. We hope to guide the perplexed instructor or departmental curriculum committee.},
journal = {SIGMOD Rec.},
month = {jul},
pages = {52–60},
numpages = {9}
}

@inproceedings{10.1145/2729104.2729129,
author = {Lipuntsov, Yuri P.},
title = {Three Types of Data Exchange in the Open Government Information Projects},
year = {2014},
isbn = {9781450334013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2729104.2729129},
doi = {10.1145/2729104.2729129},
abstract = {Open government as a new system of public administration requires a qualitatively new level of information support for digital interactions of agencies, as well as between government and citizens, experts, and businesses. This article examines three categories of government counterparties, interactions with which are based on different principles: community of interest; subject areas; a loosely coupled environment. The public sector has now many information projects, and most of them deal with the data exchange, data delivery from and to the environment. The understanding of an environment is various for different projects. The categorization of projects depends on nature and the level of intellectual interaction with the external environment needed for the correct definition of project objectives, assessment of effectiveness. With the growing number of users and the transition to an open world, semantic principles are becoming more significant. Given the shift from systems integration to the semantic method, the role of subject matter expert is growing substantially.},
booktitle = {Proceedings of the 2014 Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {88–94},
numpages = {7},
keywords = {Linked Data, Data Standardization, Core Component, Shared Environment, Open Government, Open Data},
location = {St. Petersburg, Russian Federation},
series = {EGOSE '14}
}

