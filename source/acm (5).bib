@inproceedings{10.1145/3209281.3209317,
author = {Batubara, F. Rizal and Ubacht, Jolien and Janssen, Marijn},
title = {Challenges of Blockchain Technology Adoption for E-Government: A Systematic Literature Review},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209317},
doi = {10.1145/3209281.3209317},
abstract = {The ability of blockchain technology to record transactions on distributed ledgers offers new opportunities for governments to improve transparency, prevent fraud, and establish trust in the public sector. However, blockchain adoption and use in the context of e-Government is rather unexplored in academic literature. In this paper, we systematically review relevant research to understand the current research topics, challenges and future directions regarding blockchain adoption for e-Government. The results show that the adoption of blockchain-based applications in e-Government is still very limited and there is a lack of empirical evidence. The main challenges faced in blockchain adoption are predominantly presented as technological aspects such as security, scalability and flexibility. From an organizational point of view, the issues of acceptability and the need of new governance models are presented as the main barriers to adoption. Moreover, the lack of legal and regulatory support is identified as the main environmental barrier of adoption. Based on the challenges presented in the literature, we propose future research questions that need to be addressed to inform how the public sector should approach the blockchain technology adoption.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {76},
numpages = {9},
keywords = {blockchain, government, literature review, public service, adoption},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.5555/3192424.3192631,
author = {Musciotto, Federico and Delpriori, Saverio and Castagno, Paolo and Pournaras, Evangelos},
title = {Mining Social Interactions in Privacy-Preserving Temporal Networks},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {The opportunities to empirically study temporal networks nowadays are immense thanks to Internet of Things technologies along with ubiquitous and pervasive computing that allow a real-time fine-grained collection of social network data. This empowers data analytics and data scientists to reason about complex temporal phenomena, such as disease spread, residential energy consumption, political conflicts etc., using systematic methologies from complex networks and graph spectra analysis. However, a misuse of these methods may result in privacy-intrusive and discriminatory actions that may threaten citizens' autonomy and put their life under surveillance. This paper studies highly sparse temporal networks that model social interactions such as the physical proximity of participants in conferences. When citizens can self-determine the anonymized proximity data they wish to share via privacy-preserving platforms, temporal networks may turn out to be highly sparse and have low quality. This paper shows that even in this challenging scenario of privacy-by-design, significant information can be mined from temporal networks such as the correlation of events happening during a conference or stable groups interacting over time. The findings of this paper contribute to the introduction of privacy-preserving data analytics in temporal networks and their applications.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1103–1110},
numpages = {8},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.1145/3489525.3511699,
author = {Leznik, Mark and Grohmann, Johannes and Kliche, Nina and Bauer, Andr\'{e} and Seybold, Daniel and Eismann, Simon and Kounev, Samuel and Domaschka, J\"{o}rg},
title = {Same, Same, but Dissimilar: Exploring Measurements for Workload Time-Series Similarity},
year = {2022},
isbn = {9781450391436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489525.3511699},
doi = {10.1145/3489525.3511699},
abstract = {Benchmarking is a core element in the toolbox of most systems researchers and is used for analyzing, comparing, and validating complex systems. In the quest for reliable benchmark results, a consensus has formed that a significant experiment must be based on multiple runs. To interpret these runs, mean and standard deviation are often used. In case of experiments where each run produces a time series, applying and comparing the mean is not easily applicable and not necessarily statistically sound. Such an approach ignores the possibility of significant differences between runs with a similar average. In order to verify this hypothesis, we conducted a survey of 1,112 publications of selected performance engineering and systems conferences canvassing open data sets from performance experiments. The identified 3 data sets purely rely on average and standard deviation. Therefore, we propose a novel analysis approach based on similarity analysis to enhance the reliability of performance evaluations. Our approach evaluates 12 (dis-)similarity measures with respect to their applicability in analysing performance measurements and identifies four suitable similarity measures. We validate our approach by demonstrating the increase in reliability for the data sets found in the survey.},
booktitle = {Proceedings of the 2022 ACM/SPEC on International Conference on Performance Engineering},
pages = {89–96},
numpages = {8},
keywords = {workload analysis, distance metrics, time series similarity, data sets, time series},
location = {Beijing, China},
series = {ICPE '22}
}

@inbook{10.1145/3447404.3447423,
author = {McMenemy, David},
title = {Ethics and Secure Gestures},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447423},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {339–340},
numpages = {2}
}

@inbook{10.1145/3447404.3447405,
title = {Preface},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447405},
abstract = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice provides a comprehensive resource on what has become the dominant paradigm for novel interaction design methods involving gesture, speech, text, and touch embedded in novel and emerging interfaces. These interfaces support smartphones, wearables, in-vehicle devices, virtual reality, robotic, the Internet of Things (IoT), brain–computer interaction, and many other applications that are now highly competitive commercially.This edited collection is written by international experts and pioneers in the field of digital signal processing (DSP) and machine learning (ML) for interactive systems. It provides a textbook for students, and a reference and technology roadmap for developers and professionals working in interaction design on emerging platforms. This introductory textbook presents theory chapters on statistical grounding, signal processing, and ML foundations for guiding the development of novel interactive systems. Additional chapters discuss case studies on smart cities, brain–computer interfaces (BCI), probabilistic text entry, secure gestures, personal context from mobile phones, building adaptive touch interfaces, and automotive user interfaces (UIs). The chapters on case studies also highlight an in-depth look at domain-specific language (DSL) and ML methods used, for example, in touch, gesture, electroencephalography (EEG), electrocardiography (ECG), and galvanic skin response (GSR) signals, or embedded sensor inputs. A common theme throughout is the ubiquitous support for humans as they go about their daily professional or personal activities.This introductory book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal–multisensor systems. After each chapter an expert on the legal and ethical issues explores the wider ethical issues on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during interaction with novel platforms.Parisa Eslambolchilar, Andreas Komninos, and Mark D. Dunlop, March 2020AcknowledgmentsWe would like to thank our external reviewers for their valuable feedback throughout the writing process.},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {xv–xvi}
}

@article{10.1109/TNET.2018.2829173,
author = {Tu, Zhen and Xu, Fengli and Li, Yong and Zhang, Pengyu and Jin, Depeng},
title = {A New Privacy Breach: User Trajectory Recovery From Aggregated Mobility Data},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2829173},
doi = {10.1109/TNET.2018.2829173},
abstract = {Human mobility data have been ubiquitously collected through cellular networks and mobile applications, and publicly released for academic research and commercial purposes for the last decade. Since releasing individual’s mobility records usually gives rise to privacy issues, data sets owners tend to only publish aggregated mobility data, such as the number of users covered by a cellular tower at a specific timestamp, which is believed to be sufficient for preserving users’ privacy. However, in this paper, we argue and prove that even publishing aggregated mobility data could lead to privacy breach in individuals’ trajectories. We develop an attack system that is able to exploit the uniqueness and regularity of human mobility to recover individual’s trajectories from the aggregated mobility data without any prior knowledge. By conducting experiments on two real-world data sets collected from both the mobile application and cellular network, we reveal that the attack system is able to recover users’ trajectories with an accuracy of about 73%~91% at the scale of thousands to ten thousands of mobile users, which indicates severe privacy leakage in such data sets. Our extensive analysis also reveals that by generalization and perturbation, this kind of privacy leakage can only be mitigated. Through the investigation on aggregated mobility data, this paper recognizes a novel privacy problem in publishing statistic data, which appeals for immediate attentions from both the academy and industry.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1446–1459},
numpages = {14}
}

@inproceedings{10.1145/3287560.3287577,
author = {Young, Meg and Rodriguez, Luke and Keller, Emily and Sun, Feiyang and Sa, Boyang and Whittington, Jan and Howe, Bill},
title = {Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287577},
doi = {10.1145/3287560.3287577},
abstract = {Data too sensitive to be "open" for analysis and re-purposing typically remains "closed" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {191–200},
numpages = {10},
keywords = {data sharing, data ethics, algorithmic bias, privacy, data governance},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{10.1145/2988336.2988349,
author = {Singh, Jatinder and Pasquier, Thomas and Bacon, Jean and Powles, Julia and Diaconu, Raluca and Eyers, David},
title = {Big Ideas Paper: Policy-Driven Middleware for a Legally-Compliant Internet of Things},
year = {2016},
isbn = {9781450343008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2988336.2988349},
doi = {10.1145/2988336.2988349},
abstract = {Internet of Things (IoT) applications, systems and services are subject to law. We argue that for the IoT to develop lawfully, there must be technical mechanisms that allow the enforcement of specified policy, such that systems align with legal realities. The audit of policy enforcement must assist the apportionment of liability, demonstrate compliance with regulation, and indicate whether policy correctly captures legal responsibilities. As both systems and obligations evolve dynamically, this cycle must be continuously maintained.This poses a huge challenge given the global scale of the IoT vision. The IoT entails dynamically creating new services through managed and flexible data exchange. Data management is complex in this dynamic environment, given the need to both control and share information, often across federated domains of administration.We see middleware playing a key role in managing the IoT. Our vision is for a middleware-enforced, unified policy model that applies end-to-end, throughout the IoT. This is because policy cannot be bound to things, applications, or administrative domains, since functionality is the result of composition, with dynamically formed chains of data flows.We have investigated the use of Information Flow Control (IFC) to manage and audit data flows in cloud computing; a domain where trust can be well-founded, regulations are more mature and associated responsibilities clearer. We feel that IFC has great potential in the broader IoT context. However, the sheer scale and the dynamic, federated nature of the IoT pose a number of significant research challenges.},
booktitle = {Proceedings of the 17th International Middleware Conference},
articleno = {13},
numpages = {15},
keywords = {policy specification and enforcement, Law, regulation, audit},
location = {Trento, Italy},
series = {Middleware '16}
}

@article{10.1145/3531533,
author = {Gram, Dennis and Karapanagiotis, Pantelis and Liebald, Marius and Walz, Uwe},
title = {Design and Implementation of a Historical German Firm-Level Financial Database},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3531533},
doi = {10.1145/3531533},
abstract = {Broad, long-term financial, and economic datasets are scarce resources, particularly in the European context. In this article, we present an approach for an extensible data model that is adaptable to future changes in technologies and sources. This model may constitute a basis for digitized and structured long-term historical datasets for different jurisdictions and periods. The data model covers the specific peculiarities of historical financial and economic data and is flexible enough to reach out for data of different types (quantitative as well as qualitative) from different historical sources, hence, achieving extensibility. Furthermore, we outline a relational implementation of this approach based on historical German firm and stock market data from 1920 to 1932.},
journal = {J. Data and Information Quality},
month = {jun},
articleno = {19},
numpages = {22},
keywords = {financial data, Germany, economic history, cliometrics, Databases}
}

@inproceedings{10.5555/3433701.3433811,
author = {Grosset, Pascal and Biwer, Christopher M. and Pulido, Jesus and Mohan, Arvind T. and Biswas, Ayan and Patchett, John and Turton, Terece L. and Rogers, David H. and Livescu, Daniel and Ahrens, James},
title = {Foresight: Analysis That Matters for Data Reduction},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {As the computation power of supercomputers increases, so does simulation size, which in turn produces orders-of-magnitude more data. Because generated data often exceed the simulation's disk quota, many simulations would stand to benefit from data-reduction techniques to reduce storage requirements. Such techniques include autoencoders, data compression algorithms, and sampling. Lossy compression techniques can significantly reduce data size, but such techniques come at the expense of losing information that could result in incorrect post hoc analysis results. To help scientists determine the best compression they can get while keeping their analyses accurate, we have developed Foresight, an analysis framework that enables users to evaluate how different data-reduction techniques will impact their analyses. We use particle data from a cosmology simulation, turbulence data from Direct Numerical Simulation, and asteroid impact data from xRage to demonstrate how Foresight can help scientists determine the best data-reduction technique for their simulations.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {83},
numpages = {15},
keywords = {performance evaluation, multi-layer neural network, data compression},
location = {Atlanta, Georgia},
series = {SC '20}
}

@article{10.1145/3512944,
author = {Gupta, Srishti and Jablonski, Julia and Tsai, Chun-Hua and Carroll, John M.},
title = {Instagram of Rivers: Facilitating Distributed Collaboration in Hyperlocal Citizen Science},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512944},
doi = {10.1145/3512944},
abstract = {Citizen science project leaders collecting field data in a hyperlocal community often face common socio-technical challenges, which can potentially be addressed by sharing innovations across different groups through peer-to-peer collaboration. However, most citizen science groups practice in isolation, and end up re-inventing the wheel when it comes to addressing these common challenges. This study seeks to investigate distributed collaboration between different water monitoring citizen science groups. We discovered a unique social network application called Water Reporter that mediated distributed collaboration by creating more visibility and transparency between groups using the app. We interviewed 8 citizen science project leaders who were users of this app, and 6 other citizen science project leaders to understand how distributed collaboration mediated by this app differed from collaborative practices of Non Water Reporter users. We found that distributed collaboration was an important goal for both user groups, however, the tasks that support these collaboration activities differed for the two user groups.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {97},
numpages = {22},
keywords = {sustainability, citizen science, distributed collaboration, collaboratory}
}

@inbook{10.1145/3447404.3447417,
author = {McMenemy, David},
title = {Ethics and Smart Cities},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447417},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {235–236},
numpages = {2}
}

@inproceedings{10.5555/3103620.3103631,
author = {Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua},
title = {Validating a Deep Learning Framework by Metamorphic Testing},
year = {2017},
isbn = {9781538604243},
publisher = {IEEE Press},
abstract = {Deep learning has become an important tool for image classification and natural language processing. However, the effectiveness of deep learning is highly dependent on the quality of the training data as well as the net model for the learning. The training data set for deep learning normally is fairly large, and the net model is pretty complex. It is necessary to validate the deep learning framework including the net model, executing environment, and training data set before it is used for any applications. In this paper, we propose an approach for validating the classification accuracy of a deep learning framework that includes a convolutional neural network, a deep learning executing environment, and a massive image data set. The framework is first validated with a classifier built on support vector machine, and then it is tested using a metamorphic validation approach. The effectiveness of the approach is demonstrated by validating a deep learning classifier for automated classification of biology cell images. The proposed approach can be used for validating other deep learning framework for different applications.},
booktitle = {Proceedings of the 2nd International Workshop on Metamorphic Testing},
pages = {28–34},
numpages = {7},
keywords = {support vector machine, deep learning, metamorphic testing, neural network, software validation},
location = {Buenos Aires, Argentina},
series = {MET '17}
}

@inproceedings{10.5555/3242181.3242206,
author = {McGinnis, Leon F. and Rose, Oliver},
title = {History and Perspective of Simulation in Manufacturing},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {Manufacturing systems incorporate many semi-independent, yet strongly interacting processes, usually exhibiting some stochastic behavior. As a consequence, overall system behavior, in the long run but also in the short run, is very difficult to predict. Not surprisingly, both practitioners and academics recognized in the 1950's the potential value of discrete event simulation technology in supporting manufacturing system decision-making. This short history is one perspective on the development and evolution of discrete event simulation technology and applications, specifically focusing on manufacturing applications. This assessment is based on an examination of the literature, our own experiences, and interviews with leading practitioners. History is interesting, but it's useful only if it helps us see a way forward, so we offer some opinions on the state of the research and practice of simulation in manufacturing, and the opportunities to further advance the field.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {24},
numpages = {13},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inbook{10.1145/3447404.3447419,
author = {McMenemy, David},
title = {Ethical Issues in Brain–Computer Interfaces},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447419},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {273–275},
numpages = {3}
}

@inproceedings{10.1145/3442188.3445918,
author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445918},
doi = {10.1145/3442188.3445918},
abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {560–575},
numpages = {16},
keywords = {datasets, machine learning, requirements engineering},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{10.1145/3371158.3371167,
author = {Bansal, Chahat and Jain, Arpit and Barwaria, Phaneesh and Choudhary, Anuj and Singh, Anupam and Gupta, Ayush and Seth, Aaditeshwar},
title = {Temporal Prediction of Socio-Economic Indicators Using Satellite Imagery},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371167},
doi = {10.1145/3371158.3371167},
abstract = {Machine learning models based on satellite data have been actively researched to serve as a proxy for the prediction of socio-economic development indicators. Such models have however rarely been tested for transferability over time, i.e. whether models learned on data for a certain year are able to make accurate predictions on data for another year. Using a dataset from the Indian census at two time points, for the years 2001 and 2011, we evaluate the temporal transferability of a simple machine learning model at sub-national scales of districts and propose a generic method to improve its performance. This method can be especially relevant when training datasets are small to train a robust prediction model. Then, we go further to build an aggregate development index at the district-level, on the lines of the Human Development Index (HDI) and demonstrate high accuracy in predicting the index based on satellite data for different years. This can be used to build applications to guide data-driven policy making at fine spatial and temporal scales, without the need to conduct frequent expensive censuses and surveys on the ground.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {73–81},
numpages = {9},
keywords = {socio-economic development, Landsat, satellite imagery, census, temporal prediction, poverty mapping},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@proceedings{10.1145/3551690,
title = {ICICM '22: Proceedings of the 12th International Conference on Information Communication and Management},
year = {2022},
isbn = {9781450396493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {London, United Kingdom}
}

@article{10.1145/2724730,
author = {Basole, Rahul C. and Russell, Martha G. and Huhtam\"{a}ki, Jukka and Rubens, Neil and Still, Kaisa and Park, Hyunwoo},
title = {Understanding Business Ecosystem Dynamics: A Data-Driven Approach},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/2724730},
doi = {10.1145/2724730},
abstract = {Business ecosystems consist of a heterogeneous and continuously evolving set of entities that are interconnected through a complex, global network of relationships. However, there is no well-established methodology to study the dynamics of this network. Traditional approaches have primarily utilized a single source of data of relatively established firms; however, these approaches ignore the vast number of relevant activities that often occur at the individual and entrepreneurial levels. We argue that a data-driven visualization approach, using both institutionally and socially curated datasets, can provide important complementary, triangulated explanatory insights into the dynamics of interorganizational networks in general and business ecosystems in particular. We develop novel visualization layouts to help decision makers systemically identify and compare ecosystems. Using traditionally disconnected data sources on deals and alliance relationships (DARs), executive and funding relationships (EFRs), and public opinion and discourse (POD), we empirically illustrate our data-driven method of data triangulation and visualization techniques through three cases in the mobile industry Google’s acquisition of Motorola Mobility, the coopetitive relation between Apple and Samsung, and the strategic partnership between Nokia and Microsoft. The article concludes with implications and future research opportunities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {jun},
articleno = {6},
numpages = {32},
keywords = {information visualization, Data triangulation, business ecosystem, interorganizational networks}
}

@inproceedings{10.1145/3529190.3534768,
author = {Appenzeller, Arno and Terzer, Nick and Krempel, Erik and Beyerer, J\"{u}rgen},
title = {Towards Private Medical Data Donations by Using Privacy Preserving Technologies},
year = {2022},
isbn = {9781450396318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529190.3534768},
doi = {10.1145/3529190.3534768},
abstract = {Through the growing amount of personal health data collected by the individual itself digital data donations become more and more attractive. Wearables like Apple Watch or Fitbit trackers make tracking of heart rate, daily step counts and other lifestyle data easier than ever. While this data is collected on the dedicated device, it can help research in many promising ways. Even if the potential benefit of this data is very clear, there are open questions regarding privacy. Traditional privatization measures like anonymization and pseudonymization can only provide limited privacy guarantees especially with the growing amount of personalized data. To mitigate those risks privacy enhancing technologies like differential privacy can be used. While the theoretical foundation of such technologies is strong, only limited data is available about their practical use in large scale applications and the trade-off between privacy and utility. In this paper we will present a data donation scenario that is inspired by a real-world use case using lifestyle data for its analyses. We will apply the local differential privacy technology ”RAPPOR” to improve the privacy protection for the data donors and evaluate the impact of this technique to the data utility.},
booktitle = {Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {446–454},
numpages = {9},
keywords = {Differential Privacy, Medical data protection, Privacy Enhancing Technology, Data donation},
location = {Corfu, Greece},
series = {PETRA '22}
}

@inproceedings{10.1145/2464464.2464475,
author = {Rieder, Bernhard},
title = {Studying Facebook via Data Extraction: The Netvizz Application},
year = {2013},
isbn = {9781450318891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2464464.2464475},
doi = {10.1145/2464464.2464475},
abstract = {This paper describes Netvizz, a data collection and extraction application that allows researchers to export data in standard file formats from different sections of the Facebook social networking service. Friendship networks, groups, and pages can thus be analyzed quantitatively and qualitatively with regards to demographical, post-demographical, and relational characteristics. The paper provides an overview over analytical directions opened up by the data made available, discusses platform specific aspects of data extraction via the official Application Programming Interface, and briefly engages the difficult ethical considerations attached to this type of research.},
booktitle = {Proceedings of the 5th Annual ACM Web Science Conference},
pages = {346–355},
numpages = {10},
keywords = {data extraction, social networking services, social network analysis, Facebook, research tool, media studies},
location = {Paris, France},
series = {WebSci '13}
}

@inproceedings{10.1145/2213836.2213961,
author = {Morton, Kristi and Bunker, Ross and Mackinlay, Jock and Morton, Robert and Stolte, Chris},
title = {Dynamic Workload Driven Data Integration in Tableau},
year = {2012},
isbn = {9781450312479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2213836.2213961},
doi = {10.1145/2213836.2213961},
abstract = {Tableau is a commercial business intelligence (BI) software tool that supports interactive, visual analysis of data. Armed with a visual interface to data and a focus on usability, Tableau enables a wide audience of end-users to gain insight into their datasets. The user experience is a fluid process of interaction in which exploring and visualizing data takes just a few simple drag-and-drop operations (no programming or DB experience necessary). In this context of exploratory, ad-hoc visual analysis, we describe a novel approach to integrating large, heterogeneous data sources. We present a new feature in Tableau called data blending, which gives users the ability to create data visualization mashups from structured, heterogeneous data sources dynamically without any upfront integration effort. Users can author visualizations that automatically integrate data from a variety of sources, including data warehouses, data marts, text files, spreadsheets, and data cubes. Because our data blending system is workload driven, we are able to bypass many of the pain-points and uncertainty in creating mediated schemas and schema-mappings in current pay-as-you-go integration systems.},
booktitle = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
pages = {807–816},
numpages = {10},
keywords = {visualization, data integration},
location = {Scottsdale, Arizona, USA},
series = {SIGMOD '12}
}

@inproceedings{10.1145/2723372.2750549,
author = {Wang, Xiaolan and Dong, Xin Luna and Meliou, Alexandra},
title = {Data X-Ray: A Diagnostic Tool for Data Errors},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2750549},
doi = {10.1145/2723372.2750549},
abstract = {A lot of systems and applications are data-driven, and the correctness of their operation relies heavily on the correctness of their data. While existing data cleaning techniques can be quite effective at purging datasets of errors, they disregard the fact that a lot of errors are systematic, inherent to the process that produces the data, and thus will keep occurring unless the problem is corrected at its source. In contrast to traditional data cleaning, in this paper we focus on data diagnosis: explaining where and how the errors happen in a data generative process.We develop a large-scale diagnostic framework called DATA X-RAY. Our contributions are three-fold. First, we transform the diagnosis problem to the problem of finding common properties among erroneous elements, with minimal domain-specific assumptions. Second, we use Bayesian analysis to derive a cost model that implements three intuitive principles of good diagnoses. Third, we design an efficient, highly-parallelizable algorithm for performing data diagnosis on large-scale data. We evaluate our cost model and algorithm using both real-world and synthetic data, and show that our diagnostic framework produces better diagnoses and is orders of magnitude more efficient than existing techniques.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1231–1245},
numpages = {15},
keywords = {error diagnosis, data cleaning, data profiling},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/2882903.2915232,
author = {Fan, Wenfei and Wu, Yinghui and Xu, Jingbo},
title = {Functional Dependencies for Graphs},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2915232},
doi = {10.1145/2882903.2915232},
abstract = {We propose a class of functional dependencies for graphs, referred to as GFDs. GFDs capture both attribute-value dependencies and topological structures of entities, and subsume conditional functional dependencies (CFDs) as a special case. We show that the satisfiability and implication problems for GFDs are coNP-complete and NP-complete, respectively, no worse than their CFD counterparts. We also show that the validation problem for GFDs is coNP-complete. Despite the intractability, we develop parallel scalable algorithms for catching violations of GFDs in large-scale graphs. Using real-life and synthetic data, we experimentally verify that GFDs provide an effective approach to detecting inconsistencies in knowledge and social graphs.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {1843–1857},
numpages = {15},
keywords = {implication, validation, graphs, satisfiability, functional dependencies},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/3548777,
author = {Elor, Aviv and Whittaker, Steve and Kurniawan, Sri and Michael, Sam},
title = {BioLumin: An Immersive Mixed Reality Experience for Interactive Microscopic Visualization and Biomedical Research Annotation},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2691-1957},
url = {https://doi.org/10.1145/3548777},
doi = {10.1145/3548777},
abstract = {Many recent breakthroughs in medical diagnostics and drug discovery arise from deploying machine learning algorithms to large-scale data sets. However, a significant obstacle to such approaches is that they depend on high-quality annotations generated by domain experts. This study develops and evaluates BioLumin, a novel immersive mixed reality environment that enables users to virtually shrink down to the microscopic level for navigation and annotation of 3D reconstructed images. We discuss how domain experts were consulted in the specification of a pipeline to enable automatic reconstruction of biological models for mixed reality environments, driving the design of a 3DUI system to explore whether such a system allows accurate annotation of complex medical data by non-experts. To examine the usability and feasibility of BioLumin, we evaluated our prototype through a multi-stage mixed-method approach. First, three domain experts offered expert reviews, and subsequently, nineteen non-expert users performed representative annotation tasks in a controlled setting. The results indicated that the mixed reality system was learnable and that non-experts could generate high-quality 3D annotations after a short training session. Lastly, we discuss design considerations for future tools like BioLumin in medical and more general scientific contexts.},
journal = {ACM Trans. Comput. Healthcare},
month = {nov},
articleno = {44},
numpages = {28},
keywords = {mixed reality, biomedical visualization, spatial computing, magic leap, interactive visualization, human-computer interaction, Immersive technologies}
}

@article{10.14778/3554821.3554825,
author = {Das, Prakash and Srivastava, Shivangi and Moskovich, Valentin and Chaturvedi, Anmol and Mittal, Anant and Xiao, Yongqin and Chowdhury, Mosharaf},
title = {CDI-E: An Elastic Cloud Service for Data Engineering},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554825},
doi = {10.14778/3554821.3554825},
abstract = {We live in the gilded age of data-driven computing. With public clouds offering virtually unlimited amounts of compute and storage, enterprises collecting data about every aspect of their businesses, and advances in analytics and machine learning technologies, data driven decision making is now timely, cost-effective, and therefore, pervasive. Alas, only a handful of power users can wield today's powerful data engineering tools. For one thing, most solutions require knowledge of specific programming interfaces or libraries. Furthermore, running them requires complex configurations and knowledge of the underlying cloud for cost-effectiveness.We decided that a fundamental redesign is in order to democratize data engineering for the masses at cloud scale. The result is Informatica Cloud Data Integration - Elastic (CDI-E). Since the early 1990s, Informatica has been a pioneer and industry leader in building no-code data engineering tools. Non-experts can express complex data engineering tasks using a graphical user interface (GUI). Informatica CDI-E is built to incorporate the simplicity of GUI in the design layer with an elastic and highly scalable run time to handle data in any format without little to no user input using automated optimizations. Users upload their data to the cloud in any format and can immediately use them in conjunction with their data management and analytic tools of choice using CDI-E GUI. Implementation began in the Spring of 2017, and Informatica CDI-E has been generally available since the Summer of 2019. Today, CDI-E is used in production by a growing number of small and large enterprises to make sense of data in arbitrary formats.In this paper, we describe the architecture of Informatica CDI-E and its novel no-code data engineering interface. The paper highlights some of the key features of CDI-E: simplicity without loss in productivity and extreme elasticity. It concludes with lessons we learned and an outlook of the future.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3319–3331},
numpages = {13}
}

@article{10.1145/3379504,
author = {Wu, Jian and Sheng, Victor S. and Zhang, Jing and Li, Hua and Dadakova, Tetiana and Swisher, Christine Leon and Cui, Zhiming and Zhao, Pengpeng},
title = {Multi-Label Active Learning Algorithms for Image Classification: Overview and Future Promise},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3379504},
doi = {10.1145/3379504},
abstract = {Image classification is a key task in image understanding, and multi-label image classification has become a popular topic in recent years. However, the success of multi-label image classification is closely related to the way of constructing a training set. As active learning aims to construct an effective training set through iteratively selecting the most informative examples to query labels from annotators, it was introduced into multi-label image classification. Accordingly, multi-label active learning is becoming an important research direction. In this work, we first review existing multi-label active learning algorithms for image classification. These algorithms can be categorized into two top groups from two aspects respectively: sampling and annotation. The most important component of multi-label active learning is to design an effective sampling strategy that actively selects the examples with the highest informativeness from an unlabeled data pool, according to various information measures. Thus, different informativeness measures are emphasized in this survey. Furthermore, this work also makes a deep investigation on existing challenging issues and future promises in multi-label active learning with a focus on four core aspects: example dimension, label dimension, annotation, and application extension.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {28},
numpages = {35},
keywords = {annotation, Image classification, active learning, sampling strategy, multi-label image}
}

@inproceedings{10.1145/3506860.3506939,
author = {Tsai, Yi-Shan and Singh, Shaveen and Rakovic, Mladen and Lim, Lisa-Angelique and Roychoudhury, Anushka and Gasevic, Dragan},
title = {Charting Design Needs and Strategic Approaches for Academic Analytics Systems through Co-Design},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506939},
doi = {10.1145/3506860.3506939},
abstract = {Academic analytics focuses on collecting, analysing and visualising educational data to generate institutional insights and improve decision-making for academic purposes. However, challenges that arise from navigating a complex organisational structure when introducing analytics systems have called for the need to engage key stakeholders widely to cultivate a shared vision and ensure that implemented systems create desired value. This paper presents a study that takes co-design steps to identify design needs and strategic approaches for the adoption of academic analytics, which serves the purpose of enhancing the measurement of educational quality utilising institutional data. Through semi-structured interviews with 54 educational stakeholders at a large research university, we identified particular interest in measuring student engagement and the performance of courses and programmes. Based on the observed perceptions and concerns regarding data use to measure or evaluate these areas, implications for adoption strategy of academic analytics, such as leadership involvement, communication, and training, are discussed.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {381–391},
numpages = {11},
keywords = {implementation strategy, educational quality, higher education, academic analytics, co-design},
location = {Online, USA},
series = {LAK22}
}

@article{10.1109/TCBB.2015.2474376,
author = {Lee, En-Shiun Annie and Sze-To, Ho-Yin Antonio and Wong, Man-Hon and Leung, Kwong-Sak and Lau, Terrence Chi-Kong and Wong, Andrew K. C.},
title = {Discovering Protein-DNA Binding Cores by Aligned Pattern Clustering},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2474376},
doi = {10.1109/TCBB.2015.2474376},
abstract = {Understanding binding cores is of fundamental importance in deciphering Protein-DNA TF-TFBS binding and gene regulation. Limited by expensive experiments, it is promising to discover them with variations directly from sequence data. Although existing computational methods have produced satisfactory results, they are one-to-one mappings with no site-specific information on residue/nucleotide variations, where these variations in binding cores may impact binding specificity. This study presents a new representation for modeling binding cores by incorporating variations and an algorithm to discover them from only sequence data. Our algorithm takes protein and DNA sequences from TRANSFAC a Protein-DNA Binding Database as input; discovers from both sets of sequences conserved regions in Aligned Pattern Clusters APCs; associates them as Protein-DNA Co-Occurring APCs; ranks the Protein-DNA Co-Occurring APCs according to their co-occurrence, and among the top ones, finds three-dimensional structures to support each binding core candidate. If successful, candidates are verified as binding cores. Otherwise, homology modeling is applied to their close matches in PDB to attain new chemically feasible binding cores. Our algorithm obtains binding cores with higher precision and much faster runtime (≥1,600x) than that of its contemporaries, discovering candidates that do not co-occur as one-to-one associated patterns in the raw data. Availability: http://www.pami.uwaterloo.ca/~ealee/files/tcbbPnDna2015/Release.zip.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {254–263},
numpages = {10}
}

@article{10.1145/3487043,
author = {Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan},
title = {Software Engineering for AI-Based Systems: A Survey},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3487043},
doi = {10.1145/3487043},
abstract = {AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {37e},
numpages = {59},
keywords = {Software engineering, systematic mapping study, AI-based systems, artificial intelligence}
}

@inproceedings{10.1145/3131542.3131564,
author = {Missier, Paolo and Bajoudah, Shaimaa and Capossele, Angelo and Gaglione, Andrea and Nati, Michele},
title = {Mind My Value: A Decentralized Infrastructure for Fair and Trusted IoT Data Trading},
year = {2017},
isbn = {9781450353182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131542.3131564},
doi = {10.1145/3131542.3131564},
abstract = {Internet of Things (IoT) data are increasingly viewed as a new form of massively distributed and large scale digital assets, which are continuously generated by millions of connected devices. The real value of such assets can only be realized by allowing IoT data trading to occur on a marketplace that rewards every single producer and consumer, at a very granular level. Crucially, we believe that such a marketplace should not be owned by anybody, and should instead fairly and transparently self-enforce a well defined set of governance rules. In this paper we address some of the technical challenges involved in realizing such a marketplace. We leverage emerging blockchain technologies to build a decentralized, trusted, transparent and open architecture for IoT traffic metering and contract compliance, on top of the largely adopted IoT brokered data infrastructure. We discuss an Ethereum-based prototype implementation and experimentally evaluate the overhead cost associated with Smart Contract transactions, concluding that a viable business model can indeed be associated with our technical approach.},
booktitle = {Proceedings of the Seventh International Conference on the Internet of Things},
articleno = {15},
numpages = {8},
location = {Linz, Austria},
series = {IoT '17}
}

@inproceedings{10.1145/3512290.3528801,
author = {Angrick, Sebastian and Bals, Ben and Hastrich, Niko and Kleissl, Maximilian and Schmidt, Jonas and Dosko\v{c}, Vanja and Molitor, Louise and Friedrich, Tobias and Katzmann, Maximilian},
title = {Towards Explainable Real Estate Valuation via Evolutionary Algorithms},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528801},
doi = {10.1145/3512290.3528801},
abstract = {Human lives are increasingly influenced by algorithms, which therefore need to meet higher standards not only in accuracy but also with respect to explainability. This is especially true for high-stakes areas such as real estate valuation. Unfortunately, the methods applied there often exhibit a trade-off between accuracy and explainability.One explainable approach is case-based reasoning (CBR), where each decision is supported by specific previous cases. However, such methods can be wanting in accuracy. The unexplainable machine learning approaches are often observed to provide higher accuracy but are not scrutable in their decision-making.In this paper, we apply evolutionary algorithms (EAs) to CBR predictors in order to improve their performance. In particular, we deploy EAs to the similarity functions (used in CBR to find comparable cases), which are fitted to the data set at hand. As a consequence, we achieve higher accuracy than state-of-the-art deep neural networks (DNNs), while keeping interpretability and explainability.These results stem from our empirical evaluation on a large data set of real estate offers where we compare known similarity functions, their EA-improved counterparts, and DNNs. Surprisingly, DNNs are only on par with standard CBR techniques. However, using EA-learned similarity functions does yield an improved performance.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1130–1138},
numpages = {9},
keywords = {evolutionary algorithms, real estate valuation, case-based reasoning, neural networks},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3428502.3428508,
author = {Viscusi, Gianluigi and Collins, Aengus and Florin, Marie-Valentine},
title = {Governments' Strategic Stance toward Artificial Intelligence: An Interpretive Display on Europe},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428508},
doi = {10.1145/3428502.3428508},
abstract = {This article aims to provide an interpretive display of the strategic stance toward innovation enabled by Artificial Intelligence (AI) of different governments based in Europe. The analysis includes the European Union (EU), some of its members as well as a non-member country, which we argue presents interesting characteristics. Based on a comprehensive analysis of a corpus of documents, which includes national strategies, external reports as well as web resources, the different countries considered in this article are subsequently classified using as interpretive lens, among other frameworks, the strategic types identified by Miles and Snow (defender, prospector, analyzer, reactor). The results show a prevalence of a "prospector" stance, interested in differentiation and the search of novelty. However, the results also show that similar strategic types may be driven by different values as well as governance orientation among the considered countries, thus leading to different potential ways to implement the expected AI-enabled innovation.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {44–53},
numpages = {10},
keywords = {Strategy, Strategic Types, Governance, Artificial Intelligence, Miles and Snow, Public Sector, Digitalization, Policies},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@inproceedings{10.1145/3505711.3505712,
author = {Bala Bisandu, Desmond and Salih Homaid, Mohammed and Moulitsas, irene and Filippone, Salvatore},
title = {A Deep Feedforward Neural Network and Shallow Architectures Effectiveness Comparison: Flight Delays Classification Perspective},
year = {2022},
isbn = {9781450390699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505711.3505712},
doi = {10.1145/3505711.3505712},
abstract = {Flight delays have negatively impacted the socio-economics state of passengers, airlines and airports, resulting in huge economic losses. Hence, it has become necessary to correctly predict their occurrences in decision-making because it is important for the effective management of the aviation industry. Developing accurate flight delays classification models depends mostly on the air transportation system complexity and the infrastructure available in airports, which may be a region-specific issue. However, no specific prediction or classification model can handle the individual characteristics of all airlines and airports at the same time. Hence, the need to further develop and compare predictive models for the aviation decision system of the future cannot be over-emphasised. In this research, flight on-time data records from the United State Bureau of Transportation Statistics was employed to evaluate the performances of Deep Feedforward Neural Network, Neural Network, and Support Vector Machine models on a binary classification problem. The research revealed that the models achieved different accuracies of flight delay classifications. The Support Vector Machine had the worst average accuracy than Neural Network and Deep Feedforward Neural Network in the initial experiment. The Deep Feedforward Neural Network outperformed Support Vector Machines and Neural Network with the best average percentage accuracies. Going further to investigate the Deep Feedforward Neural Network architecture on different parameters against itself suggest that training a Deep Feedforward Neural Network algorithm, regardless of data training size, the classification accuracy peaks. We examine which number of epochs works best in our flight delay classification settings for the Deep Feedforward Neural Network. Our experiment results demonstrate that having many epochs affects the convergence rate of the model; unlike when hidden layers are increased, it does not ensure better or higher accuracy in a binary classification of flight delays. Finally, we recommended further studies on the applicability of the Deep Feedforward Neural Network in flight delays prediction with specific case studies of either airlines or airports to check the impact on the model's performance.},
booktitle = {2021 The 5th International Conference on Advances in Artificial Intelligence (ICAAI)},
pages = {1–10},
numpages = {10},
keywords = {Support Vector Machine, Classification, deep neural network, deep learning, flight delays},
location = {Virtual Event, United Kingdom},
series = {ICAAI 2021}
}

@inproceedings{10.1145/3236461.3241971,
author = {Tufte, Kristin and Datta, Kushal and Jindal, Alekh and Maier, David and Bertini, Robert L.},
title = {Challenges and Opportunities in Transportation Data},
year = {2018},
isbn = {9781450357869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236461.3241971},
doi = {10.1145/3236461.3241971},
abstract = {From the time and money lost sitting in congestion and waiting for traffic signals to change, to the many people injured and killed in traffic crashes each year, to the emissions and energy consumption from our vehicles, the effects of transportation on our daily lives are immense. A wealth of transportation data is available to help address these problems; from data from sensors installed to monitor and operate the roadways and traffic signals to data from cell phone apps and -- just over the horizon -- data from connected vehicles and infrastructure. However, this wealth of data has yet to be effectively leveraged, thus providing opportunities in areas such as improving traffic safety, reducing congestion, improving traffic signal timing, personalizing routing, coordinating across transportation agencies and more. This paper presents opportunities and challenges in applying data management technology to the transportation domain.},
booktitle = {Proceedings of the 1st ACM/EIGSCC Symposium on Smart Cities and Communities},
articleno = {2},
numpages = {8},
keywords = {Smart Cities, Transportation Data, Data Management},
location = {Portland, OR, USA},
series = {SCC '18}
}

@inproceedings{10.5555/3242181.3242220,
author = {Taylor, Simon J. E. and Anagnostou, Anastasia and Fabiyi, Adedeji and Currie, Christine and Monks, Thomas and Barbera, Roberto and Becker, Bruce},
title = {Open Science: Approaches and Benefits for Modeling &amp; Simulation},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {Open Science is the practice of making scientific research accessible to all. It promotes open access to the artefacts of research, the software, data, results and the scientific articles in which they appear, so that others can validate, use and collaborate. Open Science is also being mandated by many funding bodies. The concept of Open Science is new to many Modelling &amp; Simulation (M&amp;S) researchers. To introduce Open Science to our field, this paper unpacks Open Science to understand some of its approaches and benefits. Good practice in the reporting of simulation studies is discussed and the Strengthening the Reporting of Empirical Simulation Studies (STRESS) standardized checklist approach is presented. A case study shows how Digital Object Identifiers, Researcher Registries, Open Access Data Repositories and Scientific Gateways can support Open Science practices for M&amp;S research. The article concludes with a set of guidelines for adopting Open Science for M&amp;S.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {36},
numpages = {15},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.1145/3131672.3131690,
author = {Park, Jaeyeon and Nam, Woojin and Choi, Jaewon and Kim, Taeyeong and Yoon, Dukyong and Lee, Sukhoon and Paek, Jeongyeup and Ko, JeongGil},
title = {Glasses for the Third Eye: Improving the Quality of Clinical Data Analysis with Motion Sensor-Based Data Filtering},
year = {2017},
isbn = {9781450354592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131672.3131690},
doi = {10.1145/3131672.3131690},
abstract = {Recent advances in machine learning based data analytics are opening opportunities for designing effective clinical decision support systems (CDSS) which can become the "third-eye" in the current clinical procedures and diagnosis. However, common patient movements in hospital wards may lead to faulty measurements in physiological sensor readings, and training a CDSS from such noisy data can cause misleading predictions, directly leading to potentially dangerous clinical decisions. In this work, we present MediSense, a system to sense, classify, and identify noise-causing motions and activities that affect physiological signal when made by patients on their hospital beds. Essentially, such a system can be considered as "glasses" for the clinical third eye in correctly observing medical data. MediSense combines wirelessly connected embedded platforms for motion detection with physiological signal data collected from patients to identify faulty physiological signal measurements and filters such noisy data from being used in CDSS training or testing datasets. We deploy our system in real intensive care units (ICUs), and evaluate its performance from real patient traces collected at these ICUs through a 4-month pilot study at the Ajou University Hospital Trauma Center, a major hospital facility located in Suwon, South Korea. Our results show that MediSense successfully classifies patient motions on the bed with &gt;90% accuracy, shows 100% reliability in determining the locations of beds within the ICU, and each bed-attached sensor achieves a lifetime of more than 33 days, which satisfies the application-level requirements suggested by our clinical partners. Furthermore, a simple case-study with arrhythmia patient data shows that MediSense can help improve the clinical diagnosis accuracy.},
booktitle = {Proceedings of the 15th ACM Conference on Embedded Network Sensor Systems},
articleno = {8},
numpages = {14},
keywords = {Clinical Decision Support System, Health Care Information Systems, Motion Sensing, Noise Filter, Wireless Sensor Network},
location = {Delft, Netherlands},
series = {SenSys '17}
}

@article{10.14778/2876473.2876478,
author = {Li, Zeyu and Wang, Hongzhi and Shao, Wei and Li, Jianzhong and Gao, Hong},
title = {Repairing Data through Regular Expressions},
year = {2016},
issue_date = {January 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/2876473.2876478},
doi = {10.14778/2876473.2876478},
abstract = {Since regular expressions are often used to detect errors in sequences such as strings or date, it is natural to use them for data repair. Motivated by this, we propose a data repair method based on regular expression to make the input sequence data obey the given regular expression with minimal revision cost. The proposed method contains two steps, sequence repair and token value repair.For sequence repair, we propose the Regular-expression-based Structural Repair (RSR in short) algorithm. RSR algorithm is a dynamic programming algorithm that utilizes Nondeterministic Finite Automata (NFA) to calculate the edit distance between a prefix of the input string and a partial pattern regular expression with time complexity of O(nm2) and space complexity of O(mn) where m is the edge number of NFA and n is the input string length. We also develop an optimization strategy to achieve higher performance for long strings. For token value repair, we combine the edit-distance-based method and associate rules by a unified argument for the selection of the proper method. Experimental results on both real and synthetic data show that the proposed method could repair the data effectively and efficiently.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {432–443},
numpages = {12}
}

@article{10.1145/2949741.2949756,
author = {De Sa, Christopher and Ratner, Alex and R\'{e}, Christopher and Shin, Jaeho and Wang, Feiran and Wu, Sen and Zhang, Ce},
title = {DeepDive: Declarative Knowledge Base Construction},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/2949741.2949756},
doi = {10.1145/2949741.2949756},
abstract = {The dark data extraction or knowledge base construction (KBC) problem is to populate a SQL database with information from unstructured data sources including emails, webpages, and pdf reports. KBC is a long-standing problem in industry and research that encompasses problems of data extraction, cleaning, and integration. We describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems. The key idea in DeepDive is that statistical inference and machine learning are key tools to attack classical data problems in extraction, cleaning, and integration in a unified and more effective manner. DeepDive programs are declarative in that one cannot write probabilistic inference algorithms; instead, one interacts by defining features or rules about the domain. A key reason for this design choice is to enable domain experts to build their own KBC systems. We present the applications, abstractions, and techniques of DeepDive employed to accelerate construction of KBC systems.},
journal = {SIGMOD Rec.},
month = {jun},
pages = {60–67},
numpages = {8}
}

@article{10.1145/3490384,
author = {Anand, Sanjay Kumar and Kumar, Suresh},
title = {Experimental Comparisons of Clustering Approaches for Data Representation},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3490384},
doi = {10.1145/3490384},
abstract = {Clustering approaches are extensively used by many areas such as IR, Data Integration, Document Classification, Web Mining, Query Processing, and many other domains and disciplines. Nowadays, much literature describes clustering algorithms on multivariate data sets. However, there is limited literature that presented them with exhaustive and extensive theoretical analysis as well as experimental comparisons. This experimental survey paper deals with the basic principle, and techniques used, including important characteristics, application areas, run-time performance, internal, external, and stability validity of cluster quality, etc., on five different data sets of eleven clustering algorithms. This paper analyses how these algorithms behave with five different multivariate data sets in data representation. To answer this question, we compared the efficiency of eleven clustering approaches on five different data sets using three validity metrics-internal, external, and stability and found the optimal score to know the feasible solution of each algorithm. In addition, we have also included four popular and modern clustering algorithms with only their theoretical discussion. Our experimental results for only traditional clustering algorithms showed that different algorithms performed different behavior on different data sets in terms of running time (speed), accuracy and, the size of data set. This study emphasized the need for more adaptive algorithms and a deliberate balance between the running time and accuracy with their theoretical as well as implementation aspects.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {45},
numpages = {33},
keywords = {Clustering approach, internal validation, external validation, optimal score, stability validation}
}

@article{10.1145/3243043,
author = {Nambiar, Athira and Bernardino, Alexandre and Nascimento, Jacinto C.},
title = {Gait-Based Person Re-Identification: A Survey},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3243043},
doi = {10.1145/3243043},
abstract = {The way people walk is a strong correlate of their identity. Several studies have shown that both humans and machines can recognize individuals just by their gait, given that proper measurements of the observed motion patterns are available. For surveillance applications, gait is also attractive, because it does not require active collaboration from users and is hard to fake. However, the acquisition of good-quality measures of a person’s motion patterns in unconstrained environments, (e.g., in person re-identification applications) has proved very challenging in practice. Existing technology (video cameras) suffer from changes in viewpoint, daylight, clothing, accessories, and other variations in the person’s appearance. Novel three-dimensional sensors are bringing new promises to the field, but still many research issues are open. This article presents a survey of the work done in gait analysis for re-identification in the past decade, looking at the main approaches, datasets, and evaluation methodologies. We identify several relevant dimensions of the problem and provide a taxonomic analysis of the current state of the art. Finally, we discuss the levels of performance achievable with the current technology and give a perspective of the most challenging and promising directions of research for the future.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {33},
numpages = {34},
keywords = {person re-identification, gait analysis, machine learning, biometrics, computer vision, Video surveillance}
}

@article{10.1145/3402524,
author = {M\"{a}kitalo, Niko and Flores-Martin, Daniel and Flores, Huber and Lagerspetz, Eemil and Christophe, Francois and Ihantola, Petri and Babazadeh, Masiar and Hui, Pan and Murillo, Juan Manuel and Tarkoma, Sasu and Mikkonen, Tommi},
title = {Human Data Model: Improving Programmability of Health and Well-Being Data for Enhanced Perception and Interaction},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {2691-1957},
url = {https://doi.org/10.1145/3402524},
doi = {10.1145/3402524},
abstract = {Today, an increasing number of systems produce, process, and store personal and intimate data. Such data has plenty of potential for entirely new types of software applications, as well as for improving old applications, particularly in the domain of smart healthcare. However, utilizing this data, especially when it is continuously generated by sensors and other devices, with the current approaches is complex—data is often using proprietary formats and storage, and mixing and matching data of different origin is not easy. Furthermore, many of the systems are such that they should stimulate interactions with humans, which further complicates the systems. In this article, we introduce the Human Data Model—a new tool and a programming model for programmers and end users with scripting skills that help combine data from various sources, perform computations, and develop and schedule computer-human interactions. Written in JavaScript, the software implementing the model can be run on almost any computer either inside the browser or using Node.js. Its source code can be freely downloaded from GitHub, and the implementation can be used with the existing IoT platforms. As a whole, the work is inspired by several interviews with professionals, and an online survey among healthcare and education professionals, where the results show that the interviewed subjects almost entirely lack ideas on how to benefit the ever-increasing amount of data measured of the humans. We believe that this is because of the missing support for programming models for accessing and handling the data, which can be satisfied with the Human Data Model.},
journal = {ACM Trans. Comput. Healthcare},
month = {sep},
articleno = {26},
numpages = {39},
keywords = {data management, ubiquitous computing, IoT, wearable computers, Human Data Model, data mashups, pervasive computing, Mobile computing, programmable world, Internet of Things}
}

@inproceedings{10.1145/3511808.3557468,
author = {Tian, Changxin and Lin, Zihan and Bian, Shuqing and Wang, Jinpeng and Zhao, Wayne Xin},
title = {Temporal Contrastive Pre-Training for Sequential Recommendation},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557468},
doi = {10.1145/3511808.3557468},
abstract = {Recently, pre-training based approaches are proposed to leverage self-supervised signals for improving the performance of sequential recommendation. However, most of existing pre-training recommender systems simply model the historical behavior of a user as a sequence, while lack of sufficient consideration on temporal interaction patterns that are useful for modeling user behavior.In order to better model temporal characteristics of user behavior sequences, we propose a Temporal Contrastive Pre-training method for Sequential Recommendation (TCPSRec for short). Based on the temporal intervals, we consider dividing the interaction sequence into more coherent subsequences, and design temporal pre-training objectives accordingly. Specifically, TCPSRec models two important temporal properties of user behavior, i.e., invariance and periodicity. For invariance, we consider both global invariance and local invariance to capture the long-term preference and short-term intention, respectively. For periodicity, TCPSRec models coarse-grained periodicity and fine-grained periodicity at the subsequence level, which is more stable than modeling periodicity at the item level. By integrating the above strategies, we develop a unified contrastive learning framework with four specially designed pre-training objectives for fusing temporal information into sequential representations. We conduct extensive experiments on six real-world datasets, and the results demonstrate the effectiveness and generalization of our proposed method.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {1925–1934},
numpages = {10},
keywords = {contrastive learning, sequential recommendation, pre-training},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/2993318.2993320,
author = {Esteves, Diego and Mendes, Pablo N. and Moussallem, Diego and Duarte, Julio Cesar and Zaveri, Amrapali and Lehmann, Jens},
title = {MEX Interfaces: Automating Machine Learning Metadata Generation},
year = {2016},
isbn = {9781450347525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993318.2993320},
doi = {10.1145/2993318.2993320},
abstract = {Despite recent efforts to achieve a high level of interoperability of Machine Learning (ML) experiments, positively collaborating with the Reproducible Research context, we still run into problems created due to the existence of different ML platforms: each of those have a specific conceptualization or schema for representing data and metadata. This scenario leads to an extra coding-effort to achieve both the desired interoperability and a better provenance level as well as a more automatized environment for obtaining the generated results. Hence, when using ML libraries, it is a common task to re-design specific data models (schemata) and develop wrappers to manage the produced outputs. In this article, we discuss this gap focusing on the solution for the question: "What is the cleanest and lowest-impact solution, i.e., the minimal effort to achieve both higher interoperability and provenance metadata levels in the Integrated Development Environments (IDE) context and how to facilitate the inherent data querying task?". We introduce a novel and low-impact methodology specifically designed for code built in that context, combining Semantic Web concepts and reflection in order to minimize the gap for exporting ML metadata in a structured manner, allowing embedded code annotations that are, in run-time, converted in one of the state-of-the-art ML schemas for the Semantic Web: MEX Vocabulary.},
booktitle = {Proceedings of the 12th International Conference on Semantic Systems},
pages = {17–24},
numpages = {8},
keywords = {Interoperability, Machine Learning Outputs, Reflection, MEX, Metadata, Annotation, Reproducible Research, Provenance},
location = {Leipzig, Germany},
series = {SEMANTiCS 2016}
}

@article{10.1145/3494566,
author = {Sharma, Ms Promila and Meena, Uma and Sharma, Girish Kumar},
title = {Intelligent Data Analysis Using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3494566},
doi = {10.1145/3494566},
abstract = {Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {94},
numpages = {20},
keywords = {tourism industry, Intelligent data}
}

@inproceedings{10.1145/3465481.3470037,
author = {Hus\'{a}k, Martin and La\v{s}tovi\v{c}ka, Martin and Tovar\v{n}\'{a}k, Daniel},
title = {System for Continuous Collection of Contextual Information for Network Security Management and Incident Handling},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470037},
doi = {10.1145/3465481.3470037},
abstract = {In this paper, we describe a system for the continuous collection of data for the needs of network security management. When a cybersecurity incident occurs in the network, the contextual information on the involved assets facilitates estimating the severity and impact of the incident and selecting an appropriate incident response. We propose a system based on the combination of active and passive network measurements and the correlation of the data with third-party systems. The system enumerates devices and services in the network and their vulnerabilities via fingerprinting of operating systems and applications. Further, the system pairs the hosts in the network with contacts on responsible administrators and highlights critical infrastructure and its dependencies. The system concentrates all the information required for common incident handling procedures and aims to speed up incident response, reduce the time spent on the manual investigation, and prevent errors caused by negligence or lack of information.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {112},
numpages = {8},
keywords = {Incident Response, Cybersecurity, Incident Handling, Network Monitoring, Cyber Situational Awareness},
location = {Vienna, Austria},
series = {ARES 21}
}

@inproceedings{10.1145/2618243.2618244,
author = {Chen, I-Min A. and Markowitz, Victor M. and Szeto, Ernest and Palaniappan, Krishna and Chu, Ken},
title = {Maintaining a Microbial Genome &amp; Metagenome Data Analysis System in an Academic Setting},
year = {2014},
isbn = {9781450327220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2618243.2618244},
doi = {10.1145/2618243.2618244},
abstract = {The Integrated Microbial Genomes (IMG) system integrates microbial community aggregate genomes (metagenomes) with genomes from all domains of life. IMG provides tools for analyzing and reviewing the structural and functional annotations of metagenomes and genomes in a comparative context. At the core of the IMG system is a data warehouse that contains genome and metagenome datasets provided by scientific users, as well as public bacterial, archaeal, eukaryotic, and viral genomes from the US National Center for Biotechnology Information genomic archive and a rich set of engineered, environmental and host associated metagenomes. Genomes and metagenome datasets are processed using IMG's microbial genome and metagenome sequence data processing pipelines and then are integrated into the data warehouse using IMG's data integration toolkit. Microbial genome and metagenome application specific user interfaces provide access to different subsets of IMG's data and analysis toolkits. Genome and metagenome analysis is a gene centric iterative process that involves a sequence (composition) of data exploration and comparative analysis operations, with individual operations expected to have rapid response time.From its first release in 2005, IMG has grown from an initial content of about 300 genomes with a total of 2 million genes, to 22,578 bacterial, archaeal, eukaryotic and viral genomes, and 4,188 metagenome samples, with about 24.6 billion genes as of May 1st, 2014. IMG's database architecture is continuously revised in order to cope with the rapid increase in the number and size of the genome and metagenome datasets, maintain good query performance, and accommodate new data types. We present in this paper IMG's new database architecture developed over the past three years in the context of limited financial, engineering and data management resources customary to academic database systems. We discuss the alternative commercial and open source database management systems we considered and experimented with and describe the hybrid architecture we devised for sustaining IMG's rapid growth.},
booktitle = {Proceedings of the 26th International Conference on Scientific and Statistical Database Management},
articleno = {3},
numpages = {11},
keywords = {data warehouse, genome data analysis system},
location = {Aalborg, Denmark},
series = {SSDBM '14}
}

@article{10.14778/3231751.3231759,
author = {Nazi, Azade and Ding, Bolin and Narasayya, Vivek and Chaudhuri, Surajit},
title = {Efficient Estimation of Inclusion Coefficient Using Hyperloglog Sketches},
year = {2018},
issue_date = {June 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3231751.3231759},
doi = {10.14778/3231751.3231759},
abstract = {Efficiently estimating the inclusion coefficient - the fraction of values of one column that are contained in another column - is useful for tasks such as data profiling and foreign-key detection. We present a new estimator, BML, for inclusion coefficient based on Hyperloglog sketches that results in significantly lower error compared to the state-of-the art approach that uses Bottom-k sketches. We evaluate the error of the BML estimator using experiments on industry benchmarks such as TPC-H and TPC-DS, and several real-world databases. As an independent contribution, we show how Hyperloglog sketches can be maintained incrementally with data deletions using only a constant amount of additional memory.},
journal = {Proc. VLDB Endow.},
month = {jun},
pages = {1097–1109},
numpages = {13}
}

@inbook{10.1145/3447404.3447407,
author = {McMenemy, David},
title = {Ethical Issues in Digital Signal Processing and Machine Learning},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447407},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {15–19},
numpages = {5}
}

@inproceedings{10.5555/3017447.3017501,
author = {Tang, Rong and Mon, Lorri and Beheshti, Jamshid and Li, Yuelin and Pollock, Danielle and Ni, Chaoqun and Chu, Samuel and Xiao, Lu and Caffrey, Julia and Gentry, Steven},
title = {Needs Assessment of ASIS&amp;T Publications: Bridging Information Research and Practice},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {This study reports the results of a 2016 online survey on perceptions and uses of ASIS&amp;T publications. The 190 survey respondents represented 26 countries and 5 continents, with 77% of participants coming from academia rather than practitioners. Among the emerging themes were calls for a wider scope of research from information science to be reflected in the publications (including JASIS&amp;T and the ASIS&amp;T Proceedings), and ongoing challenges in the role of the Bulletin as a bridge between research and practice. The study provides insights into the scholarly publishing practices of the ASIS&amp;T community and highlights key issues for the future direction of ASIS&amp;T's scholarly communication.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology},
articleno = {54},
numpages = {10},
keywords = {ASIS&amp;T publications, user groups, knowledge transfer, publication format and processes},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@inproceedings{10.1145/3239060.3239090,
author = {Baltodano, Sonia and Garcia-Mancilla, Jesus and Ju, Wendy},
title = {Eliciting Driver Stress Using Naturalistic Driving Scenarios on Real Roads},
year = {2018},
isbn = {9781450359467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239060.3239090},
doi = {10.1145/3239060.3239090},
abstract = {We propose a novel method for reliably inducing stress in drivers for the purpose of generating real-world participant data for machine learning, using both scripted in-vehicle stressor events and unscripted on-road stressors such as pedestrians and construction zones. On-road drives took place in a vehicle outfitted with an experimental display that lead drivers to believe they had prematurely ran out of charge on an isolated road. We describe the elicitation method, course design, instrumentation, data collection procedure and the post-hoc labeling of unplanned road events to illustrate how rich data about a variety of stress-related events can be elicited from study participants on-road. We validate this method with data including psychophysiological measurements, video, voice, and GPS data from (N=20) participants. Results from algorithmic psychophysiological stress analysis were validated using participant self-reports. Results of stress elicitation analysis show that our method elicited a stress-state in 89% of participants.},
booktitle = {Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {298–309},
numpages = {12},
keywords = {Wizard of Oz, Stress, Interaction Design, Driver Evaluation, Driver Benchmarking, Design Methods},
location = {Toronto, ON, Canada},
series = {AutomotiveUI '18}
}

@inproceedings{10.1145/3149869.3149873,
author = {Ronaghi, Zahra and Thomas, Rollin and Deslippe, Jack and Bailey, Stephen and Gursoy, Doga and Kisner, Theodore and Keskitalo, Reijo and Borrill, Julian},
title = {Python in the NERSC Exascale Science Applications Program for Data},
year = {2017},
isbn = {9781450351249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149869.3149873},
doi = {10.1145/3149869.3149873},
abstract = {We describe a new effort at the National Energy Research Scientific Computing Center (NERSC) in performance analysis and optimization of scientific Python applications targeting the Intel Xeon Phi (Knights Landing, KNL) manycore architecture. The Python-centered work outlined here is part of a larger effort called the NERSC Exascale Science Applications Program (NESAP) for Data. NESAP for Data focuses on applications that process and analyze high-volume, high-velocity data sets from experimental or observational science (EOS) facilities supported by the US Department of Energy Office of Science. We present three case study applications from NESAP for Data that use Python. These codes vary in terms of "Python purity" from applications developed in pure Python to ones that use Python mainly as a convenience layer for scientists without expertise in lower level programming languages like C, C++ or Fortran. The science case, requirements, constraints, algorithms, and initial performance optimizations for each code are discussed. Our goal with this paper is to contribute to the larger conversation around the role of Python in high-performance computing today and tomorrow, highlighting areas for future work and emerging best practices.},
booktitle = {Proceedings of the 7th Workshop on Python for High-Performance and Scientific Computing},
articleno = {4},
numpages = {10},
location = {Denver, CO, USA},
series = {PyHPC'17}
}

@inproceedings{10.1145/2611040.2611044,
author = {Kagklis, Vasileios and Verykios, Vassilios S. and Tzimas, Giannis and Tsakalidis, Athanasios K.},
title = {Knowledge Sanitization on the Web},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611044},
doi = {10.1145/2611040.2611044},
abstract = {The widespread use of the Internet caused the rapid growth of data on the Web. But as data on the Web grew larger in numbers, so did the perils due to the applications of data mining. Privacy preserving data mining (PPDM) is the field that investigates techniques to preserve the privacy of data and patterns. Knowledge Hiding, a subfield of PPDM, aims at preserving the sensitive patterns included in the data, which are going to be published. A wide variety of techniques fall under the umbrella of Knowledge Hiding, such as frequent pattern hiding, sequence hiding, classification rule hiding and so on.In this tutorial we create a taxonomy for the frequent itemset hiding techniques. We also provide as examples for each category representative works that appeared recently and fall into each one of these categories. Then, we focus on the detailed overview of a specific category, the so called linear programming-based techniques. Finally, we make a quantitative and qualitative comparison among some of the existing techniques that are classified into this category.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {4},
numpages = {11},
keywords = {LP-Based Hiding Approaches, Privacy Preserving Data Mining, Frequent Itemset Hiding, Knowledge Hiding},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@article{10.1145/3480947,
author = {Shaw, Mary},
title = {Myths and Mythconceptions: What Does It Mean to Be a Programming Language, Anyhow?},
year = {2022},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {HOPL},
url = {https://doi.org/10.1145/3480947},
doi = {10.1145/3480947},
abstract = {Modern software does not stand alone; it is embedded in complex physical and sociotechnical systems. It relies on computational support from interdependent subsystems as well as non-code resources such as data, communications, sensors, and interactions with humans. Both general-purpose programming languages and mainstream programming language research focus on symbolic notations with well-defined abstractions that are intended for use by professionals to write programs that solve precisely specified problems. There is a strong emphasis on correctness of the resulting programs, preferably by formal reasoning. However, these languages, despite their careful design and formal foundations, address only a modest portion of modern software and only a minority of software developers. Several persistent myths reinforce this focus. These myths express an idealized model of software and software development. They provide a lens for examining modern software and software development practice: highly trained professionals are outnumbered by vernacular developers. Writing new code is dominated by composition of ill-specified software and non-software components. General-purpose languages may be less appropriate for a task than domain-specific languages, and functional correctness is often a less appropriate goal than overall fitness for task. Support for programming to meet a specification is of little help to people who are programming in order to understand their problems. Reasoning about software is challenged by uncertainty and nondeterminism in the execution environment and by the increasingly dominant role of data, especially with the advent of systems that rely on machine learning. The lens of our persistent myths illuminates the dissonance between our idealized view of software development and common practice, which enables us to identify emerging opportunities and challenges for programming language research.},
journal = {Proc. ACM Program. Lang.},
month = {apr},
articleno = {234},
numpages = {44},
keywords = {problem-setting design, vernacular software developer, domain-specific programming language, sufficient correctness, closed software system, open resource coalition, generality-power tradeoffs, software credentials, general-purpose programming language, formal specifications, fitness to task, exploratory programming, problem-solving design}
}

@article{10.14778/3447689.3447703,
author = {Tata, Sandeep and Potti, Navneet and Wendt, James B. and Costa, Lauro Beltr\~{a}o and Najork, Marc and Gunel, Beliz},
title = {Glean: Structured Extractions from Templatic Documents},
year = {2021},
issue_date = {February 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3447689.3447703},
doi = {10.14778/3447689.3447703},
abstract = {Extracting structured information from templatic documents is an important problem with the potential to automate many real-world business workflows such as payment, procurement, and payroll. The core challenge is that such documents can be laid out in virtually infinitely different ways. A good solution to this problem is one that generalizes well not only to known templates such as invoices from a known vendor, but also to unseen ones.We developed a system called Glean to tackle this problem. Given a target schema for a document type and some labeled documents of that type, Glean uses machine learning to automatically extract structured information from other documents of that type. In this paper, we describe the overall architecture of Glean, and discuss three key data management challenges : 1) managing the quality of ground truth data, 2) generating training data for the machine learning model using labeled documents, and 3) building tools that help a developer rapidly build and improve a model for a given document type. Through empirical studies on a real-world dataset, we show that these data management techniques allow us to train a model that is over 5 F1 points better than the exact same model architecture without the techniques we describe. We argue that for such information-extraction problems, designing abstractions that carefully manage the training data is at least as important as choosing a good model architecture.},
journal = {Proc. VLDB Endow.},
month = {apr},
pages = {997–1005},
numpages = {9}
}

@inproceedings{10.1145/3209811.3209877,
author = {Zegura, Ellen and DiSalvo, Carl and Meng, Amanda},
title = {Care and the Practice of Data Science for Social Good},
year = {2018},
isbn = {9781450358163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209811.3209877},
doi = {10.1145/3209811.3209877},
abstract = {Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of "good" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.},
booktitle = {Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies},
articleno = {34},
numpages = {9},
keywords = {Data science for social good, HCI, care, community engagement},
location = {Menlo Park and San Jose, CA, USA},
series = {COMPASS '18}
}

@article{10.1145/2629446,
author = {Partington, Andrew and Wynn, Moe and Suriadi, Suriadi and Ouyang, Chun and Karnon, Jonathan},
title = {Process Mining for Clinical Processes: A Comparative Analysis of Four Australian Hospitals},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629446},
doi = {10.1145/2629446},
abstract = {Business process analysis and process mining, particularly within the health care domain, remain under-utilized. Applied research that employs such techniques to routinely collected health care data enables stakeholders to empirically investigate care as it is delivered by different health providers. However, cross-organizational mining and the comparative analysis of processes present a set of unique challenges in terms of ensuring population and activity comparability, visualizing the mined models, and interpreting the results. Without addressing these issues, health providers will find it difficult to use process mining insights, and the potential benefits of evidence-based process improvement within health will remain unrealized. In this article, we present a brief introduction on the nature of health care processes, a review of process mining in health literature, and a case study conducted to explore and learn how health care data and cross-organizational comparisons with process-mining techniques may be approached. The case study applies process-mining techniques to administrative and clinical data for patients who present with chest pain symptoms at one of four public hospitals in South Australia. We demonstrate an approach that provides detailed insights into clinical (quality of patient health) and fiscal (hospital budget) pressures in the delivery of health care. We conclude by discussing the key lessons learned from our experience in conducting business process analysis and process mining based on the data from four different hospitals.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {jan},
articleno = {19},
numpages = {18},
keywords = {data preparation, health care delivery, comparative analysis, Process mining, patient pathways}
}

@article{10.1145/3344258,
author = {Jin, Zhuochen and Cui, Shuyuan and Guo, Shunan and Gotz, David and Sun, Jimeng and Cao, Nan},
title = {CarePre: An Intelligent Clinical Decision Assistance System},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2691-1957},
url = {https://doi.org/10.1145/3344258},
doi = {10.1145/3344258},
abstract = {Clinical decision support systems are widely used to assist with medical decision making. However, clinical decision support systems typically require manually curated rules and other data that are difficult to maintain and keep up to date. Recent systems leverage advanced deep learning techniques and electronic health records to provide a more timely and precise result. Many of these techniques have been developed with a common focus on predicting upcoming medical events. However, although the prediction results from these approaches are promising, their value is limited by their lack of interpretability. To address this challenge, we introduce CarePre, an intelligent clinical decision assistance system. The system extends a state-of-the-art deep learning model to predict upcoming diagnosis events for a focal patient based on his or her historical medical records. The system includes an interactive framework together with intuitive visualizations designed to support diagnosis, treatment outcome analysis, and the interpretation of the analysis results. We demonstrate the effectiveness and usefulness of the CarePre&nbsp;system by reporting results from a quantities evaluation of the prediction algorithm, two case studies, and interviews with senior physicians and pulmonologists.},
journal = {ACM Trans. Comput. Healthcare},
month = {mar},
articleno = {6},
numpages = {20},
keywords = {visual analytics, user interface design, reasoning about belief and knowledge, neural networks, Personal health records}
}

@inproceedings{10.1145/3544902.3546238,
author = {Tawosi, Vali and Moussa, Rebecca and Sarro, Federica},
title = {On the Relationship Between Story Points and Development Effort in Agile Open-Source Software},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546238},
doi = {10.1145/3544902.3546238},
abstract = {Background: Previous work has provided some initial evidence that Story Point (SP) estimated by human-experts may not accurately reflect the effort needed to realise Agile software projects. Aims: In this paper, we aim to shed further light on the relationship between SP and Agile software development effort to understand the extent to which human-estimated SP is a good indicator of user story development effort expressed in terms of time needed to realise it. Method: To this end, we carry out a thorough empirical study involving a total of 37,440 unique user stories from 37 different open-source projects publicly available in the TAWOS dataset. For these user stories, we investigate the correlation between the issue development time (or its approximation when the actual time is not available) and the SP estimated by human-expert by using three widely-used correlation statistics (i.e., Pearson, Kendall and Spearman). Furthermore, we investigate SP estimations made by the human-experts in order to assess the extent to which they are consistent in their estimations throughout the project, i.e., we assess whether the development time of the issues is proportionate to the SP assigned to them. Results: The average results across the three correlation measures reveal that the correlation between the human-expert estimated SP and the approximated development time is strong for only 7% of the projects investigated, and medium (58%) or low (35%) for the remaining ones. Similar results are obtained when the actual development time is considered. Our empirical study also reveals that the estimation made is often not consistent throughout the project and the human estimator tends to misestimate in 78% of the cases. Conclusions: Our empirical results suggest that SP might not be an accurate indicator of open-source Agile software development effort expressed in terms of development time. The impact of its use as an indicator of effort should be explored in future work, for example as a cost-driver in automated effort estimation models or as the prediction target.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {183–194},
numpages = {12},
keywords = {Story Point, Software effort estimation, Agile software.},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@article{10.1145/2487259.2487260,
author = {Sadoghi, Mohammad and Jacobsen, Hans-Arno},
title = {Analysis and Optimization for Boolean Expression Indexing},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/2487259.2487260},
doi = {10.1145/2487259.2487260},
abstract = {BE-Tree is a novel dynamic data structure designed to efficiently index Boolean expressions over a high-dimensional discrete space. BE Tree-copes with both high-dimensionality and expressiveness of Boolean expressions by introducing an effective two-phase space-cutting technique that specifically utilizes the discrete and finite domain properties of the space. Furthermore, BE-Tree employs self-adjustment policies to dynamically adapt the tree as the workload changes. Moreover, in BE-Tree, we develop two novel cache-conscious predicate evaluation techniques, namely, lazy and bitmap evaluations, that also exploit the underlying discrete and finite space to substantially reduce BE-Tree's matching time by up to 75%BE-Tree is a general index structure for matching Boolean expression which has a wide range of applications including (complex) event processing, publish/subscribe matching, emerging applications in cospaces, profile matching for targeted web advertising, and approximate string matching. Finally, the superiority of BE-Tree is proven through a comprehensive evaluation with state-of-the-art index structures designed for matching Boolean expressions.},
journal = {ACM Trans. Database Syst.},
month = {jul},
articleno = {8},
numpages = {47},
keywords = {complex event processing, data structure, Boolean expressions, publish/subscribe}
}

@proceedings{10.1145/3559795,
title = {BIOTC '22: Proceedings of the 2022 4th Blockchain and Internet of Things Conference},
year = {2022},
isbn = {9781450396622},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, China}
}

@inproceedings{10.1145/2998181.2998223,
author = {Fiesler, Casey and Dye, Michaelanne and Feuston, Jessica L. and Hiruncharoenvate, Chaya and Hutto, C.J. and Morrison, Shannon and Khanipour Roshan, Parisa and Pavalanathan, Umashanthi and Bruckman, Amy S. and De Choudhury, Munmun and Gilbert, Eric},
title = {What (or Who) Is Public? Privacy Settings and Social Media Content Sharing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998223},
doi = {10.1145/2998181.2998223},
abstract = {When social networking sites give users granular control over their privacy settings, the result is that some content across the site is public and some is not. How might this content--or characteristics of users who post publicly versus to a limited audience--be different? If these differences exist, research studies of public content could potentially be introducing systematic bias. Via Mechanical Turk, we asked 1,815 Facebook users to share recent posts. Using qualitative coding and quantitative measures, we characterize and categorize the nature of the content. Using machine learning techniques, we analyze patterns of choices for privacy settings. Contrary to expectations, we find that content type is not a significant predictor of privacy setting; however, some demographics such as gender and age are predictive. Additionally, with consent of participants, we provide a dataset of nearly 9,000 public and non-public Facebook posts.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {567–580},
numpages = {14},
keywords = {facebook, mechanical turk, mixed methods, machine learning, prediction, privacy, social media, research methods, content analysis, dataset},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{10.1145/3322276.3322354,
author = {Mahyar, Narges and Nguyen, Diana V. and Chan, Maggie and Zheng, Jiayi and Dow, Steven P.},
title = {The Civic Data Deluge: Understanding the Challenges of Analyzing Large-Scale Community Input},
year = {2019},
isbn = {9781450358507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322276.3322354},
doi = {10.1145/3322276.3322354},
abstract = {Advancements in digital civics have enabled leaders to engage and gather input from a broader spectrum of the public. However, less is known about the analysis process around community input and the challenges faced by civic leaders as engagement practices scale up. To understand these challenges, we conducted 21 interviews with leaders on civic-oriented projects. We found that at a small-scale, civic leaders manage to facilitate sensemaking through collaborative or individual approaches. However, as civic leaders scale engagement practices to account for more diverse perspectives, making sense of the large quantity of qualitative data becomes a challenge. Civic leaders could benefit from training in qualitative data analysis and simple, scalable collaborative analysis tools that would help the community form a shared understanding. Drawing from these insights, we discuss opportunities for designing tools that could improve civic leaders' ability to utilize and reflect public input in decisions.},
booktitle = {Proceedings of the 2019 on Designing Interactive Systems Conference},
pages = {1171–1181},
numpages = {11},
keywords = {qualitative dataanalysis, public inpu, community engagement, digital civics},
location = {San Diego, CA, USA},
series = {DIS '19}
}

@inbook{10.1145/3310205.3310209,
title = {Data Deduplication},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310209},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/3078861.3078876,
author = {Karafili, Erisa and Lupu, Emil C.},
title = {Enabling Data Sharing in Contextual Environments: Policy Representation and Analysis},
year = {2017},
isbn = {9781450347020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078861.3078876},
doi = {10.1145/3078861.3078876},
abstract = {Internet of Things environments enable us to capture more and more data about the physical environment we live in and about ourselves. The data enable us to optimise resources, personalise services and offer unprecedented insights into our lives. However, to achieve these insights data need to be shared (and sometimes sold) between organisations imposing rights and obligations upon the sharing parties and in accordance with multiple layers of sometimes conflicting legislation at international, national and organisational levels. In this work, we show how such rules can be captured in a formal representation called "Data Sharing Agreements". We introduce the use of abductive reasoning and argumentation based techniques to work with context dependent rules, detect inconsistencies between them, and resolve the inconsistencies by assigning priorities to the rules. We show how through the use of argumentation based techniques use-cases taken from real life application are handled flexibly addressing trade-offs between confidentiality, privacy, availability and safety.},
booktitle = {Proceedings of the 22nd ACM on Symposium on Access Control Models and Technologies},
pages = {231–238},
numpages = {8},
keywords = {argumentation reasoning, data sharing, usage control, data access, policy language, abductive reasoning, cloud},
location = {Indianapolis, Indiana, USA},
series = {SACMAT '17 Abstracts}
}

@inproceedings{10.1145/2806416.2806444,
author = {Wang, Xianzhi and Sheng, Quan Z. and Fang, Xiu Susie and Li, Xue and Xu, Xiaofei and Yao, Lina},
title = {Approximate Truth Discovery via Problem Scale Reduction},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806444},
doi = {10.1145/2806416.2806444},
abstract = {Many real-world applications rely on multiple data sources to provide information on their interested items. Due to the noises and uncertainty in data, given a specific item, the information from different sources may conflict. To make reliable decisions based on these data, it is important to identify the trustworthy information by resolving these conflicts, i.e., the truth discovery problem. Current solutions to this problem detect the veracity of each value jointly with the reliability of each source for each data item. In this way, the efficiency of truth discovery is strictly confined by the problem scale, which in turn limits truth discovery algorithms from being applicable on a large scale. To address this issue, we propose an approximate truth discovery approach, which divides sources and values into groups according to a user-specified approximation criterion. The groups are then used for efficient inter-value influence computation to improve the accuracy. Our approach is applicable to most existing truth discovery algorithms. Experiments on real-world datasets show that our approach improves the efficiency compared to existing algorithms while achieving similar or even better accuracy. The scalability is further demonstrated by experiments on large synthetic datasets.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {503–512},
numpages = {10},
keywords = {recursive method, truth discovery, problem scale reduction, consistency assurance},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@inproceedings{10.1145/3477495.3531889,
author = {Tian, Changxin and Xie, Yuexiang and Li, Yaliang and Yang, Nan and Zhao, Wayne Xin},
title = {Learning to Denoise Unreliable Interactions for Graph Collaborative Filtering},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531889},
doi = {10.1145/3477495.3531889},
abstract = {Recently, graph neural networks (GNN) have been successfully applied to recommender systems as an effective collaborative filtering (CF) approach. However, existing GNN-based CF models suffer from noisy user-item interaction data, which seriously affects the effectiveness and robustness in real-world applications. Although there have been several studies on data denoising in recommender systems, they either neglect direct intervention of noisy interaction in the message-propagation of GNN, or fail to preserve the diversity of recommendation when denoising. To tackle the above issues, this paper presents a novel GNN-based CF model, named Robust Graph Collaborative Filtering (RGCF), to denoise unreliable interactions for recommendation. Specifically, RGCF consists of a graph denoising module and a diversity preserving module. The graph denoising module is designed for reducing the impact of noisy interactions on the representation learning of GNN, by adopting both a hard denoising strategy (i.e., discarding interactions that are confidently estimated as noise) and a soft denoising strategy (i.e., assigning reliability weights for each remaining interaction). In the diversity preserving module, we build up a diversity augmented graph and propose an auxiliary self-supervised task based on mutual information maximization (MIM) for enhancing the denoised representation and preserving the diversity of recommendation. These two modules are integrated in a multi-task learning manner that jointly improves the recommendation performance. We conduct extensive experiments on three real-world datasets and three synthesized datasets. Experiment results show that RGCF is more robust against noisy interactions and achieves significant improvement compared with baseline models.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {122–132},
numpages = {11},
keywords = {graph neural networks, recommender systems, denoising},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3308560.3316485,
author = {Vazquez Brust, Antonio and Olego, Tom\'{a}s and Rosati, Germ\'{a}n and Lang, Carolina and Bozzoli, Guillermo and Weinberg, Diego and Chuit, Roberto and Minnoni, Martin and Sarraute, Carlos},
title = {Detecting Areas of Potential High Prevalence of Chagas in Argentina},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316485},
doi = {10.1145/3308560.3316485},
abstract = {A map of potential prevalence of Chagas disease (ChD) with high spatial disaggregation is presented. It aims to detect areas outside the Gran Chaco ecoregion (hyperendemic for the ChD), characterized by high affinity with ChD and high health vulnerability.To quantify potential prevalence, we developed several indicators: an Affinity Index which quantifies the degree of linkage between endemic areas of ChD and the rest of the country. We also studied favorable habitability conditions for Triatoma infestans, looking for areas where the predominant materials of floors, roofs and internal ceilings favor the presence of the disease vector.We studied determinants of a more general nature that can be encompassed under the concept of Health Vulnerability Index. These determinants are associated with access to health providers and the socio-economic level of different segments of the population.Finally we constructed a Chagas Potential Prevalence Index (ChPPI) which combines the affinity index, the health vulnerability index, and the population density. We show and discuss the maps obtained. These maps are intended to assist public health specialists, decision makers of public health policies and public officials in the development of cost-effective strategies to improve access to diagnosis and treatment of ChD.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {262–271},
numpages = {10},
keywords = {migrations, call detail records, social network analysis, health vulnerability, Chagas disease, epidemics, neglected tropical diseases},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.5555/2555523.2555543,
author = {Zoumpatianos, Konstantinos and Palpanas, Themis and Mylopoulos, John and Mat\'{e}, Alejandro and Trujillo, Juan},
title = {Monitoring and Diagnosing Indicators for Business Analytics},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Modeling the strategic objectives has been shown to be useful both for understanding a business as well as planning and guiding the overall activities within an enterprise. Business strategy is modeled according to human expertise, setting up the goals as well as the indicators that monitor activities and goals. However, usually indicators provide high-level aggregated views of data, making it difficult to pinpoint problems within specific sub-areas until they have a significant impact into the aggregated value. By the time these problems become evident, they have already hindered the performance of the organization. However, performing a detailed analysis manually can be a daunting task, due to the size of the data space. In order to solve this problem, we propose a user-driven method to analyze the data related to each business indicator by means of data mining. We illustrate our approach with a real world example based on the Europe 2020 framework. Our approach allows us not only to identify latent problems, but also to highlight deviations from anticipated trends that may represent opportunities and exceptional situations, thereby enabling an organization to take advantage of them.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {177–191},
numpages = {15},
location = {Ontario, Canada},
series = {CASCON '13}
}

@article{10.1145/3185511,
author = {Dong, Roy and Ratliff, Lillian J. and C\'{a}rdenas, Alvaro A. and Ohlsson, Henrik and Sastry, S. Shankar},
title = {Quantifying the Utility--Privacy Tradeoff in the Internet of Things},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2378-962X},
url = {https://doi.org/10.1145/3185511},
doi = {10.1145/3185511},
abstract = {The Internet of Things (IoT) promises many advantages in the control and monitoring of physical systems from both efficacy and efficiency perspectives. However, in the wrong hands, the data might pose a privacy threat. In this article, we consider the tradeoff between the operational value of data collected in the IoT and the privacy of consumers. We present a general framework for quantifying this tradeoff in the IoT, and focus on a smart grid application for a proof of concept. In particular, we analyze the tradeoff between smart grid operations and how often data are collected by considering a realistic direct-load control example using thermostatically controlled loads, and we give simulation results to show how its performance degrades as the sampling frequency decreases. Additionally, we introduce a new privacy metric, which we call inferential privacy. This privacy metric assumes a strong adversary model and provides an upper bound on the adversary’s ability to infer a private parameter, independent of the algorithm he uses. Combining these two results allows us to directly consider the tradeoff between better operational performance and consumer privacy.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {may},
articleno = {8},
numpages = {28},
keywords = {smart grid, Privacy}
}

@article{10.1145/3442200,
author = {Barlaug, Nils and Gulla, Jon Atle},
title = {Neural Networks for Entity Matching: A Survey},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3442200},
doi = {10.1145/3442200},
abstract = {Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years, we have seen new methods based upon deep learning techniques for natural language processing emerge. In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {apr},
articleno = {52},
numpages = {37},
keywords = {entity resolution, record linkage, Deep learning, data matching, entity matching}
}

@article{10.1145/3476058,
author = {Scheuerman, Morgan Klaus and Hanna, Alex and Denton, Emily},
title = {Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476058},
doi = {10.1145/3476058},
abstract = {Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {317},
numpages = {37},
keywords = {computer vision, machine learning, datasets, values in design, work practice}
}

@article{10.1145/3533049,
author = {Borges, Jo\~{a}o B. and Ramos, Heitor S. and Loureiro, Antonio A. F.},
title = {A Classification Strategy for Internet of Things Data Based on the Class Separability Analysis of Time Series Dynamics},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2691-1914},
url = {https://doi.org/10.1145/3533049},
doi = {10.1145/3533049},
abstract = {This article proposes TSCLAS, a time series classification strategy for the Internet of Things (IoT) data, based on the class separability analysis of their temporal dynamics. Given the large number and incompleteness of IoT data, the use of traditional classification algorithms is not possible. Thus, we claim that solutions for IoT scenarios should avoid using raw data directly, preferring their transformation to a new domain. In the ordinal patterns domain, it is possible to capture the temporal dynamics of raw data to distinguish them. However, to be applied to this challenging scenario, TSCLAS follows a strategy for selecting the best parameters for the ordinal patterns transformation based on maximizing the class separability of the time series dynamics. We show that our method is competitive compared to other classification algorithms from the literature. Furthermore, TSCLAS is scalable concerning the length of time series and robust to the presence of missing data gaps on them. By simulating missing data gaps as long as 50% of the data, our method could beat the accuracy of the compared classification algorithms. Besides, even when losing in accuracy, TSCLAS presents lower computation times for both training and testing phases.},
journal = {ACM Trans. Internet Things},
month = {jul},
articleno = {23},
numpages = {30},
keywords = {time series classification, Bandt-Pompe transformation, ordinal patterns transformation, Internet of Things, time series dynamics, class separability index}
}

@inproceedings{10.1145/3490099.3511144,
author = {Salminen, Joni and Jung, Soon-Gyo and Jansen, Bernard},
title = {Developing Persona Analytics Towards Persona Science},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511144},
doi = {10.1145/3490099.3511144},
abstract = {Much of the reported work on personas suffers from the lack of empirical evidence. To address this issue, we introduce Persona Analytics (PA), a system that tracks how users interact with data-driven personas. PA captures users’ mouse and gaze behavior to measure users’ interaction with algorithmically generated personas and use of system features for an interactive persona system. Measuring these activities grants an understanding of the behaviors of a persona user, required for quantitative measurement of persona use to obtain scientifically valid evidence. Conducting a study with 144 participants, we demonstrate how PA can be deployed for remote user studies during exceptional times when physical user studies are difficult, if not impossible.},
booktitle = {27th International Conference on Intelligent User Interfaces},
pages = {323–344},
numpages = {22},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.1145/3539604,
author = {Han, Rong and Yan, Zheng and Liang, Xueqin and Yang, Laurence T.},
title = {How Can Incentive Mechanisms and Blockchain Benefit with Each Other? A Survey},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3539604},
doi = {10.1145/3539604},
abstract = {In a blockchain-based system, the lack of centralized control requires active participation and cooperative behaviors of system entities to ensure system security and sustainability. However, dynamic environments and unpredictable entity behaviors challenge the performances of such systems in practice. Therefore, designing a feasible incentive mechanism to regulate entity behaviors becomes essential to improve blockchain system performance. The prosperous characteristics of blockchain can also contribute to an effective incentive mechanism. Unfortunately, current literature still lacks a thorough survey on incentive mechanisms related to the blockchain to understand how incentive mechanisms and blockchain make each other better. To this end, we propose evaluation requirements in terms of the properties and costs of incentive mechanisms. On one hand, we provide a taxonomy of the incentive mechanisms of blockchain systems according to blockchain versions, incentive forms and incentive goals. On the other hand, we categorize blockchain-based incentive mechanisms according to application scenarios and incentive goals. During the review, we discuss the advantages and disadvantages of state-of-art incentive mechanisms based on the proposed evaluation requirements. Through careful review, we present how incentive mechanisms and blockchain benefit with each other, discover a number of unresolved issues, and point out corresponding potential directions for future research.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {jun},
keywords = {incentive mechanism, blockchain, non-monetary incentive, monetary incentive}
}

@inproceedings{10.1145/3236024.3236060,
author = {Lin, Qingwei and Hsieh, Ken and Dang, Yingnong and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Lou, Jian-Guang and Li, Chenggang and Wu, Youjiang and Yao, Randolph and Chintalapati, Murali and Zhang, Dongmei},
title = {Predicting Node Failure in Cloud Service Systems},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236060},
doi = {10.1145/3236024.3236060},
abstract = {In recent years, many traditional software systems have migrated to cloud computing platforms and are provided as online services. The service quality matters because system failures could seriously affect business and user experience. A cloud service system typically contains a large number of computing nodes. In reality, nodes may fail and affect service availability. In this paper, we propose a failure prediction technique, which can predict the failure-proneness of a node in a cloud service system based on historical data, before node failure actually happens. The ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes, therefore improving service availability. Predicting node failure in cloud service systems is challenging, because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals. Furthermore, the failure data is highly imbalanced. To tackle these challenges, we propose MING, a novel technique that combines: 1) a LSTM model to incorporate the temporal data, 2) a Random Forest model to incorporate spatial data; 3) a ranking model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failure-proneness, 4) a cost-sensitive function to identify the optimal threshold for selecting the faulty nodes. We evaluate our approach using real-world data collected from a cloud service system. The results confirm the effectiveness of the proposed approach. We have also successfully applied the proposed approach in real industrial practice.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {480–490},
numpages = {11},
keywords = {service availability, node failure, maintenance, cloud service systems, Failure prediction},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3219819.3219840,
author = {Ruhrl\"{a}nder, Rui Paulo and Boissier, Martin and Uflacker, Matthias},
title = {Improving Box Office Result Predictions for Movies Using Consumer-Centric Models},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219840},
doi = {10.1145/3219819.3219840},
abstract = {Recent progress in machine learning and related fields like recommender systems open up new possibilities for data-driven approaches. One example is the prediction of a movie's box office revenue, which is highly relevant for optimizing production and marketing. We use individual recommendations and user-based forecast models in a system that forecasts revenue and additionally provides actionable insights for industry professionals. In contrast to most existing models that completely neglect user preferences, our approach allows us to model the most important source for movie success: moviegoer taste and behavior. We divide the problem into three distinct stages: (i) we use matrix factorization recommenders to model each user's taste, (ii) we then predict the individual consumption behavior, and (iii) eventually aggregate users to predict the box office result. We compare our approach to the current industry standard and show that the inclusion of user rating data reduces the error by a factor of 2x and outperforms recently published research.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {655–664},
numpages = {10},
keywords = {gradient-boosted trees, recommender systems, logistic regression, user ratings, box office predictions, motion picture industry},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1145/3106774,
author = {Pellungrini, Roberto and Pappalardo, Luca and Pratesi, Francesca and Monreale, Anna},
title = {A Data Mining Approach to Assess Privacy Risk in Human Mobility Data},
year = {2017},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3106774},
doi = {10.1145/3106774},
abstract = {Human mobility data are an important proxy to understand human mobility dynamics, develop analytical services, and design mathematical models for simulation and what-if analysis. Unfortunately mobility data are very sensitive since they may enable the re-identification of individuals in a database. Existing frameworks for privacy risk assessment provide data providers with tools to control and mitigate privacy risks, but they suffer two main shortcomings: (i) they have a high computational complexity; (ii) the privacy risk must be recomputed every time new data records become available and for every selection of individuals, geographic areas, or time windows. In this article, we propose a fast and flexible approach to estimate privacy risk in human mobility data. The idea is to train classifiers to capture the relation between individual mobility patterns and the level of privacy risk of individuals. We show the effectiveness of our approach by an extensive experiment on real-world GPS data in two urban areas and investigate the relations between human mobility patterns and the privacy risk of individuals.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {dec},
articleno = {31},
numpages = {27},
keywords = {data mining, Human mobility, privacy}
}

@article{10.1007/s00778-016-0430-9,
author = {K\"{o}hler, Henning and Leck, Uwe and Link, Sebastian and Zhou, Xiaofang},
title = {Possible and Certain Keys for SQL},
year = {2016},
issue_date = {August    2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-016-0430-9},
doi = {10.1007/s00778-016-0430-9},
abstract = {Driven by the dominance of the relational model and the requirements of modern applications, we revisit the fundamental notion of a key in relational databases with NULL. In SQL, primary key columns are NOT NULL, and UNIQUE constraints guarantee uniqueness only for tuples without NULL. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that originate from an SQL table, respectively. Possible keys coincide with UNIQUE, thus providing a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns and can uniquely identify entities whenever feasible, while primary keys may not. In addition to basic characterization, axiomatization, discovery, and extremal combinatorics problems, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs occur in real-world data, and related computational problems can be solved efficiently. Certain keys are therefore semantically well founded and able to meet Codd's entity integrity rule while handling high volumes of incomplete data from different formats.},
journal = {The VLDB Journal},
month = {aug},
pages = {571–596},
numpages = {26},
keywords = {Data profiling, Null marker, Implication problem, Armstrong database, SQL, Discovery, Index, Axiomatization, Key, Extremal combinatorics}
}

@inproceedings{10.1145/3472163.3472267,
author = {Holubova, Irena and Contos, Pavel and Svoboda, Martin},
title = {Multi-Model Data Modeling and Representation: State of the Art and Research Challenges},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472267},
doi = {10.1145/3472163.3472267},
abstract = {Following the current trend, most of the well-known database systems, being relational, NoSQL, or NewSQL, denote themselves as multi-model. This industry-driven approach, however, lacks plenty of important features of the traditional DBMSs. The primary problem is a design of an optimal multi-model schema and its sufficiently general and efficient representation. In this paper, we provide an overview and discussion of the promising approaches that could potentially be capable of solving these issues, along with a summary of the remaining open problems.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {242–251},
numpages = {10},
keywords = {Inter-model relationships, Category theory, Logical models, Multi-model data, Conceptual modeling},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3404835.3462918,
author = {Zhou, Yujia and Dou, Zhicheng and Wei, Bingzheng and Xie, Ruobing and Wen, Ji-Rong},
title = {Group Based Personalized Search by Integrating Search Behaviour and Friend Network},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462918},
doi = {10.1145/3404835.3462918},
abstract = {The key to personalized search is to build the user profile based on historical behaviour. To deal with the users who lack historical data, group based personalized models were proposed to incorporate the profiles of similar users when re-ranking the results. However, similar users are mostly found based on simple lexical or topical similarity in search behaviours. In this paper, we propose a neural network enhanced method to highlight similar users in semantic space. Furthermore, we argue that the behaviour-based similar users are still insufficient to understand a new query when user's historical activities are limited. To tackle this issue, we introduce the friend network into personalized search to determine the closeness between users in another way. Since the friendship is often formed based on similar background or interest, there are plenty of personalized signals hidden in the friend network naturally. Specifically, we propose a friend network enhanced personalized search model, which groups the user into multiple friend circles based on search behaviours and friend relations respectively. These two types of friend circles are complementary to construct a more comprehensive group profile for refining the personalization. Experimental results show the significant improvement of our model over existing personalized search models.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {92–101},
numpages = {10},
keywords = {friend network, group formation, personalized search},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3219819.3219978,
author = {Yang, Tong and Gong, Junzhi and Zhang, Haowei and Zou, Lei and Shi, Lei and Li, Xiaoming},
title = {HeavyGuardian: Separate and Guard Hot Items in Data Streams},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219978},
doi = {10.1145/3219819.3219978},
abstract = {Data stream processing is a fundamental issue in many fields, such as data mining, databases, network traffic measurement. There are five typical tasks in data stream processing: frequency estimation, heavy hitter detection, heavy change detection, frequency distribution estimation, and entropy estimation. Different algorithms are proposed for different tasks, but they seldom achieve high accuracy and high speed at the same time. To address this issue, we propose a novel data structure named HeavyGuardian. The key idea is to intelligently separate and guard the information of hot items while approximately record the frequencies of cold items. We deploy HeavyGuardian on the above five typical tasks. Extensive experimental results show that HeavyGuardian achieves both much higher accuracy and higher speed than the state-of-the-art solutions for each of the five typical tasks. The source codes of HeavyGuardian and other related algorithms are available at GitHub.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2584–2593},
numpages = {10},
keywords = {data sturcture, probabilistic and approximate data, data stream processing},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1109/TNET.2015.2421897,
author = {Yang, Dejun and Xue, Guoliang and Fang, Xi and Tang, Jian},
title = {Incentive Mechanisms for Crowdsensing: Crowdsourcing with Smartphones},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2015.2421897},
doi = {10.1109/TNET.2015.2421897},
abstract = {Smartphones are programmable and equipped with a set of cheap but powerful embedded sensors, such as accelerometer, digital compass, gyroscope, GPS, microphone, and camera. These sensors can collectively monitor a diverse range of human activities and the surrounding environment. Crowdsensing is a new paradigm which takes advantage of the pervasive smartphones to sense, collect, and analyze data beyond the scale of what was previously possible. With the crowdsensing system, a crowdsourcer can recruit smartphone users to provide sensing service. Existing crowdsensing applications and systems lack good incentive mechanisms that can attract more user participation. To address this issue, we design incentive mechanisms for crowdsensing. We consider two system models: the crowdsourcer-centric model where the crowdsourcer provides a reward shared by participating users, and the user-centric model where users have more control over the payment they will receive. For the crowdsourcer-centric model, we design an incentive mechanism using a Stackelberg game, where the crowdsourcer is the leader while the users are the followers. We show how to compute the unique Stackelberg Equilibrium, at which the utility of the crowdsourcer is maximized, and none of the users can improve its utility by unilaterally deviating from its current strategy. For the user-centric model, we design an auction-based incentive mechanism, which is computationally efficient, individually rational, profitable, and truthful. Through extensive simulations, we evaluate the performance and validate the theoretical properties of our incentive mechanisms.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1732–1744},
numpages = {13},
keywords = {crowdsensing, Stackelberg game, incentive mechanism, crowdsourcing}
}

@inproceedings{10.1145/2750858.2806897,
author = {Saleheen, Nazir and Ali, Amin Ahsan and Hossain, Syed Monowar and Sarker, Hillol and Chatterjee, Soujanya and Marlin, Benjamin and Ertin, Emre and al'Absi, Mustafa and Kumar, Santosh},
title = {PuffMarker: A Multi-Sensor Approach for Pinpointing the Timing of First Lapse in Smoking Cessation},
year = {2015},
isbn = {9781450335744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2750858.2806897},
doi = {10.1145/2750858.2806897},
abstract = {Recent researches have demonstrated the feasibility of detecting smoking from wearable sensors, but their performance on real-life smoking lapse detection is unknown. In this paper, we propose a new model and evaluate its performance on 61 newly abstinent smokers for detecting a first lapse. We use two wearable sensors --- breathing pattern from respiration and arm movements from 6-axis inertial sensors worn on wrists. In 10-fold cross-validation on 40 hours of training data from 6 daily smokers, our model achieves a recall rate of 96.9%, for a false positive rate of 1.1%. When our model is applied to 3 days of post-quit data from 32 lapsers, it correctly pinpoints the timing of first lapse in 28 participants. Only 2 false episodes are detected on 20 abstinent days of these participants. When tested on 84 abstinent days from 28 abstainers, the false episode per day is limited to 1/6.},
booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {999–1010},
numpages = {12},
keywords = {smoking cessation, smartwatch, wearable sensors, smoking detection, mobile health (mHealth)},
location = {Osaka, Japan},
series = {UbiComp '15}
}

@inproceedings{10.1145/3213846.3213866,
author = {Zhang, Yuhao and Chen, Yifan and Cheung, Shing-Chi and Xiong, Yingfei and Zhang, Lu},
title = {An Empirical Study on TensorFlow Program Bugs},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213866},
doi = {10.1145/3213846.3213866},
abstract = {Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research efforts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To fill this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverflow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These findings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {129–140},
numpages = {12},
keywords = {Deep Learning, Empirical Study, TensorFlow Program Bug},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/3269206.3271747,
author = {Gupchup, Jayant and Hosseinkashi, Yasaman and Dmitriev, Pavel and Schneider, Daniel and Cutler, Ross and Jefremov, Andrei and Ellis, Martin},
title = {Trustworthy Experimentation Under Telemetry Loss},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271747},
doi = {10.1145/3269206.3271747},
abstract = {Failure to accurately measure the outcomes of an experiment can lead to bias and incorrect conclusions. Online controlled experiments (aka AB tests) are increasingly being used to make decisions to improve websites as well as mobile and desktop applications. We argue that loss of telemetry data (during upload or post-processing) can skew the results of experiments, leading to loss of statistical power and inaccurate or erroneous conclusions. By systematically investigating the causes of telemetry loss, we argue that it is not practical to entirely eliminate it. Consequently, experimentation systems need to be robust to its effects. Furthermore, we note that it is nontrivial to measure the absolute level of telemetry loss in an experimentation system. In this paper, we take a top-down approach towards solving this problem. We motivate the impact of loss qualitatively using experiments in real applications deployed at scale, and formalize the problem by presenting a theoretical breakdown of the bias introduced by loss. Based on this foundation, we present a general framework for quantitatively evaluating the impact of telemetry loss, and present two solutions to measure the absolute levels of loss. This framework is used by well-known applications at Microsoft, with millions of users and billions of sessions. These general principles can be adopted by any application to improve the overall trustworthiness of experimentation and data-driven decision making.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {387–396},
numpages = {10},
keywords = {data loss, client experimentation, telemetry loss, ab testing, online controlled experiments, experimentation trustworthiness},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.14778/2535568.2448938,
author = {Dong, Xin Luna and Saha, Barna and Srivastava, Divesh},
title = {Less is More: Selecting Sources Wisely for Integration},
year = {2012},
issue_date = {December 2012},
publisher = {VLDB Endowment},
volume = {6},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/2535568.2448938},
doi = {10.14778/2535568.2448938},
abstract = {We are often thrilled by the abundance of information surrounding us and wish to integrate data from as many sources as possible. However, understanding, analyzing, and using these data are often hard. Too much data can introduce a huge integration cost, such as expenses for purchasing data and resources for integration and cleaning. Furthermore, including low-quality data can even deteriorate the quality of integration results instead of bringing the desired quality gain. Thus, "the more the better" does not always hold for data integration and often "less is more".In this paper, we study how to select a subset of sources before integration such that we can balance the quality of integrated data and integration cost. Inspired by the Marginalism principle in economic theory, we wish to integrate a new source only if its marginal gain, often a function of improved integration quality, is higher than the marginal cost, associated with data-purchase expense and integration resources. As a first step towards this goal, we focus on data fusion tasks, where the goal is to resolve conflicts from different sources. We propose a randomized solution for selecting sources for fusion and show empirically its effectiveness and scalability on both real-world data and synthetic data.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {37–48},
numpages = {12}
}

@article{10.1145/3468854,
author = {Zhou, Yaqin and Siow, Jing Kai and Wang, Chenyu and Liu, Shangqing and Liu, Yang},
title = {SPI: Automated Identification of Security Patches via Commits},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3468854},
doi = {10.1145/3468854},
abstract = {Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93% F1-score and precision of 86.24%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {13},
numpages = {27},
keywords = {Machine learning, software security, deep learning}
}

@article{10.1145/3533708,
author = {Joshi, Madhura and Pal, Ankit and Sankarasubbu, Malaikannan},
title = {Federated Learning for Healthcare Domain - Pipeline, Applications and Challenges},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2691-1957},
url = {https://doi.org/10.1145/3533708},
doi = {10.1145/3533708},
abstract = {Federated learning is the process of developing machine learning models over datasets distributed across data centers such as hospitals, clinical research labs, and mobile devices while preventing data leakage. This survey examines previous research and studies on federated learning in the healthcare sector across a range of use cases and applications. Our survey shows what challenges, methods, and applications a practitioner should be aware of in the topic of federated learning. This paper aims to lay out existing research and list the possibilities of federated learning for healthcare industries.},
journal = {ACM Trans. Comput. Healthcare},
month = {nov},
articleno = {40},
numpages = {36},
keywords = {transfer learning, GDPR, Federated learning}
}

@inbook{10.1145/3447404.3447408,
author = {Chatzigiannakis, Ioannis and Tselios, Christos},
title = {Internet of Everything},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447408},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {21–56},
numpages = {36}
}

@article{10.1145/3154815,
author = {Li, Chao and Xue, Yushu and Wang, Jing and Zhang, Weigong and Li, Tao},
title = {Edge-Oriented Computing Paradigms: A Survey on Architecture Design and System Management},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3154815},
doi = {10.1145/3154815},
abstract = {While cloud computing has brought paradigm shifts to computing services, researchers and developers have also found some problems inherent to its nature such as bandwidth bottleneck, communication overhead, and location blindness. The concept of fog/edge computing is therefore coined to extend the services from the core in cloud data centers to the edge of the network. In recent years, many systems are proposed to better serve ubiquitous smart devices closer to the user. This article provides a complete and up-to-date review of edge-oriented computing systems by encapsulating relevant proposals on their architecture features, management approaches, and design objectives.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {39},
numpages = {34},
keywords = {architecture design, ubiquitous data processing, fog computing, resource management, edge computing, Distributed cloud}
}

@proceedings{10.1145/3545922,
title = {ICSLT '22: Proceedings of the 8th International Conference on E-Society, e-Learning and e-Technologies},
year = {2022},
isbn = {9781450396660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rome, Italy}
}

@inproceedings{10.1145/3014087.3014110,
author = {Pereira, Gabriela Viale and Testa, Maur\'{\i}cio Gregianin and Macadar, Marie Anne and Parycek, Peter and de Azambuja, Luiza Schuch},
title = {Building Understanding of Municipal Operations Centers as Smart City' Initiatives: Insights from a Cross-Case Analysis},
year = {2016},
isbn = {9781450348591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014087.3014110},
doi = {10.1145/3014087.3014110},
abstract = {Cities around the world have been facing complex challenges from the growing urbanization. The increase of urban problems is a consequence of this phenomenon, added to the lack of policies focusing in citizens' well-being and safety. Municipal operations centers have played an important role in response of social events and natural disasters as a way to address the urgency and dynamism of urban problems. This research aims at analyzing the main dimensions and factors for implementing municipal operations centers as smart city initiatives. In order to explore this phenomenon it was conducted an exploratory study, based on multiple case studies. The empirical setting of this research is determined by municipal operations centers in Rio de Janeiro, Porto Alegre and Belo Horizonte. The research findings evidenced that the implementation of the centers comprises technological, organizational and managerial factors, in addition to political and institutional factors. Increasing smart cities governance is the main result from the initiatives.},
booktitle = {Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {19–30},
numpages = {12},
keywords = {municipal operations center, multiple cases study, smart cities, smart cities governance},
location = {St. Petersburg, Russia},
series = {EGOSE '16}
}

@inproceedings{10.1145/3318464.3386143,
author = {Yan, Yan and Meyles, Stephen and Haghighi, Aria and Suciu, Dan},
title = {Entity Matching in the Wild: A Consistent and Versatile Framework to Unify Data in Industrial Applications},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386143},
doi = {10.1145/3318464.3386143},
abstract = {Entity matching -- the task of clustering duplicated database records to underlying entities -- has become an increasingly critical component in modern data integration management. Amperity provides a platform for businesses to manage customer data that utilizes a machine-learning approach to entity matching, resolving billions of customer records on a daily basis. We face several challenges in deploying entity matching to industrial applications at scale, and they are less prominent in the literature. These challenges include: (1) Providing not just a single entity clustering, but supporting clusterings at multiple confidence levels to enable downstream applications with varying precision/recall trade-off needs. (2) Many customer record attributes may be systematically missing from different sources of data, creating many pairs of records in a cluster that appear to not match due to incomplete, rather than conflicting information. Allowing these records to connect transitively without introducing conflicts is invaluable to businesses because they can acquire a more comprehensive profile of their customers without incorrect entity merges. (3) How to cluster records over time and assign persistent cluster IDs that can be used for downstream use cases such as A/B tests or predictive model training; this is made more challenging by the fact that we receive new customer data every day and clusters naturally evolving over time still require persistent IDs that refer to the same entity. In this work, we describe Amperity's entity matching framework, Fusion, and how its design provides solutions to these challenges. In particular, we describe our pairwise matching model based on ordinal regression that permits a well-defined way to produce entity clusterings at different confidence levels, a novel clustering algorithm that separates conflicting record pairs in clusters while allowing for pairs that may appear dissimilar due to missing data, and a persistent ID generation algorithm which balances stability of the identifier with ever-evolving entities.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2287–2301},
numpages = {15},
keywords = {cluster id assignment, multi-level entity matching, conflict resolution in clustering},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1145/3372274,
author = {Horv\'{a}th, G\'{a}bor and Kov\'{a}cs, Edith and Molontay, Roland and Nov\'{a}czki, Szabolcs},
title = {Copula-Based Anomaly Scoring and Localization for Large-Scale, High-Dimensional Continuous Data},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3372274},
doi = {10.1145/3372274},
abstract = {The anomaly detection method presented by this article has a special feature: it not only indicates whether or not an observation is anomalous but also tells what exactly makes an anomalous observation unusual. Hence, it provides support to localize the reason of the anomaly.The proposed approach is model based; it relies on the multivariate probability distribution associated with the observations. Since the rare events are present in the tails of the probability distributions, we use copula functions, which are able to model the fat-tailed distributions well. The presented procedure scales well; it can cope with a large number of high-dimensional samples. Furthermore, our procedure can cope with missing values as well, which occur frequently in high-dimensional datasets.In the second part of the article, we demonstrate the usability of the method through a case study, where we analyze a large dataset consisting of the performance counters of a real mobile telecommunication network. Since such networks are complex systems, the signs of sub-optimal operation can remain hidden for a potentially long time. With the proposed procedure, many such hidden issues can be isolated and indicated to the network operator.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {26},
numpages = {26},
keywords = {Anomaly scoring, copula fitting, unsupervised learning}
}

@inproceedings{10.1145/3324884.3416568,
author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Pu, Yanjun and Liu, Xudong},
title = {Learning to Handle Exceptions},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416568},
doi = {10.1145/3324884.3416568},
abstract = {Exception handling is an important built-in feature of many modern programming languages such as Java. It allows developers to deal with abnormal or unexpected conditions that may occur at runtime in advance by using try-catch blocks. Missing or improper implementation of exception handling can cause catastrophic consequences such as system crash. However, previous studies reveal that developers are unwilling or feel it hard to adopt exception handling mechanism, and tend to ignore it until a system failure forces them to do so. To help developers with exception handling, existing work produces recommendations such as code examples and exception types, which still requires developers to localize the try blocks and modify the catch block code to fit the context. In this paper, we propose a novel neural approach to automated exception handling, which can predict locations of try blocks and automatically generate the complete catch blocks. We collect a large number of Java methods from GitHub and conduct experiments to evaluate our approach. The evaluation results, including quantitative measurement and human evaluation, show that our approach is highly effective and outperforms all baselines. Our work makes one step further towards automated exception handling.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {29–41},
numpages = {13},
keywords = {code generation, deep learning, exception handling, neural network},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3548775,
author = {Chen, Shaohan and Gao, Chuanhou and Zhang, Ping},
title = {Incorporation of Data-Mined Knowledge into Black-Box SVM for Interpretability},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3548775},
doi = {10.1145/3548775},
abstract = {The lack of interpretability often makes black-box models challenging to be applied in many practical domains. For this reason, the current work, from the black-box model input port, proposes to incorporate data-mined knowledge into the black-box soft-margin SVM model to enhance accuracy and interpretability. The concept and incorporation mechanism of data-mined knowledge are successively developed, based on which a partially interpretable soft-margin SVM (pTsm-SVM) optimization model is designed and then solved through reformulating the optimization problem as standard quadratic programming. An algorithm for mining linear positive (negative) class knowledge from general data sets is also proposed, which generates a linear two-dimensional discriminative rule with specificity (sensitivity) equal to 1 and the highest possible sensitivity (specificity) among all two-dimensional feature spaces. The knowledge-integrated pTsm-SVM works by achieving a good trade-off among the “large margin”, “high specificity”, and “high sensitivity”. Our experimental results on eight UCI datasets demonstrate the superiority of the proposed pTsm-SVM over the standard soft-margin SVM both in terms of accuracy and interpretability.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jul},
keywords = {Soft-margin SVM, black-box, interpretability, knowledge, data-mined}
}

@inbook{10.1145/3447404.3447422,
author = {Liu, Can and Lindqvist, Janne},
title = {Secure Gestures—Case Study 4},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447422},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {323–338},
numpages = {16}
}

@inproceedings{10.1145/3127479.3131621,
author = {Traub, Jonas and Bre\ss{}, Sebastian and Rabl, Tilmann and Katsifodimos, Asterios and Markl, Volker},
title = {Optimized On-Demand Data Streaming from Sensor Nodes},
year = {2017},
isbn = {9781450350280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127479.3131621},
doi = {10.1145/3127479.3131621},
abstract = {Real-time sensor data enables diverse applications such as smart metering, traffic monitoring, and sport analysis. In the Internet of Things, billions of sensor nodes form a sensor cloud and offer data streams to analysis systems. However, it is impossible to transfer all available data with maximal frequencies to all applications. Therefore, we need to tailor data streams to the demand of applications.We contribute a technique that optimizes communication costs while maintaining the desired accuracy. Our technique schedules reads across huge amounts of sensors based on the data-demands of a huge amount of concurrent queries. We introduce user-defined sampling functions that define the data-demand of queries and facilitate various adaptive sampling techniques, which decrease the amount of transferred data. Moreover, we share sensor reads and data transfers among queries. Our experiments with real-world data show that our approach saves up to 87% in data transmissions.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
pages = {586–597},
numpages = {12},
keywords = {on-demand streaming, sensor data, sensor sharing, user-defined sampling, oversampling, adaptive sampling, real-time analysis},
location = {Santa Clara, California},
series = {SoCC '17}
}

@article{10.5555/3122009.3242050,
author = {Vaughan, Jennifer Wortman},
title = {Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {7026–7071},
numpages = {46},
keywords = {mechanical turk, crowdsourcing, incentives, model evaluation, hybrid intelligence, behavioral experiments, data generation}
}

@article{10.1145/3543860,
author = {Guo, Yuanyuan},
title = {Digital Trust and the Reconstruction of Trust in the Digital Society: An Integrated Model Based on Trust Theory and Expectation Confirmation Theory},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2691-199X},
url = {https://doi.org/10.1145/3543860},
doi = {10.1145/3543860},
abstract = {Digital trust is born with the evolution of digital society. It is an inescapable topic in the digital society and it is developed from traditional interpersonal trust and institutional trust and has been extensively used in the Internet space. At present, the research on digital trust is rare. Based on the Trust Theory and the Expectation Confirmation Theory, this paper puts forward an integration model with user satisfaction as the intermediary variable. Besides, this paper develops a set of scales for evaluating digital trust combined with maturity scales and points out that digital trust consists of digital cognitive trust and emotional trust. This paper assumed that user perception and user expectation indirectly affect digital trust through user satisfaction and used SPSS 23.0 to do reliability, validity test, and exploratory factor analysis. The results found that user satisfaction plays a mediating role by fitting, evaluating, and optimizing the structural equation model with AMOS23.0. User satisfaction is a partial intermediary between user perception and digital trust, and it is the complete intermediary between user expectation and digital trust. These results demonstrate two things. Firstly, In the digital society, the construction of users' digital trust is based on users' satisfaction. The government should provide diversified and high-quality e-government services as far as possible. Secondly, digital trust is directly or indirectly affected by user perception and user expectation. The government should build a safe, green, and harmonious digital environment for users and make e-government services consistent with users' expectation.},
note = {Just Accepted},
journal = {Digit. Gov.: Res. Pract.},
month = {aug},
keywords = {Digital trust, Digital environment, Digital government, Digital society}
}

@inproceedings{10.1145/3306446.3340829,
author = {TeBlunthuis, Nathan and Bayer, Tilman and Vasileva, Olga},
title = {Dwelling on Wikipedia: Investigating Time Spent by Global Encyclopedia Readers},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340829},
doi = {10.1145/3306446.3340829},
abstract = {Much existing knowledge about global consumption of peer-produced information goods is supported by data on Wikipedia page view counts and surveys. In 2017, the Wikimedia Foundation began measuring the time readers spend on a given page view (dwell time), enabling a more detailed understanding of such reading patterns. In this paper, we validate and model this new data source and, building on existing findings, use regression analysis to test hypotheses about how patterns in reading time vary between global contexts. Consistent with prior findings from self-report data, our complementary analysis of behavioral data provides evidence that Global South readers are more likely to use Wikipedia to gain in-depth understanding of a topic. We find that Global South readers spend more time per page view and that this difference is amplified on desktop devices, which are thought to be better suited for in-depth information seeking tasks.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {14},
numpages = {14},
keywords = {peer production, web analytics, quantitative methods, readership, Wikipedia, dwell time, digital divides},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@inproceedings{10.1145/3034786.3056114,
author = {Fan, Wenfei and Lu, Ping},
title = {Dependencies for Graphs},
year = {2017},
isbn = {9781450341981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3034786.3056114},
doi = {10.1145/3034786.3056114},
abstract = {This paper proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities in a graph.We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound and complete axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication and validation problems for the extensions.},
booktitle = {Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
pages = {403–416},
numpages = {14},
keywords = {axiom system, satisfiability, implication, built-in predicates, disjunction, graph dependencies, conditional functional dependencies, tgds, egds, keys, validation},
location = {Chicago, Illinois, USA},
series = {PODS '17}
}

@article{10.1145/3167970,
author = {Chung, Yeounoh and Mortensen, Michael Lind and Binnig, Carsten and Kraska, Tim},
title = {Estimating the Impact of Unknown Unknowns on Aggregate Query Results},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0362-5915},
url = {https://doi.org/10.1145/3167970},
doi = {10.1145/3167970},
abstract = {It is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) Is the integrated data set complete? and (2) What is the impact of any unknown (i.e., unobserved) data on query results?In this work, we develop and analyze techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameter-free and do not assume prior knowledge about the distribution; we also propose a parametric model that can be used instead when the data sources are imbalanced. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources.},
journal = {ACM Trans. Database Syst.},
month = {mar},
articleno = {3},
numpages = {37},
keywords = {unknown unknowns, Aggregate query processing, species estimation, crowdsourcing}
}

@article{10.1145/3329124,
author = {McDaniel, Melinda and Storey, Veda C.},
title = {Evaluating Domain Ontologies: Clarification, Classification, and Challenges},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3329124},
doi = {10.1145/3329124},
abstract = {The number of applications being developed that require access to knowledge about the real world has increased rapidly over the past two decades. Domain ontologies, which formalize the terms being used in a discipline, have become essential for research in areas such as Machine Learning, the Internet of Things, Robotics, and Natural Language Processing, because they enable separate systems to exchange information. The quality of these domain ontologies, however, must be ensured for meaningful communication. Assessing the quality of domain ontologies for their suitability to potential applications remains difficult, even though a variety of frameworks and metrics have been developed for doing so. This article reviews domain ontology assessment efforts to highlight the work that has been carried out and to clarify the important issues that remain. These assessment efforts are classified into five distinct evaluation approaches and the state of the art of each described. Challenges associated with domain ontology assessment are outlined and recommendations are made for future research and applications.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {70},
numpages = {44},
keywords = {evaluation, domain ontology, assessment, applied ontology, Ontology, ontology application, task-ontology fit, metrics, ontology development}
}

@inproceedings{10.1145/3299869.3324956,
author = {Mahdavi, Mohammad and Abedjan, Ziawasch and Castro Fernandez, Raul and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan},
title = {Raha: A Configuration-Free Error Detection System},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3324956},
doi = {10.1145/3299869.3324956},
abstract = {Detecting erroneous values is a key step in data cleaning. Error detection algorithms usually require a user to provide input configurations in the form of rules or statistical parameters. However, providing a complete, yet correct, set of configurations for each new dataset is not trivial, as the user has to know about both the dataset and the error detection algorithms upfront. In this paper, we present Raha, a new configuration-free error detection system. By generating a limited number of configurations for error detection algorithms that cover various types of data errors, we can generate an expressive feature vector for each tuple value. Leveraging these feature vectors, we propose a novel sampling and classification scheme that effectively chooses the most representative values for training. Furthermore, our system can exploit historical data to filter out irrelevant error detection algorithms and configurations. In our experiments, Raha outperforms the state-of-the-art error detection techniques with no more than 20 labeled tuples on each dataset.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {865–882},
numpages = {18},
keywords = {classification, label propagation, error detection, semi-supervised learning, machine learning, historical data, clustering, data cleaning},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@article{10.1145/3385031,
author = {Fan, Wenfei and Liu, Xueli and Lu, Ping and Tian, Chao},
title = {Catching Numeric Inconsistencies in Graphs},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3385031},
doi = {10.1145/3385031},
abstract = {Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we extend graph functional dependencies with linear arithmetic expressions and built-in comparison predicates, referred to as numeric graph dependencies (NGDs). We study fundamental problems for NGDs. We show that their satisfiability, implication, and validation problems are Σp2-complete, Πp2-complete, and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity. To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs in response to updates ΔG to G. We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in ΔG instead of the entire G. Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. In addition, to strike a balance between the efficiency and accuracy, we also develop polynomial-time parallel algorithms for detection and incremental detection of top-ranked inconsistencies. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.},
journal = {ACM Trans. Database Syst.},
month = {jun},
articleno = {9},
numpages = {47},
keywords = {Numeric errors, graph dependencies, incremental validation}
}

@article{10.1145/3487893,
author = {Xie, Yiqun and Shekhar, Shashi and Li, Yan},
title = {Statistically-Robust Clustering Techniques for Mapping Spatial Hotspots: A Survey},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3487893},
doi = {10.1145/3487893},
abstract = {Mapping of spatial hotspots, i.e., regions with significantly higher rates of generating cases of certain events (e.g., disease or crime cases), is an important task in diverse societal domains, including public health, public safety, transportation, agriculture, environmental science, and so on. Clustering techniques required by these domains differ from traditional clustering methods due to the high economic and social costs of spurious results (e.g., false alarms of crime clusters). As a result, statistical rigor is needed explicitly to control the rate of spurious detections. To address this challenge, techniques for statistically-robust clustering (e.g., scan statistics) have been extensively studied by the data mining and statistics communities. In this survey, we present an up-to-date and detailed review of the models and algorithms developed by this field. We first present a general taxonomy for statistically-robust clustering, covering key steps of data and statistical modeling, region enumeration and maximization, and significance testing. We further discuss different paradigms and methods within each of the key steps. Finally, we highlight research gaps and potential future directions, which may serve as a stepping stone in generating new ideas and thoughts in this growing field and beyond.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {36},
numpages = {38},
keywords = {Hotspot, mapping, clustering, statistical rigor, scan statistics}
}

@article{10.1109/TASLP.2021.3078883,
author = {Yu, Jianwei and Zhang, Shi-Xiong and Wu, Bo and Liu, Shansong and Hu, Shoukang and Geng, Mengzhe and Liu, Xunying and Meng, Helen and Yu, Dong},
title = {Audio-Visual Multi-Channel Integration and Recognition of Overlapped Speech},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3078883},
doi = {10.1109/TASLP.2021.3078883},
abstract = {Automatic speech recognition (ASR) technologies have been significantly advanced in the past few decades. However, recognition of overlapped speech remains a highly challenging task to date. To this end, multi-channel microphone array data are widely used in current ASR systems. Motivated by the invariance of visual modality to acoustic signal corruption and the additional cues they provide to separate the target speaker from the interfering sound sources, this paper presents an audio-visual multi-channel based recognition system for overlapped speech. It benefits from a tight integration between a speech separation front-end and recognition back-end, both of which incorporate additional video input. A series of audio-visual multi-channel speech separation front-end components based on <italic>TF masking</italic>, <italic>Filter&amp;Sum</italic> and <italic>mask-based MVDR</italic> neural channel integration approaches are developed. To reduce the error cost mismatch between the separation and the recognition components, the entire system is jointly fine-tuned using a multi-task criterion interpolation of the scale-invariant signal to noise ratio (Si-SNR) with either the connectionist temporal classification (CTC), or lattice-free maximum mutual information (LF-MMI) loss function. Experiments suggest that: the proposed audio-visual multi-channel recognition system outperforms the baseline audio-only multi-channel ASR system by up to 8.04% (31.68% relative) and 22.86% (58.51% relative) absolute WER reduction on overlapped speech constructed using either simulation or replaying of the LRS2 dataset respectively. Consistent performance improvements are also obtained using the proposed audio-visual multi-channel recognition system when using occluded video input with the lip region randomly covered up to 60%.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {may},
pages = {2067–2082},
numpages = {16}
}

@inproceedings{10.1145/3411764.3445488,
author = {Linxen, Sebastian and Sturm, Christian and Br\"{u}hlmann, Florian and Cassau, Vincent and Opwis, Klaus and Reinecke, Katharina},
title = {How WEIRD is CHI?},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445488},
doi = {10.1145/3411764.3445488},
abstract = {Computer technology is often designed in technology hubs in Western countries, invariably making it “WEIRD”, because it is based on the intuition, knowledge, and values of people who are Western, Educated, Industrialized, Rich, and Democratic. Developing technology that is universally useful and engaging requires knowledge about members of WEIRD and non-WEIRD societies alike. In other words, it requires us, the CHI community, to generate this knowledge by studying representative participant samples. To find out to what extent CHI participant samples are from Western societies, we analyzed papers published in the CHI proceedings between 2016-2020. Our findings show that 73% of CHI study findings are based on Western participant samples, representing less than 12% of the world’s population. Furthermore, we show that most participant samples at CHI tend to come from industrialized, rich, and democratic countries with generally highly educated populations. Encouragingly, recent years have seen a slight increase in non-Western samples and those that include several countries. We discuss suggestions for further broadening the international representation of CHI participant samples.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {143},
numpages = {14},
keywords = {HCI research, WEIRD, geographic diversity, sample bias, generalizability},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3332466.3374525,
author = {Tian, Jiannan and Di, Sheng and Zhang, Chengming and Liang, Xin and Jin, Sian and Cheng, Dazhao and Tao, Dingwen and Cappello, Franck},
title = {WaveSZ: A Hardware-Algorithm Co-Design of Efficient Lossy Compression for Scientific Data},
year = {2020},
isbn = {9781450368186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332466.3374525},
doi = {10.1145/3332466.3374525},
abstract = {Error-bounded lossy compression is critical to the success of extreme-scale scientific research because of ever-increasing volumes of data produced by today's high-performance computing (HPC) applications. Not only can error-controlled lossy compressors significantly reduce the I/O and storage burden but they can retain high data fidelity for post analysis. Existing state-of-the-art lossy compressors, however, generally suffer from relatively low compression and decompression throughput (up to hundreds of megabytes per second on a single CPU core), which considerably restrict the adoption of lossy compression by many HPC applications especially those with a fairly high data production rate. In this paper, we propose a highly efficient lossy compression approach based on field programmable gate arrays (FPGAs) under the state-of-the-art lossy compression model SZ. Our contributions are fourfold. (1) We adopt a wavefront memory layout to alleviate the data dependency during the prediction for higher-dimensional predictors, such as the Lorenzo predictor. (2) We propose a co-design framework named waveSZ based on the wavefront memory layout and the characteristics of SZ algorithm and carefully implement it by using high-level synthesis. (3) We propose a hardware-algorithm co-optimization method to improve the performance. (4) We evaluate our proposed waveSZ on three real-world HPC simulation datasets from the Scientific Data Reduction Benchmarks and compare it with other state-of-the-art methods on both CPUs and FPGAs. Experiments show that our waveSZ can improve SZ's compression throughput by 6.9X ~ 8.7X over the production version running on a state-of-the-art CPU and improve the compression ratio and throughput by 2.1X and 5.8X on average, respectively, compared with the state-of-the-art FPGA design.},
booktitle = {Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {74–88},
numpages = {15},
keywords = {lossy compression, throughput, FPGA, scientific data, software-hardware co-design, compression ratio},
location = {San Diego, California},
series = {PPoPP '20}
}

@article{10.14778/2809974.2809991,
author = {Shin, Jaeho and Wu, Sen and Wang, Feiran and De Sa, Christopher and Zhang, Ce and R\'{e}, Christopher},
title = {Incremental Knowledge Base Construction Using DeepDive},
year = {2015},
issue_date = {July 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2809974.2809991},
doi = {10.14778/2809974.2809991},
abstract = {Populating a database with unstructured information is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. Recent names used for this problem include dealing with dark data and knowledge base construction (KBC). In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based respectively on sampling and variational techniques. We also study the tradeoff space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate DeepDive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1310–1321},
numpages = {12}
}

@inproceedings{10.1145/3377811.3380336,
author = {Zhang, He and Zhou, Xin and Huang, Xin and Huang, Huang and Babar, Muhammad Ali},
title = {An Evidence-Based Inquiry into the Use of Grey Literature in Software Engineering},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380336},
doi = {10.1145/3377811.3380336},
abstract = {Context: Following on other scientific disciplines, such as health sciences, the use of Grey Literature (GL) has become widespread in Software Engineering (SE) research. Whilst the number of papers incorporating GL in SE is increasing, there is little empirically known about different aspects of the use of GL in SE research.Method: We used a mixed-methods approach for this research. We carried out a Systematic Literature Review (SLR) of the use of GL in SE, and surveyed the authors of the selected papers included in the SLR (as GL users) and the invited experts in SE community on the use of GL in SE research. Results: We systematically selected and reviewed 102 SE secondary studies that incorporate GL in SE research, from which we identified two groups based on their reporting: 1) 76 reviews only claim their use of GL; 2) 26 reviews report the results by including GL. We also obtained 20 replies from the GL users and 24 replies from the invited SE experts. Conclusion: There is no common understanding of the meaning of GL in SE. Researchers define the scopes and the definitions of GL in a variety of ways. We found five main reasons of using GL in SE research. The findings have enabled us to propose a conceptual model for how GL works in SE research lifecycle. There is an apparent need for research to develop guidelines for using GL in SE and for assessing quality of GL. The current work can provide a panorama of the state-of-the-art of using GL in SE for the follow-up research, as to determine the important position of GL in SE research.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1422–1434},
numpages = {13},
keywords = {survey, grey literature, systematic (literature) review, evidence-based software engineering, empirical software engineering},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1109/TNET.2020.2982685,
author = {Li, Xiaocan and Xie, Kun and Wang, Xin and Xie, Gaogang and Xie, Dongliang and Li, Zhenyu and Wen, Jigang and Diao, Zulong and Wang, Tian},
title = {Quick and Accurate False Data Detection in Mobile Crowd Sensing},
year = {2020},
issue_date = {June 2020},
publisher = {IEEE Press},
volume = {28},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2982685},
doi = {10.1109/TNET.2020.2982685},
abstract = {The attacks, faults, and severe communication/system conditions in Mobile Crowd Sensing (MCS) make false data detection a critical problem. Observing the intrinsic low dimensionality of general monitoring data and the sparsity of false data, false data detection can be performed based on the separation of normal data and anomalies. Although the existing separation algorithm based on Direct Robust Matrix Factorization (DRMF) is proven to be effective, requiring iteratively performing Singular Value Decomposition (SVD) for low-rank matrix approximation would result in a prohibitively high accumulated computation cost when the data matrix is large. In this work, we observe the quick false data location feature from our empirical study of DRMF, based on which we propose an intelligent Light weight Low Rank and False Matrix Separation algorithm (LightLRFMS) that can reuse the previous result of the matrix decomposition to deduce the one for the current iteration step. Depending on the type of data corruption, random or successive/mass, we design two versions of LightLRFMS. From a theoretical perspective, we validate that LightLRFMS only requires one round of SVD computation and thus has very low computation cost. We have done extensive experiments using a PM 2.5 air condition trace and a road traffic trace. Our results demonstrate that LightLRFMS can achieve very good false data detection performance with the same highest detection accuracy as DRMF but with up to 20 times faster speed thanks to its lower computation cost.},
journal = {IEEE/ACM Trans. Netw.},
month = {jun},
pages = {1339–1352},
numpages = {14}
}

@article{10.1109/TCBB.2018.2873010,
author = {Chowdhury, Hussain Ahmed and Bhattacharyya, Dhruba Kumar and Kalita, Jugal Kumar},
title = {Differential Expression Analysis of RNA-Seq Reads: Overview, Taxonomy, and Tools},
year = {2020},
issue_date = {March-April 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2873010},
doi = {10.1109/TCBB.2018.2873010},
abstract = {Analysis of RNA-sequence (RNA-seq) data is widely used in transcriptomic studies and it has many applications. We review RNA-seq data analysis from RNA-seq reads to the results of differential expression analysis. In addition, we perform a descriptive comparison of tools used in each step of RNA-seq data analysis along with a discussion of important characteristics of these tools. A taxonomy of tools is also provided. A discussion of issues in quality control and visualization of RNA-seq data is also included along with useful tools. Finally, we provide some guidelines for the RNA-seq data analyst, along with research issues and challenges which should be addressed.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {apr},
pages = {566–586},
numpages = {21}
}

@proceedings{10.1145/3527049,
title = {SPBPU IDE '21: Proceedings of the 3rd International Scientific Conference on Innovations in Digital Economy},
year = {2021},
isbn = {9781450386944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Saint - Petersburg, Russian Federation}
}

@article{10.1145/3368036,
author = {Hilman, Muhammad H. and Rodriguez, Maria A. and Buyya, Rajkumar},
title = {Multiple Workflows Scheduling in Multi-Tenant Distributed Systems: A Taxonomy and Future Directions},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3368036},
doi = {10.1145/3368036},
abstract = {Workflows are an application model that enables the automated execution of multiple interdependent and interconnected tasks. They are widely used by the scientific community to manage the distributed execution and dataflow of complex simulations and experiments. As the popularity of scientific workflows continue to rise, and their computational requirements continue to increase, the emergence and adoption of multi-tenant computing platforms that offer the execution of these workflows as a service becomes widespread. This article discusses the scheduling and resource provisioning problems particular to this type of platform. It presents a detailed taxonomy and a comprehensive survey of the current literature and identifies future directions to foster research in the field of multiple workflow scheduling in multi-tenant distributed computing systems.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {10},
numpages = {39},
keywords = {multi-tenant platforms, Scientific workflows, multiple workflows scheduling}
}

@article{10.1145/3459992,
author = {Cai, Zhipeng and Xiong, Zuobin and Xu, Honghui and Wang, Peng and Li, Wei and Pan, Yi},
title = {Generative Adversarial Networks: A Survey Toward Private and Secure Applications},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3459992},
doi = {10.1145/3459992},
abstract = {Generative Adversarial Networks (GANs) have promoted a variety of applications in computer vision and natural language processing, among others, due to its generative model’s compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey to summarize systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this article also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {132},
numpages = {38},
keywords = {deep learning, privacy and security, Generative adversarial networks}
}

@inbook{10.1145/3447404.3447424,
author = {Wiese, Jason},
title = {Personal Context from Mobile Phones—Case Study 5},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447424},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {341–376},
numpages = {36}
}

@inproceedings{10.1145/3411764.3445669,
author = {Lu, Xi and L. Reynolds, Tera and Jo, Eunkyung and Hong, Hwajung and Page, Xinru and Chen, Yunan and A. Epstein, Daniel},
title = {Comparing Perspectives Around Human and Technology Support for Contact Tracing},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445669},
doi = {10.1145/3411764.3445669},
abstract = {Various contact tracing approaches have been applied to help contain the spread of COVID-19, with technology-based tracing and human tracing among the most widely adopted. However, governments and communities worldwide vary in their adoption of digital contact tracing, with many instead choosing the human approach. We investigate how people perceive the respective benefits and risks of human and digital contact tracing through a mixed-methods survey with 291 respondents from the United States. Participants perceived digital contact tracing as more beneficial for protecting privacy, providing convenience, and ensuring data accuracy, and felt that human contact tracing could help provide security, emotional reassurance, advice, and accessibility. We explore the role of self-tracking technologies in public health crisis situations, highlighting how designs must adapt to promote societal benefit rather than just self-understanding. We discuss how future digital contact tracing can better balance the benefits of human tracers and technology amidst the complex contact tracing process and context.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {200},
numpages = {15},
keywords = {Public health, COVID-19, Crisis informatics, Contact tracing, Personal informatics, Self-tracking},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1007/s00778-016-0441-6,
author = {Khayyat, Zuhair and Lucia, William and Singh, Meghna and Ouzzani, Mourad and Papotti, Paolo and Quian\'{e}-Ruiz, Jorge-Arnulfo and Tang, Nan and Kalnis, Panos},
title = {Fast and Scalable Inequality Joins},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-016-0441-6},
doi = {10.1007/s00778-016-0441-6},
abstract = {Inequality joins, which is to join relations with inequality conditions, are used in various applications. Optimizing joins has been the subject of intensive research ranging from efficient join algorithms such as sort-merge join, to the use of efficient indices such as $$B^+$$B+-tree, $$R^*$$R\'{z}-tree and Bitmap. However, inequality joins have received little attention and queries containing such joins are notably very slow. In this paper, we introduce fast inequality join algorithms based on sorted arrays and space-efficient bit-arrays. We further introduce a simple method to estimate the selectivity of inequality joins which is then used to optimize multiple predicate queries and multi-way joins. Moreover, we study an incremental inequality join algorithm to handle scenarios where data keeps changing. We have implemented a centralized version of these algorithms on top of PostgreSQL, a distributed version on top of Spark SQL, and an existing data cleaning system, Nadeef. By comparing our algorithms against well-known optimization techniques for inequality joins, we show our solution is more scalable and several orders of magnitude faster.},
journal = {The VLDB Journal},
month = {feb},
pages = {125–150},
numpages = {26},
keywords = {PostgreSQL, Selectivity estimation, Incremental, Inequality join, Spark SQL}
}

@inproceedings{10.1145/3411764.3445130,
author = {Ismail, Azra and Kumar, Neha},
title = {AI in Global Health: The View from the Front Lines},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445130},
doi = {10.1145/3411764.3445130},
abstract = {There has been growing interest in the application of AI for Social Good, motivated by scarce and unequal resources globally. We focus on the case of AI in frontline health, a Social Good domain that is increasingly a topic of significant attention. We offer a thematic discourse analysis of scientific and grey literature to identify prominent applications of AI in frontline health, motivations driving this work, stakeholders involved, and levels of engagement with the local context. We then uncover design considerations for these systems, drawing from data from three years of ethnographic fieldwork with women frontline health workers and women from marginalized communities in Delhi (India). Finally, we outline an agenda for AI systems that target Social Good, drawing from literature on HCI4D, post-development critique, and transnational feminist theory. Our paper thus offers a critical and ethnographic perspective to inform the design of AI systems that target social impact.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {598},
numpages = {21},
keywords = {Qualitative, India, AI, Healthcare, HCI4D, Social Good},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.14778/2983200.2983203,
author = {Chu, Xu and Ilyas, Ihab F. and Koutris, Paraschos},
title = {Distributed Data Deduplication},
year = {2016},
issue_date = {July 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2983200.2983203},
doi = {10.14778/2983200.2983203},
abstract = {Data deduplication refers to the process of identifying tuples in a relation that refer to the same real world entity. The complexity of the problem is inherently quadratic with respect to the number of tuples, since a similarity value must be computed for every pair of tuples. To avoid comparing tuple pairs that are obviously non-duplicates, blocking techniques are used to divide the tuples into blocks and only tuples within the same block are compared. However, even with the use of blocking, data deduplication remains a costly problem for large datasets. In this paper, we show how to further speed up data deduplication by leveraging parallelism in a shared-nothing computing environment. Our main contribution is a distribution strategy, called Dis-Dedup, that minimizes the maximum workload across all worker nodes and provides strong theoretical guarantees. We demonstrate the effectiveness of our proposed strategy by performing extensive experiments on both synthetic datasets with varying block size distributions, as well as real world datasets.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {864–875},
numpages = {12}
}

@article{10.14778/3067421.3067431,
author = {Jain, Ayush and Sarma, Akash Das and Parameswaran, Aditya and Widom, Jennifer},
title = {Understanding Workers, Developing Effective Tasks, and Enhancing Marketplace Dynamics: A Study of a Large Crowdsourcing Marketplace},
year = {2017},
issue_date = {March 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3067421.3067431},
doi = {10.14778/3067421.3067431},
abstract = {We conduct an experimental analysis of a dataset comprising over 27 million microtasks performed by over 70,000 workers issued to a large crowdsourcing marketplace between 2012--2016. Using this data---never before analyzed in an academic context---we shed light on three crucial aspects of crowdsourcing: (1) Task design---helping requesters understand what constitutes an effective task, and how to go about designing one; (2) Marketplace dynamics --- helping marketplace administrators and designers understand the interaction between tasks and workers, and the corresponding marketplace load; and (3) Worker behavior --- understanding worker attention spans, lifetimes, and general behavior, for the improvement of the crowdsourcing ecosystem as a whole.},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {829–840},
numpages = {12}
}

@inproceedings{10.1145/2588555.2612176,
author = {Lang, Willis and Nehme, Rimma V. and Robinson, Eric and Naughton, Jeffrey F.},
title = {Partial Results in Database Systems},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2612176},
doi = {10.1145/2588555.2612176},
abstract = {As the size and complexity of analytic data processing systems continue to grow, the effort required to mitigate faults and performance skew has also risen. However, in some environments we have encountered, users prefer to continue query execution even in the presence of failures (e.g., the unavailability of certain data sources), and receive a "partial" answer to their query. We explore ways to characterize and classify these partial results, and describe an analytical framework that allows the system to perform coarse to fine-grained analysis to determine the semantics of a partial result. We propose that if the system is equipped with such a framework, in some cases it is better to return and explain partial results than to attempt to avoid them.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {1275–1286},
numpages = {12},
keywords = {partial results, result semantics, failures},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2998181.2998183,
author = {Priedhorsky, Reid and Osthus, Dave and Daughton, Ashlynn R. and Moran, Kelly R. and Generous, Nicholas and Fairchild, Geoffrey and Deshpande, Alina and Del Valle, Sara Y.},
title = {Measuring Global Disease with Wikipedia: Success, Failure, and a Research Agenda},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998183},
doi = {10.1145/2998181.2998183},
abstract = {Effective disease monitoring provides a foundation for effective public health systems. This has historically been accomplished with patient contact and bureaucratic aggregation, which tends to be slow and expensive. Recent internet-based approaches promise to be real-time and cheap, with few parameters. However, the question of when and how these approaches work remains open. We addressed this question using Wikipedia access logs and category links. Our experiments, replicable and extensible using our open source code and data, test the effect of semantic article filtering, amount of training data, forecast horizon, and model staleness by comparing across 6 diseases and 4 countries using thousands of individual models. We found that our minimal-configuration, language-agnostic article selection process based on semantic relatedness is effective for improving predictions, and that our approach is relatively insensitive to the amount and age of training data. We also found, in contrast to prior work, very little forecasting value, and we argue that this is consistent with theoretical considerations about the nature of forecasting. These mixed results lead us to propose that the currently observational field of internet-based disease surveillance must pivot to include theoretical models of information flow as well as controlled experiments based on simulations of disease.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1812–1834},
numpages = {23},
keywords = {epidemiology, modeling, forecasting, wikipedia, disease},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@article{10.1145/3524110,
author = {Zou, Jie and Aliannejadi, Mohammad and Kanoulas, Evangelos and Pera, Maria Soledad and Liu, Yiqun},
title = {Users Meet Clarifying Questions: Toward a Better Understanding of User Interactions for Search Clarification},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3524110},
doi = {10.1145/3524110},
abstract = {The use of clarifying questions (CQs) is a fairly new and useful technique to aid systems in recognizing the intent, context, and preferences behind user queries. Yet, understanding the extent of the effect of CQs on user behavior and the ability to identify relevant information remains relatively unexplored. In this work, we conduct a large user study to understand the interaction of users with CQs in various quality categories, and the effect of CQ quality on user search performance in terms of finding relevant information, search behavior, and user satisfaction. Analysis of implicit interaction data and explicit user feedback demonstrates that high-quality CQs improve user performance and satisfaction. By contrast, low- and mid-quality CQs are harmful, and thus allowing the users to complete their tasks without CQ support may be preferred in this case. We also observe that user engagement, and therefore the need for CQ support, is affected by several factors, such as search result quality or perceived task difficulty. The findings of this study can help researchers and system designers realize why, when, and how users interact with CQs, leading to a better understanding and design of search clarification systems.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = {apr},
keywords = {User Study; Information Seeking Systems; Clarifying Questions}
}

@inbook{10.1145/3447404.3447428,
author = {He, Dengbo and Risteska, Martina and Donmez, Birsen and Chen, Kaiyang},
title = {Driver Cognitive Load Classification Based on Physiological Data—Case Study 7},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447428},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {409–429},
numpages = {21}
}

@article{10.1145/3529260,
author = {Benarous, Maya and Toch, Eran and Ben-gal, Irad},
title = {Synthesis of Longitudinal Human Location Sequences: Balancing Utility and Privacy},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3529260},
doi = {10.1145/3529260},
abstract = {People’s location data are continuously tracked from various devices and sensors, enabling an ongoing analysis of sensitive information that can violate people’s privacy and reveal confidential information. Synthetic data have been used to generate representative location sequences yet to maintain the users’ privacy. Nonetheless, the privacy-accuracy tradeoff between these two measures has not been addressed systematically. In this article, we analyze the use of different synthetic data generation models for long location sequences, including extended short-term memory networks (LSTMs), Markov Chains (MC), and variable-order Markov models (VMMs). We employ different performance measures, such as data similarity and privacy, and discuss the inherent tradeoff. Furthermore, we introduce other measurements to quantify each of these measures. Based on the anonymous data of 300 thousand cellular-phone users, our work offers a road map for developing policies for synthetic data generation processes. We propose a framework for building data generation models and evaluating their effectiveness regarding those accuracy and privacy measures.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {118},
numpages = {27},
keywords = {location sequences, privacy, Synthetic data, long short term memory network (LSTM)}
}

@article{10.14778/2732240.2732248,
author = {Heise, Arvid and Quian\'{e}-Ruiz, Jorge-Arnulfo and Abedjan, Ziawasch and Jentzsch, Anja and Naumann, Felix},
title = {Scalable Discovery of Unique Column Combinations},
year = {2013},
issue_date = {December 2013},
publisher = {VLDB Endowment},
volume = {7},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732240.2732248},
doi = {10.14778/2732240.2732248},
abstract = {The discovery of all unique (and non-unique) column combinations in a given dataset is at the core of any data profiling effort. The results are useful for a large number of areas of data management, such as anomaly detection, data integration, data modeling, duplicate detection, indexing, and query optimization. However, discovering all unique and non-unique column combinations is an NP-hard problem, which in principle requires to verify an exponential number of column combinations for uniqueness on all data values. Thus, achieving efficiency and scalability in this context is a tremendous challenge by itself.In this paper, we devise Ducc, a scalable and efficient approach to the problem of finding all unique and non-unique column combinations in big datasets. We first model the problem as a graph coloring problem and analyze the pruning effect of individual combinations. We then present our hybrid column-based pruning technique, which traverses the lattice in a depth-first and random walk combination. This strategy allows Ducc to typically depend on the solution set size and hence to prune large swaths of the lattice. Ducc also incorporates row-based pruning to run uniqueness checks in just few milliseconds. To achieve even higher scalability, Ducc runs on several CPU cores (scale-up) and compute nodes (scale-out) with a very low overhead. We exhaustively evaluate Ducc using three datasets (two real and one synthetic) with several millions rows and hundreds of attributes. We compare Ducc with related work: Gordian and HCA. The results show that Ducc is up to more than 2 orders of magnitude faster than Gordian and HCA (631x faster than Gordian and 398x faster than HCA). Finally, a series of scalability experiments shows the efficiency of Ducc to scale up and out.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {301–312},
numpages = {12}
}

@proceedings{10.1145/3548608,
title = {ICCIR '22: Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics},
year = {2022},
isbn = {9781450397179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanjing, China}
}

@inbook{10.1145/3447404.3447414,
author = {Chatzilygeroudis, Konstantinos and Hatzilygeroudis, Ioannis and Perikos, Isidoros},
title = {Machine Learning Basics},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447414},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {143–193},
numpages = {51}
}

@article{10.1145/3340294,
author = {Zhao, Liping and Loucopoulos, Pericles and Kavakli, Evangelia and Letsholo, Keletso J.},
title = {User Studies on End-User Service Composition: A Literature Review and a Design Framework},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/3340294},
doi = {10.1145/3340294},
abstract = {Context: End-user service composition (EUSC) is a service-oriented paradigm that aims to empower end users and allow them to compose their own web applications from reusable service components. User studies have been used to evaluate EUSC tools and processes. Such an approach should benefit software development, because incorporating end users’ feedback into software development should make software more useful and usable. Problem: There is a gap in our understanding of what constitutes a user study and how a good user study should be designed, conducted, and reported. Goal: This article aims to address this gap. Method: The article presents a systematic review of 47 selected user studies for EUSC. Guided by a review framework, the article systematically and consistently assesses the focus, methodology and cohesion of each of these studies. Results: The article concludes that the focus of these studies is clear, but their methodology is incomplete and inadequate, their overall cohesion is poor. The findings lead to the development of a design framework and a set of questions for the design, reporting, and review of good user studies for EUSC. The detailed analysis and the insights obtained from the analysis should be applicable to the design of user studies for service-oriented systems as well and indeed for any user studies related to software artifacts.},
journal = {ACM Trans. Web},
month = {jul},
articleno = {15},
numpages = {46},
keywords = {qualitative studies, web services, review framework, User studies, service-oriented computing, design guideline, systematic review, empirical studies, end-user service composition, mapshups}
}

@inproceedings{10.1145/3533406.3533423,
author = {Martinez, Wendy and Benerradi, Johann and Midha, Serena and Maior, Horia A. and Wilson, Max L.},
title = {Understanding the Ethical Concerns for Neurotechnology in the Future of Work},
year = {2022},
isbn = {9781450396554},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533406.3533423},
doi = {10.1145/3533406.3533423},
abstract = {Advances in automation and autonomous systems means that the future of work will involve even more cognitive effort. For those in already cognitively demanding work, many of us aim to optimise our effort and productivity to achieve more in work, and ideally to rest outside of work. Neuroergonomics research is concerned with how neurotechnology will help improve work to be manageable and safe, often in e.g. safety critical work, operators experience high demands and mental workload. Meanwhile, Neuroethics is concerned with the largely unregulated future of this industry, involving technologies that are not technically medical devices, but will involve invasive forms of personal data. This work aims to explicate the privacy, trust, and ethical concerns that workers have about employers using neurotechnology to manage their work-forces. An online survey and themes drawn from interviews with factory and office workers are presented. We conclude by discussing these concerns and how they might affect the rapidly expanding neurotechnology industry.},
booktitle = {2022 Symposium on Human-Computer Interaction for Work},
articleno = {17},
numpages = {19},
keywords = {employees, privacy, future of work, neuroethics, employers, ethics, trust, neurotechnology},
location = {Durham, NH, USA},
series = {CHIWORK 2022}
}

@inproceedings{10.1145/3491102.3517727,
author = {Sallam, Samar and Sakamoto, Yumiko and Leboe-McGowan, Jason and Latulipe, Celine and Irani, Pourang},
title = {Towards Design Guidelines for Effective Health-Related Data Videos: An Empirical Investigation of Affect, Personality, and Video Content},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517727},
doi = {10.1145/3491102.3517727},
abstract = {Data Videos (DVs), or animated infographics that tell stories with data, are becoming increasingly popular. Despite their potential to induce attitude change, little is explored about how to produce effective DVs. This paper describes two studies that explored factors linked to the potential of health DVs to improve viewers’ behavioural change intentions. We investigated: 1) how viewers’ affect is linked to their behavioural change intentions; 2) how these affect are linked to the viewers’ personality traits; 3) which attributes of DVs are linked to their persuasive potential. Results from both studies indicated that viewers’ negative affect lowered their behavioural change intentions. Individuals with higher neuroticism exhibited higher negative affect and were harder to convince. Finally, Study 2 proved that providing any solutions to the health problem, presented in the DV, made the viewers perceive the videos as more actionable while lowering their negative affect, and importantly, induced higher behavioural change intentions.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {342},
numpages = {22},
keywords = {personality, Data Video, persuasive technology, physical activity, actionable, solution, affect, attitude change, data storytelling, narrative visualization},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@proceedings{10.1145/3543434,
title = {dg.o 2022: DG.O 2022: The 23rd Annual International Conference on Digital Government Research},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Republic of Korea}
}

@article{10.1145/3558555,
author = {Naing, Htet and Cai, Wentong and Nan, Hu and Tiantian, Wu and Liang, Yu},
title = {Dynamic Data-Driven Microscopic Traffic Simulation Using Jointly Trained Physics-Guided Long Short-Term Memory},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/3558555},
doi = {10.1145/3558555},
abstract = {Symbiotic simulation systems that incorporate data-driven methods (such as machine/deep learning) are effective and efficient tools for just-in-time (JIT) operational decision making. With the growing interest on Digital Twin City, such systems are ideal for real-time microscopic traffic simulation. However, learning-based models are heavily biased towards the training data and could produce physically inconsistent outputs. In terms of microscopic traffic simulation, this could lead to unsafe driving behaviours causing vehicle collisions in the simulation. As for symbiotic simulation, this could severely affect the performance of real-time base simulation models resulting in inaccurate or unrealistic forecasts, which could, in turn, mislead JIT what-if analyses. To overcome this issue, a physics-guided data-driven modelling paradigm should be adopted so that the resulting model could capture both accurate and safe driving behaviours. However, very few works exist in the development of such a car-following model that can balance between simulation accuracy and physical consistency. Therefore, in this paper, a new “jointly-trained physics-guided Long Short-Term Memory (JTPG-LSTM)” neural network, is proposed and integrated to a dynamic data-driven simulation system to capture dynamic car-following behaviours. An extensive set of experiments was conducted to demonstrate the advantages of the proposed model from both modelling and simulation perspectives.},
journal = {ACM Trans. Model. Comput. Simul.},
month = {nov},
articleno = {28},
numpages = {27},
keywords = {Digital Twin, Data-driven modelling, real-time simulation, car-following, online learning, physics-guided machine learning, just-in-time simulation, symbiotic simulation}
}

@inproceedings{10.1145/2463676.2465335,
author = {Koutris, Paraschos and Upadhyaya, Prasang and Balazinska, Magdalena and Howe, Bill and Suciu, Dan},
title = {Toward Practical Query Pricing with QueryMarket},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465335},
doi = {10.1145/2463676.2465335},
abstract = {We develop a new pricing system, QueryMarket, for flexible query pricing in a data market based on an earlier theoretical framework (Koutris et al., PODS 2012). To build such a system, we show how to use an Integer Linear Programming formulation of the pricing problem for a large class of queries, even when pricing is computationally hard. Further, we leverage query history to avoid double charging when queries purchased over time have overlapping information, or when the database is updated. We then present a technique that fairly shares revenue when multiple sellers are involved. Finally, we implement our approach in a prototype and evaluate its performance on several query workloads.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {613–624},
numpages = {12},
keywords = {data pricing, integer linear programming},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/2998181.2998197,
author = {Law, Edith and Gajos, Krzysztof Z. and Wiggins, Andrea and Gray, Mary L. and Williams, Alex},
title = {Crowdsourcing as a Tool for Research: Implications of Uncertainty},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998197},
doi = {10.1145/2998181.2998197},
abstract = {Numerous crowdsourcing platforms are now available to support research as well as commercial goals. However, crowdsourcing is not yet widely adopted by researchers for generating, processing or analyzing research data. This study develops a deeper understanding of the circumstances under which crowdsourcing is a useful, feasible or desirable tool for research, as well as the factors that may influence researchers' decisions around adopting crowdsourcing technology. We conducted semi-structured interviews with 18 researchers in diverse disciplines, spanning the humanities and sciences, to illuminate how research norms and practitioners' dispositions were related to uncertainties around research processes, data, knowledge, delegation and quality. The paper concludes with a discussion of the design implications for future crowdsourcing systems to support research.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1544–1561},
numpages = {18},
keywords = {interviews, crowdsourcing for research, citizen science},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@book{10.1145/3173161,
author = {Task Group on Information Technology Curricula},
title = {Information Technology Curricula 2017: Curriculum Guidelines for Baccalaureate Degree Programs in Information Technology},
year = {2017},
isbn = {9781450364164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@proceedings{10.1145/3549737,
title = {SETN '22: Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Corfu, Greece}
}

@inproceedings{10.1145/3243734.3243741,
author = {Gursoy, Mehmet Emre and Liu, Ling and Truex, Stacey and Yu, Lei and Wei, Wenqi},
title = {Utility-Aware Synthesis of Differentially Private and Attack-Resilient Location Traces},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243741},
doi = {10.1145/3243734.3243741},
abstract = {As mobile devices and location-based services become increasingly ubiquitous, the privacy of mobile users' location traces continues to be a major concern. Traditional privacy solutions rely on perturbing each position in a user's trace and replacing it with a fake location. However, recent studies have shown that such point-based perturbation of locations is susceptible to inference attacks and suffers from serious utility losses, because it disregards the moving trajectory and continuity in full location traces. In this paper, we argue that privacy-preserving synthesis of complete location traces can be an effective solution to this problem. We present AdaTrace, a scalable location trace synthesizer with three novel features: provable statistical privacy, deterministic attack resilience, and strong utility preservation. AdaTrace builds a generative model from a given set of real traces through a four-phase synthesis process consisting of feature extraction, synopsis learning, privacy and utility preserving noise injection, and generation of differentially private synthetic location traces. The output traces crafted by AdaTrace preserve utility-critical information existing in real traces, and are robust against known location trace attacks. We validate the effectiveness of AdaTrace by comparing it with three state of the art approaches (ngram, DPT, and SGLT) using real location trace datasets (Geolife and Taxi) as well as a simulated dataset of 50,000 vehicles in Oldenburg, Germany. AdaTrace offers up to 3-fold improvement in trajectory utility, and is orders of magnitude faster than previous work, while preserving differential privacy and attack resilience.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {196–211},
numpages = {16},
keywords = {mobile computing, location privacy, data privacy},
location = {Toronto, Canada},
series = {CCS '18}
}

@article{10.1007/s00778-021-00717-x,
author = {Kellou-Menouer, Kenza and Kardoulakis, Nikolaos and Troullinou, Georgia and Kedad, Zoubida and Plexousakis, Dimitris and Kondylakis, Haridimos},
title = {A Survey on Semantic Schema Discovery},
year = {2021},
issue_date = {Jul 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00717-x},
doi = {10.1007/s00778-021-00717-x},
abstract = {More and more weakly structured, and irregular data sources are becoming available every day. The schema of these sources is useful for a number of tasks, such as query answering, exploration and summarization. However, although semantic web data might contain schema information, in many cases this is completely missing or partially defined. In this paper, we present a survey of the state of the art on schema information extraction approaches. We analyze and classify these approaches into three families: (1) approaches that exploit the implicit structure of the data, without assuming that some explicit statements on the schema are provided in the dataset; (2) approaches that use the explicit schema statements contained in the dataset to complement and enrich the schema, and (3) those that discover structural patterns contained in a dataset. We compare these studies in terms of their approach, advantages and limitations. Finally we discuss the problems that remain open.},
journal = {The VLDB Journal},
month = {nov},
pages = {675–710},
numpages = {36},
keywords = {Schema discovery, Semantic web, Linked data, Irregular data}
}

@inproceedings{10.1145/3183713.3196916,
author = {Fan, Wenfei and Hu, Chunming and Liu, Xueli and Lu, Ping},
title = {Discovering Graph Functional Dependencies},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3196916},
doi = {10.1145/3183713.3196916},
abstract = {This paper studies discovery of GFDs, a class of functional dependencies defined on graphs. We investigate the fixed-parameter tractability of three fundamental problems related to GFD discovery. We show that the implication and satisfiability problems are fixed-parameter tractable, but the validation problem is co-W[1]-hard. We introduce notions of reduced GFDs and their topological support, and formalize the discovery problem for GFDs. We develop algorithms for discovering GFDs and computing their covers. Moreover, we show that GFD discovery is feasible over large-scale graphs, by providing parallel scalable algorithms for discovering GFDs that guarantee to reduce running time when more processors are used. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {427–439},
numpages = {13},
keywords = {parallel scalable, gfd discovery, fixed-parameter tractability},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@article{10.1145/3191734,
author = {Bari, Rummana and Adams, Roy J. and Rahman, Md. Mahbubur and Parsons, Megan Battles and Buder, Eugene H. and Kumar, Santosh},
title = {RConverse: Moment by Moment Conversation Detection Using a Mobile Respiration Sensor},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191734},
doi = {10.1145/3191734},
abstract = {Monitoring of in-person conversations has largely been done using acoustic sensors. In this paper, we propose a new method to detect moment-by-moment conversation episodes by analyzing breathing patterns captured by a mobile respiration sensor. Since breathing is affected by physical and cognitive activities, we develop a comprehensive method for cleaning, screening, and analyzing noisy respiration data captured in the field environment at individual breath cycle level. Using training data collected from a speech dynamics lab study with 12 participants, we show that our algorithm can identify each respiration cycle with 96.34% accuracy even in presence of walking. We present a Conditional Random Field, Context-Free Grammar (CRF-CFG) based conversation model, called rConverse, to classify respiration cycles into speech or non-speech, and subsequently infer conversation episodes. Our model achieves 82.7% accuracy for speech/non-speech classification and it identifies conversation episodes with 95.9% accuracy on lab data using a leave-one-subject-out cross-validation. Finally, the system is validated against audio ground-truth in a field study with 32 participants. rConverse identifies conversation episodes with 71.7% accuracy on 254 hours of field data. For comparison, the accuracy from a high-quality audio-recorder on the same data is 71.9%.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {2},
numpages = {27},
keywords = {Conversation Modeling, Wearable Sensing, Respiration Signal, Machine Learning}
}

@article{10.1145/3079765,
author = {Riegler, Michael and Pogorelov, Konstantin and Eskeland, Sigrun Losada and Schmidt, Peter Thelin and Albisser, Zeno and Johansen, Dag and Griwodz, Carsten and Halvorsen, P\r{a}l and Lange, Thomas De},
title = {From Annotation to Computer-Aided Diagnosis: Detailed Evaluation of a Medical Multimedia System},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3079765},
doi = {10.1145/3079765},
abstract = {Holistic medical multimedia systems covering end-to-end functionality from data collection to aided diagnosis are highly needed, but rare. In many hospitals, the potential value of multimedia data collected through routine examinations is not recognized. Moreover, the availability of the data is limited, as the health care personnel may not have direct access to stored data. However, medical specialists interact with multimedia content daily through their everyday work and have an increasing interest in finding ways to use it to facilitate their work processes. In this article, we present a novel, holistic multimedia system aiming to tackle automatic analysis of video from gastrointestinal (GI) endoscopy. The proposed system comprises the whole pipeline, including data collection, processing, analysis, and visualization. It combines filters using machine learning, image recognition, and extraction of global and local image features. The novelty is primarily in this holistic approach and its real-time performance, where we automate a complete algorithmic GI screening process. We built the system in a modular way to make it easily extendable to analyze various abnormalities, and we made it efficient in order to run in real time. The conducted experimental evaluation proves that the detection and localization accuracy are comparable or even better than existing systems, but it is by far leading in terms of real-time performance and efficient resource consumption.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {may},
articleno = {26},
numpages = {26},
keywords = {gastrointestinal tract, Medical multimedia system, evaluation}
}

@article{10.1145/3274353,
author = {Jun, Eunice and Jo, Blue A. and Oliveira, Nigini and Reinecke, Katharina},
title = {Digestif: Promoting Science Communication in Online Experiments},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274353},
doi = {10.1145/3274353},
abstract = {Online experiments allow researchers to collect data from large, demographically diverse global populations. Unlike in-lab studies, however, online experiments often fail to inform participants about the research to which they contribute. This paper is the first to investigate barriers that prevent researchers from providing such science communication in online experiments. We found that the main obstacles preventing researchers from including such information are assumptions about participant disinterest, limited time, concerns about losing anonymity, and concerns about experimental bias. Researchers also noted the dearth of tools to help them close the information loop with their study participants. Based on these findings, we formulated design requirements and implemented Digestif, a new web-based tool that supports researchers in providing their participants with science communication pages. Our evaluation shows that Digestif's scaffolding, examples, and nudges to focus on participants make researchers more aware of their participants' curiosity about research and more likely to disclose pertinent research information.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {84},
numpages = {26}
}

@proceedings{10.1145/2723372,
title = {SIGMOD '15: Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SIGMOD 2015 -- officially, the 2015 ACM SIGMOD International Conference on the Management of Data! This year's conference is being held in the beautiful cultural capital of Australia, Melbourne. During the Gold Rush period of the 19th Century, Melbourne was the richest city in the world, and as a result it is filled with many unique neighborhoods and distinctive buildings. In addition to wonderful neighborhoods to explore, the city has great museums and other cultural attractions, as well as a fine multi-cultural atmosphere. For those who would like to explore the outdoors, popular highlights are the Phillip Island Nature Park (90 minutes away), which features wild penguins who return in a parade each day at sunset, and the Great Ocean Road, one of the world's most scenic coastal drives, including the famous towering 12 Apostles.SIGMOD 2015's exciting technical program reflects not only traditional topics, but the database community's role in broader data science and data analytics. The keynote from Laura Haas, "The Power Behind the Throne: Information Integration in the Age of Data-Driven Discovery" highlights the role of database and data integration techniques in the growing field of data science. Jignesh Patel's talk, "From Data to Insights @ Bare Metal Speed," explains how hardware and software need to be co-evolved to support the needs of scalable data analytics. Jennifer Widom, winner of the 2015 ACM-W Athena Lecturer Award for fundamental contributions to computer science, will give her award talk, "Three Favorite Results," on Tuesday. Christopher R\'{e} will lead a panel on "Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?," with participants Divyakant Agrawal, Magdalena Balazinska, Michael Cafarella, Michael Jordan, Tim Kraska, and Raghu Ramakrishnan. Of course, there are also 106 research paper presentations, 4 tutorials, 30 demonstrations, and 18 industrial papers. Papers will be presented both as talks during the research sessions, and as part of plenary Poster Sessions.SIGMOD 2015 is preceded by the PhD Workshop, as well as workshops on leading-edge topics like data analytics (DanaC), databases and the Web (WebDB), exploratory search (ExploreDB), managing and mining spatial data (GeoRich), and graph data (GRADES); the New Researcher Symposium will take place on Wednesday. The banquet will be held in a Melbourne landmark, the Town Hall.As in recent years, we had two submission deadlines for SIGMOD this year, one in August and one in November. The review process was journal-style, with multiple rounds of reviews coordinated by the Group Leaders. We accepted 34 of 137 papers from the first deadline and 72 of 278 from the second deadline. The total acceptance rate was about 25.5%, and we believe that the revision processhas improved the quality of the technical program.},
location = {Melbourne, Victoria, Australia}
}

@article{10.1145/3492838,
author = {Grandhi, Sukeshini A. and Plotnick, Linda},
title = {Do I Spit or Do I Pass? Perceived Privacy and Security Concerns of Direct-to-Consumer Genetic Testing},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {GROUP},
url = {https://doi.org/10.1145/3492838},
doi = {10.1145/3492838},
abstract = {This study explores privacy concerns perceived by people with respect to having their DNA tested by direct-to-consumer (DTC) genetic testing companies such as 23andMe and Ancestry.com. Data collected from 510 respondents indicate that those who have already obtained a DTC genetic test have significantly lower levels of privacy and security concerns than those who have not obtained a DTC genetic test. Qualitative data from respondents of both these groups show that the concerns are mostly similar. However, the factors perceived to alleviate privacy concerns are more varied and nuanced amongst those who have obtained a DTC genetic test. Our data suggest that privacy concerns or lack of concerns are based on complex and multiple considerations including data ownership, access control of data and regulatory authorities of social, political and legal systems. Respondents do not engage in a full cost/benefit analysis of having their DNA tested.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {19},
numpages = {26},
keywords = {heuristics, decision making, information disclosure, rational, direct-to-consumer genetic testing, concerns, information privacy, security, privacy}
}

@article{10.1145/3551385,
author = {Mohseni, Sina and Wang, Haotao and Xiao, Chaowei and Yu, Zhiding and Wang, Zhangyang and Yadawa, Jay},
title = {Taxonomy of Machine Learning Safety: A Survey and Primer},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3551385},
doi = {10.1145/3551385},
abstract = {The open-world deployment of Machine Learning (ML) algorithms in safety-critical applications such as autonomous vehicles needs to address a variety of ML vulnerabilities such as interpretability, verifiability, and performance limitations. Research explores different approaches to improve ML dependability by proposing new models and training techniques to reduce generalization error, achieve domain adaptation, and detect outlier examples and adversarial attacks. However, there is a missing connection between ongoing ML research and well-established safety principles. In this paper, we present a structured and comprehensive review of ML techniques to improve the dependability of ML algorithms in uncontrolled open-world settings. From this review, we propose the Taxonomy of ML Safety that maps state-of-the-art ML techniques to key engineering safety strategies. Our taxonomy of ML safety presents a safety-oriented categorization of ML techniques to provide guidance for improving dependability of the ML design and development. The proposed taxonomy can serve as a safety checklist to aid designers in improving coverage and diversity of safety strategies employed in any given ML system.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {jul},
keywords = {machine learning, robustness, uncertainty quantification, verification, safety}
}

@article{10.1145/3287285,
author = {Fan, Wenfei and Lu, Ping},
title = {Dependencies for Graphs},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3287285},
doi = {10.1145/3287285},
abstract = {This article proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is defined as a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs can express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities (vertices) in a graph. We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound, complete and independent axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication, and validation problems for these extensions.},
journal = {ACM Trans. Database Syst.},
month = {feb},
articleno = {5},
numpages = {40},
keywords = {implication, built-in predicates, satisfiability, Graph dependencies, disjunction, axiom system, validation, keys, EGDs, conditional functional dependencies, TGDs}
}

@article{10.1145/3545176,
author = {Koh, Huan Yee and Ju, Jiaxin and Liu, Ming and Pan, Shirui},
title = {An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3545176},
doi = {10.1145/3545176},
abstract = {Long documents such as academic articles and business reports have been the standard format to detail out important issues and complicated subjects that require extra attention. An automatic summarization system that can effectively condense long documents into short and concise texts to encapsulate the most important information would thus be significant in aiding the reader’s comprehension. Recently, with the advent of neural architectures, significant research efforts have been made to advance automatic text summarization systems, and numerous studies on the challenges of extending these systems to the long document domain have emerged. In this survey, we provide a comprehensive overview of the research on long document summarization and a systematic evaluation across the three principal components of its research setting: benchmark datasets, summarization models, and evaluation metrics. For each component, we organize the literature within the context of long document summarization and conduct an empirical analysis to broaden the perspective on current research progress. The empirical analysis includes a study on the intrinsic characteristics of benchmark datasets, a multi-dimensional analysis of summarization models, and a review of the summarization evaluation metrics. Based on the overall findings, we conclude by proposing possible directions for future exploration in this rapidly growing field.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {jun},
keywords = {neural networks, datasets, document summarization, Transformer, language models}
}

@article{10.1007/s00778-016-0437-2,
author = {Sa, Christopher and Ratner, Alex and R\'{e}, Christopher and Shin, Jaeho and Wang, Feiran and Wu, Sen and Zhang, Ce},
title = {Incremental Knowledge Base Construction Using DeepDive},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-016-0437-2},
doi = {10.1007/s00778-016-0437-2},
abstract = {Populating a database with information from unstructured sources--also known as knowledge base construction (KBC)--is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based, respectively, on sampling and variational techniques. We also study the trade-off space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate DeepDive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality.},
journal = {The VLDB Journal},
month = {feb},
pages = {81–105},
numpages = {25},
keywords = {Performance, Incremental, Knowledge base construction}
}

@inbook{10.1145/3447404.3447418,
author = {Vourvopoulos, A. and Niforatos, E. and Bermudez i Badia, S. and Liarokapis, Fotis},
title = {Brain–Computer Interfacing with Interactive Systems—Case Study 2},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447418},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {237–272},
numpages = {36}
}

@inbook{10.1145/3447404.3447412,
author = {Alexander, Jason and Thanh Vi, Chi},
title = {DSP Basics},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447412},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {105–139},
numpages = {35}
}

@techreport{10.1145/3129538,
author = {Topi, Heikki and Karsten, Helena and Brown, Sue A. and Carvalho, Jo\~{a}o Alvaro and Donnellan, Brian and Shen, Jun and Tan, Bernard C. Y. and Thouin, Mark F.},
title = {MSIS 2016: Global Competency Model for Graduate Degree Programs in Information Systems},
year = {2017},
isbn = {9781459354325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This document, "MSIS 2016: Global Competency Model for Graduate Degree Programs in Information Systems", is the latest in the series of reports that provides guidance for degree programs in the Information Systems (IS) academic discipline. The first of these reports (Ashenhurst, 1972) was published in the early 1970s, and the work has continued ever since both at the undergraduate and master's levels. The Association for Computing Machinery (ACM) has sponsored the reports from the beginning. Since the Association for Information Systems (AIS) was established in the mid-1990s, the two organizations have collaborated on the production of curriculum recommendations for the IS discipline. At the undergraduate level, both the Association for Information Technology Professionals (AITP) (formerly DPMA) and the International Federation for Information Processing (IFIP) have also made significant contributions to the curriculum recommendations.}
}

@inbook{10.1145/3447404.3447426,
author = {Buschek, Daniel and Alt, Florian},
title = {Building Adaptive Touch Interfaces—Case Study 6},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447426},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {379–406},
numpages = {28}
}

@article{10.1162/EVCO_a_00161,
author = {Boukhelifa, N. and Bezerianos, A. and Cancino, W. and Lutton, E.},
title = {Evolutionary Visual Exploration: Evaluation of an Iec Framework for Guided Visual Search},
year = {2017},
issue_date = {Spring 2017},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {25},
number = {1},
issn = {1063-6560},
url = {https://doi.org/10.1162/EVCO_a_00161},
doi = {10.1162/EVCO_a_00161},
abstract = {We evaluate and analyse a framework for evolutionary visual exploration EVE that guides users in exploring large search spaces. EVE uses an interactive evolutionary algorithm to steer the exploration of multidimensional data sets toward two-dimensional projections that are interesting to the analyst. Our method smoothly combines automatically calculated metrics and user input in order to propose pertinent views to the user. In this article, we revisit this framework and a prototype application that was developed as a demonstrator, and summarise our previous study with domain experts and its main findings. We then report on results from a new user study with a clearly predefined task, which examines how users leverage the system and how the system evolves to match their needs. While we previously showed that using EVE, domain experts were able to formulate interesting hypotheses and reach new insights when exploring freely, our new findings indicate that users, guided by the interactive evolutionary algorithm, are able to converge quickly to an interesting view of their data when a clear task is specified. We provide a detailed analysis of how users interact with an evolutionary algorithm and how the system responds to their exploration strategies and evaluation patterns. Our work aims at building a bridge between the domains of visual analytics and interactive evolution. The benefits are numerous, in particular for evaluating interactive evolutionary computation IEC techniques based on user study methodologies.},
journal = {Evol. Comput.},
month = {mar},
pages = {55–86},
numpages = {32},
keywords = {interactive evolutionary computation, Interactive evolutionary algorithms, information visualization., data mining, visual analytics, genetic programming}
}

@article{10.1145/3379444,
author = {Alam, Iqbal and Sharif, Kashif and Li, Fan and Latif, Zohaib and Karim, M. M. and Biswas, Sujit and Nour, Boubakr and Wang, Yu},
title = {A Survey of Network Virtualization Techniques for Internet of Things Using SDN and NFV},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3379444},
doi = {10.1145/3379444},
abstract = {Internet of Things (IoT) and Network Softwarization are fast becoming core technologies of information systems and network management for the next-generation Internet. The deployment and applications of IoT range from smart cities to urban computing and from ubiquitous healthcare to tactile Internet. For this reason, the physical infrastructure of heterogeneous network systems has become more complicated and thus requires efficient and dynamic solutions for management, configuration, and flow scheduling. Network softwarization in the form of Software Defined Networks and Network Function Virtualization has been extensively researched for IoT in the recent past. In this article, we present a systematic and comprehensive review of virtualization techniques explicitly designed for IoT networks. We have classified the literature into software-defined networks designed for IoT, function virtualization for IoT networks, and software-defined IoT networks. These categories are further divided into works that present architectural, security, and management solutions. Besides, the article highlights several short-term and long-term research challenges and open issues related to the adoption of software-defined Internet of Things.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {35},
numpages = {40},
keywords = {Internet of Things, network function virtualization, software-defined IoT, software-defined network, network softwarization}
}

@inbook{10.1145/3447404.3447416,
author = {Komninos, Andreas and Dunlop, Mark D. and Wilson, John N.},
title = {Combining Infrastructure Sensor and Tourism Market Data in a Smart City Project—Case Study 1},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447416},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {197–233},
numpages = {37}
}

@inproceedings{10.1145/3411764.3445517,
author = {Utz, Christine and Becker, Steffen and Schnitzler, Theodor and Farke, Florian M. and Herbert, Franziska and Schaewitz, Leonie and Degeling, Martin and D\"{u}rmuth, Markus},
title = {Apps Against the Spread: Privacy Implications and User Acceptance of COVID-19-Related Smartphone Apps on Three Continents},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445517},
doi = {10.1145/3411764.3445517},
abstract = {The COVID-19 pandemic has fueled the development of smartphone applications to assist disease management. Many “corona apps” require widespread adoption to be effective, which has sparked public debates about the privacy, security, and societal implications of government-backed health applications. We conducted a representative online study in Germany (n = 1003), the US (n = 1003), and China (n = 1019) to investigate user acceptance of corona apps, using a vignette design based on the contextual integrity framework. We explored apps for contact tracing, symptom checks, quarantine enforcement, health certificates, and mere information. Our results provide insights into data processing practices that foster adoption and reveal significant differences between countries, with user acceptance being highest in China and lowest in the US. Chinese participants prefer the collection of personalized data, while German and US participants favor anonymity. Across countries, contact tracing is viewed more positively than quarantine enforcement, and technical malfunctions negatively impact user acceptance.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {70},
numpages = {22},
keywords = {digital contact tracing, privacy, COVID-19},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{10.1145/3412364,
author = {Ma, Qian and Gu, Yu and Lee, Wang-Chien and Yu, Ge and Liu, Hongbo and Wu, Xindong},
title = {REMIAN: Real-Time and Error-Tolerant Missing Value Imputation},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3412364},
doi = {10.1145/3412364},
abstract = {Missing value (MV) imputation is a critical preprocessing means for data mining. Nevertheless, existing MV imputation methods are mostly designed for batch processing, and thus are not applicable to streaming data, especially those with poor quality. In this article, we propose a framework, called Real-time and Error-tolerant Missing vAlue ImputatioN (REMAIN), to impute MVs in poor-quality streaming data. Instead of imputing MVs based on all the observed data, REMAIN first initializes the MV imputation model based on a-RANSAC which is capable of detecting and rejecting anomalies in an efficient manner, and then incrementally updates the model parameters upon the arrival of new data to support real-time MV imputation. As the correlations among attributes of the data may change over time in unforseenable ways, we devise a deterioration detection mechanism to capture the deterioration of the imputation model to further improve the imputation accuracy. Finally, we conduct an extensive evaluation on the proposed algorithms using real-world and synthetic datasets. Experimental results demonstrate that REMAIN achieves significantly higher imputation accuracy over existing solutions. Meanwhile, REMAIN improves up to one order of magnitude in time cost compared with existing approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {sep},
articleno = {77},
numpages = {38},
keywords = {poor-quality streaming data, Missing value, real-time imputation}
}

@article{10.1145/3311955,
author = {Pimentel, Jo\~{a}o Felipe and Freire, Juliana and Murta, Leonardo and Braganholo, Vanessa},
title = {A Survey on Collecting, Managing, and Analyzing Provenance from Scripts},
year = {2019},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3311955},
doi = {10.1145/3311955},
abstract = {Scripts are widely used to design and run scientific experiments. Scripting languages are easy to learn and use, and they allow complex tasks to be specified and executed in fewer steps than with traditional programming languages. However, they also have important limitations for reproducibility and data management. As experiments are iteratively refined, it is challenging to reason about each experiment run (or trial), to keep track of the association between trials and experiment instances as well as the differences across trials, and to connect results to specific input data and parameters. Approaches have been proposed that address these limitations by collecting, managing, and analyzing the provenance of scripts. In this article, we survey the state of the art in provenance for scripts. We have identified the approaches by following an exhaustive protocol of forward and backward literature snowballing. Based on a detailed study, we propose a taxonomy and classify the approaches using this taxonomy.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {47},
numpages = {38},
keywords = {collecting, survey, scripts, analyzing, Provenance, managing}
}

@inbook{10.1145/3447404.3447420,
author = {Vertanen, Keith},
title = {Probabilistic Text Entry—Case Study 3},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447420},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {277–320},
numpages = {44}
}

@inbook{10.1145/3447404.3447410,
author = {Arif, Ahmed Sabbir},
title = {Statistical Grounding},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447410},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {59–99},
numpages = {41}
}

@article{10.1145/2996465,
author = {Guo, Guangming and Zhu, Feida and Chen, Enhong and Liu, Qi and Wu, Le and Guan, Chu},
title = {From Footprint to Evidence: An Exploratory Study of Mining Social Data for Credit Scoring},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1559-1131},
url = {https://doi.org/10.1145/2996465},
doi = {10.1145/2996465},
abstract = {With the booming popularity of online social networks like Twitter and Weibo, online user footprints are accumulating rapidly on the social web. Simultaneously, the question of how to leverage the large-scale user-generated social media data for personal credit scoring comes into the sight of both researchers and practitioners. It has also become a topic of great importance and growing interest in the P2P lending industry. However, compared with traditional financial data, heterogeneous social data presents both opportunities and challenges for personal credit scoring. In this article, we seek a deep understanding of how to learn users’ credit labels from social data in a comprehensive and efficient way. Particularly, we explore the social-data-based credit scoring problem under the micro-blogging setting for its open, simple, and real-time nature. To identify credit-related evidence hidden in social data, we choose to conduct an analytical and empirical study on a large-scale dataset from Weibo, the largest and most popular tweet-style website in China. Summarizing results from existing credit scoring literature, we first propose three social-data-based credit scoring principles as guidelines for in-depth exploration. In addition, we glean six credit-related insights arising from empirical observations of the testbed dataset. Based on the proposed principles and insights, we extract prediction features mainly from three categories of users’ social data, including demographics, tweets, and networks. To harness this broad range of features, we put forward a two-tier stacking and boosting enhanced ensemble learning framework. Quantitative investigation of the extracted features shows that online social media data does have good potential in discriminating good credit users from bad. Furthermore, we perform experiments on the real-world Weibo dataset consisting of more than 7.3 million tweets and 200,000 users whose credit labels are known through our third-party partner. Experimental results show that (i) our approach achieves a roughly 0.625 AUC value with all the proposed social features as input, and (ii) our learning algorithm can outperform traditional credit scoring methods by as much as 17% for social-data-based personal credit scoring.},
journal = {ACM Trans. Web},
month = {dec},
articleno = {22},
numpages = {38},
keywords = {social data, user profiling, Personal credit scoring, P2P lending, consumer finance, features}
}

@proceedings{10.1145/3524458,
title = {GoodIT '22: Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@article{10.1145/3377455,
author = {Papadakis, George and Skoutas, Dimitrios and Thanos, Emmanouil and Palpanas, Themis},
title = {Blocking and Filtering Techniques for Entity Resolution: A Survey},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3377455},
doi = {10.1145/3377455},
abstract = {Entity Resolution (ER), a core task of Data Integration, detects different entity profiles that correspond to the same real-world object. Due to its inherently quadratic complexity, a series of techniques accelerate it so that it scales to voluminous data. In this survey, we review a large number of relevant works under two different but related frameworks: Blocking and Filtering. The former restricts comparisons to entity pairs that are more likely to match, while the latter identifies quickly entity pairs that are likely to satisfy predetermined similarity thresholds. We also elaborate on hybrid approaches that combine different characteristics. For each framework we provide a comprehensive list of the relevant works, discussing them in the greater context. We conclude with the most promising directions for future work in the field.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {31},
numpages = {42},
keywords = {filtering, entity resolution, Blocking}
}

@article{10.5555/3546258.3546365,
author = {Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and Goyal, Naman and Birch, Tom and Liptchinsky, Vitaliy and Edunov, Sergey and Grave, Edouard and Auli, Michael and Joulin, Armand},
title = {Beyond English-Centric Multilingual Machine Translation},
year = {2022},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric, training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open-source a training data set that covers thousands of language directions with parallel data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems from the Workshop on Machine Translation (WMT). We open-source our scripts so that others may reproduce the data, evaluation, and final M2M- 100 model},
journal = {J. Mach. Learn. Res.},
month = {jul},
articleno = {107},
numpages = {48},
keywords = {multilingual machine translation, model scaling, neural networks, many-to-many, bitext mining}
}

@article{10.1145/3428077,
author = {Ghosh, Aindrila and Nashaat, Mona and Miller, James and Quader, Shaikh},
title = {Context-Based Evaluation of Dimensionality Reduction Algorithms—Experiments and Statistical Significance Analysis},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3428077},
doi = {10.1145/3428077},
abstract = {Dimensionality reduction is a commonly used technique in data analytics. Reducing the dimensionality of datasets helps not only with managing their analytical complexity but also with removing redundancy. Over the years, several such algorithms have been proposed with their aims ranging from generating simple linear projections to complex non-linear transformations of the input data. Subsequently, researchers have defined several quality metrics in order to evaluate the performances of different algorithms. Hence, given a plethora of dimensionality reduction algorithms and metrics for their quality analysis, there is a long-existing need for guidelines on how to select the most appropriate algorithm in a given scenario. In order to bridge this gap, in this article, we have compiled 12 state-of-the-art quality metrics and categorized them into 5 identified analytical contexts. Furthermore, we assessed 15 most popular dimensionality reduction algorithms on the chosen quality metrics using a large-scale and systematic experimental study. Later, using a set of robust non-parametric statistical tests, we assessed the generalizability of our evaluation on 40 real-world datasets. Finally, based on our results, we present practitioners’ guidelines for the selection of an appropriate dimensionally reduction algorithm in the present analytical contexts.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jan},
articleno = {24},
numpages = {40},
keywords = {Dimensionality reduction, statistical significance analysis, context-based evaluation, quality metrics}
}

@proceedings{10.1145/3543712,
title = {ICCTA '22: Proceedings of the 2022 8th International Conference on Computer Technology Applications},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@article{10.1145/3397198,
author = {Fan, Wenfei and Hu, Chunming and Liu, Xueli and Lu, Ping},
title = {Discovering Graph Functional Dependencies},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3397198},
doi = {10.1145/3397198},
abstract = {This article studies discovery of Graph Functional Dependencies (GFDs), a class of functional dependencies defined on graphs. We investigate the fixed-parameter tractability of three fundamental problems related to GFD discovery. We show that the implication and satisfiability problems are fixed-parameter tractable, but the validation problem is co-W[1]-hard in general. We introduce notions of reduced GFDs and their topological support, and formalize the discovery problem for GFDs. We develop algorithms for discovering GFDs and computing their covers. Moreover, we show that GFD discovery is feasible over large-scale graphs, by providing parallel scalable algorithms that guarantee to reduce running time when more processors are used. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.},
journal = {ACM Trans. Database Syst.},
month = {sep},
articleno = {15},
numpages = {42},
keywords = {graphs, validation, Functional dependencies, discovery, implication}
}

@book{10.1145/3447404,
editor = {Eslambolchilar, Parisa and Komninos, Andreas and Dunlop, Mark},
title = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {34},
abstract = {Intelligent Computing for Interactive System Design provides a comprehensive resource on what has become the dominant paradigm in designing novel interaction methods, involving gestures, speech, text, touch and brain-controlled interaction, embedded in innovative and emerging human–computer interfaces. These interfaces support ubiquitous interaction with applications and services running on smartphones, wearables, in-vehicle systems, virtual and augmented reality, robotic systems, the Internet of Things (IoT), and many other domains that are now highly competitive, both in commercial and in research contexts.This book presents the crucial theoretical foundations needed by any student, researcher, or practitioner working on novel interface design, with chapters on statistical methods, digital signal processing (DSP), and machine learning (ML). These foundations are followed by chapters that discuss case studies on smart cities, brain–computer interfaces, probabilistic mobile text entry, secure gestures, personal context from mobile phones, adaptive touch interfaces, and automotive user interfaces. The case studies chapters also highlight an in-depth look at the practical application of DSP and ML methods used for processing of touch, gesture, biometric, or embedded sensor inputs. A common theme throughout the case studies is ubiquitous support for humans in their daily professional or personal activities.In addition, the book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal and multi-sensor systems. In a series of short additions to each chapter, an expert on the legal and ethical issues explores the emergent deep concerns of the professional community, on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during ubiquitous interaction with omnipresent computers.This carefully edited collection is written by international experts and pioneers in the fields of DSP and ML. It provides a textbook for students and a reference and technology roadmap for developers and professionals working on interaction design on emerging platforms.}
}

@proceedings{10.1145/3545729,
title = {ICMHI '22: Proceedings of the 6th International Conference on Medical and Health Informatics},
year = {2022},
isbn = {9781450396301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Japan}
}

@proceedings{10.1145/3556384,
title = {SPML '22: Proceedings of the 2022 5th International Conference on Signal Processing and Machine Learning},
year = {2022},
isbn = {9781450396912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dalian, China}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to <i>The Web Conference 2019</i>. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@article{10.1145/3529318,
author = {Braiek, Houssem Ben and Khomh, Foutse},
title = {Testing Feedforward Neural Networks Training Programs},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3529318},
doi = {10.1145/3529318},
abstract = {Nowadays, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters in order to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error-prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this paper, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design, TheDeepChecker, an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger (SMD). Results show that TheDeepChecker’s on-execution validation of DNN-based program’s properties through three sequential phases (pre-, on-, and post-fitting), succeeds in revealing several coding bugs and system misconfigurations errors, early on and at a low cost. Moreover, our property-based approach outperforms the SMD’s offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
keywords = {training programs, property-based debugging, neural networks}
}

@proceedings{10.1145/3555009,
title = {UKICER '22: Proceedings of the 2022 Conference on United Kingdom &amp; Ireland Computing Education Research},
year = {2022},
isbn = {9781450397421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dublin, Ireland}
}

@proceedings{10.1145/2970276,
title = {ASE 2016: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/3544902,
title = {ESEM '22: Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Helsinki, Finland}
}

@proceedings{10.1145/3545839,
title = {ICoMS '22: Proceedings of the 2022 5th International Conference on Mathematics and Statistics},
year = {2022},
isbn = {9781450396233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Paris, France}
}

@proceedings{10.1145/3562007,
title = {CCRIS '22: Proceedings of the 2022 3rd International Conference on Control, Robotics and Intelligent System},
year = {2022},
isbn = {9781450396851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, China}
}

@proceedings{10.1145/3549555,
title = {CBMI '22: Proceedings of the 19th International Conference on Content-Based Multimedia Indexing},
year = {2022},
isbn = {9781450397209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Graz, Austria}
}

@proceedings{10.1145/2988458,
title = {SA '16: SIGGRAPH ASIA 2016 Courses},
year = {2016},
isbn = {9781450345385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The SIGGRAPH Asia Courses program will feature a variety of instructional sessions catered to the different levels of expertise of our attendees. Sessions from introductory to advanced topics in computer graphics and interactive techniques will be conducted by speakers from renowned organizations and academic research institutions from over the world.The program has been the premier source for practitioners, developers, researchers, artists, and students who want to learn about the state-of-the-art technologies in computer graphics and their related topics.},
location = {Macau}
}

@proceedings{10.1145/3546155,
title = {NordiCHI '22: Nordic Human-Computer Interaction Conference},
year = {2022},
isbn = {9781450396998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Aarhus, Denmark}
}

@proceedings{10.1145/3549015,
title = {EuroUSEC '22: Proceedings of the 2022 European Symposium on Usable Security},
year = {2022},
isbn = {9781450397001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Karlsruhe, Germany}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/3545948,
title = {RAID '22: Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses},
year = {2022},
isbn = {9781450397049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@proceedings{10.1145/3538969,
title = {ARES '22: Proceedings of the 17th International Conference on Availability, Reliability and Security},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3543829,
title = {CUI '22: Proceedings of the 4th Conference on Conversational User Interfaces},
year = {2022},
isbn = {9781450397391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Glasgow, United Kingdom}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@book{10.1145/3015783,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations - Volume 1},
year = {2017},
isbn = {9781970001679},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {14},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces-user input involving new media (speech, multi-touch, gestures, writing) embedded in multimodal-multisensor interfaces. These interfaces support smartphones, wearables, in-vehicle, robotic, and many other applications that are now highly competitive commercially. This edited collection is written by international experts and pioneers in the field. It provides a textbook for students, and a reference and technology roadmap for professionals working in this rapidly emerging area. Volume 1 of the handbook presents relevant theory and neuroscience foundations for guiding the development of high-performance systems. Additional chapters discuss approaches to user modeling, interface design that supports user choice, synergistic combination of modalities with sensors, and blending of multimodal input and output. They also highlight an in-depth look at the most common multimodal-multisensor combinations- for example, touch and pen input, haptic and non-speech audio output, and speech co-processed with visible lip movements, gaze, gestures, or pen input. A common theme throughout is support for mobility and individual differences among users-including the world's rapidly growing population of seniors. These handbook chapters provide walk-through examples and video illustrations of different system designs and their interactive use. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal-multisensor systems. In the final chapter, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces should be designed in the future to most effectively advance human performance.}
}

@book{10.1145/3233795,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.}
}

@proceedings{10.1145/3533271,
title = {ICAIF '22: Proceedings of the Third ACM International Conference on AI in Finance},
year = {2022},
isbn = {9781450393768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/3505270,
title = {CHI PLAY '22: Extended Abstracts of the 2022 Annual Symposium on Computer-Human Interaction in Play},
year = {2022},
isbn = {9781450392112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bremen, Germany}
}

@proceedings{10.1145/3555228,
title = {SBES '22: Proceedings of the XXXVI Brazilian Symposium on Software Engineering},
year = {2022},
isbn = {9781450397353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Brazil}
}

@proceedings{10.1145/2993148,
title = {ICMI '16: Proceedings of the 18th ACM International Conference on Multimodal Interaction},
year = {2016},
isbn = {9781450345569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3543758,
title = {MuC '22: Proceedings of Mensch Und Computer 2022},
year = {2022},
isbn = {9781450396905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Darmstadt, Germany}
}

