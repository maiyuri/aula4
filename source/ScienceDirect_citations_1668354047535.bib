@article{KIBRIA2022107672,
title = {The severity prediction of the binary and multi-class cardiovascular disease − A machine learning-based fusion approach},
journal = {Computational Biology and Chemistry},
volume = {98},
pages = {107672},
year = {2022},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2022.107672},
url = {https://www.sciencedirect.com/science/article/pii/S1476927122000524},
author = {Hafsa Binte Kibria and Abdul Matin},
keywords = {Artificial neural network, Random forest, Decision tree, Adaboost, Support vector machine, Logistic regression, Cardiovascular disease, Weighted score fusion},
abstract = {In today’s world, a massive amount of data is available in almost every sector. This data has become an asset as we can use this enormous amount of data to find information. Mainly health care industry contains many data consisting of patient and disease-related information. By using the machine learning technique, we can look for hidden data patterns to predict various diseases. Recently CVDs, or cardiovascular disease, have become a leading cause of death around the world. The number of death due to CVDs is frightening. That is why many researchers are trying their best to design a predictive model that can save many lives using the data mining model. In this research, some fusion models have been constructed to diagnose CVDs along with its severity. Machine learning(ML) algorithms like artificial neural network, SVM, logistic regression, decision tree, random forest, and AdaBoost have been applied to the heart disease dataset to predict disease. Randomoversampler only for multi-class classification to make the imbalanced dataset balanced. To improve the performance of classification, a weighted score fusion approach was taken. At first, the models were trained. After training, two algorithms’ decision was combined using a weighted sum rule. A total of three fusion models have been developed from the six ML algorithms. The results were promising in the performance parameter. The proposed approach has been experimented with different test training ratios for binary and multiclass classification problems, and for both of them, the fusion models performed well. The highest accuracy for multiclass classification was found as 75%, and it was 95% for binary.}
}
@article{SHAO2022252,
title = {Simplified quality assessment for small-molecule ligands in the Protein Data Bank},
journal = {Structure},
volume = {30},
number = {2},
pages = {252-262.e4},
year = {2022},
issn = {0969-2126},
doi = {https://doi.org/10.1016/j.str.2021.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0969212621003713},
author = {Chenghua Shao and John D. Westbrook and Changpeng Lu and Charmi Bhikadiya and Ezra Peisach and Jasmine Y. Young and Jose M. Duarte and Robert Lowe and Sijian Wang and Yana Rose and Zukang Feng and Stephen K. Burley},
keywords = {ligand structure quality, composite ranking score, ligand structure, small-molecule ligand, ligand quality indicator, multivariate analysis, principal component analysis, Protein Data Bank, PDB, RCSB PDB},
abstract = {Summary
More than 70% of the experimentally determined macromolecular structures in the Protein Data Bank (PDB) contain small-molecule ligands. Quality indicators of ∼643,000 ligands present in ∼106,000 PDB X-ray crystal structures have been analyzed. Ligand quality varies greatly with regard to goodness of fit between ligand structure and experimental data, deviations in bond lengths and angles from known chemical structures, and inappropriate interatomic clashes between the ligand and its surroundings. Based on principal component analysis, correlated quality indicators of ligand structure have been aggregated into two largely orthogonal composite indicators measuring goodness of fit to experimental data and deviation from ideal chemical structure. Ranking of the composite quality indicators across the PDB archive enabled construction of uniformly distributed composite ranking score. This score is implemented at RCSB.org to compare chemically identical ligands in distinct PDB structures with easy-to-interpret two-dimensional ligand quality plots, allowing PDB users to quickly assess ligand structure quality and select the best exemplars.}
}
@incollection{FABBECOSTES2022289,
title = {Chapter 17 - Automotive supply chain digitalization: lessons and perspectives},
editor = {Bart L. MacCarthy and Dmitry Ivanov},
booktitle = {The Digital Supply Chain},
publisher = {Elsevier},
pages = {289-308},
year = {2022},
isbn = {978-0-323-91614-1},
doi = {https://doi.org/10.1016/B978-0-323-91614-1.00017-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323916141000174},
author = {Nathalie Fabbe-Costes and Lucie Lechaptois},
keywords = {Automotive industry, Car manufacturer, Digital technologies, Digitalization process, Supply chain digitalization},
abstract = {Supply chain (SC) digitalization in the automotive industry is an old story but one that raises many contemporary strategic issues. The objective of this chapter is to better understand the complexity of SC digitalization and gain greater insights into its dynamics. We combine a historical overview of the evolving process of SC digitalization in the industry with the analysis of a specific case: one car manufacturer's ongoing SC digitalization journey. The evolution of automotive SCs and their digitalization falls into five main historical eras. Digitalization has an impact on all SC management processes from the design of cars to their end-of-life. It is a complex, multifactor, multilayer, and multiactor coevolving process. Digital technologies are a major factor of change but are also combining with other macro trends in the sector. Today's automotive sector has become a complex ecosystem with new players and stakeholders. External factors interact with SC digitalization strategies, resulting in multiple projects involving heterogeneous stakeholders. We highlight the complexity of the decision-making processes on whether to adopt specific technologies, and the strategic, organizational, and operational challenges they bring. We note the changes affecting the future of mobility solutions, which in turn will affect SC digitalization in the sector.}
}
@article{JACOBS2022114552,
title = {Cultural reproduction of mental illness stigma and stereotypes},
journal = {Social Science & Medicine},
volume = {292},
pages = {114552},
year = {2022},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.114552},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621008844},
author = {Susan Jacobs and Joseph Quinn},
keywords = {Stigma, Mental illness, Stereotypes, Cultural schema, Cultural transmission, Affect control theory},
abstract = {This study investigates how schemas and stereotypes about individuals with mental illness shape how information is transmitted between people. Mental illnesses are highly stigmatized identities, and prior work illustrates the persistence of mental illness stigma, despite public health efforts aimed at increasing awareness of the biological origins of mental illness (Pescosolido et al., 2010). Recent work has also demonstrated the utility of combining cultural cognition with social psychological theories of cultural meaning to investigate how stereotypes are transmitted through secondhand narratives (Hunzaker 2014, 2016). We connect this social psychological work with medical sociological literature on mental illness stigmas and propose that stereotypes function as cultural schemas that shape the way stories are remembered and retold about individuals with a mental illness. We then conduct a narrative transmission study to test this proposal, using schizophrenia as a case of interest. Consistent with prior work, we find that individuals who retell a story about a person with schizophrenia alter the narrative so that it becomes more consistent with stereotypes about individuals with schizophrenia. We also find that stereotype-inconsistent information is more likely to be transformed to align with culturally shared beliefs about schizophrenia. The findings extend prior work on how bias shapes the reproduction of mental illness stereotypes, and demonstrate how socially learned cultural beliefs can reinforce stereotypes, biases and stigma about mental illness.}
}
@article{WEI2022112775,
title = {Full-coverage mapping and spatiotemporal variations of ground-level ozone (O3) pollution from 2013 to 2020 across China},
journal = {Remote Sensing of Environment},
volume = {270},
pages = {112775},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112775},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721004958},
author = {Jing Wei and Zhanqing Li and Ke Li and Russell R. Dickerson and Rachel T. Pinker and Jun Wang and Xiong Liu and Lin Sun and Wenhao Xue and Maureen Cribb},
keywords = {Ozone, Air pollution, Ensemble learning, COVID-19, China},
abstract = {Ozone (O3) is an important trace and greenhouse gas in the atmosphere, posing a threat to the ecological environment and human health at the ground level. Large-scale and long-term studies of O3 pollution in China are few due to highly limited direct ground and satellite measurements. This study offers a new perspective to estimate ground-level O3 from solar radiation intensity and surface temperature by employing an extended ensemble learning of the space-time extremely randomized trees (STET) model, together with ground-based observations, remote sensing products, atmospheric reanalysis, and an emission inventory. A full-coverage (100%), high-resolution (10 km) and high-quality daily maximum 8-h average (MDA8) ground-level O3 dataset covering China (called ChinaHighO3) from 2013 to 2020 was generated. Our MDA8 O3 estimates (predictions) are reliable, with an average out-of-sample (out-of-station) coefficient of determination of 0.87 (0.80) and root-mean-square error of 17.10 (21.10) μg/m3 in China. The unique advantage of the full coverage of our dataset allowed us to accurately capture a short-term severe O3 pollution exposure event that took place from 23 April to 8 May in 2020. Also, a rapid increase and recovery of O3 concentrations associated with variations in anthropogenic emissions were seen during and after the COVID-19 lockdown, respectively. Trends in O3 concentration showed an average growth rate of 2.49 μg/m3/yr (p < 0.001) from 2013 to 2020, along with the continuous expansion of polluted areas exceeding the daily O3 standard (i.e., MDA8 O3 = 160 μg/m3). Summertime O3 concentrations and the probability of occurrence of daily O3 pollution have significantly increased since 2015, especially in the North China Plain and the main air pollution transmission belt (i.e., the “2 + 26” cities). However, a decline in both was seen in 2020, mainly due to the coordinated control of air pollution and ongoing COVID-19 effects. This carefully vetted and smoothed dataset is valuable for studies on air pollution and environmental health in China.}
}
@article{SYED2022330,
title = {High-Intensity Hospital Utilization Among Adults With Diabetic Foot Ulcers: A Population-based Study},
journal = {Canadian Journal of Diabetes},
volume = {46},
number = {4},
pages = {330-336.e7},
year = {2022},
issn = {1499-2671},
doi = {https://doi.org/10.1016/j.jcjd.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1499267121004184},
author = {Muzammil H. Syed and Mohammed Al-Omran and Joel G. Ray and Muhammad Mamdani and Charles {de Mestral}},
keywords = {Canada, diabetic foot ulcers, epidemiology, health-care burden, population-based, Canada, ulcères du pied diabétique, épidémiologie, fardeau des soins de santé, populationnelle},
abstract = {Background
Diabetic foot ulcers (DFUs) are common and disabling, necessitating lengthy hospitalizations. In this study we sought to identify potentially modifiable determinants of high-intensity hospital care use among adults with DFUs.
Methods
Three related case–control studies were conducted using Canada-wide cohorts of adults hospitalized with a DFU from 2011 to 2015. In study 1, cases comprised the top 10% with the highest cumulative 1-year acute care hospital costs; controls were randomly selected from those below the top 10%. Study 2 comprised cases/controls within/below the top 10% for cumulative acute care hospital length of stay (LOS). Study 3 included cases/controls within/below the top 10% for cumulative number of acute care hospitalizations. Using generalized linear models, predictor variables were tested between cases and controls, while adjusting for age and sex.
Results
In study 1, mean acute care costs among 8,971 cases and 3,174 controls were $71,757 and $13,687, respectively. Sepsis conferred the greatest excess cost (mean, $38,790; 95% confidence interval [CI], $34,597 to $43,508), followed by chronic kidney disease (mean, $30,607; 95% CI, $28,389 to $32,825) and major lower limb amputation (mean, $30,884; 95% CI, $28,613 to $33,155). In study 2, mean LOS was higher among 8,477 cases (69 days) than 3,467 controls (12 days). Lower limb amputation conferred the greatest adjusted excess in mean LOS (mean, 28 days; 95% CI, 27 to 28 days). In study 3, there was a mean of 3 hospitalizations among 10,341 cases and 1 among 5,509 controls. Peripheral artery disease conferred the greatest excess number of hospitalizations (1.3 more hospitalizations; 1.2 to 1.4).
Conclusions
Early aggressive treatment of chronic kidney disease and peripheral artery disease, alongside guideline-based amputation prevention strategies, may reduce high-intensity hospital care use among adults with DFUs.
Résumé
Introduction
Les ulcères du pied diabétique (UPD) sont fréquents et handicapants, et nécessitent de longues hospitalisations. Dans la présente étude, nous avons cherché à cerner les facteurs déterminants potentiellement modifiables du recours considérable aux soins hospitaliers chez les adultes atteints d’UPD.
Méthodes
Trois études cas témoins connexes ont été menées auprès de cohortes d’adultes atteints d’UPD et hospitalisés dans l’ensemble du Canada de 2011 à 2015. Dans l’étude 1, les cas représentaient les coûts hospitaliers cumulatifs les plus élevés en soins de courte durée pendant 1 année dans les 10 % supérieurs; les témoins étaient sélectionnés de façon aléatoire parmi ceux en dessous des 10 % supérieurs. L’étude 2 était composée de cas et de témoins dont la durée de séjour (DS) cumulative à l’hôpital en soins de courte durée était dans ou en dessous des 10 % supérieures. L’étude 3 portait sur des cas et des témoins dont le nombre cumulatif d’hospitalisations en soins de courte durée était dans ou en dessous des 10 % supérieurs. À l’aide des modèles linéaires généralisés, nous avons vérifié les variables de prédiction entre les cas et les témoins, tout en les ajustant selon l’âge et le sexe.
Résultats
Dans l’étude 1, les coûts moyens en soins de courte durée des 8971 cas et des 3174 témoins étaient respectivement de 71 757 $ et 13 687 $. La sepsie générait les coûts excédentaires les plus importants (moyenne, 38 790 $; intervalle de confiance [IC] à 95 %, de 34 597 $ à 43 508 $), puis suivaient l’insuffisance rénale chronique (moyenne, 30 607 $; IC à 95 %, de 28 389 $ à 32 825 $) et l’amputation majeure d’un membre inférieur (moyenne, 30 884 $; IC à 95 %, de 28 613 $ à 33 155 $). Dans l’étude 2, la DS moyenne des 8477 cas (69 jours) était plus élevée que chez les 3467 témoins (12 jours). L’amputation d’un membre inférieur générait l’allongement de la DS moyenne ajustée le plus important (moyenne, 28 jours; IC à 95 %, de 27 à 28 jours). Dans l’étude 3, il y avait une moyenne de 3 hospitalisations chez les 10 341 cas et 1 chez les 5509 témoins. La maladie artérielle périphérique générait le nombre excédentaire d’hospitalisations le plus important (1,3 hospitalisation excédentaire; de 1,2 à 1,4).
Conclusions
Le traitement vigoureux précoce de l’insuffisance rénale chronique et de la maladie artérielle périphérique, et les stratégies de prévention de l’amputation fondées sur les lignes directrices peuvent faire diminuer le recours considérable aux soins hospitaliers chez les adultes atteints d’UPD.}
}
@article{FENG2022130816,
title = {The spatial spillover effects and impact paths of financial agglomeration on green development: Evidence from 285 prefecture-level cities in China},
journal = {Journal of Cleaner Production},
volume = {340},
pages = {130816},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130816},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622004541},
author = {Yidai Feng and Longhui Zou and Huaxi Yuan and Lu Dai},
keywords = {Financial agglomeration, Green development, Spatial spillover effect, Heterogeneity analysis, Impact path},
abstract = {Financial agglomeration is a key way to promote the green transformation of the global economy and environmental governance. The existing literature mainly analyzed the impact of financial agglomeration on economic development, but rarely examines its comprehensive impact on economic growth and environmental governance considering spatial spillover effects. Based on the perspective of spatial spillover effects, this paper uses panel data of 285 cities above the prefecture-level in China to examine the impact and mechanism of financial agglomeration on green development. The results show that: (1) During the study period, financial agglomeration and green development in China are significantly different in directions of spatial distribution and expansion, but the spatial correlation between the two is relatively high. (2) Financial agglomeration can exert significant spatial spillover effects on green development. (3) Financial agglomeration has more obvious positive spatial spillover effects on green development of cities in the servitization stage or with high administrative level. (4) Industrial structure upgrading effect, labor upgrading effect and technological innovation effect are the main paths that financial agglomeration affects green development. The research results can provide a new perspective and approach to promote the green transformation of economy and sustainable development of the world.}
}
@article{ERDOS2022105639,
title = {The UK and the EU personal data framework after Brexit: A new trade and cooperation partnership grounded in Council of Europe Convention 108+?},
journal = {Computer Law & Security Review},
volume = {44},
pages = {105639},
year = {2022},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105639},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921001126},
author = {David Erdos},
keywords = {Adequacy, Brexit, Council of Europe, Data protection, Data Protection Convention 108+, GDPR, Law Enforcement Directive 2016/680, Transparency rules, Sensitive data},
abstract = {The EU-UK post-Brexit agreements provide for the UK to have the closest relationship on personal data with the EU outside of the European Economic Area (EEA) and Switzerland. In the area of justice and security, the Trade and Cooperation Agreement itself provides for very extensive data exchange including DNA and fingerprints and is complemented by the first ever mutual adequacy agreement within the area of law enforcement. In some contrast, the general area of data protection is underpinned only by mutual adequacy. Whilst mandating “essentially equivalent” (GDPR, recital 104) protection, significant flexibilities may be retained. Bona fide implementation of the Council of Europe's Data Protection Convention 108+ could provide a good lodestar for a more graduated regime which also seeks to clearly reconcile data protection with competing rights. The article tentatively examines what that might entail for the data protection's core substance including the proactive transparency rules, sensitive data regime, integrity provisions and specific restrictions. Any such reform would require great care and should not detract from the need for much more effective practical enforcement.}
}
@article{GAN2022104713,
title = {EIoT-PBFT: A multi-stage consensus algorithm for IoT edge computing based on PBFT},
journal = {Microprocessors and Microsystems},
volume = {95},
pages = {104713},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2022.104713},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122002435},
author = {Bo Gan and Yaojie Wang and Qiwu Wu and Yang Zhou and Lingzhi Jiang},
keywords = {IoT edge computing, Blockchain, PBFT algorithm, Geographical location, Trust score},
abstract = {With the introduction of edge computing into the field of Internet of Things (IoT), the Cognitive Internet of Things (CIoT) has emerged as the next-generation solution for trust and intelligent reasoning in the IoT. That also puts blockchain, with its unique consensus mechanism, transparency and trustworthiness, on the stage of IoT applications. At present, not much research is focused on blockchain's application in CIoT, whose development is to a large extent restricted by the inefficiency of the consensus algorithm. Considering the characteristics of CIoT, a multi-stage consensus algorithm of EIoT-PBFT is proposed on the basis of PBFT algorithm, which includes the Grouping stage, Scoring stage and Consensus reaching stage. EIoT-PBFT meets the IoT edge computing setup by adopting a two-phase improved PBFT algorithm and a scoring mechanism based on both location and reputation, thus achieving a great increase in consensus efficiency. Evaluation results show that EIoT-PBFT takes 36.4% less time than PBFT for a single consensus, and the performance remains stable over the 2500 node configurations we set up. Moreover, at a scale of 1000 nodes, the number of edge nodes to be configured to reduce the number of system communications by 90% compared to the PBFT algorithm is only 5, making blockchain more customized for CIoT settings.}
}
@article{LI2022110968,
title = {An adaptive prognostics method based on a new health index via data fusion and diffusion process},
journal = {Measurement},
volume = {193},
pages = {110968},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.110968},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122002433},
author = {Peng Li and Ahmed Maged and Aibo Zhang and Min Xie and Wei Dang and Congmin Lyu},
keywords = {Remaining useful life (RUL), Genetic algorithm, Adaptive extended Kalman filter (AEKF), Solid-state drives (SSDs)},
abstract = {Remaining useful life prediction (RUL) is critical in predictive maintenance for components or systems prone to deterioration. However, direct RUL prediction methods have difficulties tracking health trends and realizing online prognostics. To address this issue, this paper proposes a novel health index (HI) based adaptive prognostics method by leveraging the advantages of both data fusion to handle multi-dimensional data and the adaptive extended Kalman filter (AEKF) algorithm for parameter identification in the diffusion process. A fitness metric is proposed for feature selection, and then the composite HI sequence is constructed via data fusion using the genetic algorithm. Furthermore, a diffusion process model is built to characterize HI degradation while considering multi-source uncertainties. Model parameters are then updated using the fitting-based AEKF method. Finally, the proposed method is validated on a real-world dataset of solid-state drives in data centers, and prediction results and comparative studies verify its superiority.}
}
@article{KATAL2022123817,
title = {Urban building energy and microclimate modeling – From 3D city generation to dynamic simulations},
journal = {Energy},
volume = {251},
pages = {123817},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.123817},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222007204},
author = {Ali Katal and Mohammad Mortezazadeh and Liangzhu (Leon) Wang and Haiyi Yu},
keywords = {UBEM, Microclimate, Digital city, GIS, Dynamic simulation, Archetype},
abstract = {Dynamic urban simulations often face three main challenges: 3D digital city generations, building archetype creations, and inclusions of urban microclimate impacts due to limited data and computing resources available. This study introduces a new approach for the 3D city generation by integrating publicly available data sets (OpenStreetMap and Microsoft footprints) and a free program (Google Earth). These data sets provide 2D building footprints, whereas Google Earth provides digital surface models of terrains and buildings. The building archetype library of non-geometrical properties was created based on building types and years of constructions in the form of shapefiles joined with the 3D city data through QGIS. The proposed workflow also includes the dynamic integration of urban microclimate (CityFFD) and building thermal/energy models (CityBEM). The dynamic simulations were achieved using weather station data as boundary conditions, including air temperature, solar radiation, and wind speed and direction, instead of typical meteorological year data. The transient microclimate results were validated using local weather station data, and dynamic energy simulation results were validated using measured power consumption data. The study provides a solution to dynamic urban building energy and microclimate modeling by publicly available data sets and tools.}
}
@article{KIM2022510,
title = {Human-guided auto-labeling for network traffic data: The GELM approach},
journal = {Neural Networks},
volume = {152},
pages = {510-526},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608022001794},
author = {Meejoung Kim and Inkyu Lee},
keywords = {Human-guided labeling, Auto-labeling process, Generalized extreme learning machine, Moore–Penrose generalized inverse, Network traffic, Attack prediction},
abstract = {Data labeling is crucial in various areas, including network security, and a prerequisite for applying statistical-based classification and supervised learning techniques. Therefore, developing labeling methods that ensure good performance is important. We propose a human-guided auto-labeling algorithm involving the self-supervised learning concept, with the purpose of labeling data quickly, accurately, and consistently. It consists of three processes: auto-labeling, validation, and update. A labeling scheme is proposed by considering weighted features in the auto-labeling, while the generalized extreme learning machine (GELM) enabling fast training is applied to validate assigned labels. Two different approaches are considered in the update to label new data to investigate labeling speed and accuracy. We experiment to verify the suitability and accuracy of the algorithm for network traffic, applying the algorithm to five traffic datasets, some including distributed denial of service (DDoS), DoS, BruteForce, and PortScan attacks. Numerical results show the algorithm labels unlabeled datasets quickly, accurately, and consistently and the GELM’s learning speed enables labeling data in real-time. It also shows that the performances between auto- and conventional labels are nearly identical on datasets containing only DDoS attacks, which implies the algorithm is quite suitable for such datasets. However, the performance differences between the two labels are not negligible on datasets, including various attacks. Several reasons that require further investigation can be considered, including the selected features and the reliability of conventional labels. Even with this limitation of the current study, the algorithm will provide a criterion for labeling data in real-time occurring in many areas.}
}
@article{SUNDUS2022101088,
title = {Solving the multicollinearity problem to improve the stability of machine learning algorithms applied to a fully annotated breast cancer dataset},
journal = {Informatics in Medicine Unlocked},
volume = {33},
pages = {101088},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101088},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822002246},
author = {Katrina I. Sundus and Bassam H. Hammo and Mohammad B. Al-Zoubi and Amal Al-Omari},
keywords = {Breast cancer, Recurrent breast cancer, Machine learning, Multicollinearity, Dimensionality reduction, Feature selection, Stacking classifier},
abstract = {Among the different types of cancer, breast cancer is the most common cancer affecting females in Jordan. Recurrent breast cancer after treatment is a significant concern for patients and oncologists. Developing countries like Jordan suffer from a lack of quality data on computational medicine (CM). This paper discusses the design, construction, and evaluation of an extensive, fully annotated breast cancer dataset extracted from the King Hussein Cancer Center's registry database (KHCC) in Amman, Jordan. The Jordan Breast Cancer dataset (JBRCA) has 20 attributes and 7562 instances of breast cancer patients. It can be considered a valuable resource to motivate future research in CM in the country. By illustration, the study describes the problems facing the compilation of the dataset. A thorough analysis of the dataset brought up many issues that required remedies before the dataset could be used in machine learning (ML) applications. These issues included missing values and outliers, unnormalized and imbalanced data, and the multicollinearity problem between the attributes. Multicollinearity occurs when two or more independent variables are highly correlated in a regression model, which might affect its stability. This is mainly a problem because we might not differentiate between the effects of the independent variables on the dependent variable. To handle these issues, we deleted missing values and outliers, applied the min-max normalization to control the attributes' different scales, and used SMOTE to solve the highly imbalanced problem. We also used the variance inflation factor (VIF) to solve the multicollinearity problem. Domain experts from KHCC help to identify the best subset of attributes to be removed from the dataset to enhance the stability and performance of the ML algorithms. We used classification models such as logistic regression, decision tree, k-nearest neighbors, Gaussian Naive Bayes, multilayer perceptron, and a stacking classifier combining all five classifiers to evaluate the compiled dataset. The stacking classifier outperformed the other base learners based on accuracy, sensitivity, and F1-score rates.}
}
@article{YANG2022103761,
title = {Innovation and sustainable: Can innovative city improve energy efficiency?},
journal = {Sustainable Cities and Society},
volume = {80},
pages = {103761},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103761},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722000920},
author = {Jingyi Yang and Guangqin Xiong and Daqian Shi},
keywords = {Innovative city, Policy effect, Energy efficiency, Propensity score matching–difference in differences, JEL Classification: O31 Q40 Q48 R58},
abstract = {This paper studies the impact of the construction of innovative cities on cities' energy efficiency. Using panel data of cities from 2005 to 2017, we take the implementation of the National Innovative Polit City Policy (NIPCP) in China as a quasi-natural experiment and identify its impact on improving cities' energy efficiency. Further, for the potential mechanism, we find that NIPCP not only enhances energy efficiency directly by establishing energy-related assessment indicators but also indirectly through the optimization of industrial structures and the promotion of urban innovation level, promoting the transformation to energy-saving cities. Considering the heterogeneity of cities, NIPCP has a greater effect on cities in the west, resource-based cities, provincial capitals, and small and medium-sized cities. Our empirical evidence strongly supports that NIPCP can realize a win-win situation for the energy and economy.}
}
@article{HO2022103858,
title = {OpenComm: Open community platform for data integration and privacy preserving for 311 calls},
journal = {Sustainable Cities and Society},
volume = {83},
pages = {103858},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103858},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722001858},
author = {Duy H. Ho and Yugyung Lee and Srichakradhar Nagireddy and Charan Thota and Brent Never and Ye Wang},
keywords = {311 calls, Data curation, Data privacy, Data integration, Categorization, Data visualization, Machine learning},
abstract = {Local governments are increasingly leveraging administrative data to drive performance. Likewise, cities are interested in improving responsiveness to citizens’ demands and cost savings through data analytics. However, city managers face many challenges when utilizing secondary data, such as 311 call records and the US Census. The challenge of interest to the current study is boundary issues as a result of data being collected at divergent geographic levels over different time horizons. Accordingly, an inductive analytical methodology was developed to create units of analysis that were both pragmatically and analytically appropriate for city managers and local policymakers. We created an open data analytics framework called OpenComm to harmonize administrative and secondary data using administrative data derived from Kansas City, Missouri. This framework produced robust inferences regarding the spatial and temporal aspects for the communities. Privacy-preserving technology, in particular, has been applied to public data to protect community privacy. The findings illustrate the power of inductive data aggregation, leading to empirical insights into hidden patterns of city service disparity over a decade-long time horizon. An application for the Open Data Platform is available at http://kc311.herokuapp.com/.}
}
@article{GRAY2022104550,
title = {Stakeholders’ insights on learning analytics: Perspectives of students and staff},
journal = {Computers & Education},
volume = {187},
pages = {104550},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104550},
url = {https://www.sciencedirect.com/science/article/pii/S036013152200121X},
author = {Geraldine Gray and Ana Elena Schalk and Gordon Cooke and Phelim Murnion and Pauline Rooney and K.C. O'Rourke},
keywords = {Data science applications in education, Evaluation methodologies, Improving classroom teaching, Information literacy, Teacher professional development},
abstract = {Learning analytics has drawn the attention of academics and administrators in recent years as a tool to better understand students' needs and to tailor appropriate and timely responses. While many value the potential of learning analytics, it is not without critics, especially with regards to ethical concerns surrounding the level and type of data gathered, and scepticism on data's ability to measure something useful and actionable. This paper gathers the thoughts of key stakeholders, including staff and students, and their expectations of learning analytics, their priorities for using student data, and how they should be supported to act on the data, with the aim of aiding institutions with their plans to implement learning analytics. For this analysis we explored stakeholder awareness, concerns, priorities and support needs with respect to effectively accessing, interpreting and utilising learning data through the use of surveys and focus groups. These were adapted from the previously published SHEILA framework protocols with additional topics added relating to awareness, uses of data, and support. The focus groups were used to capture prevalent themes, followed by surveys to gain perspectives on these themes from a wider stakeholder audience. Overall, results suggest there are significant differences in the perspectives of each of the stakeholders. There is also a strong need for both additional training and ongoing support to manage and realise stakeholder understanding and goals around learning analytics. Further research is necessary to explore the needs of a greater diversity of stakeholders.}
}
@article{KHORANA202275,
title = {The changing contours of global value chains post-COVID: Evidence from the Commonwealth},
journal = {Journal of Business Research},
volume = {153},
pages = {75-86},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.07.044},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322006567},
author = {Sangeeta Khorana and Hubert Escaith and Salamat Ali and Sushma Kumari and Quynh Do},
keywords = {COVID-19, Commonwealth, Reshoring, Decoupling, Global value chains, Protectionism},
abstract = {The COVID-19 pandemic emphasised the global value chains (GVCs) debate by focussing on whether gains from GVC participation outweigh firms associated risks of demand and supply shocks amid rising protectionism. This paper bridges the gap between the international trade and management literature by examining the impact of COVID-19 on Commonwealth countries, an area that has received scant attention in academic literature. Using the Eora database, we simulate scenarios to examine Commonwealth countries’ participation in GVCs post-COVID. We draw on the transaction cost economics (TCE) theory to develop a framework that investigates whether growing protectionism, associated with reshoring, decoupling and nearshoring, could potentially affect the constellation and participation of Commonwealth countries in GVCs post-COVID. Results show that trade protectionism is likely to impact the supply chains and lead to GVC reconfiguration, which could offer opportunities for the Commonwealth countries and firms to potentially gain following the geographical redistribution of suppliers.}
}
@article{HUANG2022,
title = {Priori-guided and data-driven hybrid model for wind power forecasting},
journal = {ISA Transactions},
year = {2022},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2022.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0019057822003846},
author = {Yi Huang and Guo-Ping Liu and Wenshan Hu},
keywords = {Wind power forecasting, Priori-guided machine learning, Explainable representation, Ultra-short-term forecasting, Practical power curve},
abstract = {To overcome the high uncertainty and randomness of wind and enable the grid to optimize advance preparation, a priori-guided and data-driven hybrid method is proposed to provide accurate and reasonable wind power forecasting results. Fuzzy C-Means (FCM) clustering algorithm is used first to recognize the characteristics of the weather in different regions. Then, for the purpose of making full use of both priori information and collected measured data, a three-stage hierarchical framework is designed. First, via fuzzy inference and dimension reduction of Numerical Weather Prediction (NWP), more applicable wind speed information is obtained. Second, the accessible wind power generation patterns are served as a guide for mining the actual power curve. Third, the forecasted power is derived through the recorded data and the predictable wind conditions via data-driven model. This forecasting framework ingeniously introduces a gateway that can import priori knowledge to steer the iterative learning, thus possessing both adaptive learning ability and Volterra polynomial representation, and can present forecasted outcomes with robustness, accuracy and interpretability. Finally, a real-world dataset of a wind farm as well as an open source dataset are used to verify the performance of the proposed forecasting method. Results of the ablation analyses and comparative experiments demonstrate that the introduction of domain knowledge improves the forecasting performance.}
}
@article{CONLEY2022101752,
title = {Using a deep learning model to quantify trash accumulation for cleaner urban stormwater},
journal = {Computers, Environment and Urban Systems},
volume = {93},
pages = {101752},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101752},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001599},
author = {Gary Conley and Stephanie Castle Zinn and Taylor Hanson and Krista McDonald and Nicole Beck and Howard Wen},
keywords = {Urban trash, Litter, Stormwater, Machine learning, Mask R-CNN},
abstract = {With growing understanding of trash impacts on aquatic habitats throughout the world, cities increasingly face regulatory requirements to reduce trash inputs to local waterways and the ocean, but they often rely upon insufficient monitoring data to prioritize and measure trash reduction effectiveness. We present an approach designed to make urban trash monitoring more cost-efficient and align the data collected with critical information needs of cities. We quantified urban trash accumulation along roadsides using vehicle mounted cameras and a deep convolutional neural network model to identify trash in the imagery captured. We compared the trash detection performance of three different models, with the best performing model (Mask R-CNN) achieving 91% recall, 83% precision, and 77% accuracy using data collected along 84 road segments in two California Cities. Trash detection model outputs were interpreted via a statistical model to relate the proportion of image pixels identified as trash to measured trash volumes. The resulting model estimates explained 67% of the variance in measured trash volumes collected on roadsides, which is more than double the variance explained by walking visual assessments. With vastly more efficient data collection compared to the visual assessments, deep learning-based monitoring approaches can provide a stronger basis for understanding urban trash sources, changes over time, and cost-effective compliance with stormwater regulatory requirements.}
}
@article{JETTER2022105907,
title = {Post-Cold War civil conflict and the role of history and religion: A stochastic search variable selection approach},
journal = {Economic Modelling},
volume = {114},
pages = {105907},
year = {2022},
issn = {0264-9993},
doi = {https://doi.org/10.1016/j.econmod.2022.105907},
url = {https://www.sciencedirect.com/science/article/pii/S0264999322001535},
author = {Michael Jetter and Rafat Mahmood and Christopher F. Parmeter and Andrés Ramírez-Hassan},
keywords = {Civil conflict, Civil war, Stochastic search variable selection (), Greed versus grievances, Religion and conflict},
abstract = {Despite colossal economic and human losses caused by conflict and violence, designing effective policies to avoid conflict remains challenging. While the literature has proposed a voluminous set of candidate predictors, their robustness is questionable and model uncertainty masks the true drivers of conflicts and wars. Considering a comprehensive set of 34 potential determinants in 175 post-Cold-War countries, we employ stochastic search variable selection (SSVS) to sort through all 234 possible models to address model uncertainty. We find past conflict constitutes the most powerful predictor of current conflict: Path dependency matters. Also, larger shares of Jewish, Muslim, or Christian citizens are associated with increased conflict, while economic and political factors remain less relevant than colonial origin and religion. Our results help future researchers and policymakers by inching towards causality and providing a standard set of covariates that need to be accounted for in designing any relevant policies.}
}
@article{HUANG2022130401,
title = {A license plate recognition data to estimate and visualise the restriction policy for diesel vehicles on urban air quality: A case study of Shenzhen},
journal = {Journal of Cleaner Production},
volume = {338},
pages = {130401},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130401},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622000476},
author = {Wenke Huang and Xiaoxiao Xu and Mingwei Hu and Wenwei Huang},
keywords = {Diesel vehicle, License plate recognition data, Environmental effect, Traffic policy},
abstract = {Diesel vehicles for road freight are primary contributors to PM2.5 and NOX emissions in numerous cities. Shenzhen, which is a megacity in China, has made efforts to promote the transition to green transport by implementing license plate restrictions. Nevertheless, it is still unclear whether the restrictions have greatly improved urban air quality. An effective framework for accurately estimating and visualising the effect of restriction policy on a large scale is still lacking. Therefore, this study aims to develop a novel method to visualise and evaluate the effect of license plate restriction policy by bridging diesel truck's license plate recognition data to emission inventories. The results reveal that the impact of the peak restriction on air quality was limited if it only affected nonlocal diesel vehicles. While the promotion of eco-friendly vehicles could reduce PM2.5 and NOX emissions. The findings could provide references for other cities or countries to estimate air pollution from diesel vehicles and recognise high emission zones at a large scale and thus create effective policies and initiatives.}
}
@incollection{WARREN2022151,
title = {Chapter 8 - Operational aspects of deep learning solutions for Alzheimer’s disease},
editor = {Ahmed A. Moustafa},
booktitle = {Alzheimer's Disease},
publisher = {Academic Press},
pages = {151-173},
year = {2022},
isbn = {978-0-12-821334-6},
doi = {https://doi.org/10.1016/B978-0-12-821334-6.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128213346000028},
author = {Samuel L. Warren and Ahmed A. Moustafa and Dustin {van der Haar}},
keywords = {Alzheimer’s disease, machine learning, deep learning, operational aspects},
abstract = {In this article, we discuss operational aspects of deep learning solutions for Alzheimer’s disease. First, we introduce clinical and neural aspects of Alzheimer’s disease. After that, we discuss traditional computer-aided diagnosis methods, such as support vector machines, random forests, and logistic regressions, which use statistical and machine learning techniques to identify and predict Alzheimer’s disease. We then describe basic operational aspects of the use of deep learning, and how they provide some benefits over traditional computer-aided diagnosis methods. Finally, we describe the advantages and limitations of using deep learning, and future directions on the applications of deep learning to Alzheimer’s disease.}
}
@article{LONG202214183,
title = {Research on short-term wind speed prediction based on deep learning model in multi-fan scenario of distributed generation},
journal = {Energy Reports},
volume = {8},
pages = {14183-14199},
year = {2022},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.10.399},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722023344},
author = {Hongyu Long and Yongsheng He and Hui Cui and Qionghui Li and Hao Tan and Bangrui Tang},
keywords = {Short-term wind speed prediction, Distributed generation, Convolutional neural network (CNN), Modified sparrow search algorithm (MSSA), Deep learning model},
abstract = {The climate and environmental pollution problems caused by carbon dioxide and other harmful gases emitted from traditional fossil fuel thermal power plants are increasingly threatening the living environment of mankind. In September, 2020, the Chinese government clearly put forward the national strategic goal of “Carbon Peak and Carbon Neutrality”. Distributed generation is the main means to effectively reduce carbon emissions, especially the rapid development of wind power generation. Accurate and stable wind speed prediction can reasonably formulate power generation plans and optimize power dispatching, which is an effective means to reduce the overall carbon emissions of the power system. This paper designs and proposes a hybrid wind speed prediction model based on convolutional neural network and long short-term memory network deep learning model. Based on the historical wind speed data set collected at the location of multi-fan in the same wind farm, high-precision and stable short-term wind speed prediction is realized. Firstly, singular spectrum analysis aims to remove the noise components in the wind speed series and reduce the impact of noise on the prediction performance of the model. Secondly, convolutional neural network (CNN) is introduced to extract the features of the de-noised wind speed sequence, which provides more effective information for the training of the prediction network. Then, a prediction network based on sparrow search algorithm and long short-term memory network is constructed, and the modified sparrow search algorithm is used to optimize the selection of long short-term memory (LSTM) Hyper-parameters. Eventually, to verify the superiority of the proposed model, an evaluation system based on accuracy, stability and complexity indicators is constructed. The experimental results show that the index values of the wind speed prediction model proposed in this paper based on dataset 1 in the one-step prediction simulation experiment are the smallest among all comparison models, and the same is true for dataset 2.}
}
@article{CHILDERS2022109778,
title = {Fracture diagnostic technologies with process workflow for implementation},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109778},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109778},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521013991},
author = {D. Childers and X. Wu},
keywords = {Hydraulic fracturing, Diagnosis, Unconventional reservoir, Fracture conductivity, Stimulated reservoir volume},
abstract = {Hydraulic Fracturing (HF) is a frequently used well stimulation technology to improve a well's productivity by increasing the contact area with the formation matrix. Hydraulic fractures are created by following a customized design based on imperfect knowledge of the subsurface formation and idealizations. Therefore, the resulting fracture system is often different from the original design, which calls for post-fracturing evaluations. Fracture diagnostics are utilized to understand fracture behavior during or after hydraulic fracturing. Understanding the geomechanics of how fractures form during fracturing has been studied in detail; however, predicting fracture genesis and propagation behavior has been a challenging endeavor because of reservoir heterogeneity. This paper will reveal the current fracture diagnostic technologies and assess their ability to detect HF extension and quantify the properties of hydraulic fractures in terms of conductivity and stimulated reservoir volumes. This paper is not an abridged version of the technical manual regarding how each method is conducted or used but focuses on insight into what they measure and their difference from others. With the critical reviews on current HF diagnostic technologies, we intend to (1) define key terminologies that are often used in HF evaluation to set up a common linguistic lexicon for comparison, (2) provide a brief discussion on their mechanisms; (3) stipulate a framework on how to choose proper HF diagnostic tools and analysis methods for different HF applications.}
}
@article{ZHANG2022118981,
title = {Retrieving soil heavy metals concentrations based on GaoFen-5 hyperspectral satellite image at an opencast coal mine, Inner Mongolia, China},
journal = {Environmental Pollution},
volume = {300},
pages = {118981},
year = {2022},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2022.118981},
url = {https://www.sciencedirect.com/science/article/pii/S0269749122001956},
author = {Bo Zhang and Bin Guo and Bin Zou and Wei Wei and Yongzhi Lei and Tianqi Li},
keywords = {Heavy metals, Hyperspectral image, Mining areas, Direct standardization algorithm, Random forest},
abstract = {Soil heavy metals pollution has been becoming one of the severely environmental issues globally. Previous studies reported laboratory-measured spectra could be used to infer soil heavy metals concentrations to some extent. However, using field-obtained spectra to estimate soil heavy metals concentrations is still a great challenge due to the low precision and weak efficiency at large scales. The present study collected 110 topsoil samples from an Opencast Coal Mine of Ordos, Inner Mongolia, China. Then, the spectra and soil heavy metals concentrations of samples were measured under laboratory conditions. The direct standardization (DS) algorithm was introduced to calibrate the Gaofen-5 (GF-5) hyperspectral image based on the measured spectra of samples. The spectral reflectance of the GF-5 hyperspectral image was reconstructed using continuous wavelet transform (CWT) at different scales. The characteristic bands of GF-5 for estimating heavy metals concentrations were selected by the Boruta algorithm. Finally, the random forest (RF), the extreme learning machine (ELM), the support vector machine (SVM), and the back-propagation neural network (BPNN) algorithms were used to predict the heavy metals concentrations. Some findings were achieved. First, CWT can effectively eliminate the noise of satellite hyperspectral data. The characteristic bands of Zn (480–677, 827–1029, 1241–1334, 1435–1797, and 1949–2500 nm), Ni (514–630, 835–985, 1258–1325, 1460–1578, and 1949–2319 nm), and Cu (822–831; 1029–1300, 1486–1595, and 1730–2294 nm) can be effectively retrieved via the Boruta algorithm. Second, the estimation accuracy was significantly improved by using the DS algorithm. For zinc (Zn), nickel (Ni), and copper (Cu), the determination coefficients of the validation dataset (Rv2) were 0.77 (RF), 0.62 (RF), and 0.56 (ELM), respectively. Third, the distribution trends of heavy metals were almost consistent with the results of actual ground measurements. This paper revealed that the GF-5 can be one of the reliable satellite hyperspectral imagery for mapping soil heavy metals.}
}
@article{EDMONDSON2022104097,
title = {Distributed Quasi-Poisson regression algorithm for modeling multi-site count outcomes in distributed data networks},
journal = {Journal of Biomedical Informatics},
volume = {131},
pages = {104097},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104097},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422001137},
author = {Mackenzie J. Edmondson and Chongliang Luo and Md. {Nazmul Islam} and Natalie E. Sheils and John Buresh and Zhaoyi Chen and Jiang Bian and Yong Chen},
keywords = {Distributed algorithm, Distributed data network, Electronic health records, Overdispersion, Poisson regression},
abstract = {Background
Observational studies incorporating real-world data from multiple institutions facilitate study of rare outcomes or exposures and improve generalizability of results. Due to privacy concerns surrounding patient-level data sharing across institutions, methods for performing regression analyses distributively are desirable. Meta-analysis of institution-specific estimates is commonly used, but has been shown to produce biased estimates in certain settings. While distributed regression methods are increasingly available, methods for analyzing count outcomes are currently limited. Count data in practice are commonly subject to overdispersion, exhibiting greater variability than expected under a given statistical model.
Objective
We propose a novel computational method, a one-shot distributed algorithm for quasi-Poisson regression (ODAP), to distributively model count outcomes while accounting for overdispersion.
Methods
ODAP incorporates a surrogate likelihood approach to perform distributed quasi-Poisson regression without requiring patient-level data sharing, only requiring sharing of aggregate data from each participating institution. ODAP requires at most three rounds of non-iterative communication among institutions to generate coefficient estimates and corresponding standard errors. In simulations, we evaluate ODAP under several data scenarios possible in multi-site analyses, comparing ODAP and meta-analysis estimates in terms of error relative to pooled regression estimates, considered the gold standard. In a proof-of-concept real-world data analysis, we similarly compare ODAP and meta-analysis in terms of relative error to pooled estimatation using data from the OneFlorida Clinical Research Consortium, modeling length of stay in COVID-19 patients as a function of various patient characteristics. In a second proof-of-concept analysis, using the same outcome and covariates, we incorporate data from the UnitedHealth Group Clinical Discovery Database together with the OneFlorida data in a distributed analysis to compare estimates produced by ODAP and meta-analysis.
Results
In simulations, ODAP exhibited negligible error relative to pooled regression estimates across all settings explored. Meta-analysis estimates, while largely unbiased, were increasingly variable as heterogeneity in the outcome increased across institutions. When baseline expected count was 0.2, relative error for meta-analysis was above 5% in 25% of iterations (250/1000), while the largest relative error for ODAP in any iteration was 3.59%. In our proof-of-concept analysis using only OneFlorida data, ODAP estimates were closer to pooled regression estimates than those produced by meta-analysis for all 15 covariates. In our distributed analysis incorporating data from both OneFlorida and the UnitedHealth Group Clinical Discovery Database, ODAP and meta-analysis estimates were largely similar, while some differences in estimates (as large as 13.8%) could be indicative of bias in meta-analytic estimates.
Conclusions
ODAP performs privacy-preserving, communication-efficient distributed quasi-Poisson regression to analyze count outcomes using data stored within multiple institutions. Our method produces estimates nearly matching pooled regression estimates and sometimes more accurate than meta-analysis estimates, most notably in settings with relatively low counts and high outcome heterogeneity across institutions.}
}
@article{MCDONNELL20221402,
title = {The impact of noise and missing fragmentation cleavages on de novo peptide identification algorithms},
journal = {Computational and Structural Biotechnology Journal},
volume = {20},
pages = {1402-1412},
year = {2022},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2022.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S2001037022000836},
author = {Kevin McDonnell and Enda Howley and Florence Abram},
keywords = { peptide sequencing, Machine learning, Peptide identification, Noise, Fragmentation cleavage sites, Peptide fragmentation},
abstract = {Proteomics aims to characterise system-wide protein expression and typically relies on mass-spectrometry and peptide fragmentation, followed by a database search for protein identification. It has wide ranging applications from clinical to environmental settings and virtually impacts on every area of biology. In that context, de novo peptide sequencing is becoming increasingly popular. Historically its performance lagged behind database search methods but with the integration of machine learning, this field of research is gaining momentum. To enable de novo peptide sequencing to realise its full potential, it is critical to explore the mass spectrometry data underpinning peptide identification. In this research we investigate the characteristics of tandem mass spectra using 8 published datasets. We then evaluate two state of the art de novo peptide sequencing algorithms, Novor and DeepNovo, with a particular focus on their performance with regard to missing fragmentation cleavage sites and noise. DeepNovo was found to perform better than Novor overall. However, Novor recalled more correct amino acids when 6 or more cleavage sites were missing. Furthermore, less than 11% of each algorithms’ correct peptide predictions emanate from data with more than one missing cleavage site, highlighting the issues missing cleavages pose. We further investigate how the algorithms manage to correctly identify peptides with many of these missing fragmentation cleavages. We show how noise negatively impacts the performance of both algorithms, when high intensity peaks are considered. Finally, we provide recommendations regarding further algorithms’ improvements and offer potential avenues to overcome current inherent data limitations.}
}
@article{GAO2022283,
title = {FGFL: A blockchain-based fair incentive governor for Federated Learning},
journal = {Journal of Parallel and Distributed Computing},
volume = {163},
pages = {283-299},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522000259},
author = {Liang Gao and Li Li and Yingwen Chen and ChengZhong Xu and Ming Xu},
keywords = {Federated Learning, Incentive mechanism, Attack detection},
abstract = {Federated Learning is a framework that coordinates a large amount of workers to train a shared model in a distributed manner, in which the training data are located on the workers' sides in order to preserve data privacy. There are two challenges in the crowdsourcing of FL, the workers who participant in training need to consume computing and communication resources, so that they are reluctant to participate in the training process if they can not get reasonable rewards. Moreover, there may be attackers who send arbitrary updates to get undeserving compensation or even destroy the model, thus, effective prevention of malicious workers is also critical. An incentive mechanism is urgently required in order to encourage high-quality workers to participate in FL and to punish the attackers. In this paper, we propose FGFL, a blockchain-based incentive governor for Federated Learning. In FGFL, we assess the participants with reputation and contribution indicators. Then the task publisher rewards workers fairly to attract efficient ones while the malicious ones are punished and eliminated. In addition, we propose a blockchain-based incentive management system to manage the incentive mechanism. We evaluate the effectiveness and fairness of FGFL through theoretical analysis and comprehensive experiments. The evaluation results show that FGFL fairly rewards workers according to their corresponding behavior and quality. FGFL increases the system revenue by 0.2% to 3.4% in reliable federations compared with baselines. And in the unreliable scenario where contains attackers, the system revenue of FGFL outperforms the baselines by more than 46.7%.}
}
@article{BAI2022102741,
title = {A new data mining method for time series in visual analysis of regional economy},
journal = {Information Processing & Management},
volume = {59},
number = {1},
pages = {102741},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102741},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002235},
author = {Yang Bai and Min Zhao and Rong Li and Peizhu Xin},
keywords = {Time series, Regional economy, Visualization},
abstract = {In order to improve the effect of visual analysis of regional economy, this paper uses machine learning algorithms to analyze time series data, uses various models and methods of intelligent data analysis to mine data laws from huge data, statistical data reports, and find problems in economic development. Moreover, this paper combines the time series algorithm to design and plan the functional structure of the system, and design a separate module structure from the actual situation of regional economic analysis, and build a model system from the overall structure. After constructing the system, this paper tests the system. From the results of the experimental research, we can see that the regional economic visualization system based on time series constructed in this paper has perfect system functions and can meet the needs of regional economic analysis.}
}
@article{WUYUN2022150286,
title = {The spatiotemporal change of cropland and its impact on vegetation dynamics in the farming-pastoral ecotone of northern China},
journal = {Science of The Total Environment},
volume = {805},
pages = {150286},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.150286},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721053638},
author = {Deji Wuyun and Liang Sun and Zhongxin Chen and Anhong Hou and Luís Guilherme Teixeira Crusiol and Lifeng Yu and Ruiqing Chen and Zheng Sun},
keywords = {Land use classification, “Grain for Green” program, Land Use Change Trajectory, Vegetation index, Vegetation restoration},
abstract = {Due to the unfavorable soil conditions and water resources, the cropland use pattern in the farming-pastoral ecotone in northern China is complex. The program named “Grain for Green” has accelerated the cropland change. However, the complex cropland and retired cropland are challenging to monitor with remote sensing due to their spatially dispersed and easily confused with spectrally similar land use classes such as nature grasslands and non-cropped fields. Taking farming-pastoral ecotone in the northern foot of the Yinshan Mountains as a case study, we explored a classification approach for complex cropland and retired cropland, which was introduced as a specific land use class by using multi-temporal Landsat TM and OLI images with Google Earth Engine. During 1990–2000, cropland increased with a sharper growth and increased with a slower growth from 2001 to 2010, and then decreased significantly from 2011 to 2019, to lead the cropland area in 2019 was smaller than an area in 1990. We analyzed the spatiotemporal trajectories of retired cropland in 2019 using the Land Use Change Trajectory method to evaluate its source. In our finding, approximately 77% of retired cropland was labelled as cropland before 2019; albeit, not all retired cropland was converted from cropland. Moreover, we qualitatively assessed the vegetation dynamics in the study area by utilizing the long-term NDVI-mean value to reveal that vegetation coverage has shown a continuously increasing trend. It is related to the decline of cropland and the increase of retired cropland at the same rate. Our results highlighted that the “Grain for Green” program had led the vegetation restoration in the farming-pastoral ecotone. Our approach for monitoring cropland and retired cropland can improve the understanding of the driving factors and consequences of these critical land use change trajectories.}
}
@article{LIU2022624,
title = {Reproductive tissue-specific translatome of a rice thermo-sensitive genic male sterile line},
journal = {Journal of Genetics and Genomics},
volume = {49},
number = {7},
pages = {624-635},
year = {2022},
issn = {1673-8527},
doi = {https://doi.org/10.1016/j.jgg.2022.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1673852722000066},
author = {Wei Liu and Jing Sun and Ji Li and Chunyan Liu and Fuyan Si and Bin Yan and Zhen Wang and Xianwei Song and Yuanzhu Yang and Yuxian Zhu and Xiaofeng Cao},
keywords = {TGMS rice, Translatome, MEL1, Reproductive tissue, Translating ribosome affinity purification (TRAP), Fertility alternation},
abstract = {Translational regulation, especially tissue- or cell type-specific gene regulation, plays essential roles in plant growth and development. Thermo-sensitive genic male sterile (TGMS) lines have been widely used for hybrid breeding in rice (Oryza sativa). However, little is known about translational regulation during reproductive stage in TGMS rice. Here, we use translating ribosome affinity purification (TRAP) combined with RNA sequencing to investigate the reproductive tissue-specific translatome of TGMS rice expressing FLAG-tagged ribosomal protein L18 (RPL18) from the germline-specific promoter MEIOSIS ARRESTED AT LEPTOTENE1 (MEL1). Differentially expressed genes at the transcriptional and translational levels are enriched in pollen and anther-related formation and development processes. These contain a number of genes reported to be involved in tapetum programmed cell death (PCD) and lipid metabolism during pollen development and anther dehiscence in rice, including several encoding transcription factors and key enzymes, as well as several long non-coding RNAs (lncRNAs) that potentially affect tapetum and pollen-related genes in male sterility. This study represents the comprehensive reproductive tissue-specific characterization of the translatome in TGMS rice. These results contribute to our understanding of the molecular basis of sterility in TGMS rice and will facilitate further genetic manipulation of TGMS rice in two-line breeding systems.}
}
@article{OLIVATO20221232,
title = {Machine Learning Models for Predicting Short-Long Length of Stay of COVID-19 Patients},
journal = {Procedia Computer Science},
volume = {207},
pages = {1232-1241},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.179},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010614},
author = {Matteo Olivato and Nicholas Rossetti and Alfonso E. Gerevini and Mattia Chiari and Luca Putelli and Ivan Serina},
abstract = {During 2020 and 2021, managing limited healthcare resources and hospital beds has been a fundamental aspect of the fight against the COVID-19 pandemic. Predicting in advance the length of stay, and in particular identifying whether a patient is going to stay in the hospital longer or less than a week, can provide important support in handling resources allocation. However, there have been significant changes in terms of containment measures, virus diffusion, new treatments, vaccines, and new variants of SARS-CoV-2 during the last period. These changes pose several conceptual drift issues that can limit the usefulness of machine learning in this context. In this work, we present a machine learning system trained and tested using data from more than 6000 hospitalised patients in northern Italy, distributed over almost two years of pandemic. We show how machine learning can be effective even by analysing data over this long period of time, also exploiting a model that predicts the patient's outcome in terms of discharge or death. Furthermore, learning from data that also consider deceased patients is a common issue in predicting the length of stay because they have severe conditions similar to patients with a long stay period, but may actually have a very short duration of hospitalisation. For this purpose, we present a method for handling data from alive and deceased patients, exploiting more patient records, increasing the robustness of the model and its performance in this task. Finally, we investigate the features that are most relevant to the prediction of the simplified length of stay.}
}
@article{NATANELOV2022100389,
title = {Blockchain smart contracts for supply chain finance: Mapping the innovation potential in Australia-China beef supply chains},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100389},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100389},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000565},
author = {Valeri Natanelov and Shoufeng Cao and Marcus Foth and Uwe Dulleck},
keywords = {Blockchain, Smart contracts, Supply chain finance, Reverse factoring, Supply chain risk},
abstract = {This paper explores and demonstrates the innovation potential of blockchain and smart contracts for supply chain finance (SCF) based on cross-border beef supply chains from Australia to China. Our study adopts mechanism design and design-driven activities, and specifically employs the Agents Events Data (AED) process mapping method, which is a hybrid approach that combines Business Process Redesign (BPR) and the Resources Events Assets (REA) accounting model. The AED method consists of three sequential stages: (1) map supply chain's present state or condition; (2) introduce blockchain and smart contracts to improve supply chain processes and “traditional” SCF models; and (3) evaluate the technology and innovation impact and create new models for SCF innovation. This study identified examples of how financial risks can be mitigated or reduced with blockchain and smart contracts; and, where the credit financing itself could be fundamentally transformed. Based on this analysis, the paper's contribution is twofold: First, it proposes the group buying business model as a basis of whole-of-supply-chain finance promising new areas for SCF models that can reduce financial cost and improve cash flow performance due to greater buyer-led financing certainty and the direct involvement of buy-side demand for supply-side improvements. Second, the study demonstrates the utility of the AED process mapping as a holistic framework with fine granular levels for future empirical and fieldwork and opens up new avenues for research.}
}
@article{B2022103224,
title = {Fractional salp swarm algorithm: An association rule based privacy-preserving strategy for data sanitization},
journal = {Journal of Information Security and Applications},
volume = {68},
pages = {103224},
year = {2022},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2022.103224},
url = {https://www.sciencedirect.com/science/article/pii/S2214212622000989},
author = {Suma B and Shobha G},
keywords = {Privacy preservation, Data sanitization, Association rule, Tanimoto measure, Minkowski distance},
abstract = {Background
Data mining is the process of extracting hidden patterns from huge repositories. Privacy preservation at the time of data shared for mining is a demanding dilemma. Conventional techniques, such as access control and authentication have been used to handle data privacy. Various data sanitization techniques like perturbation, generalization, and sampling are utilized to preserve confidential information from disclosure.
Method
This paper develops a Fractional-Salp swarm algorithm (Fractional-SSA) for data sanitization using privacy preserved data. The Fractional-SSA is developed by integrating Fractional calculus (FC) and the Salp swarm algorithm (SSA). The proposed Fractional-SSA hides sensitive rules considering a large transactional database and recovers the original database against malicious attacks. Here, the random key generation of the sanitization process is performed using the proposed Fractional-SSA algorithm by arbitrarily initializing various keys. In addition, the sanitized database is obtained from sanitization that derives association rules considering certain factors that involve privacy success, information preservation, hiding success, database difference, privacy loss, information loss, hiding failure, and database difference. Finally, the key value gets updated to estimate the best solution. The fitness is obtained using success and failure scenarios.
Results
The proposed Fractional-SSA offered enhanced efficiency providing the highest privacy success of 0.933, information preservation of 0.487, minimal hiding failure of 0.054, and database difference of 0.008.}
}
@incollection{DING2022129,
title = {6 - Blockchain for future renewable energy},
editor = {Mohsen Parsa Moghaddam and Reza Zamani and Hassan Haes Alhelou and Pierluigi Siano},
booktitle = {Decentralized Frameworks for Future Power Systems},
publisher = {Academic Press},
pages = {129-146},
year = {2022},
isbn = {978-0-323-91698-1},
doi = {https://doi.org/10.1016/B978-0-323-91698-1.00011-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032391698100011X},
author = {Jianguo Ding and Vahid Naserinia},
keywords = {Renewable energy, Decentralized framework, Decentralized management, Blockchain},
abstract = {To better optimize and control the renewable energy system and its integration with traditional grid systems and other energy systems, corresponding technologies are needed to meet its growing practical application requirements: decentralized management and control, support for decentralized decision-making, fine-grained and timely data sharing, maintain data and business privacy, support fast and low-cost electricity market transactions, maintain the security and reliability of system operation data, and prevent malicious cyberattacks. Blockchain is based on core technologies such as distributed ledgers, asymmetric encryption, consensus mechanisms, and smart contracts and has some excellent features such as decentralization, openness, independence, security, and anonymity. These characteristics seem to meet the technical requirements of future renewable energy systems partially. This chapter will systematically review how blockchain technology can potentially solve the challenges with decentralized solutions for future renewable energy systems and show a guideline to implement blockchain-based corresponding applications for future renewable energy.}
}
@incollection{COSTA20221,
title = {Chapter 1 - Achieving a more sustainable wine supply chain—Environmental and socioeconomic issues of the industry},
editor = {J. Miguel Costa and Sofia Catarino and José M. Escalona and Piergiorgio Comuzzo},
booktitle = {Improving Sustainable Viticulture and Winemaking Practices},
publisher = {Academic Press},
pages = {1-24},
year = {2022},
isbn = {978-0-323-85150-3},
doi = {https://doi.org/10.1016/B978-0-323-85150-3.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851503000098},
author = {J. Miguel Costa and Sofia Catarino and José M. Escalona and Piergiorgio Comuzzo},
keywords = {Innovation and competitiveness, Oenology, Supply chain, Sustainability issues, Viticulture},
abstract = {Sustainable development involves three basic pillars: environmental, economic, and social. In the case of the wine sector, sustainability needs to integrate the concept defined by economics, ecology, and community dimensions for both grape and wine production. The wine industry is a large, globalized, and diversified sector encompassing multiple production systems and cultures, diverse management choices and a wide range of monitoring tools and solutions. This chapter presents and discusses the most relevant risks and concerns of modern wine industry and major sustainability issues related to wine production and related supply chain. The wine sector must implement more sustainable practices to mitigate climate change impacts and to decrease its environmental impact while ensuring its important economic and social function. Metrics and standards are required to support audits, efficient management, and regulatory parameters. Social issues must be addressed by the sector, especially because it strongly relies on human resources and manual labor. Research and development activities and related innovation (e.g., digitalization, sensors, mechanization, and recycling) can result in improved sustainability and resilience while the lack of transparency of the sector will harm confidence of consumers and competiveness.}
}
@article{WANG2022133463,
title = {How does agricultural specialization affect carbon emissions in China?},
journal = {Journal of Cleaner Production},
volume = {370},
pages = {133463},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133463},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203044X},
author = {Ruru Wang and Yu Zhang and Cunming Zou},
keywords = {Agricultural carbon emissions, Agricultural specialization, Agrarian transition, China},
abstract = {Agricultural carbon emissions have long attracted scholars' interest. In recent decades, China has gradually explored the path of agricultural specialization without large-scale operation based on agricultural machinery service outsourcing driven by structural transformation. To date, our understanding of the impact of China's unique agricultural specialization on agricultural carbon emissions remains scant. Nevertheless, this issue is important for promoting China's carbon emission reduction and exploring a path of agricultural transformation that is both competitive and sustainable. Based on China's provincial panel data from 2000 to 2019, this paper constructed a mediating model to test empirically the impacts of agricultural specialization on agricultural carbon emissions. The results showed that agricultural specialization exerted a significant positive effect on agricultural carbon emissions by increasing agricultural external inputs. Specifically, in addition to promoting increased mechanization, agricultural specialization caused an even greater, excessive application of chemical fertilizers, which largely follows metabolic rift theory and substitution and remittance effects. The farmland operation scale had no significant effect on agricultural carbon emissions due to the offset of its significant negative effect on chemicalization and significant positive effect on mechanization. This finding demonstrates that, in the context of smallholder farming, the moderate expansion of farm size can reduce agricultural environmental pollution, but it cannot be proved to reduce agricultural carbon emissions. These findings challenge China's current unique path of agricultural specialization from the perspective of carbon emission reduction and demonstrate the applicability of the theory of metabolic rift to the current path of agricultural development in China. Accordingly, corresponding short-term and long-term implications related to agrarian transition and agricultural carbon emissions are provided.}
}
@article{ROHAAN2022115925,
title = {Using supervised machine learning for B2B sales forecasting: A case study of spare parts sales forecasting at an after-sales service provider},
journal = {Expert Systems with Applications},
volume = {188},
pages = {115925},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115925},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421012793},
author = {D. Rohaan and E. Topan and C.G.M. Groothuis-Oudshoorn},
keywords = {Supervised machine learning, Natural Language Processing (NLP), B2B sales forecasting, Prioritization on sales potential, Information Extraction, Imbalanced data},
abstract = {In this paper, we present a method to use advance demand information (ADI), taking the form of request for quotation (RFQ) data, in B2B sales forecasting. We apply supervised machine learning and Natural Language Processing techniques to analyze and learn from RFQs. We apply and test our approach in a case study at a large after-sales service and maintenance provider. After evaluation we found that our approach identifies ∼ 70% of actual sales (recall) with a precision rate of ∼ 50%, which represents a performance improvement of slightly more than a factor 2.5 over the current labor-intensive manual process at the service and maintenance provider. Our research contributes to literature by giving step-by-step guidance on incorporating artificial intelligence in B2B sales forecasting and revealing potential pitfalls along the way. Furthermore, our research gives an indication of the performance improvement that can be expected when adopting supervised machine learning into B2B sales forecasting.}
}
@article{JAVAID202258,
title = {Significance of machine learning in healthcare: Features, pillars and applications},
journal = {International Journal of Intelligent Networks},
volume = {3},
pages = {58-73},
year = {2022},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2022.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666603022000069},
author = {Mohd Javaid and Abid Haleem and Ravi {Pratap Singh} and Rajiv Suman and Shanay Rab},
keywords = {Machine learning, Data, Healthcare, Patient outcome, Efficiency, Treatment},
abstract = {Machine Learning (ML) applications are making a considerable impact on healthcare. ML is a subtype of Artificial Intelligence (AI) technology that aims to improve the speed and accuracy of physicians' work. Countries are currently dealing with an overburdened healthcare system with a shortage of skilled physicians, where AI provides a big hope. The healthcare data can be used gainfully to identify the optimal trial sample, collect more data points, assess ongoing data from trial participants, and eliminate data-based errors. ML-based techniques assist in detecting early indicators of an epidemic or pandemic. This algorithm examines satellite data, news and social media reports, and even video sources to determine whether the sickness will become out of control. Using ML for healthcare can open up a world of possibilities in this field. It frees up healthcare providers' time to focus on patient care rather than searching or entering information. This paper studies ML and its need in healthcare, and then it discusses the associated features and appropriate pillars of ML for healthcare structure. Finally, it identified and discussed the significant applications of ML for healthcare. The applications of this technology in healthcare operations can be tremendously advantageous to the organisation. ML-based tools are used to provide various treatment alternatives and individualised treatments and improve the overall efficiency of hospitals and healthcare systems while lowering the cost of care. Shortly, ML will impact both physicians and hospitals. It will be crucial in developing clinical decision support, illness detection, and personalised treatment approaches to provide the best potential outcomes.}
}
@article{AO20221239,
title = {Automatic segmentation of stem and leaf components and individual maize plants in field terrestrial LiDAR data using convolutional neural networks},
journal = {The Crop Journal},
volume = {10},
number = {5},
pages = {1239-1250},
year = {2022},
note = {Crop phenotyping studies with application to crop monitoring},
issn = {2214-5141},
doi = {https://doi.org/10.1016/j.cj.2021.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S2214514121002191},
author = {Zurui Ao and Fangfang Wu and Saihan Hu and Ying Sun and Yanjun Su and Qinghua Guo and Qinchuan Xin},
keywords = {Terrestrial LiDAR, Phenotype, Organ segmentation, Convolutional neural networks},
abstract = {High-throughput maize phenotyping at both organ and plant levels plays a key role in molecular breeding for increasing crop yields. Although the rapid development of light detection and ranging (LiDAR) provides a new way to characterize three-dimensional (3D) plant structure, there is a need to develop robust algorithms for extracting 3D phenotypic traits from LiDAR data to assist in gene identification and selection. Accurate 3D phenotyping in field environments remains challenging, owing to difficulties in segmentation of organs and individual plants in field terrestrial LiDAR data. We describe a two-stage method that combines both convolutional neural networks (CNNs) and morphological characteristics to segment stems and leaves of individual maize plants in field environments. It initially extracts stem points using the PointCNN model and obtains stem instances by fitting 3D cylinders to the points. It then segments the field LiDAR point cloud into individual plants using local point densities and 3D morphological structures of maize plants. The method was tested using 40 samples from field observations and showed high accuracy in the segmentation of both organs (F-score =0.8207) and plants (F-score =0.9909). The effectiveness of terrestrial LiDAR for phenotyping at organ (including leaf area and stem position) and individual plant (including individual height and crown width) levels in field environments was evaluated. The accuracies of derived stem position (position error =0.0141 m), plant height (R2 >0.99), crown width (R2 >0.90), and leaf area (R2 >0.85) allow investigating plant structural and functional phenotypes in a high-throughput way. This CNN-based solution overcomes the major challenges in organ-level phenotypic trait extraction associated with the organ segmentation, and potentially contributes to studies of plant phenomics and precision agriculture.}
}
@incollection{KIMM202261,
title = {Chapter 4 - Classes of AI tools, techniques, and methods},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {61-83},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00012-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000123},
author = {Geoff Kimm},
keywords = {Artificial intelligence, AI tools, AI survey, Urban planning, Intelligent agents},
abstract = {In urban design and planning, there is no canonical classification of uses and approaches of artificial intelligence (AI), nor even a definitive framework for developing such. The increasing capabilities of AI continually change its scope and possible roles. In its urban application, there are multiple perspectives—from that of the practitioner, the citizen, and even notionally the machine—that are all equally valid. This chapter therefore looks at classifying AI in urban design and planning in three ways. Initially, algorithms are discussed as the essential AI tool. Subsequently, in a reductionist approach, techniques are considered via Russell and Norvig's schema of intelligent agents with respect to the objectives of the practitioner. Finally, methods are considered in a teleological approach by considering the purposes AI tools and techniques serve for the practitioner rather than the underlying mechanisms themselves. The discussion is founded on a working definition of AI that is predicated on the fact that synthetic reasoning and outcomes may be fundamentally nonhuman.}
}
@incollection{MISTRY202219,
title = {Chapter 2 - The role of climate datasets in understanding climate extremes},
editor = {Victor Ongoma and Hossein Tabari},
booktitle = {Climate Impacts on Extreme Weather},
publisher = {Elsevier},
pages = {19-48},
year = {2022},
isbn = {978-0-323-88456-3},
doi = {https://doi.org/10.1016/B978-0-323-88456-3.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884563000058},
author = {Malcolm N. Mistry},
keywords = {Climate extremes, Climate datasets, ETCCDI, Expert team on sector-specific climate indices (ET-SCI)},
abstract = {A rapidly growing body of literature routinely employs historical observations to study extremes in past and current climate. The inconsistencies in observations often lead to erroneous results and drawing incorrect inferences when undertaking analyses of climate extremes at regional or global scales. Understanding the potential inhomogeneity in climate datasets is therefore central to the study of climate extremes, especially when attributing any shifts in extremes to a changing climate. Despite the best efforts in assembling quality-controlled input data sources, inconsistencies in data are inherently embedded within long-term records of observations. Knowing the strengths and limitations of climate datasets can potentially facilitate better analyses of climate extremes. This chapter begins with an overview of climate extremes and Climate Extreme Indices (CEIs). The importance of quality control input meteorological variables, tools for assembling the CEIs, and a detailed list of recommended CEIs suitable for examining a broad array of temperature- and precipitation-based extremes are described next. Different sources of global and regional input climate data along with their strengths and limitations for assembling the CEIs form the crux of the next section. Existing datasets of CEIs and recommendations for future research conclude the chapter as the final two sections.}
}
@article{ZHANG2022106296,
title = {Industrial Development and Economic Impacts of Forest Biomass for Bioenergy: A Data-Driven Holistic Analysis Framework},
journal = {Resources, Conservation and Recycling},
volume = {182},
pages = {106296},
year = {2022},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2022.106296},
url = {https://www.sciencedirect.com/science/article/pii/S0921344922001446},
author = {Xufeng Zhang and Jingxin Wang and Michael P. Strager},
keywords = {Forest biomass, Bioenergy, Holistic framework, Machine learning, Suitability assessment, Economic impacts},
abstract = {A data-driven holistic analysis framework was developed to aid the industrial development of forest biomass for bioenergy to promote the regional bioeconomy. Leveraging the existing but fragmented multi-source data, four components of industrial bioenergy development were integrated into the framework including spatial statistical analysis of biomass feedstock and bioenergy production, machine learning-based suitability assessment, bioenergy plant sites identification and ranking, and socio-economic impacts assessment. A case study was conducted for forest biomass to pellet fuel in the U.S. Mid-Atlantic region. Our results indicate that the great potential of forest biomass with high variation at the county level is primarily clustered in western Pennsylvania and eastern North Carolina. Integrating the datasets of biomass feedstock, road conditions, employment status, income status, population, and current bioenergy production, the machine-learning model demonstrates good performance for bioenergy industry suitability assessment, with the high-suitable areas accounting for 19.76%, medium-suitable areas for 34.74%, and low-suitable areas for 54.49% in the region. Forest biomass availability and distance to major roads are the two top factors affecting bioenergy industry development. We identified 65 industrial sites within the suitable areas and their rankings were derived as a reference of the bioenergy development priority. The socio-economic impacts assessment indicates that the one-year construction of a medium-size pellet fuel facility (75,000 dry tons/year) could create 127 jobs, $8.78 million of labor income, while the operation could create 202 jobs, $10.52 million of labor income, $14.66 million of value-added, and $33.61 million of output in total per year for the state-level economy.}
}
@article{NIU2022107836,
title = {Green manufacturers’ power strategy in the smart grid era},
journal = {Computers & Industrial Engineering},
volume = {163},
pages = {107836},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107836},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221007403},
author = {Baozhuang Niu and Jian Dong and Fanzhuo Zeng},
keywords = {Wind power, Energy procurement, Green operations, Power stability, Dual sourcing},
abstract = {Conventional electricity generation from coal and natural gas is one of the largest greenhouse gas sources and using wind power is an effective way to achieve climate neutrality. Realizing this, green manufacturers such as SC Johnson and Silk decide to use 100% wind power for their manufacture. This enables the manufacturers to receive green certifications and hence, attract more customers. However, wind power can be unstable, so some green manufacturers like New Belgium Brewing purchase power from both the regular power supplier and the wind power supplier, resulting in a discount of market expansion. Modelling this complex system can be challenging that requires the use of meta-heuristic algorithms, so we build a game-theoretic model comprising of a wind power supplier, a regular power supplier and a green manufacturer to examine whether the green manufacturer is more benefited under all-wind power strategy. This helps clarify the strategic perdition about the firms’ equilibrium decisions. We find that the green manufacturer prefers all-wind power strategy when (a) the discount effect is significant and the market potential is greatly expanded; (b) both the discount effect and the market expansion are moderate. We further study the impact of the environmental effect, the improvement of wind power stability and the production cost uncertainty, finding that the main results are qualitatively unchanged.}
}
@incollection{PALMA20225085,
title = {Chapter 91 - Neuroeconomics: An overview and applications to agricultural and food economics},
editor = {Christopher B. Barrett and David R. Just},
series = {Handbook of Agricultural Economics},
publisher = {Elsevier},
volume = {6},
pages = {5085-5116},
year = {2022},
issn = {1574-0072},
doi = {https://doi.org/10.1016/bs.hesagr.2022.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S157400722200007X},
author = {Marco A. Palma},
keywords = {Biometrics, Choice process data, Emotions, Eye-tracking, Neuroeconomics},
abstract = {This chapter provides an overview of the neuroeconomics field, with particular emphasis and applications to the agricultural and applied economics profession. First, I provide a brief overview of the brain. Next, I highlight the priority areas in the applied economics agenda, including developing, testing, and refining theory; the value of using neurophysiological data to enrich the underlying motivations of the choice process preference formation; evaluating treatment compliance and effort (internal validity); generalizability of behavior from the lab to the real world (external validity); and recent neuroeconomic advances in the prediction power of choice models. Next, I provide an overview of a wide range of available neuro-physiological tools varying in cost, obtrusiveness, and complexity. I highlight some basic, low-cost measures that can be incorporated using existing resources for most researchers. Throughout the chapter, I discuss opportunities for the neuroeconomics agenda to address relevant questions in the food and agriculture domain. Finally, I raise potential ethical concerns about the use of the neuroeconomics paradigm to induce changes that could harm individuals and result in suboptimal and costly behavior.}
}
@article{WANG2022103159,
title = {You are what the permissions told me! Android malware detection based on hybrid tactics},
journal = {Journal of Information Security and Applications},
volume = {66},
pages = {103159},
year = {2022},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2022.103159},
url = {https://www.sciencedirect.com/science/article/pii/S2214212622000485},
author = {Huanran Wang and Weizhe Zhang and Hui He},
keywords = {Android malware detection, Deep learning, Permission sequence},
abstract = {Recent years have witnessed a significant increase in the use of Android devices in many aspects of our life. However, users can download Android apps from third-party channels, which provides numerous opportunities for malware. Attackers utilize unsolicited permissions to gain access to the sensitive private intelligence of users. Since signature-based antivirus solutions no longer meet practical needs, efficient and adaptable solutions are desperately needed, especially in new variants. As a remedy, we propose a hybrid Android malware detection approach that combines dynamic and static tactics. We firstly adopt static analysis inferring different permission usage patterns between malware and benign apps based on the machine-learning-based method. To classify the suspicious apps further, we extract the object reference relationships from the memory heap to construct a dynamic feature base. We then present an improved state-based algorithm based on DAMBA. Experimental results on a real-world dataset of 21,708 apps show that our approach outperforms the well-known detector with 97.5% F1-measure. Besides, our system is demonstrated to resist permission abuse behaviors and obfuscation techniques.}
}
@article{CHANG2022104746,
title = {Micro-fault diagnosis of electric vehicle batteries based on the evolution of battery consistency relative position},
journal = {Journal of Energy Storage},
volume = {52},
pages = {104746},
year = {2022},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.104746},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X22007575},
author = {Chun Chang and XiaPing Zhou and Jiuchun Jiang and Yang Gao and Yan Jiang and Tiezhou Wu},
keywords = {Battery pack, Micro-fault diagnosis, Charging cell voltage curve, Consistency, Relative fluctuation},
abstract = {Micro-faults in Li-ion batteries are a safety hazard for battery packs, and accurately identifying micro-faulted batteries is a complex problem to solve. In this paper, we propose a micro-fault diagnosis method based on the evolution of the consistent relative position of cells within multiple charging segments. The CCVC (Charging cell voltage curve) transformation is applied to match the charging voltage curve of each battery with the reference battery, and the particle swarm algorithm obtains the matching transform parameters reflecting the degree of battery consistency with adaptive inertia weights. The transformation parameters are normalized to obtain each battery's relative position Z-Score values in the consistency comparison of the transform parameters. Based on the constant consistency relative position hypothesis, the evolution of the consistency relative position of each cell is scored quantitatively by constructing consistency relative position fluctuation scores. The anomaly detection algorithm based on the 3 − σ criterion is used to identify and locate the micro faulted battery by comparing the scoring results of each battery. The method's effectiveness is verified using the collected actual breakdown vehicle data, and the influence of different reference cells on the technique is analyzed. The results show that the consistency relative position of healthy cells is almost stable over medium-term scales, while the consistency relative position of faulty cells falls. This method can accurately and effectively locate faulty cells even though the battery pack does not exhibit abnormal voltage fluctuations or significant inconsistencies.}
}
@article{TANTSCHER202235,
title = {Digital Retrofitting of legacy machines: A holistic procedure model for industrial companies},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {36},
pages = {35-44},
year = {2022},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2021.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S1755581721001760},
author = {Dominik Tantscher and Barbara Mayer},
keywords = {Digital Transformation, Digital Retrofitting, Industry 4.0, Procedure model},
abstract = {Digital Transformation is a demanding evolutional process for industrial companies that touches all important business areas. A central role for the development towards a more flexible and efficient manufacture play production data. However, most machines in use lack a digital interface, such that they are not capable to connect in a network. Thus, data cannot be utilized. One solution from the technical perspective is Digital Retrofitting focusing on the implementation of additional sensors and edge devices to digitally connect the machine and transforming it into a cyber-physical system. This approach is also economically promising for companies since investing in new machines is far more expensive than the retrofitting of legacy systems. Nevertheless, industrial practice shows that Digital Retrofitting projects often do not lead to sustainable solutions because they lack integration into the digital transformation process. This paper thus introduces a holistic procedure model for Digital Retrofitting that is the result of an analysis and clustering of existing process models. The framework incorporates the perspectives on digital strategy, interdisciplinarity, change and lean management and, therefore, provides a guideline for companies leading to a more effective and efficient implementation process with sustainable impact. Furthermore, a description of a successful validation of the procedure model is given on the example of a condition monitoring use case in the Smart Production Lab of FH JOANNEUM.}
}
@article{ZHANG2022121303,
title = {Operationalizing the telemedicine platforms through the social network knowledge: An MCDM model based on the CIPFOHW operator},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121303},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121303},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100737X},
author = {Mengdan Zhang and Chonghui Zhang and Qiule Shi and Shouzhen Zeng and Tomas Balezentis},
keywords = {Telemedicine platform, Multi-criteria decision-making, SERVQUAL, Comprehensive assessment},
abstract = {Telemedicine is an effective way to address the excessive concentration of quality medical resources. Telemedicine platforms provide users with diversified high-quality medical and health services. These issues are topical in the context of the recent pandemic and the resulting socioeconomic transformations. In order to meet the multilevel and personalised health needs of users, optimise the management process of a telemedicine platform and rationally allocate medical resources, it is necessary to conduct scientific evaluations of telemedicine platforms. Therefore, this paper proposes a multi-criteria evaluation framework based on the interval value confidence induced Pythagorean fuzzy ordered hybrid weighted integration (CIPFOHW) operator and social network. To improve the fusion efficiency of user evaluation information and confidence level, the CIPFOHW operator is proposed. At the same time, since the trust relationship between users of the telemedicine platform influence the evaluation, the trust transfer network between users is given. In addition, based on SERVQUAL method, a five-dimension evaluation framework for telemedicine platforms is put forward. Finally, an applied case with five typical patients is used to verify the effectiveness of the method.}
}
@article{YAO2022107757,
title = {KfreqGAN: Unsupervised detection of sequence anomaly with adversarial learning and frequency domain information},
journal = {Knowledge-Based Systems},
volume = {236},
pages = {107757},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107757},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121009837},
author = {Yueyue Yao and Jianghong Ma and Yunming Ye},
keywords = {Sequence anomaly detection, Prediction-based method, Adversarial learning, Frequency information},
abstract = {Sequence anomaly detection in time series is of critical importance to wide applications ranging from finance, healthcare to IT system monitoring. Most current researches use the reconstruction-based deep learning algorithms to solve the problem. In this article, we aim to use a prediction-based method to detect sequence anomalies in univariate time series, because the latter methods can detect anomalies using historical information revealing normal patterns in time series whereas the former methods simply consider current sequences. However, it is challenging because there exists both uncertainty in the future and performance deterioration under long detection horizon. To tackle the challenges, we propose an unsupervised algorithm called KfreqGAN, which is based on adversarially trained sequence predictor. The adversarial learning architecture helps the model make accurate predictions for future sequences. In addition, auxiliary information from frequency domain is used to help the model capture the characteristics of time series for achieving satisfactory predictions. We conduct extensive experiments on two public-available datasets, with results demonstrating the effectiveness of the proposed algorithm and its superiority to baseline algorithms.}
}
@article{WU2022,
title = {A GTFS data acquisition and processing framework and its application to train delay prediction},
journal = {International Journal of Transportation Science and Technology},
year = {2022},
issn = {2046-0430},
doi = {https://doi.org/10.1016/j.ijtst.2022.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2046043022000090},
author = {Jianqing Wu and Bo Du and Zengyang Gong and Qiang Wu and Jun Shen and Luping Zhou and Chen Cai},
keywords = {General transit feed specification, Delay prediction, Train delay, Long short-term memory, Data fusion},
abstract = {With advanced artificial intelligence and deep learning techniques, a growing number of data sources are playing more and more critical roles in planning and operating transportation services. The General Transit Feed Specification (GTFS), with standard open-source data in both static and real-time formats, is being widely used in public transport planning and operation management. However, compared to other extensively studied data sources such as smart card data and GPS trajectory data, the GTFS data lacks proper investigation yet. Utilization of the GTFS data is challenging for both transport planners and researchers due to its difficulty and complexity of understanding, processing, and leveraging the raw data. In this paper, a GTFS data acquisition and processing framework is proposed to offer an efficient and effective benchmark tool for converting and fusing the GTFS data to a ready-to-use format. To validate and test the proposed framework, a multivariate multistep Long Short-Term Memory is developed to predict train delay with minor anomaly in Sydney as a case study. The contribution of this new framework will render great potential for broader applications and deeper research.}
}
@article{HERRERASEMENETS2022107963,
title = {A fast instance reduction algorithm for intrusion detection scenarios},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {107963},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107963},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622002397},
author = {Vitali Herrera-Semenets and Raudel Hernández-León and Jan {van den Berg}},
keywords = {Instance selection, Data reduction, Intrusion detection, Data preprocessing, Data mining, Supervised classification},
abstract = {We live in a world that is being driven by data. This leads to challenges of extracting and analyzing knowledge from large volumes of data. An example of such a challenge is intrusion detection. Intrusion detection data sets are characterized by huge volumes, which affects the learning of the classifier. So there is a need to reduce the size of the training sets. Fortunately, inspection and analysis of available intrusion detection data sets showed that many instances are very similar and do not provide relevant information to the classification process. This prompted to look for possibilities to use a fast algorithm that, as much as possible, removes similar instances in intrusion detection data sets while enforcing the detection rate. In this work, a new fast instance reduction algorithm is presented. The proposed algorithm provides greater efficiency during the training stage, without significantly affecting the efficacy during the intrusion detection task.}
}
@article{GUO2022127367,
title = {Construction of rapid early warning and comprehensive analysis models for urban waterlogging based on AutoML and comparison of the other three machine learning algorithms},
journal = {Journal of Hydrology},
volume = {605},
pages = {127367},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2021.127367},
url = {https://www.sciencedirect.com/science/article/pii/S0022169421014177},
author = {Yuchen Guo and Lihong Quan and Lili Song and Hao Liang},
keywords = {Urban waterlogging, Automatic machine learning algorithm based on genetic algorithms, Rapid early warning, XGBoost, CatBoost, BPDNN, Comprehensive analysis},
abstract = {Urban waterlogging often causes urban disasters, and the rapid early warning and comprehensive analysis of the urban waterlogging can help disaster defenses. However, the warning of waterlogging through the monitoring data cannot give grid distribution and the forecast of hydrological models cannot ensure rapid early warning. To obtain a grid rapid early warning result for a region, like an urban area, a method needs to be proposed which can meet the above problems. In this research, AutoML (automatic machine learning based on genetic algorithm) was recommended to construct the rapid early warning and comprehensive analysis models for urban waterlogging by compared with the other three machine learning algorithms, CatBoost (Categorical Boosting), XGBoost (eXtreme Gradient Boosting), and BPDNN (Back Propagation Deep Learning Neural Network). In the models, the forecast and historical precipitation obtained from the Integrated Nowcasting through Comprehensive analysis system (INCA), the difference of elevation, and the urban waterlogging risk maps provided by Tianjin Meteorological Administration were employed as the input sources. The input precipitation duration was determined as 12 h based on the sensitivity analysis of the influence of various precipitation duration on waterlogging depths. Due to the non-digital (discrete dataset) features, the urban waterlogging risk maps were transformed to the weight of each corresponding risk level according to the area of each risk level and the number of samples falling in each risk level. The difference of elevation was characterized by the average elevations of various distances from the points of concern. The output waterlogging depths were compared with the waterlogging depths monitored in Tianjin, China, whose quality was controlled by eliminating the records of the waterlogging depths lasting for a long time after the end of rainfall. The comparison of the models constructed by different methods demonstrated that the AutoML performed better (NSE and R2 > 0.92, CC > 0.95, RMSE1.1–1.9 cm) than the other three models. The forecast waterlogging depths by AutoML was also coherent with the monitoring waterlogging depths (NSE and R2 ≥ 0.9, CC ≥ 0.95, RMSE 1.7–2.2 cm). For that local topography and waterlogging risk are considered, the AutoML models can be used in the area without the monitoring of water level, quickly predict waterlogging depths and give spatial grid results for rapidly early warning.}
}
@article{WILSON2022101652,
title = {Public engagement and AI: A values analysis of national strategies},
journal = {Government Information Quarterly},
volume = {39},
number = {1},
pages = {101652},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101652},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000885},
author = {Christopher Wilson},
keywords = {Public engagement, Public values, Public value management, Artificial intelligence, Technology frames, Technology policy, Participation, Open government},
abstract = {Calls for public engagement and participation in AI governance align strongly with a public value management approach to public administration. Simultaneously, the prominence of commercial vendors and consultants in AI discourse emphasizes market value and efficiency in a way often associated with the private sector and New Public Management. To understand how this might influence the consolidation of AI governance regimes and decision-making by public administrators, 16 national strategies for AI are subjected to content analysis. References to the public's role and public engagement mechanisms are mapped across national strategies, as is the articulation of values related to professionalism, efficiency, service, engagement, and the private sector. Though engagement rhetoric is common, references to specific engagement mechanisms and activities are rare. Analysis of value relationships highlights congruence of engagement values with professionalism and private sector values, and raises concerns about neoliberal technology frames that normalize AI, obscuring policy complexity and trade-offs.}
}
@article{LIU2022106856,
title = {Context2Vector: Accelerating security event triage via context representation learning},
journal = {Information and Software Technology},
volume = {146},
pages = {106856},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106856},
url = {https://www.sciencedirect.com/science/article/pii/S095058492200026X},
author = {Jia Liu and Runzi Zhang and Wenmao Liu and Yinghua Zhang and Dujuan Gu and Mingkai Tong and Xingkai Wang and Jianxin Xue and Huanran Wang},
keywords = {Context modeling, Event triage, Topic model, Representation learning},
abstract = {Context:
Security teams are overwhelmed by thousands of alerts and events everyday, which are comprehensively collected for threat analysis in security operations center. Although methods based on rules, intelligence and data mining are utilized, the alert fatigue situation is still a challenging problem, slowing down the overall threat investigation process.
Objective:
‘Event polysemy’ phenomenon broadly exists in large-scale event dataset, which means that events of the same category can reveal different purposes in different contexts. This paper aims at exploring, revealing and evaluating the latent patterns embedding in the event contexts, to gain insight on context semantics and reduce manual intervention in event triage tasks.
Method:
A context representation learning based method, named Context2Vector, is proposed. Contexts are extracted from multiple behavioral views. Then, both dense event representations and sparse topic representations are learnt at the same time and in the same space. A human-in-the-loop topic annotation process is involved and finally, a context deviation detection based method is integrated to generate explainable and informative labels for automated context semantic decoding.
Results:
Various experiments are conducted on a enterprise-scale event dataset. The topic annotation, context related feature importance and top-N event ranking evaluation results show that Context2Vector outperforms traditional methods on the high-risk event identification problems, improving the attacker recall rate by up to 2.25 times within limited events to be investigated.
Conclusion:
It is concluded that event contexts imply practicable and abundant information in regard to behaviors and intents of real threat actors. More precise profiling of network entities can be extracted from contexts, compared to rules, intelligence, and anomaly detectors used in practice.}
}
@article{LIN2022103650,
title = {Cultivating proactive information security behavior and individual creativity: The role of human relations culture and IT use governance},
journal = {Information & Management},
volume = {59},
number = {6},
pages = {103650},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103650},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000623},
author = {Canchu Lin and Jenell L.S. Wittmer and Xin (Robert) Luo},
keywords = {Proactive information security behavior, Individual creativity, IT use governance, Human relations culture},
abstract = {This study developed the concept of proactive information security behavior and examined its connections with individual creativity and two organizational context factors: human relations culture (values and beliefs that promote participation and autonomy in decision-making) and IT use governance. Reliability and validity of this construct were tested with survey data. Findings of this study support its positive relationship with individual creativity, human relations culture, and IT use governance, and show partial mediation effects of individual creativity on the relationships between human relations culture and IT use governance, and proactive information security behavior. Theoretical and practice implications are discussed.}
}
@article{YIN2022107106,
title = {Impact of gamification elements on user satisfaction in health and fitness applications: A comprehensive approach based on the Kano model},
journal = {Computers in Human Behavior},
volume = {128},
pages = {107106},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.107106},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221004295},
author = {Siqi Yin and Xianling Cai and Ziyang Wang and Yuning Zhang and Shenghui Luo and Jingdong Ma},
keywords = {Gamification, User satisfaction, Kano model, Health and fitness apps},
abstract = {As health and fitness applications (apps) take on larger markets, attracting and satisfying users has become a crucial task. Gamification, a popular ancillary design used in various fields, was gradually introduced into apps to make the products more attractive and practical. As previous studies mainly focused on the mechanism or efficiency merely on specific gamification elements (GE), heterogeneity was prominent, and scope was limited. Hence, this study comprehensively arranges GE into four dimensions. The main objective is to determine the correlation between gamification dimensions and user satisfaction. Natural language process and sentimental analysis were used in online reviews (N = 45,659) of a typical fitness app to build a hierarchical conception vocabulary of gamification words, elements, and dimensions, as well as calculate the degree of elements’ realization. Meanwhile, Kano model–based questionnaires (n = 110) quantitatively classified GE into different quality classifications. A user satisfaction score was provided at the end to quantitatively measure the correlation between GE and user satisfaction, demonstrating that different dimensions corresponded to a different level of contribution to user satisfaction (non-negative, stable positive, limitation, and uncertain). Furthermore, suggestions for designing strategy were given as an instructor for further improvement and design.}
}
@article{MERKERT2022100797,
title = {The impact of engine standardization on the cost efficiency of airlines},
journal = {Research in Transportation Business & Management},
pages = {100797},
year = {2022},
issn = {2210-5395},
doi = {https://doi.org/10.1016/j.rtbm.2022.100797},
url = {https://www.sciencedirect.com/science/article/pii/S2210539522000189},
author = {Rico Merkert},
keywords = {Fleet/engine standardization, Airline performance, DEA, Cost efficiency, Competitive advantage},
abstract = {Fleet standardization, which refers to the homogeneity and harmonization of the fleet in terms of the number of manufacturers and models, as a strategy has been deployed across many sectors, including airline operations. Given that engines are substantial aircraft components in terms of both capital and operating cost, representing nearly 50% of the aerospace aftermarket, we investigate whether – and to what extent – standardization strategies in the context of engines can improve cost efficiency of the airline industry. Using engine data of 12,305 aircraft and financial data of years with high (2013/14) and low (2016/17) fuel prices, we apply bootstrapped Data Envelopment Analysis (DEA) followed by random effects panel regression analysis to 84 airlines across the globe. Our quantitative findings are validated by qualitative stakeholder interviews. Our results suggest that both airframe and engine commonality impact on airline cost efficiency, but that engine cost effects are significantly larger in magnitude than standard airframe cost effects. In contrast to current management practices focusing exclusively on airframe commonality and tactical activities (such as renegotiating supplier and maintenance contracts), we demonstrate that an engine standardization strategy (through procurement, storage, retirement, maintenance and spare part optimization) improves the cost competitiveness and efficiency of airlines in an OEM servitized world.}
}
@article{ZHANG2022108745,
title = {Relationships between 3D urban form and ground-level fine particulate matter at street block level: Evidence from fifteen metropolises in China},
journal = {Building and Environment},
volume = {211},
pages = {108745},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108745},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321011343},
author = {Anqi Zhang and Chang Xia and Weifeng Li},
keywords = {3D urban form, Surface PM, Spatial metrics, Street block, Comparative analysis},
abstract = {Substantial efforts have been devoted to exploring the effects of urban form on fine particulate matter (PM2.5), but the complexity has been far away from being fully understood. The current remarkable inconsistencies with regards to measurements, ascertainment, and findings make the evidence across continents, regions, or cities be necessary to verify the robustness and generalizability of urban form effects. Besides, existing measurements of urban form are often unsystematic and limit analyses in both horizontal and vertical dimensions. In this paper, fifteen metropolises in China were selected to examine the relationships between three-dimensional (3D) urban form and PM2.5 concentrations at the street block level, using 3D spatial metrics and multivariate linear regression. Satellite-derived surface PM2.5 estimates of fine spatial resolution, building footprint, and multiple geographic open datasets were used. Our results demonstrated that urban form effects hold for the metropolises in China, and street accessibility, length of road segments, topography, urban vegetation, surrounding open and green spaces, and transportation facilities were found to be the influential factors of the concentrations of PM2.5. We also revealed the complicated and place-varying effects of urban form indicators, represented by the largely different or opposite effects of many urban form indicators in different cities. For example, building density, building height, and land use mixture have relatively limited and inconsistent effects in most cities. Results of this work suggest critical reflections on some of the current ideas that have been accepted to be vital for improving PM2.5.}
}
@article{N2022110086,
title = {E-learning course recommendation based on sentiment analysis using hybrid Elman similarity},
journal = {Knowledge-Based Systems},
pages = {110086},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110086},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122011820},
author = {Vedavathi N. and Anil Kumar K.M.},
keywords = {E-learning, Recommendation system, Pre-processing, Feature extraction, Sentimental analysis, Optimization, Classification, Elman Minimal Redundancy Maximum Relevance (EMRMR)},
abstract = {Online learning is also referred to as E-learning which has gained huge attention and attracted most people during the COVID-19 lockdowns. Due to the excess of online information, users face severe challenges and difficulties realizing the best course that is being competitive in the global market. Therefore, it is necessary to develop an online recommendation system that supports the users in selecting the finest course with E-learning. Thus, the proposed work develops a robust RS model using different approaches. Initially, the pre-processing stage is performed to reduce the presented noise in the website data. Then, the feature extraction stage is done to extract the needed features using Improved TF-IDF, W2V (Word 2 Vector), and Hybrid N-gram. Finally, Elman Minimal Redundancy Maximum Relevance and Enhanced Aquila Optimization (EMRMR_EAO) model is proposed to provide Robust course recommendations. In this work, the ERNN method is used to classify the sentiments based on the similarity measure of the MRMR model. The top course recommendation is afforded depending on the similarity scores like Jaccard similarity, cosine similarity and euclidean similarity. Also, the loss function in the classifier is reduced by optimizing the weight parameters using the EAO approach. The performance analysis shows that the proposed recommendation model obtains improved results in terms of accuracy of 99.98%, recall of 99.81%, precision of 99.65%, and F-measure of 99.95%. The comparative analysis exhibit that the proposed EMRMR_EAO model attains better performance than the other existing works in the literature.}
}
@article{XIONG2022103139,
title = {Intelligent additive manufacturing and design state of the art and future perspectives},
journal = {Additive Manufacturing},
volume = {59},
pages = {103139},
year = {2022},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2022.103139},
url = {https://www.sciencedirect.com/science/article/pii/S2214860422005280},
author = {Yi Xiong and Yunlong Tang and Qi Zhou and Yongsheng Ma and David W. Rosen},
keywords = {Additive manufacturing, Intelligent manufacturing, Cyber-physical system, Integrated design and manufacturing},
abstract = {In additive manufacturing (AM), intelligent technologies are proving to be a powerful tool for facilitating economic, efficient, and effective decision-making within the product and service development. Such capabilities hold great promise to significantly improve the producibility, repeatability, and reproducibility of the additive manufacturing process and unlock its complete design freedom for product innovation. This paper defines the concept of intelligent additive manufacturing and design (IAMD) while providing a triple-layer model for reference. Details about these three layers, i.e., digital thread layer, cyber-physical layer, and intelligent service layer, are presented. Moreover, both scientific and engineering challenges raised during the studies and implementations of IAMD are discussed together with potential solutions. The paper also outlines the future perspective on IAMD towards the directions of integrated design and manufacturing, cyber-physical AM, advanced artificial intelligence for AM, digital materials and products, as well as design for AM process chain.}
}
@article{YAGLI2022111909,
title = {Ensemble solar forecasting and post-processing using dropout neural network and information from neighboring satellite pixels},
journal = {Renewable and Sustainable Energy Reviews},
volume = {155},
pages = {111909},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111909},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121011734},
author = {Gokhan Mert Yagli and Dazhi Yang and Dipti Srinivasan},
keywords = {Dropout neural network, Ensemble solar forecasting, Machine learning, Monte Carlo sampling, Post-processing, Satellite-derived irradiance},
abstract = {Ensemble weather forecasts are often found to be under-dispersed and biased. Post-processing using spatio-temporal information is, therefore, required if one wishes to improve the quality of the raw forecasts. It is on this account that the present article generates and post-processes ensemble solar forecasts using satellite-derived irradiance not only from the focal pixel but also from the neighboring pixels. The ensemble forecasting model of choice is a dropout neural network with Monte Carlo sampling, eliminating the need for training multiple models and ensuring parameter diversity in ensemble forecasting. Subsequently, ensemble forecasts are post-processed using both parametric and nonparametric post-processing techniques, such as nonhomogenous regression, generalized additive model, linear quantile regression, or quantile random forests. The proposed forecasting framework is demonstrated and verified using four years of half-hourly data, at seven locations in the United States. Continuous ranked probability skill scores as high as 66% have been obtained when comparing the proposed method to a conditional climatology reference. The content of this article may be useful to a wide range of stakeholders in the power system, including but not limited to: independent system operators, who aim at efficiently maintaining the system’s reliability; utility- and distributed-scale PV plant owners, who wish to avoid penalties for power deviation between the scheduled and real-time delivery; and forecast retailers, who can benefit from selling solar forecasts of higher quality.}
}
@incollection{GUO2022551,
title = {Chapter 22 - Sensing and monitoring of urban roadway traffic state with large-scale ride-sourcing vehicles},
editor = {Amir H. Alavi and Maria Q. Feng and Pengcheng Jiao and Zahra Sharif-Khodaei},
booktitle = {The Rise of Smart Cities},
publisher = {Butterworth-Heinemann},
pages = {551-582},
year = {2022},
isbn = {978-0-12-817784-6},
doi = {https://doi.org/10.1016/B978-0-12-817784-6.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128177846000035},
author = {Shuocheng Guo and Xinwu Qian and Sagar Dasgupta and Mizanur Rahman and Steven Jones},
keywords = {Traffic sensing, Ride-sourcing vehicle, Sensor observability},
abstract = {The monitoring of urban roadway traffic state is an essential task for the efficient operation of urban transportation system and mobility of urban population at scale. Current practices of sensing urban traffic are primarily based on embedded sensors (e.g., loop detector) on a roadway or roadside sensors (e.g., roadside camera) and the crowdsourcing data from massive urban travelers through mobile application such as Google traffic data, which are associated with high costs or high degree of uncertainty due to lack of sufficient data from users. This chapter investigates an innovative approach for monitoring urban roadway traffic state with large-scale ride-sourcing vehicles (RVs). An example can be monitoring roadway traffic state through sensing the state of entire fleet of for-hire vehicles such as Uber and Lyft. The RVs serve as a connected mobile sensor network, which ensures sufficient coverage in high demand areas and can be routed to supply additional observations in other areas when off duty. Moreover, the RV fleet can sense, monitor, and predict overall roadway traffic condition that is beyond traditional traffic sensing techniques, such as future travel demand and shortage of mobility supply. This chapter reviews current traffic sensing and monitoring techniques and their comparison with RV-based approach. We present a real-time RV-based roadway traffic sensing and monitoring framework along with a computational architecture integrating edge and cloud computing concept. Finally, we present a case study on the reliability of the monitoring and prediction of roadway network travel time using real-world RV trajectory data. We report that while only 65% of road segments are explicitly traversed by RVs, over 80% of total network information can be inferred from the actual observations, and only less than 10% of additional links need to be visited in order to reach the 100% network coverage rate.}
}
@incollection{KAMBOURIS202275,
title = {Chapter 6 - Nonmicrobial biothreats: DNA, prions, and (bio)regulators/(bio)toxins},
editor = {Manousos E. Kambouris},
booktitle = {Genomics in Biosecurity},
publisher = {Academic Press},
pages = {75-91},
year = {2022},
series = {Translational and Applied Genomics},
isbn = {978-0-323-85236-4},
doi = {https://doi.org/10.1016/B978-0-323-85236-4.00011-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032385236400011X},
author = {Manousos E. Kambouris and Georgios Skiniotis},
keywords = {Prions, Bioregulators, Toxins, Antitoxins, Antidotes, Naked DNA, Biochemical agents, Structural studies, Electron microscopy, Spectrometry},
abstract = {Biochemical agents, originally comprising toxins and bioregulators but belatedly enriched by prions and naked DNAs, comprise a highly heterogeneous category of biorisk factors minimally apprehensible by standard genomic approaches, even the most elaborate ones. Capable of serious detrimental effects, with their disruptive potential multiplied by the terror they cause after millennia of illicit use and due to the prospect of highly lethal bioengineered spinoffs, they are of quintessential importance in every biosecurity-oriented approach. Detecting and tackling these threats requires cutting-edge approaches able to prognose functional specifics and structure-function correlations so as to predict activity, characteristics, and vulnerabilities of such agents, in a process reminiscent of standard pharmacological research, so as to device proper response strategies in an integrated biosecurity context.}
}
@article{LI2022344,
title = {Spectral index-driven FCN model training for water extraction from multispectral imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {192},
pages = {344-360},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622002283},
author = {Zhenshi Li and Xueliang Zhang and Pengfeng Xiao},
keywords = {Water delineation, Spectral index, Sentinel-2, Semantic labeling, Deep fully convolutional network},
abstract = {Terrestrial water is a fundamental component of the land surface. Accurate and robust water delineation of surface water is rather challenging due to the high intra-class variability and the spectral similarity with shadows and other dark surfaces. This study proposes a new method called the water index-driven deep fully convolutional neural network (WIDFCN) for high-accuracy water delineation with no need to collect samples manually. We formulate water delineation as a semantic labeling problem and solve it by training a deep fully convolutional network (FCN), whose capability of effectively extracting multilevel spatial and spectral features is exploited for discriminating water from complex surroundings. The main obstacle of using FCN, which requires a large volume of labeled training samples, is settled by utilizing the water recognition ability of the water spectral index (WI). Specifically, the training samples are automatically generated from WI by extracting a high-precision but incomplete water mask at first, which is then expanded to enhance the completeness. This strategy ensures the high quality of the automatically generated training samples and thus the water extraction performance of the trained FCN model. Twelve test sites from Sentinel-2 imagery with various water delineation challenges all over the world are used to assess the performance relative to that of supervised and unsupervised classification methods and water spectral index thresholding methods, in terms of Kappa coefficient, precision, recall, and F1-score. Overall, WIDFCN can achieve the highest precision and comparable recall, leading to the best water delineation accuracy with Kappa coefficient 0.9673 and F1-score 0.9696, as well as the lowest fluctuation in terms of various test sites. The results further demonstrate that WIDFCN can effectively deal with the scale and spectra variance of surface water, and has distinct robustness with respect to different kinds of shadows, including building, mountain, and cloud shadows. The findings in this study demonstrate a novel, robust, low-cost, and manual labor-free water delineation method that performs well in terms of both precision and completeness. Moreover, the core idea could provide a reference for extraction of geographic information by utilizing FCN models with no needs of manually labeling costs. Code and data will be available at https://github.com/LZhenShi/WIDFCN.}
}
@article{RANI2022118085,
title = {An efficient format-independent watermarking framework for large-scale data sets},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118085},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118085},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422012829},
author = {Sapana Rani and Raju Halder},
keywords = {Digital watermarking, Large-scale data sets, MapReduce, Pig, Hive},
abstract = {In this paper, we propose an efficient distortion-free watermarking of large-scale data sets in various formats by exploiting the power of parallel and distributed computing environment. In particular, we adapt MapReduce, Pig and Hive paradigms for the data in CSV, XML and JSON formats by identifying key computational steps involved in the sequential watermarking algorithms. Following this, we design a middleware which allows watermark generation and verification (under any computing paradigm of user’s choice) of large-scale data sets (in any suitable format of user’s interest) and their conversion without affecting the watermark. The experimental evaluation on large-scale benchmark data sets shows a significant reduction of watermark generation and verification times. Interestingly, in case of XML and JSON formats, Pig and Hive outperform the MapReduce paradigm, whereas MapReduce shows better performance in case of CSV format. To the best of our knowledge, this is the first proposal towards large-scale data sets watermarking, considering popular distributed computing paradigms and data formats.}
}
@article{BESSEN2022104513,
title = {The role of data for AI startup growth},
journal = {Research Policy},
volume = {51},
number = {5},
pages = {104513},
year = {2022},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2022.104513},
url = {https://www.sciencedirect.com/science/article/pii/S0048733322000415},
author = {James Bessen and Stephen Michael Impink and Lydia Reichensperger and Robert Seamans},
keywords = {Artificial intelligence, Competition, Data, Algorithms, Venture capital},
abstract = {Artificial intelligence (AI)-enabled products are expected to drive economic growth. Training data are important for firms developing AI-enabled products; without training data, firms cannot develop or refine their algorithms. This is particularly the case for AI startups developing new algorithms and products. However, there is no consensus in the literature on which aspects of training data are most important. Using unique survey data of AI startups, we find a positive correlation between having proprietary training data and obtaining future venture capital funding. Moreover, this correlation is greater for startups in markets where data is a major advantage and for startups using more sophisticated algorithms, such as neural networks and ensemble learning.}
}
@incollection{LAMARCA20223,
title = {Chapter 1 - User vs. machine-based seismic attribute selection for unsupervised machine learning techniques: Does human insight provide better results than statistically chosen attributes?},
editor = {Shuvajit Bhattacharya and Haibin Di},
booktitle = {Advances in Subsurface Data Analytics},
publisher = {Elsevier},
pages = {3-30},
year = {2022},
isbn = {978-0-12-822295-9},
doi = {https://doi.org/10.1016/B978-0-12-822295-9.00002-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222959000029},
author = {Karelia {La Marca} and Heather Bedle},
keywords = {Machine learning techniques, Principal component analysis, Self-organized maps},
abstract = {In geosciences, a variety of machine learning (ML) algorithms are currently being employed for multiple purposes, for example, facies classification, fault prediction, and reservoir characterization. Among these are two clustering methods: principal component analysis (PCA) and self-organized maps (SOMs), which provide a fast organization of data into groups or clusters (with no geologic supervision) that aid in preliminary geological interpretation. With increasingly common usage of these techniques, the motivation of this chapter is to investigate the impact of a user-controlled selection of attributes to perform SOM for deepwater seismic facies classification versus a machine-selected result through PCA. Results reveal that whereas an appropriate combination of attributes with a clear interpretation objective enhances the SOM’s results and facilitates the interpreter understanding of the output classes, PCA provides insightful information regarding the contribution of seismic attributes that may not have been initially considered. While machine learning techniques are a powerful “tool” for geological interpretation, user control on initial input attributes and validation of output using an “in-context” interpretation is necessary for an optimal elucidation, at least in unsupervised machine learning methods.}
}
@article{ZHENG2022631,
title = {Hybrid model of a cement rotary kiln using an improved attention-based recurrent neural network},
journal = {ISA Transactions},
volume = {129},
pages = {631-643},
year = {2022},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2022.02.018},
url = {https://www.sciencedirect.com/science/article/pii/S0019057822000775},
author = {Jinquan Zheng and Liang Zhao and Wenli Du},
keywords = {Hybrid model, Rotary kiln, Attention mechanism, Recurrent neural network},
abstract = {A rotary kiln is core equipment in cement calcination. Significant time delay, time-varying, and nonlinear characteristics cause challenges in the advance process control and operational optimization of the rotary kiln. However, the traditional mechanism model with many assumptions cannot accurately represent the dynamic kiln process because kinetic parameters are difficult to obtain. This paper proposes a novel hybrid strategy to develop a dynamic model of a rotary kiln by combining a process mechanism and a recurrent neural network to address this issue. A time delay mechanism is used to estimate the kiln’s residence time to compensate for the time delay. A long short-term memory model that combines an attention mechanism and an ordinary differential equation solver is proposed to capture the time-varying and nonlinear behaviors of the kiln process. Case studies from two real-world cement plants with different processing loads are used to verify the effectiveness of the proposed hybrid modeling strategy. The results show that the proposed method has better accuracy and robustness than the traditional methods. The sensitivity analysis of the proposed model also makes it practical for t control system design and real-time optimization.}
}
@article{YANG2022105366,
title = {IoT data analytics in dynamic environments: From an automated machine learning perspective},
journal = {Engineering Applications of Artificial Intelligence},
volume = {116},
pages = {105366},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105366},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622003803},
author = {Li Yang and Abdallah Shami},
keywords = {IoT data analytics, AutoML, Concept drift, Machine learning},
abstract = {With the wide spread of sensors and smart devices in recent years, the data generation speed of the Internet of Things (IoT) systems has increased dramatically. In IoT systems, massive volumes of data must be processed, transformed, and analyzed on a frequent basis to enable various IoT services and functionalities. Machine Learning (ML) approaches have shown their capacity for IoT data analytics. However, applying ML models to IoT data analytics tasks still faces many difficulties and challenges, specifically, effective model selection, design/tuning, and updating, which have brought massive demand for experienced data scientists. Additionally, the dynamic nature of IoT data may introduce concept drift issues, causing model performance degradation. To reduce human efforts, Automated Machine Learning (AutoML) has become a popular field that aims to automatically select, construct, tune, and update machine learning models to achieve the best performance on specified tasks. In this paper, we conduct a review of existing methods in the model selection, tuning, and updating procedures in the area of AutoML in order to identify and summarize the optimal solutions for every step of applying ML algorithms to IoT data analytics. To justify our findings and help industrial users and researchers better implement AutoML approaches, a case study of applying AutoML to IoT anomaly detection problems is conducted in this work. Lastly, we discuss and classify the challenges and research directions for this domain.}
}
@article{JIA2022233,
title = {A meteorologically adjusted ensemble Kalman filter approach for inversing daily emissions: A case study in the Pearl River Delta, China},
journal = {Journal of Environmental Sciences},
volume = {114},
pages = {233-248},
year = {2022},
note = {Atmospheric Chemistry in the Complex Air Pollution},
issn = {1001-0742},
doi = {https://doi.org/10.1016/j.jes.2021.08.048},
url = {https://www.sciencedirect.com/science/article/pii/S1001074221003521},
author = {Guanglin Jia and Zhijiong Huang and Xiao Tang and Jiamin Ou and Menghua Lu and Yuanqian Xu and Zhuangmin Zhong and Qing'e Sha and Huangjian Wu and Chuanzeng Zheng and Tao Deng and Duohong Chen and Min He and Junyu Zheng},
keywords = {Emission inversion, Daily emissions, Meteorological adjustment, Ensemble Kalman filter},
abstract = {The conventional Ensemble Kalman filter (EnKF), which is now widely used to calibrate emission inventories and to improve air quality simulations, is susceptible to simulation errors of meteorological inputs, making accurate updates of high temporal-resolution emission inventories challenging. In this study, we developed a novel meteorologically adjusted inversion method (MAEInv) based on the EnKF to improve daily emission estimations. The new method combines sensitivity analysis and bias correction to alleviate the inversion biases caused by errors of meteorological inputs. For demonstration, we used the MAEInv to inverse daily carbon monoxide (CO) emissions in the Pearl River Delta (PRD) region, China. In the case study, 60% of the total CO simulation biases were associated with sensitive meteorological inputs, which would lead to the overestimation of daily variations of posterior emissions. Using the new inversion method, daily variations of emissions shrank dramatically, with the percentage change decreased by 30%. Also, the total amount of posterior CO emissions estimated by the MAEInv decreased by 14%, indicating that posterior CO emissions might be overestimated using the conventional EnKF. Model evaluations using independent observations revealed that daily CO emissions estimated by MAEInv better reproduce the magnitude and temporal patterns of ambient CO concentration, with a higher correlation coefficient (R, +37.0%) and lower normalized mean bias (NMB, -17.9%). Since errors of meteorological inputs are major sources of simulation biases for both low-reactive and reactive pollutants, the MAEInv is also applicable to improve the daily emission inversions of reactive pollutants.}
}
@article{BOKHORST2022108599,
title = {Assessing to what extent smart manufacturing builds on lean principles},
journal = {International Journal of Production Economics},
volume = {253},
pages = {108599},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2022.108599},
url = {https://www.sciencedirect.com/science/article/pii/S0925527322001827},
author = {Jos A.C. Bokhorst and Wilfred Knol and Jannes Slomp and Thomas Bortolotti},
keywords = {Smart manufacturing, Industry 4.0, Lean principles, Operational performance, Necessary condition analysis},
abstract = {This study explores to what extent the adoption and performance of smart manufacturing technologies builds on the adoption of lean principles. Primary explorative survey data on the level of adoption of smart manufacturing technologies and lean principles and various operational performance outcomes were collected from a set of Dutch manufacturers and analysed using Cluster Analysis, ANOVA, and Necessary Condition Analysis (NCA). The Cluster Analysis shows that while lean is also applied without smart (“lean-only” companies), smart technologies are mostly applied in conjunction with lean (“lean and smart” companies), suggesting that the presence of lean principles is necessary for smart implementation. A third group of companies shows a low use of lean and smart (“non-adopters”). The NCAs further specify the extent of this necessity by showing that all individual smart manufacturing technologies used in our construct require presence of lean principles, with MES systems having the strongest dependency. Performance wise, lean-only and lean and smart companies have comparable superior performance compared to non-adopters when considering an aggregate operational performance measure using the dimensions of quality, delivery, flexibility and cost. When analysed separately, the aggregate level results remain true for quality and delivery performance. However, for flexibility, the superiority of lean-only companies is more apparent, while for cost, lean and smart companies are superior. This shows that implementing smart requires lean, but lean may suffice depending on the specific performance objectives strived for.}
}
@article{CHENG2022201,
title = {OPTDP: Towards optimal personalized trajectory differential privacy for trajectory data publishing},
journal = {Neurocomputing},
volume = {472},
pages = {201-211},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.04.137},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016271},
author = {Wenqing Cheng and Ruxue Wen and Haojun Huang and Wang Miao and Chen Wang},
keywords = {Trajectory data publishing, Personalized differential privacy, Semantic similarity, Privacy level},
abstract = {With the development of location-based applications, more and more trajectory data are collected. Trajectory data often contains users’ sensitive information, and direct release it may pose a threat to users’ privacy. Differential privacy, as a privacy preserving method with solid mathematical foundation, has been widely used in trajectory data publishing. However, current trajectory data publishing methods based on differential privacy cannot fully realize the personalized privacy protection. In this paper, an optimal personalized trajectory differential privacy mechanism is proposed. Firstly, by establishing the probabilistic mobility model of trajectories, we cluster the locations to achieve semantic location matching between different trajectories. Based on the semantic similarity, we identify the templet trajectory, and propose a privacy level allocation method based on stay-points and frequent sub-trajectories. Then, according to the location matching results, we can automatically identify the privacy level of all locations. Combined with the optimal location differential privacy mechanism, we disturb the location points on the user’s trajectory before publishing, where different location privacy levels correspond to different privacy budgets. Experiment results on real-world datasets show that our mechanism provides a better tradeoff between privacy protection and data utility compared with traditional differential privacy methods.}
}
@article{GUNGOR2022103660,
title = {STEWART: STacking Ensemble for White-Box AdversaRial Attacks Towards more resilient data-driven predictive maintenance},
journal = {Computers in Industry},
volume = {140},
pages = {103660},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103660},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000574},
author = {Onat Gungor and Tajana Rosing and Baris Aksanli},
keywords = {Cybersecurity in Industrial IoT, Predictive maintenance, Adversarial machine learning, Ensemble learning},
abstract = {Industrial Internet of Things (I-IoT) is a network of devices that focus on monitoring industrial assets and continuously collecting data. This data can be utilized by Machine Learning (ML) methods to perform Predictive Maintenance (PDM) which identifies an optimal maintenance schedule for the industrial assets. The computational systems in the I-IoT are usually not designed with security in mind. Their limited computational power creates security vulnerabilities that attackers can exploit to prevent asset availability, sabotage communication, and corrupt system data. In this work, we first demonstrate that cyber-attacks can impact the performance of ML-based PDM methods significantly, leading up to 120 × prediction performance loss. Next, we develop a stacking ensemble learning-based framework that stays resilient against various white-box adversarial attacks. The results show that our framework performs well in the presence of cyber-attacks and has up to 60% higher resiliency compared to the most resilient individual ML method.}
}
@article{ZHANG2022101204,
title = {The situational nature of impulse buying on mobile platforms: A cross-temporal investigation},
journal = {Electronic Commerce Research and Applications},
volume = {56},
pages = {101204},
year = {2022},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2022.101204},
url = {https://www.sciencedirect.com/science/article/pii/S1567422322000874},
author = {Lin Zhang and Zhen Shao and Jing Zhang and Xiaotong Li},
keywords = {Digital marketing, Impulse purchase, S–O–R framework, Website quality, Price attributes, Situational stimulus},
abstract = {While online impulse buying has attracted increasing attentions from researchers, there is still limited research that investigates consumers’ impulse buying behavior across different situations. Categorizing external stimuli into three types (website, marketing and situational stimuli), our study examines their joint influences on consumers’ affective and cognitive reactions as well as their online impulse buying behavior. Our empirical findings indicate that the positive effects of website stimuli and marketing stimuli on consumers’ internal reactions exhibit significant variations based on a situational stimulus (i.e., a non-holiday season versus a holiday season). Specifically, consumers react more sensitively to perceived website quality to form both hedonic and utilitarian values during the non-holiday season, while they focus more on prices to judge utilitarian value rather than hedonic value during the holiday season. Furthermore, our results suggest that the cascading mediation effects of consumers’ internal responses vary greatly between the non-holiday season and the holiday season.}
}
@article{YANG2022100731,
title = {Nodes clustering and multi-hop routing protocol optimization using hybrid chimp optimization and hunger games search algorithms for sustainable energy efficient underwater wireless sensor networks},
journal = {Sustainable Computing: Informatics and Systems},
volume = {35},
pages = {100731},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100731},
url = {https://www.sciencedirect.com/science/article/pii/S221053792200066X},
author = {Yang Yang and Yunqiang Wu and Hongji Yuan and Mohammad Khishe and Mokhtar Mohammadi},
keywords = {Clustering, Chimp optimization algorithm, Routing, Hierarchical, Energy efficiency},
abstract = {Clustering and routing processes in underwater wireless sensor networks (UWSNs) are challenging tasks in the underwater environment due to the multiplicity of sensor nodes, transmission bandwidth, and limited energy resources. In order to address the shortcomings mentioned above, this paper proposes a novel hybrid Chimp Optimization and Hunger Games Search (ChOA-HGS) algorithms for clustering and multi-hop routing optimization in UWSNs. In this approach, first, the ChOA is used to choose cluster heads and efficiently structure clusters. Then, the HGS-based routing procedure is used to determine the network’s best pathways. The proposed approach combines the advantages of clustering and routing, resulting in optimal network lifetime and energy efficiency. The proposed ChOA-HGS is validated using a variety of measures after it is simulated using three different scenarios. In order to evaluate the performance of the ChOA-HGS, results are compared to PSO, MPSO, IPSO-GWO, TEEN, and LEACH. The results show that the ChOA-HGS outperformed other benchmarks in terms of lifetime and energy consumption.}
}
@article{YULI20221159,
title = {Identify impacting factor for urban rail ridership from built environment spatial heterogeneity},
journal = {Case Studies on Transport Policy},
volume = {10},
number = {2},
pages = {1159-1171},
year = {2022},
issn = {2213-624X},
doi = {https://doi.org/10.1016/j.cstp.2022.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2213624X22000785},
author = {Xiang {Yu Li} and Gobi {Krishna Sinniah} and Ruiwei Li},
keywords = {Built environment, Spatial heterogeneity, Urban rail system, GWR, Kuala Lumpur},
abstract = {The urban rail system is significant for the urban transport system due to its faster, dedicated lane and mass capacity feature. Many countries or cities devout build it to relieve urban traffic pressure. In an urban rail system, the station is one of the most vital elements because directly relative to station availability and user’s accessibility. This paper identifies the urban rail station spatial heterogeneity from the built environment for passenger flow under five variables that select Kuala Lumpur as a case study. Under this foundation, this paper measures and typologies the spatial heterogeneity based on the built environment by Geographic Weighting Regression (GWR). The result indicates that selecting indicator will impact the ridership but has an apparent spatial heterogeneity because the impacting situation is different in various station catchment areas even measuring variables are the same. According to this outcome, we could tangibly recognize the urban rail system station’s built environment and the impacting factor and the specific impacting ability distribution. That benefits improving or optimizing this system more efficiently from the micro-level, generating more ridership to enhance urban rail operating performance and relieve urban traffic stress.}
}
@incollection{RANI2022143,
title = {Chapter 6 - Machine learning for soil moisture assessment},
editor = {Ramesh Chandra Poonia and Vijander Singh and Soumya Ranjan Nayak},
booktitle = {Deep Learning for Sustainable Agriculture},
publisher = {Academic Press},
pages = {143-168},
year = {2022},
series = {Cognitive Data Science in Sustainable Computing},
isbn = {978-0-323-85214-2},
doi = {https://doi.org/10.1016/B978-0-323-85214-2.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032385214200001X},
author = {Alka Rani and Nirmal Kumar and Jitendra Kumar and Jitendra Kumar and Nishant K. Sinha},
keywords = {Soil moisture, Machine learning, Remote sensing, Pedotransfer functions, Downscaling},
abstract = {Soil moisture plays a key role in the Earth’s hydrological cycle and meteorological and climatic processes. The information on soil moisture content is required for irrigation scheduling, crop yield prediction, studies on weather and climate change, monitoring and forecasting extreme weather events like floods and drought, and estimation of runoff and soil erosion. The accurate and timely estimation and forecasting of soil moisture are necessary for these applications. Machine learning (ML) algorithms, like artificial neural networks, support vector machines, decision trees, random forest, and so on, are widely used for soil moisture assessment due to their ability to model nonlinear and complex relationships between variables. These algorithms are used to develop pedotransfer functions that can predict soil hydraulic properties, like available water capacity, hydraulic conductivity, soil water retention curve, and more. These algorithms are also used for the retrieval of soil moisture through remote sensing. By providing meteorological, vegetation, topographic, and historical input data about soil moisture variation, these ML algorithms can accurately forecast soil moisture after a few days. This information can be used for scheduling irrigation in the automated smart irrigation system. These algorithms are also extensively used for downscaling coarse resolution satellite-derived soil moisture products to finer spatial resolutions so that these products can be applied at the regional or watershed level. ML algorithms are contributing significantly to the progress of soil moisture research. In this chapter, an overview of the applicability of ML algorithms for soil moisture assessment in the various domains of soil moisture research is presented.}
}
@article{JULIO20221299,
title = {Long term assessment of a successful e-bike-sharing system. Key drivers and impact on travel behaviour},
journal = {Case Studies on Transport Policy},
volume = {10},
number = {2},
pages = {1299-1313},
year = {2022},
issn = {2213-624X},
doi = {https://doi.org/10.1016/j.cstp.2022.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S2213624X22000943},
author = {Raky Julio and Andres Monzon},
keywords = {Bike-sharing, Travel patterns, Cycling behaviour, Electric bicycles, Cycling factors, System evolution},
abstract = {The many benefits of cycling, such as eco-friendliness, low cost, health benefits, and efficiency in congested areas, had encouraged governmental strategies to promote it, triggering a global growth of bike-sharing systems (BSS). In this line, it is important to avoid service termination by assessing the evolution, identifying drawbacks and success factors, that could be determinant on the system’s future. Nonetheless, in many cases, subjective and objective information regarding BSS was not collected nor compared. In this study, we analyse the evolution of Madrid’s pioneer electric system, by combining the subjective data of three surveys, conducted since 2014 to 2019, with objective data from the service operator. The insights extracted shed light on the key factors determining the system’s success, and its influence on travel behaviour. Results suggest that the user profile of the young early adopters evolved to middle-aged workers. Strong maintenance campaigns and network expansions improved bikes availably and user satisfaction. Slope of the streets is one of the lowest importance factors, whereas pedelec assistance the highest. It is likely to believe that there is a relationship between both, suggesting that electric assistance encourages cycling in a hilly city like Madrid. Transferable experiences to other cities evolving from traditional to e-BSS could be valuable, like the results suggesting that the introduction of an electric BSS is a potential trigger for bicycle adoption in dense urban environments. In addition, that subscribers tend to reduce the use of private car while increase cycling. This longitudinal analysis offers valuable policy implications, like those related with bike maintenance, network extension, and measures focused on keeping the new subscribers of the COVID-19 post-lockdown.}
}
@article{KASPRZYKHORDERN2022107143,
title = {Wastewater-based epidemiology in hazard forecasting and early-warning systems for global health risks},
journal = {Environment International},
volume = {161},
pages = {107143},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2022.107143},
url = {https://www.sciencedirect.com/science/article/pii/S0160412022000691},
author = {B. Kasprzyk-Hordern and B. Adams and I.D. Adewale and F.O. Agunbiade and M.I. Akinyemi and E. Archer and F.A. Badru and J. Barnett and I.J. Bishop and M. {Di Lorenzo} and P. Estrela and J. Faraway and M.J. Fasona and S.A. Fayomi and E.J. Feil and L.J. Hyatt and A.T. Irewale and T. Kjeldsen and A.K.S. Lasisi and S. Loiselle and T.M. Louw and B. Metcalfe and S.A. Nmormah and T.O. Oluseyi and T.R. Smith and M.C. Snyman and T.O. Sogbanmu and D. Stanton-Fraser and S. Surujlal-Naicker and P.R. Wilson and G. Wolfaardt and C.O. Yinka-Banjo},
keywords = {Early warning system, Global health, Urban water fingerprinting, Wastewater-based epidemiology, Socio-economic fingerprints, Citizen science},
abstract = {With the advent of the SARS-CoV-2 pandemic, Wastewater-Based Epidemiology (WBE) has been applied to track community infection in cities worldwide and has proven succesful as an early warning system for identification of hotspots and changingprevalence of infections (both symptomatic and asymptomatic) at a city or sub-city level. Wastewater is only one of environmental compartments that requires consideration. In this manuscript, we have critically evaluated the knowledge-base and preparedness for building early warning systems in a rapidly urbanising world, with particular attention to Africa, which experiences rapid population growth and urbanisation. We have proposed a Digital Urban Environment Fingerprinting Platform (DUEF) – a new approach in hazard forecasting and early-warning systems for global health risks and an extension to the existing concept of smart cities. The urban environment (especially wastewater) contains a complex mixture of substances including toxic chemicals, infectious biological agents and human excretion products. DUEF assumes that these specific endo- and exogenous residues, anonymously pooled by communities’ wastewater, are indicative of community-wide exposure and the resulting effects. DUEF postulates that the measurement of the substances continuously and anonymously pooled by the receiving environment (sewage, surface water, soils and air), can provide near real-time dynamic information about the quantity and type of physical, biological or chemical stressors to which the surveyed systems are exposed, and can create a risk profile on the potential effects of these exposures. Successful development and utilisation of a DUEF globally requires a tiered approach including: Stage I: network building, capacity building, stakeholder engagement as well as a conceptual model, followed by Stage II: DUEF development, Stage III: implementation, and Stage IV: management and utilization. We have identified four key pillars required for the establishment of a DUEF framework: (1) Environmental fingerprints, (2) Socioeconomic fingerprints, (3) Statistics and modelling and (4) Information systems. This manuscript critically evaluates the current knowledge base within each pillar and provides recommendations for further developments with an aim of laying grounds for successful development of global DUEF platforms.}
}
@article{FRIEDERICH2022103586,
title = {A framework for data-driven digital twins of smart manufacturing systems},
journal = {Computers in Industry},
volume = {136},
pages = {103586},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103586},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001937},
author = {Jonas Friederich and Deena P. Francis and Sanja Lazarova-Molnar and Nader Mohamed},
keywords = {Data-driven, Digital twin, Machine learning, Process mining, Reconfigurable manufacturing, Smart factory},
abstract = {Adoption of digital twins in smart factories, that model real statuses of manufacturing systems through simulation with real time actualization, are manifested in the form of increased productivity, as well as reduction in costs and energy consumption. The sharp increase in changing customer demands has resulted in factories transitioning rapidly and yielding shorter product life cycles. Traditional modeling and simulation approaches are not suited to handle such scenarios. As a possible solution, we propose a generic data-driven framework for automated generation of simulation models as basis for digital twins for smart factories. The novelty of our proposed framework is in the data-driven approach that exploits advancements in machine learning and process mining techniques, as well as continuous model improvement and validation. The goal of the framework is to minimize and fully define, or even eliminate, the need for expert knowledge in the extraction of the corresponding simulation models. We illustrate our framework through a case study.}
}
@incollection{WU20221,
title = {Chapter 1 - Chronotopologic data analysis},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {1-32},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000058},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Topos, Chronos, Chronotopos, Scientific inquiry, Chronotopologic data analysis, Application, Software},
abstract = {The concepts of topos, chronos, and chronotopos are introduced in the broad scientific inquiry context; the notions of chronotopologic variability, dependency, uncertainty, estimation, and mapping are introduced; chronotopologic data analysis techniques and visualization technology are reviewed; last, the chronotopologic applications are outlined and a list of public domain software libraries is provided.}
}
@article{WEI2022694,
title = {Generating training images with different angles by GAN for improving grocery product image recognition},
journal = {Neurocomputing},
volume = {488},
pages = {694-705},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.080},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221017550},
author = {Yuchen Wei and Shuxiang Xu and Byeong Kang and Sabera Hoque},
keywords = {Grocery product recognition, Data augmentation, Generative adversarial network (GAN), Convolutional neural network (CNN)},
abstract = {Image recognition based on deep learning methods has gained remarkable achievements by feeding with abundant training data. Unfortunately, collecting a tremendous amount of annotated images is time-consuming and expensive, especially in grocery product recognition tasks. It is challenging to recognise grocery products accurately when the deep learning model is trained with insufficient data. This paper proposes multi-angle Generative Adversarial Networks (MAGAN), which can generate realistic training images with different angles for data augmentation. Mutual information is employed in the novel GAN to achieve the learning of angles in an unsupervised manner. This paper aims to create training images containing grocery products from different angles, thus improving grocery product recognition accuracy. We first enlarge the fruit dataset by using MAGAN and the state-of-the-art GAN variants. Then, we compare the top-1 accuracy results from CNN classifiers trained with different data augmentation methods. Finally, our experiments demonstrate that the MAGAN exceeds the existing GANs for grocery product recognition tasks, obtaining a significant increase in the accuracy.}
}
@article{WANG202269,
title = {FORSETI: A visual analysis environment enabling provenance awareness for the accountability of e-autopsy reports},
journal = {Visual Informatics},
volume = {6},
number = {3},
pages = {69-80},
year = {2022},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2022.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X22000407},
author = {Baoqing Wang and Noboru Adachi and Issei Fujishiro},
keywords = {Computational forensics, Legal medicine, Accountability, Provenance, Immersive analytics, Authority},
abstract = {Autopsy reports play a pivotal role in forensic science. Medical examiners (MEs) and diagnostic radiologists (DRs) cross-reference autopsy results in the form of autopsy reports, while judicial personnel derive legal documents from final autopsy reports. In our prior study, we presented a visual analysis system called the forensic autopsy system for e-court instruments (FORSETI) with an extended legal medicine markup language (x-LMML) that enables MEs and DRs to author and review e-autopsy reports. In this paper, we present our extended work to incorporate provenance infrastructure with authority management into FORSETI for forensic data accountability, which contains two features. The first is a novel provenance management mechanism that combines the forensic autopsy workflow management system (FAWfMS) and a version control system called lmmlgit for x-LMML files. This management mechanism allows much provenance data on e-autopsy reports and their documented autopsy processes to be individually parsed. The second is provenance-supported immersive analytics, which is intended to ensure that the DRs’ and MEs’ autopsy provenances can be viewed, listed, and analyzed so that a principal ME can author their own report through accountable autopsy referencing in an augmented reality setting. A fictitious case with a synthetic wounded body is used to demonstrate the effectiveness of the provenance-aware FORSETI system in terms of data accountability through the experience of experts in legal medicine.}
}
@article{CULICGAMBIROZA2022101842,
title = {Dynamic monitoring frequency for energy-efficient data collection in Internet of Things},
journal = {Journal of Computational Science},
volume = {64},
pages = {101842},
year = {2022},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2022.101842},
url = {https://www.sciencedirect.com/science/article/pii/S1877750322002010},
author = {Jelena {Čulić Gambiroža} and Toni Mastelić and Ivana {Nižetić Kosović} and Mario Čagalj},
keywords = {Dynamic monitoring frequency, Sensor data, Internet of Things, Energy efficiency},
abstract = {With growth of Internet of Things, number of connected sensors increases as well, along with data being collected by those sensors. Most sensors are battery powered and commonly collect data in short and equally spaced time periods resulting with large amount of redundant and often irrelevant data. In this paper, we propose a dynamic monitoring frequency (DMF) algorithm that aims at collecting data only when sensor readings change by more than a predefined value between consecutive readings. Thus, a sensor is turned on only when a change in monitored phenomenon value exceeds a predefined threshold. Two algorithms are analyzed, namely statistical and machine learning. DMF shows notable performance, resulting either with up to ∼70% less missed readings, or it collects up to ∼40% less data compared to the baseline algorithm that collects data with static monitoring frequency.}
}
@article{LNENICKA2022101745,
title = {Benchmarking open data efforts through indices and rankings: Assessing development and contexts of use},
journal = {Telematics and Informatics},
volume = {66},
pages = {101745},
year = {2022},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2021.101745},
url = {https://www.sciencedirect.com/science/article/pii/S0736585321001842},
author = {Martin Lnenicka and Mariusz Luterek and Anastasija Nikiforova},
keywords = {Open data, Benchmarking, Index, Ranking, Development, Indicator},
abstract = {This paper aims to provide a broad perspective on the development of benchmarking open data efforts through indices and rankings over the years, both at the level of countries and allowing for a cross-country comparison. The methodology follows a systematic search for the relevant resources, their classification and identification of six open data benchmarks to be further analyzed, the identification of their key components through decomposition, their description, and identifying the similarities and differences. Three major groups of indices and four periods that characterize the efforts to benchmark and measure the development of open data are identified, where the first measure the openness of the selected categories of data, the second focuses on different aspects of the open data ecosystem, using a large number of variables, and the third is a combination of both approaches. Recommendations as well as trends that can form the benchmarking frameworks in the future are also discussed. The findings are of a high importance for individual countries, which allow for correct and accurate interpretation of the results changes in the scope of a given index or rank, i.e., whether the difference in results is the result of national efforts or the subject of changes in the specific index, as well as how to combine and interpret the results of a number of indices for correct decision-making and for the definition of the future actions where the results vary significantly. In addition, the findings are also important for international organizations publishing benchmarking reports.}
}
@article{KIM2022103773,
title = {Does roadwork improve road speed? Evidence from urban freeways in California},
journal = {Regional Science and Urban Economics},
volume = {93},
pages = {103773},
year = {2022},
issn = {0166-0462},
doi = {https://doi.org/10.1016/j.regsciurbeco.2022.103773},
url = {https://www.sciencedirect.com/science/article/pii/S0166046222000047},
author = {Jinwon Kim},
keywords = {Traffic volume, Speed, Roadwork, Lane closure, Traffic congestion},
abstract = {This paper estimates the effects of roadwork on road speed and traffic volume using panel data from urban freeways in California. The empirical model is specified to identify the dynamic responses of road speed and traffic volume to a shift in the cost curve generated by roadwork. The estimates indicate that roadwork increases road speed shortly after the roadwork is completed, but this effect does not last longer than one year. Traffic volume does not immediately respond to roadwork but does increase after around one year. These empirical results support the “induced-demand hypothesis” of Downs (1962, 1992). This paper also quantifies the time-cost savings of roadwork to evaluate public spending on freeways. It is concluded that the congestion-relieving effect of roadwork alone is not enough to justify the state's large expenditures on roadwork.}
}
@article{LIU2022130960,
title = {DeepSniffer: A meta-learning-based chemiresistive odor sensor for recognition and classification of aroma oils},
journal = {Sensors and Actuators B: Chemical},
volume = {351},
pages = {130960},
year = {2022},
issn = {0925-4005},
doi = {https://doi.org/10.1016/j.snb.2021.130960},
url = {https://www.sciencedirect.com/science/article/pii/S0925400521015288},
author = {Chuanjun Liu and Hitoshi Miyauchi and Kenshi Hayashi},
keywords = {Chemiresistive sensor array, Meta learning, Deep metric learning, Siamese network, Aroma oil, Recognition and classification},
abstract = {A meta-learning algorithm, conventionally used for visual recognition, was applied to the recognition and classification of aroma oils. A printable chemiresistive sensor array was fabricated, based on composites of carbon black with various active materials. Standard aromatherapy kits with 30 types of essential oils were used as targets in an odor sensing experiment. Benefiting from the pattern recognition ability of the fabricated sensor array, a high-quality dataset was obtained with 30 aroma oil classes, in which each class had nine replicate samples. A deep metric learning model, based on a Siamese neural network and a multilayer perceptron, was used to perform the N-way k-shot meta-learning. A test accuracy of over 98.7% was obtained for 31-way 9-shot learning, on discriminating whether the input pair samples were taken from similar or dissimilar classes. The model was effective in extracting meta-features of the aroma oils; this was proved by the improved clustering effect of samples in the spaces of principal components analysis and t-distributed stochastic neighbor embedding. The 30 aroma oils were divided into two datasets according to 6-fold cross-validation: 25 aroma oil classes (plus one blank class) as seen classes for constructing 26-way 9-shot learning models and the remaining five aroma oils as unseen classes for prediction. Average accuracies of 93.5% and 93.9% were achieved for recognition of the unseen aroma oils from the seen classes and classification of the unseen aroma oils themselves, respectively, demonstrating the effectiveness of the developed sensor and model for odor recognition and classification.}
}
@article{LU2022106022,
title = {Using computer vision to recognize composition of construction waste mixtures: A semantic segmentation approach},
journal = {Resources, Conservation and Recycling},
volume = {178},
pages = {106022},
year = {2022},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2021.106022},
url = {https://www.sciencedirect.com/science/article/pii/S0921344921006303},
author = {Weisheng Lu and Junjie Chen and Fan Xue},
keywords = {Construction and demolition waste, Waste composition, Construction waste management, Artificial intelligence, Computer vision, Semantic segmentation},
abstract = {Timely and accurate recognition of construction waste (CW) composition can provide yardstick information for its subsequent management (e.g., segregation, determining proper disposal destination). Increasingly, smart technologies such as computer vision (CV), robotics, and artificial intelligence (AI) are deployed to automate waste composition recognition. Existing studies focus on individual waste objects in well-controlled environments, but do not consider the complexity of the real-life scenarios. This research takes the challenges of the mixture and clutter nature of CW as a departure point and attempts to automate CW composition recognition by using CV technologies. Firstly, meticulous data collection, cleansing, and annotation efforts are made to create a high-quality CW dataset comprising 5,366 images. Then, a state-of-the-art CV semantic segmentation technique, DeepLabv3+, is introduced to develop a CW segmentation model. Finally, several training hyperparameters are tested via orthogonal experiments to calibrate the model performance. The proposed approach achieved a mean Intersection over Union (mIoU) of 0.56 in segmenting nine types of materials/objects with a time performance of 0.51 s per image. The approach was found to be robust to variation of illumination and vehicle types. The study contributes to the important problem of material composition recognition, formalizing a deep learning-based semantic segmentation approach for CW composition recognition in complex environments. It paves the way for better CW management, particularly in engaging robotics, in the future. The trained models are hosted on GitHub, based on which researchers can further finetune for their specific applications.}
}
@incollection{KOLTAY202277,
title = {Chapter 4 - Research data management},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {77-108},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000023},
author = {Tibor Koltay},
keywords = {Research data management, Readiness, Skills and competencies, Planning, Research data life cycle, Data management plans, Data reference, Data citation, Data retrieval, Data curation},
abstract = {Research data management (RDM) should be central for both researchers and academic libraries. The latter provide related services that are described in this chapter. RDM embraces the entire research cycle, aiming at making the research process as efficient as possible and facilitating cooperation with other players involved in it. To get a clear picture of the nature of RDM services, a short history of the academic library’s readiness and involvement is described. Skills and competencies necessary for serving research and researchers are enumerated, followed by a portrayal of the planning and building of services, giving particular attention to the research data life cycle and to the importance of data management plans. The tasks related to data reference, data citation, and data retrieval are presented. The relationship between RDM and data curation, as well as between RDM and research support services, is characterized.}
}
@article{REIS2022117510,
title = {Predicting the lifetime of Lithium–Ion batteries: Integrated feature extraction and modeling through sequential Unsupervised-Supervised Projections (USP)},
journal = {Chemical Engineering Science},
volume = {252},
pages = {117510},
year = {2022},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2022.117510},
url = {https://www.sciencedirect.com/science/article/pii/S000925092200094X},
author = {Marco S. Reis and Benben Jiang},
keywords = {Lithium-ion batteries, Battery lifetime prediction, Battery degradation, Predictive analytics, Feature extraction, Latent variable modeling},
abstract = {Lithium-ion batteries are among the most used rechargeable batteries in the market, from portable electronics, and mobility solutions to process industry and aerospace. The ability to accurately predict battery lifetime and health status is of great interest for R&D activities, quality control & product release, predictive maintenance and recycling, among others. However, the complexity and multiplicity of degradation modes and usage/charging conditions raise considerable challenges to the construction of robust predictive models for battery lifecycle prediction. In this work, we propose an integrated methodology that is able to extract operational features and use them in the scope of a predictive model. Results demonstrate the superior feature extraction and accuracy of the proposed approach based only on the discharge information collected in the early usage periods (40–60 cycles), an aspect of practical interest. Data used regard commercial high-power LFP/graphite A123 cells, with nominal capacity of 1.1Ah and nominal voltage of 3.3 V.}
}
@article{YAN2022,
title = {Private owners’ propensity to engage in shared parking schemes under uncertainty: comparison of alternate hybrid expected utility-regret-rejoice choice models},
journal = {Transportation Letters},
year = {2022},
issn = {1942-7867},
doi = {https://doi.org/10.1080/19427867.2022.2088568},
url = {https://www.sciencedirect.com/science/article/pii/S1942786722005094},
author = {Qianqian Yan and Tao Feng and Harry Timmermans},
keywords = {Shared parking, owners, propensity, hybrid expected utility-regret-rejoice models, perception},
abstract = {ABSTRACT
To develop effective strategies for the supply of shared parking and study various theoretical choice models under uncertainty, this paper investigates private parking space owners’ propensity to engage in shared parking schemes using a stated choice experiment that involves an uncertain key attribute. A hybrid expected utility-regret model incorporating rejoice is specified to explore the participation behavior. Equivalent models considering the perception of attribute differences are also estimated. Results show that socio-demographic characteristics, social influence, government’s role, media attention, platform fee, and revenues are all important factors explaining private parking owners’ propensity to engage in shared parking schemes. Besides, the model incorporating all these components, including the emotions of regret and rejoice and the perception of attribute differences, yields the best results. These findings could help promote the policy development toward increasing people’s engagement in shared parking.}
}
@article{WANG2022123189,
title = {Energy consumption characteristics based driving conditions construction and prediction for hybrid electric buses energy management},
journal = {Energy},
volume = {245},
pages = {123189},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.123189},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222000925},
author = {Yue Wang and Keqiang Li and Xiaohua Zeng and Bolin Gao and Jichao Hong},
keywords = {Hybrid electric buses, Energy management, Energy consumption characteristics, Driving condition information},
abstract = {The energy consumption characteristics of driving condition is very important for hybrid electric buses energy management. In this paper, an energy consumption characteristics based driving conditions construction and prediction method was proposed. Under connected vehicular-cloud environment, missing data and noise data was processed by BP neural network method and wavelet transform method, respectively. According to the proposed the analysis method of energy consumption characteristics, the 7 characteristic parameters of the driving conditions related to energy consumption characteristics were extracted from 30 parameters. Based on the extracted characteristic parameters considering energy consumption, driving conditions construction and prediction were developed. In the driving cycle construction, it is found that the characteristic parameters error is less than 5% by comparing the original constructed cycles. Thus, the construction driving cycle can reflect the actual driving characteristics. In the driving cycle prediction, the prediction combining least squares support vector machine with BP neural network is proposed and compared with different topologies. The root mean square error of the proposed prediction model is 0.22 km/h, achieving the best prediction performance. Finally, energy management using the above driving condition information can significantly improve the energy economy.}
}
@article{BAAZOUZI20222727,
title = {Towards an Efficient FAIRification Approach of Tabular Data with Knowledge Graph Models},
journal = {Procedia Computer Science},
volume = {207},
pages = {2727-2736},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.331},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012200},
author = {Wiem Baazouzi and Marouen Kachroudi and Sami Faiz},
keywords = {Tabular Data, Knowledge Graph, FAIR principles},
abstract = {In this article, we present Kepler-aSI, a matching approach to overcome possible semantic gaps in tabular data by referring to a Knowledge Graph. The task proves difficult for the machines, which requires extra effort to deploy the cognitive ability in the matching methods. Indeed, the ultimate goal of our new method is to implement a fast and efficient approach to annotate tabular data with features from a Knowledge Graph. The approach combines search and filter services combined with text pre-processing techniques. The experimental evaluation was conducted in the context of the SemTab 2021 challenge and yielded encouraging and promising results referring to its performance and ranks held.}
}
@article{CHEN2022107365,
title = {Short-term wind speed forecasting based on long short-term memory and improved BP neural network},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {134},
pages = {107365},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107365},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521006049},
author = {Gonggui Chen and Bangrui Tang and Xianjun Zeng and Ping Zhou and Peng Kang and Hongyu Long},
keywords = {Short-term Wind Speed Forecasting, Data Preprocessing, Fuzzy Entropy (FE), LSTM, Improved Sparrow Search Algorithm-BP (ISSA-BP)},
abstract = {Accurate and reasonable wind speed prediction system has a significant impact on the utilization of wind energy. A novel combination forecasting model based on Long Short-Term Memory (LSTM) network and BP neural network is designed in this paper. This model combines the principle of deep learning algorithm and the improved BP neural network to deal with nonlinear wind speed prediction. Before the prediction, singular spectrum analysis (ssa) and complete ensemble empirical model decomposition adaptive noise (CEEMDAN) are selected as the data pretreatment part to de-noise the original wind speed data and decompose it into multiple components. This part is conducive to improving the signal-to-noise ratio (SNR) of wind speed data and simplifying the characteristics of wind speed data. Then, in order to reduce the error accumulation and computation redundancy, fuzzy entropy (FE) is used to calculate the time complexity of each component, according to the Spearman correlation, the inherent mode function (IMF) components are recombined to form a new subsequence. Experimental results show that the error accumulation can be reduced by 48.65% for dataset 1 and 29.53% for dataset 2, and the operation time can be reduced by about 50% for two datasets. To avoid the limitation of a single model, introducing the LSTM and improved BPNN which improved by sparrow search algorithm (SSA) two different prediction models are used to predict the sub sequences with high complexity and the low complexity subsequences, respectively. Finally, the predicted values of the models are superimposed to get the final values. In order to verify the validity of the proposed model, the final predictions, compared with six different prediction models, show that this model can achieve the best performance and obtain higher prediction accuracy. Such as the performance evaluation indexes (RMSE = 0.051, MAPE = 0.929%) are smallest obtained from dataset1 by one-step prediction, and (RMSE = 0.086, MAPE = 0.966%) are smallest obtained from dataset2 by one-step prediction. In addition, the Pearson correlation between the predicted value and the true wind speed value obtained by the prediction model applied to the two data sets is the highest 99.17% and 98.73%, respectively.}
}
@incollection{LI2022349,
title = {Chapter 12 - Blockchain-enabled product lifecycle management},
editor = {Dimitris Mourtzis},
booktitle = {Design and Operation of Production Networks for Mass Personalization in the Era of Cloud Technology},
publisher = {Elsevier},
pages = {349-379},
year = {2022},
isbn = {978-0-12-823657-4},
doi = {https://doi.org/10.1016/B978-0-12-823657-4.00013-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128236574000130},
author = {Zhi Li and Zonggui Tian and Lihui Wang and Ray Y. Zhong},
keywords = {Industry 4.0, Fourth industrial revolution, Product lifecycle management, Industrial blockchain, Mass customization},
abstract = {The rapid advances of information technology have pushed society into the fourth industrial revolution (I4.0). It requires an integration of value chain organization and management across the product lifecycle driven by the Internet industrialization, industrial digitalization and intellectualization, and industrial integration. PLM is the business activity of managing a company’s products from the idea of product development to the end of product life. It aims to seamlessly manage all products, information, and knowledge generated throughout the product’s lifecycle in order to achieve business competitiveness. Blockchain is an emerging technology that brings transparency to opaque systems, verifiability, and immutability to transactions and processes, which gives it the potential to drive I4.0 revolution. This chapter will demonstrate the real-world applicability of blockchain potential using existing industrial case studies. Besides, an industrial blockchain-based PLM framework is introduced to facilitate data exchange and service sharing in the product lifecycle.}
}
@article{KIM2022114868,
title = {Is walking or riding your bike when a tourist different? Applying VAB theory to better understand active transport behavior},
journal = {Journal of Environmental Management},
volume = {311},
pages = {114868},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.114868},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722004418},
author = {Myung Ja Kim and C. Michael Hall},
keywords = {Sustainability, Sustainable transport, Walking, Cycling, Value-attitude-behavior (VAB) theory, Sustainable tourism},
abstract = {Active transport (walking and biking) has significant environmental, health, and social benefits. Despite the importance of active transport, theoretically framed research has not sufficiently considered what makes consumers walk or bike based on activity types, particularly in an Asian context. This is an important topic as it helps provides a basis for better targeted marketing and promotion to encourage greater public engagement with active transport. To fill this knowledge gap, this work applied the value-attitude-behavior (VAB) theory to understand walkers and bikers’ behaviors in comparing tourism, leisure, and work activity. Results indicate that value on attitude has the greatest influence, followed by personal, and then social norm. Behavior for active transport is significantly influenced by personal norm, followed by attitude and social norm. Interestingly, from the three types of activities, the tourism group has the strongest relationship of value and attitude and the highest prediction for attitude and behavior.}
}
@article{WANG2022102093,
title = {Weakly supervised deep learning for prediction of treatment effectiveness on ovarian cancer from histopathology images},
journal = {Computerized Medical Imaging and Graphics},
volume = {99},
pages = {102093},
year = {2022},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2022.102093},
url = {https://www.sciencedirect.com/science/article/pii/S0895611122000660},
author = {Ching-Wei Wang and Cheng-Chang Chang and Yu-Ching Lee and Yi-Jia Lin and Shih-Chang Lo and Po-Chao Hsu and Yi-An Liou and Chih-Hung Wang and Tai-Kuang Chao},
keywords = {Epithelial ovarian cancer, Precision Oncology, Weakly supervised deep learningframework, Whole-slide image analysis, Histopathological Images},
abstract = {Despite the progress made during the last two decades in the surgery and chemotherapy of ovarian cancer, more than 70 % of advanced patients are with recurrent cancer and decease. Surgical debulking of tumors following chemotherapy is the conventional treatment for advanced carcinoma, but patients with such treatment remain at great risk for recurrence and developing drug resistance, and only about 30 % of the women affected will be cured. Bevacizumab is a humanized monoclonal antibody, which blocks VEGF signaling in cancer, inhibits angiogenesis and causes tumor shrinkage, and has been recently approved by FDA as a monotherapy for advanced ovarian cancer in combination with chemotherapy. Considering the cost, potential toxicity, and finding that only a portion of patients will benefit from these drugs, the identification of new predictive method for the treatment of ovarian cancer remains an urgent unmet medical need. In this study, we develop weakly supervised deep learning approaches to accurately predict therapeutic effect for bevacizumab of ovarian cancer patients from histopathological hematoxylin and eosin stained whole slide images, without any pathologist-provided locally annotated regions. To the authors’ best knowledge, this is the first model demonstrated to be effective for prediction of the therapeutic effect of patients with epithelial ovarian cancer to bevacizumab. Quantitative evaluation of a whole section dataset shows that the proposed method achieves high accuracy, 0.882 ± 0.06; precision, 0.921 ± 0.04, recall, 0.912 ± 0.03; F-measure, 0.917 ± 0.07 using 5-fold cross validation and outperforms two state-of-the art deep learning approaches Coudray et al. (2018), Campanella et al. (2019). For an independent TMA testing set, the three proposed methods obtain promising results with high recall (sensitivity) 0.946, 0.893 and 0.964, respectively. The results suggest that the proposed method could be useful for guiding treatment by assisting in filtering out patients without positive therapeutic response to suffer from further treatments while keeping patients with positive response in the treatment process. Furthermore, according to the statistical analysis of the Cox Proportional Hazards Model, patients who were predicted to be invalid by the proposed model had a very high risk of cancer recurrence (hazard ratio = 13.727) than patients predicted to be effective with statistical signifcance (p < 0.05).}
}
@article{FU2022112560,
title = {Identifying residential building occupancy profiles with demographic characteristics: using a national time use survey data},
journal = {Energy and Buildings},
volume = {277},
pages = {112560},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112560},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822007319},
author = {Jiasha Fu and Shan Hu and Xin He and Shunsuke Managi and Da Yan},
keywords = {Occupant behavior, Occupancy pattern, Household survey, Time of use, Building energy efficiency},
abstract = {Occupancy patterns in residential buildings and profiles with family information are essential for the energy system design and optimization, for the electricity demand responses and management of buildings, and for constructing household energy efficiency policies. To address the shortcomings of current research, this study conducted a national-wide survey in China with a valid sample of 8,190 urban households representing 19,352 individuals. On this basis, a novel approach was generated and conducted to identify the occupancy patterns in residential buildings and describe familiy profiles of typical occupancy pattern. Results indicate that occupied duration for urban residential building is 20.3 h per day during weekdays and 20.1 h per day during weekend. Four occupancy patterns during workday and five occupancy patterns during weekend were identified. Family demographic characteristics, housing characteristics, geographical location, education experience, work experience, and income level are revealed to be correlated with residents’ occupancy pattern. These results could be widely used in building-level energy performance simulation and optimization, and community-level energy and emission policy tools design.}
}
@article{NASSEF2022108820,
title = {A survey: Distributed Machine Learning for 5G and beyond},
journal = {Computer Networks},
volume = {207},
pages = {108820},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.108820},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622000421},
author = {Omar Nassef and Wenting Sun and Hakimeh Purmehdi and Mallik Tatipamula and Toktam Mahmoodi},
keywords = {Machine Learning, Distributed machine learning, Distributed inference, Latency, 5G networks},
abstract = {5G is the fifth generation of cellular networks. It enables billions of connected devices to gather and share information in real time; a key facilitator in Industrial Internet of Things (IoT) applications. It has more capabilities in terms of bandwidth, latency/delay, processing powers and flexibility to utilize either edge or cloud resources. Furthermore, 6G is expected to be equipped with the new capability to converge ubiquitous communication, computation, sensing and controlling for a variety of sectors, which heightens the complexity in a more heterogeneous environment This increased complexity, combined with energy efficiency and Service Level Agreement (SLA) requirements makes application of Machine Learning (ML) and distributed ML necessary. A decentralized approach stemming from distributed learning is a very attractive option compared with a centralized architecture for model learning and inference. Distributed ML exploits recent Artificial Intelligence (AI) technology advancements to allow collaborated ML, whilst safeguarding private data, minimizing both communication and computation overhead along with addressing ultra-low latency requirements. In this paper, we review a number of distributed ML architectures and designs, that focus on optimizing communication, computation and resource distribution. Privacy, information security and compute frameworks, are also analyzed and compared with respect to different distributed ML approaches. We summarize the major contributions and trends in this area and highlight the potential of distributed ML to help researchers and practitioners make informed decisions on selecting the right ML approach for 5G and Beyond related AI applications. To enable distributed ML for 5G and Beyond, communication, security, and computing platform often counter balance each other, thus, consideration and optimization of these aspects at an overall system level is crucial to realize the full potential of AI for 5G and Beyond. These different aspects do not only pertain to 5G, but will also enable careful design of distributed machine learning architectures to circumvent the same hurdles that will inevitably burden 5G and Beyond network generations. This is the first survey paper that brings together all these aspects for distributed ML.}
}