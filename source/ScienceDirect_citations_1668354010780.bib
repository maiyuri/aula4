@incollection{GRIMALDI2022111,
title = {Chapter 4 - Roadmap to develop a data-driven city},
editor = {Didier Grimaldi and Carlos Carrasco-Farré},
booktitle = {Implementing Data-Driven Strategies in Smart Cities},
publisher = {Elsevier},
pages = {111-151},
year = {2022},
isbn = {978-0-12-821122-9},
doi = {https://doi.org/10.1016/B978-0-12-821122-9.00006-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211229000063},
author = {Didier Grimaldi and Jose M. Sallan and Josep Miquel {Piqué Huerta} and Jesús Soler Puebla and Kristi Shalla and Carlos Carrasco-Farré},
keywords = {Roadmap, Data-driven city, Smart city, Inception, Maturity, Smart index},
abstract = {Cities play a relevant role in economic and social development. According to data from the United Nations, 55% of the global human population lives in cities, amounting to 4.2 billion people. From the same source, we learn that this fraction amounts to 78.5% in more developed countries. On the other hand, cities are estimated to consume 2% of land, generate around 70% of global GDP, and use 60% of global energy. As much of the growth of urban population is coming from less developed cities, it is safe to predict that the weight of cities in the global social landscape will increase in the coming years.}
}
@article{MEYER20221992,
title = {On the Importance of Data for Supply Chain Network Design Projects},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1992-1997},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.691},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322020092},
author = {Christoph Manuel Meyer},
keywords = {Network design, supply chain, supply chain management project, project management, project success, data excellence, data availability, data quality},
abstract = {This paper investigates success factors for supply chain network design projects in practice. A structural equation model of project success is developed, in which project management excellence and supply chain data excellence are utilized as drivers of project success. Using data from an online survey, the model is validated with a partial least squares structural equation modeling approach. The main contributions of this study are as follows: First, it presents an approach for solving network design problems in practice. Second, it proves that while being secondary to project management excellence, the availability and quality of supply chain data is of high importance for this special type of supply chain management project.}
}
@article{WANG2022123178,
title = {Lithium-ion battery state-of-charge estimation for small target sample sets using the improved GRU-based transfer learning},
journal = {Energy},
volume = {244},
pages = {123178},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.123178},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222000810},
author = {Ya-Xiong Wang and Zhenhang Chen and Wei Zhang},
keywords = {State-of-charge (SOC), Open-source battery datasets, Deep learning, Gated recurrent unit (GRU), Source domain model, Transfer learning},
abstract = {Accurate estimation of the state-of-charge (SOC) of lithium-ion batteries is a key technique for automotive battery management systems to overcome the non-linearity and complications of practical applications. The data-driven approach for estimating SOC requires a large number of training samples and costly input. To this end, an improved gated recurrent unit (GRU)-based transfer learning SOC estimation is proposed for small target sample sets. To ensure the completeness and consistency of data features, Lagrangian interpolations and standard normalization are used for analyzing the open-source battery datasets. The source domain GRU model is pre-trained to obtain rich battery characteristics with the preprocessed datasets; the GRU hidden unit structure can be enhanced, and it is advantageously used in conjunction with transfer learning. Moreover, weight parameters of the source domain are transferred to the GRU model of target batteries. The experimental results show that the proposed improved GRU-based transfer learning can use small target samples to achieve fast and accurate SOC estimations by ordinary computing hardware. In particular, the RMSEs are 1.115%, 1.867%, and 1.141% under dynamic conditions, 32 °C-FUDS, 36 °C-US06, and 50 °C-UDDS, respectively. The proposed method demonstrates the potential of SOC estimation using small target samples-based big data techniques in practice.}
}
@article{MORKNER2022104945,
title = {Distilling data to drive carbon storage insights},
journal = {Computers & Geosciences},
volume = {158},
pages = {104945},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.104945},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421002314},
author = {Paige Morkner and Jennifer Bauer and C. Gabriel Creason and Michael Sabbatino and Patrick Wingo and Randall Greenburg and Samuel Walker and David Yeates and Kelly Rose},
keywords = {Geologic carbon sequestration, Energy data exchange, Natural language processing, FAIR (Findable, Accessible, Interoperable, Reusable), Data availability, Spatial data density analysis},
abstract = {Wide-spread implementation of carbon capture and storage has the potential to decrease carbon emissions and aid in meeting global climate change mitigation goals. Data availability is one of the biggest challenges faced by the carbon capture and storage (CCS) community for modeling risks associated with CCS, necessary for wide-spread implementation in coming years. Collecting, integrating, and intuitively managing data is a time-consuming process, but one which is fundamental to establishing necessary access to carbon storage data. The US Department of Energy (US DOE) has been a major supporter of energy research in the US, including significant investment into carbon capture and storage research and technology development over the last ten years. The US DOE investments into the Regional Carbon Sequestration Partnerships, the National Risk Assessment Partnership, and other CCS related research has resulted in a large volume of data, of which much has been made public through the National Energy Technology Laboratories data repository, the Energy Data eXchange (EDX). Researchers at the National Energy Technology Laboratory have developed workflows, tools, and other methods that leverage EDX, open-source software, machine learning, and natural language processing to discover, curate, label, organize and visualize available data. This paper describes the available data on EDX for carbon storage applications, describes the results of a spatial and temporal analysis of the data, describes where it is most geographically available, makes a general assessment of the quality of the available data, and discusses visualization tools and natural language processing tools developed for understanding, discovering and reusing the data.}
}
@article{ZACHLOD20221064,
title = {Analytics of social media data – State of characteristics and application},
journal = {Journal of Business Research},
volume = {144},
pages = {1064-1076},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322001321},
author = {Cécile Zachlod and Olga Samuel and Andrea Ochsner and Sarah Werthmüller},
keywords = {Social media analytics, Social media analysis, Social media data, Social media monitoring, Social media listening, Literature review},
abstract = {The spread and use of social networks provide a rich data source that can be used to answer a wide range of research questions from various disciplines. However, the nature of social media data poses a challenge to the analysis. The aim of this study is to provide an in-depth overview of the research that analyzes social media data since 2017. An extensive literature review based on 94 papers led to the findings that clear definitions are neither established nor commonly applied. Predominant research domains include marketing, hospitality and tourism, disaster management, and disruptive technology. The majority of analyzed social media data are taken from Twitter. Sentiment and content analysis are the current prevailing methods. Half of the studies include practical implications. Based on the literature review, clear definitions are provided, and future avenues for high-quality research are suggested.}
}
@article{ZHANG2022103642,
title = {Data Matters: A Strategic Action Framework for Data Governance},
journal = {Information & Management},
volume = {59},
number = {4},
pages = {103642},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103642},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000544},
author = {Qingqiang Zhang and Xinbo Sun and Mingchao Zhang},
keywords = {Data, Data governance, Digitalization, Digital transformation, Strategic action, Framework},
abstract = {While there has been a wealth of research exploring data governance, there are still some gaps in how firms deploy data governance and what strategic actions they take to do so, especially as the volume of data increases dramatically and the pace of data assetization accelerates. To achieve this end, through an in-depth case study of a Chinese gold mining company, namely Shandong Gold, we develop a framework to explain how firms configure data governance activities and conduct related strategic actions. Our study identifies four key data governance activities that are supported by two strategic actions. Overall, we contribute to research in data governance and strategic action fields and also provide an alternative implementation framework for practitioners.}
}
@article{KONG2022110352,
title = {Data-driven EUR modeling and optimization in the liquid-rich Duvernay Formation, western Canada sedimentary basin, Canada},
journal = {Journal of Petroleum Science and Engineering},
volume = {213},
pages = {110352},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2022.110352},
url = {https://www.sciencedirect.com/science/article/pii/S092041052200239X},
author = {Bing Kong and Zhuoheng Chen},
keywords = {EUR, Machine learning, Shapley value, Stacked model},
abstract = {Estimated Ultimate Recovery (EUR) is one of the focuses of the feasibility assessment for oil and gas development projects. EUR is the utmost recoverable oil and gas volume under the current assumption of technology and economics. Many factors including geology, drilling, completion, operation, and commodity prices influence EUR, which makes the prediction a difficult task. Reservoir numerical simulation and production decline curve analysis (DCA) are two broadly accepted method to calculate EUR. However, the former requires substantial data and resources, while the latter is lack of causative mechanism to associate the fundamentals to productivity. This study proposes a machine learning (ML) procedure in EUR modeling, by which EUR is linked to fundamental variables from available data and the variation in EUR can be explained by various factors so that the results can be applied to optimize future projects. In the proposed procedure, the EUR was estimated using a probabilistic dual flow regime model and Markov Chain Monte Carlo (MCMC) simulation. The resulting EUR in each well was then modeled using a two-level stacked ensemble ML approach, while Shapley value was used to explain feature sensitivity in the trained model. In the last, the EUR is optimized by adjusting the most sensitive factors in the trained model. The trained ML model achieved high accuracy on EUR prediction, and the Shapley value analysis showed that completion length, condensate gas ratio and fracturing fluid volume are among the most important features to EUR. The EUR optimization result showed that there is large room for improvement by adjusting the key features. This proposed approach provides a new perspective to find associations between the fundamental factors and the well EUR which improves the understanding of oil and gas production in unconventional reservoirs.}
}
@article{STVILIA2022101160,
title = {Seeking and sharing datasets in an online community of data enthusiasts},
journal = {Library & Information Science Research},
volume = {44},
number = {3},
pages = {101160},
year = {2022},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2022.101160},
url = {https://www.sciencedirect.com/science/article/pii/S0740818822000238},
author = {Besiki Stvilia and Leila Gibradze},
keywords = {Dataset practices, Data curation, Online community, Metadata, Data seeking, Data sharing},
abstract = {This study examined discussions of the r/Datasets community on Reddit. It identified three activities in which the community engaged: question answering, data sharing, and community building. Members of the community used 21 types of data and information sources in their activities. The findings of this research enhance our understanding of the activity structures, data and information sources used, and challenges and problems encountered when users search for, share, and make sense of datasets on the web, outside the traditional information and data ecosystems. Data librarians and curators can use the findings of this study in the design of their data management and reference services. The typology of data sources and the metadata model developed through this study can be used in annotating and categorizing data sources and informing the design of metadata schemas and vocabularies for datasets.}
}
@article{KHAN2022278,
title = {Handling missing data through deep convolutional neural network},
journal = {Information Sciences},
volume = {595},
pages = {278-293},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.02.051},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522001931},
author = {Hufsa Khan and Xizhao Wang and Han Liu},
keywords = {Missing value, Data imputation, Fuzzy clustering, Convolutional neural network},
abstract = {The presence of missing data is a challenging issue in processing real-world datasets. It is necessary to improve the data quality by imputing the missing values so that effective learning from data can be achieved. Recently, deep learning has become the most powerful type of machine learning techniques, which can be used for discovering the hidden knowledge that exists in a large dataset to make accurate predictions. In this paper, we propose an imputation method that involves using a convolutional neural network to impute the missing values. The missing value of each instance is imputed essentially by using a trained kernel. The weights of the kernel are determined by learning from the given data that are arranged spatially in the data matrix. The kernel carries out a weighted sum of neighboring elements in an array for imputing the missing values. In addition, in the absence of the true values with which the missing values are expected to be replaced, a loss function is designed without the need to know the true value. Our method is evaluated on UCI datasets in comparison with state-of-the-art methods. The experimental results show that the proposed approach performs closely to or better than other methods.}
}
@article{KAMAL2022108562,
title = {Super-encoder with cooperative autoencoder networks},
journal = {Pattern Recognition},
volume = {126},
pages = {108562},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108562},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322000437},
author = {Imam Mustafa Kamal and Hyerim Bae},
keywords = {Autoencoder, Dimensionality reduction, Feature extraction, Pattern recognition, Cooperative neural networks},
abstract = {Dimensionality reduction plays a crucial role in classification, object detection, and pattern recognition tasks. Its main objective is to decrease the dimension of the original data while retaining the most distinctive information. With the emergence of deep learning, an autoencoder has become a state-of-the-art non-linear dimensionality-reduction method. Nonetheless, as the existing autoencoder models are devised to follow the data distribution and employ similarity techniques, preserving distinctive information can be problematic. To tackle this issue, we propose super-encoder (SE) networks trained in a supervised and cooperative manner. The SE consists of an encoder, separator, and decoder networks. The encoder combined with separator networks are dedicated to generating separable latent representation based on the label, and the decoder network should be able to reconstruct it to the original data simultaneously. Herein, we introduce a novel cooperative learning mechanism with a new loss function; therefore, the encoder, separator, and decoder networks can cooperate to achieve these objectives. Extensive experiments using benchmark datasets were conducted. The results indicated that the SE is more effective in extracting separable latent code than the existing supervised and unsupervised dimensionality-reduction models. Furthermore, as a generator, it can obtain highly competitive realistic images.}
}
@incollection{YU2022177,
title = {Chapter 6 - Data-driven estimation for urban travel shareability},
editor = {Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
booktitle = {Big Data and Mobility as a Service},
publisher = {Elsevier},
pages = {177-202},
year = {2022},
isbn = {978-0-323-90169-7},
doi = {https://doi.org/10.1016/B978-0-323-90169-7.00007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901697000075},
author = {Qing Yu and Weifeng Li and Dongyuan Yang},
keywords = {Urban travel shareability, Geospatial big data, Agent-based model, Human mobility, Bicycle-sharing, Ride-sharing},
abstract = {The potential analysis of urban travel shareability is a fast-growing research topic, but it is still in its infancy. With the support of pervasively collected geospatial data generated by individuals, this chapter establishes the potential of the Agent-Based Model (ABM) towards enhancing sharing transportation modeling, data-driven estimation of shareability, and policy making. Our analysis of existing literature demonstrates that big geospatial data-driven ABM has had little mention to date within the application of urban travel shareability estimation. Therefore, we summarized the key technologies, the existing application, future opportunities, and challenges of ABM in travel shareability evaluation. As a potential application of ABM, this chapter also introduces an example of estimating the shareability of bicycle sharing in the case of dynamic electric fence planning. This is of significant importance due to the growing awareness of a need to make decisions based on the monitoring, simulation, and evaluation of shared transportation to deliver the next-generation smart city development.}
}
@article{USHIZIMA2022118790,
title = {Deep learning for Alzheimer's disease: Mapping large-scale histological tau protein for neuroimaging biomarker validation},
journal = {NeuroImage},
volume = {248},
pages = {118790},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118790},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921010570},
author = {Daniela Ushizima and Yuheng Chen and Maryana Alegro and Dulce Ovando and Rana Eser and WingHung Lee and Kinson Poon and Anubhav Shankar and Namrata Kantamneni and Shruti Satrawada and Edson Amaro Junior and Helmut Heinsen and Duygu Tosun and Lea T. Grinberg},
keywords = {Machine learning, Deep learning, Convolutional neural networks, Alzheimer's disease, Histopathology, Digital pathology, Big data, Imaging},
abstract = {Abnormal tau inclusions are hallmarks of Alzheimer's disease and predictors of clinical decline. Several tau PET tracers are available for neurodegenerative disease research, opening avenues for molecular diagnosis in vivo. However, few have been approved for clinical use. Understanding the neurobiological basis of PET signal validation remains problematic because it requires a large-scale, voxel-to-voxel correlation between PET and (immuno) histological signals. Large dimensionality of whole human brains, tissue deformation impacting co-registration, and computing requirements to process terabytes of information preclude proper validation. We developed a computational pipeline to identify and segment particles of interest in billion-pixel digital pathology images to generate quantitative, 3D density maps. The proposed convolutional neural network for immunohistochemistry samples, IHCNet, is at the pipeline's core. We have successfully processed and immunostained over 500 slides from two whole human brains with three phospho-tau antibodies (AT100, AT8, and MC1), spanning several terabytes of images. Our artificial neural network estimated tau inclusion from brain images, which performs with ROC AUC of 0.87, 0.85, and 0.91 for AT100, AT8, and MC1, respectively. Introspection studies further assessed the ability of our trained model to learn tau-related features. We present an end-to-end pipeline to create terabytes-large 3D tau inclusion density maps co-registered to MRI as a means to facilitate validation of PET tracers.}
}
@article{CHEN2022109070,
title = {Leaf chlorophyll contents dominates the seasonal dynamics of SIF/GPP ratio: Evidence from continuous measurements in a maize field},
journal = {Agricultural and Forest Meteorology},
volume = {323},
pages = {109070},
year = {2022},
issn = {0168-1923},
doi = {https://doi.org/10.1016/j.agrformet.2022.109070},
url = {https://www.sciencedirect.com/science/article/pii/S0168192322002581},
author = {Ruonan Chen and Liangyun Liu and Xinjie Liu},
keywords = {SIF, GPP, Physiological property, Seasonal variation},
abstract = {The coupling of solar-induced chlorophyll fluorescence (SIF) and gross primary production (GPP) is the foundation of SIF-based GPP estimations; however, the relationship between them varies in different conditions. Structural changes contribute much to the dynamics of their relationship at the canopy scale, whereas the role of physiological mechanisms is not very clear. Here, based on three-year continuous observations from 2018 to 2020 in a maize field in Northwest China, we obtained the total SIF (tSIF) at the photosystem scale and investigated the seasonal dynamics of its link with GPP. Using the ratio of tSIF to GPP, we eliminated the contribution of canopy structure and discovered an increase in the ratio during the late reproductive and ripening stages. Seasonal variation in the ratio was tracked by the leaf chlorophyll contents (LCC) related to the photosynthetic functional maturity (represented by maximum carboxylation rate, Vcmax). In addition, we also found that there was variation in the regression slope of the relationship between SIF/GPP and LCC at different growth stages. The correlation between tSIF/GPP and LCC was better than that between dSIF/GPP (dSIF, the ratio of directional SIF at canopy scale to GPP) and LCC, which demonstrated that the physiological information is reinforced after the elimination of structural contributions. Overall, the seasonal dynamics of the GPP–tSIF relationship in our study highlighted the necessity of considering the growing stage in SIF-based GPP estimations. Although they are usually covered up by the contribution of the canopy structure, physiological mechanisms still impacted the dynamics of the GPP–SIF relationship.}
}
@article{CRONIN2022100213,
title = {A review of in silico toxicology approaches to support the safety assessment of cosmetics-related materials},
journal = {Computational Toxicology},
volume = {21},
pages = {100213},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2022.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2468111322000019},
author = {Mark T.D. Cronin and Steven J. Enoch and Judith C. Madden and James F. Rathman and Andrea-Nicole Richarz and Chihae Yang},
keywords = {Cosmetics, Risk assessment, , Computational, Read-across, Quantitative structure-activity relationship},
abstract = {In silico tools and resources are now used commonly in toxicology and to support the “Next Generation Risk Assessment” (NGRA) of cosmetics ingredients or materials. This review provides an overview of the approaches that are applied to assess the exposure and hazard of a cosmetic ingredient. For both hazard and exposure, databases of existing information are used routinely. In addition, for exposure, in silico approaches include the use of rules of thumb for systemic bioavailability as well as physiologically-based kinetics (PBK) and multi-scale models for estimating internal exposure at the organ or tissue level. (Internal) Thresholds of Toxicological Concern are applicable for the safety assessment of ingredients at low concentrations. The use of structural rules, (Quantitative) Structure-Activity Relationships ((Q)SARs) and read-across are the most typically applied modelling approaches to predict hazard. Data from exposure and hazard assessment are increasingly being brought together in NGRA to provide an overall assessment of the safety of a cosmetic ingredient. All in silico approaches are reviewed in terms of their maturity and robustness for use.}
}
@article{LNENICKA2022103906,
title = {Transparency of open data ecosystems in smart cities: Definition and assessment of the maturity of transparency in 22 smart cities},
journal = {Sustainable Cities and Society},
volume = {82},
pages = {103906},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103906},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722002281},
author = {Martin Lnenicka and Anastasija Nikiforova and Mariusz Luterek and Otmane Azeroual and Dandison Ukpabi and Visvaldis Valtenbergs and Renata Machova},
keywords = {Open data, Smart city, Transparency, Maturity, Ecosystem, Expert assessment},
abstract = {This paper focuses on the issue of the transparency maturity of open data ecosystems seen as the key for the development and maintenance of sustainable, citizen-centered, and socially resilient smart cities. This study inspects smart cities’ data portals and assesses their compliance with transparency requirements for open (government) data. The expert assessment of 34 portals representing 22 smart cities, with 36 features, allowed us to rank them and determine their level of transparency maturity according to four predefined levels of maturity - developing, defined, managed, and integrated. In addition, recommendations for identifying and improving the current maturity level and specific features have been provided. An open data ecosystem in the smart city context has been conceptualized, and its key components were determined. Our definition considers the components of the data-centric and data-driven infrastructure using the systems theory approach. We have defined five predominant types of current open data ecosystems based on prevailing data infrastructure components. The results of this study should contribute to the improvement of current data ecosystems and build sustainable, transparent, citizen-centered, and socially resilient open data-driven smart cities.}
}
@article{SARNTHEIN2022100860,
title = {Neurosurgery outcomes and complications in a monocentric 7-year patient registry},
journal = {Brain and Spine},
volume = {2},
pages = {100860},
year = {2022},
issn = {2772-5294},
doi = {https://doi.org/10.1016/j.bas.2022.100860},
url = {https://www.sciencedirect.com/science/article/pii/S2772529422000017},
author = {Johannes Sarnthein and Victor E. Staartjes and Luca Regli and Kevin Akeret and Delal Bektas and David Bellut and Oliver Bichsel and Oliver Bozinov and Elisa Colombo and Sandra Dias and Giuseppe Esposito and Menno R. Germans and Anna-Sophie Hofer and Michael Hugelshofer and Arian Karbe and Niklaus Krayenbühl and Alexander Küffer and Marian C. Neidert and Markus F. Oertel and Luis Padevit and Luca Regli and Jonas Rohr and Ahmed Samma and Johannes Sarnthein and Martina Sebök and Carlo Serra and Victor Staartjes and Lennart Stieglitz and Martin N. Stienen and Lazar Tosic and Tristan {van Doormaal} and Bas {van Niftrik} and Flavio Vasella and Stefanos Voglis and Fabio {von Faber-Castell}},
keywords = {Adverse events, Morbidity and mortality rounds, Quality monitoring},
abstract = {Introduction
Capturing adverse events reliably is paramount for clinical practice and research alike. In the era of “big data”, prospective registries form the basis of clinical research and quality improvement.
Research question
To present results of long-term implementation of a prospective patient registry, and evaluate the validity of the Clavien-Dindo grade (CDG) to classify complications in neurosurgery.
Materials and methods
A prospective registry for cranial and spinal neurosurgical procedures was implemented in 2013. The CDG – a complication grading focused on need for unplanned therapeutic intervention – was used to grade complications. We assess construct validity of the CDG.
Results
Data acquisition integrated into our hospital workflow permitted to include all eligible patients into the registry. We have registered 8226 patients that were treated in 11994 surgeries and 32494 consultations up until December 2020. Similarly, we have captured 1245 complications on 6308 patient discharge forms (20%) since full operational status of the registry. The majority of complications (819/6308 ​= ​13%) were treated without invasive treatment (CDG 1 or CDG 2). At discharge, there was a clear correlation of CDG and the Karnofsky Performance Status (KPS, rho ​= ​-0.29, slope -7 KPS percentage points per increment of CDG) and the length of stay (rho ​= ​0.43, slope 3.2 days per increment of CDG).
Discussion and conclusion
Patient registries with high completeness and objective capturing of complications are central to the process of quality improvement. The CDG demonstrates construct validity as a measure of complication classification in a neurosurgical patient population.}
}
@article{ZHANG2022111697,
title = {Motor current signal analysis using hypergraph neural networks for fault diagnosis of electromechanical system},
journal = {Measurement},
volume = {201},
pages = {111697},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111697},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122009058},
author = {Kongliang Zhang and Hongkun Li and Shunxin Cao and Chen Yang and Fubiao Sun and Zibo Wang},
keywords = {Deep learning, Time shifting, Hypergraph neural networks, Motor current signal analysis, Electromechanical system fault diagnosis},
abstract = {Graph based networks are becoming an emerging trend in the field of fault diagnosis because of their powerful ability to mine the interrelationships between nodes. However, the existing graph-based networks are limited to mining the association relationship between adjacent nodes, which cannot reflect the strong association relationship between multiple nodes and thus affect the graph data quality. To solve these problems, a time-shifting based hypergraph neural network (TS-HGNN) is proposed for the accurate classification of fault types in electromechanical coupled systems. First, the time shifting method is applied to pre-process the original current signal to remove the power-line interference. Then, a hypergraph structure applicable to current signal is established to form complex interrelationships and a hyperedge convolution operation is designed to obtain the interrelationships of higher-order data for representation learning. Finally, several datasets are designed to verify the superiority and robustness of TS-HGNN in current signal fault classification.}
}
@incollection{BATTINENI2022265,
title = {Chapter 20 - Opportunities and challenges in healthcare with the management of big biomedical data},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {265-275},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00017-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000170},
author = {Gopi Battineni},
keywords = {Big data challenges, Biomedical data, EHRs, Healthcare security, Medical imaging},
abstract = {It is a challenge to develop supervised models in clinical evaluation. Although the profundities of this challenge are frequently learned at that point, they failed to remember or willfully overlooked. This should be the situation, since dwelling too long on this limitation may bring about a skeptical viewpoint. Disregarding this challenge, we keep on using supervised model learning algorithms, and they perform better in practice. The timely translation of the machine learning model in clinical practice needs to be validated and accurately measured and the models with benefits for everyone are a big challenge. Number of factors, including the size of biomedical datasets and model fitting issues, with unbalanced datasets create unnecessary biases in medical practice. Therefore, robust clinical evaluation using simple datasets helps in simple model learning and also understands how much data is precisely needed to do performance calculations. In this chapter, the author explores the opportunities and medical challenges associated with large datasets in deep learning, including data streaming, model scalability, and distributed computing.}
}
@article{ZHAO202256,
title = {Research on the evolution of the innovation ecosystem of the Internet of Things: A case study of Xiaomi(China)},
journal = {Procedia Computer Science},
volume = {199},
pages = {56-62},
year = {2022},
note = {The 8th International Conference on Information Technology and Quantitative Management (ITQM 2020 & 2021): Developing Global Digital Economy after COVID-19},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922000084},
author = {Wu Zhao and Lei Yi},
keywords = {IoT ecosystem, Business system, Hardware intelligent ecological chain, Bamboo forest ecology, Case studies},
abstract = {With the support of industry trends and policies, the Internet of Things is seen as another revolution in the world’s information technology industry. Many companies hope to establish a huge Internet of Things ecosystem through the business ecosystem to achieve rapid expansion and growth. This research selected Xiaomi, the world’s largest consumer-level IoT platform, and tried to explore the growth and development mechanism of its IoT innovation ecosystem by analyzing the development logic of Xiaomi enterprises. The research conclusions show that the Xiaomi IoT innovation ecosystem has evolved through four stages of development, expansion, layout, and maturity. Through the four paths of building market entry, investing in incubating niche products, establishing a bamboo forest ecology, and building an IoT open platform, the ecological map of the Xiaomi IoT ecosystem is finally formed. This research aims to provide guidance and suggestions on business models, evolution strategies, and dynamic capabilities for other companies through path analysis of typical cases.}
}
@incollection{SEBASTIANCOLEMAN20221,
title = {Section 1 - Data in Today’s Organizations},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {1-2},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00021-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000213},
author = {Laura Sebastian-Coleman}
}
@article{FONTES2022102137,
title = {AI-powered public surveillance systems: why we (might) need them and how we want them},
journal = {Technology in Society},
volume = {71},
pages = {102137},
year = {2022},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2022.102137},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X22002780},
author = {Catarina Fontes and Ellen Hohma and Caitlin C. Corrigan and Christoph Lütge},
keywords = {AI, Surveillance, Dataveillance, AI governance},
abstract = {In this article, we address the introduction of AI-powered surveillance systems in our society by looking at the deployment of real-time facial recognition technologies (FRT) in public spaces and public health surveillance technologies, in particular contact tracing applications. Both cases of surveillance technologies assist public authorities in the enforcement of the law by allowing the tracking of individual movements and extrapolating results towards monitoring and predicting social behavior. Therefore, they are considered as potentially useful tools in response to societal crises, such as those generated by crime and health related pandemics. To approach the assessment of the potentials and threats of such tools, we offer a framework with three dimensions. A function dimension, examines the type, quality and quantity of data the system needs to employ to work effectively.The consent dimension considers the user's right to be informed about and reject the use of surveillance, questioning whether consent is achievable and whether the user can decide fully autonomously/independently. Finally, a societal dimension that frames vulnerabilities and the impacts of the increased empowerment of established political regimes through new means to control populations based on data surveillance. Our analysis framework can assist public authorities in their decisions on how to design and deploy public surveillance tools in a way that enables compliance with the law while highlighting individual and societal tradeoffs.}
}
@article{COLANGELO2022493,
title = {Maturity Model for AI in Smart Production Planning and Control System},
journal = {Procedia CIRP},
volume = {107},
pages = {493-498},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002980},
author = {Eduardo Colangelo and Christian Fries and Theresa-Franziska Hinrichsen and Ádám Szaller and Gábor Nick},
keywords = {Production Planning, Control, Artificial Intelligence, Maturity Model, Smart Production},
abstract = {The utilization of Artificial Intelligence (AI) to improve processes constitutes a main subject for many enterprises. The area of Production Planning and Control (PPC) possesses several functions that could profit from such approaches. However, manufacturing companies find themselves often limited in the application of these approaches. This paper concentrates on three elements to assist enterprises: 1) the clarification of what AI is (in the manufacturing context) and its application to the field of PPC; 2) a review performed together with manufacturing enterprises in Germany and Hungary in order to understand the obstacles for the implementation of AI; and 3) the proposal of a maturity model to help enterprises understand where they are in regards to AI, as a way to help them create a roadmap to achieve their objectives.}
}
@article{WAN2022186,
title = {EEG fading data classification based on improved manifold learning with adaptive neighborhood selection},
journal = {Neurocomputing},
volume = {482},
pages = {186-196},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.039},
url = {https://www.sciencedirect.com/science/article/pii/S092523122101715X},
author = {Zitong Wan and Rui Yang and Mengjie Huang and Weibo Liu and Nianyin Zeng},
keywords = {Adaptive neighborhood selection, Data fading, Electroencephalogram, Manifold learning},
abstract = {In electroencephalogram (EEG) signal analysis, data fading problem exists from signal production to collection by brain-computer interface (BCI) device, which can be raised by BCI device deficiency, dynamic network limitation and subject issue. EEG data fading problem changes the distribution of data, which results in the movement of the cluster center and fuzzy class boundary after feature extraction with negative effects in EEG classification results. To decrease the adverse influence of data fading, a novel fading data classification method based on manifold learning and adaptive neighborhood selection is proposed in this paper to mitigate this adverse effect of data fading. In the proposed method, after neighborhood selection according to local linearity, data are mapped into manifold space through local tangent space alignment (LTSA) for dimensionality reduction. The method is carried out on BCI Competition 2008 – Graz data set A of four-class EEG data of motor imagery (MI) experiments. The experimental results are compared with conventional LTSA and indicate that the proposed method effectively improves the classification accuracy of fading data.}
}
@article{PAHREN202273,
title = {A Novel Method in Intelligent Synthetic Data Creation for Machine Learning-based Manufacturing Quality Control},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {73-78},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.186},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014008},
author = {Laura Pahren and Paul Thomas and Xiaodong Jia and Jay Lee},
keywords = {quality inspections, artificial intelligence, deep learning, resampling, manufacturing},
abstract = {This paper aims to help improve the creation of deep learning-based vision systems for consumer goods manufacturing at Procter and Gamble (P&G) using smart, realistic synthetic data creation. This synthetic data creation is based on a novel data resampling technique that utilizes ordinal class information to create hard-to-capture minority class defects, which has been appropriately name Class Ordered Minority Oversampling Technique (COSMOTE), while also minimizing the overall data collection efforts, which can be a costly and disruptive process to the plants themselves. A particular challenge to these applications is the small number of samples that may be captured for defective products, especially when changes in the process or artwork are made. By leveraging these ordinal quality classes, the information from the classes themselves can enable a minimal training dataset size for faster start to finish model development. A brief literature review of existing resampling techniques is provided to highlight the gaps in these sparse sample use cases and the workflow to generate and validate these synthetic images is also outlined. This paper explains the benefits of intelligent synthetic data creation within this particular manufacturing space by addressing both data imbalance and sparse sample datasets.}
}
@incollection{MANZELLA2022319,
title = {Chapter Six - How can ocean science observations contribute to humanity?},
editor = {Giuseppe Manzella and Antonio Novellino},
booktitle = {Ocean Science Data},
publisher = {Elsevier},
pages = {319-335},
year = {2022},
isbn = {978-0-12-823427-3},
doi = {https://doi.org/10.1016/B978-0-12-823427-3.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234273000050},
author = {Giuseppe M.R. Manzella and William Emery},
keywords = {Data management, Interdisciplinarity, Knowledge, Mutual understanding, Ocean data science, Solution-oriented education},
abstract = {Experience is needed to prepare young ocean researchers to work in interdisciplinary environments and to teach them how to deal with complex processes. Such experience can be provided in courses/internships aimed at preparing qualified personnel to work on solution-oriented projects. These lessons are designed to deepen understanding in particular elements of ocean data science education: oceanography as a science in evolution, mutual understanding, the enrichment of data, and the process of moving from data to information. Such lessons combine the history of ocean science with ocean data methodologies and technologies, data quality elements, “fitness for use”/“fitness for purpose” and analyses. The approach consists of a significant mentoring program aimed at strengthening “thinking skills”—critical and creative thinking—and therefore the ability to solve complex problems.}
}
@article{ZHANG202281,
title = {Adjoint dynamical kernel density for anomaly detection},
journal = {Neurocomputing},
volume = {499},
pages = {81-92},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222005379},
author = {Panpan Zhang and Hui Cao and Yanbin Zhang and Jingcheng Wang and Lixin Jia and Feihu Hu},
keywords = {Anomaly detection, Outlier factor, The density changes, Adjoin dynamical kernel density, Kernel function},
abstract = {Anomalies affect data quality and lead to unexpected analysis results in data mining. Although techniques to handle anomalies do exist, they can fail in considering density changes of object along with incremental neighbors. This paper proposes an outlier factor based on the change of adjoint dynamical kernel density (ADD) to represent the degree of the object being an anomaly. The factor is equal to the ratios of the adjoint dynamical kernel density fluctuation (ADDF) of the object and the average ADDF of its neighborhood. ADDF is estimated by the difference of ADD, which describes the difference of every two consecutive adjoint kernel densities (AKD) of the object. AKD indicates kernel densities of the object while its neighbors are added one by one. Importantly, the kernel function is adopted to measure the distance between objects where the kernel trick improves discriminability between objects and reduces the computational burden of the algorithm. The experiments are performed on eight datasets to evaluate the effectiveness of the proposed method with different kernel functions. The experimental results have shown that the proposed method with the Gaussian kernel function has better performance of anomalies recognition and higher adaption of the parameter k of k-nearest neighbors over some other anomaly detection methods.}
}
@article{TIAN2022119297,
title = {A deep learning-based multisite neuroimage harmonization framework established with a traveling-subject dataset},
journal = {NeuroImage},
volume = {257},
pages = {119297},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119297},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922004165},
author = {Dezheng Tian and Zilong Zeng and Xiaoyi Sun and Qiqi Tong and Huanjie Li and Hongjian He and Jia-Hong Gao and Yong He and Mingrui Xia},
keywords = {Big data, Machine learning, Multicenter, Gray matter, Convolutional network, Site effect},
abstract = {The accumulation of multisite large-sample MRI datasets collected during large brain research projects in the last decade has provided critical resources for understanding the neurobiological mechanisms underlying cognitive functions and brain disorders. However, the significant site effects observed in imaging data and their derived structural and functional features have prevented the derivation of consistent findings across multiple studies. The development of harmonization methods that can effectively eliminate complex site effects while maintaining biological characteristics in neuroimaging data has become a vital and urgent requirement for multisite imaging studies. Here, we propose a deep learning-based framework to harmonize imaging data obtained from pairs of sites, in which site factors and brain features can be disentangled and encoded. We trained the proposed framework with a publicly available traveling subject dataset from the Strategic Research Program for Brain Sciences (SRPBS) and harmonized the gray matter volume maps derived from eight source sites to a target site. The proposed framework significantly eliminated intersite differences in gray matter volumes. The embedded encoders successfully captured both the abstract textures of site factors and the concrete brain features. Moreover, the proposed framework exhibited outstanding performance relative to conventional statistical harmonization methods in terms of site effect removal, data distribution homogenization, and intrasubject similarity improvement. Finally, the proposed harmonization network provided fixable expandability, through which new sites could be linked to the target site via indirect schema without retraining the whole model. Together, the proposed method offers a powerful and interpretable deep learning-based harmonization framework for multisite neuroimaging data that can enhance reliability and reproducibility in multisite studies regarding brain development and brain disorders.}
}
@article{GO2022700,
title = {Application of data mining algorithms to study data trends for corneal transplantation},
journal = {Journal Français d'Ophtalmologie},
volume = {45},
number = {7},
pages = {700-709},
year = {2022},
issn = {0181-5512},
doi = {https://doi.org/10.1016/j.jfo.2022.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0181551222002212},
author = {J.A. Go and J. Tran and M. Khan and Z. Al-Mohtaseb},
keywords = {Cornea, Transplant, Epidemiology, Demographic, Socioeconomic, Cornée, Transplantation, Épidémiologie, Démographique, Socio-économique},
abstract = {Summary
Purpose
To utilize data mining for analysis of corneal transplantations (CT) in Florida from 2005-2014, segmented by demographics, geography, and transplantation technique.
Methods
A retrospective, database study was performed utilizing data queried from the Healthcare and Cost Utilization Project using Current Procedural Terminology codes for lamellar keratoplasty (ALK), endothelial keratoplasty (EK), and penetrating keratoplasty (PKP). Payer status, ethnic group, age, gender, and geography (urban versus rural) was extracted from each surgical encounter and reconfigured to provide a “clean”, congruous dataset for statistical analysis. This Institutional Review Board-approved study did not utilize identifiable patient information; thus, individual informed consent was not required.
Results
From 2005–2014, CT (n=28,607) represented less than 1% of the total ambulatory surgeries (n=12,695,932) performed in Florida. EK volume increased while PKP and ALK volume decreased, year-over-year. Statistical significance was found between transplantation technique by sex (P<0.001) and ethnic group (P<0.001). The largest sex discrepancy was EK (59% female, 41% male). White patients underwent relatively fewer PKP than EK (71% vs. 83% of totals), while Black patients underwent relatively more PKP than EK (14% vs 6% of totals). Statistical significance was found between techniques by payer (P<0.001). Medicare was the most common payer for all techniques, but ALK and PKP had higher percentages of private insurance and self-pay. No statistical significance was found between techniques by geographic location. Corneal edema (22.4%), endothelial dystrophy (17.5%), and bullous keratopathy (10.9%) were erroneously coded as indications for ALK. Corneal scars (2.5%) and corneal opacity (1.7%) were erroneously coded as indications for EK.
Conclusions
CT rates in Florida appear to overrepresent the female sex and underrepresent ethnic minorities, with propensities between PKP and African Americans, EK and female patients, and EK and Medicare reimbursement. Our study further confirms the utility of data mining for providing efficient, detailed, and practical insights into ophthalmology procedures, while highlighting the intrinsic challenges of large datasets.
Résumé
Objectif
Utiliser l’exploration de données pour analyser les transplantations de cornée (CT) en Floride de 2005 à 2014, segmentées par démographie, géographie et technique de transplantation.
Méthodes
Une étude rétrospective de la base de données a été réalisée à partir de données extraites du Healthcare and Cost Utilization Project en utilisant les codes de la terminologie procédurale courante pour la kératoplastie lamellaire (ALK), la kératoplastie endothéliale (EK) et la kératoplastie pénétrante (PKP). Le statut du payeur, le groupe ethnique, l’âge, le sexe et la géographie (urbaine ou rurale) ont été extraits de chaque rencontre chirurgicale et reconfigurés pour fournir un ensemble de données « propre » et congruent pour l’analyse statistique. Cette étude approuvée par l’Institutional Review Board n’a pas utilisé d’informations identifiables sur les patients; le consentement éclairé individuel n’était donc pas nécessaire.
Résultats
De 2005 à 2014, les CT (n=28 607) ont représenté moins de 1 % du total des chirurgies ambulatoires (n=12 695 932) réalisées en Floride. Le volume des EK a augmenté tandis que celui des PKP et des ALK a diminué, d’une année sur l’autre. Une signification statistique a été trouvée entre la technique de transplantation par sexe (p<0,001) et par groupe ethnique (p<0,001). L’écart le plus important entre les sexes était l’EK (59 % de femmes, 41 % d’hommes). Les patients blancs ont subi relativement moins de PKP que d’EK (71 % vs. 83 % des totaux), tandis que les patients noirs ont subi relativement plus de PKP que d’EK (14 % vs. 6 % des totaux). Une significativité statistique a été trouvée entre les techniques par payeur (p<0,001). Medicare était le payeur le plus courant pour toutes les techniques, mais les techniques ALK et PKP présentaient des pourcentages plus élevés d’assurance privée et d’auto-paiement. Aucune significativité statistique n’a été trouvée entre les techniques selon la localisation géographique. L’œdème cornéen (22,4 %), la dystrophie endothéliale (17,5 %) et la kératopathie bulleuse (10,9 %) ont été codés par erreur comme des indications de l’ALK. Les cicatrices cornéennes (2,5 %) et l’opacité cornéenne (1,7 %) ont été codées par erreur comme des indications de l’EK.
Conclusions
Les taux de CT en Floride semblent surreprésenter le sexe féminin et sous-représenter les minorités ethniques, avec des propensions entre PKP et Afro-Américains, EK et patients féminins, et EK et remboursement Medicare. Notre étude confirme l’utilité de l’exploration de données pour fournir des informations efficaces, détaillées et pratiques sur les procédures ophtalmologiques, tout en soulignant les défis intrinsèques des grands ensembles de données.}
}
@article{MMOHAMMED2022197,
title = {Selective ensemble of classifiers trained on selective samples},
journal = {Neurocomputing},
volume = {482},
pages = {197-211},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221017227},
author = {Amgad {M. Mohammed} and Enrique Onieva and Michał Woźniak},
keywords = {Meta-heuristics, Ensemble selection, Data reduction, Ensemble pruning, Multiple classifier systems, Big data, Machine learning, Difficult samples, Ordering-based pruning},
abstract = {Classifier ensembles are characterized by the high quality of classification, thanks to their generalizing ability. Most existing ensemble algorithms use all learning samples to learn the base classifiers that may negatively impact the ensemble’s diversity. Also, the existing ensemble pruning algorithms often return suboptimal solutions that are biased by the selection criteria. In this work, we present a proposal to alleviate these drawbacks. We employ an instance selection method to query a reduced training set that reduces both the space complexity of the formed ensemble members and the time complexity to classify an instance. Additionally, we propose a guided search-based pruning schema that perfectly explores large-size ensembles and brings on a near-optimal subensemble with less computational requirements in reduced memory space and improved prediction time. We show experimentally how the proposed method could be an alternative to large-size ensembles. We demonstrate how to form less-complex, small-size, and high-accurate ensembles through our proposal. Experiments on 25 datasets show that the proposed method can produce effective ensembles better than Random Forest and baseline classifier pruning methods. Moreover, our proposition is comparable with the Extreme Gradient Boosting Algorithm in terms of accuracy.}
}
@article{ZHANG2022106594,
title = {Artificial intelligence-aided railroad trespassing detection and data analytics: Methodology and a case study},
journal = {Accident Analysis & Prevention},
volume = {168},
pages = {106594},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2022.106594},
url = {https://www.sciencedirect.com/science/article/pii/S0001457522000306},
author = {Zhipeng Zhang and Asim Zaman and Jinxuan Xu and Xiang Liu},
keywords = {Trespassing, Railroad safety, Artificial Intelligence, Computer vision, Risk management},
abstract = {The railroad industry plays a principal role in the transportation infrastructure and economic prosperity of the United States, and safety is of the utmost importance. Trespassing is the leading cause of rail-related fatalities and there has been little progress in reducing the trespassing frequency and deaths for the past ten years in the United States. Although the widespread deployment of surveillance cameras and vast amounts of video data in the railroad industry make witnessing these events achievable, it requires enormous labor-hours to monitor real-time videos or archival video data. To address this challenge and leverage this big data, this study develops a robust Artificial Intelligence (AI)-aided framework for the automatic detection of trespassing events. This deep learning-based tool automatically detects trespassing events, differentiates types of violators, generates video clips, and documents basic information of the trespassing events into one dataset. This study aims to provide the railroad industry with state-of-the-art AI tools to harness the untapped potential of video surveillance infrastructure through the risk analysis of their data feeds in specific locations. In the case study, the AI has analyzed over 1,600 h of archival video footage and detected around 3,000 trespassing events from one grade crossing in New Jersey. The data generated from these big video data will potentially help understand human factors in railroad safety research and contribute to specific trespassing proactive safety risk management initiatives and improve the safety of the train crew, rail passengers, and road users through engineering, education, and enforcement solutions to trespassing.}
}
@article{ALKEZ2022133633,
title = {Exploring the sustainability challenges facing digitalization and internet data centers},
journal = {Journal of Cleaner Production},
volume = {371},
pages = {133633},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133633},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622032115},
author = {Dlzar {Al Kez} and Aoife M. Foley and David Laverty and Dylan Furszyfer {Del Rio} and Benjamin Sovacool},
keywords = {Data center emissions, Dark data, Data storage, Environmental footprints, Power consumption},
abstract = {Internet data centers have received significant scientific, public, and media attention due to the challenges associated with their greenhouse gas, water, and land footprint. This resource greedy data services sector continues to rapidly grow driven by data storage, data mining, and file sharing activities by a wide range of end-users. A fundamentally important question then arises; what impact does data storage have on the environment and is it sustainable? Water is used extensively in data centers, both directly for liquid cooling and indirectly to generate electricity. Data centers house a huge number of servers, which consume a vast amount of energy to respond to information requests and store files and large amounts of resulting data. Here we examine the environmental footprint of global data storage utilizing extensive datasets from the latest global electricity generation mix to throw light on this data sustainability issue. The analysis also provides a broad perspective of carbon, water, and land footprints due to worldwide data storage to through some light on the real impact of data centers globally. The findings indicate that if not properly handled, the annual global carbon, water and land footprints resulting from storing dark data might approach 5.26 million tons, 41.65 Gigaliters, and 59.45 square kilometers, respectively.}
}
@article{ALHADDAD202283,
title = {A data–information–knowledge cycle for modeling driving behavior},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
volume = {85},
pages = {83-102},
year = {2022},
issn = {1369-8478},
doi = {https://doi.org/10.1016/j.trf.2021.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S1369847821002953},
author = {Christelle {Al Haddad} and Constantinos Antoniou},
keywords = {Data collection, Information extraction, Impacts of AVs, Behavioral modeling, Data analytics, Data fusion},
abstract = {When talking about automation, “autonomous vehicles”, often abbreviated as AVs, come to mind. In transitioning from the “driver” mode to the different automation levels, there is an inevitable need for modeling driving behavior. This often happens through data collection from experiments and studies, but also information extraction, a key step in behavioral modeling. Particularly, naturalistic driving studies and field operational trials are used to collect meaningful data on drivers’ interactions in real–world conditions. On the other hand, information extraction methods allow to predict or mimic driving behavior, by using a set of statistical learning methods. In simple words, the way to understand drivers’ needs and wants in the era of automation can be represented in a data–information cycle, starting from data collection, and ending with information extraction. To develop this cycle, this research reviews studies with keywords “data collection”, “information extraction”, “AVs”, while keeping the focus on driving behavior. The resulting review led to a screening of about 161 papers, out of which about 30 were selected for a detailed analysis. The analysis included an investigation of the methods and equipment used for data collection, the features collected, the size and frequency of the data along with the main problems associated with the different sensory equipment; the studies also looked at the models used to extract information, including various statistical techniques used in AV studies. This paved the way to the development of a framework for data analytics and fusion, allowing the use of highly heterogeneous data to reach the defined objectives; for this paper, the example of impacts of AVs on a network level and AV acceptance is given. The authors suggest that such a framework could be extended and transferred across the various transportation sectors.}
}
@incollection{SEBASTIANCOLEMAN2022119,
title = {Chapter 6 - The Technical Challenge: Data/Technology Balance},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {119-130},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000067},
author = {Laura Sebastian-Coleman},
keywords = {Technology hype, data quality tools, technology funding, data quality management},
abstract = {This chapter discusses the deep connection between the data we produce and the technology through which we create, collect, manage, access, and use it. Data brings value only when it is used. Without reliable, technical management of data, people cannot access and use data. Unfortunately, incorrect assumptions about the relationship between technology and data often result in poor-quality data. Organizations must manage their technology to support their data strategy, while avoiding the risk of being sucked into technology hype. Both data and technology must serve organizational goals.}
}
@article{LI2022124771,
title = {Data-driven battery state of health estimation based on interval capacity for real-world electric vehicles},
journal = {Energy},
volume = {257},
pages = {124771},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124771},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222016747},
author = {Renzheng Li and Jichao Hong and Huaqin Zhang and Xinbo Chen},
keywords = {Electric vehicle, Battery system, SOH estimation, Interval capacity, Catboost},
abstract = {State of health (SOH) estimation is critical to the safety of battery systems in real-world electric vehicles. Accurate battery health status is difficult to be measured during dynamic and robust vehicular operation conditions. This paper proposes a novel SOH estimation model based on Catboost and interval capacity during the charging process. A year-long operation dataset of an electric taxi is derived with all charging segments separated to construct the research dataset. The charging patterns are analyzed, and the segments with rich aging information are extracted, then a general aging feature of interval capacity is extracted by incremental capacity analysis. Furthermore, comparison with the other six machine learning methods is conducted, and five inputs are determined through Pearson correlation analysis, including start charging state of charge (SOC), end charging SOC, mileage, temperature of probe, and current. The results show the Catboost-based model achieves the best accuracy, with the mean absolute percentage error and root mean squared error limited within 2.74% and 1.12%, respectively. More importantly, a battery aging evaluation strategy and its further research plan is proposed for the application in real-world electric vehicles.}
}
@article{FIROUZI2022101840,
title = {The convergence and interplay of edge, fog, and cloud in the AI-driven Internet of Things (IoT)},
journal = {Information Systems},
volume = {107},
pages = {101840},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101840},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000776},
author = {Farshad Firouzi and Bahar Farahani and Alexander Marinšek},
keywords = {Internet of Things (IoT), Cloud Computing, Fog Computing, Edge Computing, Mobile Computing, Edge-Fog-Cloud, Cloud IoT, Cloudlet, Offloading, Resource Management, Service Placement, Privacy-Preserving Machine Learning, Security and Privacy, Healthcare IoT, Case Studies},
abstract = {The Internet of Things (IoT) tsunami, public embracement, and the ubiquitous adoption of smart devices are affecting virtually every industry, directly or indirectly. The success of the current and future landscape of IoT and connected devices requires service provision characterized by scalability, ubiquity, reliability, and high-performance, among others. In order to achieve this attribution, the integration of IoT and Cloud Computing (CC), known as cloud IoT, has emerged as a new paradigm providing advanced services specific to aggregating, storing, and processing data generated by IoT. While the convergence of IoT and Cloud brings opportunities, it suffers from specific limitations such as bandwidth, latency, and connectivity. The increasing need for supporting interaction between cloud and IoT led to Edge and Fog Computing (FC) in which computing and storage resources are located not only in the cloud but also at the edges near the source of data. The hierarchical and collaborative edge–fog–cloud architecture brings tremendous benefits as it enables us to distribute the intelligence and computation – including Artificial Intelligence (AI), Machine Learning (ML), and big data analytics – to achieve an optimal solution while satisfying the given constraints e.g., delay-energy tradeoff. Due to the hierarchical, cross-layer, and distributed nature of this model, achieving an osmotic and effective convergence of IoT, edge, fog, and cloud computing requires overcoming many challenges with respect to design and implementation, as well as deployment and evaluation. This paper provides a comprehensive insight into the edge-fog-cloud computing paradigm by providing a blend of discussions on all important aspects of the underlying technologies to offer opportunities for more holistic studies and to accelerate knowledge acquisition. To gain a deep understanding of edge–fog–cloud, we will begin this paper by providing an in-depth tutorial and presenting the main requirements, state-of-the-art reference architectures, building blocks, components, protocols, applications, and other similar computing paradigms, including their similarities and differences. Following this, a holistic reference architecture for edge–fog–cloud IoT is presented and the major corresponding design and deployment considerations (e.g., service models, infrastructure design, provisioning, resource allocation, offloading, service migration, performance evaluation, and security concerns) are discussed. Next, we will take a look at the role of privacy-preserving, distributed, and collaborative analytics as well as the interaction between edge, fog, and cloud computing. Finally, we will overview the main challenges in the field of edge–fog–cloud computing that need to be tackled to realize the full potential of IoT.}
}
@article{SHARMA20226962,
title = {Artificial intelligence framework for MSME sectors with focus on design and manufacturing industries},
journal = {Materials Today: Proceedings},
volume = {62},
pages = {6962-6966},
year = {2022},
note = {International Conference on Additive Manufacturing and Advanced Materials (AM2)},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.12.360},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321081013},
author = {Paawan Sharma and Jigarkumar Shah and Reema Patel},
keywords = {Artificial Intelligence (AI), Industry 4.0, Internet of Things (IoT), Cyber Physical Systems (CPS), Industrial IoT (IIoT)},
abstract = {Artificial Intelligence (AI) is gaining high popularity in multiple domains targeting different industries. Though initial adoption of AI was thought of as an enabler for automation in industries, but later it presented vast possibilities in innovation and design. Also, with advent of Industry 4.0 standards, AI emerged to play a big role in its adoption across different sectors of industries. Micro, Small and medium enterprises (MSMEs) with limited resources look upon AI as a tool for accelerating their growth. In India, the MSME sector although varies with different States, but dominantly includes textiles, machinery and parts, mining and quarrying, basic metal industries, electrical machinery and apparatus, transport equipment and parts, paper products and printing, food products, chemical and chemical products, leather, wood, rubber, plastic and other non-metallic mineral products, beverages and tobacco products. This paper proposes architectural framework and also provides probable applications of Internet of Things (IoT) and AI for design and manufacturing MSMEs of India.}
}
@article{TSUMURA2022100043,
title = {Examining potentials and practical constraints of mobile phone data for improving transport planning in developing countries},
journal = {Asian Transport Studies},
volume = {8},
pages = {100043},
year = {2022},
issn = {2185-5560},
doi = {https://doi.org/10.1016/j.eastsj.2021.100043},
url = {https://www.sciencedirect.com/science/article/pii/S2185556021000110},
author = {Yuma Tsumura and Yoshihisa Asada and Hiroshi Kanasugi and Ayumi Arai and Ryosuke Shibasaki and Hirohisa Kawaguchi and Kaoru Yamada},
keywords = {Call detail records, Mobile phone data, Big data, OD matrix, Travel behavior, Household travel survey},
abstract = {The advantages and constraints of mobile phone data, call detail records (CDRs), for comprehending travel patterns were argued in previous studies. However, the spatio-temporal tendencies of the estimated travel patterns from CDR data have been underrepresented from the viewpoint of transport planning practice. This study scrutinizes the benefits and constraints of CDR data for grasping spatio-temporal travel patterns through multidimensional comparison between origin-destination matrices generated from aggregated CDR data and household travel survey (HTS) data from Colombo, Sri Lanka. The results show the potential of CDR data to complement the conventional drawbacks of HTSs. They also indicate practical constraints owing to the nature of CDR data and the possible impacts of data protection measures. This study summarizes how the results could be used for interpreting the time coverage and distribution, trip type, spatial distribution, and population projection and coverage to discuss future directions toward improving transport planning in developing countries.}
}
@article{JANSEN2022100020,
title = {The illusion of data validity: Why numbers about people are likely wrong},
journal = {Data and Information Management},
pages = {100020},
year = {2022},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2022.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122001188},
author = {Bernard J. Jansen and Joni Salminen and Soon-gyo Jung and Hind Almerekhi},
keywords = {People data, Measurement, Quantitative paradigm, Statistics},
abstract = {This reflection article addresses a difficulty faced by scholars and practitioners working with numbers about people, which is that those who study people want numerical data about these people. Unfortunately, time and time again, this numerical data about people is wrong. Addressing the potential causes of this wrongness, we present examples of analyzing people numbers, i.e., numbers derived from digital data by or about people, and discuss the comforting illusion of data validity. We first lay a foundation by highlighting potential inaccuracies in collecting people data, such as selection bias. Then, we discuss inaccuracies in analyzing people data, such as the flaw of averages, followed by a discussion of errors that are made when trying to make sense of people data through techniques such as posterior labeling. Finally, we discuss a root cause of people data often being wrong – the conceptual conundrum of thinking the numbers are counts when they are actually measures. Practical solutions to address this illusion of data validity are proposed. The implications for theories derived from people data are also highlighted, namely that these people theories are generally wrong as they are often derived from people numbers that are wrong.}
}
@article{YANG2022104007,
title = {Characterizing residential load patterns on multi-time scales utilizing LSTM autoencoder and electricity consumption data},
journal = {Sustainable Cities and Society},
volume = {84},
pages = {104007},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104007},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722003274},
author = {Wei Yang and Xinhao Li and Chao Chen and Jingke Hong},
keywords = {Multi-time scale, Load patterns, LSTM Autoencoder, Electricity consumption data, Clustering},
abstract = {Load patterns represent a clear picture of electricity usage, reflecting the consumer's habits. Previous works mainly focused on load patterns discovery on a fixed scale, but limited to characterize load patterns on multi-time scales utilizing electricity consumption data (ECD). Therefore, we propose a novel framework to characterize residential load patterns on multi-time scales. The long-short-term memory autoencoder (LSTM-AE) model is designed for dimensionality reduction and feature extraction. Furthermore, a two-level clustering method is proposed to discover and characterize typical load patterns (TLPs) and multifaceted load patterns (MLPs) on multi-time scales. The proposed framework is comprehensively evaluated via extensive experiments on three real ECD. Results show that: (1) Reconstruction errors of LSTM-AE are lower than 6 benchmark models across different time scales, which validates the superiority of LSTM-AE. (2) TLPs and MLPs on daily, weekly, monthly and yearly scale are discovered by the two-level clustering method. TLPs profile the resident's electricity usages from a global view. (3) MLPs present the consumer segmentation and characterize residential load patterns of individual and groups. Especially, customer groups and electricity usage habits or lifestyles are revealed thoroughly to customize personal demand response strategies. This study can provide new valuable insights for smart grid applications.}
}
@article{YONG2022438,
title = {Robust deep auto-encoding network for real-time anomaly detection at nuclear power plants},
journal = {Process Safety and Environmental Protection},
volume = {163},
pages = {438-452},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S095758202200430X},
author = {Shi Yong and Zhang Linzi},
keywords = {Nuclear Power Plant, Anomaly detection, Multi-sensor, Time series, ConvGRU},
abstract = {Detecting anomaly conditions in nuclear reactor is a critical issue in safety management of Nuclear Power Plants (NPPs). Conventionally, the operating status are monitored in transient data with pre-designed labels by human operators or basic diagnosis systems. Nowadays, continuous time series data from multi-sensors are increasingly collected and emerging unlabeled abnormal status are monitored during the operation, making it challenging to capture both spatial and temporal dependency at each time steps without supervised labels. In this paper, a robust unsupervised Multi-Variate Convolutional GRU Encoder-Dncoder (MVCGED) method is proposed to perform anomaly detection and fault diagnosis in multi-sensor operation time series data. Specifically, MVCGED first construct each time steps into signature matrices to maintain both spatial and temporal features via sliding windows with inner-correlation and forget mechanism. Subsequently, A CNN feature extraction network, CNN-based GRU encoding network and CNN decoding network are implemented successively to capture and reconstruct the hidden patterns of the signature matrices. Finally, the reconstruction loss are further utilized to detect anomalies and diagnose faults. Extensive empirical studies based on PCTRAN nuclear power plant operation data demonstrate that MVCGED outperforms commonly-used baseline methods.}
}
@article{YANG2022117018,
title = {ISBFK-means: A new clustering algorithm based on influence space},
journal = {Expert Systems with Applications},
volume = {201},
pages = {117018},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117018},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422004365},
author = {Yuqing Yang and Jianghui Cai and Haifeng Yang and Yating Li and Xujun Zhao},
keywords = {Clustering, Influence space, Region partition, Representative data objects},
abstract = {The time overhead is huge and the clustering quality is unstable when running the K-means algorithm on massive raw data. To solve these problems, the concept of the influence space is introduced, and on this basis, a new clustering algorithm named ISBFK-means based on the influence space is proposed in this paper. First, the influence space divides the given data set into multiple small regions. Then, the representative data objects in each region are obtained to form a new data set, in which the class labels of representative data objects are those of all the data objects in the correlation influence space. Next, the K-means clustering is performed on the new data set, thereby obtaining the final clustering result. Theoretical analysis and experimental results show that this approach effectively reduces the amount of data in the clustering process and improves the stability of clustering quality. As a major feature of this work, the celestial spectral data observed by the LAMOST survey are especially employed to verify the algorithm ISBFK-means. The experimental results indicate that this algorithm has higher performance than other similar algorithms on the correctness, efficiency and sensitivity to the quality of spectral data.}
}
@article{IPENZA2022109093,
title = {QDS-COVID: A visual analytics system for interactive exploration of millions of COVID-19 healthcare records in Brazil},
journal = {Applied Soft Computing},
volume = {124},
pages = {109093},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109093},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622003787},
author = {Juan Carlos Carbajal Ipenza and Noemi Maritza Lapa Romero and Melina Loreto and Nivan Ferreira Júnior and João Luiz Dihl Comba},
keywords = {COVID-19, Electronic healthcare records, Visual analytics},
abstract = {COVID-19 is responsible for the deaths of millions of people around the world. The scientific community has devoted its knowledge to finding ways that reduce the impact and understand the pandemic. In this work, the focus is on analyzing electronic health records for one of the largest public healthcare systems globally, the Brazilian public healthcare system called Sistema Único de Saúde (SUS). SUS collected more than 42 million flu records in a year of the pandemic and made this data publicly available. It is crucial, in this context, to apply analysis techniques that can lead to the optimization of the health care resources in SUS. We propose QDS-COVID, a visual analytics prototype for creating insights over SUS records. The prototype relies on a state-of-the-art datacube structure that supports slicing and dicing exploration of charts and Choropleth maps for all states and municipalities in Brazil. A set of analysis questions drives the development of the prototype and the construction of case studies that demonstrate the potential of the approach. The results include comparisons against other studies and feedback from a medical expert.}
}
@article{CHIEN2022108245,
title = {Decision-based virtual metrology for advanced process control to empower smart production and an empirical study for semiconductor manufacturing},
journal = {Computers & Industrial Engineering},
volume = {169},
pages = {108245},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108245},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222003151},
author = {Chen-Fu Chien and Wei-Tse Hung and Chin-Wei Pan and Tran Hong {Van Nguyen}},
keywords = {Virtual metrology, Digital decision, Isolation forest, Advanced process control, Semiconductor manufacturing},
abstract = {Virtual metrology (VM) has been employed to improve the performance of advanced process control for semiconductor manufacturing. A number of VM models have been proposed to predict the quality characteristics for the wafers that have not been sampled and measured. However, little research has been done to address the interrelations between the VM model and associated decisions for advanced process control and yield enhancement. There is a research need for developing a framework that can integrate the confidence level of VM prediction and domain knowledge to derive appropriate decisions for real-time control. To fill the gaps, this study aims to develop a decision-based virtual metrology framework that integrates clustering and regression models to enhance the prediction and ensure the decision quality for the R2R controller. In particular, Isolation Forest is employed to cluster the data group for multi-recipes and multi-tools. Random Forest Regression is developed for the prediction model for each category respectively to enhance the accuracy of predicted results. Furthermore, this approach designs an overall confidence score based on data integrity and predicted results to suggest the optimal decision rules for R2R control in real time. This approach is validated with an empirical study in a leading semiconductor manufacturing company in Taiwan. Indeed, the results have demonstrated practical viability and the developed solution has been implemented.}
}
@incollection{REDMAN2022xxi,
title = {Foreword},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {xxi-xxii},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00019-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000195},
author = {Thomas C. Redman}
}
@article{NAFA2022107729,
title = {Active deep learning on entity resolution by risk sampling},
journal = {Knowledge-Based Systems},
volume = {236},
pages = {107729},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107729},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121009679},
author = {Youcef Nafa and Qun Chen and Zhaoqiang Chen and Xingyu Lu and Haiyang He and Tianyi Duan and Zhanhuai Li},
keywords = {Active learning, Deep learning, Risk analysis, Entity resolution},
abstract = {While the state-of-the-art performance on entity resolution (ER) has been achieved by deep learning, its effectiveness depends on large quantities of accurately labeled training data. To alleviate the data labeling burden, Active Learning (AL) presents itself as a feasible solution that focuses on data deemed useful for model training. Building upon the recent advances in risk analysis for ER, which can provide a more refined estimate on label misprediction risk than the simpler classifier outputs, we propose a novel AL approach of risk sampling for ER. Risk sampling leverages misprediction risk estimation for active instance selection. Based on the core-set characterization for AL, we theoretically derive an optimization model which aims to minimize core-set loss with non-uniform Lipschitz continuity. Since the defined weighted K-medoids problem is NP-hard, we then present an efficient heuristic algorithm. Finally, we empirically verify the efficacy of the proposed approach on real data by a comparative study. Our extensive experiments have shown that it outperforms the existing alternatives by considerable margins.}
}
@article{LIU2022102956,
title = {Trust secure data aggregation in WSN-based IIoT with single mobile sink},
journal = {Ad Hoc Networks},
volume = {136},
pages = {102956},
year = {2022},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.102956},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522001330},
author = {Xiaowu Liu and Jiguo Yu and Kan Yu and Guijuan Wang and Xingjian Feng},
keywords = {Industrial Internet of Things, Secure data aggregation, Trust evaluation},
abstract = {Wireless Sensor Networks (WSNs), as the fundamental infrastructure, are indispensable for the Industrial Internet of Things (IIoT). In particular, the security and effectiveness of WSNs in IIoT are universal and inevitable issues. In this paper, a WSN-based IIoT Model (WIM) is designed characterized by a single mobile sink. Based on WIM, a high robust aggregation tree algorithm and an outlier elimination scheme are proposed in a virtual grid network with the mobile sink, which can acquire data in a real-time and accurate manner. Moreover, a trust secure data aggregation mechanism is applied in WIM which takes both the direct trust and the indirect trust into consideration to perform the secure data aggregation without losing the effectiveness of network even if the sink moves randomly. The simulation results show that the proposed model and algorithms can promote the performances of WSNs in terms of accuracy, effectiveness and delay.}
}
@article{GHAZALBASH2022197,
title = {Impact of multimorbidity and frailty on adverse outcomes among older delayed discharge patients: Implications for healthcare policy},
journal = {Health Policy},
volume = {126},
number = {3},
pages = {197-206},
year = {2022},
issn = {0168-8510},
doi = {https://doi.org/10.1016/j.healthpol.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0168851022000045},
author = {Somayeh Ghazalbash and Manaf Zargoush and Fabrice Mowbray and Andrew Costa},
keywords = {Multimorbidity, Frailty, Discharge policy, Delayed discharge, Hospital readmission, Mortality, Geriatrics},
abstract = {Objective
To assess the impacts of multiple chronic conditions (MCC) and frailty on 30-day post-discharge readmission and mortality among older patients with delayed discharge.
Data source/extraction
We used a retrospective cohort of older patients in the Discharge Abstract Database (DAD) between 2004 and 2017 in Ontario, Canada. We extracted data on patients aged ≥ 65 who experienced delayed discharge during hospitalization (N = 353,106).
Study design
We measured MCC and frailty using the Elixhauser Comorbidity Index (ECI) and the Hospital Frailty Risk Score (HFRS), respectively. We used multinomial logistic regression to model the main and interactive effects of MCC and frailty on the adverse outcomes.
Principal findings
After adjusting for sex, discharge destination, urban/rural residency, wait time for alternative care, and socioeconomic status, the coexistence of MCC and high frailty increased the relative risk of 30-day mortality and readmission when compared to the references group, i.e., non-MCC patients with low-to-moderate frailty.
Conclusions
Multimorbidity and frailty each provide unique information about adverse outcomes among older patients with delayed discharge but are most informative when examined in unison.
Implications for health policy
To minimize the risk of adverse outcomes among older delayed discharge patients, discharge planning must be tailored to their concurrent multimorbidity and frailty status.}
}
@incollection{KOLTAY2022ix,
title = {Introduction},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {ix-xi},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00007-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000072},
author = {Tibor Koltay}
}
@article{DAUM2022103353,
title = {Connected cows and cyber chickens? Stocktaking and case studies of digital livestock tools in Kenya and India},
journal = {Agricultural Systems},
volume = {196},
pages = {103353},
year = {2022},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2021.103353},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X21003061},
author = {Thomas Daum and Thanammal Ravichandran and Juliet Kariuki and Mizeck Chagunda and Regina Birner},
keywords = {Farming 4.0, Digital farming, Digital livestock, Smartphones, Africa, India},
abstract = {CONTEXT
There are high hopes that digital tools can reduce constraints to livestock development, which in turn promises to alleviate poverty, improve food and nutrition security, and reduce environmental footprints. Yet, little systematic evidence exists on the state of digital livestock in low- and middle-income-countries. Thus, it remains unclear whether such high hopes are justified.
OBJECTIVE
Focusing on India and Kenya, we aim to better understand, among others, the degree of technological sophistication of the digital tools used, the types of value chains and constraints addressed, the types of business models pursued, and more broadly the opportunities and challenges of digital tools for agricultural development.
METHOD
We combine a review of digital tools in India and Kenya with three “on-the-ground” case studies: Herdman, a tool for Indian dairy organizations working with small-scale livestock keepers, facilitating data collection and supervision of field agents; Farmtree, a tool supporting medium-scale livestock keepers in India to manage their herds, and iCow, an e-extension tool for farmers in Kenya. For the review, we develop a conceptual framework that distinguishes different types of tools: 1) “simple digital tools”, providing generic information, 2) “smart digital tools”, providing tailored information based on data entered by livestock keepers, 3) “smart digital tools”, using data from sensors, 4) “digital tools for value chains”, enabling the integration of value chain actors, 5) “automated digital systems”, which are coupled with robots, allowing for automation.
RESULTS AND CONCLUSIONS
Digital tools provide many new options to address constraints to livestock development. So far, most tools are “simple digital tools”, followed by “smart digital tools” using manual data and tools for value chains. Such tools that only require smartphone ownership are the “sweet spot” for supporting digital livestock development; however, even embodied “smart digital tools” using sensors can be of relevance for small-scale livestock keepers with appropriate organizational models. Most digital tools focus on dairy production, suggesting neglect of other types of livestock, and there are few tools for pastoralists.
SIGNIFICANCE
The conceptual framework as well as many of the lessons learned are of relevance to understanding the contribution of digital tools to livestock development - and agricultural development more broadly - in low- and middle-income-countries. While digital tools are no silver bullets – and come with some new challenges such as data security and sovereignty concerns - they are likely to become a key pillar of agricultural and livestock development in the near future.}
}
@incollection{2022xvii,
title = {Preface},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {xvii-xx},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00025-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032391907400025X}
}
@article{FEST2022100604,
title = {Paper vs. practice: How legal and ethical frameworks influence public sector data professionals in the Netherlands},
journal = {Patterns},
volume = {3},
number = {10},
pages = {100604},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100604},
url = {https://www.sciencedirect.com/science/article/pii/S266638992200229X},
author = {Isabelle Fest and Maranke Wieringa and Ben Wagner},
keywords = {DSML2: Proof-of-concept: Data science output has been formulated, implemented, and tested for one domain/problem},
abstract = {Summary
Recent years have seen a massive growth in ethical and legal frameworks to govern data science practices. Yet one of the core questions associated with ethical and legal frameworks is the extent to which they are implemented in practice. A particularly interesting case in this context comes to public officials, for whom higher standards typically exist. We are thus trying to understand how ethical and legal frameworks influence the everyday practices on data and algorithms of public sector data professionals. The following paper looks at two cases: public sector data professionals (1) at municipalities in the Netherlands and (2) at the Netherlands Police. We compare these two cases based on an analytical research framework we develop in this article to help understanding of everyday professional practices. We conclude that there is a wide gap between legal and ethical governance rules and the everyday practices.}
}
@article{KHOSRAVI2022111707,
title = {Modification of correlation optimized warping method for position alignment of condition measurements of linear assets},
journal = {Measurement},
volume = {201},
pages = {111707},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111707},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122009149},
author = {Mahdi Khosravi and Iman Soleimanmeigouni and Alireza Ahmadi and Arne Nissen and Xun Xiao},
keywords = {Position alignment, Correlation optimized warping, Data quality, Linear assets, Positional error, Condition measurements},
abstract = {This paper proposes a modification to a well-known alignment method, correlation optimized warping (COW), to improve the efficiency of the method and reduce the positional errors in the measurements of linear assets. The modified method relaxes the restrictions of COW in aligning the start and end of datasets and decreases the computational time. Furthermore, the method takes advantage of the interdependencies between simultaneously measured channels to overcome the missing data problem. A case study on railway track geometry measurements was conducted to implement the proposed method and assess its performance in reducing the positioning inaccuracy of the measurements. The findings revealed that the modified method could decrease the positional errors of defects to below 25 cm in 94 % of the trials.}
}
@article{CHEN2022102907,
title = {The role of imaging radar in cultural heritage: From technologies to applications},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102907},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102907},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222001091},
author = {Fulong Chen and Huadong Guo and Deodato Tapete and Francesca Cigna and Salvatore Piro and Rosa Lasaponara and Nicola Masini},
keywords = {Imaging radar, GPR, Cultural heritage, Technical integration, Interdisciplinary},
abstract = {Imaging radar has been dramatically developed over the past decades enabling a better understanding of cultural heritage from a microwave perspective. Nonetheless, a dedicated survey and analysis of the performance of such technology in cultural heritage monitoring and management is required. In order to fill this gap, we first review the technology advance of imaging radar, including ground penetration radar, ground-based and airborne/satellite radar, in the focused cultural applications to grasp the development trend of these technologies. We then analyse the performance and limitations of imaging radar technologies based on their respective characteristics to facilitate the technology service in practical applications. Finally, we propose a flexible solution of imaging radar in cultural heritage through technical integration with pilot synergy applications in archaeological prospection and cultural heritage diagnosis and conservation.}
}
@article{KALAITZI2022108466,
title = {Supply chain analytics adoption: Determinants and impacts on organisational performance and competitive advantage},
journal = {International Journal of Production Economics},
volume = {248},
pages = {108466},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2022.108466},
url = {https://www.sciencedirect.com/science/article/pii/S0925527322000597},
author = {Dimitra Kalaitzi and Naoum Tsolakis},
keywords = {Supply chain analytics, TOE Framework, Acceptance and adoption, Survey, Manufacturing industry},
abstract = {Despite manufacturing companies recognising the potential benefits associated with the adoption of Supply Chain Analytics (SCA), only a few firms adopt data-based decision-making processes due to fundamental technical, organisational and environmental challenges. In this regard, this research explores the determinants influencing SCA adoption and the impacts on firm performance and competitive advantage. Specifically, the Technological, Organisational, and Environmental (TOE) framework was applied to identify the key determinants influencing SCA adoption. Data was collected from 217 executives working in the UK manufacturing sector through a questionnaire-based survey. The research model was tested using a quantitative approach, i.e., Partial Least Squares Structural Equation Modelling. Surprisingly, none of the identified technological factors leads manufacturing companies to adopt SCA. On the contrary, organisational and environmental factors have a crucial role in influencing supply chain and logistics managers to adopt SCA. This research also emphasises and validates the importance of SCA adoption in improving firm performance and fostering competitive advantage. On evaluating SCA adoption, supply chain managers should concentrate on aspects other than technological competence. Manufacturing companies looking to make investment decisions regarding SCA adoption should mainly consider organisational and environmental factors; hence, SCA systems can be used effectively and efficiently. This study is the first to explore the TOE framework regarding the adoption determinants within an SCA context along with its implications on organisational performance and competitive edge.}
}
@article{CLAPP2022658,
title = {Does accreditation matter? An analysis of complications of bariatric cases using the Metabolic and Bariatric Surgery Accreditation and Quality Improvement Program and National Quality Improvement Program databases},
journal = {Surgery for Obesity and Related Diseases},
volume = {18},
number = {5},
pages = {658-665},
year = {2022},
issn = {1550-7289},
doi = {https://doi.org/10.1016/j.soard.2022.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S1550728922000302},
author = {Benjamin Clapp and Samuel Grasso and Jesus Gamez and Jensen Edwards and Cristopher Dodoo and Ray Portela and Omar M. Ghanem and Brian R. Davis},
keywords = {MBSAQIP, NSQIP, Database, Big Data, Bariatric, Complications},
abstract = {Background
Two large nationwide databases collect data on common operations in the United States. The Metabolic and Bariatric Surgery Accreditation and Quality Improvement Program (MBSAQIP) collects bariatric data, whereas the National Quality Improvement Program (NSQIP) gathers details on a broader range of general surgical cases.
Objective
Evaluate the differences in rates of complications between both databases regarding Roux-en-Y gastric bypass and sleeve gastrectomy.
Setting
National databases, United States.
Methods
We evaluated the MBSAQIP and NSQIP from 2017 to 2019 using the procedure codes 43644 and 43775. Fifteen common complications were evaluated. Propensity-matched analyses (PMAs) were done to control for differences across databases. Significantly different variables after a PMA were included in multivariable models. The data were examined for differences between the 2 databases before and after the PMA, with and without adjustment for operation type.
Results
There were 483,361 cases reported in the MBSAQIP and 57,598 in the NSQIP. PMA matched 57,479 cases for each database. Seven complications were different, with higher rates reported in the NSQIP than in the MBSAQIP: myocardial infarction, sepsis, organ/space surgical site infections, deep vein thrombosis, urinary tract infections, pulmonary embolism, ventilator dependence >48 hours, and pneumonia. When adjusting for the procedure performed, sleeve gastrectomy in the NSQIP had higher rates of organ/space surgical site infections, deep vein thrombosis, sepsis, and death. Roux-en-Y gastric bypass in the NSQIP had higher rates of organ/space surgical site infections, ventilator dependence >48 hours, urinary tract infections, myocardial infarction, deep vein thrombosis, and sepsis.
Conclusion
When compared with the MBSAQIP, the NSQIP reports higher rates of bariatric complications. Further studies are needed to confirm the reasons behind this.}
}
@article{LOPES202255,
title = {Effective network intrusion detection via representation learning: A Denoising AutoEncoder approach},
journal = {Computer Communications},
volume = {194},
pages = {55-65},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002742},
author = {Ivandro O. Lopes and Deqing Zou and Ihsan H. Abdulqadder and Francis A. Ruambo and Bin Yuan and Hai Jin},
keywords = {Deep learning, Denoising autoencoder, Intrusion detection, Cybersecurity},
abstract = {The introduction of deep learning techniques in intrusion detection problems has enabled an enhanced standard of detection effectiveness. However, most of the progress has occurred in supervised learning, which required a vast amount of labeled training samples. In the real world, there is a limited amount of labeled data available to train a deep neural network, affecting the classifier’s detection performance. Therefore, to address the lack of labeled network traffic required to train an effective supervised classifier, this study introduces a semi-supervised intrusion detection framework that combines the unsupervised and supervised techniques. The unsupervised pre-training approach is implemented based on a denoising autoencoder (DAE), to compress the intrusion dataset and obtain the lower-dimensional features representation. Then a portion of the compressed data is used to train the DNN classifier based on a multiclass supervised approach. The network architecture is optimized by tuning hyper-parameters using a trial-and-error approach. Comparative analysis is performed between the proposed approach and the most relevant deep learning methods available in the literature against the CICIDS2018 dataset, consisting of recent network attack traces. Our approach outperforms competitive methods while maintaining stable classification results above 99.6% on F1-score, precision, and recall metrics. Additionally, it is trained in 64 min while achieving a low false alarm rate. Furthermore, the DAE module reduces the input network traffic data to one-tenth of the size of the input dataset.}
}
@article{TEMIZ2022102535,
title = {Open data: Lost opportunity or unrealized potential?},
journal = {Technovation},
volume = {114},
pages = {102535},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102535},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222000827},
author = {Serdar Temiz and Marcus Holgersson and Joakim Björkdahl and Martin W. Wallin},
keywords = {Big data, Business model, Digitalization, Open data, Open innovation, Value capture, Value creation},
abstract = {The promise of open data is grand, but the results are often meager. To resolve this conundrum and make headway in the adoption of effective open data practices, we take a step back and investigate the underlying reasons for investing in open data. Based on survey results, interviews, and complementary evidence from secondary sources, we explore the motives and beliefs about open data investment expressed by open data experts in both public and private organizations. To our surprise, in both public and private organizations we find that open data investments are driven more by legitimacy-seeking than a quest to realize the value creation potential of open data. The results are worrisome, as such motives and beliefs do not necessarily lead to investment in the complementary assets needed to realize the potential associated with open data—instead, open data risks becoming a lost opportunity. Clearly, it's time to move beyond the open data hype and get down to business. Our paper provides insights for practice and calls on future research to unpack antecedents and mechanisms for value creation, and to identify appropriate complementary investments in open data, for example in terms of technologies, tools, and systems.}
}
@article{BILJECKI2022101809,
title = {Global Building Morphology Indicators},
journal = {Computers, Environment and Urban Systems},
volume = {95},
pages = {101809},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2022.101809},
url = {https://www.sciencedirect.com/science/article/pii/S0198971522000539},
author = {Filip Biljecki and Yoong Shin Chow},
keywords = {Urban planning, GIScience, Morphometrics, OpenStreetMap, GeoAI, Spatial analysis},
abstract = {Characterising and analysing urban morphology is a continuous task in urban data science, environmental analyses, and many other domains. As the availability and quality of data on them have been increasing, buildings have gained more attention. However, tools and data facilitating large-scale studies, together with an interdisciplinary consensus on metrics, remain scarce and often inadequate. We present Global Building Morphology Indicators (GBMI) — a three-pronged contribution addressing such shortcomings: (i) a comprehensive list of hundreds of building form multi-scale measures derived through a systematic literature review; (ii) a methodology and tool for the computation of these metrics in a database suited for big data and comparative studies, and release the code freely and open-source; and (iii) we carry out the computations using high performance computing, generating a public repository with data quantifying the form of selected urban areas around the world, and demonstrate their value with novel analyses comparing morphological parameters across cities. GBMI introduces a formalised, structured, modular, and extensible method to compute, manage, and disseminate urban indicators at a large scale and high resolution, while the precomputed dataset facilitates comparative studies. The theory and implementation traverse multiple scales: at the building level, both individual and contextual ones based on encircling buildings by multiple buffers, and aggregations at several hierarchical administrative levels and at multiple grids. Our open dataset, comprising billions of records on a growing scope of urban areas worldwide, is the most comprehensive instance of morphological data parametrising the individual building stock, supporting studies in urban analytics and a range of disciplines.}
}
@article{KACHA20224075,
title = {KAB: A new k-anonymity approach based on black hole algorithm},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {7},
pages = {4075-4088},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001002},
author = {Lynda Kacha and Abdelhafid Zitouni and Mahieddine Djoudi},
keywords = {Privacy, Anonymization, K-anonymity, Clustering, Black hole algorithm},
abstract = {K-anonymity is the most widely used approach to privacy preserving microdata which is mainly based on generalization. Although generalization-based k-anonymity approaches can achieve the privacy protection objective, they suffer from information loss. Clustering-based approaches have been successfully adapted for k-anonymization as they enhance the data quality, however, the computational complexity of finding an optimal solution has shown as NP-hard. Nature-inspired optimization algorithms are effective in finding solutions to complex problems. We propose, in this paper, a novel algorithm based on a simple nature-inspired metaheuristic called Black Hole Algorithm (BHA), to address such limitations. Experiments on real data set show that data utility has been improved by our approach compared to k-anonymity, BHA-based k-anonymity and clustering-based k-anonymity approaches.}
}
@article{HU2022123195,
title = {Industrial artificial intelligence based energy management system: Integrated framework for electricity load forecasting and fault prediction},
journal = {Energy},
volume = {244},
pages = {123195},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.123195},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222000986},
author = {Yusha Hu and Jigeng Li and Mengna Hong and Jingzheng Ren and Yi Man},
keywords = {Electricity load, Dynamic forecasting model, Energy system analysis, Energy system optimisation, Artificial intelligence},
abstract = {Forecasting accuracy electricity load can help industrial enterprises optimise production scheduling based on peak and off-peak electricity prices. The electricity load forecasting results can be provided to an electricity system to improve electricity generation efficiency and minimize energy consumption by developing electricity generation plans in advance and by avoiding over or under the generation of electricity. However, because of the different informatization levels in different industries, few reliable intelligent electricity management systems are applied on the power supply side. Based on industrial big data and machine learning algorithms, this study proposes an integrated model to forecast short-term electricity load. The hybrid model based on the hybrid mode decomposition algorithms is proposed to decompose the total electricity load signal. To improve the generalisation ability of the forecasting model, a dynamic forecasting model is proposed based on the improved hybrid intelligent algorithm to forecast the short-term electricity load. The results show that the accuracy of the proposed dynamic integrated electricity load forecasting model is as high as 99%. The integrated framework could forecast abnormal electricity consumption in time and provide reliable evidence for production process scheduling.}
}
@article{VARELA2022982,
title = {Risks of Data Science Projects - A Delphi Study},
journal = {Procedia Computer Science},
volume = {196},
pages = {982-989},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.100},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921023231},
author = {Cristina Varela and Luísa Domingues},
keywords = {Data Science, project sucess, project risk management, risk assessment, Delphi study},
abstract = {Risk is one of the most crucial components of a project. Its proper evaluation and treatment increase the chances of a project’s success. This article presents the risks in Data Science projects, assessed through a study conducted with the Delphi technique, to answer the question, "What are the risks of Data Science projects". The study allowed the identification of specific risks related to data science projects, however it was possible to verify that over a half of the most mentioned risks are similar to other types of IT projects. This paper describes the research from expert selection, risk identification and analysis, and the first conclusions.}
}
@article{YAN2022112155,
title = {Research on repair method of abnormal energy consumption data of lighting and plug based on similar features},
journal = {Energy and Buildings},
volume = {268},
pages = {112155},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112155},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822003267},
author = {Huiyu Yan and Liangdong Ma and Tianyi Zhao and Jili Zhang},
keywords = {Energy consumption monitoring, NN-BS algorithm, Abnormal data, Data repair},
abstract = {The public building energy consumption monitoring platforms (BECMPs) play an important role in building energy conservation and energy-related decision-making. However, problems such as data missing and data mutation are quite common that they impede the effectiveness of the BECMPs. Based on the kNN algorithm, this paper proposes a kNN-BS data repair algorithm with the characteristics of electricity consumption trends taken into consideration. After data classification and cleaning, the data of hourly lighting and plug power consumption of an office building of Dalian University of Technology is repaired using the kNN-BS algorithm. The cross-validation method is used to determine the optimal k value. The relative repair error of the total daily electricity consumption of kNN-BS is less than 4% for working days and less than 6% for non-working days. Compared with kNN, the average CVRMSE of the kNN-BS algorithm is reduced by 1.36%∼1.59% for working days and 5.49%∼8.17% for non-working days. The kNN-BS algorithm shows excellent stability and robustness compared with kNN, polynomial and BPNN, which makes it very suitable for practical use, especially in efficiency-demanding scenarios. The kNN-BS algorithm can effectively improve the quality of public building energy consumption monitoring data and provide a theoretical basis and technical means for solving data quality problems in the BECMPs.}
}
@article{YANG2022551,
title = {Automated Analysis of Doppler Echocardiographic Videos as a Screening Tool for Valvular Heart Diseases},
journal = {JACC: Cardiovascular Imaging},
volume = {15},
number = {4},
pages = {551-563},
year = {2022},
issn = {1936-878X},
doi = {https://doi.org/10.1016/j.jcmg.2021.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X21006434},
author = {Feifei Yang and Xiaotian Chen and Xixiang Lin and Xu Chen and Wenjun Wang and Bohan Liu and Yao Li and Haitao Pu and Liwei Zhang and Dangsheng Huang and Meiqing Zhang and Xin Li and Hui Wang and Yueheng Wang and Huayuan Guo and Yujiao Deng and Lu Zhang and Qin Zhong and Zongren Li and Liheng Yu and Yongjie Duan and Peifang Zhang and Zhenzhou Wu and Daniel Burkhoff and Qiushuang Wang and Kunlun He},
keywords = {aortic regurgitation, aortic stenosis, deep learning, mitral regurgitation, mitral stenosis},
abstract = {Objectives
This study sought to develop a deep learning (DL) framework to automatically analyze echocardiographic videos for the presence of valvular heart diseases (VHDs).
Background
Although advances in DL have been applied to the interpretation of echocardiograms, such techniques have not been reported for interpretation of color Doppler videos for diagnosing VHDs.
Methods
The authors developed a 3-stage DL framework for automatic screening of echocardiographic videos for mitral stenosis (MS), mitral regurgitation (MR), aortic stenosis (AS), and aortic regurgitation (AR) that classifies echocardiographic views, detects the presence of VHDs, and, when present, quantifies key metrics related to VHD severities. The algorithm was trained (n = 1,335), validated (n = 311), and tested (n = 434) using retrospectively selected studies from 5 hospitals. A prospectively collected set of 1,374 consecutive echocardiograms served as a real-world test data set.
Results
Disease classification accuracy was high, with areas under the curve of 0.99 (95% CI: 0.97-0.99) for MS; 0.88 (95% CI: 0.86-0.90) for MR; 0.97 (95% CI: 0.95-0.99) for AS; and 0.90 (95% CI: 0.88-0.92) for AR in the prospective test data set. The limits of agreement (LOA) between the DL algorithm and physician estimates of metrics of valve lesion severities compared to the LOAs between 2 experienced physicians spanned from −0.60 to 0.77 cm2 vs −0.48 to 0.44 cm2 for MV area; from −0.27 to 0.25 vs −0.23 to 0.08 for MR jet area/left atrial area; from −0.86 to 0.52 m/s vs −0.48 to 0.54 m/s for peak aortic valve blood flow velocity (Vmax); from −10.6 to 9.5 mm Hg vs −10.2 to 4.9 mm Hg for average peak aortic valve gradient; and from −0.39 to 0.32 vs −0.31 to 0.32 for AR jet width/left ventricular outflow tract diameter.
Conclusions
The proposed deep learning algorithm has the potential to automate and increase efficiency of the clinical workflow for screening echocardiographic images for the presence of VHDs and for quantifying metrics of disease severity.}
}
@article{CHEN2022106826,
title = {Effects of assignments of dedicated automated vehicle lanes and inter-vehicle distances of automated vehicle platoons on car-following performance of nearby manual vehicle drivers},
journal = {Accident Analysis & Prevention},
volume = {177},
pages = {106826},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2022.106826},
url = {https://www.sciencedirect.com/science/article/pii/S0001457522002615},
author = {Facheng Chen and Guangquan Lu and Haitian Tan and Miaomiao Liu and Hongfei Wan},
keywords = {DAVL, AV platoons, Lane assignment, Inter-vehicle distance, AV platoon speed, Car-following performance},
abstract = {Deploying dedicated lanes for automated vehicles (AVs) can effectively alleviate the coordination issues between AVs and manual vehicles (MVs). However, AV platoons running on dedicated AV lanes (DAVLs) have a prominent collective behavior characteristic of small inter-vehicle distance. The nearby MV drivers’ imitation of this characteristic may reduce their car-following time headway (THW). The researchers conducted a simulation experiment to investigate the influence of DAVL assignments, inter-vehicle distances of AV platoons and AV platoon speed on the car-following performance of nearby MV drivers. The data of mean THW, standard deviation of THW, standard deviation of lateral position, standard deviation of velocity, standard deviation of horizontal gaze position and mean saccadic peak velocity were collected from 36 participants. Statistical analysis results show that the three factors considerably affected the MV drivers’ car-following performance. In particular, the MV drivers showed a worse car-following safety but a better driving stability when the left lane was dedicated to AVs than when the right lane was dedicated to AVs (Note the experiments were done in a drive-on-the-left environment.). With respect to the inter-vehicle distances of AV platoons, the MV drivers’ car-following safety was poorer under the 4 m condition than that under the 10 and 18 m conditions. In addition, the MV drivers showed a poorer car-following safety and bore a larger mental workload when driving next to the AV platoons running at 110 km/h. This study may provide some suggestions for DAVLs. Assigning the right lane of a three-lane motorway as the DAVL may have a slighter negative impact on the nearby MV drivers in China. In terms of traffic management in DAVLs, the inter-vehicle distance of AV platoons can be reduced to 10 m, and the speed of AVs should not be higher than the design speed of adjacent MV lanes.}
}
@article{SUDRE2022,
title = {A mega-analytic study of white matter microstructural differences across five cohorts of youth with attention deficit hyperactivity disorder},
journal = {Biological Psychiatry},
year = {2022},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2022.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0006322322016298},
author = {Gustavo Sudre and Luke Norman and Marine Bouyssi-Kobar and Jolie Price and Gauri Shastri and Philip Shaw},
keywords = {ADHD, diffusion tensor imaging, mega-analysis, white matter tracts, big data, fractional anisotropy},
abstract = {Background
While ADHD has been associated with differences in the structural connections formed by the brain’s white matter tracts, studies of such differences have returned inconsistent findings, likely reflecting small sample sizes. Thus, we conducted a mega-analysis on in vivo measures of white matter microstructure obtained through diffusion tensor imaging of over 6000 participants, from five cohorts.
Methods
In a mega-analysis, linear mixed models tested for associations between the fractional anisotropy of 42 white matter tracts and ADHD traits and diagnosis. Contrasts were made against measures of mood, anxiety, and other externalizing problems.
Findings
Overall, 6993 participants (between ages 6 to 18 years, mean 10.62 [SD 1.99]; 3,368 girls, 3,625 boys; 4146 white, non-Hispanic, 764 African American, 2083 other race/ethnicities) had either measures of ADHD and other emotional/behavioral symptoms (N=6933) and/or enough clinical data to allow a diagnosis of ADHD (N=951) or its absence (N=4884). Both the diagnosis and symptoms of ADHD were associated with lower fractional anisotropy of inferior longitudinal and left uncinate fasciculi (at FDR adjusted p<0.05). Associated effect sizes were small (the strongest association with ADHD traits had an effect size of partial-r=-0.14, while the largest case control difference was associated with an effect size of d=-0.3). Similar microstructural anomalies were not present for anxiety, mood, or externalizing problems. Findings held when ADHD cases and controls were matched on in-scanner motion.
Interpretation
While present across cohorts, ADHD-associated microstructural differences had small effects, underscoring the limited clinical utility of this imaging modality in isolation.}
}
@article{WACH20222374,
title = {Determinants of the use of predictive models in the management of investment portfolios, on the example of KGHM Polska Miedź S.A.},
journal = {Procedia Computer Science},
volume = {207},
pages = {2374-2383},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.296},
url = {https://www.sciencedirect.com/science/article/pii/S187705092201184X},
author = {Maciej Wach and Iwona Chomiak-Orsa},
keywords = {Project management, investment projects, predictive analytics},
abstract = {The authors present the determinants of the use of predictive analysis to support decision-making processes in the area of investment project portfolio management. The requirements are analyzed on the example of investment project portfolios of a mining company. The research is complemented by the description of already conducted tests, where predictive models have been created to determine most effective algorithms and key project attributes allowing to predict possible budget deviations. The proposed requirements for the implementation of the predictive analysis can be applied in other organizations managing projects according to project management methodologies with access to structured project data.}
}
@article{LYU2022878,
title = {How accident causation theory can facilitate smart safety management: An application of the 24Model},
journal = {Process Safety and Environmental Protection},
volume = {162},
pages = {878-890},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.04.068},
url = {https://www.sciencedirect.com/science/article/pii/S0957582022003846},
author = {Qian Lyu and Gui Fu and Yuxin Wang and Jing Li and Meng Han and Feng Peng and Chun Yang},
keywords = {Smart safety management, Solution design, Accident causation theory, 24Model},
abstract = {Smart safety management (SSM) in organizations is an inevitable trend in a more intelligent era. However, the adoption of safety science theory lags behind the application of intelligent technology in SSM, posing several challenges (functional dispersion, low-quality data, and lack of versatility). Thus, the accident causation theory (ACT) is adopted to address the existing problem. This study develops a conceptual framework for SSM using the 24Model, a popular ACT in China. The main work conducted in this study is summarized as follows: (a) the description of 24Model and its characteristics, as well as an analysis of its feasibility and applicability in SSM; (b) a detailed presentation of the functions, operation principle, and control paths of unsafe acts in the 24Model-based SSM framework; and (c) a discussion of the framework’s advantages, limitations in this research, and suggestions for future research. Research shows that the SSM framework based on the ACT can integrate the functions of the current SSM, establish management sustainability, enhance data quality, and ensure the versatility of the industry, which are the key factors that facilitate SSM. This study can offer a theoretical and practical basis for safety management in the intelligent era and provide implications for the application of the ACT.}
}
@article{SOLTANI2022100016,
title = {Transfer learning from citizen science photographs enables plant species identification in UAV imagery},
journal = {ISPRS Open Journal of Photogrammetry and Remote Sensing},
volume = {5},
pages = {100016},
year = {2022},
issn = {2667-3932},
doi = {https://doi.org/10.1016/j.ophoto.2022.100016},
url = {https://www.sciencedirect.com/science/article/pii/S2667393222000059},
author = {Salim Soltani and Hannes Feilhauer and Robbert Duker and Teja Kattenborn},
keywords = {Remote sensing, Convolutional Neural Network (CNN), Crowd-sourced data, Plant species, Transfer learning, Drones},
abstract = {Accurate information on the spatial distribution of plant species and communities is in high demand for various fields of application, such as nature conservation, forestry, and agriculture. A series of studies has shown that Convolutional Neural Networks (CNNs) accurately predict plant species and communities in high-resolution remote sensing data, in particular with data at the centimeter scale acquired with Unoccupied Aerial Vehicles (UAV). However, such tasks often require ample training data, which is commonly generated in the field via geocoded in-situ observations or labeling remote sensing data through visual interpretation. Both approaches are laborious and can present a critical bottleneck for CNN applications. An alternative source of training data is given by using knowledge on the appearance of plants in the form of plant photographs from citizen science projects such as the iNaturalist database. Such crowd-sourced plant photographs typically exhibit very different perspectives and great heterogeneity in various aspects, yet the sheer volume of data could reveal great potential for application to bird’s eye views from remote sensing platforms. Here, we explore the potential of transfer learning from such a crowd-sourced data treasure to the remote sensing context. Therefore, we investigate firstly, if we can use crowd-sourced plant photographs for CNN training and subsequent mapping of plant species in high-resolution remote sensing imagery. Secondly, we test if the predictive performance can be increased by a priori selecting photographs that share a more similar perspective to the remote sensing data. We used two case studies to test our proposed approach with multiple RGB orthoimages acquired from UAV with the target plant species Fallopia japonica and Portulacaria afra respectively. Our results demonstrate that CNN models trained with heterogeneous, crowd-sourced plant photographs can indeed predict the target species in UAV orthoimages with surprising accuracy. Filtering the crowd-sourced photographs used for training by acquisition properties increased the predictive performance. This study demonstrates that citizen science data can effectively anticipate a common bottleneck for vegetation assessments and provides an example on how we can effectively harness the ever-increasing availability of crowd-sourced and big data for remote sensing applications.}
}
@article{RIBEIRO2022109674,
title = {Issues with species occurrence data and their impact on extinction risk assessments},
journal = {Biological Conservation},
volume = {273},
pages = {109674},
year = {2022},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2022.109674},
url = {https://www.sciencedirect.com/science/article/pii/S0006320722002270},
author = {Bruno R. Ribeiro and Karlo Guidoni-Martins and Geiziane Tessarolo and Santiago José Elías Velazco and Lucas Jardim and Steven P. Bachman and Rafael Loyola},
keywords = {Biodiversity data, Fitness-for-use, Data quality, GBIF, Plants, Rapid extinction risk assessment},
abstract = {Species extinction risk status is critical to support conservation actions. However, full assessments published on the Red List are slow and resource intensive. To tackle assessments for mega-diverse groups, gains can be made through preliminary assessments that can help prioritize efforts toward full assessments. Here, we quantified how incomplete data collation and errors in the taxonomic, spatial, and temporal dimensions of species-occurrence data translate into misclassifications of extinction risk. Using a dataset of >30 million records of terrestrial plants occurring in Brazil compiled from nine databases we conducted preliminary risk assessments for ~94 % of the 6046 species assessed by the Brazilian Red List authority. We found that no unique database contained data sufficient to perform extinction risk assessment of all species; e.g., the risk of 78 % of species can be assessed using data from GBIF. The overall accuracy (66–75 %) and specificity (89–98 %, correct prediction of non-threatened species) were less affected by incomplete data collation and issues in species-occurrence records. Sensitivity rates (correct prediction of threatened species) were commonly low to moderate and strongly affected by incomplete data collation (13–47 %) and spatial issues (38 %). Our results demonstrate that species' preliminary risk assessments have high accuracy in identifying non-threatened species, even when data collection is low and in the presence of issues in species occurrence data highlighting that such an approach can be used to efficiently prioritize species for full Red List assessments. In addition, caution is needed before declaring a species as threatened without considering data collation intensity and quality.}
}
@article{HUANG2022105152,
title = {Location-Refining neural network: A new deep learning-based framework for Heavy Rainfall Forecast},
journal = {Computers & Geosciences},
volume = {166},
pages = {105152},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105152},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422001078},
author = {Xu Huang and Chuyao Luo and Yunming Ye and Xutao Li and Bowen Zhang},
keywords = {Weather forecast, Radar images prediction, Deep learning, Artificial neural networks},
abstract = {Precipitation nowcasting aims to predict the rainfall distribution within a short-term period. However, it pays the same attention to all locations instead of emphasizing those regions with heavy rainfall that has more threats to human activity. Therefore, we develop an important task named Heavy Rainfall Forecast (HRF), which mainly focuses on the movement and change of heavy rainfall areas. It sets aside one hour to give meteorological administration sufficient time to issue warning information. To tackle this task, firstly, we rebuild the meteorological radar dataset based on three criteria to obtain the samples involving heavy rainfall. Secondly, we propose the Location-Refining (LR) neural network to combine the advantages of the optical flow-based and deep learning-based methods in predicting higher intensity and more accurate position, respectively. LR neural network consists of a location network and a refining network. The former is responsible for the accurate predictions of position and trend of rainfall, and the later accounts for more accurately estimating the intensity. To make the model pay more attention to the high echo region, we design new loss functions and introduce auxiliary information of high echo values. A series of experiments show that our model has a significant improvement on this task. Specifically, compared with existing methods, we improve the valid mean square error by 6.4% for the threshold being 20 and 15.1% for the threshold being 30. The critical success indexes are improved by 12.8% for the threshold being 20 and 24.8% for the threshold being 30. We also improve the heidke skill score by 9.9% for the threshold being 20 and 21.4% for the threshold being 30. Furthermore, the proposed framework can be well transferred to other deep learning-based models, and improves their performance.}
}
@article{BERGMANN20221204,
title = {Tool failure recognition using inconsistent data},
journal = {Procedia CIRP},
volume = {107},
pages = {1204-1209},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.132},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004164},
author = {Júlia Bergmann and Klaudia Éva Zeleny and József Váncza and Andrea Kő},
keywords = {Type your keywords here, separated by semicolons, artificial intelligence, failure detection, data preparation, t-SNE, deep insight, linear programming},
abstract = {Data is everything - at least this is one of the main messages of the ongoing industrial revolution. Manufacturing companies all over the world are expanding their digital infrastructure and knowledge on data analysis in the hope of increasing their KPIs with the help of artificial intelligence (AI). Although several well-designed data-driven solutions are available, the most crucial part, data preparation is still not fully supported. In this paper a framework is presented for processing sensor data of machining processes with variable cycle times in an unstable environment. Traditional and novel AI algorithms are tested on the data of a vulcanization process from the automotive industry, namely from tire manufacturing’s curing phase. The process in question consists of several subprocesses, and the quality of curing is mostly dependent of the status of a specific type of machine tool. Conventional methods (e.g., examining the cured product manually) are currently used for failure recognition, however the examination is only feasible after a long delay due to the extreme level of heat, which leads to unnecessary and unwanted scrap production. Therefore, a more sophisticated and complex approach is required to increase quality score. A combination of mathematical methods is proposed combining t-SNE feature representation, convolutional neural network, and linear programming optimization. The model highly relies on the tool’s continuous degradation characteristics. The threshold for the given binary classification is set by maximizing the accuracy of the detection model. The main contribution of the research is the method of inconsistent sensor data manipulation which supports a unique combination of AI models for early failure recognition.}
}
@article{CHEN2022104445,
title = {A training pattern recognition algorithm based on weight clustering for improving cooling load prediction accuracy of HVAC system},
journal = {Journal of Building Engineering},
volume = {52},
pages = {104445},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.104445},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222004582},
author = {Sihao Chen and Liangzhu (Leon) Wang and Jing Li and Guang Zhou and Xiaoqing Zhou},
keywords = {Cooling load prediction, Pattern recognition, Clustering algorithm, Data preprocessing, Mode identification},
abstract = {The cooling load-based optimal control is an advanced technology for the efficient operation of heating, ventilation, and air conditioning (HVAC). Thus, the prediction reliability of cooling load plays a key role in HVAC's optimal control. Current publications primarily focused on the structure optimization of prediction models, while less on the clustering-based cooling load prediction. However, the data quality determines the upper limit of the model's prediction performance. Thus, a training pattern recognition algorithm based on weight clustering is proposed for improving cooling load prediction accuracy. Compared with the existing clustering-based prediction methods, the main innovations of the proposed method are: (i) considering the input variables' weights on cooling load in the clustering process; and (ii) investigating the matching between the various prediction models and the K-means clustering algorithm. The case studies showed that the proposed method achieves a significant improvement in the prediction performance, such as MAPEs of the MLR, MNR, and ANN decrease by 34.67%, 35.56%, and 14.53% on average, respectively. Compared with the non-weights clustering method, the introduction of the weights can further improve the above models' prediction accuracy, such as their MAPEs decrease by 6.30%, 7.59%, and 3.07% on average, respectively. These results also demonstrated that the clustering-based prediction method is more suitable for the regression models (e.g., MLR and MNR) with low complexity compared to the ANN. When the clustering number is about 4, the models' prediction performances were more robust. Applying the proposed method to the time-series models (i.e., AR, ARX, and ANN) resulted in their MAPEs as low as 1.79%, 1.78%, and 2.06%, respectively. the proposed method can provide a new idea for improving the accuracy of cooling load prediction.}
}
@article{BURKE2022101598,
title = {Tag Frequency Difference: Rapid estimation of image set relevance for species occurrence data using general-purpose image classifiers},
journal = {Ecological Informatics},
volume = {69},
pages = {101598},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101598},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122000474},
author = {Hannah M. Burke and Reid Tingley and Alan Dorin},
keywords = {Biodiversity monitoring, Computer vision, Data mining, iEcology, Social media},
abstract = {iEcology is used to supplement traditional ecological data by sourcing large quantities of media from the internet. Images and their metadata are widely available online and can provide information on species occurrence, behaviour and visible traits. However, this data is inherently noisy and data quality varies significantly between sources. Many iEcology studies utilise data from a single source for simplicity and efficiency. Hence, a tool to compare the suitability of different media sources in addressing a particular research question is needed. We provide a simple, novel way to estimate the fraction of images within multiple unverified datasets that potentially depict a specified target fauna. Our method, the Sum of Tag Frequency Differences (STFD), uses any pretrained, general-purpose image classifier. One of the method's innovations is that it does not require training the classifier to recognise the target fauna. Instead, STFD analyses the frequency of the generic text-tags returned by a classifier for multiple datasets and compares them to the corresponding frequencies of an authoritative image dataset that depicts only the target organism. From this comparison, STFD allows us to deduce the fraction of images of the target in unverified datasets. To validate the STFD approach, we processed images from five sources: Flickr, iNaturalist, Instagram, Reddit and Twitter. For each media source, we conducted an STFD analysis of three fauna invasive to Australia: Cane toads (Rhinella marina), German wasps (Vespula germanica), and the higher-level colloquial taxonomic classification, “wild rabbits”. We found the STFD provided an accurate assessment of image source relevance across all data sources and target organisms. This was demonstrated by the consistent, very strong correlation (toads r ≥0.97, wasps r ≥0.95, wild rabbits≥ 0.95) between STFD predictions, and the fraction of target images in a source dataset observed by a human expert. The STFD provides a low-cost, simple and accurate comparison of the relevance of online image sources to specific fauna for iEcology applications. It does not require expertise in machine learning or training neural-network species-specific classifiers. The method enables researchers to assess multiple image sources to select those warranting detailed investigation for the development of tools for web-scraping, citizen science campaigns, further monitoring or analysis.}
}
@article{KYEONG2022117660,
title = {Mechanism design for data reliability improvement through network-based reasoning model},
journal = {Expert Systems with Applications},
volume = {205},
pages = {117660},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117660},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422009629},
author = {Nohkyum Kyeong and Kihwan Nam},
keywords = {Business intelligence, Data reliability, Network effect, Influence, Recommender system},
abstract = {The importance of business intelligence is increasing every day and many corporations are using data analysis and its results in the decision-making process. Although collecting reliable data is a key prerequisite for the effectiveness of business intelligence, the existing research has focused on improving the reliability of data that is already collected. This research points out the limitations of previous research related to data reliability and presents a new theoretical model that can secure highly reliable data to enhance the business intelligence effect. We designed a mechanism that introduces the concept of the power of influence by using the network effect based reasoning model and applying the two-step flow theory of social exchange theory and information. More than 2 million pieces of real users’ preference data was collected and verified by applying them to a recommendation system. More specifically, I made them recognize what the influence of the users’ preference data input behavior on each individual would be. Also, the limitation on the user input data with the low existing reliability was overcome by applying the recommendation system based on the network effect, which weights the preference data of the users having high influence. As a result, we have shown that the data collection mechanisms based on influence are more efficient in terms of data collection and data analysis.}
}
@article{HOFMAN2022101246,
title = {Distant calibration of low-cost PM and NO2 sensors; evidence from multiple sensor testbeds},
journal = {Atmospheric Pollution Research},
volume = {13},
number = {1},
pages = {101246},
year = {2022},
issn = {1309-1042},
doi = {https://doi.org/10.1016/j.apr.2021.101246},
url = {https://www.sciencedirect.com/science/article/pii/S1309104221003093},
author = {Jelle Hofman and Mania Nikolaou and Sharada Prasad Shantharam and Christophe Stroobants and Sander Weijs and Valerio Panzica {La Manna}},
keywords = {Air quality, Urban, Sensors, Calibration, Cloud, Network},
abstract = {Air quality improved significantly over the past decades. Nevertheless, air pollution continuous to have significant health impacts worldwide. To better assess people's exposure to air pollution, there is a need for higher, more personalized monitoring granularity. IoT sensor technologies can meet these requirements and pave the way towards more fine-grained air quality monitoring, improving our understanding while creating a higher public awareness driving behavioural change. This work tested the validity of scalable PM and NO2 calibration algorithms on various types of sensors (SDS011, OPC-N3, SPS30, NO2-A43F) in five different sensor testbeds deployed at various locations in Belgium and the Netherlands. The calibration models account for sensor gain and offset, while compensating for observed sensitivities of low-cost optical and electrochemical sensors. The calibration improves sensor data considerably (accuracy, linearity and correlation) up to sensitizing and supplementary (EU Class 1) categories at hourly and daily resolutions. Thanks to its cloud implementation and openly available input data, this calibration can be provided “as a service” on top of existing sensor networks in any city, on any sensor. Although distant calibration approaches improve sensor data, the ultimate performance will still depend on the applied sensor type, unit (design of sensor box) and granularity of the available reference monitoring network.}
}
@incollection{KOLTAY20221,
title = {Chapter 1 - Information, data, text, document},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {1-14},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000035},
author = {Tibor Koltay},
keywords = {Information, Data, Text, Document, Big data, Information and data ecosystem, Debates},
abstract = {In this chapter, the most important concepts needed to clarify the subject of this book, such as information, data, and documents, are identified and described. Some of the relationships between them are also described. Many of these concepts are interdisciplinary by their nature, and thus are studied by varied disciplines and tied to a number of professions. Nonetheless, this book, and within it this chapter, focuses mainly on the approaches of library and information science (LIS). The difficulties related to identifying information, data, text, and documents are outlined. The treatment of these issues is detailed, but we did not strive to be exhaustive. Neither did we aspire to find final and definitive answers to all the questions that unavoidably arise surrounding these concepts.}
}
@article{NING2022101808,
title = {Converting street view images to land cover maps for metric mapping: A case study on sidewalk network extraction for the wheelchair users},
journal = {Computers, Environment and Urban Systems},
volume = {95},
pages = {101808},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2022.101808},
url = {https://www.sciencedirect.com/science/article/pii/S0198971522000527},
author = {Huan Ning and Zhenlong Li and Cuizhen Wang and Michael E. Hodgson and Xiao Huang and Xiaoming Li},
keywords = {Street view image, Land cover, Sidewalk, Width, Wheelchair users},
abstract = {Street view images are now widely used in web map services, providing on-site photos of street scenes for users to explore without physically being in the field. These photos record detailed visual information of the street environment with geospatial controls; therefore, they can be used for metric mapping purposes. In this study, we present a method to convert street view images to measurable land cover maps using their associated depthmap data. The proposed method can autonomously extract and measure land cover objects over large areas covered by a mosaic of street view images. In the case study, we demonstrated the use of land cover maps derived from Google Street View images to extract sidewalk features and to measure sidewalk clear widths for wheelchair users. Sidewalk feature slopes were also extracted from the metadata of street view images. Using the Washington D.C., U.S. as the study area, our method extracted a sidewalk network of 2561 km in length with the precision of 0.8662 and recall of 0.8525. The width mean error of extracted sidewalks wide between 1 and 2 m is 0.24 m, and the slope mean error is 0.638°. In Washington D.C., most sidewalks meet the minimum width requirement (0.9 m), but 20% of them have slopes that exceed the maximum allowance (1:20 or about 2.9°). These results demonstrate the converted land cover maps from street view images can be used for metric mapping purposes. The extracted sidewalk network can serve as a valuable inventory for urban planners to promote equitable walkability for mobility disabled users. And if widely available, mobility-impaired users could consult them prior to planning a route.}
}
@article{FLAHERTY2022114546,
title = {The conspiracy of Covid-19 and 5G: Spatial analysis fallacies in the age of data democratization},
journal = {Social Science & Medicine},
volume = {293},
pages = {114546},
year = {2022},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.114546},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621008789},
author = {Eoin Flaherty and Tristan Sturm and Elizabeth Farries},
keywords = {Conspiracy theories, Spatial data, Health geography, Public data, COVID-19, 5G},
abstract = {In a context of mistrust in public health institutions and practices, anti-COVID/vaccination protests and the storming of Congress have illustrated that conspiracy theories are real and immanent threat to health and wellbeing, democracy, and public understanding of science. One manifestation of this is the suggested correlation of COVID-19 with 5G mobile technology. Throughout 2020, this alleged correlation was promoted and distributed widely on social media, often in the form of maps overlaying the distribution of COVID-19 cases with the instillation of 5G towers. These conspiracy theories are not fringe phenomena, and they form part of a growing repertoire for conspiracist activist groups with capacities for organised violence. In this paper, we outline how spatial data have been co-opted, and spatial correlations asserted by conspiracy theorists. We consider the basis of their claims of causal association with reference to three key areas of geographical explanation: (1) how social properties are constituted and how they exert complex causal forces, (2) the pitfalls of correlation with spatial and ecological data, and (3) the challenges of specifying and interpreting causal effects with spatial data. For each, we consider the unique theoretical and technical challenges involved in specifying meaningful correlation, and how their discarding facilitates conspiracist attribution. In doing so, we offer a basis both to interrogate conspiracists’ uses and interpretation of data from elementary principles and offer some cautionary notes on the potential for their future misuse in an age of data democratization. Finally, this paper contributes to work on the basis of conspiracy theories in general, by asserting how – absent an appreciation of these key methodological principles – spatial health data may be especially prone to co-option by conspiracist groups.}
}
@article{HERRMANN2022101716,
title = {The arcanum of artificial intelligence in enterprise applications: Toward a unified framework},
journal = {Journal of Engineering and Technology Management},
volume = {66},
pages = {101716},
year = {2022},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2022.101716},
url = {https://www.sciencedirect.com/science/article/pii/S0923474822000467},
author = {Heinz Herrmann},
keywords = {Artificial intelligence, Unified framework, Systematic review, Science mapping, Systematic science mapping},
abstract = {Disagreement and confusion about artificial intelligence (AI) terminology impede researchers, innovators, and practitioners when developing and implementing enterprise applications. The prevailing ambiguities and use of buzzwords are exacerbated by media and vendor marketing hype. This study identifies several ambiguities within and across AI fields and subfields. Combining a systematic review with a sequential mixed-models design, a total of 26,143 publications were reviewed and mapped, making this the largest conceptual study in the AI field. A unified framework is proposed as an Euler diagram to bring about clarity through a "common language" for AI researchers, innovators, and practitioners.}
}
@article{REID2022,
title = {Harmonising Individual Patient Level Cardiac Registry Data Across the Asia Pacific Region—A Feasibility Study of In-Hospital Outcomes of STEMI Patients From the Asia Pacific Evaluation of Cardiovascular Therapies (ASPECT) Network},
journal = {Heart, Lung and Circulation},
year = {2022},
issn = {1443-9506},
doi = {https://doi.org/10.1016/j.hlc.2022.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S1443950622010848},
author = {Christopher M. Reid and HuiJun Chih and Stephen J. Duffy and Angela L. Brennan and Andrew E. Ajani and John Beltrame and Rosanna Tavella and Bryan P. Yan and Diem Dinh and Chee Tang Chin and Loi Doan Do and Quang Ngoc Nguyen and Hoai T.T. Nguyen and Ika Prasetya Wijaya and Muhammad Yamin and Lusiani Rusdi and Idrus Alwi and Kui Hian Sim and Alan Yean {Yip Fong} and Wan Azman {Wan Ahmad} and Khung Keong Yeo},
keywords = {Cardiovascular outcome, STEMI, Registry, Asia-Pacific},
abstract = {Objective
The Asia-Pacific Evaluation of Cardiovascular Therapies (ASPECT) collaboration was established to inform on percutaneous coronary intervention (PCI) in the Asia-Pacific Region. Our aims were to (i) determine the operational requirements to assemble an international individual patient dataset and validate the processes of governance, data quality and data security, and subsequently (ii) describe the characteristics and outcomes for ST-elevation myocardial infarction (STEMI) patients undergoing PCI in the ASPECT registry.
Methods
Seven (7) ASPECT members were approached to provide a harmonised anonymised dataset from their local registry. Patient characteristics were summarised and associations between the characteristics and in-hospital outcomes for STEMI patients were analysed.
Results
Six (6) participating sites (86%) provided governance approvals for the collation of individual anonymised patient data from 2015 to 2017. Five (5) sites (83%) provided >90% of agreed data elements and 68% of the collated elements had <10% missingness. From the registry (n=12,620), 84% were male. The mean age was 59.2±12.3 years. The Malaysian cohort had a high prevalence of previous myocardial infarction (34%), almost twice that of any other sites (p<0.001). Adverse in-hospital outcomes were the lowest in Hong Kong whilst in-hospital mortality varied from 2.7% in Vietnam to 7.9% in Singapore.
Conclusions
Governance approvals for the collation of individual patient anonymised data was achieved with a high level of data alignment. Secure data transfer process and repository were established. Patient characteristics and presentation varied significantly across the Asia-Pacific region with this likely to be a major predictor of variations in the clinical outcomes observed across the region.}
}
@article{SEGUNDOSEVILLA2022107772,
title = {State-of-the-art of data collection, analytics, and future needs of transmission utilities worldwide to account for the continuous growth of sensing data},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {137},
pages = {107772},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107772},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521009947},
author = {Felix Rafael {Segundo Sevilla} and Yanli Liu and Emilio Barocio and Petr Korba and Manuel Andrade and Federica Bellizio and Jorrit Bos and Balarko Chaudhuri and Hector Chavez and Jochen Cremer and Robert Eriksson and Camille Hamon and Miguel Herrera and Marnick Huijsman and Michael Ingram and Danny Klaar and Venkat Krishnan and Jorge Mola and Marcos Netto and Mario Paolone and Panagiotis Papadopoulos and Miguel Ramirez and Jose Rueda and Walter Sattinger and Vladimir Terzija and Simon Tindemans and Alberto Trigueros and Yajun Wang and Junbo Zhao},
keywords = {Data handling, Data analytics, Phasor measurement units, Wide-area monitoring, System dynamic performance, Stability assessment, Survey, Transmission system operator, Grid operation and management},
abstract = {Nowadays, transmission system operators require higher degree of observability in real-time to gain situational awareness and improve the decision-making process to guarantee a safe and reliable operation. Digitalization of energy systems allows utilities to monitor the system dynamic performance in real-time at fast time scales. The use of such technologies has unlocked new opportunities to introduce new data driven algorithms for improving the stability assessment and control of the system. Motivated by these challenges, a group of experts have worked together to highlight and establish a baseline set of these common concerns, which can be used as motivation to propose innovative analytics and data-driven solutions. In this document, the results of a survey on 10 transmission system operators around the world are presented and it aims to understand the current practices of the participating companies, in terms of data acquisition, handling, storage, modelling and analytics. The overall objective of this document is to capture the actual needs from the interviewed utilities, thereby laying the groundwork for setting valid assumptions for the development of advanced algorithms in this field.}
}
@article{ZHANG2022103476,
title = {An improved method for evaluating eco-driving behavior based-on speed-specific vehicle-specific power distributions},
journal = {Transportation Research Part D: Transport and Environment},
volume = {113},
pages = {103476},
year = {2022},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2022.103476},
url = {https://www.sciencedirect.com/science/article/pii/S1361920922003029},
author = {Leqi Zhang and Zijun Zhu and Zeyu Zhang and Guohua Song and Zhiqiang Zhai and Lei Yu},
keywords = {Eco-Driving, Driving Behavior, Vehicle specific power distributions, Evaluation Method},
abstract = {Eco-driving is the driving behavior that improves fuel efficiency and reduces emissions. It is important to develop a method to accurately evaluate driving behaviors to support eco-driving training. However, the lack of evaluation baseline in the previous studies leads to uncertainty in the eco-driving evaluation under different traffic conditions. This study develops a method for evaluating driving behaviors in relation to the vehicle trajectory data of 19,779 drivers based on speed-specific distributions of vehicle-specific power (VSP), and develops the speed-specific baseline of VSP distributions and the baseline of fuel rates from the drivers’ driving behaviors. The results show the following: (1) the speed-specific VSP distributions can evidently characterize the differences among individuals’ eco-driving behavior; (2) the proposed evaluation method based on the baseline is able to overcome the uncertainty of eco-driving evaluation under different traffic conditions. The findings are helpful in supporting eco-driving training.}
}
@article{CAO2022102731,
title = {An analysis on the role of blockchain-based platforms in agricultural supply chains},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {163},
pages = {102731},
year = {2022},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2022.102731},
url = {https://www.sciencedirect.com/science/article/pii/S1366554522001223},
author = {Yu Cao and Chaoqun Yi and Guangyu Wan and Hanli Hu and Qingsong Li and Shouyang Wang},
keywords = {Blockchain-based platforms, Agricultural supply chain, Financing risk, Counterparty risk, Consumer trust},
abstract = {The traditional agricultural supply chain (ASC) has been overwhelmed by several challenges, including financing risk, counterparty risk, and lack of consumer trust. Platforms based on blockchain technology combined with Internet-of-Things technology have emerged to address these challenges by improving supply chain visibility, guaranteeing the execution of contracts, and increasing the authenticity of products’ provenance information in the ASC. This study analyzes how the adoption of a blockchain-based platform can affect the decisions of ASC participants and identifies how the platform creates value for the supply chain by addressing these three challenges. We consider a two-level supply chain featuring a typical cooperative and a buyer and establish stylized game models with and without the blockchain-based platform. By comparing equilibrium outcomes with and without the blockchain-based platform, we show that the involvement of the blockchain-based platform can lead to increased production quantity and total surplus of the supply chain. This can also motivate more sustainability/green investment to produce greener products. Interestingly, we show that the value of the blockchain-based platform decreases in the credibility of the business environment in which the supply chain operates. Furthermore, the buyer will always benefit from the established blockchain-based platform, whereas the cooperative can benefit in most cases but could be worse off under certain conditions. The adoption and operational costs could outweigh the benefits caused by the addition of the blockchain-based platform.}
}
@article{CHANG2022235,
title = {Information Quality for Effective Asset Management: A literature review},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {235-240},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.213},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014318},
author = {Janet Y. Chang and Jorge Merino Garcia and Xiang Xie and Nicola Moretti and Ajith Parlikad},
keywords = {Information Quality, Data Quality, Asset Information Management, Asset Management, Building Information Modelling},
abstract = {Information quality is critical to successful asset management decision-making. Substandard quality information will likely cause significant negative short- and long-term consequences. The ongoing digital transformation in the Architecture, Engineering, and Construction (AEC) industry has influenced ways to manage physical assets. Yet many asset owners lack a clear understanding of identifying indispensable quality dimensions that satisfy the business, system, and technical requirements. This paper aims to comprehensively analyse asset information quality management with a systematic literature review. The study reveals that the quality dimension of ‘accuracy’ alone cannot support various asset management functions. Additionally, quality deficiencies remain in Building Information Modelling (BIM)-based project delivery handover documents, establishing insufficient asset baselines for future planning. Moreover, the limited knowledge of the quality complications of information generated through technical solutions suggests additional work is required to gain insights into vital quality dimensions. The findings of this study underpin the basis for classifying quality dimensions to support essential asset management processes while pointing to future study areas.}
}
@article{WANG2022103925,
title = {Unsupervised machine learning in urban studies: A systematic review of applications},
journal = {Cities},
volume = {129},
pages = {103925},
year = {2022},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2022.103925},
url = {https://www.sciencedirect.com/science/article/pii/S026427512200364X},
author = {Jing Wang and Filip Biljecki},
keywords = {GeoAI, Urban planning, GIScience, Urban data science, k-means, Latent Dirichlet allocation},
abstract = {Unsupervised learning (UL) has a long and successful history in untangling the complexity of cities. As the counterpart of supervised learning, it discovers patterns from intrinsic data structures without crafted labels, which is believed to be the key to real AI-generated decisions. This paper provides a systematic review of the use of UL in urban studies based on 140 publications. Firstly, the topic, technique, application, data type, and evaluation method of each paper are recorded, deriving statistical insights into the evolution and trends. Clustering is the most prominent method, followed by topic modeling. With the strong momentum of deep learning, a growing application field of UL methods is representing the complex real-world urban systems at multiple scales through multi-source data integration. Subsequently, a detailed review discusses how UL is applied in a broad range of urban topics, which are concluded by four dominant themes: urbanization and regional studies, built environment, urban sustainability, and urban dynamics. Finally, the review addresses common limitations regarding data quality, subjective interpretation, and validation difficulty of the results, which increasingly require interdisciplinary knowledge. Research opportunities are found in the rapidly evolving technological landscape of UL and in certain domains where supervised learning dominates.}
}
@article{MARDIA2022104862,
title = {Principal component analysis and clustering on manifolds},
journal = {Journal of Multivariate Analysis},
volume = {188},
pages = {104862},
year = {2022},
note = {50th Anniversary Jubilee Edition},
issn = {0047-259X},
doi = {https://doi.org/10.1016/j.jmva.2021.104862},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X21001408},
author = {Kanti V. Mardia and Henrik Wiechers and Benjamin Eltzner and Stephan F. Huckemann},
keywords = {Adaptive linkage clustering, Circular mode hunting, Dimension reduction, Multivariate wrapped normal, SARS-CoV-2 geometry, Stratified spheres, Torus PCA},
abstract = {Big data, high dimensional data, sparse data, large scale data, and imaging data are all becoming new frontiers of statistics. Changing technologies have created this flood and have led to a real hunger for new modeling strategies and data analysis by scientists. In many cases data are not Euclidean; for example, in molecular biology, the data sit on manifolds. Even in a simple non-Euclidean manifold (circle), to summarize angles by the arithmetic average cannot make sense and so more care is needed. Thus non-Euclidean settings throw up many major challenges, both mathematical and statistical. This paper will focus on the PCA and clustering methods for some manifolds. Of course, the PCA and clustering methods in multivariate analysis are one of the core topics. We basically deal with two key manifolds from a practical point of view, namely spheres and tori. It is well known that dimension reduction on non-Euclidean manifolds with PCA-like methods has been a challenging task for quite some time but recently there has been some breakthrough. One of them is the idea of nested spheres and another is transforming a torus into a sphere effectively and subsequently use the technology of nested spheres PCA. We also provide a new method of clustering for multivariate analysis which has a fundamental property required for molecular biology that penalizes wrong assignments to avoid chemically no go areas. We give various examples to illustrate these methods. One of the important examples includes dealing with COVID-19 data.}
}
@article{HARTMANN2022101782,
title = {A text and image analysis workflow using citizen science data to extract relevant social media records: Combining red kite observations from Flickr, eBird and iNaturalist},
journal = {Ecological Informatics},
volume = {71},
pages = {101782},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101782},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002321},
author = {Maximilian C. Hartmann and Moritz Schott and Alishiba Dsouza and Yannick Metz and Michele Volpi and Ross S. Purves},
keywords = {User-generated content, Volunteered geographic information, Data integration, Image content analysis, Convolutional neural networks},
abstract = {There is an urgent need to develop new methods to monitor the state of the environment. One potential approach is to use new data sources, such as User-Generated Content, to augment existing approaches. However, to date, studies typically focus on a single date source and modality. We take a new approach, using citizen science records recording sightings of red kites (Milvus milvus) to train and validate a Convolutional Neural Network (CNN) capable of identifying images containing red kites. This CNN is integrated in a sequential workflow which also uses an off-the-shelf bird classifier and text metadata to retrieve observations of red kites in the Chilterns, England. Our workflow reduces an initial set of more than 600,000 images to just 3065 candidate images. Manual inspection of these images shows that our approach has a precision of 0.658. A workflow using only text identifies 14% less images than that including image content analysis, and by combining image and text classifiers we achieve almost perfect precision of 0.992. Images retrieved from social media records complement those recorded by citizen scientists spatially and temporally, and our workflow is sufficiently generic that it can easily be transferred to other species.}
}
@article{LIN2022100440,
title = {DAISM-DNNXMBD: Highly accurate cell type proportion estimation with in silico data augmentation and deep neural networks},
journal = {Patterns},
volume = {3},
number = {3},
pages = {100440},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100440},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922000137},
author = {Yating Lin and Haojun Li and Xu Xiao and Lei Zhang and Kejia Wang and Jingbo Zhao and Minshu Wang and Frank Zheng and Minwei Zhang and Wenxian Yang and Jiahuai Han and Rongshan Yu},
keywords = {cell type proportion estimation, deconvolution, data augmentation, data simulation, deep learning},
abstract = {Summary
Understanding the immune cell abundance of cancer and other disease-related tissues has an important role in guiding disease treatments. Computational cell type proportion estimation methods have been previously developed to derive such information from bulk RNA sequencing data. Unfortunately, our results show that the performance of these methods can be seriously plagued by the mismatch between training data and real-world data. To tackle this issue, we propose the DAISM-DNNXMBD (XMBD: Xiamen Big Data, a biomedical open software initiative in the National Institute for Data Science in Health and Medicine, Xiamen University, China.) (denoted as DAISM-DNN) pipeline that trains a deep neural network (DNN) with dataset-specific training data populated from a certain amount of calibrated samples using DAISM, a novel data augmentation method with an in silico mixing strategy. The evaluation results demonstrate that the DAISM-DNN pipeline outperforms other existing methods consistently and substantially for all the cell types under evaluation in real-world datasets.}
}
@article{SUN2022e10568,
title = {Distribution characteristics of ABO blood groups in China},
journal = {Heliyon},
volume = {8},
number = {9},
pages = {e10568},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e10568},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022018564},
author = {Yang Sun and Liqin Wang and Jiameng Niu and Ting Ma and Lili Xing and Aowei Song and Wenhua Wang and Yuan Shen and Jiangcun Yang},
keywords = {China population, ABO blood group, Phenotype,  frequency, Distribution characteristics},
abstract = {ABO blood groups distribution shows obvious geographical differences globally, but the reliability of the Blood data for assessing relationships between population groups is limited. This is mostly due to the lack of availability and interchange of this important data. We collected data of 23 million ABO blood group population from 34 provincial-level administrative regions in China. To ensure the reliability of the results, we standardized the 23 million data by the China seventh census data. The ranking of ABO blood groups phenotypic distribution in China is O > A > B > AB. The proportions of A, B, O and AB type in China population are 28.72%, 28.17%, 34.20%, and 8.91%, respectively. Accordingly, the frequencies of p [A], q [B], and r [O] gene at the ABO blood group are 0.211, 0.208, and 0.584, respectively. China blood phenotype is dominated by O type, but the r gene frequency is obviously lower than other countries. The distribution of ABO blood groups in China varies geographically. Clustering analysis results show that ABO blood groups divide into four regions from north to south in China, and reveal that the r [O] gene shows an increasing trend from North to South, and conversely the q [B] gene exhibited a decreasing trend at these coordinates. These analyses present interesting characteristics of the blood group distribution across the geography of China.}
}
@article{WU2022101522,
title = {An integrated framework for blockchain-enabled supply chain trust management towards smart manufacturing},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101522},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101522},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002706},
author = {Yue Wu and Yingfeng Zhang},
keywords = {Blockchain, Smart manufacturing, Supply chain, Trust management, Coal mine industry},
abstract = {With the development of a new generation of information technology, smart manufacturing has put forward higher requirements for supply chain. It is necessary to ensure the synchronization of the supply chain operation and maintain the reliability of the supply chain management, therefore the trust evaluation for the supply chain becomes extremely important. Traditional supply chain management has problems such as information flow is easy to be tampered with, logistics is difficult to trace, and capital flow is not true, which leads to increased opportunity costs due to the lack of trust among transaction entities in the supply chain. The emergence of blockchain technology provides an opportunity to improve the supply chain ecosystem. In this paper, an integrated framework for blockchain-enabled supply chain trust management towards smart manufacturing is proposed to explain how to enhance trust management with the help of blockchain from the perspectives of information flow, logistics, and capital flow. An optimized trust management model is designed for better entities evaluation in supply chain. A coal mine equipment manufacturing industry scenario is presented to illustrate the effectiveness of the proposed framework.}
}
@article{SHREE2022108131,
title = {Autonomous development of theoretical framework for intelligence automation system using decision tree algorithm},
journal = {Computers and Electrical Engineering},
volume = {102},
pages = {108131},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108131},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622003810},
author = {S. Raja Shree},
keywords = {Large data classification, Probability statistics, Functional, Differential equations},
abstract = {A novel technique is proposed to arrange enormous information on likelihood which actually depends on the Numerical Model. Having adopted the Two-Fold Development Investigation Model, the non-linear differential conditions are presented using information measurements, characterization of stable and curved capacity. This becomes an ideal estimation technique to build the parameters such as test insights, likelihood, thickness, capacity of the dispersion of information and grouping. The present research also aims to get the information arrangement by Sigma Test, examines the likelihood rule, rejects the span, and demonstrates the solidness of the numerical model & the progressive union. Through the reenactment, information investigation, the available results of the proposed method show that the model precision rate is high, the normal mistake rate is low, and it also becomes acceptable in the context of assembly. Here, the ideal forecast of enormous data is being done by control input arrangement, the ideal target work, utilizing the mayhem factors, nonlinear arbitrary crossing of classification, focus crossing. It also sets up the topological connection between information focuses, and further develops the order calculation. Recreation results show that the proposed calculation can adequately work on the exactness of huge information order and decrease the misclassification rate.}
}
@article{KIM2022104759,
title = {Development and validation of a management system and dataset quality assessment tool for the Radiology Common Data Model (R_CDM): A case study in liver disease},
journal = {International Journal of Medical Informatics},
volume = {162},
pages = {104759},
year = {2022},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104759},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622000739},
author = {Tae-Hoon Kim and SiHyeong Noh and Youe Ree Kim and ChungSub Lee and Ji Eon Kim and Chang-Won Jeong and Kwon-Ha Yoon},
keywords = {Chronic liver disease (CLD), Metadata, Radiology_common data model (R_CDM), Standardization},
abstract = {Background
The Observational Medical Outcomes Partnership—Common Data Model (OMOP-CDM), a distributed research network, has low clinical data coverage. Radiological data are valuable, but imaging metadata are often incomplete, and a standardized recording format in the OMOP-CDM is lacking. We developed a web-based management system and data quality assessment (RQA) tool for a radiology_CDM (R_CDM) and evaluated the feasibility of clinically applying this dataset.
Methods
We designed an R_CDM with Radiology_Occurrence and Radiology_Image tables. This was seamlessly linked to the OMOP-CDM clinical data. We adopted the standardized terminology using the RadLex playbook and mapped 5,753 radiology protocol terms to the OMOP vocabulary. An extract, transform, and load (ETL) process was developed to extract detailed information that was difficult to extract from metadata and to compensate for missing values. Image-based quantification was performed to measure liver surface nodularity (LSN), using customized Wonkwang abdomen and liver total solution (WALTS) software.
Results
On a PACS, 368,333,676 DICOM files (1,001,797 cases) were converted to R_CDM chronic liver disease (CLD) data (316,596 MR images, 228 cases; 926,753 CT images, 782 cases) and uploaded to a web-based management system. Acquisition date and resolution were extracted accurately, but other information, such as “contrast administration status” and “photography direction”, could not be extracted from the metadata. Using WALTS, 9,609 pre-contrast axial-plane abdominal MR images (197 CLD cases) were assigned LSN scores by METAVIR fibrosis grades, which differed significantly by ANOVA (p < 0.001). The mean RQA score (83.5) indicated good quality.
Conclusion
This study developed a web-based system for management of the R_CDM dataset, RQA tool, and constructed a CLD R_CDM dataset, with good quality for clinical application. Our management system and R_CDM CLD dataset would be useful for multicentric and image-based quantification researches.}
}
@article{ASGARIMEHR2022112801,
title = {GNSS reflectometry global ocean wind speed using deep learning: Development and assessment of CyGNSSnet},
journal = {Remote Sensing of Environment},
volume = {269},
pages = {112801},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112801},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721005216},
author = {Milad Asgarimehr and Caroline Arnold and Tobias Weigel and Chris Ruf and Jens Wickert},
keywords = {Wind speed, Ocean, GNSS reflectometry, CyGNSS, Deep learning, CNN, Fully connected layers},
abstract = {GNSS Reflectometry (GNSS-R) is a novel remote sensing technique for the monitoring of geophysical parameters using reflected GNSS signals from the Earth's surface. Ocean wind speed monitoring is the main objective of the recently launched Cyclone GNSS (CyGNSS), a GNSS-R constellation of eight microsatellites, launched in late 2016. In this study, the capability of deep learning, especially, for an operational wind speed data derivation from the measured Delay-Doppler Maps (DDMs) is characterized. CyGNSSnet is based on convolutional layers for the feature extraction from bistatic radar cross section (BRCS) DDMs, along with fully connected layers for processing ancillary technical and higher-level input parameters. The best architecture is determined on a validation set and is evaluated over a completely blind dataset from a different time span than that of the training data to validate the generality of the model for operational usage. After a data quality control, CyGNSSnet results in an RMSE of 1.36 m/s leading to a significant improvement by 28% in comparison to the officially operational retrieval algorithm. The RMSE is the lowest among those seen in the literature for any conventional or machine learning-based algorithm. The benefits of the convolutional layers, the advantages and weaknesses of the model are discussed. CyGNSSnet offers efficient processing of GNSS-R measurements for high-quality global ocean winds.}
}
@incollection{GRIMALDI202247,
title = {Chapter 2 - Governance, decision-making, and strategy for urban development},
editor = {Didier Grimaldi and Carlos Carrasco-Farré},
booktitle = {Implementing Data-Driven Strategies in Smart Cities},
publisher = {Elsevier},
pages = {47-87},
year = {2022},
isbn = {978-0-12-821122-9},
doi = {https://doi.org/10.1016/B978-0-12-821122-9.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211229000014},
author = {Didier Grimaldi and Eula Bianca Villar and Laurent Dupont and Jose M. Sallan and Carlos Carrasco-Farré},
keywords = {Bibliometric methodology, VOSviewer software, Sustainability, PDRF, Stakeholders, Multiple sectors, Urban development},
abstract = {This chapter starts with a systematic review of the academic state of art about the trending topics that deal with data-driven approaches in an urban environment. Then, we select a specific example and analyze recent progress in the use of data to improve the urban process, which is as strategic and a priority as the resilience process. In this vein, we then use the case of the Philippines and show their data-driven policy to cope with disasters that drastically affect their urban areas.}
}
@article{DU2022130798,
title = {Life cycle assessment of recycled NiCoMn ternary cathode materials prepared by hydrometallurgical technology for power batteries in China},
journal = {Journal of Cleaner Production},
volume = {340},
pages = {130798},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130798},
url = {https://www.sciencedirect.com/science/article/pii/S095965262200436X},
author = {Shiwei Du and Feng Gao and Zuoren Nie and Yu Liu and Boxue Sun and Xianzheng Gong},
keywords = {EVs, End-of-life power battery, Life cycle assessment, Environmental improvement potential, Recycled ternary cathode materials},
abstract = {Power lithium-ion batteries (LIBs) are core components of electric vehicles (EVs), and the cathode material is the key to the performance of LIBs. Nickel-cobalt-manganese oxide (NCM) cathode formulations have emerged as dominant choices in the battery industry. This work presents a life cycle assessment of recycled NCM ternary cathode materials produced from spent batteries in China. The environmental impacts of virgin and recycled material production were compared based on the ReCiPe 2016 method. The results demonstrated that the highest environmental pressure was generated during the leaching and extraction process due to the high consumption of electricity and auxiliary materials, which contributed nearly half to all three endpoint impact categories. Sensitivity analysis revealed that the environmental impacts of the leaching and extraction process could be effectively reduced by optimizing the production process to reduce the consumption of sulfuric acid, electricity, hydrogen peroxide, and sodium hydroxide. The comparative results indicated that the production of recycled NCM materials consumes 74% less energy, and compared to virgin NCM materials, the three endpoint environmental impact categories are reduced by 72%, 59% and 57%. Comparison of the global warming potential (GWP) between different recycling techniques in the literature indicated that the GWP of hydrometallurgical technology is lower. In addition, we estimated the GWP reduction potential per kg under optimized power scenarios. The annual GWP reduction and energy savings benefits of recycling in China from 2021 to 2035 were predicted. Based on the results, it is necessary to maintain high recycling rates through a variety of initiatives. This Chinese case study demonstrated that the adoption of cathode material production by recycling spent power LIBs through cleaner technology is of great practical significance for resource conservation and sustainable development of the EV industry.}
}
@article{BRESTER20221,
title = {Epidemiological predictive modeling: lessons learned from the Kuopio ischemic heart disease risk factor study☆},
journal = {Annals of Epidemiology},
volume = {70},
pages = {1-8},
year = {2022},
issn = {1047-2797},
doi = {https://doi.org/10.1016/j.annepidem.2022.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1047279722000448},
author = {Christina Brester and Ari Voutilainen and Tomi-Pekka Tuomainen and Jussi Kauhanen and Mikko Kolehmainen},
keywords = {Machine learning, Prediction of cardiovascular death, Population study, Epidemiology},
abstract = {ABSTRACT
Purpose
The use of predictive models in epidemiology is relatively narrow as most of the studies report results of traditional statistical models such as Linear, Logistic, or Cox regressions. In this study, a high-dimensional epidemiological cohort, collected within the Kuopio Ischemic Heart Disease Risk Factor Study in 1984–1989, was used to investigate the predictive ability of models with embedded variable selection.
Methods
Simple Logistic Regression with seven preselected risk factors was compared to k-Nearest Neighbors, Logistic Lasso Regression, Decision Tree, Random Forest, and Multilayer Perceptron in predicting cardiovascular death for the aged men from Kuopio Ischemic Heart Disease Risk Factor for the long horizon of 30 ± 3 years: 746 predictor variables were available for 2682 men (705 cardiovascular deaths were registered). We considered two scenarios of handling competing risks (removing subjects and treating them as non-cases).
Results
The best average AUC on the test sample was 0.8075 (95%CI, 0.8051–0.8099) in scenario 1 and 0.7155 (95%CI, 0.7128–0.7183) in scenario 2 achieved with Logistic Lasso Regression, which was 6.04% and 5.50% higher than the baseline AUC provided by Logistic Regression with manually preselected predictors.
Conclusions
In both scenarios Logistic Lasso Regression, Random Forest, and Multilayer Perceptron outperformed Simple Logistic Regression.}
}
@article{AKHMATOVA20221512,
title = {Integrating quality management systems (TQM) in the digital age of intelligent transportation systems industry 4.0},
journal = {Transportation Research Procedia},
volume = {63},
pages = {1512-1520},
year = {2022},
note = {X International Scientific Siberian Transport Forum — TransSiberia 2022},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.06.163},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522004185},
author = {Malika-Sofi Akhmatova and Antonina Deniskina and Dzhennet-Mari Akhmatova and Larisa Prykina},
keywords = {TQM, Industry 4.0, digitalization, quality, improvement},
abstract = {The rapid expansion of Total Quality Management (TQM) was a response to the challenges posed by increased levels of competition in the global market and heightened attention to issues of quality planning, assurance, control, and improvement. Currently, entering the digital age and the advancement of human life in every field affect the development of TQM through the diversification of Industry 4.0 techniques and applications. This article is to explore the digital concepts relevant for TQM and identify possible challenges emerging while implementing these concepts in practice. In line with this, this article integrates three stages, thus filling in gaps in the existing research. First of all, it tracks the transition from the concept of quality control to digital-friendly TQM, highlighting the meaning of quality, specific features of TQM development, and breakthroughs in the history of TQM. It is noted that the contemporary TQM represents quality as a category open to the achievements of scientific and technological progress that can assist in meeting the customers’ expectations and attaining competitiveness. Second, the article analyzes the TQM in the context of the fourth industrial revolution. Finally, the research results emphasize the most distressing issues faced by quality management systems (QMS) in the digital age and suggest recommendations to combat them.}
}
@article{FERNANDEZBASSO2022108870,
title = {A fuzzy-based medical system for pattern mining in a distributed environment: Application to diagnostic and co-morbidity},
journal = {Applied Soft Computing},
volume = {122},
pages = {108870},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108870},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002538},
author = {Carlos Fernandez-Basso and Karel Gutiérrez-Batista and Roberto Morcillo-Jiménez and Maria-Amparo Vila and Maria J. Martin-Bautista},
keywords = {Association rules, Fuzzy logic, Data mining, Medical records},
abstract = {In this paper we have addressed the extraction of hidden knowledge from medical records using data mining techniques such as association rules in conjunction with fuzzy logic in a distributed environment. A significant challenge in this domain is that although there are a lot of studies devoted to analysing health data, very few focus on the understanding and interpretability of the data and the hidden patterns present within the data. A major challenge in this area is that many health data analysis studies have focussed on classification, prediction or knowledge extraction and end users find little interpretability or understanding of the results. This is due to the use of black-box algorithms or because the nature of the data is not represented correctly. This is why it is necessary to focus the analysis not only on knowledge extraction but also on the transformation and processing of the data to improve the modelling of the nature of the data. Techniques such as association rule mining and fuzzy logic help to improve the interpretability of the data and treat it with the inherent uncertainty of real-world data. To this end, we propose a system that automatically: a) pre-processes the database by transforming and adapting the data for the data mining process and enriching the data to generate more interesting patterns, b) performs the fuzzification of the medical database to represent and analyse real-world medical data with its inherent uncertainty, c) discovers interrelations and patterns amongst different features (diagnostic, hospital discharge, etc.), and d) visualizes the obtained results efficiently to facilitate the analysis and improve the interpretability of the information extracted. Our proposed system yields a significant increase in the compression and interpretability of medical data for end-users, allowing them to analyse the data correctly and make the right decisions. We present one practical case using two health-related datasets to demonstrate the feasibility of our proposal for real data.}
}
@article{GUO2022128245,
title = {Consistency and uncertainty of gridded terrestrial evapotranspiration estimations over China},
journal = {Journal of Hydrology},
volume = {612},
pages = {128245},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2022.128245},
url = {https://www.sciencedirect.com/science/article/pii/S0022169422008174},
author = {Linan Guo and Yanhong Wu and Hongxing Zheng and Bing Zhang and Lanxin Fan and Haojing Chi and Bokun Yan and Xiaoqi Wang},
keywords = {Terrestrial evapotranspiration, Remote sensing, Consistency and uncertainty, China},
abstract = {Terrestrial evapotranspiration (ET) is a critical process in water, energy and carbon cycles but challenging to estimate. Several gridded terrestrial ET products have been developed for regional or global applications based on different algorithms and forcing inputs and with different temporal-spatial resolutions and accuracy. This study systematically investigates the consistency and uncertainty of eight ET products over China for the period from 2003 to 2014. Temporal and spatial pairwise correlation analysis indicates that the ET products are largely consistent with each other, among which the ET estimates from GLEAM and EB-ET presenting highest spatial consistency with each other (R2 = 0.91). Compared to the ground-based observations from the flux network, the centered root-mean-squared-deviation (RMSD) for each of the eight products is found lower than 30 mm/month, according to which CR and PMLv2 outperform the others. The ET products however show considerable difference in the estimated mean annual ET averaged over China (from 378 mm/a to 528 mm/a). Uncertainty assessment based on the extended collocation method (ECM) suggests the uncertainties in ET estimates are more evident in the arid region than in the humid region. It also indicates that model averaging (like that in generating the GLASS ET product) could substantially reduce the uncertainties. The findings of this research could facilitate the application of the ET products to assess water balance dynamics over China while considering their uncertainties.}
}
@article{SADRI2022100629,
title = {Data reduction in fog computing and internet of things: A systematic literature survey},
journal = {Internet of Things},
volume = {20},
pages = {100629},
year = {2022},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2022.100629},
url = {https://www.sciencedirect.com/science/article/pii/S254266052200110X},
author = {Ali Akbar Sadri and Amir Masoud Rahmani and Morteza Saberikamarposhti and Mehdi Hosseinzadeh},
keywords = {Data reduction, Fog computing, Edge computing, Internet of Things},
abstract = {Cloud computing, the most crucial computing and storage tool in IoT (Internet of Things), still meets various challenges. The remoteness of IoT end devices from cloud platforms may lead to significant issues for real-time applications such as disaster handling, healthcare application, etc. In order to address these issues, fog computing is a new platform with specialized features that perform essential IoT data management and real-time application management tasks. Data management in IoT via fog computing is critical for decreasing latency in real-time IoT applications and is needed to produce more professional knowledge and smart decisions. Reduction in the size of data sent to the cloud layer is a fundamental topic in data management on the fog computing platform. In this paper, to select and survey studies about data size reduction in fog computing, we applied the Systematic Literature Review (SLR) process to comprehend and classify the different topics and related approaches in this field. In addition, the studies presented in the edge computing field, which were close to our goal, were also investigated. The primary purpose of this study is to classify and analyze the fog data reduction (FDR) studies published between 2016 and 2022. The topics and related approaches of selected papers are presented in an approach-based taxonomy. The topics of provided taxonomy include data filtering, data compression, data aggregation, data prediction, data pattern recognition, and general facets and related approaches. Finally, open issues for FDR and relevant main challenges are presented in upcoming research.}
}