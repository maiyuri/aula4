@inproceedings{10.1145/2910896.2926735,
author = {Fox, Edward A. and Xie, Zhiwu and Klein, Martin},
title = {WADL 2016: Third International Workshop on Web Archiving and Digital Libraries},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2926735},
doi = {10.1145/2910896.2926735},
abstract = {This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {293–294},
numpages = {2},
keywords = {web archiving, internet archive},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.5555/3200334.3200410,
author = {Fox, Edward A. and Xie, Zhiwu and Klein, Martin},
title = {Web Archiving and Digital Libraries (WADL)},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {352–353},
numpages = {2},
keywords = {web archiving, internet archive},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.1145/3482632.3482675,
author = {Sun, Wen},
title = {Cloud Service Context and Feedback Fusion of Product Design Creative Demand Perception Technology},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482675},
doi = {10.1145/3482632.3482675},
abstract = {After experiencing the stages of document delivery service, information service and knowledge service, the traditional product design creative demand perception technology has gradually transformed to the cloud service stage driven by new technologies such as big data and cloud computing. With the advent of the era of big data and artificial intelligence, multi-source heterogeneous and massive data resources have specific fusion characteristics and application trends. The generation of new artificial intelligence technologies and methods is to respond to the above characteristics and trends. Multi-source heterogeneous resources and vast amounts of data driven product design requirements perception from the user requirements perception, perception technology content and creative product design requirements capturing perception technology scenario-based push these three core function implementation requirements perception technology cloud service mode, implement the product design requirements perception technology innovation process. Of this study was to study the cloud service situation and feedback the fusion of product design requirements perception technology, cloud service components analysis demand perception technology and its features, with case studies to explore data driven era product design demand perception technology to the cloud service model transformation of ideas, requirements for perception technology transformation provides scientific theory and practice.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {207–213},
numpages = {7},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3423603.3424004,
author = {Darmont, J\'{e}r\^{o}me and Favre, C\'{e}cile and Loudcher, Sabine and No\^{u}s, Camille},
title = {Data Lakes for Digital Humanities},
year = {2020},
isbn = {9781405377539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423603.3424004},
doi = {10.1145/3423603.3424004},
abstract = {Traditional data in Digital Humanities projects bear various formats (structured, semi-structured, textual) and need substantial transformations (encoding and tagging, stemming, lemmatization, etc.) to be managed and analyzed. To fully master this process, we propose the use of data lakes as a solution to data siloing and big data variety problems. We describe data lake projects we currently run in close collaboration with researchers in humanities and social sciences and discuss the lessons learned running these projects.},
booktitle = {Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress},
articleno = {6},
numpages = {4},
keywords = {digital humanities, data lakes, metadata},
location = {Virtual Event, Tunisia},
series = {DTUC '20}
}

@inproceedings{10.1145/3144826.3145387,
author = {Moreira, Fernando and Gon\c{c}alves, Ramiro and Martins, Jos\'{e} and Branco, Frederico and Au-Yong-Oliveira, Manuel},
title = {Learning Analytics as a Core Component for Higher Education Disruption: Governance Stakeholder},
year = {2017},
isbn = {9781450353861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144826.3145387},
doi = {10.1145/3144826.3145387},
abstract = {Higher education institutions are at this stage, on the one hand, faced with challenges never seen before and, on the other hand, their action is moving very rapidly into digital learning spaces. These challenges are increasingly complex because of the global competition for resources, students and teachers. In addition, the amount of data produced inside and outside higher education institutions has grown exponentially, so more and more institutions are exploring the potential of Big Data to meet these challenges. In this context, higher education institutions and key stakeholders (students, teachers, and governance) can derive multiple benefits from learning analytics using different data analysis strategies to produce summative, real-time and predictive information and recommendations. However, it may be questioned whether institutions, academic administrative staff as well as including those with responsibility for governance, are prepared for learning analytics? As a response to the question raised in this paper is presented an extension of a disruptive conceptual approach to higher education, using information gathered by IoT and based on Big Data &amp; Cloud Computing and Learning Analytics analysis tools, with the main focus on the stakeholder governance.},
booktitle = {Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality},
articleno = {37},
numpages = {8},
keywords = {Disruption, Learning Analytics, Governance, Higher Education Institutions},
location = {C\'{a}diz, Spain},
series = {TEEM 2017}
}

@inproceedings{10.1145/3106426.3106436,
author = {Saberi, Morteza and Hussain, Omar K. and Chang, Elizabeth},
title = {An Online Statistical Quality Control Framework for Performance Management in Crowdsourcing},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106436},
doi = {10.1145/3106426.3106436},
abstract = {The big data research topic has grown rapidly for the past decade due to the advent of the "data deluge". Recent advancements in the literature leverage human computing power known as crowdsourcing to manage and harness big data for various applications. However, human involvement in the completion of crowdsourcing tasks is an error-prone process that affects the overall performance of the crowd. Thus, controlling the quality of workers is an essential step for crowdsourcing systems, which due to unavailability of ground-truth data for any task at hand becomes increasingly challenging. To propose a solution to this problem, in this study, we propose OSQC (Online Statistical Quality Control Framework) for managing the performance of workers in crowdsourcing. OSQC ascertains the worker's performance by using a statistical model and then leverages the traditional statistical control techniques to decide whether to retain a worker for crowdsourcing or to evict him. We evaluate our proposed framework on a real dataset and demonstrate how OSQC assists crowdsourcing to maintain its accuracy.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {476–482},
numpages = {7},
keywords = {multiple choice HIT, crowd workers, crowdsourcing management, statistical quality control},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3093241.3093268,
author = {Smith, Jeffrey and Rege, Manjeet},
title = {The Data Warehousing (R) Evolution: Where's It Headed Next?},
year = {2017},
isbn = {9781450352413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093241.3093268},
doi = {10.1145/3093241.3093268},
abstract = {This paper provides an overview of the history and current state of data warehousing and corporate analytics. It begins with a quick review of the history of the data warehouse and then does a deeper dive into subsets of this space including data integration, the DBMS, business intelligence and analytics, advanced analytics, and information stewardship. It finishes with a quick review of some of the leading trends in data warehousing including Big Data and the Logical Data Warehouse, Hybrid Transaction Analytical Processing and In-Memory Computing.},
booktitle = {Proceedings of the International Conference on Compute and Data Analysis},
pages = {104–108},
numpages = {5},
keywords = {intelligence, Data, business, ETL, warehouse},
location = {Lakeland, FL, USA},
series = {ICCDA '17}
}

@inproceedings{10.1145/3183713.3183753,
author = {Fan, Wenfei and Liu, Xueli and Lu, Ping and Tian, Chao},
title = {Catching Numeric Inconsistencies in Graphs},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183753},
doi = {10.1145/3183713.3183753},
abstract = {Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we propose to extend graph functional dependencies with linear arithmetic expressions and comparison predicates, referred to as NGDs. We study fundamental problems for NGDs. We show that their satisfiability, implication and validation problems are Σ 2 p-complete, ¶II2 p-complete and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity.To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs, in response to updates Δ G to G. We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in Δ G instead of the entire G. Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {381–393},
numpages = {13},
keywords = {incremental validation, graph dependencies, numeric errors},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.5555/2602339.2602400,
author = {Petrou, Lambros and Larkou, George and Laoudias, Christos and Zeinalipour-Yazti, Demetrios and Panayiotou, Christos G.},
title = {Demonstration Abstract: Crowdsourced Indoor Localization and Navigation with Anyplace},
year = {2014},
isbn = {9781479931460},
publisher = {IEEE Press},
abstract = {In this demonstration paper, we present the Anyplace system that relies on the abundance of sensory data on smartphones (e.g., WiFi signal strength and inertial measurements) to deliver reliable indoor geolocation information. Our system features two highly desirable properties, namely crowdsourcing and scalability. Anyplace implements a set of crowdsourcing-supportive mechanisms to handle the enormous amount of crowdsensed data, filter incorrect user contributions and exploit WiFi data from heterogeneous mobile devices. Moreover, Anyplace follows a big-data architecture for efficient and scalable storage and retrieval of localization and mapping data.},
booktitle = {Proceedings of the 13th International Symposium on Information Processing in Sensor Networks},
pages = {331–332},
numpages = {2},
keywords = {crowdsourcing, navigation, indoor localization},
location = {Berlin, Germany},
series = {IPSN '14}
}

@inproceedings{10.1145/3500931.3501016,
author = {Gao, Mengke and Zhang, Yan and Gao, Yue},
title = {Research Progress of User Portrait Technology in Medical Field},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3501016},
doi = {10.1145/3500931.3501016},
abstract = {In recent years, due to the rise of big data mining and intelligent recommendation, user portrait technology has gradually become a hot topic. As a new data analysis method, user portrait technology aims to mine user characteristics from a large number of user behavior data, and complete the user information panorama of the monomer or group through information mining, so as to prepare for the realization of precision service and personalized recommendation in various industries. It has been widely used in personalized recommendation and precision recommendation, personalized service and intelligent service, group characteristics analysis, prediction analysis and auxiliary decision-making. User portrait technology in the domestic research started late, currently in e-commerce, commercial precision marketing, book recommendation management, network personalized search, video entertainment and other fields of application is relatively mature. However, the application of user portrait technology in the field of medical and health is still in the preliminary exploration stage, and the combination of medical big data and user portrait technology can vividly depict the portraits of patients, doctors, residents and other different groups in promoting health and disease prevention. According to the different characteristics extracted, it can provide reference for meeting the needs of patients and achieving precision medicine. Therefore, this study reviews the concept, elements, implementation process and application of user portrait technology in the medical field, and provides reference for the subsequent application research of user portrait technology in the medical field},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {500–504},
numpages = {5},
keywords = {User portrait, Medical treatment, Review},
location = {Beijing, China},
series = {ISAIMS 2021}
}

@inproceedings{10.1145/3230833.3233288,
author = {Jirovsk\'{y}, V\'{a}clav and Pastorek, Andrej and M\"{u}hlh\"{a}user, Max and Tundis, Andrea},
title = {Cybercrime and Organized Crime},
year = {2018},
isbn = {9781450364485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230833.3233288},
doi = {10.1145/3230833.3233288},
abstract = {The way of live in the modern society has changed radically over the past few decades. In particular, thanks to the strong use of information technology, many activities have moved from the real world to the digital world. This has obviously introduced advantages in terms of data management and communication efficiency. Nevertheless, it has given also to the criminals the possibility to move into cybernetic space and, as a consequence, to exploit all the technological advantages available for carrying out their activities. In this context the paper provide an overview on the cybercrime and organized crime by focusing on the concept of crime as a service as well as the main issues related to big data by highlighting the social aspects.},
booktitle = {Proceedings of the 13th International Conference on Availability, Reliability and Security},
articleno = {61},
numpages = {5},
keywords = {Cyber-security, Cyber-crime, Privacy, Data Security},
location = {Hamburg, Germany},
series = {ARES 2018}
}

@inproceedings{10.1145/3079856.3080241,
author = {Boyapati, Rahul and Huang, Jiayi and Majumder, Pritam and Yum, Ki Hwan and Kim, Eun Jung},
title = {APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080241},
doi = {10.1145/3079856.3080241},
abstract = {The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.Our evaluation shows that APPROX-NoC achieves on average up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while maintaining low application error. Additionally, with a data intensive graph processing application we achieve a 36.7% latency reduction compared to state-of-the-art compression mechanisms.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {666–677},
numpages = {12},
keywords = {Networks-On-Chip, Data Compression, Approximate Computing},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{10.1145/3140659.3080241,
author = {Boyapati, Rahul and Huang, Jiayi and Majumder, Pritam and Yum, Ki Hwan and Kim, Eun Jung},
title = {APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080241},
doi = {10.1145/3140659.3080241},
abstract = {The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.Our evaluation shows that APPROX-NoC achieves on average up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while maintaining low application error. Additionally, with a data intensive graph processing application we achieve a 36.7% latency reduction compared to state-of-the-art compression mechanisms.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {666–677},
numpages = {12},
keywords = {Data Compression, Approximate Computing, Networks-On-Chip}
}

@inproceedings{10.1145/3340531.3418506,
author = {Huang, Ruihong},
title = {Approximate Event Pattern Matching over Heterogeneous and Dirty Sources},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3418506},
doi = {10.1145/3340531.3418506},
abstract = {Pattern matching is an important task in the field of Complex Event Processing (CEP). However, exact event pattern matching methods could suffer from low hit rate and loss for meaningful events identification due to the heterogeneous and dirty sources in the big data era. Since both events and patterns could be imprecise, the actual event trace may have different event names as well as structures from the pre-defined pattern. The low-quality data even intensifies the difficulty of matching. In this work, we propose to learn embedding representations for patterns and event traces separately and calculate their similarity as the scores for approximate matching.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3237–3240},
numpages = {4},
keywords = {low-quality data, cep, heterogeneous source, dirty source, approximate match, complex event processing},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/2851581.2892379,
author = {Verma, Nitya and Voida, Amy},
title = {Mythologies of Business Intelligence},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892379},
doi = {10.1145/2851581.2892379},
abstract = {We present results from a case study of the use of business intelligence (BI) systems in a human services organization. In their organizational trajectory towards a "culture of data," our informants perceived four values associated with BI: data-driven, predictive and proactive, shared accountability, and inquisitive. Each value corresponds to a mythology of big data and BI. For each, we highlight the ways in which the enactment of the mythology is problematized by disconnects between aggregate and drill-down views of data that often impede the desired actionability. Our findings contribute initial empirical evidence of the ways in which the epistemological biases of BI systems influence organizations. We suggest design implications for better enabling data-driven decision making.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {2341–2347},
numpages = {7},
keywords = {mythology, business intelligence, analytics, values},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{10.1109/MET.2019.00018,
author = {Yan, Boyang and Yecies, Brian and Zhou, Zhi Quan},
title = {Metamorphic Relations for Data Validation: A Case Study of Translated Text Messages},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MET.2019.00018},
doi = {10.1109/MET.2019.00018},
abstract = {In conventional metamorphic testing, metamorphic relations (MRs) are identified as necessary properties of a computer program's intended functionality, whereby violations of MRs reveal faults in the program---under the assumption that the source and follow-up inputs (test cases used in metamorphic testing) are valid. In the present study, the authors argue that MRs can also be used to validate and assess the quality of the program's input data---under the assumption that the source or follow-up inputs can be inappropriately generated. Using this new perspective, a case study in the natural language processing domain is used to explore the different types of text messages that are difficult to interpret by (Chinese-English) machine translation. A total of 46,180 short user comments on Personal Tailor (a 2013 Chinese film), collected from Douban (a popular Chinese social media platform), has been used as the primary dataset of this study, and the analysis of results demonstrates that the proposed MR-based data validation method is useful for the automatic identification of poorly translated text messages.},
booktitle = {Proceedings of the 4th International Workshop on Metamorphic Testing},
pages = {70–75},
numpages = {6},
keywords = {Oracle problem, data quality assessment, metamorphic testing, data validation, natural language processing, metamorphic relation, Douban, machine translation, sentiment analysis, social media},
location = {Montreal, Quebec, Canada},
series = {MET '19}
}

@inproceedings{10.1145/3335550.3335577,
author = {Li, Ruixue and Peng, Can and Sun, Huiliang},
title = {Product Selection Strategy Analysis of Crowdsourcing Platform from the Full Cost Perspective},
year = {2019},
isbn = {9781450362641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335550.3335577},
doi = {10.1145/3335550.3335577},
abstract = {From the perspective of full cost, this paper uses Coase's transaction cost theory to analyze the causes of crowdsourcing, and on this basis to analyze the applicability of crowdsourcing platform products. At the same time, based on the crowdsourcing platform--zbj.com, we use the big data technology to grasp and analyze the related data of the crowdsourcing platform's successful cases in the past five months, and use the relevant statistical analysis method to categorize and analyze the industry attributes of the top five orders of the success cases of the zbj.com, in order to verify the theory mentioned in the article.},
booktitle = {Proceedings of the 2019 International Conference on Management Science and Industrial Engineering},
pages = {92–97},
numpages = {6},
keywords = {Crowdsourcing platform, Selection Strategy Analysis, Full cost, Appropriate products},
location = {Phuket, Thailand},
series = {MSIE 2019}
}

@inproceedings{10.1145/3301761.3301767,
author = {Tang, Haijing and Zhou, Yangdong and Yang, Xu and Gao, Keyan and Zheng, Wenhao and Zhao, Jinfeng},
title = {Adopting Data Analysis and Visualization Technology to Construct Clinical Research Data Management and Analysis System},
year = {2018},
isbn = {9781450361279},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301761.3301767},
doi = {10.1145/3301761.3301767},
abstract = {With the development of information technology, information systems have been widely used in medical institutions, and more and more clinical research data has been digitized, which provides the possibility to carry out clinical research with data as the source. However, the complexity and multi-dimensionality of clinical data make medical scientists' progress slow, and comprehensive use of various big data technologies is needed to help improve the efficiency of clinical research. Visualization technology can display data in an intuitive and easy-to-read way, helping medical researchers understand data, while parallel computing can greatly improve computing efficiency. Therefore, this paper explores the application strategies of data analysis technology and visualization technology in the management and analysis of clinical research data, and builds a set of clinical research data management analysis system, which combines various technologies to help effectively promote medical clinical.},
booktitle = {Proceedings of the 2018 2nd International Conference on Software and E-Business},
pages = {49–53},
numpages = {5},
keywords = {Data analysis, Visualization, Clinical data},
location = {Zhuhai, China},
series = {ICSEB '18}
}

@article{10.1145/2694428.2694441,
author = {Abadi, Daniel and Agrawal, Rakesh and Ailamaki, Anastasia and Balazinska, Magdalena and Bernstein, Philip A. and Carey, Michael J. and Chaudhuri, Surajit and Dean, Jeffrey and Doan, AnHai and Franklin, Michael J. and Gehrke, Johannes and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Jagadish, H. V. and Kossmann, Donald and Madden, Samuel and Mehrotra, Sharad and Milo, Tova and Naughton, Jeffrey F. and Ramakrishnan, Raghu and Markl, Volker and Olston, Christopher and Ooi, Beng Chin and R\'{e}, Christopher and Suciu, Dan and Stonebraker, Michael and Walter, Todd and Widom, Jennifer},
title = {The Beckman Report on Database Research},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/2694428.2694441},
doi = {10.1145/2694428.2694441},
abstract = {Every few years a group of database researchers meets to discuss the state of database research, its impact on practice, and important new directions. This report summarizes the discussion and conclusions of the eighth such meeting, held October 14- 15, 2013 in Irvine, California. It observes that Big Data has now become a defining challenge of our time, and that the database research community is uniquely positioned to address it, with enormous opportunities to make transformative impact. To do so, the report recommends significantly more attention to five research areas: scalable big/fast data infrastructures; coping with diversity in the data management landscape; end-to-end processing and understanding of data; cloud services; and managing the diverse roles of people in the data life cycle.},
journal = {SIGMOD Rec.},
month = {dec},
pages = {61–70},
numpages = {10}
}

@inproceedings{10.1145/2523616.2525963,
author = {Heintz, Benjamin and Chandra, Abhishek and Sitaraman, Ramesh K.},
title = {Wide-Area Streaming Analytics: Distributing the Data Cube},
year = {2013},
isbn = {9781450324281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523616.2525963},
doi = {10.1145/2523616.2525963},
abstract = {To date, much research in data-intensive computing has focused on batch computation. Increasingly, however, it is necessary to derive knowledge from big data streams. As a motivating example, consider a content delivery network (CDN) such as Akamai [4], comprising thousands of servers in hundreds of globally distributed locations. Each of these servers produces a stream of log data, recording for example every user it serves, along with each video stream they access, when they play and pause streams, and more. Each server also records network- and system-level data such as TCP connection statistics. In aggregate, the servers produce billions of lines of log data from over a thousand locations daily.},
booktitle = {Proceedings of the 4th Annual Symposium on Cloud Computing},
articleno = {55},
numpages = {2},
location = {Santa Clara, California},
series = {SOCC '13}
}

@inproceedings{10.1145/3018661.3022759,
author = {Ensan, Faezeh and Noorian, Zeinab and Bagheri, Ebrahim},
title = {Mining Actionable Insights from Social Networksat WSDM 2017},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3022759},
doi = {10.1145/3018661.3022759},
abstract = {The first international workshop on Mining Actionable Insights from Social Networks (MAISoN'17) is to be held on February 10, 2017; co-located with the Tenth ACM International Web Search and Data Mining (WSDM) Conference in Cambridge, UK. MAISoN'17 aims at bringing together researchers and participants from different disciplines such as computer science, big data mining, machine learning, social network analysis and other related areas in order to identify challenging problems and share ideas, algorithms, and technologies for mining actionable insight from social network data. We organized a workshop program that includes the presentation of eight peer-reviewed papers and keynote talks, which foster discussions around state-of-the-art in social network mining and will hopefully lead to future collaborations and exchanges.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {821–822},
numpages = {2},
keywords = {predictive modeling, web mining, social network analysis},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/3465480.3466926,
author = {Kourtellis, Nicolas and Herodotou, Herodotos and Grzenda, Maciej and Wawrzyniak, Piotr and Bifet, Albert},
title = {S2CE: A Hybrid Cloud and Edge Orchestrator for Mining Exascale Distributed Streams},
year = {2021},
isbn = {9781450385558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465480.3466926},
doi = {10.1145/3465480.3466926},
abstract = {The explosive increase in volume, velocity, variety, and veracity of data generated by distributed and heterogeneous nodes such as IoT and other devices, continuously challenge the state of art in big data processing platforms and mining techniques. Consequently, it reveals an urgent need to address the ever-growing gap between this expected exascale data generation and the extraction of insights from these data. To address this need, this position paper proposes Stream to Cloud &amp; Edge (S2CE), a first of its kind, optimized, multi-cloud and edge orchestrator, easily configurable, scalable, and extensible. S2CE will enable machine and deep learning over voluminous and heterogeneous data streams running on hybrid cloud and edge settings, while offering the necessary functionalities for practical and scalable processing: data fusion and preprocessing, sampling and synthetic stream generation, cloud and edge smart resource management, and distributed processing.},
booktitle = {Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems},
pages = {103–113},
numpages = {11},
keywords = {cloud analytics, data stream analysis, edge analytics, stream mining, machine and deep learning},
location = {Virtual Event, Italy},
series = {DEBS '21}
}

@inproceedings{10.1145/3423603.3424007,
author = {B\"{u}chler, Marco and Riegert, Sarah and Alpi, Federico and Cadeddu, Francesca},
title = {Towards Big Religious Data: RESILIENCE Research Infrastructure for Data on Religion in the Digital Age},
year = {2020},
isbn = {9781405377539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423603.3424007},
doi = {10.1145/3423603.3424007},
abstract = {Data in and for religion is arguably as old as humanity. Religious significance has been attached to an immense variety of artifacts and documents, often in written form, in nearly all spoken and written languages over the past millennia. The rise of the digital age gives to the scholar in religious studies the opportunity to build research over a much wider array of data than ever before; institutions which have data repositories (such as libraries, museums, universities, etc.) similarly have the chance to make their collections available to a larger community. On the other hand, however, there is a serious risk that a considerable amount of data gets lost during the "Digital transition". This paper presents the approach of the RESILIENCE Research Infrastructure in dealing with the issue of big data and data loss within the field of religious studies.},
booktitle = {Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress},
articleno = {9},
numpages = {5},
keywords = {digital transformation, research infrastructures, big religious data, religious studies},
location = {Virtual Event, Tunisia},
series = {DTUC '20}
}

@inproceedings{10.1145/2968219.2971593,
author = {De Masi, Alexandre and Ciman, Matteo and Gustarini, Mattia and Wac, Katarzyna},
title = {MQoL Smart Lab: Quality of Life Living Lab for Interdisciplinary Experiments},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2971593},
doi = {10.1145/2968219.2971593},
abstract = {As a base for hypothesis formulation and testing, accurate, timely and reproducible data collection is a challenge for all researchers. Data collection is especially challenging in uncontrolled environments, outside of the lab and when it involves many collaborating disciplines, where the data must serve quality research in all of them. In this paper, we present own "mQoL Smart Lab" for interdisciplinary research efforts on individuals' "Quality of Life" improvement. We present an evolution of our current in-house living lab platform enabling continuous, pervasive data collection from individuals' smartphones. We discuss opportunities for mQoL stemming from developments in machine learning and big data for advanced data analytics in different disciplines, better meeting the requirements put on the platform.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {635–640},
numpages = {6},
keywords = {smartphones, platforms, data analysis, people centric sensing, data science, data collection},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3268866.3268877,
author = {Zhong, Junmei and Gao, Chuangui and Yi, Xiu},
title = {Categorization of Patient Disease into ICD-10 with NLP and SVM for Chinese Electronic Health Record Analysis},
year = {2018},
isbn = {9781450365246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268866.3268877},
doi = {10.1145/3268866.3268877},
abstract = {The electronic health record (EHR) analysis has become an increasingly important application for artificial intelligence (AI) algorithms to leverage the insight from the big data for improving the quality of human healthcare. In a lot of Chinese EHR analysis applications, it is very important to categorize the patients' diseases according to the medical coding standard. In this paper, we develop NLP and machine learning algorithms to automatically categorize each patient's individual diseases into the ICD-10 coding standard. Experimental results show that the support vector machine algorithm (SVM) accomplishes very promising classification results.},
booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Pattern Recognition},
pages = {101–106},
numpages = {6},
keywords = {SVM, machine learning, ICD-10, NLP, Electronic health record},
location = {Beijing, China},
series = {AIPR 2018}
}

@inproceedings{10.1145/3390557.3394127,
author = {Zhan, Lin and Junhua, Zhao and Fan, Li and Zhifei, Wang},
title = {Research on Intelligent Management Platform of Highspeed Railway Traffic Safety Equipment Based on CPS},
year = {2020},
isbn = {9781450376587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3390557.3394127},
doi = {10.1145/3390557.3394127},
abstract = {From the view of high-speed railway traffic safety, this paper establishes an intelligent management platform for operation safety equipment based on CPS for "person-equipment-environment", and designs a framework of traffic safety system composed of perception control hardware, Internet of Things, cognitive decision-making and information services. The deep fusion of information system and traffic safety equipment is discussed, and the fault diagnosis method of driving equipment based on complex sensing technology is given, such as intelligent identification, online monitoring and ubiquitous sensing of the characteristics of safety protection equipment. Through the application of equipment fault diagnosis, it realizes the rapid retrieval and active collection of safety information, provides early warning and auxiliary decision-making, big data analysis and prediction, and improves the traffic safety.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Innovation in Artificial Intelligence},
pages = {147–154},
numpages = {8},
keywords = {equipment fault diagnosis, High-speed railway, CPS, traffic safety},
location = {Xiamen, China},
series = {ICIAI 2020}
}

@article{10.1145/3404820.3404824,
author = {Gao, Song and Rao, Jinmeng and Kang, Yuhao and Liang, Yunlei and Kruse, Jake},
title = {Mapping County-Level Mobility Pattern Changes in the United States in Response to COVID-19},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
url = {https://doi.org/10.1145/3404820.3404824},
doi = {10.1145/3404820.3404824},
abstract = {To contain the COVID-19 epidemic, one of the non-pharmacological epidemic control measures is reducing the transmission rate of SARS-COV-2 in the population through social distancing. An interactive web-based mapping platform that provides timely quantitative information on how people in different counties and states reacted to the social distancing guidelines was developed by the GeoDS Lab @UW-Madison with the support of the National Science Foundation RAPID program. The web portal integrates geographic information systems (GIS) and daily updated human mobility statistical patterns (median travel distance and stay-at-home dwell time) derived from large-scale anonymized and aggregated smartphone location big data at the county-level in the United States, and aims to increase risk awareness of the public, support data-driven public health and governmental decision-making, and help enhance community responses to the COVID-19 pandemic.},
journal = {SIGSPATIAL Special},
month = {jul},
pages = {16–26},
numpages = {11}
}

@inproceedings{10.1145/2638404.2638526,
author = {Wang, Maximilian J. and Mao, Guifen and Chen, Haiquan},
title = {Mining Multivariate Outliers: A Mixture Model-Based Framework},
year = {2014},
isbn = {9781450329231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2638404.2638526},
doi = {10.1145/2638404.2638526},
abstract = {Mining outliers has become more and more important in recent years. It has wide applications in military surveillance for enemy activities, detection of potential terrorist attacks, credit card fraud detection, network intrusion, computer virus attack, clinical trials, severe weather prediction, athlete performance analysis, and many other data mining tasks. In today's big data age, multivariate data sets are very complex. Variables among different dimensions are usually correlated with different variations. Classical data mining methods with Euclidean distance measure are not working well for mining multivariate outliers. In this study, we propose a normal mixture model-based framework of multivariate outlier detection. We fit the model parameters through the robust EM algorithm. The K-means clustering algorithm is used to provide the initial inputs for the EM algorithm. The well-know Mahalanobis distance is used to determine the cutoff points for outlier detection via the chi-square distribution critical values. Implementation details of this framework are also discussed.},
booktitle = {Proceedings of the 2014 ACM Southeast Regional Conference},
articleno = {51},
numpages = {4},
keywords = {mahalanobis distance, normal mixture models, EM algorithm, outlier detection, data mining, k-means clustering algorithm},
location = {Kennesaw, Georgia},
series = {ACM SE '14}
}

@inproceedings{10.1145/3170427.3174367,
author = {Edge, Darren and Larson, Jonathan and White, Christopher},
title = {Bringing AI to BI: Enabling Visual Analytics of Unstructured Data in a Modern Business Intelligence Platform},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3174367},
doi = {10.1145/3170427.3174367},
abstract = {The Business Intelligence (BI) paradigm is challenged by emerging use cases such as news and social media analytics in which the source data are unstructured, the analysis metrics are unspecified, and the appropriate visual representations are unsupported by mainstream tools. This case study documents the work undertaken in Microsoft Research to enable these use cases in the Microsoft Power BI product. Our approach comprises: (a) back-end pipelines that use AI to infer navigable data structures from streams of unstructured text, media and metadata; and (b) front-end representations of these structures grounded in the Visual Analytics literature. Through our creation of multiple end-to-end data applications, we learned that representing the varying quality of inferred data structures was crucial for making the use and limitations of AI transparent to users. We conclude with reflections on BI in the age of AI, big data, and democratized access to data analytics.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {business intelligence, visual analytics, data, hci, ai},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/3325112.3325245,
author = {Harrison, Teresa and F. Luna-Reyes, Luis and Pardo, Theresa and De Paula, Nic and Najafabadi, Mahdi and Palmer, Jillian},
title = {The Data Firehose and AI in Government: Why Data Management is a Key to Value and Ethics},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325245},
doi = {10.1145/3325112.3325245},
abstract = {Technical and organizational innovations such as Open Data, Internet of Things and Big Data have fueled renewed interest in policy analytics in the public sector. This revamped version of policy analysis continues the long-standing tradition of applying statistical modeling to better understand policy effects and decision making, but also incorporates other computational approaches such as artificial intelligence (AI) and computer simulation. Although much attention has been given to the development of capabilities for data analysis, there is much less attention to understanding the role of data management in a context of AI in government. In this paper, we argue that data management capabilities are foundational to data analysis of any kind, but even more important in the present AI context. This is so because without proper data management, simply acquiring data or systems will not produce desired outcomes. We also argue that realizing the potential of AI for social good relies on investments specifically focused on this social outcome, investments in the processes of building trust in government data, and ensuring the data are ready and suitable for use, for both immediate and future uses.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {171–176},
numpages = {6},
keywords = {DMBOK, Policy Analysis, Data Management, Data Analytics, Artificial Intelligence},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@inproceedings{10.1145/2948992.2949007,
author = {Almeida, Ricardo and Maio, Paulo and Oliveira, Paulo and Barroso, Jo\~{a}o},
title = {Ontology Based Rewriting Data Cleaning Operations},
year = {2016},
isbn = {9781450340755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948992.2949007},
doi = {10.1145/2948992.2949007},
abstract = {Dealing with increasing amounts of data creates the need to deal with redundant, inconsistent and/or complementary repositories which may be different in their data models and/or in their schema. Current data cleaning techniques developed to tackle data quality problems are just suitable for scenarios were all repositories share the same model and schema. Recently, an ontology-based methodology was proposed to overcome this limitation. In this paper, this methodology is briefly described and applied to a real scenario in the health domain with data quality problems.},
booktitle = {Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering},
pages = {85–88},
numpages = {4},
keywords = {Ontology, Data Cleaning, Vocabulary, Schema, Rewriting Process},
location = {Porto, Portugal},
series = {C3S2E '16}
}

@inproceedings{10.5555/2693848.2694087,
author = {Rabe, Markus and Scheidler, Anne Antonia},
title = {An Approach for Increasing the Level of Accuracy in Supply Chain Simulation by Using Patterns on Input Data},
year = {2014},
publisher = {IEEE Press},
abstract = {Setting up simulation scenarios in the field of Supply Chains (SCs) is a big challenge because complex input data must be specified and careful input data management as well as precise model design are necessary. SC simulation needs a large amount of input data -- especially in times of big data, in which the data is often approximated by statistical distributions from real world observations. This paper deals with the question how the model itself and its input can be effectively complemented. This takes into account the commonly known fact, that the accuracy of a model output depends on the model input. Therefore an approach for using techniques of Knowledge Discovery in Databases is introduced to derive logical relations from the data. We discuss how Knowledge Discovery would be applied, as a preprocessing step for simulation scenario setups, in order to provide benefits for the level of accuracy in simulation models.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {1897–1906},
numpages = {10},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/3282933.3282935,
author = {Mack, Vincent Z. W. and Kam, Tin Seong},
title = {Is There Space for Violence? A Data-Driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict},
year = {2018},
isbn = {9781450360326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282933.3282935},
doi = {10.1145/3282933.3282935},
abstract = {With recent increases in incidences of political violence globally, the world has now become more uncertain and less predictable. Of particular concern is the case of violence against civilians, who are often caught in the crossfire between armed state or non-state actors. Classical methods of studying political violence and international relations need to be updated. Adopting the use of data analytic tools and techniques of studying big data would enable academics and policy makers to make sense of a rapidly changing world.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities},
articleno = {1},
numpages = {10},
keywords = {Africa, geospatial autocorrelation, knowledge discovery, hotspot detection, political violence},
location = {Seattle, WA, USA},
series = {GeoHumanities'18}
}

@inproceedings{10.1145/3531072.3535322,
author = {Longo, Antonella and Zappatore, Marco and Martella, Angelo and Rucco, Chiara},
title = {Enhancing Data Education with Datathons: An Experience with Open Data on Renewable Energy Systems},
year = {2022},
isbn = {9781450393508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531072.3535322},
doi = {10.1145/3531072.3535322},
abstract = {Data literacy and the fundamentals of big data management are becoming interdisciplinary in Higher Education curricula, also due to the widespread need of data science skills. This casts the need for presenting novel (and more engaging) learning activities to students. Data hackathons (also known as datathons) represent a viable option to allow students practicing with real use cases and datasets, as well as addressing their learning experiences collaboratively. Moreover, datathons promise to improve soft skills and offer hands-on learning opportunities. Therefore, we present in this paper a datathon on a publicly available dataset about renewable energy systems. The datathon involved students from three data-focused courses of three different M.S. degrees at the University of Salento (Italy). A detailed analysis of the design, implementation and evaluation choices is proposed, along with a series of gathered insights and lessons learned that might help systematizing the introduction and use of datathons in data education.},
booktitle = {1st International Workshop on Data Systems Education},
pages = {26–31},
numpages = {6},
keywords = {renewable energy systems, data education curricula, datathon, open data},
location = {Philadelphia, PA, USA},
series = {DataEd '22}
}

@inproceedings{10.1145/3173574.3174043,
author = {Verma, Nitya and Dombrowski, Lynn},
title = {Confronting Social Criticisms: Challenges When Adopting Data-Driven Policing Strategies},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174043},
doi = {10.1145/3173574.3174043},
abstract = {Proponents of data-driven policing strategies claim that it makes policing organizations more effective, efficient, and accountable and has the potential to address some policing social criticisms (e.g. racial bias, lack of accountability and training). What remains less understood are the challenges when adopting data-driven policing as a response to these criticisms. We present results from a qualitative field study about the adoption of data-driven policing strategies in a Midwestern police department in the United States. We identify three key challenges police face with data-driven adoption efforts: data-driven frictions, precarious and inactionable insights, and police metis concerns. We demonstrate the issues that data-driven initiatives create for policing and the open questions police agents face. These findings contribute an empirical account of how policing agents attend to the strengths and limits of big data's knowledge claims. Lastly, we present data and design implications for policing.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {policing, data-driven organizations, challenges, law enforcement, data practices, metis},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3414752.3414800,
author = {Huang, Qibao and Huang, Yiqi},
title = {The Significance of Urban Cockpit for Urban Brain Construction},
year = {2020},
isbn = {9781450388016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414752.3414800},
doi = {10.1145/3414752.3414800},
abstract = {The urban cockpit will comprehensively perceive and process all kinds of data in the city operation, establish the data chassis of the smart city, objectively, comprehensively and multi dimensionally display the operation situation of the city, and carry out early warning, prediction and scientific disposal of outstanding problems and emergencies in the city operation. Moreover, in the near future, with the introduction and use of 5G, artificial intelligence, big data, data Luan Sheng and edge computing in the city brain project, the city cockpit will also give the city managers and visitors a better and more beautiful feeling in the display effect (such as immersion and three-dimensional), thus accelerating the promotion and landing of the city brain project and promoting social governance Intelligent and professional, improve the level of comprehensive city governance, and change the transformation and upgrading of the city from extensive to precise and refined.},
booktitle = {2020 The 11th International Conference on E-Business, Management and Economics},
pages = {70–73},
numpages = {4},
keywords = {Urban cockpit, Data, Urban brain},
location = {Beijing, China},
series = {ICEME 2020}
}

@inproceedings{10.1145/2330601.2330605,
author = {Siemens, George},
title = {Learning Analytics: Envisioning a Research Discipline and a Domain of Practice},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330605},
doi = {10.1145/2330601.2330605},
abstract = {Learning analytics are rapidly being implemented in different educational settings, often without the guidance of a research base. Vendors incorporate analytics practices, models, and algorithms from datamining, business intelligence, and the emerging "big data" fields. Researchers, in contrast, have built up a substantial base of techniques for analyzing discourse, social networks, sentiments, predictive models, and in semantic content (i.e., "intelligent" curriculum). In spite of the currently limited knowledge exchange and dialogue between researchers, vendors, and practitioners, existing learning analytics implementations indicate significant potential for generating novel insight into learning and vital educational practices. This paper presents an integrated and holistic vision for advancing learning analytics as a research discipline and a domain of practices. Potential areas of collaboration and overlap are presented with the intent of increasing the impact of analytics on teaching, learning, and the education system.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {4–8},
numpages = {5},
keywords = {ethics, collaboration, data integration, theory, learning analytics, research, practice},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@article{10.1145/3124391,
author = {Santana, Eduardo Felipe Zambom and Chaves, Ana Paula and Gerosa, Marco Aurelio and Kon, Fabio and Milojicic, Dejan S.},
title = {Software Platforms for Smart Cities: Concepts, Requirements, Challenges, and a Unified Reference Architecture},
year = {2017},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3124391},
doi = {10.1145/3124391},
abstract = {Information and communication technologies (ICT) can be instrumental in progressing towards smarter city environments, which improve city services, sustainability, and citizens’ quality of life. Smart City software platforms can support the development and integration of Smart City applications. However, the ICT community must overcome current technological and scientific challenges before these platforms can be widely adopted. This article surveys the state of the art in software platforms for Smart Cities. We analyzed 23 projects concerning the most used enabling technologies, as well as functional and non-functional requirements, classifying them into four categories: Cyber-Physical Systems, Internet of Things, Big Data, and Cloud Computing. Based on these results, we derived a reference architecture to guide the development of next-generation software platforms for Smart Cities. Finally, we enumerated the most frequently cited open research challenges and discussed future opportunities. This survey provides important references to help application developers, city managers, system operators, end-users, and Smart City researchers make project, investment, and research decisions.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {78},
numpages = {37},
keywords = {Wireless sensor networks, software platforms}
}

@inproceedings{10.1145/3106426.3106434,
author = {Martins, Denis Mayr Lima and Vossen, Gottfried and de Lima Neto, Fernando Buarque},
title = {Intelligent Decision Support for Data Purchase},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106434},
doi = {10.1145/3106426.3106434},
abstract = {The Big Data era is affording a paradigm change on decision-making approaches. More and more, companies as well as individuals are relying on data rather than on the so called "gut feeling" to make decisions. However, searching the Web for carrying out purchases is not completely satisfactory yet, given the arduousness of finding suitable quality data. This has contributed to the emergence of data marketplaces as an alternative to traditional data commerce, as they provide appropriate online environments for data offering and purchasing. Nevertheless, as the number of available datasets to purchase increases, the task of buying appropriate offers is, very often, challenging. In this sense, we propose an intelligent decision support system to help buyers in purchasing data offers based on a multiple-criteria decision analysis. Experimental results show that our approach provides an interactive way that addresses buyers' needs, allowing them to state and easily refine their preferences, without any specific order, via a series of dataset recommendations.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {396–402},
numpages = {7},
keywords = {data purchase, decision support, personalization, computational intelligence},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/2939672.2945388,
author = {Zhu, Qiang and Guo, Songtao and Ogilvie, Paul and Liu, Yan},
title = {Business Applications of Predictive Modeling at Scale},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2945388},
doi = {10.1145/2939672.2945388},
abstract = {Predictive modeling is the art of building statistical models that forecast probabilities and trends of future events. It has broad applications in industry across different domains. Some popular examples include user intention predictions, lead scoring, churn analysis, etc. In this tutorial, we will focus on the best practice of predictive modeling in the big data era and its applications in industry, with motivating examples across a range of business tasks and relevance products. We will start with an overview of how predictive modeling helps power and drive various key business use cases. We will introduce the essential concepts and state of the art in building end-to-end predictive modeling solutions, and discuss the challenges, key technologies, and lessons learned from our practice, including case studies of LinkedIn feed relevance and a platform for email response prediction. Moreover, we will discuss some practical solutions of building predictive modeling platform to scale the modeling efforts for data scientists and analysts, along with an overview of popular tools and platforms used across the industry.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2139–2140},
numpages = {2},
keywords = {predictive modeling, machine learning platforms, machine learning, business analytics},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3377458.3377464,
author = {Yuwei, Sun and Jianbao, Zhu and Qingshan, Ma and Xinchun, Yu and Ye, Shi and Yu, Chen},
title = {Picture Management of Power Supply Safety Management System Based on Deep Learning Technology},
year = {2020},
isbn = {9781450372640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377458.3377464},
doi = {10.1145/3377458.3377464},
abstract = {With the advent of the era of big data, power supply security management systems will get a lot of picture data. In the face of massive image data, this paper studies the image management technology based on convolutional neural network. Aiming at the high repetition rate of self built image database samples and the problem that many sample classes contain uncorrelated images, two algorithms are proposed to improve the quality of the database: de duplication and de uncorrelation. By using the depth convolution neural network, the Embedding represented by the corresponding image is taken, and the distance between Embedding is calculated in the Euclidean space to achieve the purpose of de duplication and de uncorrelation. In this paper, "time" and "accuracy" are used to evaluate the performance of de duplication and de uncorrelation algorithms. The comparison examples of some sample classes before and after removing repetition and before and after removing uncorrelation are shown. The Recall-value of the database after removing duplicate and uncorrelated is tested based on the GoogLe Netplus-model respectively, which proves the effectiveness of the two filtering algorithms and overcomes the complexity of the traditional filtering process.},
booktitle = {Proceedings of the 2019 5th International Conference on Systems, Control and Communications},
pages = {71–75},
numpages = {5},
keywords = {Picture management, convolutional neural network, deep learning},
location = {Wuhan, China},
series = {ICSCC 2019}
}

@inproceedings{10.1145/3531146.3533151,
author = {Fischer, Maximilian T. and Hirsbrunner, Simon David and Jentner, Wolfgang and Miller, Matthias and Keim, Daniel A. and Helm, Paula},
title = {Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533151},
doi = {10.1145/3531146.3533151},
abstract = {Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science &amp; Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts’ understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {877–889},
numpages = {13},
keywords = {Machine Learning, Critical Algorithm Studies, Ethic Awareness, Interdisciplinary Research, Visual Analytics, Critical Data Studies, Intelligence Analysis, Communication Analysis, Science &amp; Technology Studies},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3408877.3432457,
author = {Fekete, Alan and Kay, Judy and R\"{o}hm, Uwe},
title = {A Data-Centric Computing Curriculum for a Data Science Major},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432457},
doi = {10.1145/3408877.3432457},
abstract = {Many universities are introducing a new major in Data Science into their offering, to reflect the explosive growth in this field and the career opportunities it provides. As a field Data Science has elements from Computer Science and from Statistics, and curricula plans differ widely, both in the balance between the CS and Stats aspects, and also in the emphasis within the computing topics. This paper reports on the curriculum that has been taught for three years now at the University of Sydney. In particular, we describe the approach of a sequence of computing subjects which were developed specifically for the major, in order to bring students over several years to a sophisticated understanding of the data-handling aspects of Data Science. Students also take traditional subjects from both CS (such as Data Structures or AI) and from Statistics (such as Learning from Data and Statistical Inference). The data-centric specially-designed subjects we discuss in this paper are (i) Informatics: Data and Computation (in the first year), (ii) Big Data and Data Diversity (in the second year), and then upper-division subjects on (iii) Data Science Platforms, and (iv) Human-in-the-Loop Data Analytics.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {865–871},
numpages = {7},
keywords = {data science, curriculum},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{10.1145/2522848.2522892,
author = {Neumann, Alexander and Schnier, Christian and Hermann, Thomas and Pitsch, Karola},
title = {Interaction Analysis and Joint Attention Tracking in Augmented Reality},
year = {2013},
isbn = {9781450321297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522848.2522892},
doi = {10.1145/2522848.2522892},
abstract = {Multimodal research in human interaction has to consider a variety of factors, ranging from local short-time phenomena to complex interaction patterns. As of today, no single discipline engaged in communication research offers the methods and tools to investigate the full complexity continuum in a time-efficient way. A synthesis of qualitative and quantitative analysis is required to merge insights about micro-sequential structures with big data patterns. Using the example of a co-present dyadic negotiation analysis to combine methods offered by Conversation Analysis and Data Mining, we show how such a partnership can benefit each discipline and lead to insights as well as new hypotheses evaluation opportunities.},
booktitle = {Proceedings of the 15th ACM on International Conference on Multimodal Interaction},
pages = {165–172},
numpages = {8},
keywords = {interaction studies, data mining, multimodality, conversation analysis},
location = {Sydney, Australia},
series = {ICMI '13}
}

@article{10.14778/3554821.3554899,
author = {Fan, Wenfei},
title = {Big Graphs: Challenges and Opportunities},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554899},
doi = {10.14778/3554821.3554899},
abstract = {Big data is typically characterized with 4V's: Volume, Velocity, Variety and Veracity. When it comes to big graphs, these challenges become even more staggering. Each and every of the 4V's raises new questions, from theory to systems and practice. Is it possible to parallelize sequential graph algorithms and guarantee the correctness of the parallelized computations? Given a computational problem, does there exist a parallel algorithm for it that guarantees to reduce parallel runtime when more machines are used? Is there a systematic method for developing incremental algorithms with effectiveness guarantees in response to frequent updates? Is it possible to write queries across relational databases and semistructured graphs in SQL? Can we unify logic rules and machine learning, to improve the quality of graph-structured data, and deduce associations between entities? This paper aims to incite interest and curiosity in these topics. It raises as many questions as it answers.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3782–3797},
numpages = {16}
}

@article{10.1145/3532784,
author = {Li, Yuanxia and Currim, Faiz and Ram, Sudha},
title = {Data Completeness and Complex Semantics in Conceptual Modeling: The Need for a Disaggregation Construct},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1936-1955},
url = {https://doi.org/10.1145/3532784},
doi = {10.1145/3532784},
abstract = {Conceptual modeling is important for developing databases that maintain the integrity and quality of stored information. However, classical conceptual models have often been assumed to work on well-maintained and high-quality data. With the advancement and expansion of data science, it is no longer the case. The need to model and store data has emerged for settings with lower data quality, which creates the need to update and augment conceptual models to represent lower-quality data. In this paper, we focus on the intersection between data completeness (an important aspect of data quality) and complex class semantics (where a complex class entity represents information that spans more than one simple class entity). We propose a new disaggregation construct to allow the modeling of incomplete information. We demonstrate the use of our disaggregation construct for diverse modeling problems and discuss the anomalies that could occur without this construct. We provide formal definitions and thorough comparisons between various types of complex constructs to guide future application and prove the unique interpretation of our newly proposed disaggregation construct.},
note = {Just Accepted},
journal = {J. Data and Information Quality},
month = {aug},
keywords = {Database design, Complex semantics, Disaggregation construct}
}

@inproceedings{10.1145/2611040.2611086,
author = {Omitola, Tope and Davies, John and Duke, Alistair and Glaser, Hugh and Shadbolt, Nigel},
title = {Linking Social, Open, and Enterprise Data},
year = {2014},
isbn = {9781450325387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2611040.2611086},
doi = {10.1145/2611040.2611086},
abstract = {The new world of big data, of the LOD cloud, of the app economy, and of social media means that organisations no longer own, much less control, all the data they need to make the best informed business decisions. In this paper, we describe how we built a system using Linked Data principles to bring in data from Web 2.0 sites (LinkedIn, Salesforce), and other external business sites such as OpenCorporates, linking these together with pertinent internal British Telecommunications enterprise data into that enterprise data space. We describe the challenges faced during the implementation, which include sourcing the datasets, finding the appropriate "join points" from the individual datasets, as well as developing the client application used for data publication. We describe our solutions to these challenges and discuss the design decisions made. We conclude by drawing some general principles from this work.},
booktitle = {Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14)},
articleno = {41},
numpages = {8},
keywords = {Navigation, Architectures, Hypertext/Hypermedia, User issues, Semantic networks},
location = {Thessaloniki, Greece},
series = {WIMS '14}
}

@inproceedings{10.1145/2331801.2331803,
author = {Alsubaiee, Sattam and Behm, Alexander and Grover, Raman and Vernica, Rares and Borkar, Vinayak and Carey, Michael J. and Li, Chen},
title = {ASTERIX: Scalable Warehouse-Style Web Data Integration},
year = {2012},
isbn = {9781450312394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2331801.2331803},
doi = {10.1145/2331801.2331803},
abstract = {A growing wealth of digital information is being generated on a daily basis in social networks, blogs, online communities, etc. Organizations and researchers in a wide variety of domains recognize that there is tremendous value and insight to be gained by warehousing this emerging data and making it available for querying, analysis, and other purposes. This new breed of "Big Data" applications poses challenging requirements against data management platforms in terms of scalability, flexibility, manageability, and analysis capabilities. At UC Irvine, we are building a next-generation database system, called ASTERIX, in response to these trends. We present ongoing work that approaches the following questions: How does data get into the system? What primitives should we provide to better cope with dirty/noisy data? How can we support efficient data analysis on spatial data? Using real examples, we show the capabilities of ASTERIX for ingesting data via feeds, supporting set-similarity predicates for fuzzy matching, and answering spatial aggregation queries.},
booktitle = {Proceedings of the Ninth International Workshop on Information Integration on the Web},
articleno = {2},
numpages = {4},
keywords = {cloud computing, semistructured data, ASTERIX, data-intensive computing, hyracks},
location = {Scottsdale, Arizona, USA},
series = {IIWeb '12}
}

@inproceedings{10.1145/3290605.3300561,
author = {Deeb-Swihart, Julia and Endert, Alex and Bruckman, Amy},
title = {Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300561},
doi = {10.1145/3290605.3300561},
abstract = {In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {qualitative, human trafficking, needs analysis, law enforcement},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3535508.3545519,
author = {Goel, Aekansh and Mudge, Zachary and Bi, Sarah and Brenner, Charles and Huffman, Nicholas and Giuste, Felipe and Marteau, Benoit and Shi, Wenqi and Wang, May D.},
title = {Identification of COVID-19 Severity and Associated Genetic Biomarkers Based on ScRNA-Seq Data},
year = {2022},
isbn = {9781450393867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535508.3545519},
doi = {10.1145/3535508.3545519},
abstract = {Bio-marker identification for COVID-19 remains a vital research area to improve current and future pandemic responses. Innovative artificial intelligence and machine learning-based systems may leverage the large quantity and complexity of single cell sequencing data to quickly identify disease with high sensitivity. In this study, we developed a novel approach to classify patient COVID-19 infection severity using single-cell sequencing data derived from patient BronchoAlveolar Lavage Fluid (BALF) samples. We also identified key genetic biomarkers associated with COVID-19 infection severity. Feature importance scores from high performing COVID-19 classifiers were used to identify a set of novel genetic biomarkers that are predictive of COVID-19 infection severity. Treatment development and pandemic reaction may be greatly improved using our novel big-data approach. Our implementation is available on https://github.com/aekanshgoel/COVID-19_scRNAseq.},
booktitle = {Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {45},
numpages = {5},
keywords = {bronchoalveolar lavage fluid, health informatics, gene markers, model interpretation, COVID-19, single cell RNA sequencing},
location = {Northbrook, Illinois},
series = {BCB '22}
}

@inproceedings{10.1145/3487664.3487719,
author = {Caruccio, Loredana and Cirillo, Stefano and Deufemia, Vincenzo and Polese, Giuseppe},
title = {Efficient Discovery of Functional Dependencies from Incremental Databases},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487719},
doi = {10.1145/3487664.3487719},
abstract = {With the advent of Big Data there is an increasing necessity to incrementally mine information from data originating from sensors and other dynamic sources. Thus, it is necessary to devise algorithms capable of mining useful information upon possible evolutions of databases. Among these, there are certainly data profiling info, such as functional dependencies (fd for short), which are particularly useful for data integration and for assessing the quality of data. The incremental scenario requires the definition of search strategies and validation methods able to analyze only the portion of the dataset affected by the last changes. In this paper, we propose a new validation method, which exploits regular expressions and compressed data structures to efficiently verify whether a candidate fd holds on an updated version of the dataset. Experimental results demonstrate the effectiveness of the proposed method on real-world datasets adapted for incremental scenarios, also compared with a baseline incremental fd discovery algorithm.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {400–409},
numpages = {10},
keywords = {Functional Dependency, Data Profiling, Incremental Discovery, Data Mining},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3347146.3359090,
author = {Zhao, Kai and Feng, Jie and Xu, Zhao and Xia, Tong and Chen, Lin and Sun, Funing and Guo, Diansheng and Jin, Depeng and Li, Yong},
title = {DeepMM: Deep Learning Based Map Matching with Data Augmentation},
year = {2019},
isbn = {9781450369091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347146.3359090},
doi = {10.1145/3347146.3359090},
abstract = {Map matching is important in many trajectory based applications like route optimization and traffic schedule, etc. As the widely used methods, Hidden Markov Model and its variants are well studied to provide accurate and efficient map matching service. However, HMM based methods fail to utilize the value of enormous trajectory big data, which are useful for the map matching task. Furthermore, with many following-up works, they are still easily influenced by the noisy records, which are very common in the real system. To solve these problems, we revisit the map matching task from the data perspective, and propose to utilize the great power of data to help solve these problems. We build a deep learning based model to utilize all the trajectory data for joint training and knowledge sharing. With the help of embedding techniques and sequence learning model with attention enhancement, our system does the map matching in the latent space, which is tolerant to the noise in the physical space. Extensive experiments demonstrate that our model outperforms the widely used HMM based methods more than 10% (absolute accuracy) and works robustly in the noisy settings in the meantime.},
booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {452–455},
numpages = {4},
keywords = {map matching, data driven system, deep learning},
location = {Chicago, IL, USA},
series = {SIGSPATIAL '19}
}

@inproceedings{10.1145/3035918.3058740,
author = {Castro Fernandez, Raul and Deng, Dong and Mansour, Essam and Qahtan, Abdulhakim A. and Tao, Wenbo and Abedjan, Ziawasch and Elmagarmid, Ahmed and Ilyas, Ihab F. and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan},
title = {A Demo of the Data Civilizer System},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3058740},
doi = {10.1145/3035918.3058740},
abstract = {Finding relevant data for a specific task from the numerous data sources available in any organization is a daunting task. This is not only because of the number of possible data sources where the data of interest resides, but also due to the data being scattered all over the enterprise and being typically dirty and inconsistent. In practice, data scientists are routinely reporting that the majority (more than 80%) of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. We propose to demonstrate DATA CIVILIZER to ease the pain faced in analyzing data "in the wild". DATA CIVILIZER is an end-to-end big data management system with components for data discovery, data integration and stitching, data cleaning, and querying data from a large variety of storage engines, running in large enterprises.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1639–1642},
numpages = {4},
keywords = {data integration, data discovery, data cleaning, polystore queries, join path discovery, data stitching},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3291064.3291074,
author = {H G, Monika Rani and R, Sapna and Mishra, Shakti},
title = {An Investigative Study on the Quality Aspects of Linked Open Data},
year = {2018},
isbn = {9781450365765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291064.3291074},
doi = {10.1145/3291064.3291074},
abstract = {Linked Open Data refers to a set of best practices that empowers enterprises to publish and interlink their data using existing ontologies on the Semantic Web. The focus of linked open data is to move from document-based Web to a Web of interlinked data, created by typed links between data from different data sources. Linked open data expert group has taken cognizance of data quality importance, as the amount of linked data publications grown on the Web substantially. Measures have been taken to check the linked data quality. But, these measures are diverse in nature with respect to quality terms. This makes the comparison and evaluation difficult, leading to an incorrect selection of accurate data sources based on quality requirements. In this paper, we carried out an analysis on linked data, the quality of linked data, the frameworks to assess the quality of linked data and the challenges to achieve the quality of linked open data.},
booktitle = {Proceedings of the 2018 International Conference on Cloud Computing and Internet of Things},
pages = {33–39},
numpages = {7},
keywords = {Linked open data, Semantic Web, Quality of linked open data},
location = {Singapore, Singapore},
series = {CCIOT 2018}
}

@inproceedings{10.1145/3429889.3429938,
author = {Chen, Juan and Lu, Yan and Zhang, Ting and Ouyang, Zhaolian},
title = {Artificial Intelligence in Medicine in the United States, China and India},
year = {2020},
isbn = {9781450388603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429889.3429938},
doi = {10.1145/3429889.3429938},
abstract = {Objective: To compare the development status of artificial intelligence (AI) in medicine among the United States (US), China and India with bibliometric analysis. Methods: Articles involving AI in medicine published from 2015 to 2019 were retrieved on March 30, 2020 from Web of Science Core Collection. The country-level and the institution-level performance of the US, China and India in the field of AI in medicine were compared with indicators including the amount of papers, 5-year Compound Annual Growth Rate (CAGR) of the amount of papers, the amount of highly-cited papers, the proportion of highly-cited papers and the average citations per paper. In addition, the research hotspots and international cooperation of the three countries in recent 5 years were compared by conducting keywords co-occurrence analysis and co-authorship analysis in VOSviewer. Results: From 2015 to 2019, The US has published 7838 papers and 154 highly-cited papers in the field of AI in medicine, with an average citations per paper to be 9.3, and the proportion of highly-cited papers to be 2.0 %. China has output 6635 papers and 73 highly-cited papers in this field, with an average citations per paper to be 5.3, and the proportion of highly-cited papers to be 1.1%. India has output 3895 papers and 22 highly-cited papers in this field, with an average citations per paper to be 3.6, and the proportion of highly-cited papers to be 0.6%. The 5-year CAGR of the US, China and India in the period of 2015~2019 were 16.0%, 25.4% and 2.4%, respectively. At the institutional level, most of these indicators were significantly better for the US institutions than for Chinese and Indian ones. There were four research hotspots in this field, namely medical imaging technology, health big data mining, disease prediction with biomarkers and genetic information, and early diagnosis of neurological disease. The three countries focused on different hotspots, with China focusing relatively less on health big data mining, while the US and India being complementary to each other. As to international cooperation, the average links per paper to other countries were 0.60, 0.40 and 0.20, respectively, for the US, China and India. Conclusions: In the field of AI in medicine, the US, with a number of competitive institutions in AI and medical researches, is taking a definitely leading role, having conducted many innovative researches and cooperated extensively with other countries. China is taking the second leading role at the country level, with top institutions somewhat less productive than those in the US. India is the third productive country, with top institutions obvious less productive than those in the US, and with research hotspots exactly complementary to the US.},
booktitle = {Proceedings of the 2020 International Symposium on Artificial Intelligence in Medical Sciences},
pages = {257–264},
numpages = {8},
keywords = {Bibliometric analysis, Artificial intelligence, Medical sciences, Information technology, Development status},
location = {Beijing, China},
series = {ISAIMS 2020}
}

@inproceedings{10.1145/3522664.3528621,
author = {Shome, Arumoy and Cruz, Lu\'{\i}s and van Deursen, Arie},
title = {Data Smells in Public Datasets},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528621},
doi = {10.1145/3522664.3528621},
abstract = {The adoption of Artificial Intelligence (AI) in high-stakes domains such as healthcare, wildlife preservation, autonomous driving and criminal justice system calls for a data-centric approach to AI. Data scientists spend the majority of their time studying and wrangling the data, yet tools to aid them with data analysis are lacking. This study identifies the recurrent data quality issues in public datasets. Analogous to code smells, we introduce a novel catalogue of data smells that can be used to indicate early signs of problems or technical debt in machine learning systems. To understand the prevalence of data quality issues in datasets, we analyse 25 public datasets and identify 14 data smells.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {205–216},
numpages = {12},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@inproceedings{10.1145/3486611.3491133,
author = {Chowdhury, Tahiya and Ding, Qizhen and Mandel, Ilan and Ju, Wendy and Ortiz, Jorge},
title = {Tracking Urban Heartbeat and Policy Compliance through Vision and Language-Based Sensing},
year = {2021},
isbn = {9781450391146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486611.3491133},
doi = {10.1145/3486611.3491133},
abstract = {Sensing activities at the city scale using big data can enable applications to improve the quality of citizen life. While there are approaches to sense the urban heartbeat using sound, vision, radio frequency (RF), and other sensors, capturing changes at urban scale using such sensing modalities is challenging. Due to the enormous amount of data they produce and the associated annotation and processing requirement, such data can be of limited use. In this paper, we present a vision-to-language modeling approach to capture patterns and transitions that occur in New York City from March 2020 to August 2020. We use the model on ~1 million street images captured by dashcams over 6 months. We then use the captions to train a language model based on Latent Dirichlet Allocation [4] and compare models from different periods using probabilistic distance measures. We observe distribution shifts in the model that correlate well with social distancing policies and are corroborated by different data sources, such as mobility traces. This language-based sensing introduces a new sensing modality to capture dynamics in the city with lower storage requirements and privacy concerns.},
booktitle = {Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {302–306},
numpages = {5},
keywords = {COVID-19, urban sensing, computer vision and language},
location = {Coimbra, Portugal},
series = {BuildSys '21}
}

@inproceedings{10.1145/3543106.3543113,
author = {Meiryani, Meiryani and Aprilia, Kanaya Regina and Warganegara, Dezie Leonarda and Yanti, Yanti},
title = {Challenges of the Accounting Profession in the Era of the Industrial Revolution 4.0},
year = {2022},
isbn = {9781450397162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543106.3543113},
doi = {10.1145/3543106.3543113},
abstract = {In the current industrial era, namely 4.0, it greatly facilitates the industrial world and human work and brings changes in human work adjustments. Where there are many changes in human life, for example in communication and data processing, both for individuals and for companies [1]. The characteristic of the industrial revolution 4.0 is the emergence of many applied technologies, such as the internet of things, cloud computing, Big Data, and artificial intelligence which as a whole can change business models and production patterns, especially in various industrial sectors [4]. In the current era of the industrial revolution, it is a challenge that is not easy and difficult for accountants as there is a lot of information technology present because financial transactions do not only use cash, but also use digital money [1]. The research objectives in this paper are to determine the role of accountants in the era of the industrial revolution 4.0 and to find out how accountants challenge the industrial revolution 4.0. The data in this study were obtained from library sources such as collecting journals, websites, and related party documents, and sources from other media that can support the completeness of research data so that this research can run correctly and according to reality.},
booktitle = {Proceedings of the 2022 International Conference on E-Business and Mobile Commerce},
pages = {39–46},
numpages = {8},
keywords = {Industrial revolution 4.0, Accountants, Challenges},
location = {Seoul, Republic of Korea},
series = {ICEMC '22}
}

@inproceedings{10.1145/3274192.3274219,
author = {Strey, Mateus Rambo and Pereira, Roberto and de Castro Salgado, Luciana C.},
title = {Human Data-Interaction: A Systematic Mapping},
year = {2018},
isbn = {9781450366014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274192.3274219},
doi = {10.1145/3274192.3274219},
abstract = {Big Data, e-Science and Internet of Things have contributed to increase the production, processing and storage of data, changing the way people deal and live with data. Although the problem is not new, the "human aspect" of data and the possible impact of Human-Data Interaction (HDI) in human life have been explored and discussed as an emerging research area. On the one hand, HDI offers plenty of opportunities for research and development, and on the other hand it demands characterization, grounding, critical discussions, empirical results and thinking tools to support research and practice. This paper presents a Systematic Mapping of Literature on HDI in Computer Science, identifying the different definitions for the area, elements or objects of investigation, contexts of application, stakeholders, etc. Based on 28 selected papers, results point out to a lack of definition or agreement on what HDI is, but suggest that there are different aspects that can characterize it, and allow identifying concerns and objects of study, such as privacy, ownership and transparency. Results suggest a demand for theoretical and methodological frameworks to support the understanding, design and evaluation of HDI via computing systems.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {27},
numpages = {12},
keywords = {Human-Computer Interaction, Human-Data Interaction, Systematic Mapping Review},
location = {Bel\'{e}m, Brazil},
series = {IHC 2018}
}

@inproceedings{10.1145/2791347.2791371,
author = {Efros, Pavel and Buchmann, Erik and Englhardt, Adrian and B\"{o}hm, Klemens},
title = {How to Quantify the Impact of Lossy Transformations on Change Detection},
year = {2015},
isbn = {9781450337090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791347.2791371},
doi = {10.1145/2791347.2791371},
abstract = {To ease the proliferation of big data, it frequently is transformed, be it by compression, be it by anonymization. Such transformations however modify characteristics of the data, such as changes in the case of time series. Changes however are important for subsequent analyses. The impact of those modifications depends on the application scenario, and quantifying it is far from trivial. This is because a transformation can shift or modify existing changes or introduce new ones. In this paper, we propose MILTON, a flexible and robust Measure for quantifying the Impact of Lossy Transformations on subsequent change detectiON. MILTON is applicable to any lossy transformation technique on time-series data and to any general-purpose change-detection approach. We have evaluated it with three real-world use cases. Our evaluation shows that MILTON allows to quantify the impact of lossy transformations and to choose the best one from a class of transformation techniques for a given application scenario.},
booktitle = {Proceedings of the 27th International Conference on Scientific and Statistical Database Management},
articleno = {17},
numpages = {11},
location = {La Jolla, California},
series = {SSDBM '15}
}

@article{10.1145/3353401.3353406,
author = {Horne, Benjamin D. and Nevo, Dorit and Adal\i{}, Sibel},
title = {Recognizing Experts on Social Media: A Heuristics-Based Approach},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/3353401.3353406},
doi = {10.1145/3353401.3353406},
abstract = {Knowing who is an expert on social media is a challenging yet important task, especially in a world where misleading information is commonplace and where social media is an important information source for knowledge seekers. In this paper we investigate expertise heuristics by comparing features of experts versus non-experts in big data settings. We employ a large set of features to classify experts and non-experts using data collected on two social media platform (Twitter and reddit). Our results show a good ability to predict who is an expert, especially using language-based features, validating that heuristics can be developed to differentiate experts from novices organically, based on social media use. Our results contribute to the development of expertise location and identification systems as well as our understanding on how experts present themselves on social media.},
journal = {SIGMIS Database},
month = {jul},
pages = {66–84},
numpages = {19},
keywords = {expertise location, data analytics., social media}
}

@article{10.14778/2824032.2824070,
author = {Dasu, Tamraparni and Shkapenyuk, Vladislav and Srivastava, Divesh and Swayne, Deborah F.},
title = {FIT to Monitor Feed Quality},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824070},
doi = {10.14778/2824032.2824070},
abstract = {While there has been significant focus on collecting and managing data feeds, it is only now that attention is turning to their quality. In this paper, we propose a principled approach to online data quality monitoring in a dynamic feed environment. Our goal is to alert quickly when feed behavior deviates from expectations.We make contributions in two distinct directions. First, we propose novel enhancements to permit a publish-subscribe approach to incorporate data quality modules into the DFMS architecture. Second, we propose novel temporal extensions to standard statistical techniques to adapt them to online feed monitoring for outlier detection and alert generation at multiple scales along three dimensions: aggregation at multiple time intervals to detect at varying levels of sensitivity; multiple lengths of data history for varying the speed at which models adapt to change; and multiple levels of monitoring delay to address lagged data arrival.FIT, or Feed Inspection Tool, is the result of a successful implementation of our approach. We present several case studies outlining the effective deployment of FIT in real applications along with user testimonials.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1728–1739},
numpages = {12}
}

@inproceedings{10.1145/3544109.3544151,
author = {Zhang, Jun and Liu, Longlong},
title = {Research on Recommendation Strategy of E-Commerce User Portrait Based on User Dynamic Interest Factor Hybrid Recommendation Algorithm},
year = {2022},
isbn = {9781450395786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544109.3544151},
doi = {10.1145/3544109.3544151},
abstract = {In modern society with the good and fast development of mobile e-commerce, the commodity information and the user behavior data accompanying it show an explosive and sudden growth trend, which also leads to the emergence of information overload on the e-commerce platform, and the proposed personalized recommendation system for e-commerce users largely alleviates this problem mentioned above. The personalized recommendation system for e-commerce users aims to solve the information overload of e-commerce platform by analyzing the user behavior data of e-commerce platform, so as to explore the interest preference of e-commerce platform users and make active recommendation of advertising content related to e-commerce platform. Although the research on recommendation algorithms for e-commerce platforms has made great progress, there are still challenges in terms of sparse data, static user features and interpretability of e-commerce platform recommendation results in terms of big data feature recognition. Therefore, in this paper, a hybrid recommendation algorithm based on the forgetting curve of e-commerce platform and the automatic feature construction of e-commerce platform is studied in the e-commerce scenario of e-commerce platform, combined with the e-commerce data collected in real field, for the sparsity of e-commerce platform data, the interpretability of recommendation results and the static nature of e-commerce platform user features.},
booktitle = {Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {225–230},
numpages = {6},
location = {Dalian, China},
series = {IPEC '22}
}

@inproceedings{10.1145/2442952.2442955,
author = {Mehta, Paras and Voisard, Agn\`{e}s},
title = {Analysis of User Mobility Data Sources for Multi-User Context Modeling},
year = {2012},
isbn = {9781450316941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442952.2442955},
doi = {10.1145/2442952.2442955},
abstract = {Finding the right data source for research is a challenge that many of us face. Although we live in times where 'Open Data' and 'Big Data' have become buzzwords, getting hold of a reasonable size and quality dataset is often hard. When it comes to user data such as mobility data, this becomes even tougher due to privacy-related concerns. This paper briefly explains our research in the area of multi-user context modeling and presents some criteria that we believe are important while selecting a dataset for testing different approaches in this domain. To find the right dataset, some relevant publicly available human mobility datasets are examined using these criteria. The following are the datasets that have been analyzed: Microsoft Research GeoLife Trajectory Dataset, Tracking Delft I Pedestrian Trajectory Dataset, MIT Media Lab Reality Mining Dataset and LifeMap Dataset. Besides these, some other useful data sources for researchers have been cited.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Crowdsourced and Volunteered Geographic Information},
pages = {9–14},
numpages = {6},
keywords = {model, mobility, dataset, situation, context, multi-user},
location = {Redondo Beach, California},
series = {GEOCROWD '12}
}

@inproceedings{10.1145/2525314.2525455,
author = {McKenzie, Grant and Janowicz, Krzysztof and Adams, Benjamin},
title = {Weighted Multi-Attribute Matching of User-Generated Points of Interest},
year = {2013},
isbn = {9781450325219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525314.2525455},
doi = {10.1145/2525314.2525455},
abstract = {To a large degree, the attraction of Big Data lies in the variety of its heterogeneous multi-thematic and multi-dimensional data sources and not merely its volume. To fully exploit this variety, however, requires conflation. This is a two step process. First, one has to establish identity relations between information entities across the different data sources; and second, attribute values have to be merged according to certain procedures which avoid logical contradictions. The first step, also called matching, can be thought of as a weighted combination of common attributes according to some similarity measures. In this work, we propose such a matching based on multiple attributes of Points of Interests (POI) from the Location-based Social Network Foursquare and the Yelp local directory service. While both contain overlapping attributes that can be use for matching, they have specific strengths and weaknesses which makes their conflation desirable. We present a weighted multi-attribute matching strategy and evaluate its performance. Our strategy can automatically match 97% of randomly selected Yelp POI to their corresponding Foursquare entities.},
booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {440–443},
numpages = {4},
keywords = {volunteered geographic information, conflation, POI, location-based services, point of interest, similarity},
location = {Orlando, Florida},
series = {SIGSPATIAL'13}
}

@article{10.1145/2685352,
author = {Aalst, Wil Van Der and Zhao, J. Leon and Wang, Harry Jiannan},
title = {Editorial: “Business Process Intelligence: Connecting Data and Processes”},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2685352},
doi = {10.1145/2685352},
abstract = {This introduction to the special issue on Business Process Intelligence (BPI) discusses the relation between data and processes. The recent attention for Big Data illustrates that organizations are aware of the potential of the torrents of data generated by today's information systems. However, at the same time, organizations are struggling to extract value from this overload of data. Clearly, there is a need for data scientists able to transform event data into actionable information. To do this, it is crucial to take a process perspective. The ultimate goal of BPI is not to improve information systems or the recording of data; instead the focus should be in improving the process. For example, we may want to aim at reducing costs, minimizing response times, and ensuring compliance. This requires a “confrontation” between process models and event data. Recent advances in process mining allow us to automatically learn process models showing the bottlenecks from “raw” event data. Moreover, given a normative model, we can use conformance checking to quantify and understand deviations. Automatically learned models may also be used for prediction and recommendation. BPI is rapidly developing as a field linking data science to business process management. This article aims to provide an overview thereby paving the way for the other contributions in this special issue.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {apr},
articleno = {18e},
numpages = {7},
keywords = {Process mining, performance analysis, business process intelligence, process modeling, compliance checking}
}

@inproceedings{10.1145/3442381.3449956,
author = {Yang, Longqi and Zhang, Liangliang and Tang, Yuhua},
title = {Scalable Auto-Weighted Discrete Multi-View Clustering},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449956},
doi = {10.1145/3442381.3449956},
abstract = {Multi-view clustering has been widely studied in machine learning, which uses complementary information to improve clustering performance. However, challenges remain when handling large-scale multi-view data due to the traditional approaches’ high time complexity. Besides, the existing approaches suffer from parameter selection. Due to the lack of labeled data, parameter selection in practical clustering applications is difficult, especially in big data. In this paper, we propose a novel approach for large-scale multi-view clustering to overcome the above challenges. Our approach focuses on learning the low-dimensional binary embedding of multi-view data, preserving the samples’ local structure during binary embedding, and optimizing the embedding and clustering in a unified framework. Furthermore, we proposed to learn the parameters using a combination of data-driven and heuristic approaches. Experiments on five large-scale multi-view datasets show that the proposed method is superior to the state-of-the-art in terms of clustering quality and running time.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3269–3278},
numpages = {10},
keywords = {binary coding, parameter selection, graph regularization, multi-view clustering},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3145623,
author = {Wang, Chang and Zhu, Yongxin and Shi, Weiwei and Chang, Victor and Vijayakumar, P. and Liu, Bin and Mao, Yishu and Wang, Jiabao and Fan, Yiping},
title = {A Dependable Time Series Analytic Framework for Cyber-Physical Systems of IoT-Based Smart Grid},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3145623},
doi = {10.1145/3145623},
abstract = {With the emergence of cyber-physical systems (CPS), we are now at the brink of next computing revolution. The Smart Grid (SG) built on top of IoT (Internet of Things) is one of the foundations of this CPS revolution, which involves a large number of smart objects connected by networks. The volume of time series of SG equipment is tremendous and the raw time series are very likely to contain missing values because of undependable network transferring. The problem of storing a tremendous volume of raw time series thereby providing a solid support for precise time series analytics now becomes tricky. In this article, we propose a dependable time series analytics (DTSA) framework for IoT-based SG. Our proposed DTSA framework is capable of providing a dependable data transforming from CPS to the target database with an extraction engine to preliminary refining raw data and further cleansing the data with a correction engine built on top of a sensor-network-regularization-based matrix factorization method. The experimental results reveal that our proposed DTSA framework is capable of effectively increasing the dependability of raw time series transforming between CPS and the target database system through the online lightweight extraction engine and the offline correction engine. Our proposed DTSA framework would be useful for other industrial big data practices.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {aug},
articleno = {7},
numpages = {18},
keywords = {cyber-physical-systems, dependable time series analytics, IoT-based smart grid, sensor-network-regularization-based matrix factorization}
}

@inproceedings{10.1145/3396868.3402495,
author = {Kumar, Santosh},
title = {Sensitivity, Specificity, Generalizability, and Reusability Aspirations for Machine Learning (ML) Models in MHealth},
year = {2020},
isbn = {9781450380126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396868.3402495},
doi = {10.1145/3396868.3402495},
abstract = {Mobile sensor big data collected from smartphones, smartwatches, fitness trackers, and other wearables can be mined for signatures (called mHealth biomarkers) of subtle changes in daily behaviors (e.g., mobility, gait, sleep, etc.) and/or physiology (e.g., heart function, breathing, sweating, etc.). Clinical adoption of these mHealth biomarkers can lead to potent temporally-precise interventions, enabling patients to initiate and sustain the healthy lifestyle choices and treatment regimes that are necessary to prevent and/or successfully manage the growing burden of multiple chronic conditions.However, for any new biomarker to be successfully used for clinical diagnosis or treatment, its clinical utility must be established. mHealth biomarkers are usually derived by training a machine learning (ML) algorithm on mobile sensor data. The published models differ in feature construction (domain-derived features fed to a supervised ML model vs. data-driven features discovered by a deep learning (DL) model), data collection setting (lab vs. field), data selection and preparation (covering all twenty-four hours of the day vs. awake hours, vs. only when performing certain tasks), data labeling (retrospective self-reported aggregate labels vs. time-synchronized labels from first-person video), data size and diversity (e.g., number of participants, gender, ethnicity, age group, number of days, hours per day, etc.), experiment design (cross-validation vs. cross-subject validation), and performance (e.g., accuracy, F1 score, confusion matrix, AUC, etc.). As a result, there is wide diversity in published models on their potential for reusability, generalizability, and eventual clinical utility.This talk will describe an aspirational framework for specificity, sensitivity, generalizability, and reusability of mHealth biomarkers with some concrete performance targets so that they have a higher chance of widespread clinical utility. It will draw upon the presenter's decade-long transdisciplinary research experience in developing machine learning models to detect a wide variety of daily behaviors such as stress, speaking, smoking, and brushing from wearable physiological and inertial sensors.The talk will use the analogy of five nines (i.e., 99.999%) paradigm in the area of service-level agreements to quantify high-availability. Similar to how these managed services are expected to be available 24-7-365, with the downtime limited to 5.26 minutes per year, we can express the performance requirements of mHealth biomarkers that are expected to detect subtle signs of health and behaviour deterioration anytime and anywhere. Five nines guarantee for the detection of a health event (e.g., fall, stress) translates to one false positive every 100 person-days, if the model runs on 1,000 minutes of sensor data collected each day. Achieving five nines to claim the detection of non-event (e.g., smoking abstinence) is even more challenging, as there are several other failure scenarios for missing an event, in addition to model failure, such as the non-wearing of sensors when performing the event of interest, poor data quality, mismatch of model to where (on the body) and how the sensor is worn (e.g., smartwatch on non-dominant hand), battery failure, and data loss. To achieve generalizability, the model performance must be achieved on independent test data that covers all aspects of daily life, without any data selection. Finally, for the model to be used by others for real-life, the model should not only be accessible to the community, it should also be possible to train and test the model on different datasets by independent non-ML-expert researchers.},
booktitle = {Proceedings of Deep Learning for Wellbeing Applications Leveraging Mobile Devices and Edge Computing},
pages = {1},
numpages = {1},
keywords = {Mobile Health (mHealth), Machine Learning Models},
location = {Toronto, ON, Canada},
series = {HealthDL'20}
}

@inproceedings{10.1145/3548785.3548797,
author = {Guyot, Alexis and Gillet, Annabelle and Leclercq, Eric and Cullot, Nadine},
title = {A Formal Framework for Data Lakes Based on Category Theory},
year = {2022},
isbn = {9781450397094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548785.3548797},
doi = {10.1145/3548785.3548797},
abstract = {The management of Big Data requires flexible systems to handle the heterogeneity of data models as well as the complexity of analytical workflows. Traditional systems like data warehouses have reached their limits due to their rigid schema-on-write paradigm, that requires well identified and defined use cases to ingest data. Data lakes, with their schema-on-read paradigm, have been proposed as more flexible systems in which raw data are directly stored in their original format associated with metadata, to be accessed and transformed only when users need to process or analyze them. Thus, it is necessary to define and control the different levels of abstraction and the dependencies among functionalities of a data lake to use it efficiently. In this article, we present a formal framework aiming to define a data lake pattern and to unify the interactions among the functionalities. We use the category theory as theoretical foundations to benefit from its high level of abstraction and its compositionality. By relying on different categories and functors, we ensure the navigation among the functionalities and allow the composition of multiples operations, while keeping track of the entire lineage of data. We also show how our framework can be applied on a simple example of data lake.},
booktitle = {Proceedings of the 26th International Database Engineered Applications Symposium},
pages = {75–83},
numpages = {9},
keywords = {Data Lakes, Category Theory, Architecture Pattern},
location = {Budapest, Hungary},
series = {IDEAS '22}
}

@inproceedings{10.1145/3447568.3448537,
author = {Sang, Go Muan and Xu, Lai and de Vrieze, Paul and Bai, Yuewei and Pan, Fangyu},
title = {Predictive Maintenance in Industry 4.0},
year = {2021},
isbn = {9781450376556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447568.3448537},
doi = {10.1145/3447568.3448537},
abstract = {In the context of Industry 4.0, the manufacturing related processes have shifted from conventional processes within one organization to collaborative processes cross different organizations, for example, product design processes, manufacturing processes, and maintenance processes across different factories and enterprises. The application of Internet of things, i.e. smart devices and sensors increases collection and availability of diverse data. Advanced technologies such as big data analytics and cloud computing offer new opportunities for effective optimization of manufacturing related processes, e.g. predictive maintenance. Predictive maintenance provides a detailed examination of the detection, location and diagnosis of faults in related machineries using various analyses. RAMI4.0 is a framework for thinking about the various efforts that constitute Industry 4.0. It spans the entire product life cycle &amp; value stream axis, hierarchical structure axis and functional classification axis. The Industrial Data Space (now International Data Space) is a virtual data space using standards and common governance models to facilitate the secure exchange and easy linkage of data in business ecosystems. It thereby provides a basis for creating and using smart services and innovative business processes, while at the same time ensuring digital sovereignty of data owners. This paper looks at how to support predictive maintenance in the context of Industry 4.0? Especially, applying RAMI 4.0 architecture supports the predictive maintenance using FIWARE framework, which leads to deal with data exchanging among different organizations with different security requirements as well as modularizing of related functions.},
booktitle = {Proceedings of the 10th International Conference on Information Systems and Technologies},
articleno = {29},
numpages = {11},
keywords = {Predictive maintenance, Collaborative business process, Blockchain, Industrial data space, Industry 4.0, FIWARE},
location = {Lecce, Italy},
series = {ICIST '20}
}

@inproceedings{10.1145/3388176.3388210,
author = {Duan, Xuliang and Guo, Bing and Shen, Yan and Shen, Yuncheng and Dong, Xiangqian and Zhang, Hong},
title = {Research on Parallel Data Currency Rule Algorithms},
year = {2020},
isbn = {9781450377256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388176.3388210},
doi = {10.1145/3388176.3388210},
abstract = {Data currency is a temporal reference of data, which is related to the value of data and affects the results of data analysis and mining. The currency rules that reflect the time series features of data can be used not only for data repairing, but also for data quality evaluation. However, with the rapid growth and dynamic update of data volume, both the forms and algorithms of basic currency rule are facing severe challenges in application. Therefore, based on the research on data currency repairing, we extended the basic currency rule form, and proposed rule extraction and incremental updating algorithms that can run in parallel on dynamic data set. The experimental results show that, compared with non-parallel methods, the efficiency of parallel algorithms is significantly improved.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Information Science and System},
pages = {24–28},
numpages = {5},
keywords = {dynamic data, parallel algorithm, data currency rule, Data currency},
location = {Cambridge, United Kingdom},
series = {ICISS 2020}
}

@inproceedings{10.1145/3548785.3548793,
author = {Endres, Markus and Mannarapotta Venugopal, Asha and Tran, Tung Son},
title = {Synthetic Data Generation: A Comparative Study},
year = {2022},
isbn = {9781450397094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548785.3548793},
doi = {10.1145/3548785.3548793},
abstract = {Generating synthetic data similar to realistic data is a crucial task in data augmentation and data production. Due to the preservation of authentic data distribution, synthetic data provide concealment of sensitive information and therefore enable Big Data acquisition for model training without facing privacy challenges. Nevertheless, the obstacles arise starting with acquiring real-world open-source data to effectively synthesizing new samples as genuine as possible. In this paper, a comparative study is conducted by considering the efficacy of different generative models like Generative Adversarial Networks (GAN), Variational Autoencoder (VAE), Synthetic Minority Oversampling Technique (SMOTE), Data Synthesizer (DS), Synthetic Data Vault with Gaussian Copula (SDV-G), Conditional Generative Adversarial Networks (SDV-GAN), and SynthPop Non-Parametric (SP-NP) approach to synthesize data with regard to various datasets. We used the pairwise correlation and Synthetic Data (SD) metrics as utility measures respectively between real data and generated data for evaluation. Accordingly, this paper investigates the effects of various data generation models, and the processing time of every model is included as one of the evaluation metrics.},
booktitle = {Proceedings of the 26th International Database Engineered Applications Symposium},
pages = {94–102},
numpages = {9},
keywords = {Generative Models, Synthetic Data, Neural Networks},
location = {Budapest, Hungary},
series = {IDEAS '22}
}

@inproceedings{10.1145/2882903.2904442,
author = {Zhang, Ce and Shin, Jaeho and R\'{e}, Christopher and Cafarella, Michael and Niu, Feng},
title = {Extracting Databases from Dark Data with DeepDive},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2904442},
doi = {10.1145/2882903.2904442},
abstract = {DeepDive is a system for extracting relational databases from dark data: the mass of text, tables, and images that are widely collected and stored but which cannot be exploited by standard relational tools. If the information in dark data --- scientific papers, Web classified ads, customer service notes, and so on --- were instead in a relational database, it would give analysts access to a massive and highly-valuable new set of "big data" to exploit.DeepDive is distinctive when compared to previous information extraction systems in its ability to obtain very high precision and recall at reasonable engineering cost; in a number of applications, we have used DeepDive to create databases with accuracy that meets that of human annotators. To date we have successfully deployed DeepDive to create data-centric applications for insurance, materials science, genomics, paleontologists, law enforcement, and others. The data unlocked by DeepDive represents a massive opportunity for industry, government, and scientific researchers.DeepDive is enabled by an unusual design that combines large-scale probabilistic inference with a novel developer interaction cycle. This design is enabled by several core innovations around probabilistic training and inference.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {847–859},
numpages = {13},
keywords = {dark data, data integration, information extraction, knowledge base construction},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/3176648,
author = {Skorin-Kapov, Lea and Varela, Mart\'{\i}n and Ho\ss{}feld, Tobias and Chen, Kuan-Ta},
title = {A Survey of Emerging Concepts and Challenges for QoE Management of Multimedia Services},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3176648},
doi = {10.1145/3176648},
abstract = {Quality of Experience (QoE) has received much attention over the past years and has become a prominent issue for delivering services and applications. A significant amount of research has been devoted to understanding, measuring, and modelling QoE for a variety of media services. The next logical step is to actively exploit that accumulated knowledge to improve and manage the quality of multimedia services, while at the same time ensuring efficient and cost-effective network operations. Moreover, with many different players involved in the end-to-end service delivery chain, identifying the root causes of QoE impairments and finding effective solutions for meeting the end users’ requirements and expectations in terms of service quality is a challenging and complex problem. In this article, we survey state-of-the-art findings and present emerging concepts and challenges related to managing QoE for networked multimedia services. Going beyond a number of previously published survey articles addressing the topic of QoE management, we address QoE management in the context of ongoing developments, such as the move to softwarized networks, the exploitation of big data analytics and machine learning, and the steady rise of new and immersive services (e.g., augmented and virtual reality). We address the implications of such paradigm shifts in terms of new approaches in QoE modeling and the need for novel QoE monitoring and management infrastructures.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {may},
articleno = {29},
numpages = {29},
keywords = {QoE monitoring, SDN, crowdsourcing, monitoring probes, data analytics, encrypted traffic, NFV, QoE management, QoE modeling}
}

@inproceedings{10.1145/3330204.3330259,
author = {Mendes, Yan and Braga, Regina and Str\"{o}ele, Victor and de Oliveira, Daniel},
title = {Polyflow: A SOA for Analyzing Workflow Heterogeneous Provenance Data in Distributed Environments},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330259},
doi = {10.1145/3330204.3330259},
abstract = {In the last decade the (big) data-driven science paradigm became a wide-spread reality. However, this approach has some limitations such as a performance dependency on the quality of the data and the lack of reproducibility of the results. In order to enable this reproducibility, many tools such as Workflow Management Systems were developed to formalize process pipelines and capture execution traces. However, interoperating data generated by these solutions became a problem, since most systems adopted proprietary data models. To support interoperability across heterogeneous provenance data, we propose a Service Oriented Architecture with a polystore storage design in which provenance is conceptually represented utilizing the ProvONE model. A wrapper layer is responsible for transforming data described by heterogeneous formats into ProvONE-compliant. Moreover, we propose a query layer that provides location and access transparency to users. Furthermore, we conduct two feasibility studies, showcasing real usecase scenarios. Firstly, we illustrate how two research groups can compare their processes and results. Secondly, we show how our architecture can be used as a queriable provenance repository. We show Polyflow's viability for both scenarios using the Goal-Question-Metric methodology. Finally, we show our solution usability and extensibility appeal by comparing it to similar approaches.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {49},
numpages = {8},
keywords = {polystore, heterogeneous provenance data integration, Workflows interoperability},
location = {Aracaju, Brazil},
series = {SBSI'19}
}

@inproceedings{10.1145/3357777.3357781,
author = {Lv, Zhining and Hu, Ziheng and Ning, Baifeng and Li, Wei and Yan, Gangfeng and Ding, Lifu and Shi, Xiasheng and Guo, Ningxuan},
title = {Safety Monitoring of Power Industrial Control Terminals Based on Data Cleaning},
year = {2019},
isbn = {9781450372312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357777.3357781},
doi = {10.1145/3357777.3357781},
abstract = {Stable and high-quality electric energy is the main driving force for the development of social science, technology, and the national economic leap. The assessment and monitoring of electrical safety rely on the generation, collection and statistics of large amounts of data by the power system. For the possible problems and impurities in these data, this paper uses the 'local Chebyshev theorem' and the 'near data averaging method' for the attribute values. The error is cleaned, and the 'sorting neighbor algorithm' is used to clean the duplicate data, thereby improving the data quality and realizing the accuracy of the safety monitoring of the power grid of the smart grid.},
booktitle = {Proceedings of the 2019 the International Conference on Pattern Recognition and Artificial Intelligence},
pages = {7–11},
numpages = {5},
keywords = {Data cleaning, Proximity data averaging, Power monitoring, Chebyshev theory},
location = {Wenzhou, China},
series = {PRAI '19}
}

@article{10.1145/2893482,
author = {Aiken, Peter},
title = {EXPERIENCE: Succeeding at Data Management—BigCo Attempts to Leverage Data},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2893482},
doi = {10.1145/2893482},
abstract = {In a manner similar to most organizations, BigCompany (BigCo) was determined to benefit strategically from its widely recognized and vast quantities of data. (U.S. government agencies make regular visits to BigCo to learn from its experiences in this area.) When faced with an explosion in data volume, increases in complexity, and a need to respond to changing conditions, BigCo struggled to respond using a traditional, information technology (IT) project-based approach to address these challenges. As BigCo was not data knowledgeable, it did not realize that traditional approaches could not work. Two full years into the initiative, BigCo was far from achieving its initial goals. How much more time, money, and effort would be required before results were achieved? Moreover, could the results be achieved in time to support a larger, critical, technology-driven challenge that also depended on solving the data challenges? While these questions remain unaddressed, these considerations increase our collective understanding of data assets as separate from IT projects. Only by reconceiving data as a strategic asset can organizations begin to address these new challenges. Transformation to a data-driven culture requires far more than technology, which remains just one of three required “stool legs” (people and process being the other two). Seven prerequisites to effectively leveraging data are necessary, but insufficient awareness exists in most organizations—hence, the widespread misfires in these areas, especially when attempting to implement the so-called big data initiatives. Refocusing on foundational data management practices is required for all organizations, regardless of their organizational or data strategies.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {8},
numpages = {35},
keywords = {data, conceptual modeling, policy, IT management, chief information officer, organizational design, business intelligence, data stewardship, information systems, CIO, Data management, chief data officer, data integration, enterprise data executive, data governance, enterprise architecture, BigCo, analytics, data architecture, data warehousing, CDO, strategy}
}

@article{10.1145/3461839,
author = {Wang, Jingjing and Jiang, Wenjun and Li, Kenli and Wang, Guojun and Li, Keqin},
title = {Incremental Group-Level Popularity Prediction in Online Social Networks},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3461839},
doi = {10.1145/3461839},
abstract = {Predicting the popularity of web contents in online social networks is essential for many applications. However, existing works are usually under non-incremental settings. In other words, they have to rebuild models from scratch when new data occurs, which are inefficient in big data environments. It leads to an urgent need for incremental prediction, which can update previous results with new data and conduct prediction incrementally. Moreover, the promising direction of group-level popularity prediction has not been well treated, which explores fine-grained information while keeping a low cost. To this end, we identify the problem of incremental group-level popularity prediction, and propose a novel model IGPP to address it. We first predict the group-level popularity incrementally by exploiting the incremental CANDECOMP/PARAFCAC (CP) tensor decomposition algorithm. Then, to reduce the cumulative error by incremental prediction, we propose three strategies to restart the CP decomposition. To the best of our knowledge, this is the first work that identifies and solves the problem of incremental group-level popularity prediction. Extensive experimental results show significant improvements of the IGPP method over other works both in the prediction accuracy and the efficiency.},
journal = {ACM Trans. Internet Technol.},
month = {sep},
articleno = {20},
numpages = {26},
keywords = {information diffusion, tensor analysis, incremental approach, popularity prediction, Group level, online social networks}
}

@article{10.14778/3407790.3407802,
author = {Fan, Ju and Chen, Junyou and Liu, Tongyu and Shen, Yuwei and Li, Guoliang and Du, Xiaoyong},
title = {Relational Data Synthesis Using Generative Adversarial Networks: A Design Space Exploration},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407802},
doi = {10.14778/3407790.3407802},
abstract = {The proliferation of big data has brought an urgent demand for privacy-preserving data publishing. Traditional solutions to this demand have limitations on effectively balancing the tradeoff between privacy and utility of the released data. Thus, the database community and machine learning community have recently studied a new problem of relational data synthesis using generative adversarial networks (GAN) and proposed various algorithms. However, these algorithms are not compared under the same framework and thus it is hard for practitioners to understand GAN's benefits and limitations. To bridge the gaps, we conduct so far the most comprehensive experimental study that investigates applying GAN to relational data synthesis. We introduce a unified GAN-based framework and define a space of design solutions for each component in the framework, including neural network architectures and training strategies. We conduct extensive experiments to explore the design space and compare with traditional data synthesis approaches. Through extensive experiments, we find that GAN is very promising for relational data synthesis, and provide guidance for selecting appropriate design solutions. We also point out limitations of GAN and identify future research directions.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {1962–1975},
numpages = {14}
}

@article{10.14778/3282495.3282496,
author = {Bleifu\ss{}, Tobias and Bornemann, Leon and Johnson, Theodore and Kalashnikov, Dmitri V. and Naumann, Felix and Srivastava, Divesh},
title = {Exploring Change: A New Dimension of Data Analytics},
year = {2018},
issue_date = {October 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3282495.3282496},
doi = {10.14778/3282495.3282496},
abstract = {Data and metadata in datasets experience many different kinds of change. Values are inserted, deleted or updated; rows appear and disappear; columns are added or repurposed, etc. In such a dynamic situation, users might have many questions related to changes in the dataset, for instance which parts of the data are trustworthy and which are not? Users will wonder: How many changes have there been in the recent minutes, days or years? What kind of changes were made at which points of time? How dirty is the data? Is data cleansing required? The fact that data changed can hint at different hidden processes or agendas: a frequently crowd-updated city name may be controversial; a person whose name has been recently changed may be the target of vandalism; and so on. We show various use cases that benefit from recognizing and exploring such change.We envision a system and methods to interactively explore such change, addressing the variability dimension of big data challenges. To this end, we propose a model to capture change and the process of exploring dynamic data to identify salient changes. We provide exploration primitives along with motivational examples and measures for the volatility of data. We identify technical challenges that need to be addressed to make our vision a reality, and propose directions of future work for the data management community.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {85–98},
numpages = {14}
}

@inproceedings{10.1145/2559206.2560469,
author = {Meyer, Jochen and Simske, Steven and Siek, Katie A. and Gurrin, Cathal G. and Hermens, Hermie},
title = {Beyond Quantified Self: Data for Wellbeing},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2560469},
doi = {10.1145/2559206.2560469},
abstract = {Sustaining our health and wellbeing requires lifelong efforts for prevention and healthy living. Continuously observing ourselves is one of the fundamental measures to be taken. While many devices support monitoring and quantifying our health behavior and health state, they all are facing the same trade-off: the higher the data quality is the higher are the efforts of data acquisition. However, for lifelong use, minimizing efforts for the user is crucial. Nowadays, few devices find a good balance between cost and value. In this interdisciplinary workshop we discuss how this trade-off can be approached by addressing three topics: understanding the user's information needs, exploring options for data acquisition, and discussing potential designs for life-long use.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {95–98},
numpages = {4},
keywords = {data analysis, wellbeing, user oriented design},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}

@article{10.1145/3522591,
author = {Xiao, Houping and Wang, Shiyu},
title = {Toward Quality of Information Aware Distributed Machine Learning},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3522591},
doi = {10.1145/3522591},
abstract = {In the era of big data, data are usually distributed across numerous connected computing and storage units (i.e., nodes or workers). Under such an environment, many machine learning problems can be reformulated as a consensus optimization problem, which consists of one objective and constraint terms splitting into N parts (each corresponds to a node). Such a problem can be solved efficiently in a distributed manner via Alternating Direction Method of Multipliers (ADMM). However, existing consensus optimization frameworks assume that every node has the same quality of information (QoI), i.e., the data from all the nodes are equally informative for the estimation of global model parameters. As a consequence, they may lead to inaccurate estimates in the presence of nodes with low QoI. To overcome this challenge, in this article, we propose a novel consensus optimization framework for distributed machine-learning that incorporates the crucial metric, QoI. Theoretically, we prove that the convergence rate of the proposed framework is linear to the number of iterations, but has a tighter upper bound compared with ADMM. Experimentally, we show that the proposed framework is more efficient and effective than existing ADMM-based solutions on both synthetic and real-world datasets due to its faster convergence rate and higher accuracy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {109},
numpages = {28},
keywords = {Distributed machine learning, quality of information}
}

@article{10.1145/3564276,
author = {Srinivasan, Karthik and Currim, Faiz and Ram, Sudha},
title = {A Human-in-the-Loop Segmented Mixed-Effects Modeling Method For Analyzing Wearables Data},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2158-656X},
url = {https://doi.org/10.1145/3564276},
doi = {10.1145/3564276},
abstract = {Wearables are an important source of big data as they provide real-time high-resolution data logs of health indicators of individuals. Higher-order associations between pairs of variables is common in wearables data. Representing higher-order association curves as piece-wise linear segments in a regression model makes them more interpretable. However, existing methods for identifying the change points for segmented modeling either overfit or have low external validity for wearables data containing repeated measures. Therefore, we propose a human-in-the-loop method for segmented modeling of higher-order pairwise associations between variables in wearables data. Our method uses the smooth function estimated by a generalized additive mixed model to allow the analyst to annotate change point estimates for a segmented mixed-effects model, and thereafter employs the Brent's constrained optimization procedure to fine-tuning the manually provided estimates. We validate our method using three real-world wearables datasets. Our method not only outperforms state-of-the-art modeling methods in terms of prediction performance but also provides more interpretable results. Our study contributes to health data science in terms of developing a new method for interpretable modeling of wearables data. Our analysis uncovers interesting insights on higher order associations for health researchers.},
note = {Just Accepted},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {sep},
keywords = {human-in-the-loop method, segmented mixed-effects regression, interpretable modeling, explainability, wearables, smart health}
}

@article{10.1145/3275520,
author = {Sangogboye, Fisayo Caleb and Jia, Ruoxi and Hong, Tianzhen and Spanos, Costas and Kj\ae{}rgaard, Mikkel Baun},
title = {A Framework for Privacy-Preserving Data Publishing with Enhanced Utility for Cyber-Physical Systems},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3–4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3275520},
doi = {10.1145/3275520},
abstract = {Cyber-physical systems have enabled the collection of massive amounts of data in an unprecedented level of spatial and temporal granularity. Publishing these data can prosper big data research, which, in turn, helps improve overall system efficiency and resiliency. The main challenge in data publishing is to ensure the usefulness of published data while providing necessary privacy protection. In our previous work&nbsp;(Jia et al. 2017a), we presented a privacy-preserving data publishing framework (referred to as PAD hereinafter), which can guarantee k-anonymity while achieving better data utility than traditional anonymization techniques. PAD learns the information of interest to data users or features from their interactions with the data publishing system and then customizes data publishing processes to the intended use of data. However, our previous work is only applicable to the case where the desired features are linear in the original data record. In this article, we extend PAD to nonlinear features. Our experiments demonstrate that for various data-driven applications, PAD can achieve enhanced utility while remaining highly resilient to privacy threats.},
journal = {ACM Trans. Sen. Netw.},
month = {nov},
articleno = {30},
numpages = {22},
keywords = {smart buildings, Privacy preservation, cyber-physical systems, deep learning, k-anonymity}
}

@inproceedings{10.1145/2964284.2976761,
author = {Tang, Mengfan and Pongpaichet, Siripen and Jain, Ramesh},
title = {Research Challenges in Developing Multimedia Systems for Managing Emergency Situations},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2976761},
doi = {10.1145/2964284.2976761},
abstract = {With an increasing amount of diverse heterogeneous data and information, the methodology of multimedia analysis has become increasingly relevant in solving challenging societal problems such as managing emergency situations during disasters. Using cybernetic principles combined with multimedia technology, researchers can develop effective frameworks for using diverse multimedia (including traditional multimedia as well as diverse multimodal) data for situation recognition, and determining and communicating appropriate actions to people stranded during disasters. We present known issues in disaster management and then focus on emergency situations. We show that an emergency management problem is fundamentally a multimedia information assimilation problem for situation recognition and for connecting people's needs to available resources effectively, efficiently, and promptly. Major research challenges for managing emergency situations are identified and discussed. We also present a intelligently detecting evolving environmental situations, and discuss the role of multimedia micro-reports as spontaneous participatory sensing data streams in emergency responses. Given enormous progress in concept recognition using machine learning in the last few years, situation recognition may be the next major challenge for learning approaches in multimedia contextual big data. The data needed for developing such approaches is now easily available on the Web and many challenging research problems in this area are ripe for exploration in order to positively impact our society during its most difficult times.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {938–947},
numpages = {10},
keywords = {situation prediction, situation recognition, eventshop, micro-reports, disaster},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@article{10.5555/3204979.3204980,
author = {Nie, Yu and Talburt, John and Li, Xinming and Xiao, Zhongdong},
title = {Chief Data Officer (CDO) Role and Responsibility Analysis},
year = {2018},
issue_date = {May 2018},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {33},
number = {5},
issn = {1937-4771},
abstract = {While the number of organizations creating the role of Chief Data Officer (CDO) is increasing each year, the nature of the role is still emerging. CDO management responsibilities can vary widely from company to company. The study focuses on the various management responsibilities of the CDO role and their commonalities across organizations. After collecting and analyzing CDO job description from 411 organizations, we came to the following conclusions. Data analytics and business management are the most often cited and thus the most important management responsibilities for the CDO. Second is the management of data quality and data governance programs. Third, the CDO should keep abreast of new information technologies that could help firms design and execute an enterprise data strategy that coordinates the firm's business intelligence processes, leads to the development of new products, and acquires new customers through new data media.},
journal = {J. Comput. Sci. Coll.},
month = {may},
pages = {4–12},
numpages = {9},
keywords = {business management, chief data officer (CDO), data analytics, role and responsibility analysis}
}

@article{10.1145/3470918,
author = {Karmaker (“Santu”), Shubhra Kanti and Hassan, Md. Mahadi and Smith, Micah J. and Xu, Lei and Zhai, Chengxiang and Veeramachaneni, Kalyan},
title = {AutoML to Date and Beyond: Challenges and Opportunities},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3470918},
doi = {10.1145/3470918},
abstract = {As big data becomes ubiquitous across domains, and more and more stakeholders aspire to make the most of their data, demand for machine learning tools has spurred researchers to explore the possibilities of automated machine learning (AutoML). AutoML tools aim to make machine learning accessible for non-machine learning experts (domain experts), to improve the efficiency of machine learning, and to accelerate machine learning research. But although automation and efficiency are among AutoML’s main selling points, the process still requires human involvement at a number of vital steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training dataset, and selecting a promising machine learning technique. These steps often require a prolonged back-and-forth that makes this process inefficient for domain experts and data scientists alike and keeps so-called AutoML systems from being truly automatic. In this review article, we introduce a new classification system for AutoML systems, using a seven-tiered schematic to distinguish these systems based on their level of autonomy. We begin by describing what an end-to-end machine learning pipeline actually looks like, and which subtasks of the machine learning pipeline have been automated so far. We highlight those subtasks that are still done manually—generally by a data scientist—and explain how this limits domain experts’ access to machine learning. Next, we introduce our novel level-based taxonomy for AutoML systems and define each level according to the scope of automation support provided. Finally, we lay out a roadmap for the future, pinpointing the research required to further automate the end-to-end machine learning pipeline and discussing important challenges that stand in the way of this ambitious goal.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {175},
numpages = {36},
keywords = {interactive data science, Automated machine learning, predictive analytics, democratization of artificial intelligence}
}

@inproceedings{10.1145/2939672.2945365,
author = {Mierswa, Ingo},
title = {The Wisdom of Crowds: Best Practices for Data Prep &amp; Machine Learning Derived from Millions of Data Science Workflows},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2945365},
doi = {10.1145/2939672.2945365},
abstract = {With hundreds of thousands of users, RapidMiner is the most frequently used visual workflow platform for machine learning. It covers the full spectrum of analytics from data preparation to machine learning and model validation. In this presentation, I will take you on a tour of machine learning which spans the last 15 years of research and industry applications and share key insights with you about how data scientists perform their daily analysis tasks. These patterns are extracted from mining millions of analytical workflows that have been created with RapidMiner over the past years. This talk will address important questions around the data mining process such as: What are the most frequently used solutions for typical data quality problems? How often are analysts using decision trees or neural networks? And does this behavior change over time or depend on the users experience level?},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {411},
numpages = {1},
keywords = {machine learning tools, data visualization, wisdom of the crowds, visual workflow, analytics},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3469213.3470246,
author = {Wu, Yang and Zou, Wentao and Liu, Shuangquan and Jiang, Yan and Shao, Qizhuan and Zhou, Han},
title = {Rule-Based Data Verification Method in Electricity Spot Market},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470246},
doi = {10.1145/3469213.3470246},
abstract = {In the electric power spot market, input data quality is critical to an accurate and reliable clearing result. Missing and abnormal data will lead to the result that the clearing algorithm diverges or the results mismatch reality. This would seriously affect power system security and reduce the efficiency of the market. Therefore, to ensure a smooth convergence of the algorithm and reasonable results, this paper proposes a rule-based verification method for the input data in the power spot market. Data integrity and logical verification rules are established. During the verification process, information will be divided into different warning levels and displayed so that users can modify relevant data according to the actual situation.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {46},
numpages = {5},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/2832087.2832090,
author = {Lopez, M. Graham and Young, Jeffrey and Meredith, Jeremy S. and Roth, Philip C. and Horton, Mitchel and Vetter, Jeffrey S.},
title = {Examining Recent Many-Core Architectures and Programming Models Using SHOC},
year = {2015},
isbn = {9781450340090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2832087.2832090},
doi = {10.1145/2832087.2832090},
abstract = {The Scalable HeterOgeneous Computing (SHOC) benchmark suite was released in 2010 as a tool to evaluate the stability and performance of emerging heterogeneous architectures and to compare different programming models for compute devices used in those architectures. Since then, high-performance computing (HPC) system architectures have increasingly incorporated both discrete and fused multi-core and many-core processors. The TOP500 list illustrates this trend: heterogeneous systems grew from a 3.4% to 18.0% share of the list between June 2010 and June 2015. Not only are there more heterogeneous systems on the TOP500 list today, those machines are responsible for a disproportionately large percentage of list's aggregate performance: as of June 2015, the performance share for heterogeneous systems has grown to 33.7%.Part of this shift toward heterogeneous architectures has stemmed from new products in the hardware accelerator market, such as Intel's Xeon Phi coprocessor, and improvements in the approaches for programming such accelerators. Existing approaches such as CUDA and OpenCL have become more powerful and easy to use, and directive-based programming models such as OpenACC, OpenMP 4.0, and Intel's Language Extensions for Offload (LEO) are rapidly gaining user acceptance. The benefits of these hardware and software advances are not limited to HPC; other problem domains such as "big data" are reaping the rewards also.The original SHOC benchmarks had adequate support for CUDA and OpenCL for graphics processing units, but did not support more recent programming models and devices. We extended SHOC to support evaluation of recent heterogeneous architectures and programming models such as OpenACC and LEO, and we added new benchmarks to increase SHOC's application domain coverage. In this paper, we describe our modifications to the stock SHOC distribution and present several examples of using our augmented version of SHOC for evaluation of recent heterogeneous architectures and programming models.},
booktitle = {Proceedings of the 6th International Workshop on Performance Modeling, Benchmarking, and Simulation of High Performance Computing Systems},
articleno = {3},
numpages = {12},
keywords = {accelerators, performance, benchmarking},
location = {Austin, Texas},
series = {PMBS '15}
}

@inproceedings{10.1145/3559795.3559798,
author = {Meng, Qi},
title = {Blockchain-Based Security Governance Framework of Agricultural Product Traceability Data},
year = {2022},
isbn = {9781450396622},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3559795.3559798},
doi = {10.1145/3559795.3559798},
abstract = {The traceability of agricultural products provides a means for modern management of agricultural product quality and safety. Due to factors such as diverse collection modes and poor sharing and exchange channels in the agricultural product traceability system, the traceability data faces problems such as data centralization, low data quality, and weak data security management and control. By introducing blockchain technology in the whole life cycle of agricultural product traceability data, a blockchain-based security governance framework of agricultural product traceability data is established, and preliminary implementation is carried out in Hyperledger Fabric, which provides thoughts and suggestions for security governance of agricultural product traceability data.},
booktitle = {Proceedings of the 2022 4th Blockchain and Internet of Things Conference},
pages = {16–21},
numpages = {6},
keywords = {Data security governance, Agricultural product traceability data, Hyperledger Fabric, Blockchain},
location = {Tokyo, China},
series = {BIOTC '22}
}

@inproceedings{10.1145/2882903.2903736,
author = {Rheinl\"{a}nder, Astrid and Lehmann, Mario and Kunkel, Anja and Meier, J\"{o}rg and Leser, Ulf},
title = {Potential and Pitfalls of Domain-Specific Information Extraction at Web Scale},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2903736},
doi = {10.1145/2882903.2903736},
abstract = {In many domains, a plethora of textual information is available on the web as news reports, blog posts, community portals, etc. Information extraction (IE) is the default technique to turn unstructured text into structured fact databases, but systematically applying IE techniques to web input requires highly complex systems, starting from focused crawlers over quality assurance methods to cope with the HTML input to long pipelines of natural language processing and IE algorithms. Although a number of tools for each of these steps exists, their seamless, flexible, and scalable combination into a web scale end-to-end text analytics system still is a true challenge. In this paper, we report our experiences from building such a system for comparing the "web view" on health related topics with that derived from a controlled scientific corpus, i.e., Medline. The system combines a focused crawler, applying shallow text analysis and classification to maintain focus, with a sophisticated text analytic engine inside the Big Data processing system Stratosphere. We describe a practical approach to seed generation which led us crawl a corpus of ~1 TB web pages highly enriched for the biomedical domain. Pages were run through a complex pipeline of best-of-breed tools for a multitude of necessary tasks, such as HTML repair, boilerplate detection, sentence detection, linguistic annotation, parsing, and eventually named entity recognition for several types of entities. Results are compared with those from running the same pipeline (without the web-related tasks) on a corpus of 24 million scientific abstracts and a third corpus made of ~250K scientific full texts. We evaluate scalability, quality, and robustness of the employed methods and tools. The focus of this paper is to provide a large, real-life use case to inspire future research into robust, easy-to-use, and scalable methods for domain-specific IE at web scale.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {759–771},
numpages = {13},
keywords = {massively parallel data analysis, focused crawling, information extraction},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3334480.3382864,
author = {Sharbatdar, Nasim and Lamine, Yassine and Milord, Brigitte and Morency, Catherine and Cheng, Jinghui},
title = {Capturing the Practices, Challenges, and Needs of Transportation Decision-Makers},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382864},
doi = {10.1145/3334480.3382864},
abstract = {Transportation decision-makers from government agencies play an important role in addressing the traffic network conditions, which in turn, have a major impact on the well-being of citizens. The practices, challenges, and needs of this group of practitioners are less represented in the HCI literature. We address this gap through an interview study with 19 practitioners from Transports Qu\'{e}bec, a government agency responsible for transportation infrastructures in Qu\'{e}bec, Canada. We found that this group of decision-makers can most benefit from research about data analysis tools and platforms that (1) provide information to support data quality awareness, (2) are interoperable with other tools in the complex workflow of the practitioners, and (3) support intuitive and customizable visual analytics. These implications can also be informative to the design of tools supporting other decision-making tasks and domains.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–7},
numpages = {7},
keywords = {decision-making, persona, user study, decision-maker, transportation management and planning},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@article{10.1145/3439873,
author = {Neto, Nelson Novaes and Madnick, Stuart and Paula, Anchises Moraes G. De and Borges, Natasha Malara},
title = {Developing a Global Data Breach Database and the Challenges Encountered},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3439873},
doi = {10.1145/3439873},
abstract = {If the mantra “data is the new oil” of our digital economy is correct, then data leak incidents are the critical disasters in the online society. The initial goal of our research was to present a comprehensive database of data breaches of personal information that took place in 2018 and 2019. This information was to be drawn from press reports, industry studies, and reports from regulatory agencies across the world. This article identified the top 430 largest data breach incidents among more than 10,000 data breach incidents.In the process, we encountered many complications, especially regarding the lack of standardization of reporting. This article should be especially interesting to the readers of JDIQ because it describes both the range of data quality and consistency issues found as well as what was learned from the database created.The database that was created, available at https://www.databreachdb.com, shows that the number of data records breached in those top 430 incidents increased from around 4B in 2018 to more than 22B in 2019. This increase occurred despite the strong efforts from regulatory agencies across the world to enforce strict rules on data protection and privacy, such as the General Data Protection Regulation (GDPR) that went into effect in Europe in May 2018. Such regulatory effort could explain the reason why there is such a large number of data breach cases reported in the European Union when compared to the U.S. (more than 10,000 data breaches publicly reported in the U.S. since 2018, while the EU reported more than 160,0001 data breaches since May 2018). However, we still face the problem of an excessive number of breach incidents around the world.This research helps to understand the challenges of proper visibility of such incidents on a global scale. The results of this research can help government entities, regulatory bodies, security and data quality researchers, companies, and managers to improve the data quality of data breach reporting and increase the visibility of the data breach landscape around the world in the future.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {3},
numpages = {33},
keywords = {data aggregation, data breach, semantics of data, privacy, Cyber security}
}

@inproceedings{10.1145/3511716.3511730,
author = {Yang, Jie and Cao, Yong},
title = {The Classification of Gene Sequencer Based on Machine Learning},
year = {2022},
isbn = {9781450395687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511716.3511730},
doi = {10.1145/3511716.3511730},
abstract = {Abstract: Biological sequencing plays a very important role in life science, especially with the improvement of sequencing technology and the development of sequencing instruments, and a large number of biological sequencing quality data are produced every day. Because of different sequencers, the quality of sequencing is different. In the process of sequencing quality control, the model of sequencer can be deduced according to the quality of gene sequence. Therefore, in this paper, five sequencers of Illumina HiSeq series, Illumina HiSeq 2000, Illumina HiSeq 2500, Illumina HiSeq 3000, Illumina HiSeq 4000 and Illumina HiSeq XTen, are selected as the classification objects. Firstly, the sequencing quality data of the five sequencers are preprocessed. Then, the classification model is trained by three machine learning algorithms: decision tree, logistic regression and support vector machine. The experimental results show that the accuracy rates of the three machine learning algorithms are 96.67%, 97.50% and 97.50% respectively. These algorithms are very good to solve the problem of using biological sequencing data quality to classify sequencer.},
booktitle = {2021 4th International Conference on E-Business, Information Management and Computer Science},
pages = {84–88},
numpages = {5},
keywords = {Classification of gene Sequencer, Quality of sequencing, Machine learning},
location = {Hong Kong, China},
series = {EBIMCS 2021}
}

@inproceedings{10.1145/3312714.3312717,
author = {Li, Pei and Dai, Chaofan and Wang, Wenqian},
title = {Application of Attribute Correlation in Unsupervised Data Cleaning},
year = {2019},
isbn = {9781450362351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312714.3312717},
doi = {10.1145/3312714.3312717},
abstract = {Referring to the supervised learning and unsupervised learning in machine learning, we divide the data cleaning processes into supervised and unsupervised two forms too, and then, we reclassify the data quality problems into canonicalization error, redundancy error, strong logic error and weak logic error according to the characteristics of unsupervised cleaning. For the weak logic errors, we propose a repair framework AC-Framework and an algorithm AC-Repair based on the attribute correlation. When repairing, we first establish a priority queue(PQ) for elements to be repaired according to the minimum cost idea and take the corresponding conflict-free data set(Icf) as a training set to learn the correlation among attributes. Then, we select the first element in PQ list as the candidate element to repair, and recompute the PQ list after one repair round to improve the efficiency. Finally, in order to prevent the algorithm from endless loops, we set a label flag to mark the repaired elements, in this way, every error element will be repaired at most once. In the experimental part, we compare the AC-Repair algorithm with the interpolation-based repair algorithm to verify its validity.},
booktitle = {Proceedings of the 5th International Conference on E-Society, e-Learning and e-Technologies},
pages = {45–51},
numpages = {7},
keywords = {weak logic errors, attribute correlation, minimum repair cost, Unsupervised data cleaning, machine learning},
location = {Vienna, Austria},
series = {ICSLT '19}
}

@inproceedings{10.1145/3110025.3110161,
author = {Ahmadov, Ahmad and Thiele, Maik and Lehner, Wolfgang and Wrembel, Robert},
title = {Context Similarity for Retrieval-Based Imputation},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3110161},
doi = {10.1145/3110025.3110161},
abstract = {Completeness as one of the four major dimensions of data quality is a pervasive issue in modern databases. Although data imputation has been studied extensively in the literature, most of the research is focused on inference-based approach. We propose to harness Web tables as an external data source to effectively and efficiently retrieve missing data while taking into account the inherent uncertainty and lack of veracity that they contain.Existing approaches mostly rely on standard retrieval techniques and out-of-the-box matching methods which result in a very low precision, especially when dealing with numerical data. We, therefore, propose a novel data imputation approach by applying numerical context similarity measures which results in a significant increase in the precision of the imputation procedure, by ensuring that the imputed values are of the same domain and magnitude as the local values, thus resulting in an accurate imputation.We use Dresden Web Table Corpus which is comprised of more than 125 million web tables extracted from the Common Crawl as our knowledge source. The comprehensive experimental results demonstrate that the proposed method well outperforms the default out-of-the-box retrieval approach.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {1017–1024},
numpages = {8},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3336191.3371871,
author = {Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin and Mukherjee, Avijit},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371871},
doi = {10.1145/3336191.3371871},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the "why" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [11, 14]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 [23]. It was attended by around 150 participants.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {877–880},
numpages = {4},
keywords = {user experience evaluation, controlled experiments, a/b testing, online metrics},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@article{10.1145/3552490.3552494,
author = {Dave, Dev and Celestino, Angelica and Varde, Aparna S. and Anu, Vaibhav},
title = {Management of Implicit Requirements Data in Large SRS Documents: Taxonomy and Techniques},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3552490.3552494},
doi = {10.1145/3552490.3552494},
abstract = {Implicit Requirements (IMR) identification is part of the Requirements Engineering (RE) phase in Software Engineering during which data is gathered to create SRS (Software Requirements Specifications) documents. As opposed to explicit requirements clearly stated, IMRs constitute subtle data and need to be inferred. Research has shown that IMRs are crucial to the success of software development. Many software systems can encounter failures due to lack of IMR data management. SRS documents are large, often hundreds of pages, due to which manually identifying IMRs by human software engineers is not feasible. Moreover, such data is evergrowing due to the expansion of software systems. It is thus important to address the crucial issue of IMR data management. This article presents a survey on IMRs in SRS documents with the definition and overview of IMR data, detailed taxonomy of IMRs with explanation and examples, practices in managing IMR data, and tools for IMR identification. In addition to reviewing classical and state-of-the-art approaches, we highlight trends and challenges and point out open issues for future research. This survey article is interesting based on data quality, hidden information retrieval, veracity and salience, and knowledge discovery from large textual documents with complex heterogeneous data.},
journal = {SIGMOD Rec.},
month = {jul},
pages = {18–29},
numpages = {12}
}

@article{10.14778/3317315.3317318,
author = {Fan, Wenfei and Lu, Ping and Tian, Chao and Zhou, Jingren},
title = {Deducing Certain Fixes to Graphs},
year = {2019},
issue_date = {March 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3317315.3317318},
doi = {10.14778/3317315.3317318},
abstract = {This paper proposes to deduce certain fixes to graphs G based on data quality rules Σ and ground truth Γ (i.e., validated attribute values and entity matches). We fix errors detected by Σ in G such that the fixes are assured correct as long as Σand Γ are correct. We deduce certain fixes in two paradigms. (a) We interact with users and "incrementally" fix errors online. Whenever users pick a small set V0 of nodes in G, we fix all errors pertaining to V0 and accumulate ground truth in the process. (b) Based on accumulated Γ, we repair the entire graph G offline; while this may not correct all errors in G, all fixes are guaranteed certain.We develop techniques for deducing certain fixes. (1) We define data quality rules to support conditional functional dependencies, recursively defined keys and negative rules on graphs, such that we can deduce fixes by combining data repairing and object identification. (2) We show that deducing certain fixes is Church-Rosser, i.e., the deduction converges at the same fixes regardless of the order of rules applied. (3) We establish the complexity of three fundamental problems associated with certain fixes. (4) We provide (parallel) algorithms for deducing certain fixes online and offline, and guarantee to reduce running time when given more processors. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of our methods.},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {752–765},
numpages = {14}
}

@article{10.14778/2536274.2536304,
author = {Civili, Cristina and Console, Marco and De Giacomo, Giuseppe and Lembo, Domenico and Lenzerini, Maurizio and Lepore, Lorenzo and Mancini, Riccardo and Poggi, Antonella and Rosati, Riccardo and Ruzzi, Marco and Santarelli, Valerio and Savo, Domenico Fabio},
title = {Mastro Studio: Managing Ontology-Based Data Access Applications},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536274.2536304},
doi = {10.14778/2536274.2536304},
abstract = {Ontology-based data access (OBDA) is a novel paradigm for accessing large data repositories through an ontology, that is a formal description of a domain of interest. Supporting the management of OBDA applications poses new challenges, as it requires to provide effective tools for (i) allowing both expert and non-expert users to analyze the OBDA specification, (ii) collaboratively documenting the ontology, (iii) exploiting OBDA services, such as query answering and automated reasoning over ontologies, e.g., to support data quality check, and (iv) tuning the OBDA application towards optimized performances. To fulfill these challenges, we have built a novel system, called MASTRO STUDIO, based on a tool for automated reasoning over ontologies, enhanced with a suite of tools and optimization facilities for managing OBDA applications. To show the effectiveness of MASTRO STUDIO, we demonstrate its usage in one OBDA application developed in collaboration with the Italian Ministry of Economy and Finance.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1314–1317},
numpages = {4}
}

@inproceedings{10.1145/3366424.3383117,
author = {Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383117},
doi = {10.1145/3366424.3383117},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the “why” behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program 11, 14. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 23. It was attended by around 150 participants. This tutorial has also been accepted for the WSDM 2020 conference.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {317–319},
numpages = {3},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3177732.3177739,
author = {Lin, Xueling and Chen, Lei},
title = {Domain-Aware Multi-Truth Discovery from Conflicting Sources},
year = {2018},
issue_date = {January 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.1145/3177732.3177739},
doi = {10.1145/3177732.3177739},
abstract = {In the Big Data era, truth discovery has served as a promising technique to solve conflicts in the facts provided by numerous data sources. The most significant challenge for this task is to estimate source reliability and select the answers supported by high quality sources. However, existing works assume that one data source has the same reliability on any kinds of entity, ignoring the possibility that a source may vary in reliability on different domains. To capture the influence of various levels of expertise in different domains, we integrate domain expertise knowledge to achieve a more precise estimation of source reliability. We propose to infer the domain expertise of a data source based on its data richness in different domains. We also study the mutual influence between domains, which will affect the inference of domain expertise. Through leveraging the unique features of the multi-truth problem that sources may provide partially correct values of a data item, we assign more reasonable confidence scores to value sets. We propose an integrated Bayesian approach to incorporate the domain expertise of data sources and confidence scores of value sets, aiming to find multiple possible truths without any supervision. Experimental results on two real-world datasets demonstrate the feasibility, efficiency and effectiveness of our approach.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {635–647},
numpages = {13}
}

@article{10.1145/3187009.3177739,
author = {Lin, Xueling and Chen, Lei},
title = {Domain-Aware Multi-Truth Discovery from Conflicting Sources},
year = {2018},
issue_date = {January 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.1145/3187009.3177739},
doi = {10.1145/3187009.3177739},
abstract = {In the Big Data era, truth discovery has served as a promising technique to solve conflicts in the facts provided by numerous data sources. The most significant challenge for this task is to estimate source reliability and select the answers supported by high quality sources. However, existing works assume that one data source has the same reliability on any kinds of entity, ignoring the possibility that a source may vary in reliability on different domains. To capture the influence of various levels of expertise in different domains, we integrate domain expertise knowledge to achieve a more precise estimation of source reliability. We propose to infer the domain expertise of a data source based on its data richness in different domains. We also study the mutual influence between domains, which will affect the inference of domain expertise. Through leveraging the unique features of the multi-truth problem that sources may provide partially correct values of a data item, we assign more reasonable confidence scores to value sets. We propose an integrated Bayesian approach to incorporate the domain expertise of data sources and confidence scores of value sets, aiming to find multiple possible truths without any supervision. Experimental results on two real-world datasets demonstrate the feasibility, efficiency and effectiveness of our approach.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {635–647},
numpages = {13}
}

@inproceedings{10.1145/3129416.3129441,
author = {Cohen, L.},
title = {Impacts of Business Intelligence on Population Health: A Systematic Literature Review},
year = {2017},
isbn = {9781450352505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129416.3129441},
doi = {10.1145/3129416.3129441},
abstract = {"Business Intelligence" is an area of Information Technology (IT) that involves the collection, analysis and presentation of large amounts of data. BI has been successfully applied to promote good decision making in a variety of environments, and has high potential to make a significant impact in the domain of population health. The promotion of population health is a key concern of government authorities and various health institutions and officials making decisions about interventions that may impact on population health would benefit from the use of information on population health. BI could clearly be a facilitator in this regard, but evidence of its current application and impact in this field is not easily accessible to policy makers. This systematic literature review explored the literature and provided a synthesis of information available on the current use of BI in this area, and evidence of the impact of its use on population health. An array of applications of BI for population health were found, including data warehouses, analytics, reports, data warehouse browsers, OLAP, GIS, Dashboards and Alerts. Evidence of the impact of these applications on population health was mainly anecdotal, with only one empirical study found. Issues and challenges encountered in the development and use of BI are Privacy and Security, Data Quality and Development and Maintenance of BI infrastructure},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists},
articleno = {9},
numpages = {9},
keywords = {business intelligence, population health, systematic literature review},
location = {Thaba 'Nchu, South Africa},
series = {SAICSIT '17}
}

@inproceedings{10.1145/3514221.3522567,
author = {Nargesian, Fatemeh and Asudeh, Abolfazl and Jagadish, H. V.},
title = {Responsible Data Integration: Next-Generation Challenges},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3522567},
doi = {10.1145/3514221.3522567},
abstract = {Data integration has been extensively studied by the data management community and is a core task in the data pre-processing step of ML pipelines. When the integrated data is used for analysis and model training, responsible data science requires addressing concerns about data quality and bias. We present a tutorial on data integration and responsibility, highlighting the existing efforts in responsible data integration along with research opportunities and challenges. In this tutorial, we encourage the community to audit data integration tasks with responsibility measures and develop integration techniques that optimize the requirements of responsible data science. We focus on three critical aspects: (1) the requirements to be considered for evaluating and auditing data integration tasks for quality and bias; (2) the data integration tasks that elicit attention to data responsibility measures and methods to satisfy these requirements; and, (3) techniques, tasks, and open problems in data integration that help achieve data responsibility.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {2458–2464},
numpages = {7},
keywords = {responsible ai, data integration, distribution tailoring, data equity, fair ML, data collection},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3284179.3284322,
author = {Dorn, Amelie and Wandl-Vogt, Eveline and Palfinger, Thomas and D\'{\i}az, Jos\'{e} Luis Preza and Piringer, Barbara and Schatek, Alexander and Zoubek, Rainer},
title = {Applying Commercial Computer Vision Tools to Cope with Uncertainties in a Citizen-Driven Archive: The Case Study Topothek@exploreAT!},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284322},
doi = {10.1145/3284179.3284322},
abstract = {Uncertainties in data, e.g., incomplete data sets, data quality issues or inconsistencies in annotations, are a common phenomenon across disciplines. How to address these issues is context dependent. In this paper, we address uncertainties in the citizen-driven archive Topotheque as a concrete use-case in the Digital Humanities project exploreAT!, and demonstrate, how to deal with uncertainties by benchmarking a set of selected commercial computer vision (CV) tools. The approach aims to enrich Topotheque's data to enable better access, connectivity and analysis for both researchers and citizens. Results show that by applying CV, existing uncertainties are noticeably reduced, but new ones also introduced. Better grounds for semantic structuring are provided, enabling higher connectivity and linking within Topotheque, but also across other data sets. Ultimately, the enrichment of the archive is for the benefit of both researchers and citizens enabled by addressing and tackling apparent uncertainties.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {845–851},
numpages = {7},
keywords = {uncertainty, computer vision, Digital Humanities, citizen-driven archive, AI},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@article{10.5555/3344081.3344082,
author = {Ye, Yumeng and Talburt, John R.},
title = {Generating Synthetic Data to Support Entity Resolution Education and Research},
year = {2019},
issue_date = {April 2019},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {34},
number = {7},
issn = {1937-4771},
abstract = {Almost all organizations use some type of Entity Resolution (ER) methods to uniquely identify their customers and vendors across different channels of contact. In the case of persons, this requires the use of personally identifying information (PII) such as name, address, phone number, and email address. Because of the growing concerns over data privacy and identity theft, organizations are reluctant to release personally-identifiable customer information even for education and research purposes. An alternative is to generate synthetic data to use in student exercises and for research related to entity resolution methods and techniques. One advantage of synthetically generated data for ER is it can be fully annotated with the correct linking making it very easy to calculate the precision and recall of linking operations. This paper discusses a simple method to generate synthetic data as input for ER processes. The method allows the user to randomly assign certain types and levels of data quality errors along with other types of non-error variations to the data, such as nicknames, different date formats, and changes in address. For ER research in particular, the method can create introduce data redundancy by copying records referencing the same person into the same file or into different files with different record layouts.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {12–19},
numpages = {8}
}

@inproceedings{10.1145/3491396.3506519,
author = {Li, Xiang and Zhang, Zhaoqian and Zhao, Zhigang and Wu, Lu and Huo, Jidong and Zhang, Jian and Wang, Yinglong},
title = {ECNN: One Online Deep Learning Model for Streaming Ocean Data Prediction},
year = {2022},
isbn = {9781450391603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491396.3506519},
doi = {10.1145/3491396.3506519},
abstract = {Despite been extensively explored, current techniques in sequential data modeling and prediction are generally designed for solving regression tasks in a batch learning setting, making them not only computationally inefficient but also poorly scalable in real-world applications, especially for real-time intelligent ocean data quality control (QC), where the data arrives sequentially and the QC should be conducted in real time. This paper investigates the online learning for ocean data streams by resolving two main challenges: (i) how to develop a deep learning model to capture the complex ocean data distribution that could evolve dynamically, namely tackling the 'concept drift' problem for non-stationary time series; (ii) how to develop a deep learning model that can dynamically adapt its structure from shallow to deep with the inflow of the data to overcome under-fitting problem, namely tackling the 'model selection' problem. To tackle these challenges, we propose one Evolutive Convolutional Neural Network (ECNN) that dynamically re-weighting the sub-structure of the model from data streams in a sequential or online learning fashion, by which the capacity scalability and sustainability are introduced into the model. The experiments on real ocean observation data verify the effectiveness of our model. As far as we know, it is the first work that introduce online deep learning techniques into ocean data prediction research.},
booktitle = {Proceedings of the 2021 ACM International Conference on Intelligent Computing and Its Emerging Applications},
pages = {170–175},
numpages = {6},
keywords = {Time Series Prediction, Ocean Data, CNN, Online Learning, Attention Network},
location = {Jinan, China},
series = {ACM ICEA '21}
}

@inproceedings{10.1145/3438872.3439085,
author = {Wu, Yi and Song, Yan and Yang, Hongshan},
title = {Intelligent Distributed Web Crawler Based on Attention Mechanism},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439085},
doi = {10.1145/3438872.3439085},
abstract = {With the rapid development of the Internet, webpages' content has become the central platform for people to publish and retrieve information. Recently, web crawlers could quickly and accurately find the information users need from the massive network information resources. There have been many different types of web crawlers in the literature, developed for data retrieval. However, most of the existing web crawlers have significant limitations. For example, they focus on the effective overall architecture instead of paying attention to the actual data's complexity. Moreover, the advertising links in the news and the public platform's promotional content have become ubiquitous noise. The existing web crawler collection strategy lacks sufficient identification of advertising information. The degree of automation to detect advertisements is low, so it isn't easy to form a complete and deployable large-scale distributed data crawling system. Therefore, the research and improvement of distributed web crawlers that intelligently distinguish advertisements is a work of practical significance. The distributed intelligent web crawler system designed and implemented in this paper solves low manual crawler efficiency and poor data quality. The crawler system can effectively identify and eliminate advertising information and significantly improve the automatically extracted data in the distributed crawler system from the experimental results.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {229–233},
numpages = {5},
keywords = {Artificial Intelligence, Deep Learning, Intelligent Web Crawler, Distributed Framework},
location = {Shanghai, China},
series = {RICAI 2020}
}

@inproceedings{10.1145/3522664.3528593,
author = {Altendeitering, Marcel and Pampus, Julia and Larrinaga, Felix and Legaristi, Jon and Howar, Falk},
title = {Data Sovereignty for AI Pipelines: Lessons Learned from an Industrial Project at Mondragon Corporation},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528593},
doi = {10.1145/3522664.3528593},
abstract = {The establishment of collaborative AI pipelines, in which multiple organizations share their data and models, is often complicated by lengthy data governance processes and legal clarifications. Data sovereignty solutions, which ensure data is being used under agreed terms and conditions, are promising to overcome these problems. However, there is limited research on their applicability in AI pipelines. In this study, we extended an existing AI pipeline at Mondragon Corporation, in which sensor data is collected and subsequently forwarded to a data quality service provider with a data sovereignty component. By systematically reflecting and generalizing our experiences during the twelve-month action research project, we formulated ten lessons learned, four benefits, and three barriers to data-sovereign AI pipelines that can inform further research and custom implementations. Our results show that a data sovereignty component can help reduce existing barriers and increase the success of collaborative data science initiatives.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {193–204},
numpages = {12},
keywords = {AI engineering, lessons learned, data sovereignty, collaborative AI},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@book{10.1145/3310205,
author = {Ilyas, Ihab F. and Chu, Xu},
title = {Data Cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.}
}

@inproceedings{10.1145/3292500.3332297,
author = {Shi, Xiaolin and Dmitriev, Pavel and Gupta, Somit and Fu, Xin},
title = {Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3332297},
doi = {10.1145/3292500.3332297},
abstract = {A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the "why" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [18, 22]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3189–3190},
numpages = {2},
keywords = {a/b testing, user experience evaluation, online metrics, controlled experiments},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1145/3326164,
author = {Cong, Phan Thanh and Tam, Nguyen Thanh and Yin, Hongzhi and Zheng, Bolong and Stantic, Bela and Hung, Nguyen Quoc Viet},
title = {Efficient User Guidance for Validating Participatory Sensing Data},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3326164},
doi = {10.1145/3326164},
abstract = {Participatory sensing has become a new data collection paradigm that leverages the wisdom of the crowd for big data applications without spending cost to buy dedicated sensors. It collects data from human sensors by using their own devices such as cell phone accelerometers, cameras, and GPS devices. This benefit comes with a drawback: human sensors are arbitrary and inherently uncertain due to the lack of quality guarantee. Moreover, participatory sensing data are time series that exhibit not only highly irregular dependencies on time but also high variance between sensors. To overcome these limitations, we formulate the problem of validating uncertain time series collected by participatory sensors. In this article, we approach the problem by an iterative validation process on top of a probabilistic time series model. First, we generate a series of probability distributions from raw data by tailoring a state-of-the-art dynamical model, namely <u>G</u>eneralised <u>A</u>uto <u>R</u>egressive <u>C</u>onditional <u>H</u>eteroskedasticity (GARCH), for our joint time series setting. Second, we design a feedback process that consists of an adaptive aggregation model to unify the joint probabilistic time series and an efficient user guidance model to validate aggregated data with minimal effort. Through extensive experimentation, we demonstrate the efficiency and effectiveness of our approach on both real data and synthetic data. Highlights from our experiences include the fast running time of a probabilistic model, the robustness of an aggregation model to outliers, and the significant effort saving of a guidance model.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {jul},
articleno = {37},
numpages = {30},
keywords = {trust management, Participatory sensing, probabilistic database}
}

@article{10.1145/3530991,
author = {Killeen, Patrick and Kiringa, Iluju and Yeap, Tet},
title = {Unsupervised Dynamic Sensor Selection for IoT-Based Predictive Maintenance of a Fleet of Public Transport Buses},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2691-1914},
url = {https://doi.org/10.1145/3530991},
doi = {10.1145/3530991},
abstract = {In recent years, big data produced by the Internet of Things has enabled new kinds of useful applications. One such application is monitoring a fleet of vehicles in real time to predict their remaining useful life. The consensus self-organized models (COSMO) approach is an example of a predictive maintenance system. The present work proposes a novel Internet of Things based architecture for predictive maintenance that consists of three primary nodes: the vehicle node, the server leader node, and the root node, which enable on-board vehicle data processing, heavy-duty data processing, and fleet administration, respectively. A minimally viable prototype of the proposed architecture was implemented and deployed to a local bus garage in Gatineau, Canada.The present work proposes improved consensus self-organized models (ICOSMO), a fleet-wide unsupervised dynamic sensor selection algorithm. To analyze the performance of ICOSMO, a fleet simulation was implemented. The J1939 data gathered from a hybrid bus was used to generate synthetic data in the simulations. Simulation results that compared the performance of the COSMO and ICOSMO approaches revealed that in general ICOSMO improves the average area under the curve of COSMO by approximately 1.5% when using the Cosine distance and 0.6% when using Hellinger distance.},
journal = {ACM Trans. Internet Things},
month = {jul},
articleno = {21},
numpages = {36},
keywords = {predictive analytics, J1939, predictive maintenance, Controller Area Network, Internet of Things, fleet management, machine learning, sensor selection}
}

@inproceedings{10.1145/3447548.3469441,
author = {Zhu, Feida and Pei, Jian},
title = {The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD2021): Joint Workshop with SIGKDD 2021 Trust Day},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3469441},
doi = {10.1145/3447548.3469441},
abstract = {Today's computing is characterized by an increasing degree of complexity, comprehensiveness and collaboration. The complexity can be observed by the wide application of gigantic models with a huge number of parameters and structures of an unprecedented level of sophistication. The comprehensiveness is best illustrated by the high heterogeneity of data both in terms of format and source. The collaboration, finally, becomes an obvious trend when computing systems grow more open and decentralized in which various entities interact to achieve collective intelligence with the presence of potentially malicious behavior. Trust, therefore, has become critical at multiple levels: At model level to assure its integrity, fairness and interpretability; At data level to safeguard data quality, compliance and privacy; At system level to govern resilience, performance and incentive. Moreover, the notion of trust has long been discussed in different domains in both academia and industry with different definition and understanding. The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD'21) will be held as a joint workshop with the special-themed "Trust Day" of KDD 2021, which has therefore aimed to bring together researchers, practitioners and experts from various communities to exchange and explore ideas, frontiers, opportunities and challenges under the broad theme of "trust" in a highly interdisciplinary manner.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {4185–4186},
numpages = {2},
keywords = {data governance, data pricing, data auditing, data asset, privacy, distributed ledger technology},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3307339.3342169,
author = {Perkins, Patrick and Heber, Steffen},
title = {Using a Novel Negative Selection Inspired Anomaly Detection Algorithm to Identify Corrupted Ribo-Seq and RNA-Seq Samples},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3342169},
doi = {10.1145/3307339.3342169},
abstract = {RNA-seq and Ribo-seq are popular techniques for quantifying cellular transcription and translation. These experiments use next-generation sequencing to produce genome-wide high-resolution snapshots of the total populations of mRNAs and translating ribosomes within the investigated samples. When performed in concert, these experiments yield valuable information about protein synthesis rates and translational efficiency. Due to their intricate experimental protocols and demanding data processing requirements, quality control and analysis of such experiments are often challenging. Therefore, methods for accurately assessing data quality, and for identifying contaminated samples, are greatly needed. In the following we use a novel negative selection inspired algorithm called Boundary Detection Using Nearest Neighbors (BDUNN), for the identification of corrupted samples. Our algorithm constructs a detector set and reduced training set that defines the boundaries between normal data points and potential anomalies. Subsequently, a nearest neighbor algorithm is used to classify unseen observations. We compare the performance of BDUNN with other popular negative selection and one-class classification algorithms, and show that BDUNN is capable of accurately and efficiently detecting anomalies in standard anomaly detection datasets and simulated RNA-seq and Ribo-seq data sets. Furthermore, we have implemented our method within an existing R Shiny platform for analyzing RNA-seq an Ribo-seq datasets, which permits downstream analysis of anomalous samples.},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {457–465},
numpages = {9},
keywords = {negative selection algorithm, machine learning, sample quality, anomaly detection, ribosome profiling, rna-seq},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1145/2846012.2846026,
author = {Lipuntsov, Yuri P.},
title = {On the Relationship Between the Information and Analytical Components in the Shared E-Government},
year = {2015},
isbn = {9781450340700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2846012.2846026},
doi = {10.1145/2846012.2846026},
abstract = {Economic and mathematical models and information models are the two main components of the information environment. These two components perform different functions - the information models is responsible for the data quality, the data delivery, and the economic and mathematical models defines data mining and intelligence. This category of models is constantly being developed often independently of each other. The information models as methods of data presentation and data integration are considered as separate from economic and mathematical modeling area. This paper discuss the relationship between the two types of models as sequence of steps for models development with the horizontal and vertical traceability. The connection between two types of models presented as the reflection of the real word logic to the data layer and after that to the software layer, and the feedback from the application to the information and to the operation logic.},
booktitle = {Proceedings of the 2015 2nd International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {109–115},
numpages = {7},
keywords = {Shared environment, Data exchange, Economic and mathematical modeling, Simulation, Information modeling},
location = {St. Petersburg, Russian Federation},
series = {EGOSE '15}
}

@inproceedings{10.1145/3491101.3503724,
author = {Pine, Kathleen and Bossen, Claus and Holten M\o{}ller, Naja and Miceli, Milagros and Lu, Alex Jiahong and Chen, Yunan and Horgan, Leah and Su, Zhaoyuan and Neff, Gina and Mazmanian, Melissa},
title = {Investigating Data Work Across Domains: New Perspectives on the Work of Creating Data},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3503724},
doi = {10.1145/3491101.3503724},
abstract = {In the wake of the hype around big data, artificial intelligence, and “data-drivenness,” much attention has been paid to developing novel tools to capitalize upon the deluge of data being recorded and gathered automatically through IT systems. While much of this literature tends to overlook the data itself—sometimes even characterizing it as “data exhaust” that is readily available to be fed into algorithms, which will unlock the insights held within it—a growing body of literature has recently been directed at the (often intensive and skillful) work that goes into creating, collecting, managing, curating, analyzing, interpreting, and communicating data. These investigations detail the practices and processes involved in making data useful and meaningful so that aims of becoming ‘data-driven’ or ‘data-informed’ can become real. Further, In some cases, increased demands for data work have led to the formation of new occupations, whereas at other times data work has been added to the task portfolios of existing occupations and professions, occasionally affecting their core identity. Thus, the evolving forms of data work are requiring individual and organizational resources, new and re-tooled practices and tools, development of new competences and skills, and creation of new functions and roles. While differences exist across the global North and the global South experience of data work, such factors of data production remain paramount even as they exist largely for the benefit of the data-driven system [21, 32]. This one-day workshop will investigate existing and emerging tasks of data work. Further, participants will seek to understand data work as it impacts: individual data workers; occupations tasked with data work (existing and emerging); organizations (e.g. changing their skill-mix and infrastructuring to support data work); and teaching institutions (grappling with incorporation of data work into educational programs). Participants are required to submit a position paper or a case study drawn from their research to be reviewed and accepted by the organizing committee (submissions should be up to four pages in length). Upon acceptance, participants will read each other's paper, prepare to shortly present and respond to comments by two discussants and other participants. Subsequently, the workshop will focus on developing a set of core processes and tasks as well as an outline of a research agenda for a CHI-perspective on data work in the coming years.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {87},
numpages = {6},
keywords = {Datafication, Data-Driven, Data Work, Labor, Occupations},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/2944165.2944172,
author = {El-Atawy, Sameh S. and Khalefa, Mohamed E.},
title = {Building an Ontology-Based Electronic Health Record System},
year = {2016},
isbn = {9781450342933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2944165.2944172},
doi = {10.1145/2944165.2944172},
abstract = {Electronic health record (EHR) solutions are complex, spanning multiple specialties and domains of expertise. These systems need to handle clinical concepts, temporal data, documents, and financial transactions, which leads to a large code base that is tightly coupled with data models and inherently hard to maintain. These difficulties can greatly increase the cost of developing EHR systems, result in a high failure rate of implementation, and threaten investments in this sector. Moreover, due to the wide variance in the level of detail across different settings, data exchange is becoming a serious problem, further increasing the cost of development and maintenance.To overcome these issues, we adopt ontologies to model our proposed EHR solution, not only allowing code reuse; but also enabling later extension and customization. Adopting software factory techniques, we build tools to transform ontological models into deployment-ready code. This automatically provides handling of data persistence, access, and exchange. Business logic is expressed as ontology-based process flows and rules, ensuring data quality and supporting special needs. This logic is enforced transparently and can be modified on the fly. We optimized the user experience by facilitating fast data entry and retrieval.In this paper, we present the requirements of an effective EHR solution, explain the techniques we employed, describe the main modules of our proposed system, and discuss the technical decisions we made.},
booktitle = {Proceedings of the 2nd Africa and Middle East Conference on Software Engineering},
pages = {40–45},
numpages = {6},
keywords = {Ontology, Electronic Health Record Management, Query Language},
location = {Cairo, Egypt},
series = {AMECSE '16}
}

@inproceedings{10.1145/3219819.3219914,
author = {Samel, Karan and Miao, Xu},
title = {Active Deep Learning to Tune Down the Noise in Labels},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219914},
doi = {10.1145/3219819.3219914},
abstract = {The great success of supervised learning has initiated a paradigm shift from building a deterministic software system to a probabilistic artificial intelligent system throughout the industry. The historical records in enterprise domains can potentially bootstrap the traditional business into the modern data-driven approach almost everywhere. The introduction of the Deep Neural Networks (DNNs) significantly reduces the efforts of feature engineering so that supervised learning becomes even more automated. The last bottleneck is to ensure the data quality, particularly the label quality, because the performance of supervised learning is bounded by the errors present in labels. In this paper, we present a new Active Deep Denoising (ADD) approach that first builds a DNN noise model, and then adopts an active learning algorithm to identify the optimal denoising function. We prove that under the low noise condition, we only need to query the oracle with log n examples where n is the total number in the data. We apply ADD on one enterprise application and show that it can effectively reduce 1/3 of the prediction error with only 0.1% of examples verified by the oracle.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {685–694},
numpages = {10},
keywords = {denoising, active learning, classification, deep neural networks},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{10.1145/3460000,
author = {Thirumuruganathan, Saravanan and Kunjir, Mayuresh and Ouzzani, Mourad and Chawla, Sanjay},
title = {Automated Annotations for AI Data and Model Transparency},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3460000},
doi = {10.1145/3460000},
abstract = {The data and Artificial Intelligence revolution has had a massive impact on enterprises, governments, and society alike. It is fueled by two key factors. First, data have become increasingly abundant and are often available openly. Enterprises have more data than they can process. Governments are spearheading open data initiatives by setting up data portals such as data.gov and releasing large amounts of data to the public. Second, AI engineering development is becoming increasingly democratized. Open source frameworks have enabled even an individual developer to engineer sophisticated AI systems. But with such ease of use comes the potential for irresponsible use of data.Ensuring that AI systems adhere to a set of ethical principles is one of the major problems of our age. We believe that data and model transparency has a key role to play in mitigating the deleterious effects of AI systems. In this article, we describe a framework to synthesize ideas from various domains such as data transparency, data quality, data governance among others to tackle this problem. Specifically, we advocate an approach based on automated annotations (of both data and the AI model), which has a number of appealing properties. The annotations could be used by enterprises to get visibility of potential issues, prepare data transparency reports, create and ensure policy compliance, and evaluate the readiness of data for diverse downstream AI applications. We propose a model architecture and enumerate its key components that could achieve these requirements. Finally, we describe a number of interesting challenges and opportunities.},
journal = {J. Data and Information Quality},
month = {dec},
articleno = {2},
numpages = {9},
keywords = {data cleaning, Data transparency, machine learning}
}

@inproceedings{10.1145/3487553.3524718,
author = {Yu, Shuo and Huang, Huafei and Dao, Minh N. and Xia, Feng},
title = {Graph Augmentation Learning},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524718},
doi = {10.1145/3487553.3524718},
abstract = {Graph Augmentation Learning (GAL) provides outstanding solutions for graph learning in handling incomplete data, noise data, etc. Numerous GAL methods have been proposed for graph-based applications such as social network analysis and traffic flow forecasting. However, the underlying reasons for the effectiveness of these GAL methods are still unclear. As a consequence, how to choose optimal graph augmentation strategy for a certain application scenario is still in black box. There is a lack of systematic, comprehensive, and experimentally validated guideline of GAL for scholars. Therefore, in this survey, we in-depth review GAL techniques from macro (graph), meso (subgraph), and micro (node/edge) levels. We further detailedly illustrate how GAL enhance the data quality and the model performance. The aggregation mechanism of augmentation strategies and graph learning models are also discussed by different application scenarios, i.e., data-specific, model-specific, and hybrid scenarios. To better show the outperformance of GAL, we experimentally validate the effectiveness and adaptability of different GAL strategies in different downstream tasks. Finally, we share our insights on several open issues of GAL, including heterogeneity, spatio-temporal dynamics, scalability, and generalization.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1063–1072},
numpages = {10},
keywords = {Graph augmentation learning, Graph neural networks, Graph representation learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.14778/2809974.2809975,
author = {K\"{o}hler, Henning and Link, Sebastian and Zhou, Xiaofang},
title = {Possible and Certain SQL Keys},
year = {2018},
issue_date = {July 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2809974.2809975},
doi = {10.14778/2809974.2809975},
abstract = {Driven by the dominance of the relational model, the requirements of modern applications, and the veracity of data, we revisit the fundamental notion of a key in relational databases with NULLs. In SQL database systems primary key columns are NOT NULL by default. NULL columns may occur in unique constraints which only guarantee uniqueness for tuples which do not feature null markers in any of the columns involved, and therefore serve a different function than primary keys. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that can originate from an SQL table, respectively. Possible keys coincide with the unique constraint of SQL, and thus provide a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns, and thus form a sufficient and necessary condition to identify tuples uniquely, while primary keys are only sufficient for that purpose. In addition to basic characterization, axiomatization, and simple discovery approaches for possible and certain keys, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs do occur in real-world databases, and that related computational problems can be solved efficiently. Certain keys are therefore semantically well-founded and able to maintain data quality in the form of Codd's entity integrity rule while handling the requirements of modern applications, that is, higher volumes of incomplete data from different formats.},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {1118–1129},
numpages = {12}
}

@inproceedings{10.1145/3546930.3547494,
author = {Lou, Yuze and Cafarella, Michael},
title = {Enabling Useful Provenance in Scripting Languages with a Human-in-the-Loop},
year = {2022},
isbn = {9781450394420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546930.3547494},
doi = {10.1145/3546930.3547494},
abstract = {Most data scientists must build substantial data pipelines using scripting languages like Python and R. These pipelines are hard to get correct due to the large volume of data they process (thus the long execution time), and the fact that they are tested mainly by inspection of output data quality. It is therefore crucial for developers to reason about data through each step in the pipeline, starting from the raw input; this information is akin to data provenance in a relational setting. Past efforts for capturing data provenance for scripting languages have required substantial manual modifications to the scripts, or else yield information that is too inflexible for many debugging tasks.We instead propose a "human-in-the-loop" provenance generation model with three key improvements: (1) allowing humans to express the desired provenance through a provenance schema, (2) enabling one-time execution capture of scripts to produce traces that are later combined with different provenance schemata to yield useful provenance for different tasks, (3) providing a modular rule-based recommendation component to help design provenance schemata through a user interaction interface. We describe the concepts, the user experience with our system, explain the system components, and present preliminary experiment results.},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {5},
numpages = {7},
location = {Philadelphia, Pennsylvania},
series = {HILDA '22}
}

@article{10.5555/2809974.2809975,
author = {K\"{o}hler, Henning and Link, Sebastian and Zhou, Xiaofang},
title = {Possible and Certain SQL Keys},
year = {2015},
issue_date = {July 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {11},
issn = {2150-8097},
abstract = {Driven by the dominance of the relational model, the requirements of modern applications, and the veracity of data, we revisit the fundamental notion of a key in relational databases with NULLs. In SQL database systems primary key columns are NOT NULL by default. NULL columns may occur in unique constraints which only guarantee uniqueness for tuples which do not feature null markers in any of the columns involved, and therefore serve a different function than primary keys. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that can originate from an SQL table, respectively. Possible keys coincide with the unique constraint of SQL, and thus provide a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns, and thus form a sufficient and necessary condition to identify tuples uniquely, while primary keys are only sufficient for that purpose. In addition to basic characterization, axiomatization, and simple discovery approaches for possible and certain keys, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs do occur in real-world databases, and that related computational problems can be solved efficiently. Certain keys are therefore semantically well-founded and able to maintain data quality in the form of Codd's entity integrity rule while handling the requirements of modern applications, that is, higher volumes of incomplete data from different formats.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1118–1129},
numpages = {12}
}

@article{10.14778/3297753.3297757,
author = {Li, Yanying and Sun, Haipei and Dong, Boxiang and Wang, Hui (Wendy)},
title = {Cost-Efficient Data Acquisition on Online Data Marketplaces for Correlation Analysis},
year = {2018},
issue_date = {December 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3297753.3297757},
doi = {10.14778/3297753.3297757},
abstract = {Incentivized by the enormous economic profits, the data marketplace platform has been proliferated recently. In this paper, we consider the data marketplace setting where a data shopper would like to buy data instances from the data marketplace for correlation analysis of certain attributes. We assume that the data in the marketplace is dirty and not free. The goal is to find the data instances from a large number of datasets in the marketplace whose join result not only is of high-quality and rich join informativeness, but also delivers the best correlation between the requested attributes. To achieve this goal, we design DANCE, a middleware that provides the desired data acquisition service. DANCE consists of two phases: (1) In the off-line phase, it constructs a two-layer join graph from samples. The join graph includes the information of the datasets in the marketplace at both schema and instance levels; (2) In the online phase, it searches for the data instances that satisfy the constraints of data quality, budget, and join informativeness, while maximizing the correlation of source and target attribute sets. We prove that the complexity of the search problem is NP-hard, and design a heuristic algorithm based on Markov chain Monte Carlo (MCMC). Experiment results on two benchmark and one real datasets demonstrate the efficiency and effectiveness of our heuristic data acquisition algorithm.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {362–375},
numpages = {14}
}

@inproceedings{10.1145/3414274.3414505,
author = {Fu, Qingwen and Zhu, Jiahui and Chen, Yuepeng and Wan, Jintao and He, Bin},
title = {An Automatic Learning Model for Trajectory Outlier Detection},
year = {2020},
isbn = {9781450376044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414274.3414505},
doi = {10.1145/3414274.3414505},
abstract = {The rapid development of global positioning system has given birth to a large number of spatial-temporal data, and there are many outliers of points obviously in these trajectory data. It is very important to detect the outliers in the trajectory to improve the data quality and accuracy of trajectory mining. In this paper, we propose a trajectory outlier detection algorithm based on bi-directional long short-term memory model and attention mechanism. Firstly, an eight-dim eigenvector is extracted from each point of trajectory, and then a two-layer bi-directional long short-term memory model is constructed. Finally, representing the trajectory points in an interactive way which is called attention mechanism. The input of the model is the trajectory point with a certain length, and the output is the type of the trajectory point. The model can automatically learn the difference between the normal point and the adjacent abnormal point with motion features. Experimental dataset based on real trajectory data of taxi from Beijing, and results showed that the performance of this algorithm is significantly better than constant speed threshold method or classical machine learning classification. Especially the precision and recall reaches 0.93 and 0.90 separately, which proves the effectiveness of this algorithm.},
booktitle = {Proceedings of the 3rd International Conference on Data Science and Information Technology},
pages = {220–226},
numpages = {7},
keywords = {Bi-LSTM, outlier detection, attention mechanism, spatial-temporal data},
location = {Xiamen, China},
series = {DSIT 2020}
}

@inproceedings{10.1145/3184558.3192324,
author = {Spaniol, Marc and Baeza-Yates, Ricardo and Masan\`{e}s, Julien},
title = {TempWeb 2018 Chairs' Welcome and Organization},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3192324},
doi = {10.1145/3184558.3192324},
abstract = {Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1729–1730},
numpages = {2},
keywords = {data aggregation, systematic exploitation of web archives, community detection and evolution, content evolution on the web, web scale data analytics, large scale data storage, web science, topic mining, distributed data analytics, time aware web archiving, web dynamics, large scale data processing, data quality metrics, web spam evolution, web trends, temporal web analytics, terminology evolution},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.1109/TNET.2021.3105427,
author = {Shi, Zhiguo and Yang, Guang and Gong, Xiaowen and He, Shibo and Chen, Jiming},
title = {Quality-Aware Incentive Mechanisms Under Social Influences in Data Crowdsourcing},
year = {2022},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3105427},
doi = {10.1109/TNET.2021.3105427},
abstract = {Incentive mechanism design and quality control are two key challenges in data crowdsourcing, because of the need for recruitment of crowd users and their limited capabilities. Without considering users’ social influences, existing mechanisms often result in low efficiency in terms of the platform’s cost. In this paper, we exploit social influences among users as incentives to motivate users’ participation, in order to reduce the cost of recruiting users. Based on social influences, we design incentive mechanisms with the goal of achieving high quality of crowdsourced data and low cost of incentivizing users’ participation. Specifically, we consider three scenarios. In the full information scenario, we design task assignment and user recruitment mechanisms to optimize the data quality while reducing the incentive cost. In the partial information scenario, users’ qualities and costs are unknown. We exploit the correlation between tasks to overcome the information asymmetry, for both cases of opportunistic crowdsourcing and participatory crowdsourcing. Further, in the dynamic social influence scenario, we investigate the dynamics of users’ social influences and design extra rewards for users to make full use of the social influence and achieve maximum cost saving. We evaluate the incentive mechanisms using numerical results, which demonstrate their effectiveness.},
journal = {IEEE/ACM Trans. Netw.},
month = {feb},
pages = {176–189},
numpages = {14}
}

@article{10.1145/3501295,
author = {Zafar, Farkhanda and Khattak, Hasan Ali and Aloqaily, Moayad and Hussain, Rasheed},
title = {Carpooling in Connected and Autonomous Vehicles: Current Solutions and Future Directions},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3501295},
doi = {10.1145/3501295},
abstract = {Owing to the advancements in communication and computation technologies, the dream of commercialized connected and autonomous cars is becoming a reality. However, among other challenges such as environmental pollution, cost, maintenance, security, and privacy, the ownership of vehicles (especially for Autonomous Vehicles) is the major obstacle in the realization of this technology at the commercial level. Furthermore, the business model of pay-as-you-go type services further attracts the consumer, because there is no need for upfront investment. In this vein, the idea of car-sharing (aka carpooling) is getting ground due to, at least in part, its simplicity, cost-effectiveness, and affordable choice of transportation. Carpooling systems are still in their infancy and face challenges such as scheduling, matching passengers interests, business model, security, privacy, and communication. To date, a plethora of research work has already been done covering different aspects of carpooling services (ranging from applications to communication and technologies); however, there is still a lack of a holistic, comprehensive survey that can be a one-stop-shop for the researchers in this area to (i) find all the relevant information and (ii) identify the future research directions. To fill these research challenges, this article provides a comprehensive survey on carpooling in autonomous and connected vehicles and covers architecture, components, and solutions, including scheduling, matching, mobility, pricing models of carpooling. We also discuss the current challenges in carpooling and identify future research directions. This survey is aimed to spur further discussion among the research community for the effective realization of carpooling.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {218},
numpages = {36},
keywords = {ride-sharing, vehicular networks, intelligent transportation systems, Connected autonomous vehicles, carpooling}
}

@inproceedings{10.1145/3383783.3383793,
author = {Dvo\v{r}\'{a}kov\'{a}, Eli\v{s}ka and Kumar, Sajal and Kl\'{e}ma, Ji\v{r}\'{\i} and \v{Z}elezn\'{y}, Filip and Drbal, Karel and Song, Mingzhou},
title = {Evaluating Model-Free Directional Dependency Methods on Single-Cell RNA Sequencing Data with Severe Dropout},
year = {2020},
isbn = {9781450372183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383783.3383793},
doi = {10.1145/3383783.3383793},
abstract = {As severe dropout in single-cell RNA sequencing (scRNA-seq) degrades data quality, current methods for network inference face increased uncertainty from such data. To examine how dropout influences directional dependency inference from scRNA-seq data, we thus studied four methods based on discrete data that are model-free without parametric model assumptions. They include two established methods: conditional entropy and Kruskal-Wallis test, and two recent methods: causal inference by stochastic complexity and function index. We also included three non-directional methods for a contrast. On simulated data, function index performed most favorably at varying dropout rates, sample sizes, and discrete levels. On an scRNA-seq dataset from developing mouse cerebella, function index and Kruskal-Wallis test performed favorably over other methods in detecting expression of developmental genes as a function of time. Overall among the four methods, function index is most resistant to dropout for both directional and dependency inference. The next best choice, Kruskal-Wallis test, carries a directional bias towards a uniformly distributed variable. We conclude that a method robust to marginal distributions with a sufficiently large sample size can reap benefits of single-cell over bulk RNA sequencing in understanding molecular mechanisms at the cellular resolution.},
booktitle = {Proceedings of the 2019 6th International Conference on Bioinformatics Research and Applications},
pages = {55–62},
numpages = {8},
keywords = {single-cell sequencing, directional dependency, model-free},
location = {Seoul, Republic of Korea},
series = {ICBRA '19}
}

@inproceedings{10.1145/3137133.3137140,
author = {Jia, Ruoxi and Sangogboye, Fisayo Caleb and Hong, Tianzhen and Spanos, Costas and Kj\ae{}rgaard, Mikkel Baun},
title = {PAD: Protecting Anonymity in Publishing Building Related Datasets},
year = {2017},
isbn = {9781450355445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3137133.3137140},
doi = {10.1145/3137133.3137140},
abstract = {The diffusion of low-cost sensor network technologies in smart buildings has enabled the collection of massive amounts of data regarding indoor environments, energy use and occupants, which, in turn, creates opportunities for knowledge- and information-based building management. Driven by benefits mutual to occupants, building managers, and research communities, there is a demand for data publication to foster more sophisticated and robust models and algorithms. Data in the original form, however, contains sensitive information about occupants' behavioral patterns, and publishing such data will violate individuals' privacy. The current practice on publishing building-related datasets relies primarily on policies for dictating which types of data can be published and agreements on the use of published data. This approach alone provides insufficient protection as it does not prevent privacy breaches from occurring in the first place.In this paper, we present PAD, which to our knowledge is the first system that provides a technological solution for publishing building related datasets in a privacy-preserving manner while maintaining high data quality. PAD is able to offer a strong anonymity guarantee by perturbing data records. The unique feature of PAD is that it offers an interface to incorporate dataset users into the loop of data publication and customizes the perturbation such that useful information in the dataset can be better retained. We study the efficacy of PAD using occupancy and plug load data collected in real buildings. The experiments demonstrate that PAD can achieve high resilience to privacy threats without introducing any significant data fidelity penalties.},
booktitle = {Proceedings of the 4th ACM International Conference on Systems for Energy-Efficient Built Environments},
articleno = {4},
numpages = {10},
keywords = {clustering, k-anonymity, occupancy privacy, convex optimization},
location = {Delft, Netherlands},
series = {BuildSys '17}
}

@inproceedings{10.1145/2623330.2623685,
author = {Li, Furong and Lee, Mong Li and Hsu, Wynne},
title = {Entity Profiling with Varying Source Reliabilities},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623685},
doi = {10.1145/2623330.2623685},
abstract = {The rapid growth of information sources on the Web has intensified the problem of data quality. In particular, the same real world entity may be described by different sources in various ways with overlapping information, and possibly conflicting or even erroneous values. In order to obtain a more complete and accurate picture for a real world entity, we need to collate the data records that refer to the entity, as well as correct any erroneous values. We observe that these two tasks are often tightly coupled: rectifying erroneous values will facilitate data collation, while linking similar records provides us with a clearer view of the data and additional evidence for error correction. In this paper, we present a framework called Comet that interleaves record linkage with error correction, taking into consideration the source reliabilities on various attributes. The proposed framework first utilizes confidence based matching to discriminate records in terms of ambiguity and source reliability. Then it performs adaptive matching to reduce the impact of erroneous values. Experiment results demonstrate that Comet outperforms the state-of-the-art techniques and is able to build complete and accurate profiles for real world entities.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1146–1155},
numpages = {10},
keywords = {source reliability, entity profiling, truth discovery, record linkage},
location = {New York, New York, USA},
series = {KDD '14}
}

@article{10.1145/2808198,
author = {Hao, Fei and Jiao, Mingjie and Min, Geyong and Yang, Laurence T.},
title = {Launching an Efficient Participatory Sensing Campaign: A Smart Mobile Device-Based Approach},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2808198},
doi = {10.1145/2808198},
abstract = {Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {oct},
articleno = {18},
numpages = {22},
keywords = {Participatory sensing, tensor, deployment, trajectory, recruitment, DTA}
}

@inproceedings{10.1145/3012071.3012077,
author = {Madera, Cedrine and Laurent, Anne},
title = {The next Information Architecture Evolution: The Data Lake Wave},
year = {2016},
isbn = {9781450342674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012071.3012077},
doi = {10.1145/3012071.3012077},
abstract = {Data warehouses and data marts have long been considered as the unique solution for providing end-users with decisional information. More recently, data lakes have been proposed in order to govern data swamps. However, no formal definition has been proposed in the literature. Existing works are not complete and miss important parts of the topic. In particular, they do not focus on the influence of the data gravity, the infrastructure role of those solutions and of course are proposing divergent definitions and positioning regarding the usage and the interaction with existing decision support system.In this paper, we propose a novel definition of data lakes, together with a comparison with other over several criteria as the way to populate them, how to use, what is the Data Lake end user profile. We claim that data lakes are complementary components in decisional information systems and we discuss their position and interactions regarding the other components by proposing an interaction model.},
booktitle = {Proceedings of the 8th International Conference on Management of Digital EcoSystems},
pages = {174–180},
numpages = {7},
keywords = {data reservoirs, digital transformation, data lakes, data lab, internet of things, data warehouses, data laboratory, data governance},
location = {Biarritz, France},
series = {MEDES}
}

@inproceedings{10.1145/2463676.2465337,
author = {Golab, Lukasz and Johnson, Theodore},
title = {Data Stream Warehousing},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465337},
doi = {10.1145/2463676.2465337},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {949–952},
numpages = {4},
keywords = {data warehousing, real-time analytics, data streams},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@article{10.1145/2430456.2430472,
author = {Dong, Xin Luna and Dragut, Eduard Constantin},
title = {10th International Workshop on Quality in Databases: QDB 2012},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2430456.2430472},
doi = {10.1145/2430456.2430472},
journal = {SIGMOD Rec.},
month = {jan},
pages = {55–59},
numpages = {5}
}

@inproceedings{10.1145/3333165.3333180,
author = {Elmekki, Hanae and Chiadmi, Dalila and Lamharhar, Hind},
title = {Open Government Data: Towards a Comparison of Data Lifecycle Models},
year = {2019},
isbn = {9781450360890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333165.3333180},
doi = {10.1145/3333165.3333180},
abstract = {Government, through Open Government Data "OGD, becomes one of the important producers of open data. OGD is an opportunity to create valuable services and innovative products useful for citizens as a primarily targeted consumer. However, the expected benefits of OGD are not yet met. That is to say, several research communities' studies insist on the necessity of creating valuable data in order to generate valuable services. These studies are still insufficient for a shared understanding of how OGD contribute to the creation of value. For this purpose, this paper presents a review of a set of data lifecycle models compared against their contribution to the creation of value in the context of OGD.},
booktitle = {Proceedings of the ArabWIC 6th Annual International Conference Research Track},
articleno = {15},
numpages = {6},
keywords = {data value creation, Open Government Data, data lifecycle},
location = {Rabat, Morocco},
series = {ArabWIC 2019}
}

@inproceedings{10.1145/3041021.3053059,
author = {Tao, Shibo and Wang, Xiaorong and Huang, Weijing and Chen, Wei and Wang, Tengjiao and Lei, Kai},
title = {From Citation Network to Study Map: A Novel Model to Reorganize Academic Literatures},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3053059},
doi = {10.1145/3041021.3053059},
abstract = {As the number of academic papers and new technologies soars, it has been increasingly difficult for researchers, especially beginners, to enter a new research field. Researchers often need to study a promising paper in depth to keep up with the forefront of technology. Traditional Query-Oriented study method is time-consuming and even tedious. For a given paper, existent academic search engines like Google Scholar tend to recommend relevant papers, failing to reveal the knowledge structure. The state-of-the-art Map-Oriented study methods such as AMiner and AceMap can structure scholar information, but they're too coarse-grained to dig into the underlying principles of a specific paper. To address this problem, we propose a Study-Map Oriented method and a novel model called RIDP (Reference Injection based Double-Damping PageRank) to help researchers study a given paper more efficiently and thoroughly. RIDP integrates newly designed Reference Injection based Topic Analysis method and Double-Damping PageRank algorithm to mine a Study Map out of massive academic papers in order to guide researchers to dig into the underlying principles of a specific paper. Experiment results on real datasets and pilot user studies indicate that our method can help researchers acquire knowledge more efficiently, and grasp knowledge structure systematically.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1225–1232},
numpages = {8},
keywords = {double-damping pagerank, study map, academic papers, reference injection, topic analysis},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3482632.3484077,
author = {Wang, Chunxia and Xie, Jian},
title = {Constructing a Computer Model for Discipline Data Governance Using the Contingency Theory and Data Mining},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3484077},
doi = {10.1145/3482632.3484077},
abstract = {Data governance is an important part of modernizing the governance capacity of universities. Discipline data governance plays an important role in promoting the development of university disciplines, and is a key factor in improving the governance of university disciplines, the science of educational decision-making and the effectiveness of management. It is an important way to promote the "precision" and "science" of discipline governance. In this paper, we construct a model of discipline data governance based on the Contingency Theory with a view to shedding light on discipline governance.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {1967–1970},
numpages = {4},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3444370.3444575,
author = {Zhang, Bo and Kong, Dehua},
title = {Dynamic Estimation Model of Insurance Product Recommendation Based on Naive Bayesian Model},
year = {2021},
isbn = {9781450387828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444370.3444575},
doi = {10.1145/3444370.3444575},
abstract = {Aiming at the dynamic estimation of insurance product recommendation, considering the particularity and complexity of purchasing insurance product and the uncertainty of influencing factors, a dynamic estimation model of insurance product recommendation based on Naive Bayes is proposed. The model combines customer insurance information with machine learning. The results show that the naive Bayesian classification algorithm can be compared with the decision tree and neural network classification algorithm, showing high accuracy and high speed.},
booktitle = {Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies},
pages = {219–224},
numpages = {6},
keywords = {Naive Bayes, dynamic estimation, insurance products, recommendation},
location = {Guangzhou, China},
series = {CIAT 2020}
}

@inproceedings{10.1145/3384544.3384588,
author = {Nugroho, Heru and Gumilang, Soni Fajar Surya},
title = {Recommendations for Improving Data Management Process in Government of Bandung Regency Using COBIT 4.1 Framework},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384588},
doi = {10.1145/3384544.3384588},
abstract = {Data is an valuable asset that potentially provides substantial benefits for the government and society. To make the performance of local government apparatus runs optimally and the public gets the best service, the government of Bandung Regency strives to improve data management. The initial stage of optimizing data management is the assessment of the maturity level in managing data (DS-11) using COBIT 4.1. Base on the assessment maturity level for DS-11, the government of Bandung Regency needs to raise the level from 2.46 (Repeatable but Intuitive) to 3.0 (Defined). Recommendations given to improve data management in Government with focuses on maintaining the completeness, accuracy, availability, and protection of data.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {57–61},
numpages = {5},
keywords = {Maturity, COBIT 4.1, Recommendations, DS-11, Data},
location = {Langkawi, Malaysia},
series = {ICSCA 2020}
}

@inproceedings{10.1145/3035918.3054772,
author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix},
title = {Data Profiling: A Tutorial},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3054772},
doi = {10.1145/3035918.3054772},
abstract = {is to understand the dataset at hand and its metadata. The process of metadata discovery is known as data profiling. Profiling activities range from ad-hoc approaches, such as eye-balling random subsets of the data or formulating aggregation queries, to systematic inference of structural information and statistics of a dataset using dedicated profiling tools. In this tutorial, we highlight the importance of data profiling as part of any data-related use-case, and we discuss the area of data profiling by classifying data profiling tasks and reviewing the state-of-the-art data profiling systems and techniques. In particular, we discuss hard problems in data profiling, such as algorithms for dependency discovery and profiling algorithms for dynamic data and streams. We also pay special attention to visualizing and interpreting the results of data profiling. We conclude with directions for future research in the area of data profiling. This tutorial is based on our survey on profiling relational data [2].},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1747–1751},
numpages = {5},
keywords = {data profiling, data exploration, dependency discovery},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@article{10.1145/2737817.2737831,
author = {Pedersen, Torben Bach and Castellanos, Malu and Dayal, Umesh},
title = {Report on the Seventh International Workshop on Business Intelligence for the Real Time Enterprise (BIRTE 2013)},
year = {2015},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2737817.2737831},
doi = {10.1145/2737817.2737831},
journal = {SIGMOD Rec.},
month = {feb},
pages = {55–58},
numpages = {4}
}

@article{10.1145/3190579,
author = {Bertino, Elisa and Jahanshahi, Mohammad R.},
title = {Adaptive and Cost-Effective Collection of High-Quality Data for Critical Infrastructure and Emergency Management in Smart Cities—Framework and Challenges},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3190579},
doi = {10.1145/3190579},
journal = {J. Data and Information Quality},
month = {may},
articleno = {1},
numpages = {6},
keywords = {edge computing, device swarms, Civil engineering}
}

@inproceedings{10.1145/3368756.3368965,
author = {Rhazal, Oumaima El and Tomader, Mazri},
title = {Study of Smart City Data: Categories and Quality Challenges},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3368965},
doi = {10.1145/3368756.3368965},
abstract = {Lately, the world attention is directed to transforming daily life to a smarter one, we cannot deny the smart city concept that became pervading. This concept will give every device the chance to communicate with other devices, it will simply create the smarter version of everything. However, data heterogeneity and quality changes are one of the best priorities and challenges that should be handled in this promising concept.In this paper we present a review about data categories circulating in a smart city depending on its required services. We also study the quality of information as one of both, major challenges and treasures in a smart city.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {4},
numpages = {7},
keywords = {internet of things (IoT), smart city, quality of information (QoI)},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@book{10.1145/3453538,
author = {ACM Data Science Task Force},
title = {Computing Competencies for Undergraduate Data Science Curricula},
year = {2021},
isbn = {9781450390606},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@inproceedings{10.1145/3463677.3463696,
author = {Geci, Mentor and CsAki, Csaba},
title = {BOLD in National Budget Planning – a Comparison of International Cases: BOLD in National Budget Planning},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463696},
doi = {10.1145/3463677.3463696},
abstract = {Anticipating the continues increase in quantity and consequently their importance, data are becoming a new currency. Overall purpose of this paper is to show the link between Big Open Linked Data (BOLD) and national budget planning. The methodology that follows is a qualitative case-study based approach leading to a comparative analysis of five cases. Research problems investigated are the commonalities and differences that may be identified in the handling of national budget data in developing countries, as well as best practices or potential ‘lessons learned’ from international cases of handling budget data as BOLD. In addition, the study investigates how Kosovo as a young developing country may benefit from the experiences of other countries. To create a framework of analysis, international cases are reviewed. Their comparison reveals that there are not that many commonalities among these developing countries in terms of issues and challenges regarding the use of BOLD in national budgeting. As it is mainly country specific, the approach used toward BOLD largely depends on the general landscape of each country. In comparison, although there is some progress made, Kosovo is still behind those countries in terms of applying BOLD.},
booktitle = {DG.O2021: The 22nd Annual International Conference on Digital Government Research},
pages = {189–197},
numpages = {9},
location = {Omaha, NE, USA},
series = {DG.O'21}
}

@article{10.1145/3015456,
author = {Cao, Longbing},
title = {Data Science: Challenges and Directions},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3015456},
doi = {10.1145/3015456},
abstract = {While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.},
journal = {Commun. ACM},
month = {jul},
pages = {59–68},
numpages = {10}
}

@article{10.1145/2983463,
author = {Peek, Geerten and Taspinar, Ahmet},
title = {One Thousand Interviews},
year = {2016},
issue_date = {Fall 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2983463},
doi = {10.1145/2983463},
abstract = {How customer insights keep one company agile, and challenge these data scientist to stay ahead in an ever-changing world.},
journal = {XRDS},
month = {sep},
pages = {11–12},
numpages = {2}
}

@inproceedings{10.1145/3379310.3379322,
author = {Rozi, Muhamad Fahru and Sucahyo, Yudho Giri and Gandhi, Arfive and Ruldeviyani, Yova},
title = {Appraising Personal Data Protection in Startup Companies in Financial Technology: A Case Study of ABC Corp},
year = {2020},
isbn = {9781450376853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379310.3379322},
doi = {10.1145/3379310.3379322},
abstract = {Financial Technology (fintech) has been immerged extensively in the last decade. In the realm of disruptive world, there are many areas in which startup companies are developing their business. There is always contradiction when dealing with innovation as core of digital disruption and how privacy remains as hot issues at the edge of everybody's talks. Internet plays important roles to sustain the trends. As rapidly growing country, 68% of Indonesian has access to the Internet. It drives startup companies on financial technology to innovate more and besides that they must comply to regulation in regard with personal data protection. This research aims to appraise how startup company on financial technology protect users' personal data. Personal data protection principles from international organization and Indonesian regulation regarding personal data protection are used to appraise how ABC Corp as a startup company that deliver financial technology service in Indonesian society. To ensure that its service is qualified and trustable, ABC Corp should be appraised using relevant criteria and qualitative approach. The results showed that most of regulations from sectorial supervising agency have been adhered by ABC Corp. The results bring meaningful insight to improve performance on personal data protection. They can became lessons for similar emerging startup companies in financial technology when acquiring their qualifications to protect users' personal data and keep their sustainability.},
booktitle = {Proceedings of the 2020 2nd Asia Pacific Information Technology Conference},
pages = {9–15},
numpages = {7},
keywords = {data privacy, Financial technology, digital economy, data protection, personal data},
location = {Bali Island, Indonesia},
series = {APIT 2020}
}

@article{10.1145/3511322.3511326,
author = {Dennis, Louise A.},
title = {Conference Reports},
year = {2022},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3511322.3511326},
doi = {10.1145/3511322.3511326},
abstract = {This section is compiled from reports of recent events sponsored or run in cooperation with ACM SIGAI. In general these reports were written and submitted by the conference organisers.},
journal = {AI Matters},
month = {jan},
pages = {15–17},
numpages = {3}
}

@article{10.14778/2824032.2824073,
author = {Qiao, Lin and Li, Yinan and Takiar, Sahil and Liu, Ziyang and Veeramreddy, Narasimha and Tu, Min and Dai, Ying and Buenrostro, Issac and Surlaker, Kapil and Das, Shirshanka and Botev, Chavdar},
title = {Gobblin: Unifying Data Ingestion for Hadoop},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824073},
doi = {10.14778/2824032.2824073},
abstract = {Data ingestion is an essential part of companies and organizations that collect and analyze large volumes of data. This paper describes Gobblin, a generic data ingestion framework for Hadoop and one of LinkedIn's latest open source products. At LinkedIn we need to ingest data from various sources such as relational stores, NoSQL stores, streaming systems, REST endpoints, filesystems, etc. into our Hadoop clusters. Maintaining independent pipelines for each source can lead to various operational problems. Gobblin aims to solve this issue by providing a centralized data ingestion framework that makes it easy to support ingesting data from a variety of sources.Gobblin distinguishes itself from similar frameworks by focusing on three core principles: generality, extensibility, and operability. Gobblin supports a mixture of data sources out-of-the-box and can be easily extended for more. This enables an organization to use a single framework to handle different data ingestion needs, making it easy and inexpensive to operate. Moreover, with an end-to-end metrics collection and reporting module, Gobblin makes it simple and efficient to identify issues in production.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1764–1769},
numpages = {6}
}

@inproceedings{10.1145/2656450.2656453,
author = {Kumar, Sathish Alampalayam},
title = {Designing a Graduate Program in Information Security and Analytics: Masters Program in Information Security and Analytics (MISA)},
year = {2014},
isbn = {9781450326865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656450.2656453},
doi = {10.1145/2656450.2656453},
abstract = {This paper introduces the concept of the Master of Information Security and Analytics (MISA) program for the graduate students with a background in CS, IS and IT. The 10-course graduate level program is benchmarked against existing masters programs in the areas of Information Security and Data Analytics, and an assessment was done on the estimated demand for MISA graduates in the nation. The program outcomes were then mapped against the course objectives to insure the correct mix of courses and topics. The program's admission requirement is also being discussed. This paper discusses the design process and possible ways to reduce risk in the start-up of a new degree program. How a program is marketed to prospective students and what program graduates will do after program completion is just as important as the initial design of the program. Planning for the administration of the program and the assessment process is an important phase of the initial design.},
booktitle = {Proceedings of the 15th Annual Conference on Information Technology Education},
pages = {141–146},
numpages = {6},
keywords = {cybersecurity, analytics, information technology education},
location = {Atlanta, Georgia, USA},
series = {SIGITE '14}
}

@inproceedings{10.1145/3195106.3195177,
author = {Baolong, Yang and Hong, Wu and Haodong, Zhang},
title = {Research and Application of Data Management Based on Data Management Maturity Model (DMM)},
year = {2018},
isbn = {9781450363532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195106.3195177},
doi = {10.1145/3195106.3195177},
abstract = {Through the analysis and contrast of the different Data Management Maturity Model, such as DCAM, DMM, DCMM and the model of IBM, we try to make empirical research under the framework of data management maturity model. This article take a project whose main research object is about the academic career of scientists and with massive unstructured data for example, through analysis of the goal, management processes and influence factors of this project in detail, we built up an evaluation system for data management for such projects under the framework of DCMM. It is expected to have a positive significance to the evaluation of similar data management capability.},
booktitle = {Proceedings of the 2018 10th International Conference on Machine Learning and Computing},
pages = {157–160},
numpages = {4},
keywords = {Data management, maturity model, Measurement and evaluation, Unstructured data},
location = {Macau, China},
series = {ICMLC 2018}
}

@inproceedings{10.1145/3463677.3463732,
author = {Ahn, Michael and Chu, Shengli},
title = {What Matters in Maintaining Effective Open Government Data Systems? The Role of Government Managerial Capacity, and Political and Legal Environment},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463732},
doi = {10.1145/3463677.3463732},
abstract = {This paper aims to identify key institutional factors that contribute to effective open data systems. Rapid advancement in new technologies such as machine learning, algorithms, IoT, and Cloud Computing has amplified the importance of national open data systems. The availability of relevant public data has become a crucial factor in creating sophisticated machine learning platforms or algorithms that will have a considerable impact on national competitiveness. Effective national open data strategies will matter in shaping an environment that will facilitate data production, dissemination, and utilization. Using multiple sources of data that measure the qualities of open data systems and various political, governmental, and legal attributes at the national level, we seek to identify key institutional factors that contribute to robust open data policies and outcomes. Our findings point to the importance of the existence of a national open data strategy and support (especially "open by default" strategy), pre-existing e-government capability, and countries operating under full democracy with its guarantees to civil liberties and political freedom. In addition, the nature of the open data matters as different managerial, political, and demographic conditions affected the quality of different open data systems. Policy implications of our findings are discussed.},
booktitle = {DG.O2021: The 22nd Annual International Conference on Digital Government Research},
pages = {444–457},
numpages = {14},
location = {Omaha, NE, USA},
series = {DG.O'21}
}

@inproceedings{10.1145/3487075.3487110,
author = {An, Zhenpeng and Zhang, Di and Liang, Yunjie},
title = {Research on Data Governance Framework for Fire Department},
year = {2021},
isbn = {9781450389853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487075.3487110},
doi = {10.1145/3487075.3487110},
abstract = {This paper analyzes data governance elements, models and frameworks, provides a clear plan for data governance for fire department. Using the method of literature research, network investigation and conclude data system of fire departments, the china domestic and foreign research status of data governance is reviewed. We build the framework of data governance for fire department, including Data resource directory system, Data technology support system and Data standardization system. This paper preliminarily forms the framework of data governance for fire department. This framework was applied to the fire information planning work. The results indicate that based on the status and characteristics of fire industry, the implementation of this framework is effective and feasible, and it is also the basis of standard fire control data governance in future.},
booktitle = {The 5th International Conference on Computer Science and Application Engineering},
articleno = {35},
numpages = {5},
keywords = {Fire Department component;, Data governance, Data standard system},
location = {Sanya, China},
series = {CSAE 2021}
}

@inproceedings{10.1145/3192975.3193004,
author = {Nabipourshiri, Rouzbeh and Abu-Salih, Bilal and Wongthongtham, Pornpit},
title = {Tree-Based Classification to Users' Trustworthiness in OSNs},
year = {2018},
isbn = {9781450364102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192975.3193004},
doi = {10.1145/3192975.3193004},
abstract = {In the light of the information revolution, and the propagation of big social data, the dissemination of misleading information is certainly difficult to control. This is due to the rapid and intensive flow of information through unconfirmed sources under the propaganda and tendentious rumors. This causes confusion, loss of trust between individuals and groups and even between governments and their citizens. This necessitates a consolidation of efforts to stop penetrating of false information through developing theoretical and practical methodologies aim to measure the credibility of users of these virtual platforms. This paper presents an approach to domain-based prediction to user's trustworthiness of Online Social Networks (OSNs). Through incorporating three machine learning algorithms, the experimental results verify the applicability of the proposed approach to classify and predict domain-based trustworthy users of OSNs.},
booktitle = {Proceedings of the 2018 10th International Conference on Computer and Automation Engineering},
pages = {190–194},
numpages = {5},
keywords = {users' trustworthiness, data mining, social media, Trust, Twitter, machine learning},
location = {Brisbane, Australia},
series = {ICCAE 2018}
}

@inproceedings{10.1145/3543712.3543749,
author = {Raab, Raphaele and Granigg, Wolfgang and Melcher, Michael},
title = {Need for Skilled Workers in the Area of Data Science and Cloud Computing in Styria},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543712.3543749},
doi = {10.1145/3543712.3543749},
abstract = {The aim of this paper is to discuss the results of a survey conducted to assess the need for skilled workers in the areas Data Science &amp; Cloud Computing in Styria, Austria. Firstly, the relevant roles and skills in the abovementioned areas had to be selected. Initially, this selection process is described. Consequently, a survey was designed and given to a representative group of companies. The survey includes questions regarding the need for skilled workers with respect to the domains and the selected skills in the areas Data Science &amp; Cloud Computing. Moreover, the respondents were asked about the importance of further education and the necessity of academic education in these areas. Overall, our survey concludes that the requirements for skilled workers in the areas of Data Science and Cloud Computing in Styria will increase significantly in the coming years.},
booktitle = {Proceedings of the 2022 8th International Conference on Computer Technology Applications},
pages = {28–34},
numpages = {7},
keywords = {data driven innovation, statistical analysis, data management, data science, Styria, artificial intelligence, business intelligence, cloud computing, machine learning, cloud platform expertise, personnel requirement},
location = {Vienna, Austria},
series = {ICCTA '22}
}

@inproceedings{10.1145/3357384.3360314,
author = {Duan, Rong and Xiao, Yanghua},
title = {Enterprise Knowledge Graph From Specific Business Task to Enterprise Knowledge Management},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3360314},
doi = {10.1145/3357384.3360314},
abstract = {Data driven Knowledge Graph is rapidly adapted by different societies. Many open domain and specific domain knowledge graphs have been constructed, and many industries have benefited from knowledge graph. Currently, enterprise related knowledge graph is classified as specific domain, but the applications span from solving a narrow specific problem to Enterprise Knowledge Management system. With the digital transform of traditional industry, Enterprise knowledge becomes more and more complicated, it involves knowledge from common domain, multiple specific domains, and corporate-specific in general. This tutorial provides an overview of current Enterprise Knowledge Graph(EKG). It distinguishes the EKG from specific domain according to the knowledge it covers, and provides the examples to illustrate the difference between EKG and specific domain KG. The tutorial further summarizes EKG into three types: Specific Business Task Enterprise KG, Specific Business Unit Enterprise KG and Cross Business Unit Enterprise KG, and illustrates the characteristics, steps, challenges, and future research in constructing and consuming of each of these three types of EKG .},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2965–2966},
numpages = {2},
keywords = {relation extraction, knowledge graph, ontology, enterprise knowledge management, entity recognition},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/2968219.2985840,
author = {Hui, Pan and Ou, Zhonghong and Zhang, Yanyong and Striegel, Aaron D},
title = {The 7th International Workshop on Hot Topics in Planet-Scale Measurement (HotPlanet '16)},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2985840},
doi = {10.1145/2968219.2985840},
abstract = {The recent advances of mobile devices, online social networks, and the emergence of the Internet of Things have driven the corresponding data collection and analytics to planetary scale. It is, thus, essential to provide a forum to discuss the technical advances, share the lessons, experiences, and challenges associated with real-world large-scale deployment. The 7th International Workshop on Hot Topics in Planet-Scale Measurement (HotPlanet '16) is to provide such a forum for the researchers and practitioners in the fields mentioned above. By bringing together the experts in these fields, and through thoughtful discussions and valuable sharing, HotPlanet '16 aims to advance the work in these fields forward.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1275–1278},
numpages = {4},
keywords = {deployment experiences, social computing, planet-scale measurement, data analytics, cloud computing, crowdsourcing, crowd sensing},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3085228.3085280,
author = {Zeleti, Fatemeh Ahmadi and Ojo, Adegboyega},
title = {Competitive Capability Framework for Open Government Data Organizations},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085280},
doi = {10.1145/3085228.3085280},
abstract = {Open data-driven organizations compete in a complex and uncertain environment with growing global competition, changing and emerging demand and market, and increasing levels of analytical tools and technology. For these organizations to exploit open data for competitive advantage, they need to develop the requisite competitive capabilities. This article presents an open data competitive capability framework grounded in theory and practice of open data. Based on extant literature and insights from domain experts, we identify and describe four dimensions of competitive capabilities required for open data driven organizations. We argue that by implementing the proposed framework, organizations can increase their chances to favorably compete in their respective markets. We further argue that by understanding open government data as a strategic resource for enterprises, government as producers or suppliers of this resource become key partners to data-driven organizations.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {250–259},
numpages = {10},
keywords = {Competitiveness in open data businesses, competitive strategies, organizational capabilities, open data organization, competitive advantage, open data capabilities},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@inproceedings{10.1145/2670757.2670779,
author = {Grillenberger, Andreas and Romeike, Ralf},
title = {A Comparison of the Field Data Management and Its Representation in Secondary CS Curricula},
year = {2014},
isbn = {9781450332507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2670757.2670779},
doi = {10.1145/2670757.2670779},
abstract = {In the last few years, the focus of data management has changed from handling relatively small amounts of data, often in relational databases, to managing large amounts of data using various different database types. In many secondary school curricula, data management is mainly considered from a "database" perspective. However, in contrast to the developments in computer science research and practice, the new and changing aspects of data management have hardly been discussed with respect to CS education. We suggest re-evaluating the focus and relevance of the established database syllabi, to discuss the educational value of the newly arising developments and to prevent the teaching of outdated concepts. In this paper, we will contrast current educational standards and curricula with an up-to-date characterization of data management in order to identify gaps between the principles and concepts of data management that are considered as important today from a professional point of view on the one side, and the emphasis in current CS education on the other side.The findings of this analysis will provide a basis for aligning the concepts taught in CS education with the developments in data management research and practice, as well as for re-evaluating the educational value of these concepts.},
booktitle = {Proceedings of the 9th Workshop in Primary and Secondary Computing Education},
pages = {29–36},
numpages = {8},
keywords = {characterization, analysis, data management, secondary school, databases, curricula, standards},
location = {Berlin, Germany},
series = {WiPSCE '14}
}

@inproceedings{10.1145/3429889.3429921,
author = {Ren, Kang and Liu, Fan and Zhuang, Haimei and Ling, Yun},
title = {AI-Based Multimodal Data Management and Intelligent Analysis System for Parkinson's Disease: GYENNO PD CIS},
year = {2020},
isbn = {9781450388603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429889.3429921},
doi = {10.1145/3429889.3429921},
abstract = {The GYENNO PD CIS is an AI-based multimodal data management and intelligent analysis system for Parkinson's disease (PD). The main purpose is to solve the problems in traditional diagnosis of PD such as lack of objective evaluation data, lack of reproducible diagnosis system, and lack of closed-loop treatment tracking, and then to construct a multimodal data management and intelligent analysis platform for PD, which can achieve the goals - standardization of data, objectification of evaluation, standardization of diagnosis, individualization of treatment, continuousness of management. It also helps Parkinson's experts in patient management, clinical data management, analysis and data mining, and supports multi-center projects, and finally lets patients benefit a lot from innovative technology.},
booktitle = {Proceedings of the 2020 International Symposium on Artificial Intelligence in Medical Sciences},
pages = {166–170},
numpages = {5},
keywords = {Multimodal, Quantitative evaluation and diagnostic data set, Intelligent analysis system, Parkinson's disease (PD)},
location = {Beijing, China},
series = {ISAIMS 2020}
}

@inproceedings{10.1145/3348445.3348464,
author = {Lin, Yuting},
title = {Government Management Model of Non-Profit Organizations Based on E-Government},
year = {2019},
isbn = {9781450371957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3348445.3348464},
doi = {10.1145/3348445.3348464},
abstract = {With the development and popularization of Internet technology, our country is increasingly aware of the importance of e-government, and continuously expands the channels and means of e-government development in policy, such as the application of e-government to the management of non-profit organizations. However, in practice, "e-government + NPO (non-profit organization) management" still has problems such as digital divide, information sharing and insufficient disclosure, and information security. Therefore, this paper proposes a more complete non-profit organization management model based on e-government. From the perspectives of optimization services, information sharing, network supervision and information security, it is explained how to effectively realize the efficient management of non-profit organizations based on e-government.},
booktitle = {Proceedings of the 7th International Conference on Computer and Communications Management},
pages = {164–168},
numpages = {5},
keywords = {non-profit organization, management model, E-government},
location = {Bangkok, Thailand},
series = {ICCCM '19}
}

@inproceedings{10.1145/3433996.3434019,
author = {Li, Ting and Zhang, Bo},
title = {Development Dilemma and Countermeasures of Data Journalism},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434019},
doi = {10.1145/3433996.3434019},
abstract = {In the field of news, with the operation of data journalism, the traditional press is facing great innovation and shock in the production, circulation, distribution and consumption of information. As McLuhan said, the birth of new media has opened up new possibilities in this era. Data, as a medium of the new era, is creating a new way for people to understand the world.This paper mainly discusses that it is still facing the problem of low degree of data opening in the current development, and the negative impact of disclosing users' personal privacy and information cocoon room. In view of these problems, relevant departments need to further strengthen the policy of data opening, improve the legal system, and optimize the link mode of information content dissemination, so as to promote the better development of data journalism and make data benefit people truly.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {127–131},
numpages = {5},
keywords = {information cocoons, visualization, data opening, Data journalism},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/2671491.2671497,
author = {Best, Daniel M. and Endert, Alex and Kidwell, Daniel},
title = {7 Key Challenges for Visualization in Cyber Network Defense},
year = {2014},
isbn = {9781450328265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2671491.2671497},
doi = {10.1145/2671491.2671497},
abstract = {What does it take to be a successful visualization in cyber security? This question has been explored for some time, resulting in many potential solutions being developed and offered to the cyber security community. However, when one reflects upon the successful visualizations in this space they are left wondering where all those offerings have gone. Excel and Grep are still the kings of cyber security defense tools; there is a great opportunity to help in this domain, yet many visualizations fall short and are not utilized.In this paper we present seven challenges, informed by two user studies, to be considered when developing a visualization for cyber security purposes. Cyber security visualizations must go beyond isolated solutions and "pretty picture" visualizations in order to impact users. We provide an example prototype that addresses the challenges with a description of how they are met. Our aim is to assist in increasing utility and adoption rates for visualization capabilities in cyber security.},
booktitle = {Proceedings of the Eleventh Workshop on Visualization for Cyber Security},
pages = {33–40},
numpages = {8},
keywords = {defense, visualization, cyber security},
location = {Paris, France},
series = {VizSec '14}
}

@article{10.1145/2430456.2430466,
author = {Beskales, George and Das, Gautam and Elmagarmid, Ahmed K. and Ilyas, Ihab F. and Naumann, Felix and Ouzzani, Mourad and Papotti, Paolo and Quiane-Ruiz, Jorge and Tang, Nan},
title = {The Data Analytics Group at the Qatar Computing Research Institute},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2430456.2430466},
doi = {10.1145/2430456.2430466},
journal = {SIGMOD Rec.},
month = {jan},
pages = {33–38},
numpages = {6}
}

@inproceedings{10.1145/3326365.3326374,
author = {Liu, Shuhua Monica and Pan, Liting and Lei, Yupei},
title = {What is the Role of New Generation of ICTs in Transforming Government Operation and Redefining State-Citizen Relationship in the Last Decade?},
year = {2019},
isbn = {9781450366441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326365.3326374},
doi = {10.1145/3326365.3326374},
abstract = {This article first introduce a new government initiative emerging after the US presidential election in 2008. Comparing to the more descriptive definitions of e-government, supporters of these new government initiatives emphasize the transformative and normative aspect of the newest generation of Information and Communication Technology (ICTs). They argue that the new initiative redefines how government should operate and transform state-citizen relationships. To understand the core of this initiative and whether it offers new opportunities to solve public problems, we collected and analyzed research papers published in the e-governance area between 2008 and 2017. Our analysis demonstrates that the use of new generation of ICTs has promoted the government information infrastructure. In other words, the application of new ICTs enables the government to accumulate and use a large amount of data, so that the government makes better decisions. The advancement of open data, the wide use of social media, and the potential of data analytics have also generated pressure to address challenging questions and issues in e-democracy. However, the analysis leads us to deliberate on whether the use of new generation of ICTs worldwide have actually achieved their goal. In the conclusion, we present challenges to be addressed before new innovative ICTs realize their potential towards better public governance.},
booktitle = {Proceedings of the 12th International Conference on Theory and Practice of Electronic Governance},
pages = {65–75},
numpages = {11},
keywords = {E-governance, Information and communication technology (ICT), Transformative governance},
location = {Melbourne, VIC, Australia},
series = {ICEGOV2019}
}

@inproceedings{10.5555/2693848.2693973,
author = {Elmegreen, Bruce G. and Sanchez, Susan M. and Szalay, Alexander S.},
title = {The Future of Computerized Decision Making},
year = {2014},
publisher = {IEEE Press},
abstract = {Computerized decision making is becoming a reality with exponentially growing data and machine capabilities. Some decision making is extremely complex, historically reserved for governing bodies or market places where the collective human experience and intelligence come to play. Other decision making can be trusted to computers that are on a path now into the future through novel software development and technological improvements in data access. In all cases, we should think about this carefully first: what data are really important for our goals and what data should be ignored or not even stored? The answer to these questions involves human intelligence and understanding before the data-to-decision process begins.},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {943–949},
numpages = {7},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.1145/3152465.3152473,
author = {Wang, Deqiang and Guo, Danhuai and Zhang, Hui},
title = {Spatial Temporal Data Visualization In Emergency Management: A View from Data-Driven Decision},
year = {2017},
isbn = {9781450354936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152465.3152473},
doi = {10.1145/3152465.3152473},
abstract = {Recent years, extreme events caused a great loss of human society. Emergency management is playing a more and more important role in handling disaster events. With the raising of data-intensive decision making, how to visualize large, multi-dimension data become an important challenge. Spatial temporal data visualization, a powerful tool, could transform data in to visual structure and make core information easily be captured by human. It could support spatial analysis, decision making and be used in all phase of emergency management. In this paper, we reviewed the general method of spatial temporal data visualization and the methods in data-intensive environment. Summarized the problems of each phase of emergency management and presented how spatial temporal visualization tools applied in each phase of emergency management. Finally, we conduct a short conclusion and outlook the future of spatial temporal visualization applied in data-driven emergency management environment.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL Workshop on Emergency Management Using},
articleno = {8},
numpages = {7},
keywords = {spatio-temporal visualization, emergency management, review},
location = {Redondo Beach, CA, USA},
series = {EM-GIS'17}
}

@inproceedings{10.5555/2814058.2814102,
author = {Barata, Andre Montoia and Prado, Edmir Parada Vasques},
title = {Data Governance in Brazilian Organizations},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {Organizations are increasingly looking for data integrity and quality to assist in strategic making decision and value creation. In this context Data Governance (DG) provide processes and practices that assist in the management and maintenance data. There are many frameworks to implementation DG process and benefits they may provide, however there are few implementation reported in the literature. This study aims to identify the DG process and frameworks implemented in Brazilian organizations and compare the benefits in implementation with those proposed by literature. For this will be carried out case studies in Brazilian organizations that implemented or are implementing DG frameworks.},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
pages = {267–272},
numpages = {6},
keywords = {Management Frameworks, Data Governance, System Information},
location = {Goiania, Goias, Brazil},
series = {SBSI 2015}
}

@inproceedings{10.1145/3491204.3527495,
author = {Bauer, Andr\'{e} and Leznik, Mark and Iqbal, Md Shahriar and Seybold, Daniel and Trubin, Igor and Erb, Benjamin and Domaschka, J\"{o}rg and Jamshidi, Pooyan},
title = {SPEC Research - Introducing the Predictive Data Analytics Working Group: Poster Paper},
year = {2022},
isbn = {9781450391597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491204.3527495},
doi = {10.1145/3491204.3527495},
abstract = {The research field of data analytics has grown significantly with the increase of gathered and available data. Accordingly, a large number of tools, metrics, and best practices have been proposed to make sense of this vast amount of data. To this end, benchmarking and standardization are needed to understand the proposed approaches better and continuously improve them. For this purpose, numerous associations and committees exist. One of them is SPEC (Standard Performance Evaluation Corporation), a non-profit corporation for the standardization and benchmarking of performance and energy evaluations. This paper gives an overview of the recently established SPEC RG Predictive Data Analytics Working Group. The mission of this group is to foster interaction between industry and academia by contributing research to the standardization and benchmarking of various aspects of data analytics.},
booktitle = {Companion of the 2022 ACM/SPEC International Conference on Performance Engineering},
pages = {13–14},
numpages = {2},
keywords = {measurements, data management, standardization, spec, data analytics, metrics, benchmarking},
location = {Bejing, China},
series = {ICPE '22}
}

@inproceedings{10.1145/3300115.3312508,
author = {Cassel, Lillian and Hongzhi, Wang},
title = {Panel: The Computing in Data Science},
year = {2019},
isbn = {9781450362597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300115.3312508},
doi = {10.1145/3300115.3312508},
abstract = {This panel brings the workings and results of the ACM Education Council Task Force on Data Science Education. The task force has gathered information on existing programs and has reviewed documents such as the result of the National Academies deliberations on data science. The task force is charged with exploring the role of computer science in data science education, understanding that data science is an inherently interdisciplinary field and not exclusively a computer science field. The panel will present a summary of the task force findings by two members of the task force and perspectives from leaders in data-intensive applications from China. The goal of the panel is to present the findings, but also to obtain perspectives from the attendees in order to enrich the task force's work.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education},
pages = {192–193},
numpages = {2},
keywords = {computing for data science, data science, computing curriculum},
location = {Chengdu,Sichuan, China},
series = {CompEd '19}
}

@article{10.1145/2579167,
author = {Raschid, Louiqa},
title = {Editorial},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2579167},
doi = {10.1145/2579167},
journal = {J. Data and Information Quality},
month = {may},
articleno = {14},
numpages = {2}
}

@article{10.1145/3524284,
author = {Abadi, Daniel and Ailamaki, Anastasia and Andersen, David and Bailis, Peter and Balazinska, Magdalena and Bernstein, Philip A. and Boncz, Peter and Chaudhuri, Surajit and Cheung, Alvin and Doan, Anhai and Dong, Luna and Franklin, Michael J. and Freire, Juliana and Halevy, Alon and Hellerstein, Joseph M. and Idreos, Stratos and Kossmann, Donald and Kraska, Tim and Krishnamurthy, Sailesh and Markl, Volker and Melnik, Sergey and Milo, Tova and Mohan, C. and Neumann, Thomas and Ooi, Beng Chin and Ozcan, Fatma and Patel, Jignesh and Pavlo, Andrew and Popa, Raluca and Ramakrishnan, Raghu and Re, Christopher and Stonebraker, Michael and Suciu, Dan},
title = {The Seattle Report on Database Research},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3524284},
doi = {10.1145/3524284},
abstract = {Every five years, a group of the leading database researchers meet to reflect on their community's impact on the computing industry as well as examine current research challenges.},
journal = {Commun. ACM},
month = {jul},
pages = {72–79},
numpages = {8}
}

@article{10.1145/3143313,
author = {Raschid, Louiqa},
title = {Editor-in-Chief (January 2014-May 2017) Farewell Report},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3143313},
doi = {10.1145/3143313},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {7},
numpages = {2}
}

@article{10.1007/s00778-015-0389-y,
author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix},
title = {Profiling Relational Data: A Survey},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-015-0389-y},
doi = {10.1007/s00778-015-0389-y},
abstract = {Profiling data to determine metadata about a given dataset is an important and frequent activity of any IT professional and researcher and is necessary for various use-cases. It encompasses a vast array of methods to examine datasets and produce metadata. Among the simpler results are statistics, such as the number of null values and distinct values in a column, its data type, or the most frequent patterns of its data values. Metadata that are more difficult to compute involve multiple columns, namely correlations, unique column combinations, functional dependencies, and inclusion dependencies. Further techniques detect conditional properties of the dataset at hand. This survey provides a classification of data profiling tasks and comprehensively reviews the state of the art for each class. In addition, we review data profiling tools and systems from research and industry. We conclude with an outlook on the future of data profiling beyond traditional profiling tasks and beyond relational databases.},
journal = {The VLDB Journal},
month = {aug},
pages = {557–581},
numpages = {25}
}

@inproceedings{10.1145/3428757.3429152,
author = {Philipp, Robert and Mladenow, Andreas and Strauss, Christine and V\"{o}lz, Alexander},
title = {Machine Learning as a Service: Challenges in Research and Applications},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429152},
doi = {10.1145/3428757.3429152},
abstract = {This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {396–406},
numpages = {11},
keywords = {Machine Learning Platform, Machine Learning as a Service, Machine Learning, MLaaS, Machine Learning Services},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@article{10.1145/3447269,
author = {Tufi\c{s}, Mihnea and Boratto, Ludovico},
title = {Toward a Complete Data Valuation Process. Challenges of Personal Data},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3447269},
doi = {10.1145/3447269},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {20},
numpages = {7},
keywords = {Datasets, data valuation, data markets}
}

@inproceedings{10.1145/3085228.3085269,
author = {Chen, Yumei and Dawes, Sharon S. and Chen, Shanshan},
title = {E-Government Support for Administrative Reform in China},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085269},
doi = {10.1145/3085228.3085269},
abstract = {This1 paper summarizes the history of Chinese administrative modernization and reform and discusses the ways in which China's e-government development agenda supports reform in the areas of transforming functions, streamlining processes, and enhancing transparency and citizen engagement. It offers a conceptual model of how e-government supports reform through policies, technologies, management improvements, and data designed to overcome the barriers of technical capability, staff resistance, and lack of cross-boundary collaboration. The analysis also shows how this interaction has generated new issues regarding official corruption and public engagement. We conclude with a future research agenda.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {329–335},
numpages = {7},
keywords = {Chinese government and reform, E-government, Administrative reform},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@inproceedings{10.1145/3379177.3388909,
author = {Munappy, Aiswarya Raj and Mattos, David Issa and Bosch, Jan and Olsson, Helena Holmstr\"{o}m and Dakkak, Anas},
title = {From Ad-Hoc Data Analytics to DataOps},
year = {2020},
isbn = {9781450375122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379177.3388909},
doi = {10.1145/3379177.3388909},
abstract = {The collection of high-quality data provides a key competitive advantage to companies in their decision-making process. It helps to understand customer behavior and enables the usage and deployment of new technologies based on machine learning. However, the process from collecting the data, to clean and process it to be used by data scientists and applications is often manual, non-optimized and error-prone. This increases the time that the data takes to deliver value for the business. To reduce this time companies are looking into automation and validation of the data processes. Data processes are the operational side of data analytic workflow.DataOps, a recently coined term by data scientists, data analysts and data engineers refer to a general process aimed to shorten the end-to-end data analytic life-cycle time by introducing automation in the data collection, validation, and verification process. Despite its increasing popularity among practitioners, research on this topic has been limited and does not provide a clear definition for the term or how a data analytic process evolves from ad-hoc data collection to fully automated data analytics as envisioned by DataOps.This research provides three main contributions. First, utilizing multi-vocal literature we provide a definition and a scope for the general process referred to as DataOps. Second, based on a case study with a large mobile telecommunication organization, we analyze how multiple data analytic teams evolve their infrastructure and processes towards DataOps. Also, we provide a stairway showing the different stages of the evolution process. With this evolution model, companies can identify the stage which they belong to and also, can try to move to the next stage by overcoming the challenges they encounter in the current stage.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {165–174},
numpages = {10},
keywords = {Data Pipelines, DevOps, Agile Methodology, Continuous Monitoring, DataOps, Data technologies},
location = {Seoul, Republic of Korea},
series = {ICSSP '20}
}

@article{10.1145/2063504.2063505,
author = {Madnick, Stuart E. and Lee, Yang W.},
title = {Editorial Notes Classification and Assessment of Large Amounts of Data: Examples in the Healthcare Industry and Collaborative Digital Libraries},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2063504.2063505},
doi = {10.1145/2063504.2063505},
journal = {J. Data and Information Quality},
month = {dec},
articleno = {12},
numpages = {2}
}

@inproceedings{10.1145/3479645.3479669,
author = {Sulistyowati, Ira and Fransisca, Dyna and Ruldeviyani, Yova},
title = {Data Analytics Readiness Model in Indonesian Government},
year = {2021},
isbn = {9781450384070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479645.3479669},
doi = {10.1145/3479645.3479669},
abstract = {The development of information technology encourages the government to digitize business processes. It is generated with a large and varied volume from various data sources so that advanced data analytics (DA) is required to overcome this to support organization's data driven decision making. It's necessary to prepare DA based on DA readiness model so that the implementation of DA can run successfully. Whereas currently, there is limited study and no standard model for DA readiness. The focus of this study is to propose model readiness of implementing data analytics that is suitable in Indonesian government. The model refers to DA readiness model based on literature review on 15 papers relevant to DA readiness. Then it's verified by 7 experts. Furthermore, online survey was conducted to test the model that affects the readiness of implementing data analytics in Indonesian government. The survey results were analyzed using factor analysis. As a result, DA readiness model contains 4 dimensions, 11 factors, and 78 indicators where its dimensions consist of information system, organizational and cultural, organization structure and resource readiness. This model can describe 85% of the data analysis readiness requirements in the Indonesian government. In order to implement data analytics successfully, the government needs to improve the readiness of information systems, organizational and cultural, organizational structures, and resources.},
booktitle = {6th International Conference on Sustainable Information Engineering and Technology 2021},
pages = {100–105},
numpages = {6},
location = {Malang, Indonesia},
series = {SIET '21}
}

@inproceedings{10.1145/2591888.2591901,
author = {Millard, Jeremy},
title = {ICT-Enabled Public Sector Innovation: Trends and Prospects},
year = {2013},
isbn = {9781450324564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591888.2591901},
doi = {10.1145/2591888.2591901},
abstract = {This experience paper is a personal thinkpiece which outlines many of the main issues and discussions taking place in Europe and elsewhere about the future of the public sector and how it can respond positively to some of the acute challenges it faces in light of the financial crisis and other global challenges. The paper examines how ICT-enabled public sector innovation highlights concepts like open governance, public value, government as a platform, open assets, open services and open engagement. It develops a vision of an 'open governance framework', moving beyond 'new public management', based on ICT-enabled societal-wide collaboration. It recognises that although the public sector can in principle create public value on its own, its potential to do so is greatly enhanced and extended by direct cooperation with other actors, or by facilitating public value creation by other actors on their own. It also examines the role of bottom-up innovation and public policy experimentation, as well as the need to focus on empowering civil servants and changing public sector working practices and mindsets.},
booktitle = {Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance},
pages = {77–86},
numpages = {10},
keywords = {government as a platform, open governance, open assets, public value, open engagement, open services},
location = {Seoul, Republic of Korea},
series = {ICEGOV '13}
}

@article{10.1145/3092931.3092933,
author = {Abiteboul, Serge and Arenas, Marcelo and Barcel\'{o}, Pablo and Bienvenu, Meghyn and Calvanese, Diego and David, Claire and Hull, Richard and H\"{u}llermeier, Eyke and Kimelfeld, Benny and Libkin, Leonid and Martens, Wim and Milo, Tova and Murlak, Filip and Neven, Frank and Ortiz, Magdalena and Schwentick, Thomas and Stoyanovich, Julia and Su, Jianwen and Suciu, Dan and Vianu, Victor and Yi, Ke},
title = {Research Directions for Principles of Data Management (Abridged)},
year = {2017},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/3092931.3092933},
doi = {10.1145/3092931.3092933},
journal = {SIGMOD Rec.},
month = {may},
pages = {5–17},
numpages = {13}
}

@inproceedings{10.1145/3468784.3471607,
author = {Umejiaku, Afamefuna and Dang, Tommy},
title = {Visualising Developing Nations Health Records: Opportunities, Challenges and Research Agenda},
year = {2021},
isbn = {9781450390125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468784.3471607},
doi = {10.1145/3468784.3471607},
abstract = {The benefits of effectively visualizing health records in huge volumes has resulted in health organizations, insurance companies, policy and decision makers, governments and drug manufactures’ transformation in the way research is conducted. This has also played a key role in determining investment of resources. Health records contain highly valuable information; processing these records in large volumes is now possible due to technological advancement which allows for the extraction of highly valuable knowledge that has resulted in breakthroughs in scientific communities. To visualize health records in large volumes, the records need to be stored in electronic forms, properly documented, processed, and analyzed. A good visualization technique is used to present the analyzed information, allowing for effective knowledge extraction which is done in a secured manner protecting the privacy of the patients whose health records were used. As research and technological advancement have improved, the quality of knowledge extracted from health records have also improved; unfortunately, the numerous benefits of visualizing health records have only been felt in developed nations, unlike other sectors where technological advancement in developed nations have had similar impact in developing nations. This paper identifies the characteristics of health records and the challenges involved in processing large volumes of health records. This is to identify possible steps that could be taken for developing nations to benefit from visualizing health records in huge volumes.},
booktitle = {The 12th International Conference on Advances in Information Technology},
articleno = {38},
numpages = {9},
keywords = {Developing Nations, Health records, Visualisation},
location = {Bangkok, Thailand},
series = {IAIT2021}
}

@article{10.1145/2782759.2782762,
author = {Laube, Patrick},
title = {The Low Hanging Fruit is Gone: Achievements and Challenges of Computational Movement Analysis},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/2782759.2782762},
doi = {10.1145/2782759.2782762},
abstract = {This position paper reviews the achievements and open challenges of movement analysis within Geographical Information Science. The paper argues that the simple problems of movement analysis have mostly been addressed to a sufficient level ("the low hanging fruit"), leaving the research community with the much more challenging problems for the years ahead ("the high hanging fruit"). Whereas the community has made good progress in structuring trajectory data (segmentation, similarity, clustering) and conceptualizing and detecting movement patterns, the much harder task of semantic annotation of structures and patterns remains difficult. The position paper summarizes both achievements and challenges with two sets assertions and calls for the establishment of a unifying theory of Computational Movement Analysis.},
journal = {SIGSPATIAL Special},
month = {may},
pages = {3–10},
numpages = {8}
}

@inproceedings{10.1145/3351108.3351110,
author = {Hattingh, Mari\'{e} and Marshall, Linda and Holmner, Marlene and Naidoo, Rennie},
title = {Data Science Competency in Organisations: A Systematic Review and Unified Model},
year = {2019},
isbn = {9781450372657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351108.3351110},
doi = {10.1145/3351108.3351110},
abstract = {The paper presents a systematic literature review of the literature on the competencies that are essential to develop a globally competitive workforce in the field of data science. The systematic review covers a wide range of literature but focuses primarily, but not exclusively, on the computing, information systems, management, and organisation science literature. The paper uses a broad research search strategy covering four separate electronic databases. The search strategy led the researchers to scan 139 titles, abstracts and keywords. Sixty potentially relevant articles were identified, of which 42 met the quality criteria and contributed to the analysis. A critical appraisal checklist assessed the validity of each empirical study. The researchers grouped the findings under six broad competency themes: organisational, technical, analytical, ethical and regulatory, cognitive and social. Thematic analysis was used to develop a unified model of data science competency based on the evidence of the findings. This model will be applied to case studies and survey research in future studies. A unified data science competency model, supported by empirical evidence, is crucial in closing the skills gap, thereby improving the quality and competitiveness of the South Africa's data science workforce. Researchers are encouraged to contribute to the further conceptual development of data science competency.},
booktitle = {Proceedings of the South African Institute of Computer Scientists and Information Technologists 2019},
articleno = {1},
numpages = {8},
keywords = {Competency, Systematic Literature Review, Skills, Data Science},
location = {Skukuza, South Africa},
series = {SAICSIT '19}
}

@inproceedings{10.1145/3397056.3397078,
author = {Ge, Juan and Han, Wenli and Zhang, Xunhu and Zhou, Jin},
title = {Research on Construction of Quality Service Platform of Survey and Mapping},
year = {2020},
isbn = {9781450377416},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397056.3397078},
doi = {10.1145/3397056.3397078},
abstract = {Quality data of Surveying and mapping is an intuitive reflection of the industry's quality situation and technical development situation. The construction of quality service platform of Surveying and mapping is discussed for the problems existing in the management of surveying and mapping quality data and for the demand for the use of quality data. It discusses the contents, framework and techniques used by the platform. The platform can be used to assist scientific decision-making and improve the service level.},
booktitle = {Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis},
pages = {57–61},
numpages = {5},
keywords = {Data management, Quality, Service platform},
location = {Marseille, France},
series = {ICGDA 2020}
}

@inproceedings{10.1145/3330431.3330457,
author = {Aljawarneh, Shadi and Radhakrishna, Vangipuram and Kumar, Gunupudi Rajesh},
title = {A Recent Survey on Challenges in Security and Privacy in Internet of Things},
year = {2019},
isbn = {9781450372121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330431.3330457},
doi = {10.1145/3330431.3330457},
abstract = {Computing environment in IoT (Internet of Things) is surrounded with huge amounts of heterogeneous data fulfilling many services in everyone's daily life. Since, communication process in IoT takes place using different devices such as smart phones, sensors, mobile devices, household devices, embedded equipment etc. With the use of these variety of devices, the exchange of data in open internet environment is prone to vulnerabilities. The main cause for these vulnerabilities is the weaknesses in the design of software components and hardware components. Bridging communications gaps in the IoT is a complex process as the data is from heterogeneous sources. An effort is made in this paper to discuss various challenges that are being faced in security and privacy of data. This will be very much helpful for researchers who want to pursue research.},
booktitle = {Proceedings of the 5th International Conference on Engineering and MIS},
articleno = {25},
numpages = {9},
keywords = {IoT classification, research issues, security and privacy, S/W weakness, IoT architecture, challenges in IoT, IoT services, vulnerability},
location = {Astana, Kazakhstan},
series = {ICEMIS '19}
}

@inproceedings{10.1145/2872518.2890599,
author = {Auer, S\"{o}ren and Heath, Tom and Bizer, Christian and Berners-Lee, Tim},
title = {LDOW2016: 9th Workshop on Linked Data on the Web},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890599},
doi = {10.1145/2872518.2890599},
abstract = {The ninth workshop on Linked Data (LDOW2016) on the Web is held in Montreal, Quebec, Canada on April 12, 2016 and co-located with the 25rd International World Wide Web Conference (WWW2016). The Web is developing from a medium for publishing textual documents into a medium for sharing structured data. This trend is fueled on the one hand by the adoption of the Linked Data principles by a growing number of data providers. On the other hand, large numbers of websites have started to semantically mark up the content of their HTML pages and thus also contribute to the wealth of structured data available on the Web. The 9th Workshop on Linked Data on the Web aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data from the Web as well as mining knowledge from the global Web of Data.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1039–1040},
numpages = {2},
keywords = {semantic web, linked data, rdf},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@article{10.1145/3385658.3385668,
author = {Abadi, Daniel and Ailamaki, Anastasia and Andersen, David and Bailis, Peter and Balazinska, Magdalena and Bernstein, Philip and Boncz, Peter and Chaudhuri, Surajit and Cheung, Alvin and Doan, AnHai and Dong, Luna and Franklin, Michael J. and Freire, Juliana and Halevy, Alon and Hellerstein, Joseph M. and Idreos, Stratos and Kossmann, Donald and Kraska, Tim and Krishnamurthy, Sailesh and Markl, Volker and Melnik, Sergey and Milo, Tova and Mohan, C. and Neumann, Thomas and Chin Ooi, Beng and Ozcan, Fatma and Patel, Jignesh and Pavlo, Andrew and Popa, Raluca and Ramakrishnan, Raghu and R\'{e}, Christopher and Stonebraker, Michael and Suciu, Dan},
title = {The Seattle Report on Database Research},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/3385658.3385668},
doi = {10.1145/3385658.3385668},
abstract = {Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {44–53},
numpages = {10}
}

@inproceedings{10.1145/3377929.3389894,
author = {Torresen, Jim},
title = {Addressing Ethical Challenges within Evolutionary Computation Applications: GECCO 2020 Tutorial},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389894},
doi = {10.1145/3377929.3389894},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1206–1223},
numpages = {18},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1145/2724721,
author = {Alonso, Omar},
title = {Challenges with Label Quality for Supervised Learning},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/2724721},
doi = {10.1145/2724721},
abstract = {Organizations that develop and use technologies around information retrieval, machine learning, recommender systems, and natural language processing depend on labels for engineering and experimentation. These labels, usually gathered via human computation, are used in machine-learned models for prediction and evaluation purposes. In such scenarios, collecting high-quality labels is a very important part of the overall process. We elaborate on these challenges and discuss research directions.},
journal = {J. Data and Information Quality},
month = {mar},
articleno = {2},
numpages = {3},
keywords = {human computation, crowdsourcing, machine learning, Label quality}
}

@inproceedings{10.5555/3017447.3017522,
author = {Lucic, Ana and Blake, Catherine},
title = {Preparing a Workforce to Effectively Reuse Data},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {For centuries, library and information science professionals have been responsible for curating and preserving access to information resources. The last few decades have seen an unprecedented change in how new knowledge is created, disseminated and reused both within academe and industry, which provides new opportunities to intervene within the data lifecycle. This paper documents efforts to create a graduate educational program that produces alum who understand both the social and technical aspects of data analytics and who can effectively employ data to address questions in academe and industry. We share perspectives gained from initial interviews with project partners who have data needs, and report on how those needs directly informed curricula development of the Socio-technical Data Analytics (SODA) program at the School of Information Sciences at the University of Illinois. We also provide a formative student evaluation of the program that was conducted to identify aspects that are successful, and those where further work is needed in order to help other schools who are developing similar programs that prepare a workforce who can effectively reuse data.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information &amp; Technology},
articleno = {75},
numpages = {10},
keywords = {survey results, data analytics and evaluation, data science, program development and evaluation},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@inproceedings{10.1145/3558819.3565165,
author = {Zhang, Tao and Ding, Jiantao and Guo, Zhiyong},
title = {Multimodal Knowledge Graph for Power Equipment Defect Data},
year = {2022},
isbn = {9781450397414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558819.3565165},
doi = {10.1145/3558819.3565165},
abstract = {Most traditional knowledge graph methods only learn knowledge representations from structured triples, ignoring the rich visual information in power images. To address this problem, this paper constructs a multimodal knowledge graph for power equipment defect data. The knowledge graph of power equipment images and defect texts is developed, and the InteractiveGraph graph database is used to realize knowledge storage, construct a multi-modal knowledge graph for power equipment defect data, and realize knowledge query and auxiliary decision-making.},
booktitle = {Proceedings of the 7th International Conference on Cyber Security and Information Engineering},
pages = {666–668},
numpages = {3},
location = {Brisbane, QLD, Australia},
series = {ICCSIE '22}
}

@inproceedings{10.1145/3227696.3227715,
author = {Tanaka, Yasuhiro and Kodate, Akihisa and Bolt, Timothy},
title = {Data Sharing System Based on Legal Risk Assessment},
year = {2018},
isbn = {9781450364652},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3227696.3227715},
doi = {10.1145/3227696.3227715},
abstract = {Regulations on protection of personal information vary from country to country. Therefore, when conducting international surveys for research, it is required to collect, manage and operate personal data properly complying with the laws and regulations of each country.We design a support system to fulfill conditions in terms of compliance for the proper and efficient management of data collection and utilization especially universities by making compliance management related to data cooperation a common foundation.This study aims to discuss requirements for the compliance management base system for data alliance and shared use of data.},
booktitle = {Proceedings of the 5th Multidisciplinary International Social Networks Conference},
articleno = {17},
numpages = {5},
keywords = {Legal Risk Assessment, Information system, Data Sharing, Privacy Protection, Personal Data},
location = {Saint-Etienne, France},
series = {MISNC '18}
}

@inproceedings{10.1145/3512826.3512836,
author = {Wu, Xueqiong and Chen, Lei and Ji, Kun and Wang, Huidong and Qian, Hao and Ma, Lidong},
title = {Design and Application of Virtual Production Command Service in Power Distribution Network Based on Artificial Intelligence},
year = {2022},
isbn = {9781450395489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512826.3512836},
doi = {10.1145/3512826.3512836},
abstract = {Abstract: As the distribution network business hub, the distribution network production command center faces the need to improve the efficiency of the distribution network production command business. This article draws on international mainstream artificial intelligence (such as Google AlphaGo) and other independent learning models to explore the integration of artificial intelligence and power grid professional business. This paper analyzes the development trend of artificial intelligence technology in the fields of power grid distribution and power knowledge map, and proposes a distribution network virtual production commander engine with dispatch operation, remote monitoring, and intelligent screen monitoring capabilities based on the distribution network knowledge map to realize power grid dispatch Intelligent applications in the fields of operation command, emergency repair, and smart services, and some functions have been verified by the State Grid Hangzhou Electric Power Company.},
booktitle = {2022 The 3rd International Conference on Artificial Intelligence in Electronics Engineering},
pages = {33–37},
numpages = {5},
keywords = {AI, Power Distribution Network Regulations, Power Distribution Network Virtual Production Command Engine, Power Knowledge Graph, Dispatch Professional Decision},
location = {Bangkok, Thailand},
series = {AIEE 2022}
}

