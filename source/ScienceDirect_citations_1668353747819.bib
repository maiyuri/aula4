@article{ROEHL201895,
title = {Modeling fouling in a large RO system with artificial neural networks},
journal = {Journal of Membrane Science},
volume = {552},
pages = {95-106},
year = {2018},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2018.01.064},
url = {https://www.sciencedirect.com/science/article/pii/S0376738817318665},
author = {Edwin A. Roehl and David A. Ladner and Ruby C. Daamen and John B. Cook and Jana Safarik and Donald W. Phipps and Peng Xie},
keywords = {Reverse osmosis system, Full-scale, Fouling, Flux decline, Modeling, Neural network},
abstract = {Artificial neural network (ANN) models were developed from a six-year process database to quantify causes of membrane fouling in the first stage of a full-scale, three-stage reverse osmosis (RO) system. The data comprised 59 hydraulic and water quality parameters, representing 190 runs between membrane cleanings. The runs were segmented into a Phase 1 period of initial particle deposition followed by a Phase 2 period of gradual biofilm and scale growth. The phases were modeled separately. Rather than specific flux, a fouling indicator Pfoul′ was calculated from RO system pressures which are normally modulated in part to compensate for fouling. The ANN modeling found that the best predictors of Phase 1 fouling were total chlorine, electrical conductance, TDS, ammonia, and the cartridge filter pressure drop. The best predictors of Phase 2 fouling were turbidity, nitrate, organic nitrogen, nitrite, and total chlorine. These results are consistent with known Phase 1 and 2 fouling mechanisms. The predictive electrical conductance, TDS, and turbidity are “bulk” water quality parameters which were found significantly correlated to sparsely measured cations, sulfates, chlorides, and alkalinity. Simulations with different chlorine concentrations demonstrate how the model could be used to reduce fouling rates.}
}
@article{KRUK2018e1196,
title = {High-quality health systems in the Sustainable Development Goals era: time for a revolution},
journal = {The Lancet Global Health},
volume = {6},
number = {11},
pages = {e1196-e1252},
year = {2018},
issn = {2214-109X},
doi = {https://doi.org/10.1016/S2214-109X(18)30386-3},
url = {https://www.sciencedirect.com/science/article/pii/S2214109X18303863},
author = {Margaret E Kruk and Anna D Gage and Catherine Arsenault and Keely Jordan and Hannah H Leslie and Sanam Roder-DeWan and Olusoji Adeyi and Pierre Barker and Bernadette Daelmans and Svetlana V Doubova and Mike English and Ezequiel García-Elorrio and Frederico Guanais and Oye Gureje and Lisa R Hirschhorn and Lixin Jiang and Edward Kelley and Ephrem Tekle Lemango and Jerker Liljestrand and Address Malata and Tanya Marchant and Malebona Precious Matsoso and John G Meara and Manoj Mohanan and Youssoupha Ndiaye and Ole F Norheim and K Srinath Reddy and Alexander K Rowe and Joshua A Salomon and Gagan Thapa and Nana A Y Twum-Danso and Muhammad Pate}
}
@article{SUBRAMANIAN2018417,
title = {Modeling Social Life Cycle Assessment framework for an electronic screen product – A case study of an integrated desktop computer},
journal = {Journal of Cleaner Production},
volume = {197},
pages = {417-434},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.06.193},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618318444},
author = {Karpagam Subramanian and Winco K.C. Yung},
keywords = {Sustainability, Electronic screen products, HP, All-in-one desktop computer, Social life cycle assessment, Decision-making},
abstract = {Large scientific literature addresses the environmental impacts of computers, while society is less considered. Although a couple of studies have analyzed laptops based on Social Life Cycle Assessment (SLCA), a quantitative weighting step is missing. Against this background, this paper aims at integrating a weighting approach from literature and proposes a comprehensive model to assess social impacts along a product's life cycle. The model uses an inventory majorly tailored on generic data, focusing on simplified list of components obtained from dismantling the product. Quantitative and semi-quantitative indicators are normalized based on a three-level scale and a weighting factor is applied to enable aggregation of results at stakeholder level and compare different life cycle stages. The model was tested in a cradle to grave case study on an integrated desktop as a first-time application. Results indicate potentially negative social impacts on workers, local community and society. In contrast, low impacts resulted for the value chain actors and consumers. Raw material extraction and productions of basic materials were documented as the most impactful phases. The model and the associated weighting step facilitates companies with a practical method to conduct social impact assessment and assess the social performances of their product's life cycle.}
}
@article{YANG20182140,
title = {The Tsinghua–Lancet Commission on Healthy Cities in China: unlocking the power of cities for a healthy China},
journal = {The Lancet},
volume = {391},
number = {10135},
pages = {2140-2184},
year = {2018},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(18)30486-0},
url = {https://www.sciencedirect.com/science/article/pii/S0140673618304860},
author = {Jun Yang and José G Siri and Justin V Remais and Qu Cheng and Han Zhang and Karen K Y Chan and Zhe Sun and Yuanyuan Zhao and Na Cong and Xueyan Li and Wei Zhang and Yuqi Bai and Jun Bi and Wenjia Cai and Emily Y Y Chan and Wanqing Chen and Weicheng Fan and Hua Fu and Jianqing He and Hong Huang and John S Ji and Peng Jia and Xiaopeng Jiang and Mei-Po Kwan and Tianhong Li and Xiguang Li and Song Liang and Xiaofeng Liang and Lu Liang and Qiyong Liu and Yongmei Lu and Yong Luo and Xiulian Ma and Bernhard Schwartländer and Zhiyong Shen and Peijun Shi and Jing Su and Tinghai Wu and Changhong Yang and Yongyuan Yin and Qiang Zhang and Yinping Zhang and Yong Zhang and Bing Xu and Peng Gong}
}
@incollection{YUZBASIOGLU201857,
title = {Chapter 4 - Biobanks as Basis of Individualized Medicine: Challenges Toward Harmonization},
editor = {Hans-Peter Deigner and Matthias Kohl},
booktitle = {Precision Medicine},
publisher = {Academic Press},
pages = {57-77},
year = {2018},
isbn = {978-0-12-805364-5},
doi = {https://doi.org/10.1016/B978-0-12-805364-5.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053645000044},
author = {Ayse Yuzbasioglu and Burcu Kesikli and Meral Ozguç},
keywords = {Biobanks, Governance, Harmonization, Ethical guidelines, Quality assurance management},
abstract = {Biobanking is a recent field where human biospecimens and related personal and clinical data are collected and stored for long-term use in biomedical research. The governance models for biobanking research need to involve all stakeholders and must be organized in both ethics and legal aspects and quality management systems. Today, biobanking is moving toward a global scale, and harmonization of all governance steps must be well defined to ensure that a larger research community can benefit from shared materials of high standards. Main ethical issues involved are mostly around protection of donor privacy, informed consent where individual autonomy must be exercised should be a norm, and use of harmonized consent model can facilitate international networking. Quality management of biobanks assures high quality and security of biosamples and associated data; this further fosters public trust and utility of sample collections. This chapter summarizes current governance structures and challenges for international harmonization of biobanks.}
}
@article{DLUGOSZ2017145,
title = {Generalized partially linear regression with misclassified data and an application to labour market transitions},
journal = {Computational Statistics & Data Analysis},
volume = {110},
pages = {145-159},
year = {2017},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167947317300166},
author = {Stephan Dlugosz and Enno Mammen and Ralf A. Wilke},
keywords = {Semiparametric regression, Measurement error, Side information},
abstract = {Large data sets that originate from administrative or operational activity are increasingly used for statistical analysis as they often contain very precise information and a large number of observations. But there is evidence that some variables can be subject to severe misclassification or contain missing values. Given the size of the data, a flexible semiparametric misclassification model would be good choice but their use in practise is scarce. To close this gap a semiparametric model for the probability of observing labour market transitions is estimated using a sample of 20 m observations from Germany. It is shown that estimated marginal effects of a number of covariates are sizeably affected by misclassification and missing values in the analysis data. The proposed generalized partially linear regression extends existing models by allowing a misclassified discrete covariate to be interacted with a nonparametric function of a continuous covariate.}
}
@article{WANG2018367,
title = {High resolution mapping of soil organic carbon stocks using remote sensing variables in the semi-arid rangelands of eastern Australia},
journal = {Science of The Total Environment},
volume = {630},
pages = {367-378},
year = {2018},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2018.02.204},
url = {https://www.sciencedirect.com/science/article/pii/S004896971830603X},
author = {Bin Wang and Cathy Waters and Susan Orgill and Jonathan Gray and Annette Cowie and Anthony Clark and De Li Liu},
keywords = {Soil organic carbon stocks, Seasonal fractional cover, Remote sensing, Machine learning, Digital soil mapping},
abstract = {Efficient and effective modelling methods to assess soil organic carbon (SOC) stock are central in understanding the global carbon cycle and informing related land management decisions. However, mapping SOC stocks in semi-arid rangelands is challenging due to the lack of data and poor spatial coverage. The use of remote sensing data to provide an indirect measurement of SOC to inform digital soil mapping has the potential to provide more reliable and cost-effective estimates of SOC compared with field-based, direct measurement. Despite this potential, the role of remote sensing data in improving the knowledge of soil information in semi-arid rangelands has not been fully explored. This study firstly investigated the use of high spatial resolution satellite data (seasonal fractional cover data; SFC) together with elevation, lithology, climatic data and observed soil data to map the spatial distribution of SOC at two soil depths (0–5cm and 0–30cm) in semi-arid rangelands of eastern Australia. Overall, model performance statistics showed that random forest (RF) and boosted regression trees (BRT) models performed better than support vector machine (SVM). The models obtained moderate results with R2 of 0.32 for SOC stock at 0–5cm and 0.44 at 0–30cm, RMSE of 3.51MgCha−1 at 0–5cm and 9.16MgCha−1 at 0–30cm without considering SFC covariates. In contrast, by including SFC, the model accuracy for predicting SOC stock improved by 7.4–12.7% at 0–5cm, and by 2.8–5.9% at 0–30cm, highlighting the importance of including SFC to enhance the performance of the three modelling techniques. Furthermore, our models produced a more accurate and higher resolution digital SOC stock map compared with other available mapping products for the region. The data and high-resolution maps from this study can be used for future soil carbon assessment and monitoring.}
}
@article{JABEUR2018495,
title = {Crowd social media computing: Applying crowd computing techniques to social media},
journal = {Applied Soft Computing},
volume = {66},
pages = {495-505},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2017.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S1568494617305653},
author = {Nafaâ Jabeur and Ahmed Nait-Sidi-Moh and Sherali Zeadally},
keywords = {Social media, Crowd computing, Return of investment, Audience engagement},
abstract = {Social media producers are currently having a fierce competition worldwide to increase their revenues. To achieve this goal, they are investigating alternative ways to attract more users, generate new user activities, and collect valuable data for personalizing contents and services. One such alternative is crowd computing. Our vision is based on the great potential of a well-coordinated and controlled joint utilization of human intelligence and computer systems that can help solve problems that would be difficult to do with individual capabilities alone. To achieve this vision, which we summarize under our concept of crowd social media computing, we investigate and model the characteristics of the social media ecosystem, we discuss the characteristics of crowd computing, and then we demonstrate how crowd computing can play a pivotal role in emerging social media applications. We also propose a new approach to evaluate the impact of crowd computing on the issue of social media Return of Investment (ROI).}
}
@article{WEGENER20181,
title = {Dawn of new machining concepts:: Compensated, intelligent, bioinspired},
journal = {Procedia CIRP},
volume = {77},
pages = {1-17},
year = {2018},
note = {8th CIRP Conference on High Performance Cutting (HPC 2018)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.08.194},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118310436},
author = {Konrad Wegener and Thomas Gittler and Lukas Weiss},
keywords = {Biologicalization, Industrie 4.0, Cognitive Technical Systems, Machine learning Strategies, Bioinspired, Biointelligent Manufacturing, Adaptive Thermal Compensation, Self-Organization, Self-Healing, Sensors in Abundance},
abstract = {The impact of Industrie 4.0 onto machine tools is significant, despite the fact, that quite some of the novelties discussed within this new paradigm have their roots decades earlier. But especially the concerted action, which strives the development of sensors, controls, data processing together with connectivity, unprecedented data integration and the notion of cyber physical production systems open up new development lines towards manufacturing systems as enablers for the progress in manufacturing. Highly developed compensation concepts are developing into state depending AI-supported strategies. Maintenance becomes predictive, as learning of machines becomes global and model based. Further inspirations taken from biological systems are adopted for machining centres and drive a biological transformation of manufacturing machines. Machine intelligence becomes the basis for executing manufacturing processes, which requires a close integration of process intelligence (CAM-systems) and machine controls.}
}
@article{TOBOGU2018240,
title = {ICT adoption in road freight transport in Nigeria – A case study of the petroleum downstream sector},
journal = {Technological Forecasting and Social Change},
volume = {131},
pages = {240-252},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517312568},
author = {Abiye Tob-Ogu and Niraj Kumar and John Cullen},
keywords = {ICT adoption and adaptation, Road freight transport, Resource based view, Institutional theory},
abstract = {This paper advances the ICT adoption discourse to explore ICT mechanism use, adaptation and contextual influences on management strategies in Africa. A polar-type multiple case studies approach is used to guide empirical data collection across 10 individual cases. 21 interviews were conducted with top executives and these were corroborated with over 30h of non-participant observations and archival documentation from these cases. Using a tripartite coding frame, thematic and content analyses were performed to identify patterns and themes in the collected data. Findings of this study evidence ICT use at firm level with significant links to local contextual factors. Additionally, whilst affirming relationships between size and adoption, the findings also suggest an inverted parallel between both variables. The paper contributes by empirically highlighting the influence of contextual factors on ICT use in road freight transportation as well as highlighting the potential for ICT developers and OEMs to acquire innovative input from local adaptation practices within the industry.}
}
@article{WANG201831,
title = {Spatio-temporal fusion for daily Sentinel-2 images},
journal = {Remote Sensing of Environment},
volume = {204},
pages = {31-42},
year = {2018},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2017.10.046},
url = {https://www.sciencedirect.com/science/article/pii/S0034425717305096},
author = {Qunming Wang and Peter M. Atkinson},
keywords = {Sentinel-2, Sentinel-3, Image fusion, Downscaling},
abstract = {Sentinel-2 and Sentinel-3 are two newly launched satellites for global monitoring. The Sentinel-2 Multispectral Imager (MSI) and Sentinel-3 Ocean and Land Colour Instrument (OLCI) sensors have very different spatial and temporal resolutions (Sentinel-2 MSI sensor 10m, 20m and 60m, 10days, albeit 5days with 2 sensors, conditional upon clear skies; Sentinel-3 OLCI sensor 300m, <1.4days with 2 sensors). For local monitoring (e.g., the growing cycle of plants) one either has the desired spatial or temporal resolution, but not both. In this paper, spatio-temporal fusion is considered to fuse Sentinel-2 with Sentinel-3 images to create nearly daily Sentinel-2 images. A challenging issue in spatio-temporal fusion is that there can be very few cloud-free fine spatial resolution images temporally close to the prediction time, or even available, strong temporal (i.e., seasonal) changes may exist. To this end, a three-step method consisting of regression model fitting (RM fitting), spatial filtering (SF) and residual compensation (RC) is proposed, which is abbreviated as Fit-FC. The Fit-FC method can be performed using only one Sentinel-3–Sentinel-2 pair and is advantageous for cases involving strong temporal changes (i.e., mathematically, the correlation between the two Sentinel-3 images is small). The effectiveness of the method was validated using two datasets. The created nearly daily Sentinel-2 time-series images have great potential for timely monitoring of highly dynamic environmental, agricultural or ecological phenomena.}
}
@incollection{MOSIER201855,
title = {Chapter 5 - The Brain Initiative—Implications for a Revolutionary Change in Clinical Medicine via Neuromodulation Technology},
editor = {Elliot S. Krames and P. Hunter Peckham and Ali R. Rezai},
booktitle = {Neuromodulation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {55-68},
year = {2018},
isbn = {978-0-12-805353-9},
doi = {https://doi.org/10.1016/B978-0-12-805353-9.00005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012805353900005X},
author = {Elizabeth M. Mosier and Michael Wolfson and Erika Ross and James Harris and Doug Weber and Kip A. Ludwig},
keywords = {Bioelectronic medicines, BRAIN initiative, Deep brain stimulation, Defense Applied Research Projects Agency, Electroceuticals, National Institutes of Health, National Science Foundation, Neuromodulation, Neuroprosthetics, Neurotechnology, Public/private partnerships},
abstract = {Launched in April 2013, the goal of the White House Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative is to catalyze the development of neurotechnology to provide new insights into the fundamental mechanisms of disease process. This ongoing international effort spans both the public and private sectors and has produced a number of specialized programs intended to facilitate partnerships between engineers, scientists, clinicians, industry, and regulatory agencies to translate new neuromodulation technologies into initial clinical studies. In this chapter, the history behind the launch of the BRAIN Initiative and the implications of the new BRAIN programs on the development of neuromodulation technologies are described.}
}
@article{TANG2017302,
title = {Towards a trust evaluation middleware for cloud service selection},
journal = {Future Generation Computer Systems},
volume = {74},
pages = {302-312},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1600011X},
author = {Mingdong Tang and Xiaoling Dai and Jianxun Liu and Jinjun Chen},
keywords = {Cloud services, Trust evaluation, Reputation, Service selection, Middleware},
abstract = {With the advent of cloud computing, employing various cloud services to build highly reliable cloud applications has become increasingly popular. The trustworthiness of cloud services is a critical issue that hinders the development of cloud applications, and thus is an urgently-required research problem. Previous studies evaluate trustworthiness of services via either QoS monitoring mechanisms or user feedback ratings, while seldom they combine both of them for enhancing service trust evaluation. This paper proposes a trustworthy selection framework for cloud service selection, named TRUSS. Aiming at developing an effective trust evaluation middleware for TRUSS, we propose an integrated trust evaluation method via combining objective trust assessment and subjective trust assessment. The objective trust assessment is based on QoS monitoring, while the subjective trust assessment is based on user feedback ratings. Experiments conducted using a synthesized dataset show that our proposed method significantly outperforms the other trust and reputation methods.}
}
@incollection{HERRMANN2018187,
title = {Chapter 11 - Designing Health Care That Works—Socio-technical Conclusions},
editor = {Mark S. Ackerman and Sean P. Goggins and Thomas Herrmann and Michael Prilla and Christian Stary},
booktitle = {Designing Healthcare That Works},
publisher = {Academic Press},
pages = {187-203},
year = {2018},
isbn = {978-0-12-812583-0},
doi = {https://doi.org/10.1016/B978-0-12-812583-0.00011-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128125830000110},
author = {Thomas Herrmann and Mark S. Ackerman and Sean P. Goggins and Christian Stary and Michael Prilla}
}
@article{RUAN201777,
title = {Introducing cybernomics: A unifying economic framework for measuring cyber risk},
journal = {Computers & Security},
volume = {65},
pages = {77-89},
year = {2017},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2016.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167404816301407},
author = {Keyun Ruan},
keywords = {Cybernomics, Cyber risk unit, Economic modelling, Risk analytics, Enterprise risk management},
abstract = {This is the first in a series of papers on the risk measures and unifying economic framework encompassing the cross-disciplinary field of “Cybernomics”. This is also the first academic paper to formally propose measurement units for cyber risk. In this paper, multidisciplinary methodologies are used to apply proven risk measurement methods in finance and medicine to define novel risk units central to cybernomics. Leveraging established risk units – MicroMort (MM) for measuring medical risk and Value-at-Risk (VaR) for measuring market risk – BitMort (BM) and hekla (named after an Icelandic volcano) are defined as cyber risk units. Risk calculation methods and examples are introduced in this paper to measure cost-effectiveness of control factors, articulate an entity's “willingness-to-pay” (risk pricing) for cyber risk reduction, cyber risk limit, and cyber risk appetite. Built around BM and hekla, cybernomics integrates cyber risk management and economics to study the requirements of a databank in order to improve risk analytics solutions for: 1) the valuation of digital assets; 2) the measurement of risk exposure of digital assets; and 3) the capital optimization for managing residual cyber risk. Establishing adequate, holistic and statistically robust data points on the entity, portfolio and global levels for the development of a cybernomics databank are essential for the resilience of our shared digital future. This paper explains the need to establish data schemes such as International Digital Asset Classification (IDAC) and International Classification of Cyber Incidents (ICCI).}
}
@article{EISENBERG201846,
title = {Applying novel technologies and methods to inform the ontology of self-regulation},
journal = {Behaviour Research and Therapy},
volume = {101},
pages = {46-57},
year = {2018},
note = {An experimental medicine approach to behavior change: The NIH Science of Behavior Change (SOBC)},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2017.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0005796717302048},
author = {Ian W. Eisenberg and Patrick G. Bissett and Jessica R. Canning and Jesse Dallery and A. Zeynep Enkavi and Susan Whitfield-Gabrieli and Oscar Gonzalez and Alan I. Green and Mary Ann Greene and Michaela Kiernan and Sunny Jung Kim and Jamie Li and Michael R. Lowe and Gina L. Mazza and Stephen A. Metcalf and Lisa Onken and Sadev S. Parikh and Ellen Peters and Judith J. Prochaska and Emily A. Scherer and Luke E. Stoeckel and Matthew J. Valente and Jialing Wu and Haiyi Xie and David P. MacKinnon and Lisa A. Marsch and Russell A. Poldrack},
keywords = {Self-regulation, Ontology, Neuroimaging, Intervention, Obesity, Smoking},
abstract = {Self-regulation is a broad construct representing the general ability to recruit cognitive, motivational and emotional resources to achieve long-term goals. This construct has been implicated in a host of health-risk behaviors, and is a promising target for fostering beneficial behavior change. Despite its clear importance, the behavioral, psychological and neural components of self-regulation remain poorly understood, which contributes to theoretical inconsistencies and hinders maximally effective intervention development. We outline a research program that seeks to define a neuropsychological ontology of self-regulation, articulating the cognitive components that compose self-regulation, their relationships, and their associated measurements. The ontology will be informed by two large-scale approaches to assessing individual differences: first purely behaviorally using data collected via Amazon's Mechanical Turk, then coupled with neuroimaging data collected from a separate population. To validate the ontology and demonstrate its utility, we will then use it to contextualize health risk behaviors in two exemplar behavioral groups: overweight/obese adults who binge eat and smokers. After identifying ontological targets that precipitate maladaptive behavior, we will craft interventions that engage these targets. If successful, this work will provide a structured, holistic account of self-regulation in the form of an explicit ontology, which will better clarify the pattern of deficits related to maladaptive health behavior, and provide direction for more effective behavior change interventions.}
}
@incollection{2018107,
title = {Chapter 4 - Study Designs for Post-Authorization Safety Studies},
editor = {Ayad K. Ali and Abraham G. Hartzema},
booktitle = {Post-Authorization Safety Studies of Medicinal Products},
publisher = {Academic Press},
pages = {107-163},
year = {2018},
isbn = {978-0-12-809217-0},
doi = {https://doi.org/10.1016/B978-0-12-809217-0.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128092170000040}
}
@article{GOTZE2018285,
title = {Insights from recent gravity satellite missions in the density structure of continental margins – With focus on the passive margins of the South Atlantic},
journal = {Gondwana Research},
volume = {53},
pages = {285-308},
year = {2018},
note = {Rifting to Passive Margins},
issn = {1342-937X},
doi = {https://doi.org/10.1016/j.gr.2017.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S1342937X1730206X},
author = {Hans-Jürgen Götze and Roland Pail},
keywords = {Continental margins, Satellite gravity missions, Spatial resolution, Omission error, Interpretation gravity effects, Interpretation gravity gradients},
abstract = {We focus on new gravity and gravity gradient data sets from modern satellite missions GOCE, GRACE and CHAMP, and their geophysical interpretation at passive continental margins of the South Atlantic. Both sides, South Africa and South America, have been targets of hydrocarbon exploration and academic research of the German Priority Program SAMPLE (South Atlantic Margin Processes and Links with onshore Evolution). The achievable spatial resolution, driven by GOCE, is 70–80km. Therefore, most of the geological structures, which cause a significant gravity effect (by both size and density contrast), can be resolved. However, one of the most important aspects is the evaluation of the omission error, which is not always in the focus of interpreters. It results from high-frequency signals of very rough topographic and bathymetric structures, which cannot be resolved by satellite gravimetry due to the exponential signal attenuation with altitude. The omission error is estimated from the difference of the combined gravity model EIGEN-6C4 and the satellite-only model GOCO05S. It can be significantly reduced by topographic reductions. Simple 2D density models and their related mathematical formulas provide insights in the magnitude of the gravity effect of masses that form a passive continental margin. They are contrasted with results from satellite-only and combined gravity models. Example geophysical interpretations are given for the western and eastern margin of the South Atlantic Ocean, where standard deviations vary from 25 to 16mGal and 21–11mGal, respectively. It could be demonstrated, that modern satellite gravity data provide significant added value in the geophysical gravity data processing domain and in the validation of heterogeneous terrestrial data bases. Combined models derived from high-resolution terrestrial gravity and homogeneous satellite data will lead to more detailed and better constrained lithospheric density models, and hence will improve our knowledge about structure, evolution and state of stress in the lithosphere.}
}
@incollection{BERMAN2018263,
title = {Chapter 8 - Precision Data},
editor = {Jules J. Berman},
booktitle = {Precision Medicine and the Reinvention of Human Disease},
publisher = {Academic Press},
pages = {263-326},
year = {2018},
isbn = {978-0-12-814393-3},
doi = {https://doi.org/10.1016/B978-0-12-814393-3.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128143933000081},
author = {Jules J. Berman},
keywords = {Timestamp, Identifiers, Data sharing, Data repurposing, Data reanalysis, Resampling statistics, Metadata},
abstract = {Virtually all of the biomedical data collected today is unusable in its raw form. In this chapter, we will describe modern principles that govern proper data collection and annotation. In the past few decades, an impressive array of sophisticated statistical and mathematical techniques has been developed for the purposes of analyzing large and complex collections of data. Consequently, there is a growing perception that the analysis of data in the Precision Medicine era is something best left to the professionals. We will recommend simple approaches that biologists can use to understand what their own precision data is trying to tell them. Finally, all data analyses must be considered preliminary and tentative until they have been validated, and this often requires data sharing and data reanalysis. This chapter discusses why data sharing is crucial to the advancement of Precision Medicine.}
}
@article{FERNANDES20181341,
title = {“Connecting worlds – a view on microfluidics for a wider application”},
journal = {Biotechnology Advances},
volume = {36},
number = {4},
pages = {1341-1366},
year = {2018},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0734975018300843},
author = {Ana C. Fernandes and Krist V. Gernaey and Ulrich Krühne},
keywords = {Microfluidics, Biotechnology, Plug-and-play, Sensor integration, Platform development guide, Modular microfluidics},
abstract = {From its birth, microfluidics has been referenced as a revolutionary technology and the solution to long standing technological and sociological issues, such as detection of dilute compounds and personalized healthcare. Microfluidics has for example been envisioned as: (1) being capable of miniaturizing industrial production plants, thereby increasing their automation and operational safety at low cost; (2) being able to identify rare diseases by running bioanalytics directly on the patient’s skin; (3) allowing health diagnostics in point-of-care sites through cheap lab-on-a-chip devices. However, the current state of microfluidics, although technologically advanced, has so far failed to reach the originally promised widespread use. In this paper, some of the aspects are identified and discussed that have prevented microfluidics from reaching its full potential, especially in the chemical engineering and biotechnology fields, focusing mainly on the specialization on a single target of most microfluidic devices and offering a perspective on the alternate, multi-use, “plug and play” approach. Increasing the flexibility of microfluidic platforms, by increasing their compatibility with different substrates, reactions and operation conditions, and other microfluidic systems is indeed of surmount importance and current academic and industrial approaches to modular microfluidics are presented. Furthermore, two views on the commercialization of plug-and-play microfluidics systems, leading towards improved acceptance and more widespread use, are introduced. A brief review of the main materials and fabrication strategies used in these fields, is also presented. Finally, a step-wise guide towards the development of microfluidic systems is introduced with special focus on the integration of sensors in microfluidics. The proposed guidelines are then applied for the development of two different example platforms, and to three examples taken from literature. With this work, we aim to provide an interesting perspective on the field of microfluidics when applied to chemical engineering and biotechnology studies, as well as to contribute with potential solutions to some of its current challenges.}
}
@article{MARZAL20183610,
title = {Current challenges and future trends in the field of communication architectures for microgrids},
journal = {Renewable and Sustainable Energy Reviews},
volume = {82},
pages = {3610-3622},
year = {2018},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2017.10.101},
url = {https://www.sciencedirect.com/science/article/pii/S1364032117314703},
author = {Silvia Marzal and Robert Salas and Raúl González-Medina and Gabriel Garcerá and Emilio Figueres},
keywords = {Microgrid, Communication protocols, Multi-agent systems, Peerto-Peer, Distributed architectures},
abstract = {The concept of microgrid has emerged as a feasible answer to cope with the increasing number of distributed renewable energy sources which are being introduced into the electrical grid. The microgrid communication network should guarantee a complete and bidirectional connectivity among the microgrid resources, a high reliability and a feasible interoperability. This is in a contrast to the current electrical grid structure which is characterized by the lack of connectivity, being a centralized-unidirectional system. In this paper a review of the microgrids information and communication technologies (ICT) is shown. In addition, a guideline for the transition from the current communication systems to the future generation of microgrid communications is provided. This paper contains a systematic review of the most suitable communication network topologies, technologies and protocols for smart microgrids. It is concluded that a new generation of peer-to-peer communication systems is required towards a dynamic smart microgrid. Potential future research about communications of the next microgrid generation is also identified.}
}
@article{SCHAGNER201744,
title = {Monitoring recreation across European nature areas: A geo-database of visitor counts, a review of literature and a call for a visitor counting reporting standard},
journal = {Journal of Outdoor Recreation and Tourism},
volume = {18},
pages = {44-55},
year = {2017},
issn = {2213-0780},
doi = {https://doi.org/10.1016/j.jort.2017.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2213078017300099},
author = {Jan Philipp Schägner and Joachim Maes and Luke Brander and Maria-Luisa Paracchini and Volkmar Hartje and Gregoire Dubois},
keywords = {Nature recreation, Visitor monitoring, Visitor counting review, Reporting standard, Visitor statistics, Visitor data sharing},
abstract = {Nature recreation and tourism is a substantial ecosystem service of Europe's countryside that has a substantial economic value and contributes considerably to income and employment of local communities. Highlighting the recreational value and economic contribution of nature areas can be used as a strong argument for the funding of protected and recreational areas. The total number of recreational visits of a nature area has been recognised as a major determinant of its economic recreational value and its contribution to local economies. This paper presents an international geo-database on recreational visitor numbers to non-urban ecosystems, containing 1267 observations at 518 separate case study areas throughout Europe. The monitored sites are described by their centroid coordinates and shape files displaying the exact extension of the sites. Therefore, the database illustrates the spatial distribution of visitor counting throughout Europe and can be used for secondary research, such as for validation of spatially explicit recreational ecosystem service models and for identifying relevant drivers of recreational ecosystem services. To develop the database, we review visitor monitoring literature throughout Europe and give an overview of such activities with special attention to visitor counting. We identify one major shortcoming in the available literature, which relates to the presentation, study area definition and methodological reporting of conducted visitor counting studies. Insufficient reporting hampers the identification of the study area, the comparability of different studies and the evaluation of the studies' quality. Based on our findings, we propose a standardised reporting template for visitor counting studies and advanced data sharing for recreational visitor data. Researchers and institutions are invited to report on their visitor counting studies via our web interface at rris.biopama.org/visitor-reporting and thereby contribute to a global visitor database that will be shared via the ESP Visualisation tool.
Management implications
The total annual visitor number is the most important variable for defining the relative importance and the economic recreational value of different recreational areas. Due to the importance of visitor counting and its increased attention in the scientific literature we: •present a geo-database on recreational visitor statistics for nature areas, which allows identifying sites for which visitor statistics exist and which can be used for secondary research•review current practice in recreational visitor counting across nature areas in Europe and give guidance for future applications,•identify shortcomings in the methodological reporting of recent visitor monitoring and counting studies and•present and recommend reporting standard for all future visitor counting studies in order to improve their comparability and to allow assessing their quality.•The reporting standard is translated into a web interface for visitor data collection, which allows for data sharing via a global map-browser.}
}
@incollection{SHEMSHADI201737,
title = {Chapter 2 - The Anatomy of An Intent Based Search and Crawler Engine for the Web of Things},
editor = {Quan Z. Sheng and Yongrui Qin and Lina Yao and Boualem Benatallah},
booktitle = {Managing the Web of Things},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {37-72},
year = {2017},
isbn = {978-0-12-809764-9},
doi = {https://doi.org/10.1016/B978-0-12-809764-9.00003-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097649000032},
author = {Ali Shemshadi and Quan Z. Sheng and Yongrui Qin},
keywords = {Web of Things, Internet of Things, Big Data, Search engine, Flight delay},
abstract = {Web of Things (WoT) is becoming increasingly interesting for researchers and professionals over the past few years. It provides numerous opportunities by disseminating the data that is generated by physical things and fills the gap between the physical and the virtual world. Despite its importance, WoT search has not been studied enough in the past. Given the dynamic challenge of the WoT, collecting data from WoT resources is not well developed. Furthermore, the effectiveness of WoT search can be significantly improved if the users' intention of the search is also considered. This can be facilitated by knowing the existing status of the WoT in real-world. In this chapter, we address multiple challenges in this area. Firstly, we depict the analytical structure of the future WoT which facilitate crawling, indexing and searching the data from physical things. Secondly, we show how we can identify WoT and extract the data from it. Thirdly, we use our crawler to crawl and analyse WoT data on the Internet. Furthermore, we provide a showcase in the analysis of the flights delay data. Finally, we provide a discussion on future research in this area.}
}
@article{TALAEIKHOEI201822,
title = {Identifying people at risk of developing type 2 diabetes: A comparison of predictive analytics techniques and predictor variables},
journal = {International Journal of Medical Informatics},
volume = {119},
pages = {22-38},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S138650561830399X},
author = {Amir Talaei-Khoei and James M. Wilson},
keywords = {Classification Algorithms, Machine Learning, Diabetes},
abstract = {Background
The present study aims to identify the patients at risk of type 2 diabetes (T2D). There is a body of literature that uses machine learning classification algorithms to predict development of T2D among patients. The current study compares the performance of these classification algorithms to identify patients who are at risk of developing T2D in short, medium and long terms. In addition, the list of predictor variables important for prediction for T2D progression is provided.
Methods
This study uses 10,911 records generated in 36 clinics from the 15th of November 2008–15th of November 2016. Syntactic minority oversampling and random under sampling were used to create a balanced dataset. The performance of Neural Networks, Support Vector Machines, Decision Tress and Logistic Regression to identify patients developing T2D in short, medium and long terms was compared. The measures were Area Under Curve, Sensitivity, Specificity, Matthew correlation coefficient and Mean Calibration Error. Through importance analysis and information fusion techniques the predictors of developing T2D were identified for short, medium and long-term risk analysis.
Results
The findings show that the performance of analytics techniques depends on both period and purpose of prediction whether the prediction is to identify people who will not develop T2D or to determine at risk patients. Oversampling as opposed to under sampling improved performance. 16 predictors and their importance to determine patients at risk of T2D in short, medium and long terms were identified.
Conclusions
This study provides guidelines for an automated system to prompt patients for screening. Several predictors are reportable by patients, others can be examined by physicians or ordered for further lab examination, which offers a potential reduction of the burden placed upon the clinical settings.}
}
@article{ANAGNOSTOPOULOS20187,
title = {Fintech and regtech: Impact on regulators and banks},
journal = {Journal of Economics and Business},
volume = {100},
pages = {7-25},
year = {2018},
note = {FinTech – Impact on Consumers, Banking and Regulatory Policy},
issn = {0148-6195},
doi = {https://doi.org/10.1016/j.jeconbus.2018.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S014861951730142X},
author = {Ioannis Anagnostopoulos},
keywords = {FinTech, RegTech, Business models, Regulation, Financial services, Future research direction},
abstract = {The purpose of this paper is to develop an insight and review the effect of FinTech development against the broader environment in financial technology. We further aim to offer various perspectives in order to aid the understanding of the disruptive potential of FinTech, and its implications for the wider financial ecosystem. By drawing upon very recent and highly topical research on this area this study examines the implications for financial institutions, and regulation especially when technology poses a challenge to the global banking and regulatory system. It is driven by a wide-ranging overview of the development, the current state, and possible future of fintech. This paper attempts to connect practitioner-led and academic research. While it draws on academic research, the perspective it takes is also practice-oriented. It relies on the current academic literature as well as insights from industry sources, action research and other publicly available commentaries. It also draws on professional practitioners’ roundtable discussions, and think-tanks in which the author has been an active participant. We attempt to interpret banking, and regulatory issues from a behavioural perspective. The last crisis exposed significant failures in regulation and supervision. It has made the Financial Market Law and Compliance a key topic on the current agenda. Disruptive technological change also seems to be important in investigating regulatory compliance followed by change. We contribute to the current literature review on financial and digital innovation by new entrants where this has also practical implications. We also provide for an updated review of the current regulatory issues addressing the contextual root causes of disruption within the financial services domain. The aim here is to assist market participants to improve effectiveness and collaboration. The difficulties arising from extensive regulation may suggest a more liberal and principled approach to financial regulation. Disruptive innovation has the potential for welfare outcomes for consumers, regulatory, and supervisory gains as well as reputational gains for the financial services industry. It becomes even more important as the financial services industry evolves. For example, the preparedness of the regulators to instil culture change and harmonise technological advancements with regulation could likely achieve many desired outcomes. Such results range from achieving an orderly market growth, further aiding systemic stability and restoring trust and confidence in the financial system. Our action-led research results have implications for both research and practice. These should be of interest to regulatory standard setters, investors, international organisations and other academics who are researching regulatory and competition issues, and their manifestation within the financial and social contexts. As a perspective on a social construct, this study appeals to regulators and law makers, entrepreneurs, and investors who participate in technology applied within the innovative financial services domain. It is also of interest to bankers who might consider FinTech and strategic partnerships as a prospective, future strategic direction.11We thank two anonymous referees for their very thoughtful and constructing comments who took a keen interest in reviewing our research and in this way contributed materially to the development of the paper. All other omissions and/or errors remain our own.}
}
@incollection{PETERS2018271,
title = {Chapter 12 - Blockchain Architectures for Electronic Exchange Reporting Requirements: EMIR, Dodd Frank, MiFID I/II, MiFIR, REMIT, Reg NMS and T2S},
editor = {David {Lee Kuo Chuen} and Robert Deng},
booktitle = {Handbook of Blockchain, Digital Finance, and Inclusion, Volume 2},
publisher = {Academic Press},
pages = {271-329},
year = {2018},
isbn = {978-0-12-812282-2},
doi = {https://doi.org/10.1016/B978-0-12-812282-2.00012-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128122822000127},
author = {Gareth W. Peters and Guy R. Vishnia},
keywords = {Blockchain, Blockchain transaction reporting, Dodd–Frank, EMIR, Exchange Regulation, MiFID I, MiFID II and MiFIR, REMIT, Reg NMS, T2S and CSD, Trade reporting, Transparency},
abstract = {Several international electronic primary financial exchanges have begun to announce they will explore the adoption of blockchain technology in their trade processing and reporting for execution and clearing. Therefore, in this work we will begin by providing a detailed discussion and overview of the new exchange regulations appearing in different jurisdictions around the world, including EMIR, Dodd–Frank, MiFID I/II, MiFIR, REMIT, Reg NMS and T2S. We will discuss their key features, specifically in regard to transparency reporting and trade/transaction reporting requirements. To achieve this we first discuss the emergence of a multitude of different trading venues that have arisen under the fragmentation directives for each asset class: equities, commodities, derivatives and currency. We highlight how each fits into this universe of different market and processing venues both primary, secondary, dark and lit and OTC.}
}
@article{ROMANOU201899,
title = {The necessity of the implementation of Privacy by Design in sectors where data protection concerns arise},
journal = {Computer Law & Security Review},
volume = {34},
number = {1},
pages = {99-110},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917302054},
author = {Anna Romanou},
keywords = {Privacy by Design, Privacy, Practical implementation of Privacy by Design, Biometrics, e-health, Video-surveillance},
abstract = {This article examines the extent to which Privacy by Design can safeguard privacy and personal data within a rapidly evolving society. This paper will first briefly explain the theoretical concept and the general principles of Privacy by Design, as laid down in the General Data Protection Regulation. Then, by indicating specific examples of the implementation of the Privacy by Design approach, it will be demonstrated why the implementation of Privacy by Design is a necessity in a number of sectors where specific data protection concerns arise (biometrics, e-health and video-surveillance) and how it can be implemented.}
}
@article{HARMS2018972,
title = {Extending the Human Connectome Project across ages: Imaging protocols for the Lifespan Development and Aging projects},
journal = {NeuroImage},
volume = {183},
pages = {972-984},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.09.060},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918318652},
author = {Michael P. Harms and Leah H. Somerville and Beau M. Ances and Jesper Andersson and Deanna M. Barch and Matteo Bastiani and Susan Y. Bookheimer and Timothy B. Brown and Randy L. Buckner and Gregory C. Burgess and Timothy S. Coalson and Michael A. Chappell and Mirella Dapretto and Gwenaëlle Douaud and Bruce Fischl and Matthew F. Glasser and Douglas N. Greve and Cynthia Hodge and Keith W. Jamison and Saad Jbabdi and Sridhar Kandala and Xiufeng Li and Ross W. Mair and Silvia Mangia and Daniel Marcus and Daniele Mascali and Steen Moeller and Thomas E. Nichols and Emma C. Robinson and David H. Salat and Stephen M. Smith and Stamatios N. Sotiropoulos and Melissa Terpstra and Kathleen M. Thomas and M. Dylan Tisdall and Kamil Ugurbil and Andre {van der Kouwe} and Roger P. Woods and Lilla Zöllei and David C. {Van Essen} and Essa Yacoub},
keywords = {Connectomics, Resting-state, Functional connectivity, Task, Diffusion, Perfusion, Development, Aging, Lifespan},
abstract = {The Human Connectome Projects in Development (HCP-D) and Aging (HCP-A) are two large-scale brain imaging studies that will extend the recently completed HCP Young-Adult (HCP-YA) project to nearly the full lifespan, collecting structural, resting-state fMRI, task-fMRI, diffusion, and perfusion MRI in participants from 5 to 100+ years of age. HCP-D is enrolling 1300+ healthy children, adolescents, and young adults (ages 5–21), and HCP-A is enrolling 1200+ healthy adults (ages 36–100+), with each study collecting longitudinal data in a subset of individuals at particular age ranges. The imaging protocols of the HCP-D and HCP-A studies are very similar, differing primarily in the selection of different task-fMRI paradigms. We strove to harmonize the imaging protocol to the greatest extent feasible with the completed HCP-YA (1200+ participants, aged 22–35), but some imaging-related changes were motivated or necessitated by hardware changes, the need to reduce the total amount of scanning per participant, and/or the additional challenges of working with young and elderly populations. Here, we provide an overview of the common HCP-D/A imaging protocol including data and rationales for protocol decisions and changes relative to HCP-YA. The result will be a large, rich, multi-modal, and freely available set of consistently acquired data for use by the scientific community to investigate and define normative developmental and aging related changes in the healthy human brain.}
}
@incollection{WITTEN20173,
title = {Chapter 1 - What’s it all about?},
editor = {Ian H. Witten and Eibe Frank and Mark A. Hall and Christopher J. Pal},
booktitle = {Data Mining (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {3-41},
year = {2017},
isbn = {978-0-12-804291-5},
doi = {https://doi.org/10.1016/B978-0-12-804291-5.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042915000015},
author = {Ian H. Witten and Eibe Frank and Mark A. Hall and Christopher J. Pal},
keywords = {Machine learning, data mining, classification, numeric prediction, data mining applications, machine learning and statistics, generalization as search, data mining and ethics},
abstract = {This book is about machine learning techniques for data mining. We start by explaining what people mean by data mining and machine learning, and give some simple example machine learning problems, including both classification and numeric prediction tasks, to illustrate the kinds of input and output involved. To demonstrate the wide applicability of machine learning in practical applications, we continue with brief reviews of several fielded applications from a diverse set of areas. Machine learning is closely linked to the study of statistics, and we discuss the connections between these two fields next. It is also strongly related to the field of artificial intelligence, particularly search techniques, and we devote a section to investigating this relationship. We conclude the chapter by discussing the ethical issues involved when applying data mining in practice.}
}
@article{GAO2017561,
title = {Automated discovery and integration of semantic urban data streams: The ACEIS middleware},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {561-581},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1730331X},
author = {Feng Gao and Muhammad Intizar Ali and Edward Curry and Alessandra Mileo},
keywords = {Semantic Web, Complex events, Service computing, RDF Stream Processing},
abstract = {With the growing popularity of Internet of Things (IoT) technologies and sensors deployment, more and more cities are leaning towards smart cities solutions that can leverage this rich source of streaming data to gather knowledge that can be used to solve domain-specific problems. A key challenge that needs to be faced in this respect is the ability to automatically discover and integrate heterogeneous sensor data streams on the fly for applications to use them. To provide a domain-independent platform and take full benefits from semantic technologies, in this paper we present an Automated Complex Event Implementation System (ACEIS), which serves as a middleware between sensor data streams and smart city applications. ACEIS not only automatically discovers and composes IoT streams in urban infrastructures for users’ requirements expressed as complex event requests, but also automatically generates stream queries in order to detect the requested complex events, bridging the gap between high-level application users and low-level information sources. We also demonstrate the use of ACEIS in a smart travel planner scenario using real-world sensor devices and datasets.}
}
@article{KINAWY2017239,
title = {Mismatches in stakeholder communication: The case of the Leslie and Ferrand transit stations, Toronto, Canada},
journal = {Sustainable Cities and Society},
volume = {34},
pages = {239-249},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717302445},
author = {S.N. Kinawy and M. {Nik Bakht} and T.E. El-Diraby},
keywords = {Community engagement, Urban infrastructure planning, Information retrieval, Content analysis, Co-creation and collaborative planning},
abstract = {Using content analysis, an approach is presented to help extract topics of interest to local community during project planning. This is helpful for fine-tuning and customizing the language used in communication with the public. Hopefully, reducing communication mismatches can help support constructive dialogue that is not “lost in translation”. The extraction of community issues/interest is becoming important also to help guide the development of plans/projects and their features in a manner that meets their needs. The two cases used in this study presented a suitable target for developing and showcasing the proposed approach. There was a reversal of public decision based on community debates/objections. This allowed us to study the mismatches before and after the decision. The proposed approach used a context-based taxonomy of terms and content analysis to compare terms/topics contained in a related twitter account, relevant news articles, and documents/presentations used in public meetings—before and after the decision. The proposed approach was designed to be mostly automatic to help future re-use. Of course, the use of such approach is only one step in a much bigger qualitative and context-specific process. Specific to the two cases, it was observed that news media articles and the contents of twitter chats had higher matching levels in the topics/themes they covered. Contents of public meetings had some levels of mismatching. Particular to the two cases, public official tended to emphasize the technical aspects of the projects with limited/clear analysis of their functions or impacts on community. It is argued that, as a result, public officials should study twitter chats and news articles as they prepare official public documents and presentations to citizens; attempt to specifically address prevalent issues in them; and even use the same nomenclature. Using a (semi) automated tool can be very helpful in this regard.}
}
@article{CHEN2017673,
title = {Multi-scaling allometric analysis for urban and regional development},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {465},
pages = {673-689},
year = {2017},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2016.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378437116305246},
author = {Yanguang Chen},
keywords = {Allometric growth, Allometric scaling, Fractal dimension, Complex spatial system, Spatio-temporal evolution, Urbanization},
abstract = {The concept of allometric growth is based on scaling relations, and it has been applied to urban and regional analysis for a long time. However, most allometric analyses were devoted to the single proportional relation between two elements of a geographical system. Few researches focus on the allometric scaling of multielements. In this paper, a process of multiscaling allometric analysis is developed for the studies on spatio-temporal evolution of complex systems. By means of linear algebra, general system theory, and by analogy with the analytical hierarchy process, the concepts of allometric growth can be integrated with the ideas from fractal dimension. Thus a new methodology of geo-spatial analysis and the related theoretical models emerge. Based on the least squares regression and matrix operations, a simple algorithm is proposed to solve the multiscaling allometric equation. Applying the analytical method of multielement allometry to Chinese cities and regions yields satisfying results. A conclusion is reached that the multiscaling allometric analysis can be employed to make a comprehensive evaluation for the relative levels of urban and regional development, and explain spatial heterogeneity. The notion of multiscaling allometry may enrich the current theory and methodology of spatial analyses of urban and regional evolution.}
}
@incollection{KORRES201851,
title = {2.04 - GIS for Hydrology},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {51-80},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.09635-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489096354},
author = {Wolfgang Korres and Karl Schneider},
keywords = {Decision support systems, GIS, Hydrological data, Hydrological models, Hydrology, Spatial interpolation, Terrain analysis, Water resources, Watershed delineation},
abstract = {This article aims at providing an overview of GIS applications for hydrology. Hydrology is inherently a spatiotemporal science. Thus, a vast and ever-growing number of applications of GIS in hydrology exists. This chapter therefore shall serve as a point of reference rather than a comprehensive account. While GIS has strongly impacted hydrological model development, data analysis and communication of issues of hydrology, hydrological applications have on the other hand evolved into standard GIS applications. This article provides an overview about some of the key applications, procedures and tasks. Data structures used in hydrology, data sources, and uncertainty in hydrological data are discussed. Standard GIS methods for hydrology are presented, including terrain analysis for flow direction, watershed delineation and drainage network processing or spatial interpolation of point data. Frequently used methods to infer hydrological parameters from generally available data sets are discussed using the example of the NRCS curve number approach and pedotransfer functions (PTFs). Numerical modeling has a long tradition in hydrology. Different integrating approaches of GIS and hydrological models exist. These are presented, along with examples of GIS applications for hydrology by international, national and local entities, including spatial data infrastructure, data portals and data services. Water resources management requires careful consideration of the interests and needs of the different stakeholders, as well as understanding of the spatial and temporal dynamics of water. Spatial decision support systems which integrate natural as well as societal processes are essential to understand water use potentials and limits. Some key elements of these decision support systems are presented and discussed, particularly against the background of the challenges of Global Change. Water is both an essence of life as well as a risk. Thus hydrological information is essential to experts and laymen alike. Approaches for citizen participation and citizen science are presented using web and smartphone applications. This article concludes with some remarks on future prospects for GIS for hydrology.}
}
@incollection{KANTERAKIS201815,
title = {Chapter 2 - Creating Transparent and Reproducible Pipelines: Best Practices for Tools, Data, and Workflow Management Systems},
editor = {Christophe G. Lambert and Darrol J. Baker and George P. Patrinos},
booktitle = {Human Genome Informatics},
publisher = {Academic Press},
pages = {15-43},
year = {2018},
isbn = {978-0-12-809414-3},
doi = {https://doi.org/10.1016/B978-0-12-809414-3.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094143000024},
author = {Alexandros Kanterakis and George Potamias and Morris A. Swertz and George P. Patrinos},
keywords = {Workflows, Bioinformatics, Open science},
abstract = {Recently, the practice of properly sharing the source code, analysis pipelines, and protocols of published studies has become commonplace in bioinformatics. In addition, there is a plethora of technically mature workflow management systems (WMS) that offer simple and user-friendly environments where users can submit tools and build transparent, shareable, and reproducible pipelines. Arguably, the adoption of open science policies and the availability of efficient WMSs constitute major progress toward battling the replication crisis, advancing research dissemination, and creating new collaborations. Yet now we still see that it is very difficult to include a large range of tools in a scientific pipeline, whereas on the other side, certain technical and design choices of modern WMSs discourage users from doing just this. Here we present three sets of easily applicable “best practices” targeting (i) bioinformatics tool developers, (ii) data curators, and (iii) WMS engineers, respectively. These practices aim to make it easier to add tools to a pipeline, to make it easier to directly process data, and to make WMSs widely hospitable for any external tool or pipeline. We also show how following these guidelines can directly benefit the research community.}
}
@article{WASESA201737,
title = {The seaport service rate prediction system: Using drayage truck trajectory data to predict seaport service rates},
journal = {Decision Support Systems},
volume = {95},
pages = {37-48},
year = {2017},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616302032},
author = {Meditya Wasesa and Andries Stam and Eric {van Heck}},
keywords = {Predictive analytics, Gradient boosting model, Trajectory data, Vehicle telematics system, Seaport appointment system, Drayage operation},
abstract = {For drayage operators the service rate of seaports is crucial for organizing their container pick-up/delivery operations. This study presents a seaport service rate prediction system that could help drayage operators to improve their predictions of the duration of the pick-up/delivery operations at a seaport by using the subordinate trucks' trajectory data. The system is constructed based on three components namely, trajectory reconstruction, geo-fencing analysis, and gradient boosting modelling. Using predictive analytic techniques, the prediction system is trained and validated using more than 15million data records from over 200 trucks over a period of 19months. The gradient boosting model-based solution provides better predictions compared with the linear model benchmark solution. Conclusions and implications are formulated.}
}
@article{SLIKKER2018115,
title = {Emerging technologies for food and drug safety},
journal = {Regulatory Toxicology and Pharmacology},
volume = {98},
pages = {115-128},
year = {2018},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2018.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0273230018301971},
author = {William Slikker and Thalita Antony {de Souza Lima} and Davide Archella and Jarbas Barbosa {de Silva} and Tara Barton-Maclaren and Li Bo and Danitza Buvinich and Qasim Chaudhry and Peiying Chuan and Hubert Deluyker and Gary Domselaar and Meiruze Freitas and Barry Hardy and Hans-Georg Eichler and Marta Hugas and Kenneth Lee and Chia-Ding Liao and Lit-Hsin Loo and Haruhiro Okuda and Orish Ebere Orisakwe and Anil Patri and Carl Sactitono and Leming Shi and Primal Silva and Frank Sistare and Shraddha Thakkar and Weida Tong and Mary Lou Valdez and Maurice Whelan and Anna Zhao-Wong},
keywords = {Global coalition, Emerging technologies, Food safety, Drug safety, Regulatory agencies, Alternative methods, Best practices, Cross-training},
abstract = {Emerging technologies are playing a major role in the generation of new approaches to assess the safety of both foods and drugs. However, the integration of emerging technologies in the regulatory decision-making process requires rigorous assessment and consensus amongst international partners and research communities. To that end, the Global Coalition for Regulatory Science Research (GCRSR) in partnership with the Brazilian Health Surveillance Agency (ANVISA) hosted the seventh Global Summit on Regulatory Science (GSRS17) in Brasilia, Brazil on September 18–20, 2017 to discuss the role of new approaches in regulatory science with a specific emphasis on applications in food and medical product safety. The global regulatory landscape concerning the application of new technologies was assessed in several countries worldwide. Challenges and issues were discussed in the context of developing an international consensus for objective criteria in the development, application and review of emerging technologies. The need for advanced approaches to allow for faster, less expensive and more predictive methodologies was elaborated. In addition, the strengths and weaknesses of each new approach was discussed. And finally, the need for standards and reproducible approaches was reviewed to enhance the application of the emerging technologies to improve food and drug safety. The overarching goal of GSRS17 was to provide a venue where regulators and researchers meet to develop collaborations addressing the most pressing scientific challenges and facilitate the adoption of novel technical innovations to advance the field of regulatory science.}
}
@article{JIANG201873,
title = {An integrated machine learning framework for hospital readmission prediction},
journal = {Knowledge-Based Systems},
volume = {146},
pages = {73-90},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118300443},
author = {Shancheng Jiang and Kwai-Sang Chin and Gang Qu and Kwok L. Tsui},
keywords = {Hospital readmission, Mutual information, Multi-objective optimization, Bare-bones particle swarm optimization, Feature selection, Greedy local search},
abstract = {Unplanned readmission (re-hospitalization) is the main source of cost for healthcare systems and is normally considered as an indicator of healthcare quality and hospital performance. Poor understanding of the relative importance of predictors and limited capacity of traditional statistical models challenge the development of accurate predictive models for readmission. This study aims to develop a robust and accurate risk prediction framework for hospital readmission, by combining feature selection algorithms and machine learning models. With regard to feature selection, an enhanced version of multi-objective bare-bones particle swarm optimization (EMOBPSO) is developed as the principal search strategy, and a new mutual information-based criterion is proposed to efficiently estimate feature relevancy and redundancy. A greedy local search strategy (GLS) is developed and merged into EMOBPSO to control the final feature subset size as desired. For the modeling process, manifold machine learning models, such as support vector machine, random forest, and deep neural network, are trained with preprocessed datasets and corresponding feature subsets. In the case study, the proposed methodology is applied to an actual hospital located in Northeast China, with various levels of data collected from the hospital information system. Results obtained from comparative experiments demonstrate the effectiveness of EMOBPSO and EMOBPSO-GLS feature selection algorithms. The combination of EMOBPSO (EMOBPSO-GLS) and deep neural network possesses robust predictive power among different datasets. Furthermore, insightful implications are abstracted from the obtained elite features and can be used by practitioners to determine the vulnerable patients for readmission and target the delivery of early resource-intensive interventions.}
}
@incollection{LOHR201825,
title = {Chapter 3 - Genomic Approaches to Hematology},
editor = {Ronald Hoffman and Edward J. Benz and Leslie E. Silberstein and Helen E. Heslop and Jeffrey I. Weitz and John Anastasi and Mohamed E. Salama and Syed Ali Abutalib},
booktitle = {Hematology (Seventh Edition)},
publisher = {Elsevier},
edition = {Seventh Edition},
pages = {25-36},
year = {2018},
isbn = {978-0-323-35762-3},
doi = {https://doi.org/10.1016/B978-0-323-35762-3.00003-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323357623000032},
author = {Jens G. Lohr and Birgit Knoechel and Todd R. Golub},
keywords = {Next generation DNA- and RNA- sequencing, Genomics, Epigenetics/Epigenomics, Genome editing, High throughput profiling, Unsupervised computational analysis}
}
@article{REN20181,
title = {Structure-oriented prediction in complex networks},
journal = {Physics Reports},
volume = {750},
pages = {1-51},
year = {2018},
note = {Structure-oriented prediction in complex networks},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0370157318301339},
author = {Zhuo-Ming Ren and An Zeng and Yi-Cheng Zhang},
keywords = {Complex networks, Prediction, Network structure, Network dynamics},
abstract = {Complex systems are extremely hard to predict due to its highly nonlinear interactions and rich emergent properties. Thanks to the rapid development of network science, our understanding of the structure of real complex systems and the dynamics on them has been remarkably deepened, which meanwhile largely stimulates the growth of effective prediction approaches on these systems. In this article, we aim to review different network-related prediction problems, summarize and classify relevant prediction methods, analyze their advantages and disadvantages, and point out the forefront as well as critical challenges of the field.}
}
@article{PLAKIDAS2017119,
title = {Evolution of the R software ecosystem: Metrics, relationships, and their impact on qualities},
journal = {Journal of Systems and Software},
volume = {132},
pages = {119-146},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.06.095},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217301371},
author = {Konstantinos Plakidas and Daniel Schall and Uwe Zdun},
keywords = {R, Software ecosystems, Evolution, Quantitative analysis, Empirical study},
abstract = {Software ecosystems are an important new concept for collaborative software development, and empirical studies on their development are important towards understanding the underlying dynamics and modelling their behaviour. We conducted an explorative analysis of the R ecosystem as an exemplar on high-level, ecosystem-wide assessment. Based principally on the documentation metadata of the R packages, we generated a variety of metrics that allow the quantification of the R ecosystem. We also categorized the ecosystem participants, both in the software marketplace and in the developer community, by characteristics that measure their activity and impact. By viewing our metrics across the ecosystem’s lifecycle for the various participant categories, we discovered interrelationships between them and determined the contribution of each category to the ecosystem as a whole.}
}
@article{OLIVARESOLIVARES2018628,
title = {Technological innovation within the Spanish tax administration and data subjects' right to access: An opportunity knocks},
journal = {Computer Law & Security Review},
volume = {34},
number = {3},
pages = {628-639},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917303825},
author = {Bernardo D. {Olivares Olivares}},
keywords = {Right to access, Personal information, Tax related data, Spanish tax administration, GDPR},
abstract = {In this paper, we analyse the data subjects' right to access their personal data in the context of the Spanish Tax Administration and the legal consequences of the upcoming General Data Protection Regulation. The results show that there are still difficulties related to the scope of this right, the establishment of proper storage criteria, and in the procedures used by the data controllers to provide accurate information to the data subjects. This situation highlights the necessity to incorporate such technological innovation as metadata labelling and automatic computerised procedures to ensure an optimum management of the data subjects' access to their tax related personal information.}
}
@article{SADEGHIAN201894,
title = {Robust probabilistic principal component analysis based process modeling: Dealing with simultaneous contamination of both input and output data},
journal = {Journal of Process Control},
volume = {67},
pages = {94-111},
year = {2018},
note = {Big Data: Data Science for Process Control and Operations},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2017.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0959152417300768},
author = {A. Sadeghian and O. Wu and B. Huang},
keywords = {Robust predictive models, Input–output outliers, Gaussian location mixture distribution, Probabilistic principal component analysis (PPCA), Expectation Maximization (EM) algorithm, Robustness, Soft sensors},
abstract = {In this work, one of the common issues, the robustness of the soft sensors, in development of such predictive models is discussed and the solution is provided. Large random errors, also known as outliers are one inseparable characteristic of data sets which can be caused by various reasons. Robust probabilistic predictive models overcome this problem by appropriate formulation of noise distributions. In this work possible outliers are considered for both input and output data in contrast to the traditional robust algorithms that have focused on output outliers only. Probabilistic principal component analysis based regression is used for the predictive model in this work and Expectation Maximization algorithm is applied to solve a complex robust estimation problem. Finally the performance of the developed robust predictive model is evaluated by simulated and industrial case studies. This work is a generalization to the traditional robust probabilistic principal component analysis based regression modeling work which considered a different type of outliers that occur in the output only.}
}
@article{BISCHOF201822,
title = {Enriching integrated statistical open city data by combining equational knowledge and missing value imputation},
journal = {Journal of Web Semantics},
volume = {48},
pages = {22-47},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2017.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1570826817300355},
author = {Stefan Bischof and Andreas Harth and Benedikt Kämpgen and Axel Polleres and Patrik Schneider},
keywords = {Open data, Linked Data, Data cleaning, Data integration},
abstract = {Several institutions collect statistical data about cities, regions, and countries for various purposes. Yet, while access to high quality and recent such data is both crucial for decision makers and a means for achieving transparency to the public, all too often such collections of data remain isolated and not re-useable, let alone comparable or properly integrated. In this paper we present the Open City Data Pipeline, a focused attempt to collect, integrate, and enrich statistical data collected at city level worldwide, and re-publish the resulting dataset in a re-useable manner as Linked Data. The main features of the Open City Data Pipeline are: (i) we integrate and cleanse data from several sources in a modular and extensible, always up-to-date fashion; (ii) we use both Machine Learning techniques and reasoning over equational background knowledge to enrich the data by imputing missing values, (iii) we assess the estimated accuracy of such imputations per indicator. Additionally, (iv) we make the integrated and enriched data, including links to external data sources, such as DBpedia, available both in a web browser interface and as machine-readable Linked Data, using standard vocabularies such as QB and PROV. Apart from providing a contribution to the growing collection of data available as Linked Data, our enrichment process for missing values also contributes a novel methodology for combining rule-based inference about equational knowledge with inferences obtained from statistical Machine Learning approaches. While most existing works about inference in Linked Data have focused on ontological reasoning in RDFS and OWL, we believe that these complementary methods and particularly their combination could be fruitfully applied also in many other domains for integrating Statistical Linked Data, independent from our concrete use case of integrating city data.}
}
@article{ATKINSON2017216,
title = {Scientific workflows: Past, present and future},
journal = {Future Generation Computer Systems},
volume = {75},
pages = {216-227},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.05.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17311202},
author = {Malcolm Atkinson and Sandra Gesing and Johan Montagnat and Ian Taylor},
keywords = {Scientific workflows, Scientific methods, Optimisation, Performance, Usability},
abstract = {This special issue and our editorial celebrate 10 years of progress with data-intensive or scientific workflows. There have been very substantial advances in the representation of workflows and in the engineering of workflow management systems (WMS). The creation and refinement stages are now well supported, with a significant improvement in usability. Improved abstraction supports cross-fertilisation between different workflow communities and consistent interpretation as WMS evolve. Through such re-engineering the WMS deliver much improved performance, significantly increased scale and sophisticated reliability mechanisms. Further improvement is anticipated from substantial advances in optimisation. We invited papers from those who have delivered these advances and selected 14 to represent today’s achievements and representative plans for future progress. This editorial introduces those contributions with an overview and categorisation of the papers. Furthermore, it elucidates responses from a survey of major workflow systems, which provides evidence of substantial progress and a structured index of related papers. We conclude with suggestions on areas where further research and development is needed and offer a vision of future research directions.}
}
@article{LEE201729,
title = {Feature selection in multimedia: The state-of-the-art review},
journal = {Image and Vision Computing},
volume = {67},
pages = {29-42},
year = {2017},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0262885617301518},
author = {Pui Yi Lee and Wei Ping Loh and Jeng Feng Chin},
keywords = {Feature selection, Multimedia, Data mining, Search strategies},
abstract = {Multimedia data mining, particularly feature selection (FS), has been successfully applied in recent classification and recognition works. However, only a few studies in the contemporary literature have reviewed FS (e.g., analyses of data pre-processing prior to classification and clustering). This study aimed to fill this research gap by presenting an extensive survey on the current development of FS in multimedia. A total of 70 related papers published from 2001 to 2017 were collected from multiple databases. Breakdowns and analyses were performed on data types, methods, search strategies, performance measures, and challenges. The development trend of FS presages the increased prominence of heuristic search strategies and hybrid FS in the latest multimedia data mining.}
}
@article{ALVAREZ201860,
title = {Discovering role interaction models in the Emergency Room using Process Mining},
journal = {Journal of Biomedical Informatics},
volume = {78},
pages = {60-77},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S153204641730285X},
author = {Camilo Alvarez and Eric Rojas and Michael Arias and Jorge Munoz-Gama and Marcos Sepúlveda and Valeria Herskovic and Daniel Capurro},
keywords = {Healthcare, Processes, Process mining, Case studies, Organizational mining, Organizational team patterns},
abstract = {Objectives
A coordinated collaboration among different healthcare professionals in Emergency Room (ER) processes is critical to promptly care for patients who arrive at the hospital in a delicate health condition, claiming for an immediate attention. The aims of this study are (i) to discover role interaction models in (ER) processes using process mining techniques; (ii) to understand how healthcare professionals are currently collaborating; and (iii) to provide useful knowledge that can help to improve ER processes.
Methods
A four step method based on process mining techniques is proposed. An ER process of a university hospital was considered as a case study, using 7160 episodes that contains specific ER episode attributes.
Results
Insights about how healthcare professionals collaborate in the ER was discovered, including the identification of a prevalent role interaction model along the major triage categories and specific role interaction models for different diagnoses. Also, common and exceptional professional interaction models were discovered at the role level.
Conclusions
This study allows the discovery of role interaction models through the use of real-life clinical data and process mining techniques. Results show a useful way of providing relevant insights about how healthcare professionals collaborate, uncovering opportunities for process improvement.}
}
@article{SANCHEZTRIGUEROS201713,
title = {Assessing the effects of temporal ambivalence on defining palaeosystem interrelations, and applicability to the analysis of archaeological survey data},
journal = {Quaternary International},
volume = {435},
pages = {13-34},
year = {2017},
note = {Staring at the ground: Archaeological surveys as a research tool},
issn = {1040-6182},
doi = {https://doi.org/10.1016/j.quaint.2015.09.089},
url = {https://www.sciencedirect.com/science/article/pii/S1040618215011349},
author = {Fernando Sánchez-Trigueros and Alfonso Benito-Calvo and Marta Navazo and Antoni Canals},
keywords = {Uncertainty, Epistemic ambivalence, Archaeological survey data, Data mining, Middle Palaeolithic, Sierra de Atapuerca},
abstract = {We present a possibility-based method with applications to the analysis of archaeological survey data under constraints of temporal ambivalence. The method is a generalizable heuristic for automated learning and data mining that estimates the set of possible responses associated with ambivalent numerical data, by (i) assessing the degree of epistemic ambivalence that is carried in a data set with ambivalent information, and by (ii) monitoring the effects of this degree on data analysis, enabling the adaptation of classical data analysis to the treatment of both crisp and fuzzy information. Owing to the strong methodological foundations of the application, a detailed introduction of the ambivalent paradigm is presented first so its adaptability to archaeological survey data can be followed later. The approach is demonstrated in selecting informative features of uncertain palaeosystems by ambivalent Principal Component Analysis (PCA), and exemplified with archaeological survey data collected in the Sierra de Atapuerca and the Arlanzón valley (Burgos, Spain), focusing on the effects of temporal ambivalence on the analysis of associations between lithological palaeoenvironments and Middle Palaeolithic technologies in an open-air context. Although survey data in this region is affected by varying degrees of temporal ambivalence that, on occasion, span several isotopic stages of the late Pleistocene sequence, its effects on feature selection, regarding PCA, are not significant enough to produce contradictory reprojections of palaeolithological data onto the principal space, thanks to a gradual stabilization of the relief in the Middle Arlanzón valley, on a regional scale, during the late Pleistocene. By applying correlation-based dissimilarity distances and cluster analysis to the grouping of ambivalent score patterns, ambivalent features that describe the palaeolithological context of the Middle Palaeolithic scatters can be grouped on the PC1 × PC2 principal plane into four contexts with a similar palaeolithological evolution among group members: the Mesozoic crest of the Sierra de Atapuerca (Group 1), the Neogene levels of the Villalval-Rubena platform (Group 2), lithologically mixed areas between fluvial valleys and Neogene hills (Group 3), and the valleys of the Arlanzón, Pico and Vena rivers (Group 4). In none of these groups the evolution of the local palaeolandscape is notable enough, once again on a regional scale, to produce conflicting descriptions of the palaeolithological signatures where the archaeological scatters occur, although Group 3 (a geographical interface between different lithological palaeoenvironments) has been more dynamic than the rest of environments over the time range the lithic scatters would have formed.}
}
@article{SCHUH2018144,
title = {Data-Based Determination of the Product-Oriented Complexity Degree},
journal = {Procedia CIRP},
volume = {70},
pages = {144-149},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.293},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304748},
author = {Günther Schuh and Christian Dölle and Stephan Schmitz and Jan Koch and Marius Höding and Alexander Menges},
keywords = {complexity management, innovation management, portfolio management, product data management (PDM), data driven design},
abstract = {Available data within today´s digitalized product life cycles comprise necessary information and is currently used in an unstructured way without sufficient description of interrelations. One major challenge in managing product complexity is the tradeoff between standardization measures and customized solutions. Therefore, a transparent overview and effective controlling of product-induced complexity is required and could be supported by product-related data analysis. This paper aims at the development of a generic approach for the data-based determination of the complexity degree.}
}
@article{LIGTHART2018691,
title = {Genome Analyses of >200,000 Individuals Identify 58 Loci for Chronic Inflammation and Highlight Pathways that Link Inflammation and Complex Disorders},
journal = {The American Journal of Human Genetics},
volume = {103},
number = {5},
pages = {691-706},
year = {2018},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2018.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0002929718303203},
author = {Symen Ligthart and Ahmad Vaez and Urmo Võsa and Maria G. Stathopoulou and Paul S. {de Vries} and Bram P. Prins and Peter J. {Van der Most} and Toshiko Tanaka and Elnaz Naderi and Lynda M. Rose and Ying Wu and Robert Karlsson and Maja Barbalic and Honghuang Lin and René Pool and Gu Zhu and Aurélien Macé and Carlo Sidore and Stella Trompet and Massimo Mangino and Maria Sabater-Lleal and John P. Kemp and Ali Abbasi and Tim Kacprowski and Niek Verweij and Albert V. Smith and Tao Huang and Carola Marzi and Mary F. Feitosa and Kurt K. Lohman and Marcus E. Kleber and Yuri Milaneschi and Christian Mueller and Mahmudul Huq and Efthymia Vlachopoulou and Leo-Pekka Lyytikäinen and Christopher Oldmeadow and Joris Deelen and Markus Perola and Jing Hua Zhao and Bjarke Feenstra and Behrooz Z. Alizadeh and H. Marike Boezen and Lude Franke and Pim {van der Harst} and Gerjan Navis and Marianne Rots and Harold Snieder and Morris Swertz and Bruce H.R. Wolffenbuttel and Cisca Wijmenga and Marzyeh Amini and Emelia Benjamin and Daniel I. Chasman and Abbas Dehghan and Tarunveer Singh Ahluwalia and James Meigs and Russell Tracy and Behrooz Z. Alizadeh and Symen Ligthart and Josh Bis and Gudny Eiriksdottir and Nathan Pankratz and Myron Gross and Alex Rainer and Harold Snieder and James G. Wilson and Bruce M. Psaty and Josee Dupuis and Bram Prins and Urmo Vaso and Maria Stathopoulou and Lude Franke and Terho Lehtimaki and Wolfgang Koenig and Yalda Jamshidi and Sophie Siest and Ali Abbasi and Andre G. Uitterlinden and Mohammadreza Abdollahi and Renate Schnabel and Ursula M. Schick and Ilja M. Nolte and Aldi Kraja and Yi-Hsiang Hsu and Daniel S. Tylee and Alyson Zwicker and Rudolf Uher and George Davey-Smith and Alanna C. Morrison and Andrew Hicks and Cornelia M. {van Duijn} and Cavin Ward-Caviness and Eric Boerwinkle and J. Rotter and Ken Rice and Leslie Lange and Markus Perola and Eco {de Geus} and Andrew P. Morris and Kari Matti Makela and David Stacey and Johan Eriksson and Tim M. Frayling and Eline P. Slagboom and Jari Lahti and Katharina E. Schraut and Myriam Fornage and Bhoom Suktitipat and Wei-Min Chen and Xiaohui Li and Teresa Nutile and Giovanni Malerba and Jian’an Luan and Tom Bak and Nicholas Schork and Fabiola {Del Greco M.} and Elisabeth Thiering and Anubha Mahajan and Riccardo E. Marioni and Evelin Mihailov and Joel Eriksson and Ayse Bilge Ozel and Weihua Zhang and Maria Nethander and Yu-Ching Cheng and Stella Aslibekyan and Wei Ang and Ilaria Gandin and Loïc Yengo and Laura Portas and Charles Kooperberg and Edith Hofer and Kumar B. Rajan and Claudia Schurmann and Wouter {den Hollander} and Tarunveer S. Ahluwalia and Jing Zhao and Harmen H.M. Draisma and Ian Ford and Nicholas Timpson and Alexander Teumer and Hongyan Huang and Simone Wahl and YongMei Liu and Jie Huang and Hae-Won Uh and Frank Geller and Peter K. Joshi and Lisa R. Yanek and Elisabetta Trabetti and Benjamin Lehne and Diego Vozzi and Marie Verbanck and Ginevra Biino and Yasaman Saba and Ingrid Meulenbelt and Jeff R. O’Connell and Markku Laakso and Franco Giulianini and Patrik K.E. Magnusson and Christie M. Ballantyne and Jouke Jan Hottenga and Grant W. Montgomery and Fernando Rivadineira and Rico Rueedi and Maristella Steri and Karl-Heinz Herzig and David J. Stott and Cristina Menni and Mattias Frånberg and Beate {St. Pourcain} and Stephan B. Felix and Tune H. Pers and Stephan J.L. Bakker and Peter Kraft and Annette Peters and Dhananjay Vaidya and Graciela Delgado and Johannes H. Smit and Vera Großmann and Juha Sinisalo and Ilkka Seppälä and Stephen R. Williams and Elizabeth G. Holliday and Matthijs Moed and Claudia Langenberg and Katri Räikkönen and Jingzhong Ding and Harry Campbell and Michele M. Sale and Yii-Der I. Chen and Alan L. James and Daniela Ruggiero and Nicole Soranzo and Catharina A. Hartman and Erin N. Smith and Gerald S. Berenson and Christian Fuchsberger and Dena Hernandez and Carla M.T. Tiesler and Vilmantas Giedraitis and David Liewald and Krista Fischer and Dan Mellström and Anders Larsson and Yunmei Wang and William R. Scott and Matthias Lorentzon and John Beilby and Kathleen A. Ryan and Craig E. Pennell and Dragana Vuckovic and Beverly Balkau and Maria Pina Concas and Reinhold Schmidt and Carlos F. {Mendes de Leon} and Erwin P. Bottinger and Margreet Kloppenburg and Lavinia Paternoster and Michael Boehnke and A.W. Musk and Gonneke Willemsen and David M. Evans and Pamela A.F. Madden and Mika Kähönen and Zoltán Kutalik and Magdalena Zoledziewska and Ville Karhunen and Stephen B. Kritchevsky and Naveed Sattar and Genevieve Lachance and Robert Clarke and Tamara B. Harris and Olli T. Raitakari and John R. Attia and Diana {van Heemst} and Eero Kajantie and Rossella Sorice and Giovanni Gambaro and Robert A. Scott and Andrew A. Hicks and Luigi Ferrucci and Marie Standl and Cecilia M. Lindgren and John M. Starr and Magnus Karlsson and Lars Lind and Jun Z. Li and John C. Chambers and Trevor A. Mori and Eco J.C.N. {de Geus} and Andrew C. Heath and Nicholas G. Martin and Juha Auvinen and Brendan M. Buckley and Anton J.M. {de Craen} and Melanie Waldenberger and Konstantin Strauch and Thomas Meitinger and Rodney J. Scott and Mark McEvoy and Marian Beekman and Cristina Bombieri and Paul M. Ridker and Karen L. Mohlke and Nancy L. Pedersen and Alanna C. Morrison and Dorret I. Boomsma and John B. Whitfield and David P. Strachan and Albert Hofman and Peter Vollenweider and Francesco Cucca and Marjo-Riitta Jarvelin and J. Wouter Jukema and Tim D. Spector and Anders Hamsten and Tanja Zeller and André G. Uitterlinden and Matthias Nauck and Vilmundur Gudnason and Lu Qi and Harald Grallert and Ingrid B. Borecki and Jerome I. Rotter and Winfried März and Philipp S. Wild and Marja-Liisa Lokki and Michael Boyle and Veikko Salomaa and Mads Melbye and Johan G. Eriksson and James F. Wilson and Brenda W.J.H. Penninx and Diane M. Becker and Bradford B. Worrall and Greg Gibson and Ronald M. Krauss and Marina Ciullo and Gianluigi Zaza and Nicholas J. Wareham and Albertine J. Oldehinkel and Lyle J. Palmer and Sarah S. Murray and Peter P. Pramstaller and Stefania Bandinelli and Joachim Heinrich and Erik Ingelsson and Ian J. Deary and Reedik Mägi and Liesbeth Vandenput and Pim {van der Harst} and Karl C. Desch and Jaspal S. Kooner and Claes Ohlsson and Caroline Hayward and Terho Lehtimäki and Alan R. Shuldiner and Donna K. Arnett and Lawrence J. Beilin and Antonietta Robino and Philippe Froguel and Mario Pirastu and Tine Jess and Wolfgang Koenig and Ruth J.F. Loos and Denis A. Evans and Helena Schmidt and George Davey Smith and P. Eline Slagboom and Gudny Eiriksdottir and Andrew P. Morris and Bruce M. Psaty and Russell P. Tracy and Ilja M. Nolte and Eric Boerwinkle and Sophie Visvikis-Siest and Alex P. Reiner and Myron Gross and Joshua C. Bis and Lude Franke and Oscar H. Franco and Emelia J. Benjamin and Daniel I. Chasman and Josée Dupuis and Harold Snieder and Abbas Dehghan and Behrooz Z. Alizadeh},
keywords = {C-reactive protein, genome-wide association study, inflammation, Mendelian randomization, inflammatory disorders, DEPICT, coronary artery disease, schizophrenia, system biology},
abstract = {C-reactive protein (CRP) is a sensitive biomarker of chronic low-grade inflammation and is associated with multiple complex diseases. The genetic determinants of chronic inflammation remain largely unknown, and the causal role of CRP in several clinical outcomes is debated. We performed two genome-wide association studies (GWASs), on HapMap and 1000 Genomes imputed data, of circulating amounts of CRP by using data from 88 studies comprising 204,402 European individuals. Additionally, we performed in silico functional analyses and Mendelian randomization analyses with several clinical outcomes. The GWAS meta-analyses of CRP revealed 58 distinct genetic loci (p < 5 × 10−8). After adjustment for body mass index in the regression analysis, the associations at all except three loci remained. The lead variants at the distinct loci explained up to 7.0% of the variance in circulating amounts of CRP. We identified 66 gene sets that were organized in two substantially correlated clusters, one mainly composed of immune pathways and the other characterized by metabolic pathways in the liver. Mendelian randomization analyses revealed a causal protective effect of CRP on schizophrenia and a risk-increasing effect on bipolar disorder. Our findings provide further insights into the biology of inflammation and could lead to interventions for treating inflammation and its clinical consequences.}
}
@article{IFAEI2017138,
title = {A systematic approach of bottom-up assessment methodology for an optimal design of hybrid solar/wind energy resources – Case study at middle east region},
journal = {Energy Conversion and Management},
volume = {145},
pages = {138-157},
year = {2017},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2017.04.097},
url = {https://www.sciencedirect.com/science/article/pii/S019689041730417X},
author = {Pouya Ifaei and Abdolreza Karbassi and Gabriel Jacome and ChangKyoo Yoo},
keywords = {Bottom-up assessment, Clustering, GIS map, Hybrid solar/wind, Optimal design, Renewable energy assessment},
abstract = {In the current study, an algorithm-based data processing, sizing, optimization, sensitivity analysis and clustering approach (DaSOSaCa) is proposed as an efficient simultaneous solar/wind assessment methodology. Accordingly, data processing is performed to obtain reliable high quality meteorological data among various datasets, which are used for hybrid photovoltaic/wind turbine/storage/converter system optimal design for consequent sites in a large region. The optimal hybrid systems are consequently simulated to meet hourly power demand in various sites. The solar/wind fraction and net present cost of the systems are then used as the technical and economic clustering variables, respectively. The clustering results are finally used as input to obtain novel hybrid solar/wind GIS maps. Iran is selected as the case study to validate the proposed methodology and detail its applicability. Ten minute annual global horizontal radiation, wind speed, and temperature data are analyzed, and the optimal, robust hybrid systems are simulated for various sites in order to classify the country. The generated GIS maps show that Iran can be efficiently clustered into four technical and five economic clusters under optimal conditions. The clustering results prove that Iran is mainly a solar country with approximately 74% solar power fraction under optimum conditions. A macroeconomic evaluation using DaSOSaCa also reveals that the nominal discount rate is recommended to be greater than 20% considering the current economic situation for the renewable energy sector in Iran. An environmental analysis results show that an average 106.68tonCO2-eq/year is produced for such hybrid systems application in Iran during a cradle to grave life cycle. Thus, Iran energy sector can be eminently promoted to an environmentally efficient stage with regard to the proposed classification plan and economic considerations.}
}
@article{NGOYE201798,
title = {Predicting the helpfulness of online reviews using a scripts-enriched text regression model},
journal = {Expert Systems with Applications},
volume = {71},
pages = {98-110},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416306649},
author = {Thomas L. Ngo-Ye and Atish P. Sinha and Arun Sen},
keywords = {Business intelligence, Online customer reviews, Review helpfulness, Script theory, Human annotation, Text regression},
abstract = {In this paper, we examine the utility of script analysis for predicting the helpfulness of online customer reviews. We employ the lens of cognitive scripts and posit that people share a cognitive script for what constitutes a helpful review in a given domain. Conceptually, a script includes the salient elements that readers look for before determining whether a review is helpful. To operationalize the construct of cognitive script, we seek the help of human annotators and ask them to highlight phrases that they believe are important for determining review helpfulness. The words in the annotated phrases are collected and become part of the script lexicon for a given domain. The lexicon entries represent the shared conception of essential elements, which are key to the evaluation of review helpfulness. We employ the words in the script lexicon as features in a text regression model to predict review helpfulness. Furthermore, we develop and empirically validate a new approach for combining script analysis and dimension reduction. The purpose of the study is to propose a new method to predict review helpfulness and to evaluate the effectiveness and efficiency of the scripts-enriched model. To demonstrate the efficacy of the scripts-enriched model, we compare it with benchmark models – a Baseline model and a bag-of-words (BOW) model. The results show that the scripts-enriched text regression model not only produces the highest accuracy, but also the lowest training, testing, and feature selection times.}
}
@article{LIU201711,
title = {Enabling effective workflow model reuse: A data-centric approach},
journal = {Decision Support Systems},
volume = {93},
pages = {11-25},
year = {2017},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616301518},
author = {Zhiyong Liu and Shaokun Fan and Harry Jiannan Wang and J. Leon Zhao},
keywords = {Workflow model reuse, Workflow model management, Data flow perspective, Data dependency},
abstract = {With increasingly widespread adoption of workflow technology as a standard solution to business process management, a large number of workflow models have been put in use in companies in the era of electronic commerce. These workflow models form a valuable resource for workflow domain knowledge, which should be reused to support workflow model design. However, current workflow modeling approaches do not facilitate workflow model reuse as a fundamental requirement, leading to a research gap in effective workflow model reuse. In this paper, we propose a novel approach called Data-centric Workflow Model Reuse framework (DWMR) to provide a solution to workflow model reuse. DWMR compliments existing control-flow-focused workflow modeling approaches by explicitly storing workflow data information, such as data dependency, data task relationships, and data similarity scores. DWMR also provides data-driven workflow model search and composition algorithms to satisfy user query requirements by automatically combining multiple workflow models. We demonstrate the feasibility of the DWMR approach by applying it to data from a well-known industry workflow model repository.}
}
@incollection{2017255,
title = {Abstracts from the First Neuroadaptive Technology Conference, 2017},
editor = {Stephen H. Fairclough and Thorsten O. Zander},
booktitle = {Current Research in Neuroadaptive Technology},
publisher = {Academic Press},
pages = {255-402},
year = {2017},
isbn = {978-0-12-821413-8},
doi = {https://doi.org/10.1016/B978-0-12-821413-8.00019-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128214138000191}
}
@article{SHIN2017208,
title = {Public value mapping of network neutrality: Public values and net neutrality in Korea},
journal = {Telecommunications Policy},
volume = {41},
number = {3},
pages = {208-224},
year = {2017},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2016.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0308596116302804},
author = {Dong-Hee Shin and Min-Kyu Lee},
keywords = {Network neutrality, Public value mapping, South Korea, User-centered policy analysis},
abstract = {Network neutrality (NN) is of broad and current interest despite its complex and multifaceted nature. This study examines NN in the Korean context in terms of policy, industry, values, and society. It addresses the policy effectiveness of ongoing NN discussions by analyzing user attitudes and perceptions. The proposed model empirically tests policy effectiveness through user perceptions by incorporating factors representative of NN. Possible NN factors are derived from the previous literature and perceptions of NN concepts. The results find that regulation and competition are two primary factors constituting network neutrality, and these factors differently influence the formation of attitudes toward policy effectiveness. Political and social implications are discussed based on the proposed model. This study provides comprehensive analysis and heuristic data on user drivers, industry dynamics, and policy implications in the NN ecosystem.}
}
@article{SMITH2018636,
title = {Phenotypic Image Analysis Software Tools for Exploring and Understanding Big Image Data from Cell-Based Assays},
journal = {Cell Systems},
volume = {6},
number = {6},
pages = {636-653},
year = {2018},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405471218302412},
author = {Kevin Smith and Filippo Piccinini and Tamas Balassa and Krisztian Koos and Tivadar Danka and Hossein Azizpour and Peter Horvath},
keywords = {high-content screening, single-cell analysis, phenomics, oncology, drug screening, freely available tools, microscopy, machine learning, cell classification, phenotypic image analysis},
abstract = {Phenotypic image analysis is the task of recognizing variations in cell properties using microscopic image data. These variations, produced through a complex web of interactions between genes and the environment, may hold the key to uncover important biological phenomena or to understand the response to a drug candidate. Today, phenotypic analysis is rarely performed completely by hand. The abundance of high-dimensional image data produced by modern high-throughput microscopes necessitates computational solutions. Over the past decade, a number of software tools have been developed to address this need. They use statistical learning methods to infer relationships between a cell's phenotype and data from the image. In this review, we examine the strengths and weaknesses of non-commercial phenotypic image analysis software, cover recent developments in the field, identify challenges, and give a perspective on future possibilities.}
}
@incollection{2018345,
title = {Chapter 9 - Human Factors},
editor = {Felipe Jiménez},
booktitle = {Intelligent Vehicles},
publisher = {Butterworth-Heinemann},
pages = {345-394},
year = {2018},
isbn = {978-0-12-812800-8},
doi = {https://doi.org/10.1016/B978-0-12-812800-8.00009-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128128008000096},
keywords = {Diver monitoring, drowsiness, tiredness, HMI, interface, driver model},
abstract = {Apart from technological issues, human factors should also be taken into account in the intelligent vehicles even when high levels of automation are reached. In this sense, this chapter deals with two specific areas related to the human driver. The first one considers the driver monitoring in order to know whether his conditions are good enough to perform the driving tasks. In the second section, the human–machine interface is explained because this interaction is essential for the effectiveness of new systems and to not disturb the main task of the driver.}
}
@incollection{ZHANG20181,
title = {Chapter One - Blockchain Technology Use Cases in Healthcare},
editor = {Pethuru Raj and Ganesh Chandra Deka},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {111},
pages = {1-41},
year = {2018},
booktitle = {Blockchain Technology: Platforms, Tools and Use Cases},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2018.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0065245818300196},
author = {Peng Zhang and Douglas C. Schmidt and Jules White and Gunther Lenz},
keywords = {Blockchain in healthcare, Healthcare interoperability, Smart contracts},
abstract = {Blockchain technology alleviates the reliance on a centralized authority to certify information integrity and ownership, as well as mediate transactions and exchange of digital assets, while enabling secure and pseudoanonymous transactions along with agreements directly between interacting parties. It possesses key properties, such as immutability, decentralization, and transparency, which potentially address pressing issues in healthcare, such as incomplete records at point of care and difficult access to patients’ own health information. An efficient and effective healthcare system requires interoperability, which allows software apps and technology platforms to communicate securely and seamlessly, exchange data, and use the exchanged data across health organizations and app vendors. Unfortunately, healthcare today suffers from siloed and fragmented data, delayed communications, and disparate workflow tools caused by the lack of interoperability. Blockchain offers the opportunity to enable access to longitudinal, complete, and tamper-aware medical records that are stored in fragmented systems in a secure and pseudoanonymous fashion. This chapter focuses on the applicability of Blockchain technology in healthcare by (1) identifying potential Blockchain use cases in healthcare, (2) providing a case study that implements Blockchain technology, and (3) evaluating design considerations when applying this technology in healthcare.}
}
@article{ULMEANU2017127,
title = {Hidden Markov Models revealing the household thermal profiling from smart meter data},
journal = {Energy and Buildings},
volume = {154},
pages = {127-140},
year = {2017},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2017.08.036},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816318229},
author = {Anatoli Paul Ulmeanu and Vlad Stefan Barbu and Vladimir Tanasiev and Adrian Badea},
keywords = {Hidden Markov chain, Emission probability matrix, Sequence observation, Building thermal load profile},
abstract = {This work describes a methodology based on Hidden Markov Models (HMMs) that are applied for revealing household thermal load profiles which are not available to direct observation. This research is motivated by the necessity of reducing the energy consumption for cooling and heating in residential buildings. Our methodology uses data that is becoming readily available at households – hourly energy consumption records collected from smart electricity meters, as well as hourly outdoor air temperature records. The heat transfer regime, namely the states corresponding to lower or higher building hourly thermal loads related to the outdoor air temperatures, will be considered as the underlying mechanism affecting the generation of observations. We aggregate the observed data to obtain a certain number of clusters. The problem of HMM estimation is addressed and the subsequent HMMs are compared on the basis of information criteria, like Akaike and Bayesian Information Criteria. Our goal is to reveal the dynamic of building thermal load (heating/cooling) under the uncertainties induced by the residents’ behavior. Consequently, we present examples of thermal load profiles generated using our best HMM on a testing facility located in the Polytechnic University of Bucharest campus, namely the UPB's passive building house.}
}
@article{HAO201875,
title = {Recognizing multi-resident activities in non-intrusive sensor-based smart homes by formal concept analysis},
journal = {Neurocomputing},
volume = {318},
pages = {75-89},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218309834},
author = {Jianguo Hao and Abdenour Bouzouane and Sébastien Gaboury},
keywords = {Multi-resident activity recognition, Formal concept analysis, Sequential pattern mining, Smart homes, Ambient intelligence},
abstract = {Activity recognition is one of the most important prerequisites for smart home applications. It is a challenging topic due to the high requirements for reliable data acquisition and efficient data analysis. Besides, the heterogeneous layouts of smart homes, the number of residents and varied human behavioral patterns also aggravate the complexity of recognition. Therefore, most human activity recognition systems are based on an unrealistic assumption that there is only one resident performing activities. In this paper, we investigate the issue of multi-resident activity recognition and propose a knowledge-driven solution on the basis of formal concept analysis (FCA) to identify human activities from non-intrusive sensor data. We extract the ontological correlations among sequential behavioral patterns. At the same time, these correlations are well organized in a graphical knowledge base, without intervention from domain experts. We propose an incremental lattice search strategy in order to retrieve the best inference given a few sensor events. Compared with other conventional probabilistic methods, our solution outperforms on the CASAS multi-resident benchmark dataset. Furthermore, we open up a promising solution of sequential pattern mining to discover the ontological features of temporal and sequential sensor data.}
}
@article{DEVITO20181191,
title = {Calibrating chemical multisensory devices for real world applications: An in-depth comparison of quantitative machine learning approaches},
journal = {Sensors and Actuators B: Chemical},
volume = {255},
pages = {1191-1210},
year = {2018},
issn = {0925-4005},
doi = {https://doi.org/10.1016/j.snb.2017.07.155},
url = {https://www.sciencedirect.com/science/article/pii/S0925400517313692},
author = {S. {De Vito} and E. Esposito and M. Salvato and O. Popoola and F. Formisano and R. Jones and G. {Di Francia}},
keywords = {Distributed chemical sensing, Multisensors calibration algorithms, Dynamic machine learning, Air quality monitoring, Indicative measurements, Internet of Things},
abstract = {Chemical multisensor devices need calibration algorithms to estimate gas concentrations. Their possible adoption as indicative air quality measurements devices poses new challenges due to the need to operate in continuous monitoring modes in uncontrolled environments. Several issues, including slow dynamics, continue to affect their real world performances. At the same time, the need for estimating pollutant concentrations on board the devices, especially for wearables and IoT deployments, is becoming highly desirable. In this framework, several calibration approaches have been proposed and tested on a variety of proprietary devices and datasets; still, no thorough comparison is available to researchers. This work attempts a benchmarking of the most promising calibration algorithms according to recent literature with a focus on machine learning approaches. We test the techniques against absolute and dynamic performances, generalization capabilities and computational/storage needs using three different datasets sharing continuous monitoring operation methodology. Our results can guide researchers and engineers in the choice of optimal strategy. They show that non-linear multivariate techniques yield reproducible results, outperforming linear approaches. Specifically, the Support Vector Regression method consistently shows good performances in all the considered scenarios. We highlight the enhanced suitability of shallow neural networks in a trade-off between performance and computational/storage needs. We confirm, on a much wider basis, the advantages of dynamic approaches with respect to static ones that only rely on instantaneous sensor array response. The latter have been shown to be best choice whenever prompt and precise response is needed.}
}
@article{CHMIELEWSKI201897,
title = {Citizen science and WebGIS for outdoor advertisement visual pollution assessment},
journal = {Computers, Environment and Urban Systems},
volume = {67},
pages = {97-109},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S019897151730234X},
author = {Szymon Chmielewski and Marta Samulowska and Michał Lupa and Danbi Lee and Bogdan Zagajewski}
}
@article{OLTEDAL2017422,
title = {The Global ECT-MRI Research Collaboration (GEMRIC): Establishing a multi-site investigation of the neural mechanisms underlying response to electroconvulsive therapy},
journal = {NeuroImage: Clinical},
volume = {14},
pages = {422-432},
year = {2017},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2017.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S2213158217300438},
author = {Leif Oltedal and Hauke Bartsch and Ole Johan Evjenth Sørhaug and Ute Kessler and Christopher Abbott and Annemieke Dols and Max L Stek and Lars Ersland and Louise Emsell and Philip {van Eijndhoven} and Miklos Argyelan and Indira Tendolkar and Pia Nordanskog and Paul Hamilton and Martin Balslev Jorgensen and Iris E Sommer and Sophie M Heringa and Bogdan Draganski and Ronny Redlich and Udo Dannlowski and Harald Kugel and Filip Bouckaert and Pascal Sienaert and Amit Anand and Randall Espinoza and Katherine L Narr and Dominic Holland and Anders M Dale and Ketil J Oedegaard},
keywords = {Electroconvulsive therapy, MRI, Longitudinal, Mega analysis, Multi-site},
abstract = {Major depression, currently the world's primary cause of disability, leads to profound personal suffering and increased risk of suicide. Unfortunately, the success of antidepressant treatment varies amongst individuals and can take weeks to months in those who respond. Electroconvulsive therapy (ECT), generally prescribed for the most severely depressed and when standard treatments fail, produces a more rapid response and remains the most effective intervention for severe depression. Exploring the neurobiological effects of ECT is thus an ideal approach to better understand the mechanisms of successful therapeutic response. Though several recent neuroimaging studies show structural and functional changes associated with ECT, not all brain changes associate with clinical outcome. Larger studies that can address individual differences in clinical and treatment parameters may better target biological factors relating to or predictive of ECT-related therapeutic response. We have thus formed the Global ECT-MRI Research Collaboration (GEMRIC) that aims to combine longitudinal neuroimaging as well as clinical, behavioral and other physiological data across multiple independent sites. Here, we summarize the ECT sample characteristics from currently participating sites, and the common data-repository and standardized image analysis pipeline developed for this initiative. This includes data harmonization across sites and MRI platforms, and a method for obtaining unbiased estimates of structural change based on longitudinal measurements with serial MRI scans. The optimized analysis pipeline, together with the large and heterogeneous combined GEMRIC dataset, will provide new opportunities to elucidate the mechanisms of ECT response and the factors mediating and predictive of clinical outcomes, which may ultimately lead to more effective personalized treatment approaches.}
}
@article{TAO201830,
title = {Massive stereo-based DTM production for Mars on cloud computers},
journal = {Planetary and Space Science},
volume = {154},
pages = {30-58},
year = {2018},
issn = {0032-0633},
doi = {https://doi.org/10.1016/j.pss.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0032063317303252},
author = {Y. Tao and J.-P. Muller and P. Sidiropoulos and Si-Ting Xiong and A.R.D. Putri and S.H.G. Walter and J. Veitch-Michaelis and V. Yershov},
keywords = {Mars, Global DTM, CTX, HiRISE, CASP-GO, Clouds computing},
abstract = {Digital Terrain Model (DTM) creation is essential to improving our understanding of the formation processes of the Martian surface. Although there have been previous demonstrations of open-source or commercial planetary 3D reconstruction software, planetary scientists are still struggling with creating good quality DTMs that meet their science needs, especially when there is a requirement to produce a large number of high quality DTMs using “free” software. In this paper, we describe a new open source system to overcome many of these obstacles by demonstrating results in the context of issues found from experience with several planetary DTM pipelines. We introduce a new fully automated multi-resolution DTM processing chain for NASA Mars Reconnaissance Orbiter (MRO) Context Camera (CTX) and High Resolution Imaging Science Experiment (HiRISE) stereo processing, called the Co-registration Ames Stereo Pipeline (ASP) Gotcha Optimised (CASP-GO), based on the open source NASA ASP. CASP-GO employs tie-point based multi-resolution image co-registration, and Gotcha sub-pixel refinement and densification. CASP-GO pipeline is used to produce planet-wide CTX and HiRISE DTMs that guarantee global geo-referencing compliance with respect to High Resolution Stereo Colour imaging (HRSC), and thence to the Mars Orbiter Laser Altimeter (MOLA); providing refined stereo matching completeness and accuracy. All software and good quality products introduced in this paper are being made open-source to the planetary science community through collaboration with NASA Ames, United States Geological Survey (USGS) and the Jet Propulsion Laboratory (JPL), Advanced Multi-Mission Operations System (AMMOS) Planetary Data System (PDS) Pipeline Service (APPS-PDS4), as well as browseable and visualisable through the iMars web based Geographic Information System (webGIS) system.}
}
@article{COSTANTINI2017250,
title = {Analysis of surface deformations over the whole Italian territory by interferometric processing of ERS, Envisat and COSMO-SkyMed radar data},
journal = {Remote Sensing of Environment},
volume = {202},
pages = {250-275},
year = {2017},
note = {Big Remotely Sensed Data: tools, applications and experiences},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2017.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0034425717303322},
author = {Mario Costantini and Alessandro Ferretti and Federico Minati and Salvatore Falco and Francesco Trillo and Davide Colombo and Fabrizio Novali and Fabio Malvarosa and Claudio Mammone and Francesco Vecchioli and Alessio Rucci and Alfio Fumagalli and Jacopo Allievi and Maria Grazia Ciminelli and Salvatore Costabile},
keywords = {SAR interferometry, Persistent scatterer, Ground surface deformation},
abstract = {Interferometric processing of series of data acquired over time by synthetic aperture radar (SAR) satellites makes it possible to measure millimetric deformations (typically due to landslides, subsidence and earthquake or volcanic phenomena) and to monitor the stability of terrain and infrastructures. Despite the unique capability to observe very large areas, this technology has been typically applied to the analysis of relatively small sites or specific geophysical phenomena. In this work, we present the first application of this technology to a national scale project, which required the processing, through advanced persistent scatterer interferometry (PSI) techniques, of about 20,000 SAR images acquired from 1992 to 2014 over the whole Italian territory by the ERS, Envisat, and COSMO-SkyMed satellites. The obtained results provide a huge database of surface deformation measurements covering the last 20years and the whole Italian territory, which represents an extremely useful and pioneering service for geo-hazards mapping and prevention.}
}
@incollection{MAKRIS2018511,
title = {Chapter 45 - Current Approaches to Risk Assessment for Developmental Neurotoxicity},
editor = {William Slikker and Merle G. Paule and Cheng Wang},
booktitle = {Handbook of Developmental Neurotoxicology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {511-526},
year = {2018},
isbn = {978-0-12-809405-1},
doi = {https://doi.org/10.1016/B978-0-12-809405-1.00045-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094051000456},
author = {Susan L. Makris and Andrew D. Kraft},
keywords = {developmental neurotoxicity, risk assessment, hazard identification, dose response analysis, exposure assessment, risk characterization, data gaps},
abstract = {Risk assessment for developmental neurotoxicity of chemicals follows a standardized paradigm that includes problem formulation and scoping, hazard identification, dose–response assessment, exposure assessment, and risk characterization. In each step, relevant data are systematically evaluated and integrated into a determination of potential risk to humans of neurological perturbation following exposure during periods of nervous system development. Human and animal data are supplemented by information from nonmammalian species, in vitro assays, toxicokinetic data, and other mechanistic studies when available. Developmental neurotoxicity risk assessment incorporates considerations of intrinsic susceptibilities, critical developmental windows of exposure and periods of assessment, the major manifestations of developmental toxicity (death, structural abnormalities, delayed growth, and functional effects), and an assessment of dose response. Methodological challenges are related to the outcomes assessed, the populations of concern, and the complexity and trajectory of nervous system development. Developmental neurotoxicity risk assessment is an indispensable tool to inform decisions for public health protection.}
}
@article{YATES2017169,
title = {Genomic Evolution of Breast Cancer Metastasis and Relapse},
journal = {Cancer Cell},
volume = {32},
number = {2},
pages = {169-184.e7},
year = {2017},
issn = {1535-6108},
doi = {https://doi.org/10.1016/j.ccell.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1535610817302970},
author = {Lucy R. Yates and Stian Knappskog and David Wedge and James H.R. Farmery and Santiago Gonzalez and Inigo Martincorena and Ludmil B. Alexandrov and Peter {Van Loo} and Hans Kristian Haugland and Peer Kaare Lilleng and Gunes Gundem and Moritz Gerstung and Elli Pappaemmanuil and Patrycja Gazinska and Shriram G. Bhosle and David Jones and Keiran Raine and Laura Mudie and Calli Latimer and Elinor Sawyer and Christine Desmedt and Christos Sotiriou and Michael R. Stratton and Anieta M. Sieuwerts and Andy G. Lynch and John W. Martens and Andrea L. Richardson and Andrew Tutt and Per Eystein Lønning and Peter J. Campbell},
keywords = {breast cancer, metastasis, relapse, genomics, somatic mutation},
abstract = {Summary
Patterns of genomic evolution between primary and metastatic breast cancer have not been studied in large numbers, despite patients with metastatic breast cancer having dismal survival. We sequenced whole genomes or a panel of 365 genes on 299 samples from 170 patients with locally relapsed or metastatic breast cancer. Several lines of analysis indicate that clones seeding metastasis or relapse disseminate late from primary tumors, but continue to acquire mutations, mostly accessing the same mutational processes active in the primary tumor. Most distant metastases acquired driver mutations not seen in the primary tumor, drawing from a wider repertoire of cancer genes than early drivers. These include a number of clinically actionable alterations and mutations inactivating SWI-SNF and JAK2-STAT3 pathways.}
}
@article{ZHOU20181,
title = {Topological mapping and assessment of multiple settlement time series in deep excavation: A complex network perspective},
journal = {Advanced Engineering Informatics},
volume = {36},
pages = {1-19},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617303075},
author = {Cheng Zhou and Lieyun Ding and Ying Zhou and Hanbin Luo},
keywords = {Deep excavation, Settlement time series, Complex network, Similarity matrix, Topological analysis, Node influence},
abstract = {This study proposed a novel methodology that integrates complex network theory and multiple time series to enhance the systematic understanding of the daily settlement behavior in deep excavation. The original time series of ground surface, surrounding buildings, and structure settlement instrumentation data over an excavation time period were measured into a similarity matrix with correlation coefficients. A threshold was then determined and binarized into adjacent matrix to identify the optimal topology and structure of the complex network. The reconstructed settlement network has nodes corresponding to multiple settlement time series individually and edges regarded as nonlinear relationships between them. A deep excavation case study of the metro station project in the Wuhan Metro network, China, was applied to validate the feasibility and potential value of the proposed approach. Results of the topological analysis corroborate a small-world phenomenon with highly compacted interactions and provide the assessment of the significance among multiple settlement time series. This approach, which provides a new way to assess the safety monitoring data in underground construction, can be implemented as a tool for extracting macro- and micro-level decision information from multiple settlement time series in deep excavation from complex system perspectives.}
}
@article{MA201727,
title = {Time-aware trustworthiness ranking prediction for cloud services using interval neutrosophic set and ELECTRE},
journal = {Knowledge-Based Systems},
volume = {138},
pages = {27-45},
year = {2017},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.09.027},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117304471},
author = {Hua Ma and Haibin Zhu and Zhigang Hu and Keqin Li and Wensheng Tang},
keywords = {Cloud services, ELECTRE, Interval neutrosophic set, Ranking prediction, Time-aware, Trustworthiness},
abstract = {The imprecise quality of service (QoS) evaluations from consumers may lead to the inappropriate prediction for the trustworthiness of cloud services in an uncertain cloud environment. The service ranking prediction is a promising idea to overcome this deficiency of values prediction approaches by probing the ordering relations between cloud services concealed in the imprecise evaluations. To address the challenges for trustworthy service selection resulting from fluctuating QoS, flexible service pricing and complicated potential risks, this paper proposes a time-aware approach to predict the trustworthiness ranking of cloud services, with the tradeoffs between performance-cost and potential risks in multiple periods. In this approach, the interval neutrosophic set (INS) theory is utilized to describe and assess the performance-costs and potential risks of cloud services: (1) the original evaluation data about cloud services are preprocessed into the trustworthiness interval neutrosophic numbers (INNs); (2) the new INS operators are proposed with the theoretical proofs to calculate the possibility degree and the ranking values of trustworthiness INNs, contributing to the identification of the neighboring users based on the Kendall rank correlation coefficient (KRCC). The problem of time-aware trustworthiness ranking prediction is formulated as a multi-criterion decision-making (MCDM) problem of creating a ranked services list using INS, and an improved ELECTRE method is developed to solve it. The proposed approach is verified by experiments based on an appropriate baseline for comparative analysis. The experimental results demonstrate that the proposed approach can achieve a better prediction quality than the existing approach. The results also show that our approach works effectively in the risk-sensitive and performance-cost-sensitive application scenarios and prevent the malignant price competition launched by low-quality services.}
}
@incollection{MARKE201835,
title = {Chapter 4 - Decoding the Current Global Climate Finance Architecture},
editor = {Alastair Marke},
booktitle = {Transforming Climate Finance and Green Investment with Blockchains},
publisher = {Academic Press},
pages = {35-59},
year = {2018},
isbn = {978-0-12-814447-3},
doi = {https://doi.org/10.1016/B978-0-12-814447-3.00004-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128144473000045},
author = {Alastair Marke and Bianca Sylvester},
keywords = {Adaptation, Blockchain, climate finance, fintech, Green Climate Fund, green investment, mitigation, NDC, Paris Agreement, trust and transparency},
abstract = {To keep the planet on the 2°C trajectory, developing countries alone will require US$500 billion annually by 2030 to adequately mitigate their carbon emissions, in addition to several hundred billion additional dollars for adaptation needs. The US$100 billion a year pledged by developed countries through the Green Climate Fund is such insufficient that most of the climate finance required will have to come from the private sector. As a scene-setting chapter, it will delineate the scale of climate investment required, the state of climate finance post-Paris Agreement, climate mitigation and adaptation investment trends, global climate finance landscape, specific hurdles against climate investment, and the problems with climate finance tracking and monitoring. The lack of trust and transparency in the global climate finance landscape provides an excellent ground on which to deploy Blockchain technology to turbo-boost global climate finance flows. This chapter will initiate the discussion on how Blockchain as a “trust machine” could address the many deeply-rooted institutional problems.}
}
@article{STAMATE2018146,
title = {The cloudUPDRS app: A medical device for the clinical assessment of Parkinson’s Disease},
journal = {Pervasive and Mobile Computing},
volume = {43},
pages = {146-166},
year = {2018},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2017.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1574119217303607},
author = {C. Stamate and G.D. Magoulas and S. Kueppers and E. Nomikou and I. Daskalopoulos and A. Jha and J.S. Pons and J. Rothwell and M.U. Luchini and T. Moussouri and M. Iannone and G. Roussos},
abstract = {Parkinson’s Disease is a neurological condition distinguished by characteristic motor symptoms including tremor and slowness of movement. To enable the frequent assessment of PD patients, this paper introduces the cloudUPDRS app, a Class I medical device that is an active transient non-invasive instrument, certified by the Medicines and Healthcare products Regulatory Agency in the UK. The app follows closely Part III of the Unified Parkinson’s Disease Rating Scale which is the most commonly used protocol in the clinical study of PD; can be used by patients and their carers at home or in the community unsupervised; and, requires the user to perform a sequence of iterated movements which are recorded by the phone sensors. The cloudUPDRS system addresses two key challenges towards meeting essential consistency and efficiency requirements, namely: (i) How to ensure high-quality data collection especially considering the unsupervised nature of the test, in particular, how to achieve firm user adherence to the prescribed movements; and (ii) How to reduce test duration from approximately 25 min typically required by an experienced patient, to below 4 min, a threshold identified as critical to obtain significant improvements in clinical compliance. To address the former, we combine a bespoke design of the user experience tailored so as to constrain context, with a deep learning approach based on Recurrent Convolutional Neural Networks, to identify failures to follow the movement protocol. We address the latter by developing a machine learning approach to personalize assessments by selecting those elements of the test that most closely match individual symptom profiles and thus offer the highest inferential power, hence closely estimating the patent’s overall score.}
}
@article{ZHANG201842,
title = {Updating authoritative spatial data from timely sources: A multiple representation approach},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {72},
pages = {42-56},
year = {2018},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2018.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0303243418302101},
author = {Xiang Zhang and Weijun Yin and Min Yang and Tinghua Ai and Jantien Stoter},
keywords = {Volunteered geographic information, OpenStreetMap, Incremental update, Multiple representation databases, Data matching, Cartographic generalization},
abstract = {Integrating updates from timely sources such as volunteered geographic information (VGI) into the spatial data maintained at official agencies is becoming a more demanding requirement but presents many challenges. This paper proposes an approach to addressing the technical challenge of propagating updates from timely sources (e.g. OpenStreetMap) to spatial data maintained at separate map scales. The main idea is to establish a multiple representation database (MRDB) for datasets at different scales and time to facilitate incremental update, where linkages between corresponding objects at different datasets are made explicit. First, two ways in which the timely sources can be integrated into official data for incremental update are discussed. To derive the linkages between different datasets, a data matching procedure based on computer vision is presented and fine-tuned to match data in different scale ranges. Furthermore, the generalization history used to produce smaller scale data from the larger ones in official data is inferred based on the linkages, and is then used to guide the update propagation. Finally, a framework for incremental generalization in MRDBs is proposed, where crucial issues like strategies for update propagation, cartographic generalization, and the so-called ‘chain reaction’ are addressed. The framework is implemented as a fully automated process where operators like simplification, enlargement, compression, displacement and typification are incorporated into the incremental update process. By testing the framework against real world data sets (i.e. OpenStreetMap and official data at 1:10k, 1:50k and 1:100k), we show that the updates are integrated consistently into existing data in terms of spatial relations and cartographic quality. Our work suggests that making use of timely sources by official mapping agencies and companies in a continuous or event-driven data update is technically feasible, with further improvement and extensions discussed.}
}
@article{GIBERT20183,
title = {Which method to use? An assessment of data mining methods in Environmental Data Science},
journal = {Environmental Modelling & Software},
volume = {110},
pages = {3-27},
year = {2018},
note = {Special Issue on Environmental Data Science and Decision Support: Applications in Climate Change and the Ecological Footprint},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2018.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218308715},
author = {Karina Gibert and Joaquín Izquierdo and Miquel Sànchez-Marrè and Serena H. Hamilton and Ignasi Rodríguez-Roda and Geoff Holmes},
keywords = {Data mining, Data science, Method selection, Multidisciplinarity, Environmental systems},
abstract = {Data Mining (DM) is a fundamental component of the Data Science process. Over recent years a huge library of DM algorithms has been developed to tackle a variety of problems in fields such as medical imaging and traffic analysis. Many DM techniques are far more flexible than more classical numerial simulation or statistical modelling approaches. These could be usefully applied to data-rich environmental problems. Certain techniques such as artificial neural networks, clustering, case-based reasoning or Bayesian networks have been applied in environmental modelling, while other methods, like support vector machines among others, have yet to be taken up on a wide scale. There is greater scope for many lesser known techniques to be applied in environmental research, with the potential to contribute to addressing some of the current open environmental challenges. However, selecting the best DM technique for a given environmental problem is not a simple decision, and there is a lack of guidelines and criteria that helps the data scientist and environmental scientists to ensure effective knowledge extraction from data. This paper provides a broad introduction to the use of DM in Data Science processes for environmental researchers. Data Science contains three main steps (pre-processing, data mining and post-processing). This paper provides a conceptualization of Environmental Systems and a conceptualization of DM methods, which are in the core step of the Data Science process. These two elements define a conceptual framework that is on the basis of a new methodology proposed for relating the characteristics of a given environmental problem with a family of Data Mining methods. The paper provides a general overview and guidelines of DM techniques to a non-expert user, who can decide with this support which is the more suitable technique to solve their problem at hand. The decision is related to the bidimensional relationship between the type of environmental system and the type of DM method. An illustrative two way table containing references for each pair Environmental System-Data Mining method is presented and discussed. Some examples of how the proposed methodology is used to support DM method selection are also presented, and challenges and future trends are identified.}
}
@article{ROTH2017307,
title = {Futures of a distributed memory. A global brain wave measurement (1800–2000)},
journal = {Technological Forecasting and Social Change},
volume = {118},
pages = {307-323},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.02.031},
url = {https://www.sciencedirect.com/science/article/pii/S004016251730269X},
author = {Steffen Roth and Carlton Clark and Nikolay Trofimov and Artur Mkrtichyan and Markus Heidingsfelder and Laura Appignanesi and Miguel Pérez-Valls and Jan Berkel and Jari Kaivo-oja},
keywords = {Global brain, Google Ngram Viewer, Culturomics, Secularization, Capitalism, Functional differentiation},
abstract = {If the global brain is a suitable model of the future information society, then one future of research in this global brain will be in its past, which is its distributed memory. In this paper, we draw on Francis Heylighen, Marta Lenartowicz, and Niklas Luhmann to show that future research in this global brain will have to reclaim classical theories of social differentiation in general and theories of functional differentiation in particular to develop higher resolution images of this brain's function and sub-functions. This claim is corroborated by a brain wave measurement of a considerable section of the global brain. We used the Google Ngram Viewer, an online graphing tool which charts annual counts of words or sentences as found in the largest available corpus of digitalized books, to analyse word frequency time-series plots of key concepts of social differentiation in the English as well as in the Spanish, French, German, Russian, and Italian sub-corpora between 1800 and 2000. The results of this socioencephalography suggest that the global brain's memory recalls distinct and not yet fully conscious biases to particular sub-functions, which are furthermore not in line with popular trend statements and self-descriptions of modern societies. We speculate that an increasingly intelligent global brain will start to critically reflect upon these biases and learn how to anticipate or even design its own desired futures.}
}
@article{XU2018195,
title = {MULAPI: Improving API method recommendation with API usage location},
journal = {Journal of Systems and Software},
volume = {142},
pages = {195-205},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.04.060},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218300840},
author = {Congying Xu and Xiaobing Sun and Bin Li and Xintong Lu and Hongjing Guo},
keywords = {API method recommendation, API usage location, Feature request, Feature location},
abstract = {During the evolution of a software system, a large number of feature requests are continuously proposed by users. To implement these feature requests, developers often utilize existing third-party libraries and make use of Application Programming Interfaces (APIs) to accelerate the feature implementation process. However, it is not always obvious which API methods are suitable and where these API methods can be used in the target program. In this paper, we propose an approach, MULAPI (Method Usage and Location for API), to recommend API methods and figure out the API usage location where these API methods would be used. MULAPI employs feature location to identify feature related files as API usage location. Further, these feature related files are taken into account to recommend API methods by exploring the source code repository and API libraries as well. We evaluate MULAPI on more than 1000 feature requests of eight Java projects (Axis/Java, CXF, Hadoop Common, Hbase, Struts2, Hadoop HDFS, Hive and Hadoop Map/Reduce), and recommend API methods from ten third-party libraries. The empirical results show that MULAPI can accurately recommend API methods and usage location, and moreover, MULAPI improves the effectiveness of API method recommendation, compared with the state-of-the-art approach.}
}
@article{JANOWSKI2018S1,
title = {Platform governance for sustainable development: Reshaping citizen-administration relationships in the digital age},
journal = {Government Information Quarterly},
volume = {35},
number = {4, Supplement },
pages = {S1-S16},
year = {2018},
note = {Platform Governance for Sustainable Development},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18303836},
author = {Tomasz Janowski and Elsa Estevez and Rehema Baguma},
keywords = {Public governance, Platform governance, Digital government, Sustainable development, Citizen-administration relationships},
abstract = {Changing governance paradigms has been shaping and reshaping the landscape of citizen-administration relationships, from impartial application of rules and regulations by administration to exercise its authority over citizens (bureaucratic paradigm), through provision of public services by administration to fulfil the needs of citizens (consumerist paradigm), to responsibility-sharing between administration and citizens for policy and service processes (participatory paradigm). The recent trend is the administration empowering citizens to create public value by themselves, through socio-technical systems that bring data, services, technologies and people together to respond to changing societal needs. Such systems are called “platforms” and the trend is called “platform paradigm”. The aim of this article is to offer a conceptual framework for citizen-administration relationships under the platform paradigm. While existing models of citizen-administration relationships mainly focus on specific types of relationships, e.g. citizen trust versus administrative transparency, or citizen satisfaction versus administrative performance, the proposed framework identifies a comprehensive set of relationships that explain how decisions by citizens or administration and the policy environment mutually agreed by them contribute to shaping such relationships and building individual and collective capacity for pursuing sustainable development. The framework comprises 15 types of relationships organized along the four governance paradigms. It is illustrated through the analysis of 11 case studies published in the current issue. Based on this analysis, the article also formulates some insights that are relevant to researchers and policymakers who intend to utilize platform governance for sustainable development.}
}
@article{PATCHELL2018941,
title = {Can the implications of the GHG Protocol's scope 3 standard be realized?},
journal = {Journal of Cleaner Production},
volume = {185},
pages = {941-958},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618306528},
author = {Jerry Patchell},
keywords = {GHG protocol scope 3, CDP, Transaction costs, Power, measurement and management},
abstract = {The GHG Protocol has become the standard for corporate reporting of greenhouse gases, adopted by governments for regulations, NGOs for accountability and corporations for compliance. The GHG Protocol's scope 1 and 2 have largely succeeded in gaining compliance from large firms to report their internal GHG emissions and those from electricity purchases. Achieving Scope 3's intent of a full audit of value chain emissions GHG, however, is a much more complicated affair and according to the CDP, scope 3 is much less successful. This lack of success challenges the premise and purpose of the standard, especially, the expectation that the power of MNCs can be used to leverage reporting and reductions through the value chain. This paper constructs a heuristic framework to explain why success has been limited. The paper discusses six interdependent factors that inhibit scope 3's ambition of promoting the measurement and management of GHG emissions throughout the value chain. These factors are transaction costs, power, responsibility allocation, uncertainty, location contingency and production costs. The impact of these factors on likelihood of compliance to the scope 3 are revealed by an examination of what the sustainable supply chain management, the supply chain management and other literature tell us about value chain interactions on environmental performance. The weight of these factors cast doubt on scope 3's ambition to compel firms to report a full audit of their scope 3 emissions. Moreover, the pursuit of that ambition diverts corporate efforts from more efficient and effective environmental efforts. The paper concludes with a discussion of options for harnessing the power of the lifecycle approach to reforming the value chain.}
}
@article{AVCI2018390,
title = {Managing electricity price modeling risk via ensemble forecasting: The case of Turkey},
journal = {Energy Policy},
volume = {123},
pages = {390-403},
year = {2018},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2018.08.053},
url = {https://www.sciencedirect.com/science/article/pii/S0301421518305834},
author = {Ezgi Avci and Wolfgang Ketter and Eric {van Heck}},
keywords = {Electricity markets, Day ahead auctions, Risk of price modeling, Ensemble forecasting, Multi-seasonality, Exogenous variable selection, Semi-transparent market},
abstract = {There are two ways of managing market price risk in electricity day ahead markets, forecasting and hedging. In emerging markets, since hedging possibilities are limited, forecasting becomes the foremost important tool to manage spot price risk. Despite the existence of great diversity of spot price forecasting methods, due to the unique characteristics of electricity as a commodity, there are still three key forecasting challenges that a market participant has to take into account: risk of selection of an inadequate forecasting method and transparency level of the market (availability level of public data) and country-specific multi-seasonality factors. We address these challenges by using a detailed market-level data from the Turkish electricity day-ahead auctions, which is an interesting research setting in that it presents a number of challenges for forecasting. We reveal the key distinguishing features of this market quantitatively which then allow us to propose individual and ensemble forecasting models that are particularly well suited to it. This forecasting study is pioneering for Turkey as it is the very first to focus specifically on electricity spot prices since the country's day-ahead market was established in 2012. We also suggested applicable policy and managerial implications for both regulatory bodies, market makers and participants.}
}
@article{KARIMI201739,
title = {Online review helpfulness: Impact of reviewer profile image},
journal = {Decision Support Systems},
volume = {96},
pages = {39-48},
year = {2017},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2017.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167923617300179},
author = {Sahar Karimi and Fang Wang},
keywords = {Online reviews, Review helpfulness, Reviewer image, Reviewer profile image, Identity information, Decorative images},
abstract = {Despite the growing number of studies on online reviews, the impact of visual cues on consumer's evaluation of review helpfulness has remained underexplored. It is not yet known whether and how images influence the way online reviews are perceived. This paper introduces and empirically examines the potential effects of reviewer profile image, a photo/image displayed next to the reviewer name, on review helpfulness by drawing on the decorative and information functions of images. With a sample of 2178 reviews from mobile gaming applications, we report that reviewer profile image can significantly enhance consumer's evaluation of review helpfulness; whereas there is no differential effect among image types (i.e. self, family, or random images). Interestingly, the effect of reviewer profile image on review helpfulness is moderated by review length, but not review valence and equivocality. Results suggest that reviewer profile image enhances the perception of review helpfulness by serving mainly as a visual decoration that creates affective responses rather than identity information.}
}
@article{KAMISALIC2018207,
title = {Formalization and acquisition of temporal knowledge for decision support in medical processes},
journal = {Computer Methods and Programs in Biomedicine},
volume = {158},
pages = {207-228},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717309756},
author = {Aida Kamišalić and David Riaño and Tatjana Welzer},
keywords = {Temporal knowledge representation, Time modelling, Time constraints generation, Decision support, Medical procedural knowledge, Clinical practice guidelines, Cardiovascular diseases},
abstract = {Background: In medical practice, long term interventions are common and they require timely planning of the involved processes. Unfortunately, evidence-based statements about time are hard to find in Clinical Practice Guidelines (CPGs) and in other sources of medical knowledge. At the same time, health care centers use medical records and information systems to register data about clinical processes and patients, including time information about the encounters, prescriptions, and other clinical actions. Consequently, medical records and health care information systems are promising sources of data from which we can detect temporal medical knowledge. Objective: The objectives were to (1) Analyze and classify the sorts of time constraints in medical processes, (2) Propose a formalism to represent these sorts of clinical time constraints, (3) Use these formalisms to enable the automatic generation of temporal models from clinical data, and (4) Study the adherence of these intervention models to CPG recommendations. Methods: In order to achieve these objectives, we carried out four studies: The identification of the sort of times involved in the long-term diagnostic and therapeutic medical procedures of fifty patients, the supervision of the indications about time contained in six CPGs on chronic diseases, the study of the time structures of two standard data models, as well as ten languages to computerize CPGs. Based on the provided studies, we synthesized two representation formalisms: Micro- and macro-temporality. We developed three algorithms for automatic generation of generalized time constraints in the form of micro- and macro-temporalities from clinical databases, which were double tested. Results: A full classification of time constraints for medical procedures is proposed. Two formalisms called micro- and macro-temporality are introduced and validated to represent these time constraints. Time constraints were generated automatically from the data about 8781 Arterial Hypertension (AH) patients. The generated macro-temporalities restricted visits to be between 1–7 weeks, whereas CPGs recommend 2–4 weeks. Micro-temporal constraints on drug-dosage therapies distinguished between the initial dosage and the target dosage, with visits every 1–6 weeks, and 2–5 months, respectively. Our algorithms obtained semi-complete maps of dosage increments and the maximum dosages for 7 drug types. Data-based time limits for lifestyle change counsels and blood pressure (BP) check-ups were fixed to 6 and 3 months, for patients with low- and high-BP, respectively, when CPGs specify a general 3–6 month range. Conclusions: Experience-based temporal knowledge detected using our algorithms complements the evidence-based knowledge about clinical procedures contained in the CPGs. Our temporal model is simple and highly descriptive when dealing with general or specific time constraints’ representations, offering temporal knowledge representation of varying detail. Therefore, it is capable of capturing all the temporal knowledge we can find in medical procedures, when dealing with chronic diseases. With our model and algorithms, an adherence analysis emerges naturally to detect CPG-compliant interventions, but also deviations whose causes and possible rationales can call into question CPG recommendations (e.g., our analysis of AH patients showed that the time between visits recommended by CPGs were too long for a proper drug therapy decision, dosage titration, or general follow-up).}
}
@incollection{GROOT201719,
title = {Chapter 2 - Taxonomy of Financial Data},
editor = {Martijn Groot},
booktitle = {A Primer in Financial Data Management},
publisher = {Academic Press},
pages = {19-64},
year = {2017},
isbn = {978-0-12-809776-2},
doi = {https://doi.org/10.1016/B978-0-12-809776-2.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097762000028},
author = {Martijn Groot},
keywords = {financial data, classification, data types, taxonomy},
abstract = {This chapter presents different ways to classify financial information. It introduces a data model to organize financial information and analyzes financial data in terms of master data versus transactional data and structured versus unstructured data. The chapter presents a categorization of different types of data sources from public, business relationship, commercial, and internal sources and the implications of these different sources for data integration. The second half of the chapter provides a taxonomy of the different data categories used in financial services—covering all master data types (instruments, clients, trade reference data, tax), transactional data types, research, analytics, and regulatory information.}
}
@article{DONG20181,
title = {Secure partial encryption with adversarial functional dependency constraints in the database-as-a-service model},
journal = {Data & Knowledge Engineering},
volume = {116},
pages = {1-20},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X16300520},
author = {Boxiang Dong and Hui (Wendy) Wang},
keywords = {Database-as-a-service, Data outsourcing, Security, integrity, and protection, Database management, Management of integrity constraints},
abstract = {Cloud computing enables end-users to outsource their dataset and data management needs to a third-party service provider. One of the major security concerns of the outsourcing paradigm is how to protect sensitive information in the outsourced dataset. In some applications, only partial values are considered sensitive. In general, the sensitive information can be protected by encryption. However, data dependency constraints (together with the unencrypted data) in the outsourced data may serve as adversary knowledge and bring security vulnerabilities to the encrypted data. In this paper, we focus on functional dependency (FD), an important type of data dependency constraints, and study the security threats by the adversarial FDs. We design a practical scheme that can defend against the FD attack by encrypting a small amount of non-sensitive data (encryption overhead). We prove that finding the scheme that leads to the optimal encryption overhead is NP-complete, and design efficient heuristic algorithms, under the presence of one or multiple FDs. We design a secure query rewriting scheme that enables the service provider to answer various types of queries on the encrypted data with provable security guarantee. We extend our study to enforce security when there are conditional functional dependencies (CFDs) and data updates. We conduct an extensive set of experiments on two real-world datasets. The experiment results show that our heuristic approach brings small amounts of encryption overhead (at most 1% more than the optimal overhead), and enjoys a 10-time speedup compared with the optimal solution. Besides, our approach can reduce up to 90% of the encryption overhead of state-of-the-art solution.}
}
@incollection{CARVAJAL2018149,
title = {Chapter Five - Workflow Automation and Intelligent Control},
editor = {Gustavo Carvajal and Marko Maucec and Stan Cullick},
booktitle = {Intelligent Digital Oil and Gas Fields},
publisher = {Gulf Professional Publishing},
address = {Boston},
pages = {149-195},
year = {2018},
isbn = {978-0-12-804642-5},
doi = {https://doi.org/10.1016/B978-0-12-804642-5.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046425000050},
author = {Gustavo Carvajal and Marko Maucec and Stan Cullick},
keywords = {Workflow automation, Production control, Virtual metering, User interface},
abstract = {The previous two chapters discussed how we must manage, condition, and analyze all the data gathered in digital oil field (DOF) operations. Next, we look at how the data can be used to automate workflows and provide intelligent control of equipment used in oilfield operations and DOF systems. This chapter introduces concepts associated with process control and then explains how process control is used in the context of automated DOF workflows. The subsequent sections describe these engineering components of automated DOF workflows: virtual multiphase metering, smart production surveillance, well-test validation, and well diagnostics. As a product of the automated workflows, advisories and tracking actions are generated for engineers.}
}
@incollection{MCGREGOR201821,
title = {2 - Architectures of Transportation Cyber-Physical Systems},
editor = {Lipika Deka and Mashrur Chowdhury},
booktitle = {Transportation Cyber-Physical Systems},
publisher = {Elsevier},
pages = {21-49},
year = {2018},
isbn = {978-0-12-814295-0},
doi = {https://doi.org/10.1016/B978-0-12-814295-0.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128142950000022},
author = {John D. McGregor and Roselane S. Silva and Eduardo S. Almeida},
keywords = {AADL, Cyber-physical systems, Internet of things, NIST, Quality attributes, System architecture, Transportation engineers},
abstract = {Architecture is the fundamental structure of a system. That structure is based on the relationships among the modules that provide the behaviour of the system. These structures appear, with variations, in many systems that address related problems. Because software is so malleable, much of this variation is implemented in the software portion of the product. These different architectures have different performance characteristics such as different levels of reliability and safety. In this chapter we will survey some of the popular architectures for cyber-physical systems, the quality attributes enhanced and degraded by each architecture and analysis techniques that are used to evaluate these qualities in the context of actual applications. Our intent is to provide the type of knowledge needed for the transportation engineer to participate in developing or acquiring software for smart transportation systems.}
}
@article{SHIN201712,
title = {Energy efficiency of milling machining: Component modeling and online optimization of cutting parameters},
journal = {Journal of Cleaner Production},
volume = {161},
pages = {12-29},
year = {2017},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617309381},
author = {Seung-Jun Shin and Jungyub Woo and Sudarsan Rachuri},
keywords = {Metal cutting, Predictive modeling, Optimization, Energy efficiency, STEP-NC, MTConnect},
abstract = {Energy consumption is a major sustainability focus in the metal cutting industry. As a result, process planning is increasingly concerned with reducing energy consumption in machine tools. The relevant literature has been categorized into two research areas. The first includes energy prediction models, which characterize the relationships between cutting parameters – the main outputs of process planning - and energy consumption. The second involves energy-consumption optimization, which uses the prediction models to find the cutting parameters that minimize energy use. However, previous energy prediction models are limited to predict energy for tool paths coded in a Numerical Control (NC) program. Previous energy optimization methods typically do not use online optimization, which enables fast optimization decision-making for supporting on-demand process planning and real-time machine control. This paper presents a component-based energy-modeling methodology to implement the online optimization needed for real-time control. Models that can predict energy up to the tool path-level at specific machining configurations are called component-models in this paper. These component-models are created using historical data that includes process plans, NC programs, and machine-monitoring data. The online optimization is implemented using a dynamic composition of component-models together with a divide-and-conquer technique. The feasibility and effectiveness of our methodology has been demonstrated in a milling-machine example.}
}
@incollection{WITTEN2017161,
title = {Chapter 5 - Credibility: Evaluating what’s been learned},
editor = {Ian H. Witten and Eibe Frank and Mark A. Hall and Christopher J. Pal},
booktitle = {Data Mining (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {161-203},
year = {2017},
isbn = {978-0-12-804291-5},
doi = {https://doi.org/10.1016/B978-0-12-804291-5.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042915000052},
author = {Ian H. Witten and Eibe Frank and Mark A. Hall and Christopher J. Pal},
keywords = {Training set, test set, cross-validation, bootstrap, paired -test, quadratic and informational loss, misclassification costs, performance charts, model selection, minimum description length principle},
abstract = {The success of machine learning in practical applications hinges on proper evaluation. This section discusses how the quality of predictions can be measured reliably. We consider the basic train-test setup for estimating predictive accuracy, before moving on to more sophisticated variants known as “cross-validation” and the “bootstrap” method. We also discuss the importance of proper parameter tuning when applying and evaluating machine learning, and explain how to use statistical significance tests when comparing the performance of two learning algorithms in a particular application domain. As well as basic classification accuracy, we consider other measures for evaluating the quality of probability estimates, learning and prediction with misclassification costs, and measures for evaluating numeric prediction schemes. The final section discusses model selection, which is the process of determining an appropriate model complexity, using the compression-based minimum description length principle, on the one hand, and evaluation on a validation set, on the other.}
}
@article{HARI20181720,
title = {IFCN-endorsed practical guidelines for clinical magnetoencephalography (MEG)},
journal = {Clinical Neurophysiology},
volume = {129},
number = {8},
pages = {1720-1747},
year = {2018},
issn = {1388-2457},
doi = {https://doi.org/10.1016/j.clinph.2018.03.042},
url = {https://www.sciencedirect.com/science/article/pii/S1388245718306576},
author = {Riitta Hari and Sylvain Baillet and Gareth Barnes and Richard Burgess and Nina Forss and Joachim Gross and Matti Hämäläinen and Ole Jensen and Ryusuke Kakigi and François Mauguière and Nobukatzu Nakasato and Aina Puce and Gian-Luca Romani and Alfons Schnitzler and Samu Taulu},
keywords = {Magnetoencephalography, Electroencephalography, Clinical neurophysiology, Evoked and event-related responses, Transient and steady-state responses, Spontaneous brain activity, Neural oscillations, Analysis and interpretation, Artifacts, Source modeling, Epilepsy, Preoperative evaluation, Stroke, Pain, Traumatic brain injury, Parkinson’s disease, Hepatic encephalopathy, Alzheimer’s disease and dementia, Neuropsychiatric disorders, Brain maturation and development, Dyslexia, Guidelines},
abstract = {Magnetoencephalography (MEG) records weak magnetic fields outside the human head and thereby provides millisecond-accurate information about neuronal currents supporting human brain function. MEG and electroencephalography (EEG) are closely related complementary methods and should be interpreted together whenever possible. This manuscript covers the basic physical and physiological principles of MEG and discusses the main aspects of state-of-the-art MEG data analysis. We provide guidelines for best practices of patient preparation, stimulus presentation, MEG data collection and analysis, as well as for MEG interpretation in routine clinical examinations. In 2017, about 200 whole-scalp MEG devices were in operation worldwide, many of them located in clinical environments. Yet, the established clinical indications for MEG examinations remain few, mainly restricted to the diagnostics of epilepsy and to preoperative functional evaluation of neurosurgical patients. We are confident that the extensive ongoing basic MEG research indicates potential for the evaluation of neurological and psychiatric syndromes, developmental disorders, and the integrity of cortical brain networks after stroke. Basic and clinical research is, thus, paving way for new clinical applications to be identified by an increasing number of practitioners of MEG.}
}
@article{ANDERMANN20181322,
title = {The Microbiome and Hematopoietic Cell Transplantation: Past, Present, and Future},
journal = {Biology of Blood and Marrow Transplantation},
volume = {24},
number = {7},
pages = {1322-1340},
year = {2018},
issn = {1083-8791},
doi = {https://doi.org/10.1016/j.bbmt.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1083879118300879},
author = {Tessa M. Andermann and Jonathan U. Peled and Christine Ho and Pavan Reddy and Marcie Riches and Rainer Storb and Takanori Teshima and Marcel R.M. {van den Brink} and Amin Alousi and Sophia Balderman and Patrizia Chiusolo and William B. Clark and Ernst Holler and Alan Howard and Leslie S. Kean and Andrew Y. Koh and Philip L. McCarthy and John M. McCarty and Mohamad Mohty and Ryotaro Nakamura and Katy Rezvani and Brahm H. Segal and Bronwen E. Shaw and Elizabeth J. Shpall and Anthony D. Sung and Daniela Weber and Jennifer Whangbo and John R. Wingard and William A. Wood and Miguel-Angel Perales and Robert R. Jenq and Ami S. Bhatt},
keywords = {Microbiome, Microbiota, Hematopoietic stem cell transplantation, Metagenomics, Prebiotic, Fecal microbiota transplant}
}
@article{WERNER201757,
title = {Financial process mining - Accounting data structure dependent control flow inference},
journal = {International Journal of Accounting Information Systems},
volume = {25},
pages = {57-80},
year = {2017},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1467089516300264},
author = {Michael Werner},
keywords = {Process mining, Financial audits, Journal entries, Business process intelligence, Business process modeling, Control flow inference, Design science research, Enterprise resource planning systems},
abstract = {The increasing integration of computer technology for the processing of business transactions and the growing amount of financially relevant data in organizations create new challenges for external auditors. The availability of digital data opens up new opportunities for innovative audit procedures. Process mining can be used as a novel data analysis technique to support auditors in this context. Process mining algorithms produce process models by analyzing recorded event logs. Contemporary general purpose mining algorithms commonly use the temporal order of recorded events for determining the control flow in mined process models. The presented research shows how data dependencies related to the accounting structure of recorded events can be used as an alternative to the temporal order of events for discovering the control flow. The generated models provide accurate information on the control flow from an accounting perspective and show a lower complexity compared to those generated using timestamp dependencies. The presented research follows a design science research approach and uses three different real world data sets for evaluation purposes.}
}
@incollection{VALLERO2018251,
title = {Chapter 9 - Environmental Data Analysis},
editor = {Daniel Vallero},
booktitle = {Translating Diverse Environmental Data into Reliable Information},
publisher = {Academic Press},
pages = {251-296},
year = {2018},
isbn = {978-0-12-812446-8},
doi = {https://doi.org/10.1016/B978-0-12-812446-8.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124468000098},
author = {Daniel Vallero},
keywords = {Data interpretation, Data reduction, Dose–response curve, Environmental decision making, No observed adverse effect level (NOAEL), Reference dose (RfD), Reliability, Risk, Safety, Thresholds, Toxicity measurement, Uncertainty},
abstract = {After data and information are found, they must be properly fitted to the knowledgebase. Indeed, before searching, the user must have a good idea as to why the data gathering is being done. Given the volume and diversity of data, this chapter provides approaches for matching the user needs with data of appropriate quality and representativeness. In addition, the chapter discusses various methods for interpreting and displaying information honed from these datasets.}
}
@article{KANARACHOS2018867,
title = {Smartphones as an integrated platform for monitoring driver behaviour: The role of sensor fusion and connectivity},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {95},
pages = {867-882},
year = {2018},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2018.03.023},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18303954},
author = {Stratis Kanarachos and Stavros-Richard G. Christopoulos and Alexander Chroneos},
keywords = {Smartphones, Driver behaviour, Sensor fusion, Connectivity, Cybernetics, Crowd-sensing},
abstract = {Nowadays, more than half of the world’s web traffic comes from mobile phones, and by 2020 approximately 70 percent of the world’s population will be using smartphones. The unprecedented market penetration of smartphones combined with the connectivity and embedded sensing capability of smartphones is an enabler for the large-scale deployment of Intelligent Transportation Systems (ITS). On the downside, smartphones have inherent limitations such as relatively limited energy capacity, processing power, and accuracy. These shortcomings may potentially limit their role as an integrated platform for monitoring driver behaviour in the context of ITS. This study examines this hypothesis by reviewing recent scientific contributions. The Cybernetics theoretical framework was employed to allow a systematic comparison. First, only a few studies consider the smartphone as an integrated platform. Second, a lack of consistency between the approaches and metrics used in the literature is noted. Last but not least, areas such as fusion of heterogeneous information sources, Deep Learning and sparse crowd-sensing are identified as relatively unexplored, and future research in these directions is suggested.}
}
@incollection{PETERS2017155,
title = {Chapter 9 - Advanced Personal Genome Sequencing as the Ultimate Diagnostic Test},
editor = {George P. Patrinos},
booktitle = {Molecular Diagnostics (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {155-172},
year = {2017},
isbn = {978-0-12-802971-8},
doi = {https://doi.org/10.1016/B978-0-12-802971-8.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128029718000092},
author = {B.A. Peters and S. Drmanac and J.S. Liu and X. Xun and R. Drmanac},
keywords = {Co-barcoding, DNA nanoballs, Genetic disease, Genome interpretation, Long fragment read technology, Perfect genome, Virtual health coaching, Whole-genome sequencing},
abstract = {In this chapter we provide a brief history of whole-genome sequencing (WGS) and the field of molecular diagnostics. Next, we focus on some important areas for improvement in WGS and what is being done. We explore some areas where WGS is being used and why it makes sense to sequence the whole human genome as the ultimate molecular diagnostic. Finally, we close with what kind of infrastructure will be necessary for population-scale WGS, what tools will be needed to understand and use that information fully, and how having it can benefit each of us in many aspects of our lives.}
}
@article{HOUBORG2018211,
title = {A Cubesat enabled Spatio-Temporal Enhancement Method (CESTEM) utilizing Planet, Landsat and MODIS data},
journal = {Remote Sensing of Environment},
volume = {209},
pages = {211-226},
year = {2018},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2018.02.067},
url = {https://www.sciencedirect.com/science/article/pii/S0034425718300786},
author = {Rasmus Houborg and Matthew F. McCabe},
keywords = {CubeSat, Planet, Landsat, MODIS, Cubist, Machine-learning, VNIR, Spatio-temporal enhancement},
abstract = {Satellite sensing in the visible to near-infrared (VNIR) domain has been the backbone of land surface monitoring and characterization for more than four decades. However, a limitation of conventional single-sensor satellite missions is their limited capacity to observe land surface dynamics at the very high spatial and temporal resolutions demanded by a wide range of applications. One solution to this spatio-temporal divide is an observation strategy based on the CubeSat standard, which facilitates constellations of small, inexpensive satellites. Repeatable near-daily image capture in RGB and near-infrared (NIR) bands at 3–4 m resolution has recently become available via a constellation of >130 CubeSats operated commercially by Planet. While the observing capacity afforded by this system is unprecedented, the relatively low radiometric quality and cross-sensor inconsistencies represent key challenges in the realization of their full potential as a game changer in Earth observation. To address this issue, we developed a Cubesat Enabled Spatio-Temporal Enhancement Method (CESTEM) that uses a multi-scale machine-learning technique to correct for radiometric inconsistencies between CubeSat acquisitions. The CESTEM produces Landsat 8 consistent atmospherically corrected surface reflectances in blue, green, red, and NIR bands, but at the spatial scale and temporal frequency of the CubeSat observations. An application of CESTEM over an agricultural dryland system in Saudi Arabia demonstrated CubeSat-based reproduction of Landsat 8 consistent VNIR data with an overall relative mean absolute deviation of 1.6% or better, even when the Landsat 8 and CubeSat acquisitions were temporally displaced by >32 days. The consistently high retrieval accuracies were achieved using a multi-scale target sampling scheme that draws Landsat 8 reference data from a series of scenes by using MODIS-consistent surface reflectance time series to quantify relative changes in Landsat-scale reflectances over given Landsat-CubeSat acquisition timespans. With the observing potential of Planet's CubeSats approaching daily nadir-pointing land surface imaging of the entire Earth, CESTEM offers the capacity to produce daily Landsat 8 consistent VNIR imagery with a factor of 10 increase in spatial resolution and with the radiometric quality of actual Landsat 8 observations. Realization of this unprecedented Earth observing capacity has far reaching implications for the monitoring and characterization of terrestrial systems at the precision scale.}
}
@article{LIN2018323,
title = {Quantifying uncertainty in short-term traffic prediction and its application to optimal staffing plan development},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {92},
pages = {323-348},
year = {2018},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18306545},
author = {Lei Lin and John C. Handley and Yiming Gu and Lei Zhu and Xuejin Wen and Adel W. Sadek},
keywords = {PSO-ELM, GARCH, Kalman filter, Prediction interval, Reliability, Sharpness, Staffing plan, Waiting time, Uncertainty},
abstract = {In this paper, we aim to quantify uncertainty in short-term traffic volume prediction by enhancing a hybrid machine learning model based on Particle Swarm Optimization (PSO) and Extreme Learning Machine (ELM) neural network. Different from the previous studies, the PSO-ELM models require no statistical inference nor distribution assumption of the model parameters, but rather focus on generating the prediction intervals (PIs) that can minimize a multi-objective function which considers two criteria, reliability and interval sharpness. The improved PSO-ELM models are developed for an hourly border crossing traffic dataset and compared to: (1) the original PSO-ELMs; (2) two state of the art models proposed by Zhang et al. (2014) and Guo et al. (2014) separately; and (3) the traditional ARMA and Kalman filter models. The results show that the improved PSO-ELM can always keep the mean PI length the lowest, and guarantee that the PI coverage probability is higher than the corresponding PI nominal confidence, regardless of the confidence level assumed. The study also probes the reasons that led to a few points being not covered by the PIs of PSO-ELMs. Finally, the study proposes a comprehensive optimization framework to make staffing plans for border crossing authority based on bounds of PIs and point predictions. The results show that for holidays, the staffing plans based on PI upper bounds generated much lower total system costs, and that those plans derived from PI upper bounds of the improved PSO-ELM models, are capable of producing the lowest average waiting times at the border. For a weekday or a typical Monday, the workforce plans based on point predictions from Zhang et al. (2014) and Guo et al. (2014) models generated the smallest system costs with low border crossing delays. Moreover, for both holiday and normal Monday scenarios, if the border crossing authority lacked the required staff to implement the plans based on PI upper bounds or point predictions, the staffing plans based on PI lower bounds from the improved PSO-ELMs performed the best, with an acceptable level of service and total system costs close to the point prediction plans.}
}
@article{SOVACOOL201812,
title = {Promoting novelty, rigor, and style in energy social science: Towards codes of practice for appropriate methods and research design},
journal = {Energy Research & Social Science},
volume = {45},
pages = {12-42},
year = {2018},
note = {Special Issue on the Problems of Methods in Climate and Energy Research},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2018.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S2214629618307230},
author = {Benjamin K. Sovacool and Jonn Axsen and Steve Sorrell},
keywords = {Validity, Research methods, Research methodology, Interdisciplinary research, Research excellence},
abstract = {A series of weaknesses in creativity, research design, and quality of writing continue to handicap energy social science. Many studies ask uninteresting research questions, make only marginal contributions, and lack innovative methods or application to theory. Many studies also have no explicit research design, lack rigor, or suffer from mangled structure and poor quality of writing. To help remedy these shortcomings, this Review offers suggestions for how to construct research questions; thoughtfully engage with concepts; state objectives; and appropriately select research methods. Then, the Review offers suggestions for enhancing theoretical, methodological, and empirical novelty. In terms of rigor, codes of practice are presented across seven method categories: experiments, literature reviews, data collection, data analysis, quantitative energy modeling, qualitative analysis, and case studies. We also recommend that researchers beware of hierarchies of evidence utilized in some disciplines, and that researchers place more emphasis on balance and appropriateness in research design. In terms of style, we offer tips regarding macro and microstructure and analysis, as well as coherent writing. Our hope is that this Review will inspire more interesting, robust, multi-method, comparative, interdisciplinary and impactful research that will accelerate the contribution that energy social science can make to both theory and practice.}
}
@article{AUSLOOS2017238,
title = {Data science for assessing possible tax income manipulation: The case of Italy},
journal = {Chaos, Solitons & Fractals},
volume = {104},
pages = {238-256},
year = {2017},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2017.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0960077917303314},
author = {Marcel Ausloos and Roy Cerqueti and Tariq A. Mir},
keywords = {Data science, Benford law, Aggregated income tax, Data manipulation, Italy},
abstract = {This paper explores a real-world fundamental theme under a data science perspective. It specifically discusses whether fraud or manipulation can be observed in and from municipality income tax size distributions, through their aggregation from citizen fiscal reports. The study case pertains to official data obtained from the Italian Ministry of Economics and Finance over the period 2007–2011. All Italian (20) regions are considered. The considered data science approach concretizes in the adoption of the Benford first digit law as quantitative tool. Marked disparities are found, - for several regions, leading to unexpected “conclusions”. The most eye browsing regions are not the expected ones according to classical imagination about Italy financial shadow matters.}
}
@article{2018XXVI,
title = {Keyword Index},
journal = {Toxicology Letters},
volume = {295},
pages = {XXVI-XLIII},
year = {2018},
note = {ABSTRACTS of the 54th Congress of the European Societies of Toxicology (EUROTOX 2018) TOXICOLOGY OUT OF THE BOX Brussels, Belgium, 2nd – 5th of September, 2018},
issn = {0378-4274},
doi = {https://doi.org/10.1016/S0378-4274(18)31785-5},
url = {https://www.sciencedirect.com/science/article/pii/S0378427418317855}
}
@incollection{MULLANE20181,
title = {Chapter 1 - Reproducibility in Biomedical Research},
editor = {Michael Williams and Michael J. Curtis and Kevin Mullane},
booktitle = {Research in the Biomedical Sciences},
publisher = {Academic Press},
pages = {1-66},
year = {2018},
isbn = {978-0-12-804725-5},
doi = {https://doi.org/10.1016/B978-0-12-804725-5.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012804725500001X},
author = {Kevin Mullane and Michael J. Curtis and Michael Williams},
keywords = {reproducibility, fraud, drug discovery, animal models, translational research},
abstract = {The biomedical research ecosystem—academic, federal, and industrial—is dedicated to understanding human disease causality and developing effective therapeutics. The successes of this system over the past century have resulted in many effective drugs leading society to expect an endless supply of new therapeutics, particularly for chronic diseases that lack effective treatments (e.g., Alzheimer’s, diabetes). An increased recognition that the quality, relevance, reliability, and reproducibility of biomedical research cannot be guaranteed despite self-correction and peer review, coupled with the incessant hyping of inconsequential data by authors and their institutions, has led to a situation that is incompatible with sustained success in drug discovery. This risks the loss of credibility with the public, which does not bode well for the future of biomedical research. This chapter outlines the reproducibility problem and its magnitude, setting the stage for potential solutions that are discussed elsewhere in this monograph.}
}
@article{YANG2017344,
title = {Efficient traffic congestion estimation using multiple spatio-temporal properties},
journal = {Neurocomputing},
volume = {267},
pages = {344-353},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S092523121731072X},
author = {Yongjian Yang and Yuanbo Xu and Jiayu Han and En Wang and Weitong Chen and Lin Yue},
keywords = {Traffic congestion estimation, Large-scale road networks, Multiple spatio-temporal properties, Dynamic weight calculation, GPS data},
abstract = {Traffic estimation is an important issue to analyze the traffic congestion in large-scale urban traffic situations. Recently, many researchers have used GPS data to estimate traffic congestion. However, how to fuse the multiple data reasonably and guarantee the accuracy and efficiency of these methods are still challenging problems. In this paper, we propose a novel method Multiple Data Estimation (MDE) to estimate the congestion status in urban environment with GPS trajectory data efficiently, where we estimate the congestion status of the area through utilizing multiple properties, including density, velocity, inflow and previous status. Among them, traffic inflow and previous status (combination of time and space factors) are not both used in other existing methods. In order to ensure the accuracy and efficiency, we apply dynamic weights of data and parameters in MDE method. To evaluate our methods, we apply it on large-scale taxi GPS data of Beijing and Shanghai. Extensive experiments on these two real-world datasets demonstrate the significant improvements of our method over several state-of-the-art methods.}
}
@article{GERHARDT201811,
title = {An Investigation to Identify Factors that Lead to Delay in Healthcare Reimbursement Process: A Brazilian case},
journal = {Big Data Research},
volume = {13},
pages = {11-20},
year = {2018},
note = {Big Medical/Healthcare Data Analytics},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2018.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214579617303039},
author = {Ricardo Gerhardt and João F. Valiati and José Vicente {Canto dos Santos}},
keywords = {Healthcare reimbursement, Process mining, Data mining},
abstract = {Healthcare reimbursement has had a tremendous impact on healthcare institutions and the economy. The healthcare reimbursement process consists of coding, billing, and payment based on the care provided to the patient. The rapid development of new medical treatments and procedures and changes in regulations and policies have been increasing the complexity of the reimbursement process, resulting in financial, operational, and care delivery issues for healthcare institutions. Therefore, methods of process analysis, such as process mining, have been used as a basic strategy to improve the organizational effectiveness of healthcare institutions. In this context, the main objective of this study is to propose an approach to investigate the factors that may cause delays in the reimbursement process using a combination of process mining and data mining techniques to extract information from the process data and support decision-making. To accomplish this analysis, process mining is applied to map the reimbursement process from the event log and to determine possible bottlenecks. In contrast, data mining is used to identify frequent patterns and interesting associations in the process data. Finally, by applying the proposed approach to a real case of a healthcare institution in Brazil, we extracted valuable insights regarding process execution and confirmed the effectiveness and potential of combining process mining and the association rules mining techniques.}
}
@incollection{TSETSOS2019363,
title = {Genetics and Population Analysis},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {363-378},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20114-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338201143},
author = {Fotis Tsetsos and Petros Drineas and Peristera Paschou},
keywords = {Admixture, Disease, Genetics, Genomics, Genotypes, Haplotypes, History, Populations, Variants},
abstract = {Population genetics, the systematic study of patterns of genetic variation, has been undergoing an unprecedented, revolutionary phase in the recent years. The advances of modern technology have enabled the rapid and accurate mass-scale output of modern and ancient genetic data. In this article, we delve into the state-of-the-art methods utilized for computational genetic analysis, the knowledge base required for crafting analytical protocols to avoid biased data, along with important considerations for the proper assessment and interpretation of the data, the methods and their output. We also present illustrative examples of population genetics in the exploration of the human past, as well as its applications in disease mapping and association studies.}
}