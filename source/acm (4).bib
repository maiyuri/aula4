@inproceedings{10.1145/3424771.3424821,
author = {Zimmermann, Olaf and Pautasso, Cesare and L\"{u}bke, Daniel and Zdun, Uwe and Stocker, Mirko},
title = {Data-Oriented Interface Responsibility Patterns: Types of Information Holder Resources},
year = {2020},
isbn = {9781450377690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424771.3424821},
doi = {10.1145/3424771.3424821},
abstract = {Remote Application Programming Interfaces (APIs) are used in almost any distributed system today, for instance in microservices-based systems, and are thus enablers for many digitalization efforts. API design not only impacts whether software provided as a service is easy and efficient to develop applications with, but also affects the long term evolution of the software system. In general, APIs are responsible for providing remote and controlled access to the functionality provided as services; however, APIs often are also used to expose and share information. We focus on such data-related aspects of microservice APIs in this paper. Depending on the life cycle of the information published through the API, its mutability and the endpoint role, data-oriented APIs can be designed following patterns such as Operational Data Holder, Master Data Holder, Reference Data Holder, Data Transfer Holder, and Link Lookup Resource. Known uses and examples of the patterns are drawn from public Web APIs as well as application development and integration projects we have been involved in.},
booktitle = {Proceedings of the European Conference on Pattern Languages of Programs 2020},
articleno = {11},
numpages = {25},
location = {Virtual Event, Germany},
series = {EuroPLoP '20}
}

@inproceedings{10.1145/3308558.3313683,
author = {Altenburger, Kristen M. and Ho, Daniel E.},
title = {Is Yelp Actually Cleaning Up the Restaurant Industry? A Re-Analysis on the Relative Usefulness of Consumer Reviews},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313683},
doi = {10.1145/3308558.3313683},
abstract = {Social media provides the government with novel methods to improve regulation. One leading case has been the use of Yelp reviews to target food safety inspections. While previous research on data from Seattle finds that Yelp reviews can predict unhygienic establishments, we provide a more cautionary perspective. First, we show that prior results are sensitive to what we call “Extreme Imbalanced Sampling”: extreme because the dataset was restricted from roughly 13k inspections to a sample of only 612 inspections with only extremely high or low inspection scores, and imbalanced by not accounting for class imbalance in the population. We show that extreme imbalanced sampling is responsible for claims about the power of Yelp information in the original classification setup. Second, a re-analysis that utilizes the full dataset of 13k inspections and models the full inspection score (regression instead of classification) shows that (a) Yelp information has lower predictive power than prior inspection history and (b) Yelp reviews do not significantly improve predictions, given existing information about restaurants and inspection history. Contrary to prior claims, Yelp reviews do not appear to aid regulatory targeting. Third, this case study highlights critical issues when using social media for predictive models in governance and corroborates recent calls for greater transparency and reproducibility in machine learning.},
booktitle = {The World Wide Web Conference},
pages = {2543–2550},
numpages = {8},
keywords = {replication, Yelp, regulation, food safety, consumer reviews},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3428757.3429972,
author = {Draheim, Dirk},
title = {On Architecture of E-Government Ecosystems: From e-Services to e-Participation: [IiWAS'2020 Keynote]},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429972},
doi = {10.1145/3428757.3429972},
abstract = {The "digital transformation" is perceived as the key enabler for increasing wealth and well-being by many in politics, media and among the citizens alike. In the same vein, e-Government steadily received and receives more and more attention. e-Government gives rise to complex, large-scale system landscapes consisting of many players and technological systems - and we call such system landscapes e-Government ecosystems. In this talk, we are interested in the architecture of e-Government ecosystems. "Form ever follows function." Now, what is the function that determines e-Government? And what is the form in which it manifests? After briefly reviewing the purpose of e-Government from a democratic as well as a technocratic viewpoint, we will discover the primacy of the state's institutional design in the architecture of e-Government ecosystems. From there, we will arrive at the notion of data governance architecture, which provides the core of all system design efforts in e-Government. A data governance architecture maps data assets to accountable legal entities and represents the essence of co-designing institutions and technological systems. Against the background of what has been achieved, we review a series of established and emerging technologies that have been explicitly designed for or are otherwise relevant for building e-Government systems.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {3–10},
numpages = {8},
keywords = {e-government, e-governance, public key infrastructures, digital transformation, X-Road, data governance, GAIA-X, persistent messaging, data exchange layers, Fiware, consent management},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@article{10.1145/3009973,
author = {Gotz, David and Sun, Shun and Cao, Nan and Kundu, Rita and Meyer, Anne-Marie},
title = {Adaptive Contextualization Methods for Combating Selection Bias during High-Dimensional Visualization},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3009973},
doi = {10.1145/3009973},
abstract = {Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. This article describes Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. The AC approach (1) monitors and models a user’s visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. This article expands on an earlier article presented at ACM IUI 2016 [16] by providing a more detailed review of the AC methodology and additional evaluation results.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {nov},
articleno = {17},
numpages = {23},
keywords = {visual analytics, selection bias, Visualization, intelligent visual interfaces, exploratory analysis}
}

@inproceedings{10.1145/2896377.2901461,
author = {Wang, Weina and Ying, Lei and Zhang, Junshan},
title = {The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits},
year = {2016},
isbn = {9781450342667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896377.2901461},
doi = {10.1145/2896377.2901461},
abstract = {We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.},
booktitle = {Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science},
pages = {249–260},
numpages = {12},
keywords = {mechanism design, differential privacy, game theory, strategic data subjects, incentive mechanism},
location = {Antibes Juan-les-Pins, France},
series = {SIGMETRICS '16}
}

@article{10.1145/2964791.2901461,
author = {Wang, Weina and Ying, Lei and Zhang, Junshan},
title = {The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2964791.2901461},
doi = {10.1145/2964791.2901461},
abstract = {We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jun},
pages = {249–260},
numpages = {12},
keywords = {game theory, incentive mechanism, mechanism design, differential privacy, strategic data subjects}
}

@inproceedings{10.1145/3291078.3291083,
author = {Zhicheng, Dai and Feng, Liu},
title = {Evaluation of the Smart Campus Information Portal},
year = {2018},
isbn = {9781450365772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291078.3291083},
doi = {10.1145/3291078.3291083},
abstract = {As the internet wave swept the world, "Internet plus education" came into being. Smart campus design and construction has since become a research hotspot. The Campus Information Portal (CIP) plays an increasingly important role in the management of smart campuses. That is why, conducting a comprehensive evaluation study on the construction level of campus information portals is necessary. By combining CIP's own characteristics and incorporating intelligent needs, a comprehensive evaluation index system for CIP was developed. An Analytic Hierarchy Process (AHP) was used to determine index weights, while a Fuzzy Comprehensive Evaluation (FCE) was used to calculate the quantitative scores of the evaluation objects. We selected 10 representative Chinese universities for a comprehensive CIP evaluation and experimental analysis. We analyze the final results of the study, evaluate the validity of our process and methods and finally provide guidance for the construction of a smart campus information portal.},
booktitle = {Proceedings of the 2018 2nd International Conference on Education and E-Learning},
pages = {73–79},
numpages = {7},
keywords = {Campus information portal, Index system, Comprehensive evaluation, Smart campus},
location = {Bali, Indonesia},
series = {ICEEL 2018}
}

@inproceedings{10.1145/3488838.3488873,
author = {Cai, Hongxia and Tan, Qiqi},
title = {Blockchain-Based Data Control for Complex Product Assembly Collaboration Process},
year = {2022},
isbn = {9781450384094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488838.3488873},
doi = {10.1145/3488838.3488873},
abstract = {The collaborative development model of complex products brings the challenge to the data interaction management. There are many manufacturers and suppliers involved in the whole life cycle of the assembly process, which makes it difficult to ensure the data security and traceability. The Hyperledger-Fabric architecture in blockchain technology has modular design, pluggable architecture, complete authority control and security, which can well solve the data security and traceability management in the collaborative development of complex products. Therefore, this paper proposes a framework based on the Hyperledger-Fabric architecture of blockchain for the whole life cycle data management of complex products. We also demonstrate the effectiveness of our proposed new framework integrating blockchain technology through the case of quality data control during the aircraft final assembly collaborative process.},
booktitle = {2021 The 3rd World Symposium on Software Engineering},
pages = {205–209},
numpages = {5},
keywords = {Collaborative Production, Complex product assembly, Blockchain},
location = {Xiamen, China},
series = {WSSE 2021}
}

@inproceedings{10.1145/3336499.3338012,
author = {Maurino, Andrea and Rula, Anisa and von, Bj\o{}rn Marius and Gomez, Mauricio Soto and Elves\ae{}ter, Brian and Roman, Dumitru},
title = {Modelling and Linking Company Data in the EuBusinessGraph Platform},
year = {2019},
isbn = {9781450368230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336499.3338012},
doi = {10.1145/3336499.3338012},
abstract = {In the business environment, knowledge of company data is essential for a variety of tasks. The European funded project euBusinessGraph enables the establishment of a company data platform where data providers and consumers can publish and access company data. The core of the platform is the semantic data model that is the conceptual representation of company data in a common way so that it is easier to share and interlink company data. In this paper we show how the unified model and Grafterizer, a tool for manipulating and transforming raw data into Linked Data, support the linking challenge proposed in FEIII 2019. Results show that geographical enrichment of RDF data supports the interlinking process between company entities in different datasets.},
booktitle = {Proceedings of the 5th Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets},
articleno = {12},
numpages = {6},
keywords = {Company data, Record Linkage, Entity Matching, RDF},
location = {Amsterdam, Netherlands},
series = {DSMM'19}
}

@inproceedings{10.1145/3170427.3170636,
author = {Russell, Daniel M. and Convertino, Gregorio and Kittur, Aniket and Pirolli, Peter and Watkins, Elizabeth Anne},
title = {Sensemaking in a Senseless World: 2018 Workshop Abstract},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3170636},
doi = {10.1145/3170427.3170636},
abstract = {Sensemaking is a common activity in the analysis of a large or complex amount of information. This active area of HCI research asks how DO people come to understand such difficult sets of information? The information workplace is increasing dominated by high velocity, high volume, complex information streams. At the same time, understanding how sensemaking operates has become an urgent need in an era of increasingly unreliable news and information sources. While there has been a huge amount of work in this space, the research involved is scattered over a number of different domains with differing approaches. This workshop will focus on the most recent work in sensemaking, the activities, technologies and behaviors that people do when making sense of their complex information spaces. In the second part of the workshop we will work to synthesize a cross-disciplinary view of how sensemaking works in people, along with the human behaviors, biases, proclivities, and technologies required to support it.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–7},
numpages = {7},
keywords = {sensemaking, information understanding, collaborative work},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/2856767.2856779,
author = {Gotz, David and Sun, Shun and Cao, Nan},
title = {Adaptive Contextualization: Combating Bias During High-Dimensional Visualization and Data Selection},
year = {2016},
isbn = {9781450341370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856767.2856779},
doi = {10.1145/2856767.2856779},
abstract = {Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. In this paper, we present Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. Our approach (1) monitors and models a user's visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. We also share results from a user study which demonstrate the effectiveness of our technique.},
booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
pages = {85–95},
numpages = {11},
keywords = {visualization, intelligent visual interfaces, exploratory analysis, visual analytics},
location = {Sonoma, California, USA},
series = {IUI '16}
}

@inproceedings{10.1145/3448016.3457542,
author = {Li, Guoliang and Zhou, Xuanhe and Cao, Lei},
title = {AI Meets Database: AI4DB and DB4AI},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457542},
doi = {10.1145/3448016.3457542},
abstract = {Database and Artificial Intelligence (AI) can benefit from each other. On one hand, AI can make database more intelligent (AI4DB). For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, learning-based techniques can alleviate this problem. On the other hand, database techniques can optimize AI models (DB4AI). For example, AI is hard to deploy, because it requires developers to write complex codes and train complicated models. Database techniques can be used to reduce the complexity of using AI models, accelerate AI algorithms and provide AI capability inside databases. DB4AI and AI4DB have been extensively studied recently. In this tutorial, we review existing studies on AI4DB and DB4AI. For AI4DB, we review the techniques on learning-based database configuration, optimization, design, monitoring, and security. For DB4AI, we review AI-oriented declarative language, data governance, training acceleration, and inference acceleration. Finally, we provide research challenges and future directions in AI4DB and DB4AI.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2859–2866},
numpages = {8},
keywords = {models, machine learning, database, AI},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3132218.3132226,
author = {Esnaola-Gonzalez, Iker and Berm\'{u}dez, Jes\'{u}s and Fern\'{a}ndez, Izaskun and Fern\'{a}ndez, Santiago and Arnaiz, Aitor},
title = {Towards a Semantic Outlier Detection Framework in Wireless Sensor Networks},
year = {2017},
isbn = {9781450352963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132218.3132226},
doi = {10.1145/3132218.3132226},
abstract = {Outlier detection in the preprocessing phase of Knowledge Discovery in Databases (KDD) processes has been a widely researched topic for many years. However, identifying the potential outlier cause still remains an unsolved challenge even though it could be very helpful for determining what actions to take after detecting it. Furthermore, conventional outlier detection methods might still overlook outliers in certain complex contexts. In this article, Semantic Technologies are used to contribute overcoming these problems by proposing the SemOD (Semantic Outlier Detection) Framework. This framework guides the data-scientist towards the detection of certain types of outliers in WSNs (Wireless Sensor Network). Feasibility of the approach has been tested in outdoor temperature sensors and results show that the proposed approach is generic enough to apply it to different sensors, even improving the accuracy, specificity and sensitivity of outlier detection as well as spotting their potential cause.},
booktitle = {Proceedings of the 13th International Conference on Semantic Systems},
pages = {152–159},
numpages = {8},
keywords = {Knowledge Discovery in Databases, Outlier Detection, Semantic Technologies, Wireless Sensor Network},
location = {Amsterdam, Netherlands},
series = {Semantics2017}
}

@article{10.1145/3511666,
author = {Schmeck, Hartmut and Monti, Antonello and Hagenmeyer, Veit},
title = {Energy Informatics: Key Elements for Tomorrow's Energy System},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3511666},
doi = {10.1145/3511666},
journal = {Commun. ACM},
month = {mar},
pages = {58–63},
numpages = {6}
}

@inproceedings{10.1145/3473714.3473788,
author = {Zhang, Jiamin},
title = {Potential Energy Saving Estimation for Retrofit Building with ASHRAE-Great Energy Predictor III Using Machine Learning},
year = {2021},
isbn = {9781450390231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473714.3473788},
doi = {10.1145/3473714.3473788},
abstract = {Energy is material basis for social development and conflicting issue of economic development around the world. With the continuous urbanization, the energy consumption of buildings will be further increased, accounting for about 40% of the total energy consumption eventually. Thus, enhancement of energy efficiency of the buildings has become an essential issue to reduce the amount of gas emission as well as fossil fuel consumption. Delivering a high-quality built environment in an energy efficient way is the crucial key to energy conservation. Energy efficiency retrofit for buildings is considered to be a promising way to achieve energy savings. Machine learning provides the ability to learn from data using multiple computer algorithms. This paper introduces several algorithms, including random forest and Light GBM, to analyze building energy consumption based on the data from Kaggle competition, providing discussion of improvement in model efficiency and economic analysis by simulating different scenarios. In addition, sensitivity analysis is conducted to show the influence of different parameters in models and metrics to quantify the accuracy of prediction are proposed. The results of this paper can help people understand quantitative influence of different variables on energy use and energy baseline models. Future works will incorporate more data type in order to enhance the performance of prediction.},
booktitle = {Proceedings of the 2021 1st International Conference on Control and Intelligent Robotics},
pages = {425–429},
numpages = {5},
keywords = {green architecture, retrofit building, machine learning, energy saving},
location = {Guangzhou, China},
series = {ICCIR '21}
}

@article{10.1007/s00779-019-01217-0,
author = {Jin, Yong and Qian, Zhenjiang and Chen, Shunjiang},
title = {Data Collection Scheme with Minimum Cost and Location of Emotional Recognition Edge Devices},
year = {2019},
issue_date = {Jul 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3–4},
issn = {1617-4909},
url = {https://doi.org/10.1007/s00779-019-01217-0},
doi = {10.1007/s00779-019-01217-0},
abstract = {This paper develops a real-time and reliable data collection system for big scale emotional recognition systems. Based on the data sample set collected in the initialization stage and by considering the dynamic migration of emotional recognition data, we design an adaptive Kth average device clustering algorithm for migration perception. We define a sub-modulus weight function, which minimizes the sum of the weights of the subsets covered by a cover to achieve high-precision device positioning. Combining the energy of the data collection devices and the energy of the wireless emotional device, we balance the data collection efficiency and energy consumption, and define a minimum access number problem based on energy and storage space constraints. By designing an approximate algorithm to solve the approximate minimum Steiner point problem, the continuous collection of emotional recognition data and the connectivity of data acquisition devices are guaranteed under the energy constraint of wireless devices. We validate the proposed algorithms through simulation experiments using different emotional recognition systems and different data scale. Furthermore, we analyze the proposed algorithms in terms of topology for devices classification, location accuracy, and data collection efficiency by comparing with the Bayesian classifier-based expectation maximization algorithm, the background difference-based moving target detection arithmetic averaging algorithm, and the Hungarian algorithm for solving the assignment problem.},
journal = {Personal Ubiquitous Comput.},
month = {jul},
pages = {595–606},
numpages = {12},
keywords = {Emotional recognition, Data collection, Location, Collection cost, Data acquisition, Edge devices}
}

@article{10.1145/2857274.2886105,
author = {Diakopoulos, Nicholas},
title = {Accountability in Algorithmic Decision-Making: A View from Computational Journalism},
year = {2015},
issue_date = {November-December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {9},
issn = {1542-7730},
url = {https://doi.org/10.1145/2857274.2886105},
doi = {10.1145/2857274.2886105},
abstract = {Every fiscal quarter automated writing algorithms churn out thousands of corporate earnings articles for the AP (Associated Press) based on little more than structured data. Companies such as Automated Insights, which produces the articles for AP, and Narrative Science can now write straight news articles in almost any domain that has clean and well-structured data: finance, sure, but also sports, weather, and education, among others. The articles aren’t cardboard either; they have variability, tone, and style, and in some cases readers even have difficulty distinguishing the machine-produced articles from human-written ones.},
journal = {Queue},
month = {nov},
pages = {126–149},
numpages = {24}
}

@inproceedings{10.1145/2565585.2565608,
author = {Welbourne, Evan and Wu, Pang and Bao, Xuan and Munguia-Tapia, Emmanuel},
title = {Crowdsourced Mobile Data Collection: Lessons Learned from a New Study Methodology},
year = {2014},
isbn = {9781450327428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2565585.2565608},
doi = {10.1145/2565585.2565608},
abstract = {In this paper we explore a scalable data collection methodology that simultaneously achieves low cost and a high degree of control. We use popular online crowdsourcing platforms to recruit 63 subjects for a 90-day data collection that resulted in over 75K hours of data. The total cost of data collection was dramatically lower than for alternative methodologies, with total subject compensation under $3.5K US, and a total of less than 10 hours/week spent by researchers managing the study. At the same time, our methodology enhances control and enables richer study protocols by allowing direct contact with subjects. We were able to conduct surveys, exchange messages, and debug remotely with feedback from subjects. In addition to reporting on study details, we also discuss interesting findings and offer lessons learned.},
booktitle = {Proceedings of the 15th Workshop on Mobile Computing Systems and Applications},
articleno = {2},
numpages = {6},
location = {Santa Barbara, California},
series = {HotMobile '14}
}

@inproceedings{10.1145/3423455.3430316,
author = {Seto, Toshikazu and Sekimoto, Yoshihide and Asahi, Kosuke and Endo, Takahiro},
title = {Constructing a Digital City on a Web-3D Platform: Simultaneous and Consistent Generation of Metadata and Tile Data from a Multi-Source Raw Dataset},
year = {2020},
isbn = {9781450381659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423455.3430316},
doi = {10.1145/3423455.3430316},
abstract = {In this study, we develop a platform that can display approximately 20 types of data via a web browser to realize a digital twin of a wider area, including a detailed reading display of block units and individual three-dimensional point cloud data (point cloud) of a city. Using actual data, we examine if the data model and visualization design correspond with the zoom level. Owing to the comparative examination of the wide-area display performance and the map representation design in a JavaScript-based open-source library, we were able to develop a platform with light architecture and an easily customizable display. Furthermore, prototyping, based on Mapbox GL JS and Deck.GL, and the display of spatiotemporal flow layers, such as background maps, point cloud data in many places, dozens of layer display types, and the General Transit Feed Specification (GTFS) allowed for the seamless transition from the local government to the wide-area display in the prefecture unit in approximately 10-20 s.It is recommended that this digital smart city platform should be standardized by other local governments, especially in areas where higher-order data visualization is yet to advance. To display this digital city in a lightweight environment, we consider the digital data situation of local governments in Japan. It is necessary to define the visualized design for each zoom level according to the characteristics of the data. We then arranged the display model of each zoom level for 20 types of urban infrastructure data related to the digital smart city by referring to the style schema of the tile form. Through these tasks, we organized the commonality and optimization of data models and formats.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
pages = {1–9},
numpages = {9},
keywords = {Mapbox GL JS, digital city, infrastructure 3D tiles, Deck.GL},
location = {Seattle, Washington},
series = {ARIC '20}
}

@inbook{10.1145/3447404.3447415,
author = {McMenemy, David},
title = {Ethical Issues in Machine Learning},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447415},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {195–196},
numpages = {2}
}

@article{10.1145/2844110,
author = {Diakopoulos, Nicholas},
title = {Accountability in Algorithmic Decision Making},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/2844110},
doi = {10.1145/2844110},
abstract = {A view from computational journalism.},
journal = {Commun. ACM},
month = {jan},
pages = {56–62},
numpages = {7}
}

@inproceedings{10.1145/2702123.2702528,
author = {Mauriello, Matthew Louis and Norooz, Leyla and Froehlich, Jon E.},
title = {Understanding the Role of Thermography in Energy Auditing: Current Practices and the Potential for Automated Solutions},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702528},
doi = {10.1145/2702123.2702528},
abstract = {The building sector accounts for 41% of primary energy consumption in the US, contributing an increasing portion of the country's carbon dioxide emissions. With recent sensor improvements and falling costs, auditors are increasingly using thermography-infrared (IR) cameras-to detect thermal defects and analyze building efficiency. Research in automated thermography has grown commensurately, aimed at reducing manual labor and improving thermal models. Though promising, we could find no prior work exploring the professional auditor's perspectives of thermography or reactions to emerging automation. To address this gap, we present results from two studies: a semi-structured interview with 10 professional energy auditors, which includes design probes of five automated thermography scenarios, and an observational case study of a residential audit. We report on common perspectives, concerns, and benefits related to thermography and summarize reactions to our automated scenarios. Our findings have implications for thermography tool designers as well as researchers working on automated solutions in robotics, computer science, and engineering.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1993–2002},
numpages = {10},
keywords = {human-robotic interaction, energy audits, robotics, design probes, formative inquiry, sustainable hci, thermography},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1145/3549555.3549594,
author = {Koulalis, Ilias and Dourvas, Nikolaos and Triantafyllidis, Theocharis and Ioannidis, Konstantinos and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
title = {A Survey for Image Based Methods in Construction: From Images to Digital Twins},
year = {2022},
isbn = {9781450397209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549555.3549594},
doi = {10.1145/3549555.3549594},
abstract = {In the construction domain, Digital twins are mostly used for facilities management of buildings, but their applications are still very limited. The virtualization of buildings and bridges in the last 15 years in the form of Building or Bridge Information Models is clearly identified as the starting point for the DTs. The industry has erected a frame with semantically rich 3D reference models that are now heavily enriched with visual sensor data captured on construction sites. This article provides an overview of the research and current practices of computer vision methods in the construction industry and presents typical examples of their applications for 3D reconstruction, safety management and structural monitoring for quality assurance. It then highlights the dominant achievements presented in the literature and concludes with the challenges and research directions applicable to digital twins that need to be addressed and exploited in the future.},
booktitle = {Proceedings of the 19th International Conference on Content-Based Multimedia Indexing},
pages = {103–110},
numpages = {8},
keywords = {Structural Health Monitoring, Computer Vision, Digital Twins, 3D Reconstruction, Safety Management},
location = {Graz, Austria},
series = {CBMI '22}
}

@inproceedings{10.1145/3548785.3548811,
author = {Passi, Kalpdrum and Shah, Aanan},
title = {Distinguishing Fake and Real News of Twitter Data with the Help of Machine Learning Techniques},
year = {2022},
isbn = {9781450397094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548785.3548811},
doi = {10.1145/3548785.3548811},
abstract = {News articles have an influence on people's belief and views about various circumstances. In this regard, some news publishers with political or ideological bias try to spread news which are distorted or totally wrong. Natural language processing was used to preprocess the text. Some general features like, number of words, sentences, stopwords, non-alphabetic words, verbs, nouns, and adjectives were identified. Word positioning was labeled to distinguish a word as a noun, a pronoun, an adjective or a verb in the sentences. Preprocessing was followed by feature extraction methods namely, count vectorizer, Term Frequency-Inverse Document Frequency (TF-IDF) vectorizer and word2vec embedding. It was observed that the results obtained by TF-IDF feature extraction method were superior compared with the other two methods. Various machine learning models were used for training the model namely, Naive Bayes, Logistic Regression, Random Forest, K-nearest neighbors (KNN), Support Vector Machine (SVM) and Recurrent Neural Network (RNN) as a deep learning model. The models were successfully tested on two datasets. On the first dataset, SVM achieved an accuracy of 98.5% and RNN achieved an accuracy of 98.03% which is much improvement over the best results of Agarwalla et al., 2019 (83.16 % accuracy). On the second dataset, SVM achieved an accuracy of 97.76%, RNN achieved 97.1% and Logistic Regression achieved 97.50% which is an improvement over the best results of Vijayraghavan et al. 2020 (94.88% accuracy).},
booktitle = {Proceedings of the 26th International Database Engineered Applications Symposium},
pages = {1–8},
numpages = {8},
location = {Budapest, Hungary},
series = {IDEAS '22}
}

@inproceedings{10.1145/3325112.3325243,
author = {Chen, Tao and Ran, Longya and Gao, Xian},
title = {AI Innovation for Advancing Public Service: The Case of China's First Administrative Approval Bureau},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325243},
doi = {10.1145/3325112.3325243},
abstract = {The adoption of artificial intelligence (AI) is becoming increasingly popular in the public sector, but there is a severe lack of relevant theoretical research. The government of China also has high expectations for AI innovation. This paper proposes a four-stage model for AI development in public sectors to help public administrators think about the impact of AI on their organizations. We empirically investigate a case of AI adoption for delivering public services in local government in China. The findings improve our understanding of not only the status of AI innovation but also the factors motivating and challenging public sectors that are intending to adopt AI. Given that AI application in public sectors is still in its infancy, this study provides us with an opportunity to conduct longitudinal tracking of AI innovation in local government in China. This could help public administrators to think more comprehensively about the changes and transformations that AI may bring to the public sector.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {100–108},
numpages = {9},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@article{10.1145/3415212,
author = {Feger, Sebastian S. and Wozniak, Pawe\l{} W. and Lischke, Lars and Schmidt, Albrecht},
title = { 'Yes, I Comply!': Motivations and Practices around Research Data Management and Reuse across Scientific Fields},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415212},
doi = {10.1145/3415212},
abstract = {As science becomes increasingly data-intensive, the requirements for comprehensive Research Data Management (RDM) grow. This often overwhelms scientists, requiring more workload and training. The failure to conduct effective RDM leads to producing research artefacts that cannot be reproduced or reused. Past research placed high value on supporting data science workers, but focused mainly on data production, collection, processing, and sensemaking. In order to understand practices and needs of data science workers in relation to documentation, preservation, sharing, and reuse, we conducted a cross-domain study with 15 scientists and data managers from diverse scientific domains. We identified five core concepts which describe requirements, drivers, and boundaries in the development of commitment for RDM, essential for generating reproducible research artefacts: Practice, Adoption, Barriers, Education, and Impact. Based on those concepts, we introduce a stage-based model of personal RDM commitment evolution. The model can be used to drive the design of future systems that support a transition to open science. We discuss infrastructure, policies, and motivations involved at the stages and transitions in the model. Our work supports designers in understanding the constraints and challenges involved in designing for reproducibility in an age of data-driven science.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {141},
numpages = {26},
keywords = {motivation, reuse, reproducibility, human data interventions, data-processing science, research data management}
}

@article{10.1145/3057266,
author = {Perera, Charith and Qin, Yongrui and Estrella, Julio C. and Reiff-Marganiec, Stephan and Vasilakos, Athanasios V.},
title = {Fog Computing for Sustainable Smart Cities: A Survey},
year = {2017},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3057266},
doi = {10.1145/3057266},
abstract = {The Internet of Things (IoT) aims to connect billions of smart objects to the Internet, which can bring a promising future to smart cities. These objects are expected to generate large amounts of data and send the data to the cloud for further processing, especially for knowledge discovery, in order that appropriate actions can be taken. However, in reality sensing all possible data items captured by a smart object and then sending the complete captured data to the cloud is less useful. Further, such an approach would also lead to resource wastage (e.g., network, storage, etc.). The Fog (Edge) computing paradigm has been proposed to counterpart the weakness by pushing processes of knowledge discovery using data analytics to the edges. However, edge devices have limited computational capabilities. Due to inherited strengths and weaknesses, neither Cloud computing nor Fog computing paradigm addresses these challenges alone. Therefore, both paradigms need to work together in order to build a sustainable IoT infrastructure for smart cities. In this article, we review existing approaches that have been proposed to tackle the challenges in the Fog computing domain. Specifically, we describe several inspiring use case scenarios of Fog computing, identify ten key characteristics and common features of Fog computing, and compare more than 30 existing research efforts in this domain. Based on our review, we further identify several major functionalities that ideal Fog computing platforms should support and a number of open challenges toward implementing them, to shed light on future research directions on realizing Fog computing for building sustainable smart cities.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {32},
numpages = {43},
keywords = {sustainability, Internet of things, smart cities, fog computing}
}

@inproceedings{10.1145/2740908.2742563,
author = {Deng, Alex},
title = {Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742563},
doi = {10.1145/2740908.2742563},
abstract = {As A/B testing gains wider adoption in the industry, more people begin to realize the limitations of the traditional frequentist null hypothesis statistical testing (NHST). The large number of search results for the query ``Bayesian A/B testing'' shows just how much the interest in the Bayesian perspective is growing. In recent years there are also voices arguing that Bayesian A/B testing should replace frequentist NHST and is strictly superior in all aspects. Our goal here is to clarify the myth by looking at both advantages and issues of Bayesian methods. In particular, we propose an objective Bayesian A/B testing framework for which we hope to bring the best from Bayesian and frequentist methods together. Unlike traditional methods, this method requires the existence of historical A/B test data to objectively learn a prior. We have successfully applied this method to Bing, using thousands of experiments to establish the priors.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {923–928},
numpages = {6},
keywords = {a/b testing, prior, objective bayes, multiple testing, bayesian statistics, empirical bayes, controlled experiments, optional stopping},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@article{10.1145/3545797,
author = {Zhang, Hang and Yang, Yajun and Wang, Xin and Gao, Hong and Hu, Qinghua},
title = {MLI: A Multi-Level Inference Mechanism for User Attributes in Social Networks},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3545797},
doi = {10.1145/3545797},
abstract = {In the social network, each user has attributes for self-description called user attributes which are semantically hierarchical. Attribute inference has become an essential way for social platforms to realize user classifications and targeted recommendations. Most existing approaches mainly focus on the flat inference problem neglecting the semantic hierarchy of user attributes which will cause serious inconsistency in multi-level tasks. In this article, we propose a multi-level model MLI, where information propagation part collects attribute information by mining the global graph structure, and the attribute correction part realizes the mutual correction between different levels of attributes. Further, we put forward the concept of generalized semantic tree, a way of representing the hierarchical structure of user attributes, whose nodes are allowed to have multiple parent nodes unlike the regular tree. Both regular and generalized semantic tree are commonly used in practice, and can be handled by our model. Besides, by making the inference start from sub-networks with sufficient attribute information, we design a “Ripple” algorithm to improve the efficiency and effectiveness of our model. For evaluation purposes, we conduct extensive verification experiments on DBLP datasets. The experimental results show the superior effect of MLI, compared with the state-of-the-art methods.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = {jun},
keywords = {social network, hierarchical inference, attribute inference}
}

@article{10.1145/3522592,
author = {Cai, Jianghui and Yang, Yuqing and Yang, Haifeng and Zhao, Xujun and Hao, Jing},
title = {ARIS: A Noise Insensitive Data Pre-Processing Scheme for Data Reduction Using Influence Space},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3522592},
doi = {10.1145/3522592},
abstract = {The extensive growth of data quantity has posed many challenges to data analysis and retrieval. Noise and redundancy are typical representatives of the above-mentioned challenges, which may reduce the reliability of analysis and retrieval results and increase storage and computing overhead. To solve the above problems, a two-stage data pre-processing framework for noise identification and data reduction, called ARIS, is proposed in this article. The first stage identifies and removes noises by the following steps: First, the influence space (IS) is introduced to elaborate data distribution. Second, a ranking factor (RF) is defined to describe the possibility that the points are regarded as noises, then, the definition of noise is given based on RF. Third, a clean dataset (CD) is obtained by removing noise from the original dataset. The second stage learns representative data and realizes data reduction. In this process, CD is divided into multiple small regions by IS. Then the reduced dataset is formed by collecting the representations of each region. The performance of ARIS is verified by experiments on artificial and real datasets. Experimental results show that ARIS effectively weakens the impact of noise and reduces the amount of data and significantly improves the accuracy of data analysis within a reasonable time cost range.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jul},
articleno = {110},
numpages = {39},
keywords = {noise identification, ranking factor, data representation, influence space, Data pre-processing scheme}
}

@article{10.1145/3533381,
author = {Adhikari, Deepak and Jiang, Wei and Zhan, Jinyu and Zhiyuan He and Rawat, Danda B. and Aickelin, Uwe and Khorshidi, Hadi A.},
title = {A Comprehensive Survey on Imputation of Missing Data in Internet of Things},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533381},
doi = {10.1145/3533381},
abstract = {Internet of Things (IoT) is enabled by the latest developments in smart sensors, communication technologies, and Internet protocols with broad applications. Collecting data from IoT and generating information from these data become tedious tasks in real-life applications when missing data is encountered in datasets. It is of critical importance to deal with the missing data timely for intelligent decision making. Hence, this survey attempts to provide a structured and comprehensive overview of the research on the imputation of incomplete data in IoT. The paper starts by providing an overview of incomplete data based on the architecture of IoT. Then, it discusses the various strategies to handle the missing data, the assumptions used, the computing platform, and the issues related to them. The paper also explores the application of imputation in the area of IoT. We encourage researchers and data analysts to use known imputation techniques and discuss various issues and challenges. Finally, potential future directions regarding the method are suggested. We believe this survey will provide a better understanding of the research of incomplete data and serve as a guide for future research.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {may},
keywords = {Multiple Imputations, Imputation of Missing Data, Deep Learning, Machine Learning, Internet of Things, Computing Platform for Incomplete data}
}

@article{10.1145/3527546.3527555,
author = {Ahlers, Dirk and Wilde, Erik and Spaniol, Marc and Baeza-Yates, Ricardo and Alonso, Omar},
title = {Report on the 11th International Workshop on Location and the Web (LocWeb 2021) and the 11th Temporal Web Analytics Workshop (TempWeb2021) at WWW2021},
year = {2022},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3527546.3527555},
doi = {10.1145/3527546.3527555},
abstract = {LocWeb and TempWeb 2021 were the eleventh events in their workshop series and took place co-located on 12th April 2021 in conjunction with The Web Conference WWW 2021. They were intended to be held in Ljubljana, Slovenia as a potentially hybrid event, but due to the pandemic, were fully moved online.LocWeb and TempWeb were held as one colocated session with a merged programme and shared topics to explore similarities and introduce attendees to the two related and complementary areas. LocWeb 2021 explored the intersection of location-based analytics and Web architecture with a focus on on Web-scale services and location-aware information access. TempWeb 2021 discussed temporal analytics at a Web scale with experts from science and industry.Date: 12 April, 2021.Websites: https://dhere.de/locweb/locweb2021 and http://temporalweb.net/.},
journal = {SIGIR Forum},
month = {mar},
articleno = {6},
numpages = {7}
}

@inproceedings{10.1145/3091478.3091521,
author = {Xu, Jiejun and Xie, Daniel and Lu, Tsai-Ching and Cafeo, John},
title = {EDSV: Emerging Defect Surveillance for Vehicles},
year = {2017},
isbn = {9781450348966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3091478.3091521},
doi = {10.1145/3091478.3091521},
abstract = {We present early findings on building a proof of concept for an automated system to identify emerging trends regarding vehicle defects. The proposed system functions by continuously collecting and monitoring publicly available data from several heterogeneous channels ranging from online social media to vehicle enthusiast forums and consumer reporting sites. By mining the collected data, the system would provide real-time detection of ongoing consumer issues with vehicles. In addition, our system has special emphasis on detecting early signals prior to the widespread knowledge of the general public. One of the system components involves estimating a baseline statistical distribution governing the frequency of observing specific types of vehicle defective complaints from our data sources and subsequently identifying irregular deviations from this distribution. A web interface is made available to visualize descriptive statistics derived from various channels, with the intent to provide timely insights for human analysts.},
booktitle = {Proceedings of the 2017 ACM on Web Science Conference},
pages = {219–222},
numpages = {4},
keywords = {business intelligence, quality management, measurement, user generated content, online social media},
location = {Troy, New York, USA},
series = {WebSci '17}
}

@article{10.1145/3352020.3352033,
author = {Du, Yifan and Issarny, Val\'{e}rie and Sailhan, Fran\c{c}oise},
title = {When the Power of the Crowd Meets the Intelligence of the Middleware: The Mobile Phone Sensing Case},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/3352020.3352033},
doi = {10.1145/3352020.3352033},
abstract = {The data gluttony of AI is well known: Data fuels the artificial intelligence. Technologies that help to gather the needed data are then essential, among which the IoT. However, the deployment of IoT solutions raises significant challenges, especially regarding the resource and financial costs at stake. It is our view that mobile crowdsensing, aka phone sensing, has a major role to play because it potentially contributes massive data at a relatively low cost. Still, crowdsensing is useless, and even harmful, if the contributed data are not properly analyzed. This paper surveys our work on the development of systems facing this challenge, which also illustrates the virtuous circles of AI. We specifically focus on how intelligent crowdsensing middleware leverages on-device machine learning to enhance the reported physical observations. Keywords: Crowdsensing, Middleware, Online learning.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jul},
pages = {85–90},
numpages = {6}
}

@article{10.1145/2795403.2795412,
author = {Alonso, Omar and Kamps, Jaap and Karlgren, Jussi},
title = {Report on the Seventh Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'14)},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/2795403.2795412},
doi = {10.1145/2795403.2795412},
abstract = {There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. The goal of the ESAIR'14 workshop remained to advance the general research agenda on this core problem, with an explicit focus on one of the most challenging aspects to address in the coming years. The main remaining challenge is on the user's side---the potential of rich document annotations can only be realized if matched by more articulate queries exploiting these powerful retrieval cues---and a more dynamic approach is emerging by exploiting new forms of query autosuggest. How can the query suggestion paradigm be used to encourage searcher to articulate longer queries, with concepts and relations linking their statement of request to existing semantic models? How do entity results and social network data in "graph search" change the classic division between searchers and information and lead to extreme personalization---are you the query? How to leverage transaction logs and recommendation, and how adaptive should we make the system? What are the privacy ramifications and the UX aspects---how to not creep out users.There was a strong feeling that we made substantial progress. Specifically, the discussion contributed to our understanding of the way forward. First, for notable (head, shoulder, but not tail) entities in semantic search we have reached the level of quality at minimal costs allowing for deployment in major web search engines---the dream has become a reality. Second, entity detection is moving fast into domain specific, personal, and business domains, and has become a vital component for a range of applications. Third, semantic web has exchanged logic for machine learning approaches, and machine learning is the natural unification of semantic web and information retrieval approaches.},
journal = {SIGIR Forum},
month = {jun},
pages = {27–34},
numpages = {8}
}

@inproceedings{10.1145/2187980.2188029,
author = {Soldatos, John and Draief, Moez and Macdonald, Craig and Ounis, Iadh},
title = {Multimedia Search over Integrated Social and Sensor Networks},
year = {2012},
isbn = {9781450312301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187980.2188029},
doi = {10.1145/2187980.2188029},
abstract = {This paper presents work in progress within the FP7 EU-funded project SMART to develop a multimedia search engine over content and information stemming from the physical world, as derived through visual, acoustic and other sensors. Among the unique features of the search engine is its ability to respond to social queries, through integrating social networks with sensor networks. Motivated by this innovation, the paper presents and discusses the state-of-the-art in participatory sensing and other technologies blending social and sensor networks.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {283–286},
numpages = {4},
keywords = {sensors, search engine, multimedia},
location = {Lyon, France},
series = {WWW '12 Companion}
}

@inproceedings{10.1145/2666310.2666471,
author = {Li, Xiang and Kardes, Hakan and Wang, Xin and Sun, Ang},
title = {HMM-Based Address Parsing: Efficiently Parsing Billions of Addresses on MapReduce},
year = {2014},
isbn = {9781450331319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666310.2666471},
doi = {10.1145/2666310.2666471},
abstract = {Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches to build models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses with Hadoop.},
booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {433–436},
numpages = {4},
keywords = {address parsing, record linkage, large-scale data},
location = {Dallas, Texas},
series = {SIGSPATIAL '14}
}

@inproceedings{10.1145/3269206.3271798,
author = {Kuo, Yu-Hsuan and Li, Zhenhui and Kifer, Daniel},
title = {Detecting Outliers in Data with Correlated Measures},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271798},
doi = {10.1145/3269206.3271798},
abstract = {Advances in sensor technology have enabled the collection of large-scale datasets. Such datasets can be extremely noisy and often contain a significant amount of outliers that result from sensor malfunction or human operation faults. In order to utilize such data for real-world applications, it is critical to detect outliers so that models built from these datasets will not be skewed by outliers. In this paper, we propose a new outlier detection method that utilizes the correlations in the data (e.g., taxi trip distance vs. trip time). Different from existing outlier detection methods, we build a robust regression model that explicitly models the outliers and detects outliers simultaneously with the model fitting. We validate our approach on real-world datasets against methods specifically designed for each dataset as well as the state of the art outlier detectors. Our outlier detection method achieves better performances, demonstrating the robustness and generality of our method. Last, we report interesting case studies on some outliers that result from atypical events.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {287–296},
numpages = {10},
keywords = {contextual outlier detection, robust regression},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1145/3480968,
author = {Tahir, Madiha and Halim, Zahid and Rahman, Atta Ur and Waqas, Muhammad and Tu, Shanshan and Chen, Sheng and Han, Zhu},
title = {Non-Acted Text and Keystrokes Database and Learning Methods to Recognize Emotions},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3480968},
doi = {10.1145/3480968},
abstract = {The modern computing applications are presently adapting to the convenient availability of huge and diverse data for making their pattern recognition methods smarter. Identification of dominant emotion solely based on the text data generated by humans is essential for the modern human–computer interaction. This work presents a multimodal text-keystrokes dataset and associated learning methods for the identification of human emotions hidden in small text. For this, a text-keystrokes data of 69 participants is collected in multiple scenarios. Stimuli are induced through videos in a controlled environment. After the stimuli induction, participants write their reviews about the given scenario in an unguided manner. Afterward, keystroke and in-text features are extracted from the dataset. These are used with an assortment of learning methods to identify emotion hidden in the short text. An accuracy of 86.95% is achieved by fusing text and keystroke features. Whereas, 100% accuracy is obtained for pleasure-displeasure classes of emotions using the fusion of keystroke/text features, tree-based feature selection method, and support vector machine classifier. The present work is also compared with four state-of-the-art techniques for the same task, where the results suggest that the present proposal performs better in terms of accuracy.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {feb},
articleno = {61},
numpages = {24},
keywords = {affective states, machine learning, Affective computing, pattern recognition, data-driven decision-making}
}

@inproceedings{10.1145/3510858.3510863,
author = {Sun, Fangyu},
title = {Manufacturing Audit Quality Analysis Model Based on Data Mining Technology},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3510863},
doi = {10.1145/3510858.3510863},
abstract = {As our country's economic development enters a new normal, the original manufacturing development model can no longer meet the needs of current economic development, and it is urgent to accelerate the transformation of the manufacturing industry. At present, the country's supply-side structural reforms are deepening, and listed manufacturing companies are the most important backbone in terms of scale and innovation opportunities. Data mining technology is used to study the impact of quality control on corporate performance. Listed companies have a positive impact on the further realization of transformation and upgrading and the improvement of corporate performance. This article aims to study the manufacturing audit quality analysis model based on data mining technology, and adopts the analysis method of the combination of supervisory research and empirical analysis, from the perspective of supervisory research, summarizes the theory of internal audit quality and company performance in the research process, and summarizes predecessors' research results and research ideas. The experimental data in this article shows that the average quality of internal audit information disclosure is 3.1156, indicating that the audit disclosure status of listed companies selected by the Shenzhen Stock Exchange is good, and to a certain extent reflects the quality level of some internal controls.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {1–6},
numpages = {6},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3209978.3210194,
author = {Kumar Chandrasekaran, Muthu and Jaidka, Kokil and Mayr, Philipp},
title = {Joint Workshop on Bibliometric-Enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018)},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210194},
doi = {10.1145/3209978.3210194},
abstract = {The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Information retrieval~(IR), bibliometric and natural language processing (NLP) techniques could enhance scholarly search, retrieval and user experience but are not yet widely used. To this purpose, we propose the third iteration of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL). The workshop is intended to stimulate IR, NLP researchers and Digital Library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop will incorporate multiple invited talks, paper sessions, a poster session and the 4th edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {1415–1418},
numpages = {4},
keywords = {digital libraries, citation analysis, information extraction, information retrieval, natural language processing, text mining, bibliometrics},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3107091.3107093,
author = {Truong, Hong-Linh and Berardinelli, Luca},
title = {Testing Uncertainty of Cyber-Physical Systems in IoT Cloud Infrastructures: Combining Model-Driven Engineering and Elastic Execution},
year = {2017},
isbn = {9781450351126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107091.3107093},
doi = {10.1145/3107091.3107093},
abstract = {Today's cyber-physical systems (CPS) span IoT and cloud-based datacenter infrastructures, which are highly heterogeneous with various types of uncertainty. Thus, testing uncertainties in these CPS is a challenging and multidisciplinary activity. We need several tools for modeling, deployment, control, and analytics to test and evaluate uncertainties for different configurations of the same CPS. In this paper, we explain why using state-of-the art model-driven engineering (MDE) and model-based testing (MBT) tools is not adequate for testing uncertainties of CPS in IoT Cloud infrastructures. We discus how to combine them with techniques for elastic execution to dynamically provision both CPS under test and testing utilities to perform tests in various IoT Cloud infrastructures.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Testing Embedded and Cyber-Physical Systems},
pages = {5–8},
numpages = {4},
keywords = {Cloud, MBT, elasticity, MDE, testing, IoT, uncertainty},
location = {Santa Barbara, CA, USA},
series = {TECPS 2017}
}

@article{10.1145/3310230,
author = {Fan, Wenfei},
title = {Dependencies for Graphs: Challenges and Opportunities},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3310230},
doi = {10.1145/3310230},
abstract = {What are graph dependencies? What do we need them for? What new challenges do they introduce? This article tackles these questions. It aims to incite curiosity and interest in this emerging area of research.},
journal = {J. Data and Information Quality},
month = {feb},
articleno = {5},
numpages = {12},
keywords = {certain fixes, error detection, satisfiability, implication, dependency discovery, graphs, Dependencies, validation}
}

@inproceedings{10.1145/3102254.3102268,
author = {Lee, Rich C. and Cuzzocrea, Alfredo and Lee, Wookey and Leung, Carson K.},
title = {An Innovative Majority Voting Mechanism in Interactive Social Network Clustering},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102268},
doi = {10.1145/3102254.3102268},
abstract = {We describe a new method of voting system in social networks environment1. We suggest a sequence of continuous support via a social network after electing representatives or exemplars in the network that is different from the typical majority voting. In other words, this paper suggests the method of elected representatives using network clustering approach to counts voting. On the network structure, sending messages from each node reflects the influence or importance to the representative and that can be readjusted and send back to each node. Where the representatives can be clustered within which the selectivity can be decided through the graph edges. In the experiment our algorithm outperformed conventional approaches in social network synthetic dataset as well as real dataset.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {14},
numpages = {10},
keywords = {vote, graph clustering, majority, social networks},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3132847.3132894,
author = {Zhao, Yan and Li, Yang and Wang, Yu and Su, Han and Zheng, Kai},
title = {Destination-Aware Task Assignment in Spatial Crowdsourcing},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132894},
doi = {10.1145/3132847.3132894},
abstract = {With the proliferation of GPS-enabled smart devices and increased availability of wireless network, spatial crowdsourcing (SC) has been recently proposed as a framework to automatically request workers (i.e., smart device carriers) to perform location-sensitive tasks (e.g., taking scenic photos, reporting events). In this paper we study a destination-aware task assignment problem that concerns the optimal strategy of assigning each task to proper worker such that the total number of completed tasks can be maximized whilst all workers can reach their destinations before deadlines after performing assigned tasks. Finding the global optimal assignment turns out to be an intractable problem since it does not imply optimal assignment for individual worker. Observing that the task assignment dependency only exists amongst subsets of workers, we utilize tree-decomposition technique to separate workers into independent clusters and develop an efficient depth-first search algorithm with progressive bounds to prune non-promising assignments. Our empirical studies demonstrate that our proposed technique is quite effective and settle the problem nicely.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {297–306},
numpages = {10},
keywords = {spatial task assignment, user mobility, spatial crowdsourcing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1109/TASLP.2015.2512041,
author = {Lin, Zheng and Jin, Xiaolong and Xu, Xueke and Wang, Yuanzhuo and Cheng, Xueqi and Wang, Weiping and Meng, Dan},
title = {An Unsupervised Cross-Lingual Topic Model Framework for Sentiment Classification},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2015.2512041},
doi = {10.1109/TASLP.2015.2512041},
abstract = {Sentiment classification aims to determine the sentiment polarity expressed in a text. In online customer reviews, the sentiment polarities of words are usually dependent on the corresponding aspects. For instance, in mobile phone reviews, we may expect the long battery time but not enjoy the long response time of the operating system. Therefore, it is necessary and appealing to consider aspects when conducting sentiment classification. Probabilistic topic models that jointly detect aspects and sentiments have gained much success recently. However, most of the existing models are designed to work well in a language with rich resources. Directly applying those models on poor-quality corpora often leads to poor results. Consequently, a potential solution is to use the cross-lingual topic model to improve the sentiment classification for a target language by leveraging data and knowledge from a source language. However, the existing cross-lingual topic models are not suitable for sentiment classification because sentiment factors are not considered therein. To solve these problems, we propose for the first time a novel cross-lingual topic model framework which can be easily combined with the state-of-the-art aspect/sentiment models. Extensive experiments in different domains and multiple languages demonstrate that our model can significantly improve the accuracy of sentiment classification in the target language.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = {mar},
pages = {432–444},
numpages = {13},
keywords = {cross-language, topic model, sentiment classification}
}

@article{10.1145/3436239,
author = {Liu, Zhicheng and Zhang, Yang and Huang, Ruihong and Chen, Zhiwei and Song, Shaoxu and Wang, Jianmin},
title = {EXPERIENCE: Algorithms and Case Study for Explaining Repairs with Uniform Profiles over IoT Data},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3436239},
doi = {10.1145/3436239},
abstract = {IoT data with timestamps are often found with outliers, such as GPS trajectories or sensor readings. While existing systems mostly focus on detecting temporal outliers without explanations and repairs, a decision maker may be more interested in the cause of the outlier appearance such that subsequent actions would be taken, e.g., cleaning unreliable readings or repairing broken devices or adopting a strategy for data repairs. Such outlier detection, explanation, and repairs are expected to be performed in either offline (batch) or online modes (over streaming IoT data with timestamps). In this work, we present TsClean, a new prototype system for detecting and repairing outliers with explanations over IoT data. The framework defines uniform profiles to explain the outliers detected by various algorithms, including the outliers with variant time intervals, and take approaches to repair outliers. Both batch and streaming processing are supported in a uniform framework. In particular, by varying the block size, it provides a tradeoff between computing the accurate results and approximating with efficient incremental computation. In this article, we present several case studies of applying TsClean in industry, e.g., how this framework works in detecting and repairing outliers over excavator water temperature data, and how to get reasonable explanations and repairs for the detected outliers in tracking excavators.},
journal = {J. Data and Information Quality},
month = {apr},
articleno = {18},
numpages = {17},
keywords = {data profiling, time series, Outlier explanation, outlier repairs}
}

@proceedings{10.1145/3549843,
title = {ICEBT '22: Proceedings of the 2022 6th International Conference on E-Education, E-Business and E-Technology},
year = {2022},
isbn = {9781450397216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@inproceedings{10.1145/3014087.3014112,
author = {Nikiforov, Alexander and Singireja, Anastasija},
title = {Open Data and Crowdsourcing Perspectives for Smart City in the United States and Russia},
year = {2016},
isbn = {9781450348591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014087.3014112},
doi = {10.1145/3014087.3014112},
abstract = {In this research paper we describe the transformation of open data strategy and implementation of crowdsourcing technologies for the city E-government services. Analysis of smart city projects provides the role of open data and crowdsourcing for smart city vision in United States and Russia. We define challenges and perspectives for collaboration of open data and crowdsourcing in smart city projects.},
booktitle = {Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {171–177},
numpages = {7},
keywords = {crowdsourcing, civic issue tracker, smart city, open innovations, open data, government 2.0, e-government},
location = {St. Petersburg, Russia},
series = {EGOSE '16}
}

@inproceedings{10.1145/2951894.2951901,
author = {Fan, Liju and Flood, Mark D.},
title = {An Ontology of Form PF},
year = {2016},
isbn = {9781450344074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2951894.2951901},
doi = {10.1145/2951894.2951901},
abstract = {Form PF, mandated by the 2010 Dodd-Frank Act, is financial regulators' primary source for supervisory data on the risk exposures of hedge funds. Investment advisers use the Securities and Exchange Commission's (SEC) Form ADV to register, and then submit Form PF to report on each of the funds that they advise. These forms embed significant internal structure that is amenable to knowledge representation via formal ontologies, which would facilitate key tasks, such as data integration and the assurance of data integrity. We argue that ontologies should be a core and integral component of information management for financial regulatory reporting. We have tested the approach by designing, developing, and integrating ontologies in OWL/RDF in prototype to consistently describe Form ADV and Form PF with precise semantics. Preliminary results indicate that this technique is feasible in practice for data search and analysis, and will yield useful functionality. We also outline directions for future research.},
booktitle = {Proceedings of the Second International Workshop on Data Science for Macro-Modeling},
articleno = {9},
numpages = {6},
keywords = {supervisory data, data integrity, investment advisers, ontologies, data integration, hedge funds, Financial regulation, knowledge representation},
location = {San Francisco, CA, USA},
series = {DSMM'16}
}

@inproceedings{10.1145/3543434.3543438,
author = {Marmier, Auriane},
title = {The Impact of Data Governance on OGD Publication – An Ethnographic Odyssey},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543434.3543438},
doi = {10.1145/3543434.3543438},
abstract = {Over the past decade, Open Government Data (OGD) strategies have become a continuing concern in administrative services. This is even truer than at any time. Given the current situation, data management, specifically consistent data publication, has been central to public institutions. The Covid-19 pandemic has shown that data collected by public administrations could make valuable contributions. However, in Switzerland, the pandemic has highlighted the limitations of public organizations' capability to lead the publication of their data. Based on an ethnography and a literature review, this paper explores how data governance components impact OGD publication process and presents a model of OGD governance. For this purpose, we identify key data governance components necessary to OGD publication - structural, procedural, and relational - and illustrate how OGD challenges rarely arise from the publication of OGD or the open nature of data itself, but a lack of data governance.},
booktitle = {DG.O 2022: The 23rd Annual International Conference on Digital Government Research},
pages = {235–243},
numpages = {9},
location = {Virtual Event, Republic of Korea},
series = {dg.o 2022}
}

@inproceedings{10.1145/2663713.2664430,
author = {Li, Xiang and Kardes, Hakan and Wang, Xin and Sun, Ang},
title = {HMM-Based Address Parsing with Massive Synthetic Training Data Generation},
year = {2014},
isbn = {9781450314596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663713.2664430},
doi = {10.1145/2663713.2664430},
abstract = {Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches of synthetic training data generation to build robust models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses.},
booktitle = {Proceedings of the 4th International Workshop on Location and the Web},
pages = {33–36},
numpages = {4},
keywords = {large-scale data, address parsing, record linkage},
location = {Shanghai, China},
series = {LocWeb '14}
}

@inproceedings{10.1145/3209415.3209459,
author = {Millard, Jeremy and Thomasen, Louise and Pastrovic, Goran and Cvetkovic, Bojan},
title = {A Roadmap for E-Participation and Open Government: Empirical Evidence from the Western Balkans},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209459},
doi = {10.1145/3209415.3209459},
abstract = {This paper describes why and how a conceptual framework for e-participation and open government has been developed and applied to six aspirant EU countries in the Western Balkans. It provides a rationale and background, and then examines the main academic and other relevant sources used. This is followed by an overview of the conceptual framework and a description of its main elements. Finally, the paper examines international data on e-participation covering the Western Balkan countries, uses this to examine the results of applying the conceptual framework in each country, and then provides conclusions and recommendations.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {191–198},
numpages = {8},
keywords = {Policy, E-government, Collaboration, Open government, Transparency, E-Participation, Participation},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@inproceedings{10.1145/1710035.1710077,
author = {Liu, Jing and Cho, Sungchol and Han, Sunyoung and Kim, Keecheon and Ha, YoungGuk and Choe, Jongwon and Kamolphiwong, Sinchai and Choo, Hyunseung and Shin, Yongtae and Kim, Chinchol},
title = {Establishment and Traffic Measurement of Overlay Multicast Testbed in KOREN, THaiREN and TEIN2},
year = {2009},
isbn = {9781605585369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1710035.1710077},
doi = {10.1145/1710035.1710077},
abstract = {Nowadays not only many of research works with various international networks are increasing more and more but also commercial works are increasing with different international networks. In this paper, we have constructed the overlay multicast testbed with KOREN and TEIN2 network, and then we also analyze and research many works with the data got from the testbed experiments, and research works for speed of transmission and transmission security when the data is forwarded to several various international network. We work out the process of problem based on several data of experiments. We analyze these problems and propose the research way to other researchers in overlay multicast area, and we also provide these useful results to other researchers in this area.},
booktitle = {Proceedings of the 6th International Conference on Mobile Technology, Application &amp; Systems},
articleno = {42},
numpages = {7},
keywords = {overlay, multicast, UniNet, measurement, TEIN2, KOREN, overlay multicast},
location = {Nice, France},
series = {Mobility '09}
}

@proceedings{10.1145/3548785,
title = {IDEAS '22: Proceedings of the 26th International Database Engineered Applications Symposium},
year = {2022},
isbn = {9781450397094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Budapest, Hungary}
}

@inproceedings{10.1145/3208159.3208187,
author = {Gavrilova, Marina L.},
title = {Machine Learning for Social Behavior Understanding},
year = {2018},
isbn = {9781450364010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208159.3208187},
doi = {10.1145/3208159.3208187},
abstract = {Human brain has an ability to perform a massive processing of auxiliary information such as visual cues, cognitive and social interactions, contextual and spatio-temporal data. Similarly to a human brain, social behavioral cues can aid the reliable decision-making of a biometric security system. Being an integral part of human behavior, social interactions are likely to possess unique behavioral patterns. This state-of-the-art review paper discusses an emerging person recognition approach based on the in-depth analysis of individuals' social behavior in order to enhance the performance of a traditional biometric system. The social behavioral information can be mined from their offline or online interactions, and can be identified as a set of Social Behavioral Biometric (SBB) features. These features could be used on their own or further combined with other behavioral and physiological patters, and classification can be enhanced by the use of machine learning approaches. An overview of open problems and challenges as well as applications of studying social behavior in various domains concludes this paper.},
booktitle = {Proceedings of Computer Graphics International 2018},
pages = {247–252},
numpages = {6},
keywords = {decision-making, social behavioral biometrics, Human behavior recognition, online networks, machine learning, virtual worlds},
location = {Bintan, Island, Indonesia},
series = {CGI 2018}
}

@inproceedings{10.1145/3243082.3264607,
author = {Costa, Daniel G.},
title = {On the Development of Visual Sensors with Raspberry Pi},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3264607},
doi = {10.1145/3243082.3264607},
abstract = {The increasing interest for Internet of Things (IoT) technologies has brought a lot of attention to microelectronics and sensors development. With the availability of affordable embedded platforms for countless applications, it is possible to develop low-cost programmable sensors to provide different types of data, benefiting applications in the IoT world. When cameras can be integrated to such development platforms, visual sensors can be easily created, supporting monitoring and controls functions based on the processing of images and videos. In this context, some of the most relevant details concerning the development of visual sensors with the Raspberry Pi platform are described herein, bringing fundamentals for the creation of highly programmable visual sensors.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {19–22},
numpages = {4},
keywords = {Wireless sensor networks, Visual sensors, Camera, Raspberry Pi, Internet of things},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@inproceedings{10.1145/3524458.3547121,
author = {Casini, Luca and Orr\`{u}, Valentina and Roccetti, Marco and Marchetti, Nicol\`{o}},
title = {When Machines Find Sites for the Archaeologists: A Preliminary Study with Semantic Segmentation Applied on Satellite Imagery of the Mesopotamian Floodplain},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524458.3547121},
doi = {10.1145/3524458.3547121},
abstract = {In the perspective of landscape archaeology, remote sensing is a very important tool that allows to recognize and locate potential sites, which will then be “groundtruthed” through a surface survey. Remote sensing is, unfortunately, a very time-consuming process that scales terribly with the size of the area under investigation. In this paper we explore the possibility of using semantic segmentation models to detect and highlight the presence of archaeological sites present in the Mesopotamian floodplain. Whereas archaeologists usually combine information from a variety of basemaps, including aerial and satellite photos taken from the 1950s onwards, we investigated the possibility of using an easily accessible online maps (in our case, Bing Maps). Trying to build an accessible and lightweight system also dictated the choice of trying pretrained segmentation models and use transfer learning. The preliminary results obtained (from different models and parameters choices), as well as the dataset, its idiosyncrasies and how we can deal with them are discussed in this paper.},
booktitle = {Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
pages = {378–383},
numpages = {6},
keywords = {human-in-the-loop, mesopotamian floodplain, archaeology, semantic segmentation},
location = {Limassol, Cyprus},
series = {GoodIT '22}
}

@inbook{10.1145/3310205.3310208,
title = {Outlier Detection},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310208},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.1145/3434173,
author = {Hockenhull, Michael and Cohn, Marisa Leavitt},
title = {Speculative Data Work &amp; Dashboards: Designing Alternative Data Visions},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3434173},
doi = {10.1145/3434173},
abstract = {This paper studies data work in an organizational context, and suggests speculative data work as a useful concept and the speculative dashboard as a design concept, to better understand and support cooperative work. Drawing on fieldwork in a Danish public sector organisation, the paper identifies and conceptualizes the speculative data work performed around processes of digitalization and the push to become data-driven. The speculative dashboard is proposed as a design concept and opportunity for design, using practices from speculative design and research to facilitate speculation about data?its sources, visualizations, practices and infrastructures. It does so by hacking the 'genre' of the business intelligence data dashboard, and using it as a framework for the juxtaposition of different kinds of data, facilitating and encouraging speculation on alternative visions for data types and use. The paper contributes an empirical study of organizational use of and attitudes towards data, informing a novel design method and concept for co-speculating on alternative visions of and for organizational data.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jan},
articleno = {264},
numpages = {31},
keywords = {data visualization, speculative design, ethnography, business intelligence, data work}
}

@inproceedings{10.1145/3368089.3417055,
author = {Chen, Zhuangbin and Kang, Yu and Li, Liqun and Zhang, Xu and Zhang, Hongyu and Xu, Hui and Zhou, Yangfan and Yang, Li and Sun, Jeffrey and Xu, Zhangwei and Dang, Yingnong and Gao, Feng and Zhao, Pu and Qiao, Bo and Lin, Qingwei and Zhang, Dongmei and Lyu, Michael R.},
title = {Towards Intelligent Incident Management: Why We Need It and How We Make It},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417055},
doi = {10.1145/3368089.3417055},
abstract = {The management of cloud service incidents (unplanned interruptions or outages of a service/product) greatly affects customer satisfaction and business revenue. After years of efforts, cloud enterprises are able to solve most incidents automatically and timely. However, in practice, we still observe critical service incidents that occurred in an unexpected manner and orchestrated diagnosis workflow failed to mitigate them. In order to accelerate the understanding of unprecedented incidents and provide actionable recommendations, modern incident management system employs the strategy of AIOps (Artificial Intelligence for IT Operations). In this paper, to provide a broad view of industrial incident management and understand the modern incident management system, we conduct a comprehensive empirical study spanning over two years of incident management practices at Microsoft. Particularly, we identify two critical challenges (namely, incomplete service/resource dependencies and imprecise resource health assessment) and investigate the underlying reasons from the perspective of cloud system design and operations. We also present IcM BRAIN, our AIOps framework towards intelligent incident management, and show its practical benefits conveyed to the cloud services of Microsoft.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1487–1497},
numpages = {11},
keywords = {Cloud Computing, AIOps, Incident Management},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1145/3484945,
author = {Bazai, Sibghat Ullah and Jang-Jaccard, Julian and Alavizadeh, Hooman},
title = {A Novel Hybrid Approach for Multi-Dimensional Data Anonymization for Apache Spark},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {2471-2566},
url = {https://doi.org/10.1145/3484945},
doi = {10.1145/3484945},
abstract = {Multi-dimensional data anonymization approaches (e.g., Mondrian) ensure more fine-grained data privacy by providing a different anonymization strategy applied for each attribute. Many variations of multi-dimensional anonymization have been implemented on different distributed processing platforms (e.g., MapReduce, Spark) to take advantage of their scalability and parallelism supports. According to our critical analysis on overheads, either existing iteration-based or recursion-based approaches do not provide effective mechanisms for creating the optimal number of and relative size of resilient distributed datasets (RDDs), thus heavily suffer from performance overheads. To solve this issue, we propose a novel hybrid approach for effectively implementing a multi-dimensional data anonymization strategy (e.g., Mondrian) that is scalable and provides high-performance. Our hybrid approach provides a mechanism to create far fewer RDDs and smaller size partitions attached to each RDD than existing approaches. This optimal RDD creation and operations approach is critical for many multi-dimensional data anonymization applications that create tremendous execution complexity. The new mechanism in our proposed hybrid approach can dramatically reduce the critical overheads involved in re-computation cost, shuffle operations, message exchange, and cache management.},
journal = {ACM Trans. Priv. Secur.},
month = {nov},
articleno = {5},
numpages = {25},
keywords = {Spark, resilient distributed dataset (RDD), data anonymization, multi-dimensional data, Mondrian}
}

@inproceedings{10.1145/3535508.3545565,
author = {Hornback, Andrew and Shi, Wenqi and Giuste, Felipe O. and Zhu, Yuanda and Carpenter, Ashley M. and Hilton, Coleman and Bijanki, Vinieth N. and Stahl, Hiram and Gottesman, Gary S. and Purnell, Chad and Iwinski, Henry J. and Wattenbarger, J. Michael and Wang, May D.},
title = {Development of a Generalizable Multi-Site and Multi-Modality Clinical Data Cloud Infrastructure for Pediatric Patient Care},
year = {2022},
isbn = {9781450393867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535508.3545565},
doi = {10.1145/3535508.3545565},
abstract = {World-renowned pediatric patient care in scoliosis, craniofacial, orthopedic, and other life-altering conditions is provided at the international Shriners Children's hospital system. The impact of scoliosis can be extreme with significant curvature of the spine that often progresses during childhood periods of growth and development. Gauging the impact of treatment is vital throughout the diagnostic and treatment process and is achieved using radiographic imaging and patient reported feedback surveys. Surgeons from multiple clinical centers have amassed a wealth of patient data from more than 1,000 scoliosis patients. However, these data are difficult to access due to data heterogeneity and poor interoperability between complex hospital systems. These barriers significantly decrease the value of these data to improve patient care. To solve these challenges, we create a generalizable multi-site and multi-modality cloud infrastructure for managing the clinical data of multiple diseases. First, we establish a standardized and secure research data repository using the Fast Health Interoperability Resources (FHIR) standard to harmonize multi-modal clinical data from different hospital sites. Additionally, we develop a SMART-on-FHIR application with a user-friendly graphical user interface (GUI) to enable non-technical users to access the harmonized clinical data. We demonstrate the generalizability of our solution by expanding it to also facilitate craniofacial microsomia and pediatric bone disease imaging research. Ultimately, we present a generalized framework for multi-site, multimodal data harmonization, which can efficiently organize and store data for clinical research to improve pediatric patient care.},
booktitle = {Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {23},
numpages = {10},
keywords = {user interface, pediatric patient care, data repository, data interoperability, SMART-on-FHIR, FHIR, ETL},
location = {Northbrook, Illinois},
series = {BCB '22}
}

@article{10.1109/TCBB.2019.2903804,
author = {Chen, Yiyuan and Wang, Yufeng and Cao, Liang and Jin, Qun},
title = {CCFS: A Confidence-Based Cost-Effective Feature Selection Scheme for Healthcare Data Classification},
year = {2019},
issue_date = {May-June 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2903804},
doi = {10.1109/TCBB.2019.2903804},
abstract = {Feature selection (FS) is one of the fundamental data processing techniques in various machine learning algorithms, especially for classification of healthcare data. However, it is a challenging issue due to the large search space. Binary Particle Swarm Optimization (BPSO) is an efficient evolutionary computation technique, and has been widely used in FS. In this paper, we proposed a Confidence-based and Cost-effective feature selection (CCFS) method using BPSO to improve the performance of healthcare data classification. Specifically, first, CCFS improves search effectiveness by developing a new updating mechanism that designs the feature confidence to explicitly take into account the fine-grained impact of each dimension in the particle on the classification performance. The feature confidence is composed of two measurements: the correlation between feature and categories, and historically selected frequency of each feature. Second, considering the fact that the acquisition costs of different features are naturally different, especially for medical data, and should be fully taken into account in practical applications, besides the classification performance, the feature cost and the feature reduction ratio are comprehensively incorporated into the design of fitness function. The proposed method has been verified in various UCI public datasets and compared with various benchmark schemes. The thoroughly experimental results show the effectiveness of the proposed method, in terms of accuracy and feature selection cost.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {902–911},
numpages = {10}
}

@inproceedings{10.1145/3524842.3528029,
author = {Tawosi, Vali and Al-Subaihin, Afnan and Moussa, Rebecca and Sarro, Federica},
title = {A Versatile Dataset of Agile Open Source Software Projects},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528029},
doi = {10.1145/3524842.3528029},
abstract = {Agile software development is nowadays a widely adopted practise in both open-source and industrial software projects. Agile teams typically heavily rely on issue management tools to document new issues and keep track of outstanding ones, in addition to storing their technical details, effort estimates, assignment to developers, and more. Previous work utilised the historical information stored in issue management systems for various purposes; however, when researchers make their empirical data public, it is usually relevant solely to the study's objective. In this paper, we present a more holistic and versatile dataset containing a wealth of information on more than half a million issues from 44 open-source Agile software, making it well-suited to several research avenues, and cross-analyses therein, including effort estimation, issue prioritization, issue assignment and many more. We make this data publicly available on GitHub to facilitate ease of use, maintenance, and extensibility.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {707–711},
numpages = {5},
keywords = {data mining, agile development, open-source software},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{10.1145/3480001.3480017,
author = {Jin, Ying and Gao, Ming and Yu, Jixiang},
title = {A Transformer Based Sales Prediction of Smart Container in New Retail Era},
year = {2021},
isbn = {9781450390163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3480001.3480017},
doi = {10.1145/3480001.3480017},
abstract = {With the advent of the new retail era, the value of unmanned smart container is increasingly prominent. Fast and flexible self-service is favored by consumers. How to use accumulated historical sales data to predict sales in the future is an important part of smart container operation management. Reasonable sales prediction can not only reduce the inventory cost, but also reduce the shortage loss of the container. Based on the smart container sales data of Dalian Xiaode New Retail Co., Ltd., through detailed exploratory analysis in many aspects, this paper carries out the feature selection of sales prediction, and uses random forest, XGBoost, Transformer and other algorithms to predict sales. The experimental results show that the prediction accuracy of Transformer is better than traditional algorithms, whose MAPE is 14.67% lower than that of the worst one. Transformer can be well applied in the field of sales prediction of smart container. And in this experiment, compared with Transformer using sine and cosine functions for positional encoding, Transformer encoded by position index has better prediction performance and stronger stability.},
booktitle = {Proceedings of the 2021 5th International Conference on Deep Learning Technologies},
pages = {46–53},
numpages = {8},
keywords = {Smart container, Transformer, Sales prediction},
location = {Qingdao, China},
series = {ICDLT '21}
}

@inbook{10.1145/3447404.3447406,
author = {Eslambolchilar, Parisa and Komninos, Andreas and Dunlop, Mark D.},
title = {Introduction},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447406},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {1–13},
numpages = {13}
}

@inproceedings{10.1145/3437800.3439203,
author = {Raj, Rajendra K. and Romanowski, Carol J. and Impagliazzo, John and Aly, Sherif G. and Becker, Brett A. and Chen, Juan and Ghafoor, Sheikh and Giacaman, Nasser and Gordon, Steven I. and Izu, Cruz and Rahimi, Shahram and Robson, Michael P. and Thota, Neena},
title = {High Performance Computing Education: Current Challenges and Future Directions},
year = {2020},
isbn = {9781450382939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437800.3439203},
doi = {10.1145/3437800.3439203},
abstract = {High Performance Computing (HPC) is the ability to process data and perform complex calculations at extremely high speeds. Current HPC platforms can achieve calculations on the order of quadrillions of calculations per second, with quintillions on the horizon. The past three decades witnessed a vast increase in the use of HPC across different scientific, engineering, and business communities on problems such as sequencing the genome, predicting climate changes, designing modern aerodynamics, or establishing customer preferences. Although HPC has been well incorporated into science curricula such as bioinformatics, the same cannot be said for most computing programs. Computing educators are only now beginning to recognize the need for HPC Education (HPCEd). Building on earlier work, this working group explored how HPCEd can make inroads into computing education, focusing on the undergraduate level. This paper presents the background of HPC and HPCEd, identifies several of the needed core HPC competencies for students, identifies the support needed by educators for HPCEd, and explores the symbiosis between HPCEd and computing education in contemporary areas such as artificial intelligence and data science, as well as how HPCEd can be applied to benefit diverse non-computing domains such as atmospheric science, biological sciences and critical infrastructure protection. Finally, the report makes several recommendations to improve and facilitate HPC education in the future.},
booktitle = {Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {51–74},
numpages = {24},
keywords = {iticse working group, contemporary computing education, high-performance computing curricula, hpc education, high performance computing, computer science education},
location = {Trondheim, Norway},
series = {ITiCSE-WGR '20}
}

@inproceedings{10.1145/3085504.3085505,
author = {Gorenflo, Christian and Golab, Lukasz and Keshav, Srinivasan},
title = {Managing Sensor Data Streams: Lessons Learned from the WeBike Project},
year = {2017},
isbn = {9781450352826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085504.3085505},
doi = {10.1145/3085504.3085505},
abstract = {We present insights on data management resulting from a field deployment of approximately 30 sensor-equipped electric bicycles (e-bikes) at the University of Waterloo. The trial has been in operation for the last two-and-a-half years, and we have collected and analyzed more than 150 gigabytes of data. We discuss best practices for the entire data management process, spanning data collection, extract-transform-load, data cleaning, and choosing a suitable data management ecosystem. We also comment on how our experiences will inform the design of a future large-scale field trial involving several thousand fully-instrumented e-bikes.},
booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
articleno = {1},
numpages = {11},
keywords = {Data feed management, Time series data management, Data management for the Internet of Things (IoT)},
location = {Chicago, IL, USA},
series = {SSDBM '17}
}

@inproceedings{10.1145/3538641.3561482,
author = {Huang, Dong and Ye, Xiucai and Sakurai, Tetsuya},
title = {Knowledge Distillation-Based Privacy-Preserving Data Analysis},
year = {2022},
isbn = {9781450393980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538641.3561482},
doi = {10.1145/3538641.3561482},
abstract = {Over the past decade, there has been an increasing focus on data privacy, which is one of the most popular areas of research today. Due to legal restrictions, it is difficult to centralize data, so multi-party collaborative learning becomes difficult to achieve. In this paper, we propose a data collaborative analysis method for protecting data privacy: Knowledge Distillation-based Analysis (KDDA). The proposed method can effectively solve the above problems. The framework of the proposed method is based on "teacher-student" network. Specifically, each institution (e.g., hospital) used sensitive data to train local models, which we call teacher models. Then we introduce unlabeled public datasets. We design an aggregator that aggregates all teacher models to generate pseudo-labels for the public dataset. It is worth noting that to minimize the risk of data privacy exposure, we limit the number of generated pseudo-labels. Finally, we train the student model using semi-supervised learning on the public dataset with pseudo-labels. Since the student model is not directly trained on sensitive data, it will not lead to the leakage of data privacy, so it can be publicized. To evaluate the effectiveness of the proposed method, we conduct experiments on three popular image benchmark datasets: MNIST, SVHN, and CIFAR-10. Experimental results show that the proposed method is effective.},
booktitle = {Proceedings of the Conference on Research in Adaptive and Convergent Systems},
pages = {15–20},
numpages = {6},
keywords = {privacy-preserving, data analysis, knowledge distillation},
location = {Virtual Event, Japan},
series = {RACS '22}
}

@inproceedings{10.1145/3357492.3358628,
author = {Dominguez, Hector and Mowry, Judith and Perez, Elisabeth and Kendrick, Christine and Martin, Kevin},
title = {Privacy and Information Protection for a New Generation of City Services},
year = {2019},
isbn = {9781450369787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357492.3358628},
doi = {10.1145/3357492.3358628},
abstract = {This paper will showcase the work that the City of Portland has done around developing Privacy and Information Protection Principles considering the current state of technology, the social digital age, and advance inference algorithms like machine learning or other Artificial Intelligence tools. By creating more responsible data stewardship in the public sector, municipalities are set to build trusted information networks involving communities and complex social issues. Particularly, the promotion of data privacy can lead to the emergence of anti-poverty and economic development strategies.The City of Portland has developed seven Privacy and Information Protection Principles: Transparency and accountability, full lifecycle stewardship, equitable data management, ethical and non-discriminatory use of data, data openness, automated decision systems, and data utility. These principles have implications in social equity and the future of technology management in smart cities projects. Principle implementation involves the collaboration of different agencies, particularly focused on ethics and human rights supporting sustainable development.This work is part of emergent strategies for a new generation of city services based on data and information, which aim to improve civic engagement, social benefits to communities in city neighborhoods and better collaboration with partners and other government agencies.},
booktitle = {Proceedings of the 2nd ACM/EIGSCC Symposium on Smart Cities and Communities},
articleno = {5},
numpages = {6},
keywords = {Privacy, Digital equity, Automatic decision systems, Digital Inclusion, government services},
location = {Portland, OR, USA},
series = {SCC '19}
}

@article{10.1109/TCBB.2016.2535251,
author = {Ma, Tianle and Zhang, Aidong},
title = {Omics Informatics: From Scattered Individual Software Tools to Integrated Workflow Management Systems},
year = {2017},
issue_date = {July 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2535251},
doi = {10.1109/TCBB.2016.2535251},
abstract = {Omic data analyses pose great informatics challenges. As an emerging subfield of bioinformatics, omics informatics focuses on analyzing multi-omic data efficiently and effectively, and is gaining momentum. There are two underlying trends in the expansion of omics informatics landscape: the explosion of scattered individual omics informatics tools with each of which focuses on a specific task in both single- and multi- omic settings, and the fast-evolving integrated software platforms such as workflow management systems that can assemble multiple tools into pipelines and streamline integrative analysis for complicated tasks. In this survey, we give a holistic view of omics informatics, from scattered individual informatics tools to integrated workflow management systems. We not only outline the landscape and challenges of omics informatics, but also sample a number of widely used and cutting-edge algorithms in omics data analysis to give readers a fine-grained view. We survey various workflow management systems WMSs, classify them into three levels of WMSs from simple software toolkits to integrated multi-omic analytical platforms, and point out the emerging needs for developing intelligent workflow management systems. We also discuss the challenges, strategies and some existing work in systematic evaluation of omics informatics tools. We conclude by providing future perspectives of emerging fields and new frontiers in omics informatics.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {jul},
pages = {926–946},
numpages = {21}
}

@article{10.1145/3502736,
author = {Varde, Aparna S.},
title = {Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3502736},
doi = {10.1145/3502736},
abstract = {Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {86},
numpages = {52},
keywords = {machine learning, graphical data mining, predictive analytics, domain knowledge, classification, scientific applications, estimation, Applied research, clustering}
}

@inproceedings{10.1145/3340531.3412105,
author = {Ranbaduge, Thilina and Schnell, Rainer},
title = {Securing Bloom Filters for Privacy-Preserving Record Linkage},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412105},
doi = {10.1145/3340531.3412105},
abstract = {Privacy-preserving record linkage (PPRL) facilitates the matching of records that correspond to the same real-world entities across different databases while preserving the privacy of the individuals in these databases. A Bloom filter (BF) is a space efficient probabilistic data structure that is becoming popular in PPRL as an efficient privacy technique to encode sensitive information in records while still enabling approximate similarity computations between attribute values. However, BF encoding is susceptible to privacy attacks which can re-identify the values that are being encoded. In this paper we propose two novel techniques that can be applied on BF encoding to improve privacy against attacks. Our techniques use neighbouring bits in a BF to generate new bit values. An empirical study on large real databases shows that our techniques provide high security against privacy attacks, and achieve better similarity computation accuracy and linkage quality compared to other privacy improvements that can be applied on BF encoding.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2185–2188},
numpages = {4},
keywords = {hardening, xor, sliding window, perturbation, random sampling},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3233347.3233373,
author = {Hagemann, Simon and Stark, Rainer},
title = {Automated Body-in-White Production System Design: Data-Based Generation of Production System Configurations},
year = {2018},
isbn = {9781450364720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233347.3233373},
doi = {10.1145/3233347.3233373},
abstract = {Design processes for production systems (PS) in the automotive body-in-white (BIW) sector tie up tremendous resources. Current challenges like the continuous increase of product variants and product complexity have direct impact on the required planning effort in production system design (PSD), which is currently increasing significantly. Analysis of these design processes have revealed a high potential for process automatization. In order to achieve this, suitable methods are required as well as a data basis of reasonable quality. Both methods and data basis are deeply investigated in this paper. The investigations' results create a solid basis for further research in the young field of automated BIW PSD.},
booktitle = {Proceedings of the 4th International Conference on Frontiers of Educational Technologies},
pages = {192–196},
numpages = {5},
keywords = {ruled-based algorithms, optimization, data mining, process automatization, knowledge-engineering, automotive, body-in-white, artificial intelligence, production system design},
location = {Moscow, Russian Federation},
series = {ICFET '18}
}

@inproceedings{10.1145/2494091.2499576,
author = {Blunck, Henrik and Bouvin, Niels Olof and Franke, Tobias and Gr\o{}nb\ae{}k, Kaj and Kjaergaard, Mikkel B. and Lukowicz, Paul and W\"{u}stenberg, Markus},
title = {On Heterogeneity in Mobile Sensing Applications Aiming at Representative Data Collection},
year = {2013},
isbn = {9781450322157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494091.2499576},
doi = {10.1145/2494091.2499576},
abstract = {Gathering representative data using mobile sensing to answer research questions is becoming increasingly popular, driven by growing ubiquity and sensing capabilities of mobile devices. However, there are pitfalls along this path, which introduce heterogeneity in the gathered data, and which are rooted in the diversity of the involved device platforms, hardware, software versions and participants. Thus, we, as a research community, need to establish good practices and methodologies for addressing this issue in order to help ensure that, e.g., scientific results and policy changes based on collective, mobile sensed data are valid. In this paper, we aim to inform researchers and developers about mobile sensing data heterogeneity and ways to combat it. We do so via distilling a vocabulary of underlying causes, and via describing their effects on mobile sensing---building on experiences from three projects within citizen science, crowd awareness and trajectory tracking.},
booktitle = {Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication},
pages = {1087–1098},
numpages = {12},
keywords = {mobile sensing, data collection, representativeness, heterogeneity},
location = {Zurich, Switzerland},
series = {UbiComp '13 Adjunct}
}

@inproceedings{10.1145/3384544.3384611,
author = {Hartner, Raphael and Mezhuyev, Vitaliy and Tschandl, Martin and Bischof, Christian},
title = {Digital Shop Floor Management: A Practical Framework For Implementation},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384611},
doi = {10.1145/3384544.3384611},
abstract = {In the context of manufacturing, shop floor management (SFM) is employed to ensure efficient production operations and workflows. Advanced technologies and methods can be used to improve the SFM and achieve close to real-time responsiveness. Even though there is a number of research available for the digitalized SFM (DSFM), a supportive framework for implementation purposes was not considered yet. Consequently, this paper utilizes concepts from related disciplines and research areas to derive an architectural framework for a DSFM. This particular architecture is then implemented to ensure its practicability and foster the understanding of challenges and opportunities. The proposed multi-layer framework and supportive methods can be employed by manufacturing companies to implement a DSFM focused on interoperability, security and low-latency.},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {41–45},
numpages = {5},
keywords = {Smart Production, Middleware, Retrofitting, Mist Computing, Fog Computing, Shop Floor Management, Industry 4.0, Cloud Computing, Lean Production},
location = {Langkawi, Malaysia},
series = {ICSCA 2020}
}

@article{10.1145/3485875,
author = {Yang, Qiang},
title = {Toward Responsible AI: An Overview of Federated Learning for User-Centered Privacy-Preserving Computing},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3–4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3485875},
doi = {10.1145/3485875},
abstract = {With the rapid advances of Artificial Intelligence (AI) technologies and applications, an increasing concern is on the development and application of responsible AI technologies. Building AI technologies or machine-learning models often requires massive amounts of data, which may include sensitive, user private information to be collected from different sites or countries. Privacy, security, and data governance constraints rule out a brute force process in the acquisition and integration of these data. It is thus a serious challenge to protect user privacy while achieving high-performance models. This article reviews recent progress of federated learning in addressing this challenge in the context of privacy-preserving computing. Federated learning allows global AI models to be trained and used among multiple decentralized data sources with high security and privacy guarantees, as well as sound incentive mechanisms. This article presents the background, motivations, definitions, architectures, and applications of federated learning as a new paradigm for building privacy-preserving, responsible AI ecosystems.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {oct},
articleno = {32},
numpages = {22},
keywords = {machine learning, decentralized AI, user privacy, blockchain, privacy-preserving computing, Federated learning, data security, responsible AI}
}

@inproceedings{10.1145/3313831.3376485,
author = {Gathani, Sneha and Lim, Peter and Battle, Leilani},
title = {Debugging Database Queries: A Survey of Tools, Techniques, and Users},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376485},
doi = {10.1145/3313831.3376485},
abstract = {Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–16},
numpages = {16},
keywords = {visualization, debugging databases, survey, empirical study, literature review},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1145/3502288,
author = {Barth, Susanne and Ionita, Dan and Hartel, Pieter},
title = {Understanding Online Privacy—A Systematic Review of Privacy Visualizations and Privacy by Design Guidelines},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3502288},
doi = {10.1145/3502288},
abstract = {Privacy visualizations help users understand the privacy implications of using an online service. Privacy by Design guidelines provide generally accepted privacy standards for developers of online services. To obtain a comprehensive understanding of online privacy, we review established approaches, distill a unified list of 15 privacy attributes and rank them based on perceived importance by users and privacy experts. We then discuss similarities, explain notable differences, and examine trends in terms of the attributes covered. Finally, we show how our results provide a foundation for user-centric privacy visualizations, inspire best practices for developers, and give structure to privacy policies.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {63},
numpages = {37},
keywords = {privacy by design, privacy icons, privacy factors, privacy labels, Privacy attributes}
}

@inproceedings{10.1145/3469213.3470272,
author = {Hu, Yerong and He, Xiangzhen and Zhang, Yihao and Zeng, Jia and Yang, Huaiyuan and Zhou, Shuaihang},
title = {Research and Application of Digital Collection Method of Human Movement},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470272},
doi = {10.1145/3469213.3470272},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {71},
numpages = {6},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3003733.3003761,
author = {Gatziolis, Kleanthis and Boucouvalas, Anthony C.},
title = {User Profile Extraction Engine},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003761},
doi = {10.1145/3003733.3003761},
abstract = {The Internet is overwhelmed by a huge amount of information every day and every user has different interests from another. It is therefore important that this information is filtered and sorted according to their preferences. Thus, the profiling systems exploit particularities and preferences of each user and finally they can be studied or used by other applications or humans. This paper analyzes the methods of collecting data (data gathering), and the ways in which this information can be used - filtered so as to create knowledge. A user profile extraction engine is presented and analyzed.},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {41},
numpages = {6},
keywords = {User Profiling, e-Shopping, E-Commerce, Mobile shopping, Retailing},
location = {Patras, Greece},
series = {PCI '16}
}

@inproceedings{10.1145/3400903.3400908,
author = {Schuler, Robert and Czajkowski, Karl and D'Arcy, Mike and Tangmunarunkit, Hongsuda and Kesselman, Carl},
title = {Towards Co-Evolution of Data-Centric Ecosystems},
year = {2020},
isbn = {9781450388146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400903.3400908},
doi = {10.1145/3400903.3400908},
abstract = {Database evolution is a notoriously difficult task, and it is exacerbated by the necessity to evolve database-dependent applications. As science becomes increasingly dependent on sophisticated data management, the need to evolve an array of database-driven systems will only intensify. In this paper, we present an architecture for data-centric ecosystems that allows the components to seamlessly co-evolve by centralizing the models and mappings at the data service and pushing model-adaptive interactions to the database clients. Boundary objects fill the gap where applications are unable to adapt and need a stable interface to interact with the components of the ecosystem. Finally, evolution of the ecosystem is enabled via integrated schema modification and model management operations. We present use cases from actual experiences that demonstrate the utility of our approach.},
booktitle = {32nd International Conference on Scientific and Statistical Database Management},
articleno = {4},
numpages = {12},
keywords = {model management, schema evolution, software ecosystems, application-database co-evolution},
location = {Vienna, Austria},
series = {SSDBM 2020}
}

@inproceedings{10.1145/3209281.3209326,
author = {Haak, Elise and Ubacht, Jolien and Van den Homberg, Marc and Cunningham, Scott and Van den Walle, Bartel},
title = {A Framework for Strengthening Data Ecosystems to Serve Humanitarian Purposes},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209326},
doi = {10.1145/3209281.3209326},
abstract = {The incidence of natural disasters worldwide is increasing. As a result, a growing number of people is in need of humanitarian support, for which limited resources are available. This requires an effective and efficient prioritization of the most vulnerable people in the preparedness phase, and the most affected people in the response phase of humanitarian action. Data-driven models have the potential to support this prioritization process. However, the applications of these models in a country requires a certain level of data preparedness. To achieve this level of data preparedness on a large scale we need to know how to facilitate, stimulate and coordinate data-sharing between humanitarian actors. We use a data ecosystem perspective to develop success criteria for establishing a "humanitarian data ecosystem". We first present the development of a general framework with data ecosystem governance success criteria based on a systematic literature review. Subsequently, the applicability of this framework in the humanitarian sector is assessed through a case study on the "Community Risk Assessment and Prioritization toolbox" developed by the Netherlands Red Cross. The empirical evidence led to the adaption the framework to the specific criteria that need to be addressed when aiming to establish a successful humanitarian data ecosystem.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {85},
numpages = {9},
keywords = {framework, humanitarian sector, data preparedness, governance, data ecosystem},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3488560.3498421,
author = {Bhargav, Samarth and Sidiropoulos, Georgios and Kanoulas, Evangelos},
title = { 'It's on the Tip of My Tongue': A New Dataset for Known-Item Retrieval},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498421},
doi = {10.1145/3488560.3498421},
abstract = {The tip of the tongue known-item retrieval (TOT-KIR) task involves the 'one-off' retrieval of an item for which a user cannot recall a precise identifier. The emergence of several online communities where users pose known-item queries to other users indicates the inability of existing search systems to answer such queries. Research in this domain is hampered by the lack of large, open or realistic datasets. Prior datasets relied on either annotation by crowd workers, which can be expensive and time-consuming, or generating synthetic queries, which can be unrealistic. Additionally, small datasets make the application of modern (neural) retrieval methods unviable, since they require a large number of data-points. In this paper, we collect the largest dataset yet with 15K query-item pairs in two domains, namely, Movies and Books, from an online community using heuristics, rendering expensive annotation unnecessary while ensuring that queries are realistic. We show that our data collection method is accurate by conducting a data study. We further demonstrate that methods like BM25 fall short of answering such queries, corroborating prior research. The size of the dataset makes neural methods feasible, which we show outperforms lexical baselines, indicating that neural/dense retrieval is superior for the TOT-KIR task.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {48–56},
numpages = {9},
keywords = {known item retrieval, tip of the tongue known item retrieval},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3325112.3325222,
author = {S. Bargh, Mortaza and Meijer, Ronald and Vink, Marco and van Den Braak, Susan and Schirm, Walter and Choenni, Sunil},
title = {Opening Privacy Sensitive Microdata Sets in Light of GDPR},
year = {2019},
isbn = {9781450372046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325112.3325222},
doi = {10.1145/3325112.3325222},
abstract = {To enhance the transparency, accountability and efficiency of the Dutch Ministry of Justice and Security, the ministry has set up an open data program to proactively stimulate sharing its (publicly funded) data sets with the public. Disclosure of personal data is considered as one of the main threats for data opening. In this contribution we argue that, according to Dutch laws, the criminal data within the Dutch justice domain are sensitive data in GDPR terms and that the criminal data can only be opened if these sensitive data are transformed to have no personal information. Subsequently, having no personal information in data sets is related to two GDPR concepts: the data being anonymous in its GDPR sense or the data being pseudonymized in its GDPR sense. These two GDPR concepts, i.e., being anonymous data or pseudonymized data in a GDPR sense, can be distinguished in our setting based on whether the data controller cannot or can revert the data protection process, respectively. (Note that the terms anonymous and pseudonymized are interpreted differently in the technical domain.) We examine realizing these GDPR concepts with the Statistical Disclosure Control (SDC) technology and subsequently argue that pseudonymized data in a GDPR sense delivers a better data utility than the other. At the end, we present a number of the consequences of adopting either of these concepts, which can inform legislators and policymakers to define their strategy for opening privacy sensitive microdata sets, like those pertaining to the Dutch criminal justice domain.},
booktitle = {Proceedings of the 20th Annual International Conference on Digital Government Research},
pages = {314–323},
numpages = {10},
keywords = {Criminal justice data, Data protection, GDPR, Microdata, Open data, Justice domain data, Privacy},
location = {Dubai, United Arab Emirates},
series = {dg.o 2019}
}

@inproceedings{10.1145/3479645.3479661,
author = {Kusnandar, Toni and Surendro, Kridanto},
title = {Camera-Based Vegetation Index from Unmanned Aerial Vehicles},
year = {2021},
isbn = {9781450384070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479645.3479661},
doi = {10.1145/3479645.3479661},
abstract = {Agriculture assumes a vital role in human life because it provides food, feed for livestock, and bioenergy. The agricultural sector is expected to meet the needs of secure and nutritious food for the community at all times to boost productivity. Providing nutrition, water and light precisely and measuredly is an important effort in plant cultivation to produce quality. This effort can be materialized by implementing smart farming involving devices and information technology. Vast field surveillance or monitoring is made easy with the advent of unmanned aerial vehicle (UAV). Detection of plant condition can be achieved by obtaining Vegetation Index (VI) through camera imaging in UAVs which are more economic compared to multispectral or hyperspectral cameras. This study aims to obtain VI that is accurate but still economical, so that it can be utilized even by small-scale agriculture. The work that will be done is to conduct repair experiments at several stages of image processing to produce a new, more accurate VI. The research stages started from experiments on previous research, to finding new research opportunities in VI. Furthermore, the experiment was carried out with the addition of white balance value parameters and other UAV sensor parameters at the Pre-Processing stage to improve its quality. The hypothesis of adding white balance parameters should prove to be more accurate in correcting shooting in various light conditions. Next, try to modify the feature extraction algorithm using Color Extraction Edge Detection. Followed by modifying it using Back Propagation Neural Network to increase accuracy at the image processing stage. After synthesizing some of these experiments, a new formula or model VI using the camera on the UAV is expected to be produced. This research will contribute to the modification of methods or algorithms at the image processing stage to produce a corrected image in producing a new VI that is more accurate using a camera on a more economical UAV.},
booktitle = {6th International Conference on Sustainable Information Engineering and Technology 2021},
pages = {173–178},
numpages = {6},
keywords = {Unmanned Aerial Vehicle, Image Processing, Vegetation Index, Precission Agriculture},
location = {Malang, Indonesia},
series = {SIET '21}
}

@inproceedings{10.1145/2905055.2905184,
author = {Jangra, Ajay and Singh, Niharika and Lakhina, Upasana},
title = {VIP: Verification and Identification Protective Data Handling Layer Implementation to Achieve MVCC in Cloud Computing},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905184},
doi = {10.1145/2905055.2905184},
abstract = {Over transactional database systems MultiVersion concurrency control is maintained for secure, fast and efficient access to the shared data file implementation scenario. An effective coordination is supposed to be set up between owners and users also the developers &amp; system operators, to maintain inter-cloud &amp; intra-cloud communication Most of the services &amp; application offered in cloud world are real-time, which entails optimized compatibility service environment between master and slave clusters. In the paper, offered methodology supports replication and triggering methods intended for data consistency and dynamicity. Where intercommunication between different clusters is processed through middleware besides slave intra-communication is handled by verification &amp; identification protection. The proposed approach incorporates resistive flow to handle high impact systems that identifies and verifies multiple processes. Results show that the new scheme reduces the overheads from different master and slave servers as they are co-located in clusters which allow increased horizontal and vertical scalability of resources.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {124},
numpages = {6},
keywords = {Verification &amp; Identification, Cloud computing, MVCC, Transaction Manager, Serializability, Data version validation},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1145/3139958.3140013,
author = {Lin, Yijun and Chiang, Yao-Yi and Pan, Fan and Stripelis, Dimitrios and Ambite, Jose Luis and Eckel, Sandrah P. and Habre, Rima},
title = {Mining Public Datasets for Modeling Intra-City PM2.5 Concentrations at a Fine Spatial Resolution},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3140013},
doi = {10.1145/3139958.3140013},
abstract = {Air quality models are important for studying the impact of air pollutant on health conditions at a fine spatiotemporal scale. Existing work typically relies on area-specific, expert-selected attributes of pollution emissions (e,g., transportation) and dispersion (e.g., meteorology) for building the model for each combination of study areas, pollutant types, and spatiotemporal scales. In this paper, we present a data mining approach that utilizes publicly available OpenStreetMap (OSM) data to automatically generate an air quality model for the concentrations of fine particulate matter less than 2.5 μm in aerodynamic diameter at various temporal scales. Our experiment shows that our (domain-) expert-free model could generate accurate PM2.5 concentration predictions, which can be used to improve air quality models that traditionally rely on expert-selected input. Our approach also quantifies the impact on air quality from a variety of geographic features (i.e., how various types of geographic features such as parking lots and commercial buildings affect air quality and from what distance) representing mobile, stationary and area natural and anthropogenic air pollution sources. This approach is particularly important for enabling the construction of context-specific spatiotemporal models of air pollution, allowing investigations of the impact of air pollution exposures on sensitive populations such as children with asthma at scale.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {25},
numpages = {10},
keywords = {Air Quality Modeling, Geospatial Data Mining, PM2.5 Concentration Prediction},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@inproceedings{10.1145/3428502.3428514,
author = {Papadopoulos, Theodoros and Charalabidis, Yannis},
title = {What Do Governments Plan in the Field of Artificial Intelligence? Analysing National AI Strategies Using NLP},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428514},
doi = {10.1145/3428502.3428514},
abstract = {The primary goal of this paper is to explore how Natural Language Processing techniques (NLP) can assist in reviewing, understanding, and drawing conclusions from text datasets. We explore NLP techniques for the analysis and the extraction of useful information from the text of twelve national strategies on artificial intelligence (AI). For this purpose, we are using a set of machine learning algorithms in order to (a) extract the most significant keywords and summarize each strategy document, (b) discover and assign topics to each document, and (c) cluster the strategies based on their pair-wise similarity. Using the results of the analysis, we discuss the findings and highlight critical issues that emerge from the national strategies for artificial intelligence, such as the importance of the data ecosystem for the development of AI, the increasing considerations about ethical and safety issues, as well as the growing ambition of many countries to lead in the AI race. Utilizing the LDA topic model, we were able to reveal the distributions of thematic sub-topics among the strategic documents. The topic modelling distributions were then used along with other document similarity measures as an input for the clustering of the strategic documents into groups. The results revealed three clusters of countries with a visible differentiation between the strategies of China and Japan on the one hand and the Scandinavian strategies (plus the German and the Luxemburgish) one on the other. The former promote technology and innovation-driven development plans in order to integrate AI with the economy, while the latter share a common view regarding the role of the public sector both as a promoter and investor but also as a user and beneficiary of AI, and give a higher priority to the ethical &amp; safety issues that are connected to the development of AI.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {100–111},
numpages = {12},
keywords = {topic modelling, Automated Text Analysis, NLP, machine learning, document similarity, AI strategies},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@article{10.1145/3570733.3570734,
author = {Stach, Christoph and Gritti, Cl\'{e}mentine and Przytarski, Dennis and Mitschang, Bernhard},
title = {Assessment and Treatment of Privacy Issues in Blockchain Systems},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3570733.3570734},
doi = {10.1145/3570733.3570734},
abstract = {The ability to capture and quantify any aspect of daily life via sensors, enabled by the Internet of Things (IoT), data have become one of the most important resources of the 21st century. However, the high value of data also renders data an appealing target for criminals. Two key protection goals when dealing with data are therefore to maintain their permanent availability and to ensure their integrity. Blockchain technology provides a means of data protection that addresses both of these objectives. On that account, blockchains are becoming increasingly popular for the management of critical data. As blockchains are operated in a decentralized manner, they are not only protected against failures, but it is also ensured that neither party has sole control over the managed data. Furthermore, blockchains are immutable and tamper-proof data stores, whereby data integrity is guaranteed. While these properties are preferable from a data security perspective, they also pose a threat to privacy and confidentiality, as data cannot be concealed, rectified, or deleted once they are added to the blockchain.In this paper, we therefore investigate which features of the blockchain pose an inherent privacy threat when dealing with personal or confidential data. To this end, we consider to what extent blockchains are in compliance with applicable data protection laws, namely the European General Data Protection Regulation (GDPR). Based on our identified key issues, we assess which concepts and technical measures can be leveraged to address these issues in order to create a privacy-by-design blockchain system.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {nov},
pages = {5–24},
numpages = {20},
keywords = {data authentication, privacy assessment, privacy control environment, GDPR, immutable, permission control, data purging, privacy filters, decentralized, tamper-proof, blockchain}
}

@inproceedings{10.1145/3473465.3473478,
author = {Sun, Xiyan and Xiao, Yu and Ji, Yuanfa and Huang, Jianhua and Bai, Yang},
title = {Multi Scale UNet Encoder-Decoder Network for Building Extraction},
year = {2021},
isbn = {9781450389884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473465.3473478},
doi = {10.1145/3473465.3473478},
abstract = {Buildings in remote sensing images have large scale differences and complex shapes. And there are often distractors with visual features similar to buildings in complex scenes. The traditional methods used to extract buildings are limited by the ability of feature representation, resulting in low accuracy and low universality. The semantic segmentation network based on the Encoder-Decoder structure can automatically learn multi-level building feature representation from the data set, and achieve end-to-end building extraction. UNet is a typical semantic segmentation Encoder-Decoder network, but UNet cannot explore enough building information. Small buildings are easy to be missed, large buildings with complex colors and shapes are incompletely extracted, boundary segmentation is inaccurate. And the network is easily affected by roads, trees, shadows and other distractors. Therefore, this article improves UNet and proposes a multi-scale Encoder-Decoder network to learn multi-scale and distinguishable features to better identify buildings and backgrounds. We experiment with the improved network and the classic U-Net on two data sets, and show that the multi-scale Encoder-Decoder network can effectively improve the accuracy of building extraction.},
booktitle = {2021 3rd International Conference on Information Technology and Computer Communications},
pages = {73–79},
numpages = {7},
location = {Guangzhou, China},
series = {ITCC 2021}
}

@inproceedings{10.1145/2631775.2631806,
author = {Zhang, Kunpeng and Bhattacharyya, Siddhartha and Ram, Sudha},
title = {Empirical Analysis of Implicit Brand Networks on Social Media},
year = {2014},
isbn = {9781450329545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2631775.2631806},
doi = {10.1145/2631775.2631806},
abstract = {This paper investigates characteristics of implicit brand networks extracted from a large dataset of user historical activities on a social media platform. To our knowledge, this is one of the first studies to comprehensively examine brands by incorporating user-generated social content and information about user interactions. This paper makes several important contributions. We build and normalize a weighted, undirected network representing interactions among users and brands. We then explore the structure of this network using modified network measures to understand its characteristics and implications. As a part of this exploration, we address three important research questions: (1) What is the structure of a brand-brand network? (2) Does an influential brand have a large number of fans? (3) Does an influential brand receive more positive or more negative comments from social users? Experiments conducted with Facebook data show that the influence of a brand has (a) high positive correlation with the size of a brand, meaning that an influential brand can attract more fans, and, (b) low negative correlation with the sentiment of comments made by users on that brand, which means that negative comments have a more powerful ability to generate awareness of a brand than positive comments. To process the large-scale datasets and networks, we implement MapReduce-based algorithms.},
booktitle = {Proceedings of the 25th ACM Conference on Hypertext and Social Media},
pages = {190–199},
numpages = {10},
keywords = {social media, mapreduce, sentiment identification, network analysis, marketing intelligence},
location = {Santiago, Chile},
series = {HT '14}
}

@inproceedings{10.1145/3477314.3506986,
author = {Stach, Christoph and Gritti, Cl\'{e}mentine and Przytarski, Dennis and Mitschang, Bernhard},
title = {Can Blockchains and Data Privacy Laws Be Reconciled? A Fundamental Study of How Privacy-Aware Blockchains Are Feasible},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3506986},
doi = {10.1145/3477314.3506986},
abstract = {Due to the advancing digitalization, the importance of data is constantly increasing. Application domains such as smart cars, smart cities, or smart healthcare rely on the permanent availability of large amounts of data to all parties involved. As a result, the value of data increases, making it a lucrative target for cyber-attacks. Particularly when human lives depend on the data, additional protection measures are therefore important for data management and provision. Blockchains, i. e., decentralized, immutable, and tamper-proof data stores, are becoming increasingly popular for this purpose. Yet, from a data protection perspective, the immutable and tamper-proof properties of blockchains pose a privacy concern. In this paper, we therefore investigate whether blockchains are in compliance with the General Data Protection Regulation (GDPR) if personal data are involved. To this end, we elaborate which articles of the GDPR are relevant in this regard and present technical solutions for those legal requirements with which blockchains are in conflict. We further identify open research questions that need to be addressed in order to achieve a privacy-by-design blockchain system.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1218–1227},
numpages = {10},
keywords = {tamper-proof, privacy assessment, blockchains, immutable, GDPR},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3313831.3376662,
author = {Shipman, Frank M. and Marshall, Catherine C.},
title = {Ownership, Privacy, and Control in the Wake of Cambridge Analytica: The Relationship between Attitudes and Awareness},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376662},
doi = {10.1145/3313831.3376662},
abstract = {Has widespread news of abuse changed the public's perceptions of how user-contributed content from social networking sites like Facebook and LinkedIn can be used? We collected two datasets that reflect participants' attitudes about content ownership, privacy, and control, one in April 2018, while Cambridge Analytica was still in the news, and another in February 2019, after the event had faded from the headlines, and aggregated the data according to participants' awareness of the story, contrasting the attitudes of those who reported the greatest awareness with those who reported the least. Participants with the greatest awareness of the news story's details have more polarized attitudes about reuse, especially the reuse of content as data. They express a heightened desire for data mobility, greater concern about networked privacy rights, increased skepticism of algorithmically targeted advertising and news, and more willingness for social media platforms to demand corrections of inaccurate or deceptive content.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {facebook, privacy, linkedin, social media attitudes, ownership, data monetization, cambridge analytica, data use},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{10.1145/2602204.2602217,
author = {Kenneally, Erin and Bailey, Michael},
title = {Cyber-Security Research Ethics Dialogue &amp; Strategy Workshop},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2602204.2602217},
doi = {10.1145/2602204.2602217},
abstract = {The inaugural Cyber-security Research Ethics Dialogue &amp; Strategy Workshop was held on May 23, 2013, in conjunction with the IEEE Security Privacy Symposium in San Francisco, California. CREDS embraced the theme of "ethics-by-design" in the context of cyber security research, and aimed to: Educate participants about underlying ethics principles and applications;Discuss ethical frameworks and how they are applied across the various stakeholders and respective communities who are involved;Impart recommendations about how ethical frameworks can be used to inform policymakers in evaluating the ethical underpinning of critical policy decisions;Explore cyber security research ethics techniques,tools,standards and practices so researchers can apply ethical principles within their research methodologies; andDiscuss specific case vignettes and explore the ethical implications of common research acts and omissions.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {apr},
pages = {76–79},
numpages = {4},
keywords = {trust, law, network measurement, cyber security, ethics}
}

@article{10.1145/3523059,
author = {Zhang, Lin and Fan, Lixin and Luo, Yong and Duan, Ling-Yu},
title = {Intrinsic Performance Influence-Based Participant Contribution Estimation for Horizontal Federated Learning},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3523059},
doi = {10.1145/3523059},
abstract = {The rapid development of modern artificial intelligence technique is mainly attributed to sufficient and high-quality data. However, in the data collection, personal privacy is at risk of being leaked. This issue can be addressed by federated learning, which is proposed to achieve efficient model training among multiple data providers without direct data access and aggregation. To encourage more parties owning high-quality data to participate in the federated learning, it is important to evaluate and reward the participant contribution in a reasonable, robust, and efficient manner. To achieve this goal, we propose a novel contribution estimation method: Intrinsic Performance Influence-based Contribution Estimation (IPICE). In particular, the class-level intrinsic performance influence is adopted as the contribution estimation criteria in IPICE, and a neural network is employed to exploit the non-linear relationship between the performance change and estimated contribution. Extensive experiments are conducted on various datasets, and the results demonstrate that IPICE is more accurate and stable than the counterpart in various data distribution settings. The computational complexity is significantly reduced in our IPICE, especially when a new party joins the federation. IPICE assigns small contributions to bad/garbage data and thus prevent them from participating and deteriorating the learning ecosystem.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {sep},
articleno = {88},
numpages = {24},
keywords = {participant contribution estimation, neural network, Federated learning}
}

@inproceedings{10.1145/2600821.2600847,
author = {Moazeni, Ramin and Link, Daniel and Boehm, Barry},
title = {COCOMO II Parameters and IDPD: Bilateral Relevances},
year = {2014},
isbn = {9781450327541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600821.2600847},
doi = {10.1145/2600821.2600847},
abstract = {The phenomenon called Incremental Development Productivity Decline (IDPD) is presumed to be present in all incremental soft-ware projects to some extent. COCOMO II is a popular parametric cost estimation model that has not yet been adapted to account for the challenges that IDPD poses to cost estimation. Instead, its cost driver and scale factors stay constant throughout the increments of a project. While a simple response could be to make these parameters variable per increment, questions are raised as to whether the existing parameters are enough to predict the behavior of an incrementally developed project even in that case. Individual COCOMO II parameters are evaluated with regard to their development over the course of increments and how they influence IDPD. The reverse is also done. In light of data collected in recent experimental projects, additional new variable parameters that either extend COCOMO II or could stand on their own are proposed.},
booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
pages = {20–24},
numpages = {5},
keywords = {incremental development, scale factors, IDPD, Parametric cost estimation, cost drivers},
location = {Nanjing, China},
series = {ICSSP 2014}
}

@inproceedings{10.1145/3307772.3328285,
author = {Kriechbaumer, Thomas and Jorde, Daniel and Jacobsen, Hans-Arno},
title = {Waveform Signal Entropy and Compression Study of Whole-Building Energy Datasets},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3328285},
doi = {10.1145/3307772.3328285},
abstract = {Electrical energy consumption has been an ongoing research area since the coming of smart homes and Internet of Things. Consumption characteristics and usages profiles are directly influenced by building occupants and their interaction with electrical appliances. Data analysis together with machine learning models can be utilized to extract valuable information for the benefit of occupants themselves (conserve energy and increase comfort levels), power plants (maintenance), and grid operators (stability). Public energy datasets provide a scientific foundation to develop and benchmark these algorithms and techniques. With datasets exceeding tens of terabytes, we present a novel study of five whole-building energy datasets with high sampling rates, their signal entropy, and how a well-calibrated measurement can have a significant effect on the overall storage requirements. We show that some datasets do not fully utilize the available measurement precision, therefore leaving potential accuracy and space savings untapped. We benchmark a comprehensive list of 365 file formats, transparent data transformations, and lossless compression algorithms. The primary goal is to reduce the overall dataset size while maintaining an easy-to-use file format and access API. We show that with careful selection of file format and encoding scheme, we can reduce the size of some datasets by up to 73%.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {58–67},
numpages = {10},
keywords = {non-intrusive load monitoring, high sampling rate, Energy dataset, file format, electricity aggregate, waveform compression},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

@article{10.1145/3422158,
author = {Kumar, Devender and Jeuris, Steven and Bardram, Jakob E. and Dragoni, Nicola},
title = {Mobile and Wearable Sensing Frameworks for MHealth Studies and Applications: A Systematic Review},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2691-1957},
url = {https://doi.org/10.1145/3422158},
doi = {10.1145/3422158},
abstract = {With the widespread use of smartphones and wearable health sensors, a plethora of mobile health (mHealth) applications to track well-being, run human behavioral studies, and clinical trials have emerged in recent years. However, the design, development, and deployment of mHealth applications is challenging in many ways. To address these challenges, several generic mobile sensing frameworks have been researched in the past decade. Such frameworks assist developers and researchers in reducing the complexity, time, and cost required to build and deploy health-sensing applications. The main goal of this article is to provide the reader with an overview of the state-of-the-art of health-focused generic mobile and wearable sensing frameworks. This review gives a detailed analysis of functional and non-functional features of existing frameworks, the health studies they were used in, and the stakeholders they support. Additionally, we also analyze the historical evolution, uptake, and maintenance after the initial release. Based on this analysis, we suggest new features and opportunities for future generic mHealth sensing frameworks.},
journal = {ACM Trans. Comput. Healthcare},
month = {dec},
articleno = {8},
numpages = {28},
keywords = {mobile sensing frameworks, mHealth sensing, mobile sensing, mHealth frameworks, wearable sensing}
}

@article{10.1145/3450518,
author = {Wei, Ziheng and Link, Sebastian},
title = {Embedded Functional Dependencies and Data-Completeness Tailored Database Design},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3450518},
doi = {10.1145/3450518},
abstract = {We establish a principled schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing redundant data value occurrences that may cause problems with processing data that meets the requirements. We establish axiomatic, algorithmic, and logical foundations for reasoning about embedded functional dependencies. These foundations enable us to introduce generalizations of Boyce-Codd and Third normal forms that avoid processing difficulties of any application data, or minimize these difficulties across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements, and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate the effectiveness of our framework for the acquisition of the constraints, the schema design process, and the performance of the schema designs in terms of updates and join queries.},
journal = {ACM Trans. Database Syst.},
month = {may},
articleno = {7},
numpages = {46},
keywords = {Boyce-Codd normal form, key, database design, updates, redundancy, decomposition, synthesis, missing value, functional dependency, normal form, third normal form}
}

@inproceedings{10.1145/3428502.3428548,
author = {Osorio-Sanabria, Mariutsi Alexandra and Amaya-Fern\'{a}ndez, Ferney and Gonz\'{a}lez-Zabala, Mayda Patricia},
title = {Developing a Model to Readiness Assessment of Open Government Data in Public Institutions in Colombia},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428548},
doi = {10.1145/3428502.3428548},
abstract = {Open data is a movement that has gained worldwide political relevance as a strategy that supports active transparency, access to public information, and the generation of public, social, and economic value. To know the progress and results of open data initiatives, governments, working groups, international organizations, and researchers have proposed indexes and evaluation models. These measurements focus on the evaluation of aspects of the preparation, implementation, and impact of open data initiatives. In Colombia, the national government within the framework of its digital government policy defined the open data project. The progress in data openings is monitored through international indexes and the open government index, which focuses solely on the publication and use of open government data. This research deals with the evaluation of the preparation for the opening of data, in public entities that have not implemented an open data initiative. The study gives a general description of the evaluation of open data at the international and national level, identifies aspects to be considered to measure the preparation, and proposes a conceptual model of evaluation to measure the preparation in open data of a public sector entity. This proposal can be considered as a tool that generates information that supports the design and implementation of an effective open data initiative.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {334–340},
numpages = {7},
keywords = {e-government, open government data, digital government, Open data, readiness assessment},
location = {Athens, Greece},
series = {ICEGOV 2020}
}

@article{10.1145/3568113.3568119,
author = {Sun, Wen and Ma, Wenqiang and Zhou, Yu and Zhang, Yan},
title = {An Introduction to Digital Twin Standards},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {2375-0529},
url = {https://doi.org/10.1145/3568113.3568119},
doi = {10.1145/3568113.3568119},
abstract = {Recently, both academia and industry have shown increasing interest in unlocking the potential applications of digital twin. As an emerging technology, digital twin builds a virtual representation of physical objects and makes predictive strategies. As compared with conventional simulation and modeling technologies, digital twin can ensure the high fidelity of the virtual model through continuous updating and self-learning. The emerging standardization of digital twin facilitates the development of digital twin, and will eventually realize the interconnection of data, models and services between different enterprises or areas. This article overviews the recent progress of digital twin standards, the progress of digital twin network, and the challenges for successfully deploying digital twins.},
journal = {GetMobile: Mobile Comp. and Comm.},
month = {oct},
pages = {16–22},
numpages = {7}
}

@inproceedings{10.1145/3543434.3543442,
author = {Bartolomucci, Federico and Bresolin, Gianluca},
title = {Fostering Data Collaboratives’ Systematisation through Models’ Definition and Research Priorities Setting},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543434.3543442},
doi = {10.1145/3543434.3543442},
abstract = {Data collaboratives (DC) [12, 18] have gained increasing attention in recent years benefitting and nurturing the momentum around the use of Data for Good [11]. However, research on the topic, derived and built upon the fields of collaborative governance, information sharing, and open data [9, 17] is still unmature, lacking a systematic body of knowledge, grounded in empirical evidence [11]. Except few studies, specifically referred to DC [31, 36, 37, 40, 42] [15, 18, 19, 24], most of the literature used in the field is encompassing broader concepts as such DataSharing, DataforGood, or Cross Sectoral Partnership.Given that the empirical field has matured sufficiently to permit more quantitative analysis, the research seeks to go beyond existing qualitative classifications and inductively define data collaborative archetypes, emphasizing their distinctions and peculiarities as a foundation for future research on the topic.The research started from a literature review on DCs, their definition and the dimensions identifying different DC's models. The dataset provided on datacollaboratives.org has been filtered based on the literature review, excluding those instances that do not meet the DCs criteria or for whom online data collection is not feasible. Once the empirical setting was defined, a phase of variables selection and population has been conducted according to different variables. The evaluation of different clustering solutions, using both qualitative and quantitative methodologies, brought to identify five mutually exclusive clusters.Each cluster is described according to 18 variables, allowing the emergence of cluster's specific peculiarities and challenges. Findings are consistent with prior classifications and taxonomies [18, 29] with additional views afforded by a larger number of instances, the use of quantitative methodologies and the analysis of additional variables. Findings demonstrate the coexistence of quite different entities under the concept of DC, each of whose challenges and progress should be examined independently by researchers.Responding to the objective to foster DCs long term sustainability, different research priorities are specified according to identified clusters and an empirical setting for conducting this research is made available. From a practitioner perspective, research's findings may enable those interested in the topic to obtain more comprehensive information about benchmark examples, which is a valuable resource for industry growth. Additionally, the research illustrates&nbsp;the efficacy of categorical variable clustering analysis for inductive exploratory studies in a novel field of research.},
booktitle = {DG.O 2022: The 23rd Annual International Conference on Digital Government Research},
pages = {35–40},
numpages = {6},
keywords = {Data Collaboratives, Cluster Analysis, Data for Good},
location = {Virtual Event, Republic of Korea},
series = {dg.o 2022}
}

@article{10.1145/3502852,
author = {Yu, Hao and Hu, Xing and Li, Ge and Li, Ying and Wang, Qianxiang and Xie, Tao},
title = {Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones via Deep Learning},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3502852},
doi = {10.1145/3502852},
abstract = {In recent years, applying deep learning to detect semantic code clones has received substantial attention from the research community. Accordingly, various evaluation benchmark datasets, with the most popular one as BigCloneBench, are constructed and selected as benchmarks to assess and compare different deep learning models for detecting semantic clones. However, there is no study to investigate whether an evaluation benchmark dataset such as BigCloneBench is properly used to evaluate models for detecting semantic code clones. In this article, we present an experimental study to show that BigCloneBench typically includes semantic clone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs. Subsequently, we propose an undesirable-by-design Linear-Model that considers only which identifiers appear in a code fragment; this model can achieve high effectiveness for detecting semantic clones when evaluated on BigCloneBench, even comparable to state-of-the-art deep learning models recently proposed for detecting semantic clones. To alleviate these issues, we abstract a subset of the identifier names (including type, variable, and method names) in BigCloneBench to result in AbsBigCloneBench and use AbsBigCloneBench to better assess the effectiveness of deep learning models on the task of detecting semantic clones.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {62},
numpages = {25},
keywords = {Code clone detection, deep learning, dataset collection}
}

@article{10.1145/3093895,
author = {Longo, Antonella and Zappatore, Marco and Bochicchio, Mario and Navathe, Shamkant B.},
title = {Crowd-Sourced Data Collection for Urban Monitoring via Mobile Sensors},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3093895},
doi = {10.1145/3093895},
abstract = {A considerable amount of research has addressed Internet of Things and connected communities. It is possible to exploit the sensing capabilities of connected communities, by leveraging the continuously growing use of cloud computing solutions and mobile devices. The pervasiveness of mobile sensors also enables the Mobile Crowd Sensing (MCS) paradigm, which aims at using mobile-embedded sensors to extend monitoring of multiple (environmental) phenomena in expansive urban areas. In this article, we discuss our approach with a cloud-based platform to pave the way for applying crowd sensing in urban scenarios. We have implemented a complete solution for environmental monitoring of several pollutants, like noise, air, electromagnetic fields, and so on in an urban area based on this paradigm. Through extensive experimentation, specifically on noise pollution, we show how the proposed infrastructure exhibits the ability to collect data from connected communities, and enables a seamless support of services needed for improving citizens’ quality of life and eventually helps city decision makers in urban planning.},
journal = {ACM Trans. Internet Technol.},
month = {oct},
articleno = {5},
numpages = {21},
keywords = {smart cities, social sensing, Mobile crowed sensing}
}

@inproceedings{10.1145/3397166.3413465,
author = {Bazzi, Alessandro and Campolo, Claudia and Masini, Barbara M. and Molinaro, Antonella},
title = {How to Deal with Data Hungry V2X Applications?},
year = {2020},
isbn = {9781450380157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397166.3413465},
doi = {10.1145/3397166.3413465},
abstract = {Current vehicular communication technologies were designed for a so-called phase 1, where cars needed to advise of their presence. Several projects, research activities and field tests have proved their effectiveness to this scope. But entering the phase 2, where awareness needs to be improved with non-connected objects and vulnerable road users, and even more with phases 3 and 4, where also coordination is foreseen, the spectrum scarcity becomes a critical issue. In this work, we provide an overview of various 5G and beyond solutions currently under investigation that will be needed to tackle the challenge. We first recall the undergoing activities at the access layer aimed to satisfy capacity and bandwidth demands. We then discuss the role that emerging networking paradigms can play to improve vehicular data dissemination, while preventing congestion and better exploiting resources. Finally, we give a look into edge computing and machine learning techniques that will be determinant to efficiently process and mine the massive amounts of sensor data.},
booktitle = {Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {333–338},
numpages = {6},
keywords = {cooperative sensing, vehicle-to-everything, 5G, connected and automated vehicles},
location = {Virtual Event, USA},
series = {Mobihoc '20}
}

@article{10.1145/3426866,
author = {Meng, Linhao and Wei, Yating and Pan, Rusheng and Zhou, Shuyue and Zhang, Jianwei and Chen, Wei},
title = {VADAF: Visualization for Abnormal Client Detection and Analysis in Federated Learning},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3–4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3426866},
doi = {10.1145/3426866},
abstract = {Federated Learning (FL) provides a powerful solution to distributed machine learning on a large corpus of decentralized data. It ensures privacy and security by performing computation on devices (which we refer to as clients) based on local data to improve the shared global model. However, the inaccessibility of the data and the invisibility of the computation make it challenging to interpret and analyze the training process, especially to distinguish potential client anomalies. Identifying these anomalies can help experts diagnose and improve FL models. For this reason, we propose a visual analytics system, VADAF, to depict the training dynamics and facilitate analyzing potential client anomalies. Specifically, we design a visualization scheme that supports massive training dynamics in the FL environment. Moreover, we introduce an anomaly detection method to detect potential client anomalies, which are further analyzed based on both the client model’s visual and objective estimation. Three case studies have demonstrated the effectiveness of our system in understanding the FL training process and supporting abnormal client detection and analysis.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {sep},
articleno = {26},
numpages = {23},
keywords = {Federated learning, anomaly detection, visual analytics}
}

@inproceedings{10.1145/3374587.3374631,
author = {Luo, Jingtang and Yao, Shiying and Gou, Jijun and Shuai, Lisha and Cao, Yu},
title = {A Secure Transmission Scheme of Sensitive Power Information in Ubiquitous Power IoT},
year = {2020},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374631},
doi = {10.1145/3374587.3374631},
abstract = {As one of the national key infrastructures, the ubiquitous power Internet of Things (IoT) provides a convenient method for large-scale power information collection. The widespread transmission of massive power information using data mining techniques for large amounts of data can yield valuable information. Therefore, hacker attacks are endless, posing a threat to the security of the state, society, collectives and individuals. In this paper, we propose a secure transmission scheme of power information, named "SSD" (Split &amp; Signature &amp; Disturbing). In the scheme, after the data is split, it will be anonymized and selected for different paths to be transferred to the destination node. After recombination, the data will be restored. The SSD ensures the indistinguishability and security of the sensitive data by data splitting and disturbing method, and protects the anonymity of individual identities by group signature. The experimental results show that the individual prediction/actual data similarity approaches 0%, and the similarity ratio of the category data (three types in the experiment) is 37.32%, which can be judged to be basically non-correlated.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {252–257},
numpages = {6},
keywords = {Data Splitting, Information Security, Ubiquitous Power IoT},
location = {Normal, IL, USA},
series = {CSAI2019}
}

@inproceedings{10.1145/3183713.3197387,
author = {Dong, Xin Luna and Rekatsinas, Theodoros},
title = {Data Integration and Machine Learning: A Natural Synergy},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3197387},
doi = {10.1145/3183713.3197387},
abstract = {There is now more data to analyze than ever before. As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1645–1650},
numpages = {6},
keywords = {data integration, data enrichment, machine learning},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3083187.3083189,
author = {Pogorelov, Konstantin and Eskeland, Sigrun Losada and de Lange, Thomas and Griwodz, Carsten and Randel, Kristin Ranheim and Stensland, H\r{a}kon Kvale and Dang-Nguyen, Duc-Tien and Spampinato, Concetto and Johansen, Dag and Riegler, Michael and Halvorsen, P\r{a}l},
title = {A Holistic Multimedia System for Gastrointestinal Tract Disease Detection},
year = {2017},
isbn = {9781450350020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3083187.3083189},
doi = {10.1145/3083187.3083189},
abstract = {Analysis of medical videos for detection of abnormalities and diseases requires both high precision and recall, but also real-time processing for live feedback and scalability for massive screening of entire populations. Existing work on this field does not provide the necessary combination of retrieval accuracy and performance.; AB@In this paper, a multimedia system is presented where the aim is to tackle automatic analysis of videos from the human gastrointestinal (GI) tract. The system includes the whole pipeline from data collection, processing and analysis, to visualization. The system combines filters using machine learning, image recognition and extraction of global and local image features. Furthermore, it is built in a modular way so that it can easily be extended. At the same time, it is developed for efficient processing in order to provide real-time feedback to the doctors. Our experimental evaluation proves that our system has detection and localisation accuracy at least as good as existing systems for polyp detection, it is capable of detecting a wider range of diseases, it can analyze video in real-time, and it has a low resource consumption for scalability.},
booktitle = {Proceedings of the 8th ACM on Multimedia Systems Conference},
pages = {112–123},
numpages = {12},
keywords = {Medical Multimedia System, Interactive, Gastrointestinal Tract, Performance, Evaluation, Medicine},
location = {Taipei, Taiwan},
series = {MMSys'17}
}

@inproceedings{10.1145/2961111.2962605,
author = {Sun, Yan and Wang, Qing and Li, Mingshu},
title = {Understanding the Contribution of Non-Source Documents in Improving Missing Link Recovery: An Empirical Study},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962605},
doi = {10.1145/2961111.2962605},
abstract = {Background: Links between issue reports and their fixing commits play an important role in software maintenance. Such link data are often missing in practice and many approaches have been proposed in order to recover them automatically. Most of existing approaches focus on comparing log messages and source code files in commits with issues reports. Besides the two kinds of data in commits, non-source documents (NSDs) such as change logs usually record the fixing activities and sometimes share similar texts as those in issue reports. However, few discussions have been made on the role of NSDs in designing link recovery approaches.Aims: This paper aims at understanding whether and how NSDs affect the performance of link recovery approaches.Method: An empirical study is conducted to evaluate the role of NSDs in link recovery approaches in 18 open source projects with 6370 issues and 22761 commits.Results: With the inclusion of NSDs, link recovery approaches can get an average increase in F-Measure ranging from 2.76% - 25.63%. Further examinations show NSDs contribute to the performance improvement in 15 projects and have exceptions in 3 projects. The performance improvement in the 15 projects is mainly from the filtering of noisy links. On average, 23.59% - 76.30% false links can be excluded by exploiting NSDs in the link recovery approach. We also analyze the 3 projects in which NSDs cannot improve the performance. Our finding shows sophisticated data selection in NSDs is necessary.Conclusions: Our preliminary findings demonstrate that involving NSDs can improve the performance of link recovery approaches in most cases.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {39},
numpages = {10},
keywords = {Mining Software Repositories, Missing Link Recovery, Software Maintenance, Non-Source Documents},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/3459955.3460617,
author = {Gosh, Saptarshi and EL Boudani, Brahim and Dagiuklas, Tasos and Iqbal, Muddesar},
title = {SO-KDN: A Self-Organised Knowledge Defined Networks Architecture for Reliable Routing},
year = {2021},
isbn = {9781450389136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459955.3460617},
doi = {10.1145/3459955.3460617},
abstract = {“When you are destined for an important appoint-ment, you would obviously opt for the most reliable route instead of the shortest in order to be well prepared”. Modern networking is presently undergoing through a quantum leap. To cope up with ambitious demands and user expectations, it is becoming more complex both structurally and functionally. Software Defined Networking (SDN) happens to be an instance of such advancements. It has significantly leveraged the network programmability, abstraction, and automation. Eventually, with acceptance form all major network infrastructure such as 5G and Cloud, SDN is becoming the standard of future networking. Likewise, Machine Learning (ML) has become the trendiest skill-in-demand recently. With its superiority of analyzing data, makes it applicable for almost every possible domain. The attempt to applying the power of ML in networking has not been too long, it allows the network to be more intelligent and capable enough to take optimal decisions to address some of its native problems. This gives rise to Self- Organized Networking (SON). In this article, Routing using Deep Neural Network (DNN) on top of SDN is addressed. We proposed a Self-organized Knowledge Defined Network (SO-KDN) architecture and an intelligent routing algorithm, that reactively finds the most reliable route, i.e., a route having least probability of fluctuation. This reduces network overhead due to re-routing and optimizes traffic congestion. Experimental data show a mean 90% accurate forecast in reliability prediction.},
booktitle = {2021 The 4th International Conference on Information Science and Systems},
pages = {160–166},
numpages = {7},
keywords = {Deep Learning, SDN, Routing, SON},
location = {Edinburgh, United Kingdom},
series = {ICISS 2021}
}

@article{10.1145/2738210.2738211,
author = {Kenneally, Erin},
title = {How to Throw the Race to the Bottom: Revisiting Signals for Ethical and Legal Research Using Online Data},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0095-2737},
url = {https://doi.org/10.1145/2738210.2738211},
doi = {10.1145/2738210.2738211},
abstract = {With research using data available online, researcher conduct is not fully prescribed or proscribed by formal ethical codes of conduct or law because of ill-fitting "expectations signals" -- indicators of legal and ethical risk. This article describes where these ordering forces breakdown in the context of online research and suggests how to identify and respond to these grey areas by applying common legal and ethical tenets that run across evolving models. It is intended to advance the collective dialogue work-in-progress toward a path that revisits and harmonizes more appropriate ethical and legal signals for research using online data between and among researchers, oversight entities, policymakers and society.},
journal = {SIGCAS Comput. Soc.},
month = {feb},
pages = {4–10},
numpages = {7},
keywords = {security research ethics, law}
}

@inproceedings{10.1145/3098954.3105822,
author = {Stupka, V\'{a}clav and Hor\'{a}k, Martin and Hus\'{a}k, Martin},
title = {Protection of Personal Data in Security Alert Sharing Platforms},
year = {2017},
isbn = {9781450352574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098954.3105822},
doi = {10.1145/3098954.3105822},
abstract = {In order to ensure confidentiality, integrity and availability (so called CIA triad) of data within network infrastructure, it is necessary to be able to detect and handle cyber security incidents. For this purpose, it is vital for Computer Security Incident Response Teams (CSIRT) to have enough data on relevant security events and threats. That is why CSIRTs share security alerts and incidents data using various sharing platforms. Even though they do so primarily to protect data and privacy of users, their use also lead to additional processing of personal data, which may cause new privacy risks. European data protection law, especially with the adoption of the new General data protection regulation, sets out very strict rules on processing of personal data which on one hand leads to greater protection of individual's rights, but on the other creates great obstacles for those who need to share any personal data. This paper analyses the General Data Protection Regulation (GDPR), relevant case-law and analyses by the Article 29 Working Party to propose optimal methods and level of personal data processing necessary for effective use of security alert sharing platforms, which would be legally compliant and lead to appropriate balance between risks.},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
articleno = {65},
numpages = {8},
keywords = {Alert sharing platform, Privacy, Personal data, Intrusion detection, Information sharing, Cyber security},
location = {Reggio Calabria, Italy},
series = {ARES '17}
}

@article{10.1145/3494582,
author = {Shen, Cong and Qian, Zhaozhi and Huyuk, Alihan and Van Der Schaar, Mihaela},
title = {MARS: Assisting Human with Information Processing Tasks Using Machine Learning},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2691-1957},
url = {https://doi.org/10.1145/3494582},
doi = {10.1145/3494582},
abstract = {This article studies the problem of automated information processing from large volumes of unstructured, heterogeneous, and sometimes untrustworthy data sources. The main contribution is a novel framework called Machine Assisted Record Selection (MARS). Instead of today’s standard practice of relying on human experts to manually decide the order of records for processing, MARS learns the optimal record selection via an online learning algorithm. It further integrates algorithm-based record selection and processing with human-based error resolution to achieve a balanced task allocation between machine and human. Both fixed and adaptive MARS algorithms are proposed, leveraging different statistical knowledge about the existence, quality, and cost associated with the records. Experiments using semi-synthetic data that are generated from real-world patients record processing in the UK national cancer registry are carried out, which demonstrate significant (3 to 4 fold) performance gain over the fixed-order processing. MARS represents one of the few examples demonstrating that machine learning can assist humans with complex jobs by automating complex triaging tasks.},
journal = {ACM Trans. Comput. Healthcare},
month = {mar},
articleno = {21},
numpages = {19},
keywords = {Data entry, human-in-the-loop decision support system, online learning}
}

@inproceedings{10.1145/3274895.3274899,
author = {Oliver, Dev and Hoel, Erik G.},
title = {A Trace Framework for Analyzing Utility Networks: A Summary of Results (Industrial Paper)},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274899},
doi = {10.1145/3274895.3274899},
abstract = {Given a utility network and one or more starting points that define where analysis should begin, the problem of analyzing utility networks entails assembling a subset of network elements that meet some specified criteria. Analyzing utility network data has several applications and provides tremendous business value to utilities. For example, analysis may answer questions about the current state of the network (e.g., what valves need to be closed to shut off water flow to a location of a pipe leak), help to design future facilities (e.g., how many houses are fed by a transformer and can the transformer supply another house without overloading its capacity?), and help to organize business practices (e.g., create circuit maps for work crews to facilitate damage assessment after an ice storm). Analyzing utility networks is a challenging problem due to 1) the size of the data, which could have many tens of millions of network elements per utility, and billions of elements at the nationwide or continental scale, 2) modeling and analyzing utility assets at high fidelity (level of detail), and 3) the different analysis requirements across utility domains (e.g., water, wastewater, sewer, district heating, gas, electric, fiber, and telecom). This paper describes the trace framework for utility network analysis that has been implemented in ArcGIS Pro 2.1/ArcGIS Enterprise 10.6. The trace framework features algorithms in a services-based architecture for addressing analysis tasks across a wide array of utility domains. Previous approaches have focused on solving specific problems in specific domains whereas the trace framework provides a more general, scalable solution. We present experiments that demonstrate the scalability of the trace framework and a case study that highlights its value in performing a wide variety of analytics on utility networks.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {249–258},
numpages = {10},
keywords = {utility networks, graphs and networks, GIS, spatial databases, graph algorithms},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@article{10.1145/3524104,
author = {Qu, Youyang and Uddin, Md Palash and Gan, Chenquan and Xiang, Yong and Gao, Longxiang and Yearwood, John},
title = {Blockchain-Enabled Federated Learning: A Survey},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3524104},
doi = {10.1145/3524104},
abstract = {Federated learning (FL) is experiencing fast booming in recent years, which is jointly promoted by the prosperity of machine learning and Artificial Intelligence along with the emerging privacy issues. In the FL paradigm, a central server and local end devices maintain the same model by exchanging model updates instead of raw data, with which the privacy of data stored on end devices is not directly revealed. In this way, the privacy violation caused by the growing collection of sensitive data can be mitigated. However, the performance of FL with a central server is reaching a bottleneck while new threats are emerging simultaneously. There are various reasons, among which the most significant ones are centralized processing, data falsification, and lack of incentives. To accelerate the proliferation of FL, blockchain-enabled FL has attracted substantial attention from both academia and industry. A considerable number of novel solutions are devised to meet the emerging demands of diverse scenarios. Blockchain-enabled FL provides both theories and techniques to improve the performances of FL from various perspectives. In this survey, we will comprehensively summarize and evaluate existing variants of blockchain-enabled FL, identify the emerging challenges, and propose potentially promising research directions in this under-explored domain.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {mar},
keywords = {Countermeasures., Attacks, Blockchain, Federated Learning}
}

@inproceedings{10.1145/3498851.3498929,
author = {Dautaras, Justas and Matskin, Mihhail},
title = {Mobile Crowdsensing with Imagery Tasks},
year = {2022},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3498929},
doi = {10.1145/3498851.3498929},
abstract = {The amount of gadgets connected to the internet has grown rapidly in the recent years. These human owned devices can potentially be used to gather sensor data without active involvement of their owners. One of the types of platforms that contribute to the utilisation of these devices are mobile crowdsensing systems. These systems can be used for different tasks including different types of community support. While these systems are quite widely used, yet little research has been done for integration of imagery data into them which require also human involvement. This paper considers a mobile crowdsensing system where gathering data from sensors is supported by crowdsourcing human intelligence for providing both textual and visual information. We also explore the best settings for such a system. Imagery processing is integrated into an already existing mobile crowdsensing platform CrowdS. The solution was evaluated both by a limited number of real life users and by conducting simulations. The simulations represent complex scenarios with multi-level variables. The results of simulation allow suggest an efficient configuration for the parameters and characteristics of the environment used in imagery integration.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {54–61},
numpages = {8},
keywords = {mobile sensing devices, crowdsourcing, mobile crowdsensing},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@inproceedings{10.1145/2750858.2807526,
author = {Hovsepian, Karen and al'Absi, Mustafa and Ertin, Emre and Kamarck, Thomas and Nakajima, Motohiro and Kumar, Santosh},
title = {CStress: Towards a Gold Standard for Continuous Stress Assessment in the Mobile Environment},
year = {2015},
isbn = {9781450335744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2750858.2807526},
doi = {10.1145/2750858.2807526},
abstract = {Recent advances in mobile health have produced several new models for inferring stress from wearable sensors. But, the lack of a gold standard is a major hurdle in making clinical use of continuous stress measurements derived from wearable sensors. In this paper, we present a stress model (called cStress) that has been carefully developed with attention to every step of computational modeling including data collection, screening, cleaning, filtering, feature computation, normalization, and model training. More importantly, cStress was trained using data collected from a rigorous lab study with 21 participants and validated on two independently collected data sets --- in a lab study on 26 participants and in a week-long field study with 20 participants. In testing, the model obtains a recall of 89% and a false positive rate of 5% on lab data. On field data, the model is able to predict each instantaneous self-report with an accuracy of 72%.},
booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {493–504},
numpages = {12},
keywords = {mobile health (mHealth), wearable sensors, stress, modeling},
location = {Osaka, Japan},
series = {UbiComp '15}
}

@article{10.1109/TCBB.2019.2953908,
author = {Wang, Bing and Mei, Changqing and Wang, Yuanyuan and Zhou, Yuming and Cheng, Mu-Tian and Zheng, Chun-Hou and Wang, Lei and Zhang, Jun and Chen, Peng and Xiong, Yan},
title = {Imbalance Data Processing Strategy for Protein Interaction Sites Prediction},
year = {2019},
issue_date = {May-June 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2953908},
doi = {10.1109/TCBB.2019.2953908},
abstract = {Protein-protein interactions play essential roles in various biological progresses. Identifying protein interaction sites can facilitate researchers to understand life activities and therefore will be helpful for drug design. However, the number of experimental determined protein interaction sites is far less than that of protein sites in protein-protein interaction or protein complexes. Therefore, the negative and positive samples are usually imbalanced, which is common but bring result bias on the prediction of protein interaction sites by computational approaches. In this work, we presented three imbalance data processing strategies to reconstruct the original dataset, and then extracted protein features from the evolutionary conservation of amino acids to build a predictor for identification of protein interaction sites. On a dataset with 10,430 surface residues but only 2,299 interface residues, the imbalance dataset processing strategies can obviously reduce the prediction bias, and therefore improve the prediction performance of protein interaction sites. The experimental results show that our prediction models can achieve a better prediction performance, such as a prediction accuracy of 0.758, or a high F-measure of 0.737, which demonstrated the effectiveness of our method.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {nov},
pages = {985–994},
numpages = {10}
}

@inproceedings{10.1145/2390045.2390062,
author = {B\"{a}r, Arian and Golab, Lukasz},
title = {Towards Benchmarking Stream Data Warehouses},
year = {2012},
isbn = {9781450317214},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390045.2390062},
doi = {10.1145/2390045.2390062},
abstract = {Data management systems are facing two challenges driven by the requirements of emerging data-intensive applications: more data and less time to process the data. Data volumes continue to increase as new sources and data collecting mechanisms appear. At the same time, these sources tend to be highly dynamic and generate data in the form of a stream, which requires quick reaction to newly arrived data. Traditional data warehouses enable scalable data storage and analytics, including the ability to define nested levels of materialized views. However, views are typically refreshed during downtimes---e.g., every night---which does not meet the latency requirements of many applications. Stream data warehousing is a new data management technology that allows nearly-continuous view refresh as new data arrive, which enables seamless integration of real-time monitoring and business intelligence with long-term data mining. In this paper, we argue that a new benchmark is required for stream warehouses, which should focus on measuring the property that determines the utility of these systems, namely how well they can keep up with the incoming data and guarantee the "freshness" of materialized views.},
booktitle = {Proceedings of the Fifteenth International Workshop on Data Warehousing and OLAP},
pages = {105–112},
numpages = {8},
keywords = {materialized view maintenance, data warehouse benchmarking, stream data warehousing},
location = {Maui, Hawaii, USA},
series = {DOLAP '12}
}

@article{10.1145/2935634.2935641,
author = {Bajpai, Vaibhav and Berger, Arthur W. and Eardley, Philip and Ott, J\"{o}rg and Sch\"{o}nw\"{a}lder, J\"{u}rgen},
title = {Global Measurements: Practice and Experience (Report on Dagstuhl Seminar #16012)},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/2935634.2935641},
doi = {10.1145/2935634.2935641},
abstract = {This article summarises a 2.5 day long Dagstuhl seminar on Global Measurements: Practice and Experience held in January 2016. This seminar was a followup of the seminar on Global Measurement Frameworks held in 2013, which focused on the development of global Internet measurement platforms and associated metrics. The second seminar aimed at discussing the practical experience gained with building these global Internet measurement platforms. It brought together people who are actively involved in the design and maintenance of global Internet measurement platforms and who do research on the data delivered by such platforms. Researchers in this seminar have used data derived from global Internet measurement platforms in order to manage networks or services or as input for regulatory decisions. The entire set of presentations delivered during the seminar is made publicly available at [1].},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {may},
pages = {32–39},
numpages = {8},
keywords = {quality of experience, internet measurements, traffic engineering, network management}
}

@inbook{10.1145/3310205.3310210,
title = {Data Transformation},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310210},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inbook{10.1145/3447404.3447411,
author = {McMenemy, David},
title = {Ethics and Statistics},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447411},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {101–103},
numpages = {3}
}

@inproceedings{10.1145/3490099.3511115,
author = {Dodge, Jonathan and Anderson, Andrew A. and Olson, Matthew and Dikkala, Rupika and Burnett, Margaret},
title = {How Do People Rank Multiple Mutant Agents?},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511115},
doi = {10.1145/3490099.3511115},
abstract = {Faced with several AI-powered sequential decision-making systems, how might someone choose on which to rely? For example, imagine car buyer Blair shopping for a self-driving car, or developer Dillon trying to choose an appropriate ML model to use in their application. Their first choice might be infeasible (i.e., too expensive in money or execution time), so they may need to select their second or third choice. To address this question, this paper presents: 1) Explanation Resolution, a quantifiable direct measurement concept; 2) a new XAI empirical task to measure explanations: “the Ranking Task”; and 3) a new strategy for inducing controllable agent variations—Mutant Agent Generation. In support of those main contributions, it also presents 4) novel explanations for sequential decision-making agents; 5) an adaptation to the AAR/AI assessment process; and 6) a qualitative study around these devices with 10 participants to investigate how they performed the Ranking Task on our mutant agents, using our explanations, and structured by AAR/AI. From an XAI researcher perspective, just as mutation testing can be applied to any code, mutant agent generation can be applied to essentially any neural network for which one wants to evaluate an assessment process or explanation type. As to an XAI user’s perspective, the participants ranked the agents well overall, but showed the importance of high explanation resolution for close differences between agents. The participants also revealed the importance of supporting a wide diversity of explanation diets and agent “test selection” strategies.},
booktitle = {27th International Conference on Intelligent User Interfaces},
pages = {191–211},
numpages = {21},
keywords = {After-Action Review, Explainable AI},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.14778/3342263.3342626,
author = {Wei, Ziheng and Link, Sebastian},
title = {Embedded Functional Dependencies and Data-Completeness Tailored Database Design},
year = {2019},
issue_date = {July 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3342263.3342626},
doi = {10.14778/3342263.3342626},
abstract = {We establish a robust schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing many redundant data value occurrences. We establish axiomatic and algorithmic foundations for reasoning about embedded functional dependencies. These foundations allow us to establish generalizations of Boyce-Codd and Third normal forms that do not permit any redundancy in any future application data, or minimize their redundancy across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate our framework, and the effectiveness and efficiency of our algorithms, but also provide quantified insight into database schema design trade-offs.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1458–1470},
numpages = {13}
}

@inproceedings{10.1145/3461702.3462605,
author = {Kelley, Patrick Gage and Yang, Yongwei and Heldreth, Courtney and Moessner, Christopher and Sedley, Aaron and Kramm, Andreas and Newman, David T. and Woodruff, Allison},
title = {Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial Intelligence in 8 Countries},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462605},
doi = {10.1145/3461702.3462605},
abstract = {As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {627–637},
numpages = {11},
keywords = {artificial intelligence, public perception},
location = {Virtual Event, USA},
series = {AIES '21}
}

@article{10.1145/3371906,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Combemale, Benoit and Bastin, Lucy and Bencomo, Nelly and Bruel, Jean-Michel and Becker, Christoph and Betz, Stefanie and Chitchyan, Ruzanna and Cheng, Betty H. C. and Klingert, Sonja and Paige, Richard F. and Penzenstadler, Birgit and Seyff, Norbert and Syriani, Eugene and Venters, Colin C.},
title = {Toward Model-Driven Sustainability Evaluation},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/3371906},
doi = {10.1145/3371906},
abstract = {Exploring the vision of a model-based framework that may enable broader engagement with and informed decision making about sustainability issues.},
journal = {Commun. ACM},
month = {feb},
pages = {80–91},
numpages = {12}
}

@article{10.1109/TCBB.2015.2453944,
author = {Masseroli, Marco and Canakoglu, Arif and Ceri, Stefano},
title = {Integration and Querying of Genomic and Proteomic Semantic Annotations for Biomedical Knowledge Extraction},
year = {2016},
issue_date = {March 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2015.2453944},
doi = {10.1109/TCBB.2015.2453944},
abstract = {Understanding complex biological phenomena involves answering complex biomedical questions on multiple biomolecular information simultaneously, which are expressed through multiple genomic and proteomic semantic annotations scattered in many distributed and heterogeneous data sources; such heterogeneity and dispersion hamper the biologists’ ability of asking global queries and performing global evaluations. To overcome this problem, we developed a software architecture to create and maintain a Genomic and Proteomic Knowledge Base (GPKB), which integrates several of the most relevant sources of such dispersed information (including Entrez Gene, UniProt, IntAct, Expasy Enzyme, GO, GOA, BioCyc, KEGG, Reactome, and OMIM). Our solution is general, as it uses a flexible, modular, and multilevel global data schema based on abstraction and generalization of integrated data features, and a set of automatic procedures for easing data integration and maintenance, also when the integrated data sources evolve in data content, structure, and number. These procedures also assure consistency, quality, and provenance tracking of all integrated data, and perform the semantic closure of the hierarchical relationships of the integrated biomedical ontologies. At http://www.bioinformatics.deib.polimi.it/GPKB/, a Web interface allows graphical easy composition of queries, although complex, on the knowledge base, supporting also semantic query expansion and comprehensive explorative search of the integrated data to better sustain biomedical knowledge extraction.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {mar},
pages = {209–219},
numpages = {11}
}

@inproceedings{10.5555/3242181.3242205,
author = {Brailsford, Sally C and Carter, Michael W and Jacobson, Sheldon H},
title = {Five Decades of Healthcare Simulation},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {In this paper we have not attempted to produce any kind of systematic review of simulation in healthcare to compete with the dozen (at least) excellent and comprehensive survey papers on this topic that already exist. We begin with a glance back at the early days of Wintersim, but then proceed, in line with the theme of this special track, to reflect on general developments in healthcare simulation over the years from our own personal perspectives. We include some memories and reflections by several pioneers in this area, both academics and healthcare practitioners, on both sides of the Atlantic. We also asked four current simulation modelers, who all specialize in healthcare applications but from very diverse perspectives, to reflect on their experiences. We endeavor to identify some common or recurring themes across the years, and end with a glimpse into the future.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {23},
numpages = {20},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.1145/3323679.3326513,
author = {Xue, Hongfei and Jiang, Wenjun and Miao, Chenglin and Yuan, Ye and Ma, Fenglong and Ma, Xin and Wang, Yijiang and Yao, Shuochao and Xu, Wenyao and Zhang, Aidong and Su, Lu},
title = {DeepFusion: A Deep Learning Framework for the Fusion of Heterogeneous Sensory Data},
year = {2019},
isbn = {9781450367646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323679.3326513},
doi = {10.1145/3323679.3326513},
abstract = {In recent years, significant research efforts have been spent towards building intelligent and user-friendly IoT systems to enable a new generation of applications capable of performing complex sensing and recognition tasks. In many of such applications, there are usually multiple different sensors monitoring the same object. Each of these sensors can be regarded as an information source and provides us a unique "view" of the observed object. Intuitively, if we can combine the complementary information carried by multiple sensors, we will be able to improve the sensing performance. Towards this end, we propose DeepFusion, a unified multi-sensor deep learning framework, to learn informative representations of heterogeneous sensory data. DeepFusion can combine different sensors' information weighted by the quality of their data and incorporate cross-sensor correlations, and thus can benefit a wide spectrum of IoT applications. To evaluate the proposed DeepFusion model, we set up two real-world human activity recognition testbeds using commercialized wearable and wireless sensing devices. Experiment results show that DeepFusion can outperform the state-of-the-art human activity recognition methods.},
booktitle = {Proceedings of the Twentieth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {151–160},
numpages = {10},
keywords = {Deep Learning, Internet of Things, Sensor Fusion},
location = {Catania, Italy},
series = {Mobihoc '19}
}

@inproceedings{10.1145/3535782.3535831,
author = {Thuensuwan, Komkrish and Chutima, Parames},
title = {Expert System Development to Predict Canned Motor Pump Status},
year = {2022},
isbn = {9781450395816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535782.3535831},
doi = {10.1145/3535782.3535831},
abstract = {This research presents the development of an Expert System to predict Canned Motor Pump (CMP) Status by applying a machine learning (ML) algorithm with domain expert knowledge in the case study plant. A Case study plant is a petrochemical plant that uses CMP to transfer process medium within inside plant battery limit (ISBL). At present, The CMP maintenance strategy is improving from condition-based maintenance to predictive maintenance. To archive desired level of predictive maintenance need CMP domain expert knowledge to find potential failure signs. This expert system is contributing to reducing expertise human load by substitution with the system. The research contains identifying system framework, experiment steps, including dataset preparation and model testing. The experiment result shows Random Forest (RF) algorithm is suitable for this system due to model performance evaluation comparing four algorithms with confusion matrix and similar data resampling and hyperparameter tuning method. Further on, this contribution is a role model, and enrolling in other equipment in the case study plant is a benefit of this work. Recommendation and key success factors found during this research are also mentioned in the conclusion for further work as a continuous improvement process cycle.},
booktitle = {Proceedings of the 4th International Conference on Management Science and Industrial Engineering},
pages = {373–382},
numpages = {10},
location = {Chiang Mai, Thailand},
series = {MSIE '22}
}

@inproceedings{10.1145/3388440.3412475,
author = {Chen, Junjie and Mowlaei, Mohammad Erfan and Shi, Xinghua},
title = {Population-Scale Genomic Data Augmentation Based on Conditional Generative Adversarial Networks},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412475},
doi = {10.1145/3388440.3412475},
abstract = {Although next generation sequencing technologies have made it possible to quickly generate a large collection of sequences, current genomic data still suffer from small data sizes, imbalances, and biases due to various factors including disease rareness, test affordability, and concerns about privacy and security. In order to address these limitations of genomic data, we develop a Population-scale Genomic Data Augmentation based on Conditional Generative Adversarial Networks (PG-cGAN) to enhance the amount and diversity of genomic data by transforming samples already in the data rather than collecting new samples. Both the generator and discriminator in the PG-CGAN are stacked with convolutional layers to capture the underlying population structure. Our results for augmenting genotypes in human leukocyte antigen (HLA) regions showed that PC-cGAN can generate new genotypes with similar population structure, variant frequency distributions and LD patterns. Since the input for PC-cGAN is the original genomic data without assumptions about prior knowledge, it can be extended to enrich many other types of biomedical data and beyond.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {26},
numpages = {6},
keywords = {genomics, machine learning, data augmentation, deep learning, generative adversarial networks},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3366623.3368140,
author = {Skluzacek, Tyler J. and Chard, Ryan and Wong, Ryan and Li, Zhuozhao and Babuji, Yadu N. and Ward, Logan and Blaiszik, Ben and Chard, Kyle and Foster, Ian},
title = {Serverless Workflows for Indexing Large Scientific Data},
year = {2019},
isbn = {9781450370387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366623.3368140},
doi = {10.1145/3366623.3368140},
abstract = {The use and reuse of scientific data is ultimately dependent on the ability to understand what those data represent, how they were captured, and how they can be used. In many ways, data are only as useful as the metadata available to describe them. Unfortunately, due to growing data volumes, large and distributed collaborations, and a desire to store data for long periods of time, scientific "data lakes" quickly become disorganized and lack the metadata necessary to be useful to researchers. New automated approaches are needed to derive metadata from scientific files and to use these metadata for organization and discovery. Here we describe one such system, Xtract, a service capable of processing vast collections of scientific files and automatically extracting metadata from diverse file types. Xtract relies on function as a service models to enable scalable metadata extraction by orchestrating the execution of many, short-running extractor functions. To reduce data transfer costs, Xtract can be configured to deploy extractors centrally or near to the data (i.e., at the edge). We present a prototype implementation of Xtract and demonstrate that it can derive metadata from a 7 TB scientific data repository.},
booktitle = {Proceedings of the 5th International Workshop on Serverless Computing},
pages = {43–48},
numpages = {6},
keywords = {serverless, data lakes, file systems, materials science, metadata extraction},
location = {Davis, CA, USA},
series = {WOSC '19}
}

@inproceedings{10.5555/3242181.3242194,
author = {Cheng, Russell},
title = {History of Input Modeling},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {In stochastic simulation, input modeling refers to the process of identifying and selecting the probability distributions, called input models, from which are generated the random variates that are the source of the stochastic variation in the simulation model when it is run. This article reviews the history of the development and use of such models with the main focus on discrete-event simulation (DES).},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {12},
numpages = {21},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@inproceedings{10.1145/3025453.3025777,
author = {Du, Fan and Plaisant, Catherine and Spring, Neil and Shneiderman, Ben},
title = {Finding Similar People to Guide Life Choices: Challenge, Design, and Evaluation},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025777},
doi = {10.1145/3025453.3025777},
abstract = {People often seek examples of similar individuals to guide their own life choices. For example, students making academic plans refer to friends; patients refer to acquaintances with similar conditions, physicians mention past cases seen in their practice. How would they want to search for similar people in databases? We discuss the challenge of finding similar people to guide life choices and report on a need analysis based on 13 interviews. Our PeerFinder prototype enables users to find records that are similar to a seed record, using both record attributes and temporal events found in the records. A user study with 18 participants and four experts shows that users are more engaged and more confident about the value of the results to provide useful evidence to guide life choices when provided with more control over the search process and more context for the results, even at the cost of added complexity.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {5498–5544},
numpages = {47},
keywords = {decision making, temporal visualization, visual analytics, similarity, temporal event analytics},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@article{10.1145/3449252,
author = {Van Kleunen, Lucy and Muller, Brian and Voida, Stephen},
title = {"Wiring a City": A Sociotechnical Perspective on Deploying Urban Sensor Networks},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449252},
doi = {10.1145/3449252},
abstract = {We use a sociotechnical perspective to expand upon prior characterizations of deploying end-to-end urban sensor networks that focus primarily on the technical aspects of such systems. Via exploratory, semi-structured interviews with those deploying a number of urban sensor networks in a single American city, we identify ways that human decision-making and collaborative processes influence how these infrastructures are built. We synthesize these findings into a framework in which sociotechnical factors show up across the phases of data collection, management, analysis, and impacts within smart city projects. Each phase can display variability in immediacy, automation, geographic scope, and ownership. Finally, we use our situated work to discuss a generalizable tension within smart city projects between cross-domain data integration and fragmentation and provide implications for CSCW research, the design of smart city data platforms, and municipal policy.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {178},
numpages = {22},
keywords = {civic data, sociotechnical system, urban sensor networks, smart cities}
}

@inproceedings{10.1145/3147234.3148104,
author = {Jansen, Christoph and Beier, Maximilian and Witt, Michael and Frey, Sonja and Krefting, Dagmar},
title = {Towards Reproducible Research in a Biomedical Collaboration Platform Following the FAIR Guiding Principles},
year = {2017},
isbn = {9781450351959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3147234.3148104},
doi = {10.1145/3147234.3148104},
abstract = {Replication of computational experiments is essential for verifiable research. However, it requires a comprehensive and unambiguous description of all employed digital artifacts, in particular data, code and the computational environment. Recently, the FAIR Guiding Principles have been published to support reproducible research. In this paper, a cloud-based biomedical collaboration platform has been evaluated regarding FAIR principles and has been extended to support reproducibility. The FAICE suite is presented, encompassing tools to thoroughly describe and reproduce a computational experiment within the original execution environment as well as within a dynamically configured VM.},
booktitle = {Companion Proceedings of The10th International Conference on Utility and Cloud Computing},
pages = {3–8},
numpages = {6},
keywords = {reproducibility, docker, repeatability, cloud computing, xnat, medical data},
location = {Austin, Texas, USA},
series = {UCC '17 Companion}
}

@article{10.1145/3232863,
author = {Wang, Weina and Ying, Lei and Zhang, Junshan},
title = {The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms, and Fundamental Limits},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2167-8375},
url = {https://doi.org/10.1145/3232863},
doi = {10.1145/3232863},
abstract = {We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. One primary goal of the data collector is to learn some desired information from the elicited data. Specifically, this information is modeled by an underlying state, and the private data of each individual represents his of her knowledge about the state. Departing from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Further, an individual takes full control of his or her own data privacy and reports only a privacy-preserving version of his or her data.In this article, the value of ϵ units of privacy is measured by the minimum payment among all nonnegative payment mechanisms, under which an individual’s best response at a Nash equilibrium is to report his or her data in an ϵ-locally differentially private manner. The higher ϵ is, the less private the reported data is. We derive lower and upper bounds on the value of privacy that are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use a lower payment to buy ϵ units of privacy, and the upper bound is given by an achievable payment mechanism that we design. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given accuracy target for learning the underlying state and show that the total payment of the designed mechanism is at most one individual’s payment away from the minimum.},
journal = {ACM Trans. Econ. Comput.},
month = {aug},
articleno = {8},
numpages = {26},
keywords = {differential privacy, randomized response, Data collection}
}

@inproceedings{10.1145/3209281.3209309,
author = {Alarabiat, Ayman and Soares, Delfina and Ferreira, Luis and de S\'{a}-Soares, Filipe},
title = {Analyzing E-Governance Assessment Initiatives: An Exploratory Study},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209309},
doi = {10.1145/3209281.3209309},
abstract = {This paper presents an exploratory study aimed at identifying, exploring, and analyzing current EGOV assessment initiatives. We do so based on data obtained from a desktop research and from a worldwide questionnaire directed to the 193 countries that are part of the list used by the Statistics Division of the United Nations Department of Economic and Social Affairs (UNDESA). The study analyses 12 EGOV assessment initiatives: a) seven of them are international/regional EGOV assessment initiatives performed by the United Nations (UN), European Union (EU), Waseda-IAC, Organisation for Economic Co-operation and Development (OECD), World Bank (WB), WWW Foundation, and Open Knowledge Network (OKN); b) five of them are country-level EGOV assessment initiatives performed by Norway, Germany, India, Saudi Arabia, and the United Arab Emirates. Further, the study provides general results obtained from a questionnaire with participation from 18 countries: Afghanistan, Angola, Brazil, Cabo Verde, Denmark, Estonia, Finland, Germany, Ghana, Ireland, Latvia, the Netherlands, Norway, Oman, Pakistan, the Philippines, Portugal, and Slovenia. The findings show that there is no shortage of interest in assessing EGOV initiatives. However, the supply side of EGOV initiatives is the dominant perspective being assessed, particularly by regional and international organizations. While there is an increasing interest in assessing the users' perspective (demand side) by individual countries, such attempts still seem to be at an early stage. Additionally, the actual use and impact of various EGOV services and activities are rarely well identified and measured. This study represents a stepping stone for developing instruments for assessing EGOV initiatives in future works. For the current stage, the study presents several general suggestions to be considered during the assessment process.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {30},
numpages = {10},
keywords = {evaluation, assessment, e-governance, e-government},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/3448016.3457552,
author = {Fu, Yupeng and Soman, Chinmay},
title = {Real-Time Data Infrastructure at Uber},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457552},
doi = {10.1145/3448016.3457552},
abstract = {Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2503–2516},
numpages = {14},
keywords = {streaming processing, real-time infrastructure},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3410566.3410606,
author = {Seong, Younho and Nuamah, Joseph and Yi, Sun},
title = {Guidelines for Cybersecurity Visualization Design},
year = {2020},
isbn = {9781450375030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410566.3410606},
doi = {10.1145/3410566.3410606},
abstract = {Cyber security visualization designers can benefit from human factors engineering concepts and principles to resolve key human factors challenges in visual interface design. We survey human factors concepts and principles that have been applied in the past decade of human factors research. We highlight these concepts and relate them to cybersecurity visualization design. We provide guidelines to help cybersecurity visualization designers address some human factors challenges in the context of interface design. We use ecological interface design approach to present human factors-based principles of interface design for visualization. Cyber security visualization designers will benefit from human factors engineering concepts and principles to resolve key human factors challenges in visual interface design.},
booktitle = {Proceedings of the 24th Symposium on International Database Engineering &amp; Applications},
articleno = {25},
numpages = {6},
keywords = {ecological interface design, cognition, cybersecurity, visualization, affordance},
location = {Seoul, Republic of Korea},
series = {IDEAS '20}
}

@article{10.1145/2993231.2993234,
author = {Vilela, Jessyka and Goncalves, Enyo and Holanda, Ana Carla and Castro, Jaelson and Figueiredo, Bruno},
title = {A Retrospective Analysis of SAC Requirements: Engineering Track},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/2993231.2993234},
doi = {10.1145/2993231.2993234},
abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {aug},
pages = {26–41},
numpages = {16},
keywords = {symposium on applied computing, systematic mapping study, retrospective, trends, scoping study, requirements engineering, SAC, relevance}
}

@article{10.1145/3185048,
author = {Zhang, Han and Hill, Shawndra and Rothschild, David},
title = {Addressing Selection Bias in Event Studies with General-Purpose Social Media Panels},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3185048},
doi = {10.1145/3185048},
abstract = {Data from Twitter have been employed in prior research to study the impacts of events. Conventionally, researchers use keyword-based samples of tweets to create a panel of Twitter users who mention event-related keywords during and after an event. However, the keyword-based sampling is limited in its objectivity dimension of data and information quality. First, the technique suffers from selection bias since users who discuss an event are already more likely to discuss event-related topics beforehand. Second, there are no viable control groups for comparison to a keyword-based sample of Twitter users. We propose an alternative sampling approach to construct panels of users defined by their geolocation. Geolocated panels are exogenous to the keywords in users’ tweets, resulting in less selection bias than the keyword panel method. Geolocated panels allow us to follow within-person changes over time and enable the creation of comparison groups. We compare different panels in two real-world settings: response to mass shootings and TV advertising. We first show the strength of the selection biases of keyword panels. Then, we empirically illustrate how geolocated panels reduce selection biases and allow meaningful comparison groups regarding the impact of the studied events. We are the first to provide a clear, empirical example of how a better panel selection design, based on an exogenous variable such as geography, both reduces selection bias compared to the current state of the art and increases the value of Twitter research for studying events. While we advocate for the use of a geolocated panel, we also discuss its weaknesses and application scenario seriously. This article also calls attention to the importance of selection bias in impacting the objectivity of social media data.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {4},
numpages = {24},
keywords = {social media, panels, survey, geolocation, selection bias, Twitter, non-response bias, coverage bias}
}

@inproceedings{10.1145/3333165.3333185,
author = {Soufan, Ayah},
title = {Deep Learning for Sentiment Analysis of Arabic Text},
year = {2019},
isbn = {9781450360890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333165.3333185},
doi = {10.1145/3333165.3333185},
abstract = {Deep learning has been very successful in the past decades, especially in Computer Vision and Speech Recognition fields. It has been also used successfully in the Natural Language Processing field because of the availability of an enormous amount of online text data, such as social networks and reviews websites, which have gained a lot of popularity and success in the past years. Sentiment Analysis is one of the hottest applications of Natural Language Processing (NLP). Many researchers have done excellent work on Sentiment Analysis for English language. However, the amount of work on Sentiment Analysis for Arabic language is, in comparison, very limited due to the complexity of the Arabic language's morphology and orthography. Unlike the English language, Arabic has many different dialects which makes Sentiment Analysis for Arabic more difficult and challenging, especially when working on data collected from social networks, which is known to be unstructured and noisy. Most of the work that has been done on Sentiment Analysis of Arabic language, focused on using lexicons and basic machine learning models. In addition, most of the work has been done on small datasets because of the limited number of the available annotated datasets for Arabic language. This paper proposes state-of-the-art research for Sentiment Analysis of Arabic microblogging using new techniques, and a sophisticated Arabic text data preprocessing.},
booktitle = {Proceedings of the ArabWIC 6th Annual International Conference Research Track},
articleno = {20},
numpages = {8},
location = {Rabat, Morocco},
series = {ArabWIC 2019}
}

@article{10.1145/3323334,
author = {Usman, Muhammad and Jan, Mian Ahmad and He, Xiangjian and Chen, Jinjun},
title = {A Survey on Big Multimedia Data Processing and Management in Smart Cities},
year = {2019},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3323334},
doi = {10.1145/3323334},
abstract = {Integration of embedded multimedia devices with powerful computing platforms, e.g., machine learning platforms, helps to build smart cities and transforms the concept of Internet of Things into Internet of Multimedia Things (IoMT). To provide different services to the residents of smart cities, the IoMT technology generates big multimedia data. The management of big multimedia data is a challenging task for IoMT technology. Without proper management, it is hard to maintain consistency, reusability, and reconcilability of generated big multimedia data in smart cities. Various machine learning techniques can be used for automatic classification of raw multimedia data and to allow machines to learn features and perform specific tasks. In this survey, we focus on various machine learning platforms that can be used to process and manage big multimedia data generated by different applications in smart cities. We also highlight various limitations and research challenges that need to be considered when processing big multimedia data in real-time.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {54},
numpages = {29},
keywords = {smart cities, management, IoMT, multimedia, machine learning}
}

@inproceedings{10.1145/3152178.3152186,
author = {Shivaprabhu, Vivek R. and Balasubramani, Booma Sowkarthiga and Cruz, Isabel F.},
title = {Ontology-Based Instance Matching for Geospatial Urban Data Integration},
year = {2017},
isbn = {9781450354950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152178.3152186},
doi = {10.1145/3152178.3152186},
abstract = {To run a smart city, data is collected from disparate sources such as IoT devices, social media, private and public organizations, and government agencies. In the US, the City of Chicago has been a pioneer in the collection of data and in the development of a framework, called OpenGrid, to curate and analyze the collected data. OpenGrid is a geospatial situational awareness platform that allows policy makers, service providers, and the general public to explore city data and to perform advanced data analytics to enable planning of services, prediction of events and patterns, and identification of incidents across the city. This paper presents the instance matching module of GIVA, a Geospatial data Integration, Visualization, and Analytics platform, as applied to the integration of information related to businesses, which is spread across several datasets. In particular, we describe the integration of two datasets, Business Licenses and Food Inspections, so as to enable predictive analytics to determine which food establishments the city should inspect first. The paper describes semantic web-based instance matching mechanisms to compare the Business Names and Address fields.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
articleno = {8},
numpages = {8},
keywords = {Geospatial Data, Data Integration, Ontology, Record Linkage, Instance Matching},
location = {Redondo Beach, CA, USA},
series = {UrbanGIS'17}
}

@inproceedings{10.1145/2608029.2608030,
author = {Gannon, Dennis and Fay, Dan and Green, Daron and Takeda, Kenji and Yi, Wenming},
title = {Science in the Cloud: Lessons from Three Years of Research Projects on Microsoft Azure},
year = {2014},
isbn = {9781450329118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2608029.2608030},
doi = {10.1145/2608029.2608030},
abstract = {Microsoft Research is now in its fourth year of awarding Windows Azure cloud resources to the academic community. As of April 2014, over 200 research projects have started. In this paper we review the results of this effort to date. We also characterize the computational paradigms that work well in public cloud environments and those that are usually disappointing. We also discuss many of the barriers to successfully using commercial cloud platforms in research and ways these problems can be overcome.},
booktitle = {Proceedings of the 5th ACM Workshop on Scientific Cloud Computing},
pages = {1–8},
numpages = {8},
keywords = {infrastructure as a service, platform as a service, cloud computing, scalable systems, cloud programming models, map reduce},
location = {Vancouver, BC, Canada},
series = {ScienceCloud '14}
}

@inproceedings{10.1145/3209415.3209481,
author = {Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil},
title = {Exploiting Data Analytics for Social Services: On Searching for Profiles of Unlawful Use of Social Benefits},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209481},
doi = {10.1145/3209415.3209481},
abstract = {In this paper we present a data-driven profiling approach that we have adopted and implemented for a municipality. Our aim was to make profiles transparent and meaningful for citizens, policymakers and authorities so that they can validate, scrutinize and challenge the profiles. Our approach relies on a Genetic Algorithm (GA) that searches for useful and human understandable group profiles. Furthermore, we discuss some of the challenges encountered, show a selection of the profiles that were found by the GA, and discuss the necessity and a number of ways of validating these profiles in accordance with, e.g., privacy and non-discrimination laws and guidelines before using them in practice.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {550–559},
numpages = {10},
keywords = {Data analytics, profiling, social benefits, Genetic Algorithm},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@inproceedings{10.1145/2889160.2889231,
author = {Barik, Titus and DeLine, Robert and Drucker, Steven and Fisher, Danyel},
title = {The Bones of the System: A Case Study of Logging and Telemetry at Microsoft},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889231},
doi = {10.1145/2889160.2889231},
abstract = {Large software organizations are transitioning to event data platforms as they culturally shift to better support data-driven decision making. This paper offers a case study at Microsoft during such a transition. Through qualitative interviews of 28 participants, and a quantitative survey of 1,823 respondents, we catalog a diverse set of activities that leverage event data sources, identify challenges in conducting these activities, and describe tensions that emerge in data-driven cultures as event data flow through these activities within the organization. We find that the use of event data span every job role in our interviews and survey, that different perspectives on event data create tensions between roles or teams, and that professionals report social and technical challenges across activities.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {92–101},
numpages = {10},
keywords = {boundary object, logging, practices, collaboration, developer tools, telemetry},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3047273.3047299,
author = {Attard, Judie and Orlandi, Fabrizio and Auer, S\"{o}ren},
title = {Exploiting the Value of Data through Data Value Networks},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047299},
doi = {10.1145/3047273.3047299},
abstract = {Open data is increasingly permeating into all dimensions of our society and has become an indispensable commodity that serves as a basis for many products and services. Governments are generating a huge amount of data spanning different dimensions. This dataification shows the paramount need to identify the means and methods in which the value of data and knowledge can be exploited. While not restricted to the government domain, this dataification is certainly relevant in a government context, particularly due to the large volume of data generated by public institutions. In this paper we identify the various activities and roles within a data value chain, and hence proceed to provide our own definition of a Data Value Network. We specifically cater for non-tangible data products and characterise three dimensions that play a vital role within the Data Value Network. We also propose a Demand and Supply Distribution Model with the aim of providing insight on how an entity can participate in the global data market by producing a data product, as well as a concrete implementation through the Demand and Supply as a Service. Through our contributions we therefore project our vision of enhancing the process of open (government) data exploitation and innovation, with the aim of achieving the highest possible impact.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {475–484},
numpages = {10},
keywords = {impacts, innovation, open data, data demand, value creation, data supply, data value chain, data value network, exploitation},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@book{10.1145/3226595,
editor = {Brodie, Michael L.},
title = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
year = {2018},
isbn = {9781947487192},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {22},
abstract = {At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series, a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's highest honor, the "Nobel Prize" for computing. This series aims to highlight the accomplishments of awardees, explaining their major contributions of lasting importance in computing."Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker," the first book in the series, celebrates Mike's contributions and impact. What accomplishments warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher, professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long leader, and research evangelist for the database community. This book describes Mike's many contributions and evaluates them in light of the Turing Award.The book describes, in 36 chapters, the unique nature, significance, and impact of Mike's achievements in advancing modern database systems over more than 40 years. The stories involve technical concepts, projects, people, prototype systems, failures, lucky accidents, crazy risks, startups, products, venture capital, and lots of applications that drove Mike Stonebraker's achievements and career. Even if you have no interest in databases at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements from the perspectives of 39 remarkable computer scientists and professionals.Today, data is considered the world's most valuable resource ("The Economist," May 6, 2017), whether it is in the tens of millions of databases used to manage the world's businesses and governments, in the billions of databases in our smartphones and watches, or residing elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems. Every one of the millions or billions of databases includes features that are celebrated by the 2014 A.M. Turing Award and are described in this book.}
}

@article{10.1145/3448119,
author = {Chen, Wenqiang and Chen, Lin and Ma, Meiyi and Parizi, Farshid Salemi and Patel, Shwetak and Stankovic, John},
title = {ViFin: Harness Passive Vibration to Continuous Micro Finger Writing with a Commodity Smartwatch},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
url = {https://doi.org/10.1145/3448119},
doi = {10.1145/3448119},
abstract = {Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90% and 91% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {45},
numpages = {25},
keywords = {vibration intelligence, text input, wearable devices, micro finger writing}
}

@article{10.1145/3447541,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh},
title = {TabReformer: Unsupervised Representation Learning for Erroneous Data Detection},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2691-1922},
url = {https://doi.org/10.1145/3447541},
doi = {10.1145/3447541},
abstract = {Error detection is a crucial preliminary phase in any data analytics pipeline. Existing error detection techniques typically target specific types of errors. Moreover, most of these detection models either require user-defined rules or ample hand-labeled training examples. Therefore, in this article, we present TabReformer, a model that learns bidirectional encoder representations for tabular data. The proposed model consists of two main phases. In the first phase, TabReformer follows encoder architecture with multiple self-attention layers to model the dependencies between cells and capture tuple-level representations. Also, the model utilizes a Gaussian Error Linear Unit activation function with the Masked Data Model objective to achieve deeper probabilistic understanding. In the second phase, the model parameters are fine-tuned for the task of erroneous data detection. The model applies a data augmentation module to generate more erroneous examples to represent the minority class. The experimental evaluation considers a wide range of databases with different types of errors and distributions. The empirical results show that our solution can enhance the recall values by 32.95% on average compared with state-of-the-art techniques while reducing the manual effort by up to 48.86%.},
journal = {ACM/IMS Trans. Data Sci.},
month = {may},
articleno = {18},
numpages = {29},
keywords = {transformers, data augmentation, bidirectional encoder, Error detection}
}

@article{10.1145/2794400,
author = {Guo, Bin and Wang, Zhu and Yu, Zhiwen and Wang, Yu and Yen, Neil Y. and Huang, Runhe and Zhou, Xingshe},
title = {Mobile Crowd Sensing and Computing: The Review of an Emerging Human-Powered Sensing Paradigm},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2794400},
doi = {10.1145/2794400},
abstract = {With the surging of smartphone sensing, wireless networking, and mobile social networking techniques, Mobile Crowd Sensing and Computing (MCSC) has become a promising paradigm for cross-space and large-scale sensing. MCSC extends the vision of participatory sensing by leveraging both participatory sensory data from mobile devices (offline) and user-contributed data from mobile social networking services (online). Further, it explores the complementary roles and presents the fusion/collaboration of machine and human intelligence in the crowd sensing and computing processes. This article characterizes the unique features and novel application areas of MCSC and proposes a reference framework for building human-in-the-loop MCSC systems. We further clarify the complementary nature of human and machine intelligence and envision the potential of deep-fused human--machine systems. We conclude by discussing the limitations, open issues, and research opportunities of MCSC.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {7},
numpages = {31},
keywords = {crowd intelligence, human-machine systems, urban/community dynamics, Mobile phone sensing, cross-space sensing and mining}
}

@inproceedings{10.1145/3491102.3501998,
author = {Cambo, Scott Allen and Gergle, Darren},
title = {Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501998},
doi = {10.1145/3491102.3501998},
abstract = {Data science and machine learning provide indispensable techniques for understanding phenomena at scale, but the discretionary choices made when doing this work are often not recognized. Drawing from qualitative research practices, we describe how the concepts of positionality and reflexivity can be adapted to provide a framework for understanding, discussing, and disclosing the discretionary choices and subjectivity inherent to data science work. We first introduce the concepts of model positionality and computational reflexivity that can help data scientists to reflect on and communicate the social and cultural context of a model’s development and use, the data annotators and their annotations, and the data scientists themselves. We then describe the unique challenges of adapting these concepts for data science work and offer annotator fingerprinting and position mining as promising solutions. Finally, we demonstrate these techniques in a case study of the development of classifiers for toxic commenting in online communities.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {572},
numpages = {19},
keywords = {position mining, human-centered machine learning, model positionality, human-centered data science, annotator fingerprinting, data science, critical data studies, Computational reflexivity},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@article{10.1145/3342515,
author = {Chen, Yueyue and Guo, Deke and Bhuiyan, MD Zakirul Alam and Xu, Ming and Wang, Guojun and Lv, Pin},
title = {Towards Profit Optimization During Online Participant Selection in Compressive Mobile Crowdsensing},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1550-4859},
url = {https://doi.org/10.1145/3342515},
doi = {10.1145/3342515},
abstract = {A mobile crowdsensing (MCS) platform motivates employing participants from the crowd to complete sensing tasks. A crucial problem is to maximize the profit of the platform, i.e., the charge of a sensing task minus the payments to participants that execute the task. In this article, we improve the profit via the data reconstruction method, which brings new challenges, because it is hard to predict the reconstruction quality due to the dynamic features and mobility of participants. In particular, two Profit-driven Online Participant Selection (POPS) problems under different situations are studied in our work: (1) for S-POPS, the sensing cost of the different parts within the target area is the Same. Two mechanisms are designed to tackle this problem, including the ProSC and ProSC+. An exponential-based quality estimation method and a repetitive cross-validation algorithm are combined in the former mechanism, and the spatial distribution of selected participants are further discussed in the latter mechanism; (2) for V-POPS, the sensing cost of different parts within the target area is Various, which makes it the NP-hard problem. A heuristic mechanism called ProSCx is proposed to solve this problem, where the searching space is narrowed and both the participant quantity and distribution are optimized in each slot. Finally, we conduct comprehensive evaluations based on the real-world datasets. The experimental results demonstrate that our proposed mechanisms are more effective and efficient than baselines, selecting the participants with a larger profit for the platform.},
journal = {ACM Trans. Sen. Netw.},
month = {aug},
articleno = {38},
numpages = {29},
keywords = {Compressive mobile crowdsensing, online participant selection, data reconstruction}
}

@inproceedings{10.1145/2536780.2536783,
author = {Hafen, Ryan and Gibson, Tara D. and van Dam, Kerstin Kleese and Critchlow, Terence},
title = {Large-Scale Exploratory Analysis, Cleaning, and Modeling for Event Detection in Real-World Power Systems Data},
year = {2013},
isbn = {9781450325103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536780.2536783},
doi = {10.1145/2536780.2536783},
abstract = {In this paper, we present an approach to large-scale data analysis, Divide and Recombine (D&amp;R), and describe a hardware and software implementation that supports this approach. We then illustrate the use of D&amp;R on large-scale power systems sensor data to perform initial exploration, discover multiple data integrity issues, build and validate algorithms to filter bad data, and construct statistical event detection algorithms. This paper also reports on experiences using a non-traditional Hadoop distributed computing setup on top of a HPC computing cluster.},
booktitle = {Proceedings of the 3rd International Workshop on High Performance Computing, Networking and Analytics for the Power Grid},
articleno = {4},
numpages = {9},
keywords = {power systems, Hadoop, R, divide and recombine, exploratory data analysis, phasor measurement unit},
location = {Denver, Colorado},
series = {HiPCNA-PG '13}
}

@inproceedings{10.1145/3170358.3170367,
author = {Tsai, Yi-Shan and Moreno-Marcos, Pedro Manuel and Tammets, Kairit and Kollom, Kaire and Ga\v{s}evi\'{c}, Dragan},
title = {SHEILA Policy Framework: Informing Institutional Strategies and Policy Processes of Learning Analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170367},
doi = {10.1145/3170358.3170367},
abstract = {This paper introduces a learning analytics policy development framework developed by a cross-European research project team - SHEILA (Supporting Higher Education to Integrate Learning Analytics), based on interviews with 78 senior managers from 51 European higher education institutions across 16 countries. The framework was developed using the RAPID Outcome Mapping Approach (ROMA), which is designed to develop effective strategies and evidence-based policy in complex environments. This paper presents three case studies to illustrate the development process of the SHEILA policy framework, which can be used to inform strategic planning and policy processes in real world environments, particularly for large-scale implementation in higher education contexts.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {320–329},
numpages = {10},
keywords = {strategy, higher education, policy, learning analytics, ROMA model},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3424771.3424822,
author = {Zimmermann, Olaf and L\"{u}bke, Daniel and Zdun, Uwe and Pautasso, Cesare and Stocker, Mirko},
title = {Interface Responsibility Patterns: Processing Resources and Operation Responsibilities},
year = {2020},
isbn = {9781450377690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424771.3424822},
doi = {10.1145/3424771.3424822},
abstract = {Remote Application Programming Interfaces (APIs), as for instance offered in microservices architectures, are used in almost any distributed system today and are thus enablers for many digitalization efforts. It is hard to design such APIs so that they are easy and effective to use; maintaining their runtime qualities while preserving backward compatibility is equally challenging. Finding well suited granularities in terms of the architectural capabilities of endpoints and the read-write semantics of their operations are particularly important design concerns. Existing pattern languages have dealt with local APIs in object-oriented programming, with remote objects, with queue-based messaging and with service-oriented computing platforms. However, patterns or equivalent guidances for the architectural design of API endpoints, operations and their request and response message structures are still missing. In this paper, we extend our microservice API pattern language (MAP) and introduce endpoint role and operation responsibility patterns, namely Processing Resource, Computation Function, State Creation Operation, Retrieval Operation, and State Transition Operation. Known uses and examples of the patterns are drawn from public Web APIs, as well as application development and system integration projects the authors have been involved in.},
booktitle = {Proceedings of the European Conference on Pattern Languages of Programs 2020},
articleno = {9},
numpages = {24},
location = {Virtual Event, Germany},
series = {EuroPLoP '20}
}

@article{10.1145/3177852,
author = {Antunes, Rodolfo S. and Seewald, Lucas A. and Rodrigues, Vinicius F. and Costa, Cristiano A. Da and Jr., Luiz Gonzaga and Righi, Rodrigo R. and Maier, Andreas and Eskofier, Bj\"{o}rn and Ollenschl\"{a}ger, Malte and Naderi, Farzad and Fahrig, Rebecca and Bauer, Sebastian and Klein, Sigrun and Campanatti, Gelson},
title = {A Survey of Sensors in Healthcare Workflow Monitoring},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3177852},
doi = {10.1145/3177852},
abstract = {Activities of a clinical staff in healthcare environments must regularly be adapted to new treatment methods, medications, and technologies. This constant evolution requires the monitoring of the workflow, or the sequence of actions from actors involved in a procedure, to ensure quality of medical services. In this context, recent advances in sensing technologies, including Real-time Location Systems and Computer Vision, enable high-precision tracking of actors and equipment. The current state-of-the-art about healthcare workflow monitoring typically focuses on a single technology and does not discuss its integration with others. Such an integration can lead to better solutions to evaluate medical workflows. This study aims to fill the gap regarding the analysis of monitoring technologies with a systematic literature review about sensors for capturing the workflow of healthcare environments. Its main scientific contribution is to identify both current technologies used to track activities in a clinical environment and gaps on their combination to achieve better results. It also proposes a taxonomy to classify work regarding sensing technologies and methods. The literature review does not present proposals that combine data obtained from Real-time Location Systems and Computer Vision sensors. Further analysis shows that a multimodal analysis is more flexible and could yield better results.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {42},
numpages = {37},
keywords = {Real-time Location Systems, healthcare, workflow monitoring, Computer Vision}
}

@inproceedings{10.1145/3234698.3234743,
author = {Nagaraja, Arun and Kumar, T. Satish},
title = {An Extensive Survey on Intrusion Detection- Past, Present, Future},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234743},
doi = {10.1145/3234698.3234743},
abstract = {Intrusion Detection is the most eminent fields in the network which can also be called as anomaly detection. Various methods used by early research tells that, the kind of measures used to detect the intrusion is not specified. Research has grown extensively in Anomaly intrusion detection by using different data mining techniques. Most researchers have not briefed on the kinds of distances measures used, the classification and feature selection techniques used in identifying intrusion detection. Intrusion detection is classified with problems as Outlier problems, Sparseness problem and Data Distribution. One of the important observations made is, High Dimensional Data Reduction is not performed, and conventional dataset is not used or maintained by any researchers. A survey is performed to identify the type of distance measures used and the type of datasets used in the early research. In this extended survey, the measures like Distance measure, pattern behaviors are used in identifying the Network Intrusion Detection. In this paper, we present the various methods used by authors to obtain feature selection methods. Also, the discussion is towards, Computation of High Dimensional Data, how to decide the Choice of Learning algorithm, Efficient Distance and similarity measures to identify the intrusion detection from different datasets.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {45},
numpages = {9},
keywords = {Anomaly Detection, Intrusion Detection, Measures, Feature Selection, Datasets, Classification, Clustering},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

@inproceedings{10.1145/3144789.3144797,
author = {Zhang, Zhiqiang and Huang, Xiangbing and Iqbal, Muhammad Faisal Buland and Ye, Songtao},
title = {Better Weather Forecasting through Truth Discovery Analysis},
year = {2017},
isbn = {9781450352871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144789.3144797},
doi = {10.1145/3144789.3144797},
abstract = {In many real world applications, the same object or event may be described by multiple sources. As a result, conflicts among these sources are inevitable and these conflicts cause confusion as we have more than one value or outcome for each object. One significant problem is to resolve the confusion and to identify a piece of information which is trustworthy. This process of finding the truth from conflicting values of an object provided by multiple sources is called truth discovery or fact-finding. The main purpose of the truth discovery is to find more and more trustworthy information and reliable sources. Because the major assumption of truth discovery is on this intuitive principle, the source that provides trustworthy information is considered more reliable, and moreover, if the piece of information is from a reliable source, then it is more trustworthy. However, previously proposed truth discovery methods either do not conduct source reliability estimation at all (Voting Method), or even if they do, they do not model multiple properties of the object separately. This is the motivation for researchers to develop new techniques to tackle the problem of truth discovery in data with multiple properties. We present a method using an optimization framework which minimizes the overall weighted deviation between the truths and the multi-source observations. In this framework, different types of distance functions can be plugged in to capture the characteristics of different data types. We use weather datasets collected by four different platforms for extensive experiments and the results verify both the efficiency and precision of our methods for truth discovery.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent Information Processing},
articleno = {4},
numpages = {7},
keywords = {truth discovery, weather, heterogeneous data},
location = {Bangkok, Thailand},
series = {IIP'17}
}

@inproceedings{10.1145/3485190.3485193,
author = {Li, Yonghan and Lv, Hongjiang},
title = {The Dilemma of Digital Transformation of China's Hotel Industry and the Construction of Technology Platform: A Survey of Hotels Industry in China},
year = {2021},
isbn = {9781450384278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485190.3485193},
doi = {10.1145/3485190.3485193},
abstract = {The digital economy has been a hot spot in social development in recent years, and all walks of life are facing the opportunities and challenges of digital transformation. Successful digital transformation can enable traditional industries to gain dynamic capabilities in a changing environment, thereby gaining a leading competitive advantage. The previous literature paid more attention to the digital transformation of traditional industries, but lacked enough attention to the hotel industry. Through the case analysis of several major hotel groups in China, this article has gained profound insights in the digital transformation, enriched the influence of digital technology on the organization and management changes of the hotel industry, and has enlightening significance for guiding the hotel industry's practice.},
booktitle = {2021 4th International Conference on Information Management and Management Science},
pages = {13–18},
numpages = {6},
keywords = {management dilemma, digital transformation, Chinese hotel groups, technology platform},
location = {Chengdu, China},
series = {IMMS 2021}
}

@inproceedings{10.1145/3387940.3391466,
author = {Suni-Lopez, Franci and Condori-Fernandez, Nelly and Catala, Alejandro},
title = {Understanding Implicit User Feedback from Multisensorial and Physiological Data: A Case Study},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391466},
doi = {10.1145/3387940.3391466},
abstract = {Ensuring the quality of user experience is very important for increasing the acceptance likelihood of software applications, which can be affected by several contextual factors that continuously change over time (e.g., emotional state of end-user). Due to these changes in the context, software continually needs to adapt for delivering software services that can satisfy user needs. However, to achieve this adaptation, it is important to gather and understand the user feedback. In this paper, we mainly investigate whether physiological data can be considered and used as a form of implicit user feedback. To this end, we conducted a case study involving a tourist traveling abroad, who used a wearable device for monitoring his physiological data, and a smartphone with a mobile app for reminding him to take his medication on time during four days. Through the case study, we were able to identify some factors and activities as emotional triggers, which were used for understanding the user context. Our results highlight the importance of having a context analyzer, which can help the system to determine whether the detected stress could be considered as actionable and consequently as implicit user feedback.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {563–569},
numpages = {7},
keywords = {Physiological data, Actionable emotion, Implicit user feedback, Case study, Context information},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/2961111.2962628,
author = {Baltes, Sebastian and Diehl, Stephan},
title = {Worse Than Spam: Issues In Sampling Software Developers},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962628},
doi = {10.1145/2961111.2962628},
abstract = {Background: Reaching out to professional software developers is a crucial part of empirical software engineering research. One important method to investigate the state of practice is survey research. As drawing a random sample of professional software developers for a survey is rarely possible, researchers rely on various sampling strategies. Objective: In this paper, we report on our experience with different sampling strategies we employed, highlight ethical issues, and motivate the need to maintain a collection of key demographics about software developers to ease the assessment of the external validity of studies. Method: Our report is based on data from two studies we conducted in the past. Results: Contacting developers over public media proved to be the most effective and efficient sampling strategy. However, we not only describe the perspective of researchers who are interested in reaching goals like a large number of participants or a high response rate, but we also shed light onto ethical implications of different sampling strategies. We present one specific ethical guideline and point to debates in other research communities to start a discussion in the software engineering research community about which sampling strategies should be considered ethical.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {52},
numpages = {6},
keywords = {Software Developers, Sampling, Ethics, Empirical Research},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@article{10.14778/2850583.2850587,
author = {Altwaijry, Hotham and Mehrotra, Sharad and Kalashnikov, Dmitri V.},
title = {QuERy: A Framework for Integrating Entity Resolution with Query Processing},
year = {2015},
issue_date = {November 2015},
publisher = {VLDB Endowment},
volume = {9},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/2850583.2850587},
doi = {10.14778/2850583.2850587},
abstract = {This paper explores an analysis-aware data cleaning architecture for a large class of SPJ SQL queries. In particular, we propose QuERy, a novel framework for integrating entity resolution (ER) with query processing. The aim of QuERy is to correctly and efficiently answer complex queries issued on top of dirty data. The comprehensive empirical evaluation of the proposed solution demonstrates its significant advantage in terms of efficiency over the traditional techniques for the given problem settings.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {120–131},
numpages = {12}
}

@inproceedings{10.1145/3501247.3539017,
author = {Stan, George Vlad and Baart, Andr\'{e} and Dittoh, Francis and Akkermans, Hans and Bon, Anna},
title = {A Lightweight Downscaled Approach to Automatic Speech Recognition for Small Indigenous Languages},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3539017},
doi = {10.1145/3501247.3539017},
abstract = {Development of fully featured Automatic Speech Recognition (ASR) systems for a complete language vocabulary generally requires large data repositories, massive computing power, and a stable digital network infrastructure. These conditions are not met in the case of many indigenous languages. Based on our research for over a decade in West Africa, we present a lightweight and downscaled approach to AI-based ASR and describe a set of associated experiments. The aim is to produce a variety of limited-vocabulary ASRs as a basis for the development of practically useful (mobile and radio) voice-based information services that fit needs, preferences and knowledge of local rural communities.},
booktitle = {14th ACM Web Science Conference 2022},
pages = {451–458},
numpages = {8},
keywords = {low resource environments, automatic speech recognition, voice-based technologies, under-resourced/indigenous languages, neural networks, machine learning},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@inproceedings{10.1145/2660114.2660119,
author = {Sulser, Fabio and Giangreco, Ivan and Schuldt, Heiko},
title = {Crowd-Based Semantic Event Detection and Video Annotation for Sports Videos},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660119},
doi = {10.1145/2660114.2660119},
abstract = {Recent developments in sport analytics have heightened the interest in collecting data on the behavior of individuals and of the entire team in sports events. Rather than using dedicated sensors for recording the data, the detection of semantic events reflecting a team's behavior and the subsequent annotation of video data is nowadays mostly performed by paid experts. In this paper, we present an approach to generating such annotations by leveraging the wisdom of the crowd. We present the CrowdSport application that allows to collect data for soccer games. It presents crowd workers short video snippets of soccer matches and allows them to annotate these snippets with event information. Finally, the various annotations collected from the crowd are automatically disambiguated and integrated into a coherent data set. To improve the quality of the data entered, we have implemented a rating system that assigns each worker a trustworthiness score denoting the confidence towards newly entered data. Using the DBSCAN clustering algorithm and the confidence score, the integration ensures that the generated event labels are of high quality, despite of the heterogeneity of the participating workers. These annotations finally serve as a basis for a video retrieval system that allows users to search for video sequences on the basis of a graphical specification of team behavior or motion of the individual player. Our evaluations of the crowd-based semantic event detection and video annotation using the Microworkers platform have shown the effectiveness of the approach and have led to results that are in most cases close to the ground truth and can successfully be used for various retrieval tasks.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {63–68},
numpages = {6},
keywords = {crowdsourcing, video annotation, sports, multimedia retrieval},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@article{10.1109/TCBB.2019.2937862,
author = {Hossain, Md. Ekramul and Khan, Arif and Moni, Mohammad Ali and Uddin, Shahadat},
title = {Use of Electronic Health Data for Disease Prediction: A Comprehensive Literature Review},
year = {2021},
issue_date = {March-April 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2937862},
doi = {10.1109/TCBB.2019.2937862},
abstract = {Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = {apr},
pages = {745–758},
numpages = {14}
}

@inproceedings{10.1145/3443467.3444711,
author = {Tian, Luogeng and Yang, Bailong and Yin, Xinli and Su, Yang},
title = {A Survey of Personalized Recommendation Based on Machine Learning Algorithms},
year = {2021},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3444711},
doi = {10.1145/3443467.3444711},
abstract = {Personalized recommendation is a key technology to effectively solve the overload of online information and eliminate information islands. It is widely known as an important way to improve the quality of information services. However, cold start, data sparseness, algorithm performance, recommendation accuracy and surprise are still the key issues that restrict users' personalized recommendations. Firstly, we review the development trend of personalized information recommendation algorithms in the past 15 years. And then we propose a new classification method for users' personalized recommendation based on machine learning algorithms with cold start, data sparseness, and the performance of the algorithm as the main goals. On this basis, we summarize and compare the ideas, practices and conclusions of related machine learning algorithms. Finally, we further summarize the main advantages and disadvantages of the 10 kinds of personalized recommendation algorithms from the perspective of classification proposed, and look forward to the development directions, difficulties, focus and methods of personalized recommendation algorithms.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {602–610},
numpages = {9},
keywords = {Sparse matrix, Machine learning, Personalized recommendation, Graph Neural Networks},
location = {Xiamen, China},
series = {EITCE 2020}
}

@inproceedings{10.1145/3468791.3469119,
author = {Sima, Ana Claudia and Mendes de Farias, Tarcisio and Anisimova, Maria and Dessimoz, Christophe and Robinson-Rechavi, Marc and Zbinden, Erich and Stockinger, Kurt},
title = {Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data},
year = {2021},
isbn = {9781450384131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468791.3469119},
doi = {10.1145/3468791.3469119},
abstract = {The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets.},
booktitle = {33rd International Conference on Scientific and Statistical Database Management},
pages = {61–72},
numpages = {12},
keywords = {Question Answering, Knowledge Graphs, Ranking},
location = {Tampa, FL, USA},
series = {SSDBM 2021}
}

@inproceedings{10.1145/3302425.3302447,
author = {Qiao, Lin and Ran, Ran and Wu, He and Zhou, Qiaoni and Liu, Sai and Liu, Yunfei},
title = {Imputation Method of Missing Values for Dissolved Gas Analysis Data Based on Iterative KNN and XGBoost},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302447},
doi = {10.1145/3302425.3302447},
abstract = {Power transformers are an important part of the power system. Accurate monitoring of its operating status is particularly important for the normal and stable operation of the entire power system and the timely diagnosis of potential faults. Dissolved Gas Analysis (DGA) can detect and judge the oil-immersed power transformer failure by comparing the dissolved gas content of the power transformer in the normal operating state and the oil in the fault state. However, in the operation process of the grid transformer, the detection data is often missing. This paper proposes an effective method based on iterative KNN and XGBoost method for missing values. Firstly, according to the XGBoost integration tree, there are missing values. Information such as the number of attribute divisions obtained by data set training calculates the importance scores of different attributes to determine the priority of the attributes, and then performs interpolation on the missing values ?in an iterative manner. The experimental results in the case of DGA dataset and different missing rate show that the proposed method is superior to the existing similar methods in accuracy, and the dataset after interpolation has a significant improvement on the classification effect of the classifier.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {11},
numpages = {7},
keywords = {Interpolation Priority, Dissolved Gas Analysis, Missing Values, Iterative KNN},
location = {Sanya, China},
series = {ACAI 2018}
}

@inproceedings{10.1145/2503859.2503863,
author = {Bento, Fernando and Costa, Carlos J.},
title = {ERP Measure Success Model; a New Perspective},
year = {2013},
isbn = {9781450322997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503859.2503863},
doi = {10.1145/2503859.2503863},
abstract = {This paper addresses the problem of defining and evaluating the success of ERP throughout the life cycle of the information system. In order to solve this problem, many of the theoretical and empirical contributions on the success of the information system are analysed and discussed.This approach allows the development of a new model; especially in Delone &amp; Mclean supported research.This work will try to establish a different perspective on the success of the ERP and can be an encouragement to some organizations or the many researchers that will be engaging in these areas, in order to help achieve more clearly the expected performance in the acquisition phase of ERPs. Many times that performance does not always happen [1].},
booktitle = {Proceedings of the 2013 International Conference on Information Systems and Design of Communication},
pages = {16–26},
numpages = {11},
keywords = {success measuring models, performance, information systems, ERP's life cycle, ERP's},
location = {Lisboa, Portugal},
series = {ISDOC '13}
}

@article{10.14778/2536222.2536238,
author = {Elmeleegy, Hazem and Li, Yinan and Qi, Yan and Wilmot, Peter and Wu, Mingxi and Kolay, Santanu and Dasdan, Ali and Chen, Songting},
title = {Overview of Turn Data Management Platform for Digital Advertising},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536222.2536238},
doi = {10.14778/2536222.2536238},
abstract = {This paper gives an overview of Turn Data Management Platform (DMP). We explain the purpose of this type of platforms, and show how it is positioned in the current digital advertising ecosystem. We also provide a detailed description of the key components in Turn DMP. These components cover the functions of (1) data ingestion and integration, (2) data warehousing and analytics, and (3) real-time data activation. For all components, we discuss the main technical and research challenges, as well as the alternative design choices. One of the main goals of this paper is to highlight the central role that data management is playing in shaping this fast growing multi-billion dollars industry.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1138–1149},
numpages = {12}
}

@inproceedings{10.1145/3473856.3473879,
author = {Herbert, Franziska and Schmidbauer-Wolf, Gina Maria and Reuter, Christian},
title = {Who Should Get My Private Data in Which Case? Evidence in the Wild},
year = {2021},
isbn = {9781450386456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473856.3473879},
doi = {10.1145/3473856.3473879},
abstract = {As a result of the ongoing digitalization of our everyday lives, the amount of data produced by everyone is steadily increasing. This happens through personal decisions and items, such as the use of social media or smartphones, but also through more and more data acquisition in public spaces, such as e.g., Closed Circuit Television. Are people aware of the data they are sharing? What kind of data do people want to share with whom? Are people aware if they have Wi-Fi, GPS, or Bluetooth activated as potential data sharing functionalities on their phone? To answer these questions, we conducted a representative online survey as well as face-to-face interviews with users in Germany. We found that most users wanted to share private data on premise with most entities, indicating that willingness to share data depends on who has access to the data. Almost half of the participants would be more willing to share data with specific entities (state bodies &amp; rescue forces) in the event that an acquaintance is endangered. For Wi-Fi and GPS the frequencies of self-reported and actual activation on the smartphone are almost equal, but 17% of participants were unaware of the Bluetooth status on their smartphone. Our research is therefore in line with other studies suggesting relatively low privacy awareness of users.},
booktitle = {Proceedings of Mensch Und Computer 2021},
pages = {281–293},
numpages = {13},
keywords = {privacy, data sharing, survey, awareness},
location = {Ingolstadt, Germany},
series = {MuC '21}
}

@inproceedings{10.1145/3491102.3502140,
author = {Rixen, Jan Ole and Colley, Mark and Askari, Ali and Gugenheimer, Jan and Rukzio, Enrico},
title = {Consent in the Age of AR: Investigating The Comfort With Displaying Personal Information in Augmented Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502140},
doi = {10.1145/3491102.3502140},
abstract = {Social Media (SM) has shown that we adapt our communication and disclosure behaviors to available technological opportunities. Head-mounted Augmented Reality (AR) will soon allow to effortlessly display the information we disclosed not isolated from our physical presence (e.g., on a smartphone) but visually attached to the human body. In this work, we explore how the medium (AR vs. Smartphone), our role (being augmented vs. augmenting), and characteristics of information types (e.g., level of intimacy, self-disclosed vs. non-self-disclosed) impact the users’ comfort when displaying personal information. Conducting an online survey (N=148), we found that AR technology and being augmented negatively impacted this comfort. Additionally, we report that AR mitigated the effects of information characteristics compared to those they had on smartphones. In light of our results, we discuss that information augmentation should be built on consent and openness, focusing more on the comfort of the augmented rather than the technological possibilities.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {295},
numpages = {14},
keywords = {Consent, Social Acceptability, User Acceptance, Personal Information, Public Experiences, Augmented Reality, Comfort, Disclosure, Mixed Reality, Data Glasses},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3502181.3531473,
author = {Yu, Xiaodong and Di, Sheng and Zhao, Kai and Tian, Jiannan and Tao, Dingwen and Liang, Xin and Cappello, Franck},
title = {Ultrafast Error-Bounded Lossy Compression for Scientific Datasets},
year = {2022},
isbn = {9781450391993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502181.3531473},
doi = {10.1145/3502181.3531473},
abstract = {Today's scientific high-performance computing applications and advanced instruments are producing vast volumes of data across a wide range of domains, which impose a serious burden on data transfer and storage. Error-bounded lossy compression has been developed and widely used in the scientific community because it not only can significantly reduce the data volumes but also can strictly control the data distortion based on the user-specified error bound. Existing lossy compressors, however, cannot offer ultrafast compression speed, which is highly demanded by numerous applications or use cases (such as in-memory compression and online instrument data compression). In this paper, we propose a novel ultrafast error-bounded lossy compressor that can obtain fairly high compression performance on both CPUs and GPUs and with reasonably high compression ratios. The key contributions are threefold. (1) We propose a generic error-bounded lossy compression framework---called SZx---that achieves ultrafast performance through its novel design comprising only lightweight operations such as bitwise and addition/subtraction operations, while still keeping a high compression ratio. (2) We implement SZx on both CPUs and GPUs and optimize the performance according to their architectures. (3) We perform a comprehensive evaluation with six real-world production-level scientific datasets on both CPUs and GPUs. Experiments show that SZx is 2~16x faster than the second-fastest existing error-bounded lossy compressor (either SZ or ZFP) on CPUs and GPUs, with respect to both compression and decompression.},
booktitle = {Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing},
pages = {159–171},
numpages = {13},
keywords = {high-speed compressor, gpu, scientific data, error-bounded lossy compression},
location = {Minneapolis, MN, USA},
series = {HPDC '22}
}

@article{10.1145/3462764,
author = {Bellio, Maura and Furniss, Dominic and Oxtoby, Neil P. and Garbarino, Sara and Firth, Nicholas C. and Ribbens, Annemie and Alexander, Daniel C. and Blandford, Ann},
title = {Opportunities and Barriers for Adoption of a Decision-Support Tool for Alzheimer’s Disease},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2691-1957},
url = {https://doi.org/10.1145/3462764},
doi = {10.1145/3462764},
abstract = {Clinical decision-support tools (DSTs) represent a valuable resource in healthcare. However, lack of Human Factors considerations and early design research has often limited their successful adoption. To complement previous technically focused work, we studied adoption opportunities of a future DST built on a predictive model of Alzheimer’s Disease (AD) progression. Our aim is two-fold: exploring adoption opportunities for DSTs in AD clinical care, and testing a novel combination of methods to support this process. We focused on understanding current clinical needs and practices, and the potential for such a tool to be integrated into the setting, prior to its development. Our user-centred approach was based on field observations and semi-structured interviews, analysed through workflow analysis, user profiles, and a design-reality gap model. The first two are common practice, whilst the latter provided added value in highlighting specific adoption needs. We identified the likely early adopters of the tool as being both psychiatrists and neurologists based in research-oriented clinical settings. We defined ten key requirements for the translation and adoption of DSTs for AD around IT, user, and contextual factors. Future works can use and build on these requirements to stand a greater chance to get adopted in the clinical setting.},
journal = {ACM Trans. Comput. Healthcare},
month = {sep},
articleno = {32},
numpages = {19},
keywords = {user-centred design, design-reality gap, Diffusion of innovation, healthcare, technology adoption}
}

@inbook{10.1145/3310205.3310212,
title = {Rule-Based Data Cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310212},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@article{10.1145/3411824,
author = {Zhang, Yun C. and Zhang, Shibo and Liu, Miao and Daly, Elyse and Battalio, Samuel and Kumar, Santosh and Spring, Bonnie and Rehg, James M. and Alshurafa, Nabil},
title = {SyncWISE: Window Induced Shift Estimation for Synchronization of Video and Accelerometry from Wearable Sensors},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3411824},
doi = {10.1145/3411824},
abstract = {The development and validation of computational models to detect daily human behaviors (e.g., eating, smoking, brushing) using wearable devices requires labeled data collected from the natural field environment, with tight time synchronization of the micro-behaviors (e.g., start/end times of hand-to-mouth gestures during a smoking puff or an eating gesture) and the associated labels. Video data is increasingly being used for such label collection. Unfortunately, wearable devices and video cameras with independent (and drifting) clocks make tight time synchronization challenging. To address this issue, we present the Window Induced Shift Estimation method for Synchronization (SyncWISE) approach. We demonstrate the feasibility and effectiveness of our method by synchronizing the timestamps of a wearable camera and wearable accelerometer from 163 videos representing 45.2 hours of data from 21 participants enrolled in a real-world smoking cessation study. Our approach shows significant improvement over the state-of-the-art, even in the presence of high data loss, achieving 90% synchronization accuracy given a synchronization tolerance of 700 milliseconds. Our method also achieves state-of-the-art synchronization performance on the CMU-MMAC dataset.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {107},
numpages = {26},
keywords = {Automatic Synchronization, Temporal Drift, Time Synchronization, Video, Wearable Sensor, Wearable Camera, Accelerometry}
}

@inproceedings{10.1145/3300061.3345456,
author = {Rodrigues, Jo\~{a}o G. P. and Aguiar, Ana},
title = {Extracting 3D Maps from Crowdsourced GNSS Skyview Data},
year = {2019},
isbn = {9781450361699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300061.3345456},
doi = {10.1145/3300061.3345456},
abstract = {3D maps of urban environments are useful in various fields ranging from cellular network planning to urban planning and climatology. These models are typically constructed using expensive techniques such as manual annotation with 3D modeling tools, extrapolated from satellite or aerial photography, or using specialized hardware with depth sensing devices. In this work, we show that 3D urban maps can be extracted from standard GNSS data, by analyzing the received satellite signals that are attenuated by obstacles, such as buildings. Furthermore, we show that these models can be extracted from low-accuracy GNSS data, crowdsourced opportunistically from standard smartphones during their user's uncontrolled daily commute trips, unleashing the potential of applying the principle to wide areas. Our proposal incorporates position inaccuracies in the calculations, and accommodates different sources of variability of the satellite signals' SNR. The diversity of collection conditions of crowdsourced GNSS positions is used to mitigate bias and noise from the data. A binary classification model is trained and evaluated on multiple urban scenarios using data crowdsourced from over 900 users. Our results show that the generalization accuracy for a Random Forest classifier in typical urban environments lies between 79% and 91% on 4 m wide voxels, demonstrating the potential of the proposed method for building 3D maps for wide urban areas.},
booktitle = {The 25th Annual International Conference on Mobile Computing and Networking},
articleno = {55},
numpages = {15},
keywords = {gnss snr measurements, 3d mapping, crowdsensing},
location = {Los Cabos, Mexico},
series = {MobiCom '19}
}

@inbook{10.1145/3310205.3310213,
title = {Machine Learning and Probabilistic Data Cleaning},
year = {2019},
isbn = {9781450371520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310205.3310213},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
booktitle = {Data Cleaning}
}

@inproceedings{10.1145/2487575.2488217,
author = {Kohavi, Ron and Deng, Alex and Frasca, Brian and Walker, Toby and Xu, Ya and Pohlmann, Nils},
title = {Online Controlled Experiments at Large Scale},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2488217},
doi = {10.1145/2487575.2488217},
abstract = {Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals more than $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1168–1176},
numpages = {9},
keywords = {controlled experiments, randomized experiments, a/b testing},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/2858036.2858445,
author = {West, Peter and Giordano, Richard and Van Kleek, Max and Shadbolt, Nigel},
title = {The Quantified Patient in the Doctor's Office: Challenges &amp; Opportunities},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858445},
doi = {10.1145/2858036.2858445},
abstract = {While the Quantified Self and personal informatics fields have focused on the individual's use of self-logged data about themselves, the same kinds of data could, in theory, be used to improve diagnosis and care planning. In this paper, we seek to understand both the opportunities and bottlenecks in the use of self-logged data for differential diagnosis and care planning during patient visits to both primary and secondary care. We first conducted a literature review to identify potential factors influencing the use of self-logged data in clinical settings. This informed the design of our experiment, in which we applied a vignette-based role-play approach with general practitioners and hospital specialists in the US and UK, to elicit reflections on and insights about using patient self-logged data. Our analysis reveals multiple opportunities for the use of self-logged data in the differential diagnosis workflow, identifying capture, representational, and interpretational challenges that are potentially preventing self-logged data from being effectively interpreted and applied by clinicians to derive a patient's prognosis and plan of care.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {3066–3078},
numpages = {13},
keywords = {quantified self, clinical decision making, self-tracking},
location = {San Jose, California, USA},
series = {CHI '16}
}

@article{10.1145/3210548,
author = {De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and Dubrawski, Artur},
title = {Machine Learning for the Developing World},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3210548},
doi = {10.1145/3210548},
abstract = {Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = {aug},
articleno = {9},
numpages = {14},
keywords = {developing countries, Global development}
}

@inproceedings{10.1145/3448016.3457330,
author = {Peng, Jinglin and Wu, Weiyuan and Lockhart, Brandon and Bian, Song and Yan, Jing Nathan and Xu, Linghao and Chi, Zhixuan and Rzeszotarski, Jeffrey M. and Wang, Jiannan},
title = {DataPrep.EDA: Task-Centric Exploratory Data Analysis for Statistical Modeling in Python},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457330},
doi = {10.1145/3448016.3457330},
abstract = {Exploratory Data Analysis (EDA) is a crucial step in any data science project. However, existing Python libraries fall short in supporting data scientists to complete common EDA tasks for statistical modeling. Their API design is either too low level, which is optimized for plotting rather than EDA, or too high level, which is hard to specify more fine-grained EDA tasks. In response, we propose DataPrep.EDA, a novel task-centric EDA system in Python. DataPrep.EDA allows data scientists to declaratively specify a wide range of EDA tasks in different granularity with a single function call. We identify a number of challenges to implement DataPrep.EDA, and propose effective solutions to improve the scalability, usability, customizability of the system. In particular, we discuss some lessons learned from using Dask to build the data processing pipelines for EDA tasks and describe our approaches to accelerate the pipelines. We conduct extensive experiments to compare DataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python. The experiments show that DataPrep.EDA significantly outperforms Pandas-profiling in terms of both speed and user experience. DataPrep.EDA is open-sourced as an EDA component of DataPrep: https://github.com/sfu-db/dataprep.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2271–2280},
numpages = {10},
keywords = {data preparation, data exploration, exploratory data analysis, data profiling, statistical modeling, python},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inbook{10.1145/3447404.3447430,
title = {Authors’ Biographies/Index},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447430},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {433–455},
numpages = {23}
}

@inproceedings{10.1145/3377049.3377083,
author = {Chowdhury, S. M. Habibul Mursaleen and Jahan, Ferdous and Sara, Sarawat Murtaza and Nandi, Dip},
title = {Secured Blockchain Based Decentralised Internet: A Proposed New Internet},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377083},
doi = {10.1145/3377049.3377083},
abstract = {Throughout this paper, we try to describe with blockchain technology the decentralization of the internet. A decentralized network that encourages the internet to operate from the smartphone or tablet of anybody instead of centralized servers. A decentralized implementation would be based on a peer-to-peer network that is dependent on a user community. Their machines connected to the internet will host the network, not a community of more powerful servers. Each site would be distributed across thousands of nodes on various devices. The data is therefore not contained, owned by private storage facilities. There is therefore no central point to hack, and no way for an oligarchy of entities to take control of it. A proposed alternative was formed based on a systematic literature review that demonstrates that Internet decentralization is what this modern technology needs in order to address not only the weaknesses of current servers including server down issue, hacking and data manipulation or single point of failure, but also to prevent companies from monetizing the data of citizens through their server and to market them to the advertisers.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {8},
numpages = {7},
keywords = {DApp, Bitcoin, Data Privacy, Decentralised Web, Encryption, Server vulnerabilities, Smart Contracts, Ethereum, Web 3.0, Peer-To-Peer Network, Blockchain, Cryptography, Whisper},
location = {Dhaka, Bangladesh},
series = {ICCA 2020}
}

@inproceedings{10.1145/3477314.3507007,
author = {Haudenschild, Christian and Vaickus, Louis and Levy, Joshua},
title = {Configuring a Federated Network of Real-World Patient Health Data for Multimodal Deep Learning Prediction of Health Outcomes},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507007},
doi = {10.1145/3477314.3507007},
abstract = {Vast quantities of electronic patient medical data are currently being collated and processed in large federated data repositories. For instance, TriNetX, Inc., a global health research network, has access to more than 300 million patients, sourced from healthcare organizations, biopharmaceutical companies, and contract research organizations. As such, pipelines that are able to algorithmically extract huge quantities of patient data from multiple modalities present opportunities to leverage machine learning and deep learning approaches with the possibility of generating actionable insight. In this work, we present a modular, semi-automated end-to-end machine and deep learning pipeline designed to interface with a federated network of structured patient data. This proof-of-concept pipeline is disease-agnostic, scalable, and requires little domain expertise and manual feature engineering in order to quickly produce results for the case of a user-defined binary outcome event. We demonstrate the pipeline's efficacy with three different disease workflows, with high discriminatory power achieved in all cases.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {627–635},
numpages = {9},
keywords = {electronic health records, lab measurements, comorbidity, federated data network, deep learning},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3450613.3459657,
author = {Amyrotos, Christos},
title = {Adaptive Visualizations for Enhanced Data Understanding and Interpretation},
year = {2021},
isbn = {9781450383660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450613.3459657},
doi = {10.1145/3450613.3459657},
abstract = {In a data driven economy where data volume and dimensions are explosively increasing, businesses rely on business intelligence and analytics (BI&amp;A) platforms for analysing their data and coming to beneficial decisions. With the ever-growing generation of data, the process of data analysis is becoming more complicated for the business users, as the exploration of more demanding use cases increases. While the existing BI&amp;A platforms provide myriads of data visualizations that support data exploration, none of those account for the user’s individual differences, needs or requirements, and thus may hinder the user’s understanding of visual data and consequently their decision-making processes. This work embarks on an interdisciplinary endeavour to introduce a human-centred adaptive data visualizations framework in the context of business, as the core of an adaptive data analytics platform, that aims to enhance the business user’s decision making by increasing her understanding of data. The framework is built using a multi-dimensional human-centred user model that goes beyond traditional user characteristics and accounts for cognitive factors, domain expertise and experience and factors related to the business context i.e., data, visualizations and tasks; a data visualization engine that will recommend to the unique-user the best-fit data visualizations based on the abovementioned user model; and an intelligent data analytics component that enhances the efficiency and effectiveness of the data exploration process by leveraging user interactions during the explorations to further inform the user model on the user’s expertise and experience.},
booktitle = {Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {291–297},
numpages = {7},
keywords = {personalization, adaptation, human factors, user modeling, data visualizations, business analytics},
location = {Utrecht, Netherlands},
series = {UMAP '21}
}

@inbook{10.1145/3447404.3447409,
author = {McMenemy, David},
title = {The Internet of Everything—Introducing Privacy},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447409},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {57–58},
numpages = {2}
}

@article{10.1145/3533378,
author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
title = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533378},
doi = {10.1145/3533378},
abstract = {In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow we show that practitioners face issues at each stage of the deployment process. The goal of this paper is to lay out a research agenda to explore approaches addressing these challenges.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {apr},
keywords = {sofware deployment, Machine learning applications}
}

@inbook{10.1145/3447404.3447425,
author = {McMenemy, David},
title = {Ethics and Personal Context},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447425},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {377–378},
numpages = {2}
}

@inbook{10.1145/3447404.3447427,
author = {McMenemy, David},
title = {Ethics and Adaptive Touch Interfaces},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447427},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {407–408},
numpages = {2}
}

@inbook{10.1145/3447404.3447413,
author = {McMenemy, David},
title = {Ethical Issues of Digital Signal Processing},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447413},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {141–142},
numpages = {2}
}

@article{10.1613/jair.1.12228,
author = {Burkart, Nadia and Huber, Marco F.},
title = {A Survey on the Explainability of Supervised Machine Learning},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12228},
doi = {10.1613/jair.1.12228},
abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
journal = {J. Artif. Int. Res.},
month = {may},
pages = {245–317},
numpages = {73}
}

@inbook{10.1145/3447404.3447429,
author = {McMenemy, David},
title = {Ethics in Automotive User Interface},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447429},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {431–432},
numpages = {2}
}

@inbook{10.1145/3447404.3447421,
author = {McMenemy, David},
title = {Ethical Issues in Probabilistic Text Entry},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447421},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {321–322},
numpages = {2}
}

@inproceedings{10.1145/3277139.3277179,
author = {Lu, Yi and Yin, Jun and Ge, Shilun},
title = {Analysis of Dynamic Complexity Feature of Information System Data Based on Visualization},
year = {2018},
isbn = {9781450364867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277139.3277179},
doi = {10.1145/3277139.3277179},
abstract = {The volume and complexity of the data in the information system continues to increase during the operation of the information system. The phenomenon leads to the fact that decision-makers are faced with data-rich and information-deficient situations. How to intuitively grasp the dynamic complexity of information system data has become a concern of scholars. Therefore, this paper collected and organized the system log data of two shipping companies, adopted multiple visualization forms, and combined the dynamic complexity measurement methods of information systems, focusing on the overall distribution status and development trend of the data dynamic complexity and internal information system and the influence of complexity of subsystems within the information system on overall data dynamic operation complexity, and then analyzed the overall feature of the dynamic complexity of information system data, and summarized the feature regulars of the complexity of the visualization method. The results show that the visual feature analysis of the dynamic complexity of information systems can improve the ability of enterprises to analyze and make decisions, and provide a basis for enterprise managers to develop corresponding management measures.},
booktitle = {Proceedings of the 2018 International Conference on Information Management &amp; Management Science},
pages = {206–215},
numpages = {10},
keywords = {management information system, feature analysis, visualization, dynamic complexity},
location = {Chengdu, China},
series = {IMMS '18}
}

