@article{FARAHZADI2018176,
title = {Middleware technologies for cloud of things: a survey},
journal = {Digital Communications and Networks},
volume = {4},
number = {3},
pages = {176-188},
year = {2018},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2017.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352864817301268},
author = {Amirhossein Farahzadi and Pooyan Shams and Javad Rezazadeh and Reza Farahbakhsh},
keywords = {CoT, IoT, Middleware, Fog computing, Cloud},
abstract = {The next wave of communication and applications will rely on new services provided by the Internet of Things which is becoming an important aspect in human and machines future. IoT services are a key solution for providing smart environments in homes, buildings, and cities. In the era of massive number of connected things and objects with high growth rate, several challenges have been raised, such as management, aggregation, and storage for big produced data. To address some of these issues, cloud computing emerged to the IoT as Cloud of Things (CoT), which provides virtually unlimited cloud services to enhance the large-scale IoT platforms. There are several factors to be considered in the design and implementation of a CoT platform. One of the most important and challenging problems is the heterogeneity of different objects. This problem can be addressed by deploying a suitable “middleware” which sits between things and applications as a reliable platform for communication among things with different interfaces, operating systems, and architectures. The main aim of this paper is to study the middleware technologies for CoT. Toward this end, we first present the main features and characteristics of middlewares. Next, we study different architecture styles and service domains. Then, we present several middlewares that are suitable for CoT-based platforms and finally, a list of current challenges and issues in the design of CoT-based middlewares is discussed.}
}
@article{ZANETTI2018151,
title = {To accelerate cancer prevention in Europe: Challenges for cancer registries},
journal = {European Journal of Cancer},
volume = {104},
pages = {151-159},
year = {2018},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0959804918313716},
author = {R. Zanetti and L. Sacchetto and J.W. Coebergh and S. Rosso},
keywords = {Cancer registration, Europe, Preventive interventions, Cancer burden, Data innovation},
abstract = {The availability of population-based cancer registry (CR) data is paramount in the development of modern oncology. Major contributions consisted in accurately measuring cancer burden (incidence, survival and prevalence, beside mortality), identifying and quantifying risk factors (case control and cohort studies that, in the last two decades, included gene variant assessment) and evaluating outcomes of treatments and preventive interventions, including mass screening. Cancer registration coverage of European populations progressed slowly since 1940 and is now almost 50%. Areas lacking high-quality national population-based cancer registration still exist within large countries such as France, Italy, Romania and Spain, Germany and Poland having national plans and legislation to reach complete coverage. Depending on programme ownership, history and institutional organisation, European CRs showed huge variations in the scope (research domain), size, available resources and finally exploitation of collected data. This reflects their heterogeneous origins stemming from different professional backgrounds and healthcare systems. This review discusses not only the potential for contributing to acceleration of prevention but also the coverage expansion by and innovation of CR organizations. The latter can be attained not only by more standardisation in institutional organisation and operative methodologies but also by intensification of scientific production and risk communication. The CR's agenda should focus on cancers caused by identifiable risk factor(s) that are amenable to preventive actions, including early detection; short-term priorities usually are with tobacco, and medium-term priorities are with alcohol, occupational exposures, infection-related cancers and ultraviolet-related skin cancers, while obesity-related cancers are likely to increase gradually further in the long term.}
}
@article{STEWART2017736,
title = {Crowdsourcing Samples in Cognitive Science},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {10},
pages = {736-748},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317301316},
author = {Neil Stewart and Jesse Chandler and Gabriele Paolacci},
abstract = {Crowdsourcing data collection from research participants recruited from online labor markets is now common in cognitive science. We review who is in the crowd and who can be reached by the average laboratory. We discuss reproducibility and review some recent methodological innovations for online experiments. We consider the design of research studies and arising ethical issues. We review how to code experiments for the web, what is known about video and audio presentation, and the measurement of reaction times. We close with comments about the high levels of experience of many participants and an emerging tragedy of the commons.}
}
@article{PI201819,
title = {Understanding Transit System Performance Using AVL-APC Data: An Analytics Platform with Case Studies for the Pittsburgh Region},
journal = {Journal of Public Transportation},
volume = {21},
number = {2},
pages = {19-40},
year = {2018},
issn = {1077-291X},
doi = {https://doi.org/10.5038/2375-0901.21.2.2},
url = {https://www.sciencedirect.com/science/article/pii/S1077291X22000601},
author = {Xidong Pi and Mark Egge and Jackson Whitmore and Zhen (Sean) Qian and Amy Silbermann},
keywords = {Transit system, Automatic Vehicle Location, Automatic Passenger Counting, data analytics platform, performance metrics, bus bunching, service quality},
abstract = {This paper introduces a novel transit data analytics platform for public transit planning, assessing service quality and revealing service problems in high spatiotemporal resolution for public transit systems based on Automatic Passenger Counting (APC) and Automatic Vehicle Location (AVL) technologies. The platform offers a systematic way for users and decision makers to understand system performance from many aspects of service quality, including passenger waiting time, stop-skipping frequency, bus bunching level, bus travel time, on-time performance, and bus fullness. The AVL-APC data from September 2012 to March 2016 were archived in a database to support the development of a user-friendly web application that allows both users and managers to interactively query bus performance metrics for any bus routes, stops, or trips for any time period. This paper demonstrates a case study using the platform to examine bus bunching in a transit system operated by the Port Authority of Allegheny County (PAAC) in Pittsburgh. It is found that the incidence of bus bunching is heavily impacted by the location on the route as well as the time of day, and the bunching problem is more severe for bus routes operating in mixed traffic than for bus rapid transit, which operates along a dedicated busway. Furthermore, a second case study is presented with a comprehensive analysis on a representative route in Pittsburgh under schedule changes. Suggestions for operation of this route to improve service quality are proposed based on the data analytics results.}
}
@article{CHAO201858,
title = {A gray-box performance model for Apache Spark},
journal = {Future Generation Computer Systems},
volume = {89},
pages = {58-67},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17323233},
author = {Zemin Chao and Shengfei Shi and Hong Gao and Jizhou Luo and Hongzhi Wang},
keywords = {Gray-box method, Apache Spark, Performance prediction model, Machine learning},
abstract = {Apache Spark is a powerful open source data processing platform. It is getting more and more popular with the growing need of processing massive amounts of data. A performance prediction model not only helps administrators to have a better understanding of system behavior, but also is useful in performance tuning. However, considering the complex application processing mechanism of Spark, it is not an easy job to model the relationship between system performance and configuration settings. In this paper, we present a gray-box performance model for Spark applications based on machine learning algorithms. Given a specific Spark application, the size of its input data and some key system parameters, this performance model is able to forecast its execution time according to history information. To achieve better accuracy, our model takes basic hardware information and the resource allocation strategy of Spark into consideration. In our experiments, result shows our gray-box model is better than typical black-box approaches in most of the cases. We consider this model is helpful for further researches on Apache Spark.}
}
@article{KIM201795,
title = {Report of the Second Asian Prostate Cancer (A-CaP) Study Meeting},
journal = {Prostate International},
volume = {5},
number = {3},
pages = {95-103},
year = {2017},
issn = {2287-8882},
doi = {https://doi.org/10.1016/j.prnil.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S2287888217300491},
author = {Choung-Soo Kim and Ji Youl Lee and Byung Ha Chung and Wun-Jae Kim and Ng Chi Fai and Lukman Hakim and Rainy Umbas and Teng Aik Ong and Jasmine Lim and Jason L. Letran and Edmund Chiong and Tong-lin Wu and Bannakij Lojanapiwat and Levent Türkeri and Declan G. Murphy and Robert A. Gardiner and Kim Moretti and Matthew Cooperberg and Peter Carroll and Seong Ki Mun and Shiro Hinotsu and Yoshihiko Hirao and Seiichiro Ozono and Shigeo Horie and Mizuki Onozawa and Yasuhide Kitagawa and Tadaichi Kitamura and Mikio Namiki and Hideyuki Akaza},
keywords = {Asia, Database, Prospective study, Prostate cancer},
abstract = {The Asian Prostate Cancer (A-CaP) Study is an Asia-wide initiative that has been developed over the course of 2 years. The study was launched in December 2015 in Tokyo, Japan, and the participating countries and regions engaged in preparations for the study during the course of 2016, including patient registration and creation of databases for the purpose of the study. The Second A-CaP Meeting was held on September 8, 2016 in Seoul, Korea, with the participation of members and collaborators from 12 countries and regions. Under the study, each participating country or region will begin registration of newly diagnosed prostate cancer patients and conduct prognostic investigations. From the data gathered, common research themes will be identified, such as comparisons among Asian countries of background factors in newly diagnosed prostate cancer patients. This is the first Asia-wide study of prostate cancer and has developed from single country research efforts in this field, including in Japan and Korea. At the Second Meeting, participating countries and regions discussed the status of preparations and discussed various issues that are being faced. These issues include technical challenges in creating databases, promoting participation in each country or region, clarifying issues relating to data input, addressing institutional issues such as institutional review board requirements, and the need for dedicated data managers. The meeting was positioned as an opportunity to share information and address outstanding issues prior to the initiation of the study. In addition to A-CaP-specific discussions, a series of special lectures was also delivered as a means of providing international perspectives on the latest developments in prostate cancer and the use of databases and registration studies around the world.}
}
@article{BUER20181035,
title = {The Data-Driven Process Improvement Cycle: Using Digitalization for Continuous Improvement},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1035-1040},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.471},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318315994},
author = {Sven-Vegard Buer and Giuseppe Ismael Fragapane and Jan Ola Strandhagen},
keywords = {digitalization, digitization, Industry 4.0, improvement cycle, lean manufacturing},
abstract = {Industry 4.0 is the first industrial revolution to be announced a priori, and there is thus a significant ambiguity surrounding the term and what it actually entails. This paper aims to clearly define digitalization, a key enabler of Industry 4.0, and illustrate how it can be used for improvement through proposing an improvement cycle and an associated digitalization typology. These tools can be used by organizations to guide improvement processes, focusing on the new possibilities introduced by the enormous amounts of data currently available. The usage of the tools is illustrated by presenting four scenarios from Kanban control, where each scenario is mapped according to their digitalization level.}
}
@incollection{CUMMINS2017183,
title = {Chapter 6 - Enterprise Data Management},
editor = {Fred A. Cummins},
booktitle = {Building the Agile Enterprise (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {183-208},
year = {2017},
series = {The MK/OMG Press},
isbn = {978-0-12-805160-3},
doi = {https://doi.org/10.1016/B978-0-12-805160-3.00006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128051603000065},
author = {Fred A. Cummins},
keywords = {Data management architecture, Master data, Data modeling, Business metadata, Data services, Data ownership, Legal records, Data residency, Analytics, Knowledge management, Enterprise logical data model},
abstract = {Data are the means to coordinate activity, record actions and responsibilities, solve problems, develop plans, and measure performance. Every business activity has supporting data that, in context, are information about the business. This chapter describes a data management architecture that is appropriate for the agile enterprise, to support the integration of the building blocks, recognize relevant events and trends, capture and share knowledge, and evaluate performance and value delivery. The architecture includes management of master data, business metadata, and other key enterprise data services.}
}
@article{OTEROSROZAS201874,
title = {Using social media photos to explore the relation between cultural ecosystem services and landscape features across five European sites},
journal = {Ecological Indicators},
volume = {94},
pages = {74-86},
year = {2018},
note = {Landscape Indicators – Monitoring of Biodiversity and Ecosystem Services at Landscape Level},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2017.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X17300572},
author = {Elisa Oteros-Rozas and Berta Martín-López and Nora Fagerholm and Claudia Bieling and Tobias Plieninger},
keywords = {Landscape values, Non-material benefits, Photos, Social preferences, User generated content (UGC)},
abstract = {Cultural ecosystem services, such as aesthetic and recreational enjoyment, as well as sense of place and local identity, play an outstanding role in the contribution of landscapes to human well-being. Online data shared on social networks, particularly geo-tagged photos, are becoming an increasingly attractive source of information about cultural ecosystem services. Landscape photographs tell about the significance of human relationships with landscapes, human practices in landscapes and the landscape features that might possess value in terms of cultural ecosystem services. Despite all the recent advances in this emerging methodological approach, some challenges remain to be explored: (a) how to assess a broad suite of cultural ecosystem services, beyond aesthetic beauty of landscapes, (b) how to identify the landscape features that are relevant for providing cultural ecosystem services and determine trade-offs and synergies among cultural ecosystem services. To address these challenges, we have developed a methodological approach suitable for eliciting the importance of cultural ecosystem services and the landscape features underpinning their provision across five different sites in Europe (in Estonia, Greece, Spain, Sweden and Switzerland). We have performed a content analysis of 1.404 photos uploaded in Flickr and Panoramio platforms that can represent cultural ecosystem services. Four bundles of landscapes features and cultural ecosystem services showed the relation of recreation with mountain areas (terrestrial recreation) and with water bodies (aquatic recreation). Cultural heritage, social and spiritual values were particularly attached to landscapes with woodpastures and grasslands, as well as urban features and infrastructures, i.e. to more anthropogenic landscapes. A positive though weak relationship was found between landscape diversity and cultural ecosystem services diversity. Particularly wood-pastures and shrubs were more frequently portrayed in all study sites in comparison with their actual land cover. The results can be of interest both for methodological purposes in the face of an increasing trend in the use of geo-tagged photos in the ecosystem services research and for the elicitation and comparison of landscape values across European cultural landscapes.}
}
@article{ZHANG2018576,
title = {DRI-RCNN: An approach to deceptive review identification using recurrent convolutional neural network},
journal = {Information Processing & Management},
volume = {54},
number = {4},
pages = {576-592},
year = {2018},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317304272},
author = {Wen Zhang and Yuhang Du and Taketoshi Yoshida and Qing Wang},
keywords = {Deceptive review identification, Recurrent convolutional vector, Contextual knowledge, Word embedding, DRI-RCNN},
abstract = {With the widespread of deceptive opinions in the Internet, how to identify online deceptive reviews automatically has become an attractive topic in research field. Traditional methods concentrate on extracting different features from online reviews and training machine learning classifiers to produce models to decide whether an incoming review is deceptive or not. This paper proposes an approach called DRI-RCNN (Deceptive Review Identification by Recurrent Convolutional Neural Network) to identify deceptive reviews by using word contexts and deep learning. The basic idea is that since deceptive reviews and truthful reviews are written by writers without and with real experience respectively, the writers of the reviews should have different contextual knowledge on their target objectives under description. In order to differentiate the deceptive and truthful contextual knowledge embodied in the online reviews, we represent each word in a review with six components as a recurrent convolutional vector. The first and second components are two numerical word vectors derived from training deceptive and truthful reviews, respectively. The third and fourth components are left neighboring deceptive and truthful context vectors derived by training a recurrent convolutional neural network on context vectors and word vectors of left words. The fifth and six components are right neighboring deceptive and truthful context vectors of right words. Further, we employ max-pooling and ReLU (Rectified Linear Unit) filter to transfer recurrent convolutional vectors of words in a review to a review vector by extracting positive maximum feature elements in recurrent convolutional vectors of words in the review. Experiment results on the spam dataset and the deception dataset demonstrate that the proposed DRI-RCNN approach outperforms the state-of-the-art techniques in deceptive review identification.}
}
@article{PIELMEIER2017271,
title = {Modeling Approach for Situational Event-handling within Production Planning and Control Based on Complex Event Processing},
journal = {Procedia CIRP},
volume = {63},
pages = {271-276},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.03.158},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117303402},
author = {Julia Pielmeier and Stefan Braunreuther and Gunther Reinhart},
keywords = {Modeling, Production, Process control},
abstract = {Nowadays industrial production environments are complex, volatile, and driven by uncertainties. Manufacturing companies are striving for flexibility and adaptability to cope with these challenges and remain competitive. Market requirements such as shortened product life cycles, increasing number of variants, and customized products lead to complexity in manufacturing systems. Possible approaches to cope with such challenges can be found in the field of ‘Industrie 4.0’. In particular, decision-making and real-time reaction systems are one way to handle the complexity. To cope with this complexity, digitalization like the vision of ‘Industrie 4.0’ can offer different solutions. However, digitalization leads to an increase of the amount of data describing the status of products and resources within an industrial production environment. In order to achieve a near real time monitoring and control of production and logistics processes, intelligent processing and analyzing of the acquired data is necessary. As a result of this development, so called “complex event processing” (CEP) is essential for analyzing extensive data streams in real-time. In order to derive the rules for a CEP engine, an event model has to be described to visualize the relations, constraints and abstraction levels of production processes. The main focus within this paper is a modeling approach for the situational handling of events within production planning and control. The requirements of the modeling method are focused on the use case of a mass production for carbon-fiber-reinforced plastic CFRP components.}
}
@article{SABERI2018356,
title = {Interactive feature selection for efficient customer recognition in contact centers: Dealing with common names},
journal = {Expert Systems with Applications},
volume = {113},
pages = {356-376},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418304299},
author = {Morteza Saberi and Martin Theobald and Omar K Hussain and Elizabeth Chang and Farookh Khadeer Hussain},
keywords = {Decision support systems, Interactive customer recognition, Entity resolution, Common personal names},
abstract = {We propose an interactive decision-making framework to assist a Customer Service Representative (CSR) in the efficient and effective recognition of customer records in a database with many ambiguous entries. Our proposed framework consists of three integrated modules. The first module focuses on the detection and resolution of duplicate records to improve effectiveness and efficiency in customer recognition. The second module determines the level of ambiguity in recognizing an individual customer when there are multiple records with the same name. The third module recommends the series of feature-related questions that the CSR should ask the customer to enable rapid recognition, based on that level of ambiguity. In the first module, the F-Swoosh approach for duplicate detection is used, and in the second module a dynamic programming-based technique is used to determine the level of ambiguity within the customer database for a given name. In the third module, Levenshtein edit distance is used for feature selection in combination with weights based on the Inverse Document Frequency (IDF) of terms. The algorithm that requires the minimum number of questions to be put to the customer to achieve recognition is the algorithm that is chosen. We evaluate the proposed framework on a synthetic dataset and demonstrate how it assists the CSR to rapidly recognize the correct customer.}
}
@article{UHLEMANN2017335,
title = {The Digital Twin: Realizing the Cyber-Physical Production System for Industry 4.0},
journal = {Procedia CIRP},
volume = {61},
pages = {335-340},
year = {2017},
note = {The 24th CIRP Conference on Life Cycle Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.11.152},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116313129},
author = {Thomas H.-J. Uhlemann and Christian Lehmann and Rolf Steinhilper},
keywords = {cyber-physical production systems, multimodal data acquisition, digital twin, production system optimization, Industry 4.0},
abstract = {Concerning current approaches to planning of manufacturing processes, the acquisition of a sufficient data basis of the relevant process information and subsequent development of feasible layout options requires 74% of the overall time-consumption. However, the application of fully automated techniques within planning processes is not yet common practice. Deficits are to be observed in the course of the use of a fully automated data acquisition of the underlying process data, a key element of Industry 4.0, as well as the evaluation and quantification and analysis of the gathered data. As the majority of the planning operations are conducted manually, the lack of any theoretical evaluation renders a benchmarking of the results difficult. Current planning processes analyze the manually achieved results with the aid of simulation. Evaluation and quantification of the planning procedure are limited by complexity that defies manual controllability. Research is therefore required with regard to automated data acquisition and selection, as the near real-time evaluation and analysis of a highly complex production systems relies on a real-time generated database. The paper presents practically feasible approaches to a multi-modal data acquisition approach, its requirements and limitations. The further concept of the Digital Twin for a production process enables a coupling of the production system with its digital equivalent as a base for an optimization with a minimized delay between the time of data acquisition and the creation of the Digital Twin. Therefore a digital data acquisition approach is necessary. As a consequence a cyber-physical production system can be generated, that opens up powerful applications. To ensure a maximum concordance of the cyber-physical process with its real-life model a multimodal data acquisition and evaluation has to be conducted. The paper therefore presents a concept for the composition of a database and proposes guidelines for the implementation of the Digital Twin in production systems in small and medium-sized enterprises.}
}
@incollection{CARVAJAL20181,
title = {Chapter One - Introduction to Digital Oil and Gas Field Systems},
editor = {Gustavo Carvajal and Marko Maucec and Stan Cullick},
booktitle = {Intelligent Digital Oil and Gas Fields},
publisher = {Gulf Professional Publishing},
address = {Boston},
pages = {1-41},
year = {2018},
isbn = {978-0-12-804642-5},
doi = {https://doi.org/10.1016/B978-0-12-804642-5.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046425000013},
author = {Gustavo Carvajal and Marko Maucec and Stan Cullick},
keywords = {Digital oil field, Data streaming, Collaboration, Workflows, Visualization},
abstract = {World energy demand will grow from about 550 QBTU in 2012 to 850 QBTU in 2040 according to 2016 projections from the International Energy Agency (IEA). Although renewable energy sources will grow by a large percentage, petroleum-based liquids (oil) and natural gas will continue to be the largest contributors to energy utilization by the world's population, representing about 55% of the total in 2040. As any current oil and gas production naturally declines, the continued growth of petroleum fuels will be made possible only by forward leaps in technology in finding, drilling, and producing those resources more efficiently and economically. One of the great stories in oil and gas production is the industry's implementation of new digital technologies that increase production for less unit cost. This “revolution” of the “digital oil field” is the subject of this book.}
}
@article{KADLEC2017258,
title = {Using crowdsourced and weather station data to fill cloud gaps in MODIS snow cover datasets},
journal = {Environmental Modelling & Software},
volume = {95},
pages = {258-270},
year = {2017},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364815217306746},
author = {Jiří Kadlec and Daniel P. Ames},
keywords = {Snow cover, Crowdsourcing, Interpolation, Winter sports},
abstract = {We present the design, development, and testing of a new software package for generating snow cover maps. Using a custom inverse distance weighting method, we combine volunteer snow reports, cross-country ski track reports and station measurements to fill cloud gaps in the Moderate Resolution Imaging Spectroradiometer (MODIS) snow cover product. The method is demonstrated by producing a continuous daily time step snow probability map dataset for the Czech Republic region. For validation, we checked the ability of our method to reconstruct MODIS snow cover under cloud by simulating cloud cover datasets and comparing estimated snow cover to actual MODIS snow cover. The percent correctly classified indicator showed accuracy between 80 and 90% using this method. The software is available as an R package. The output data sets are published on the HydroShare website for download and through a web map service for re-use in third-party applications.}
}
@article{LOBO2018241,
title = {KnowBR: An application to map the geographical variation of survey effort and identify well-surveyed areas from biodiversity databases},
journal = {Ecological Indicators},
volume = {91},
pages = {241-248},
year = {2018},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2018.03.077},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X18302322},
author = {Jorge M. Lobo and Joaquín Hortal and José Luís Yela and Andrés Millán and David Sánchez-Fernández and Emilio García-Roselló and Jacinto González-Dacosta and Juergen Heine and Luís González-Vilas and Castor Guisande},
keywords = {Spatial bias, Data limitations, Database records, Geographic distribution, Survey completeness, Wallacean shortfall},
abstract = {Biodiversity databases are typically incomplete and biased. We identify their three main limitations for characterizing the geographic distributions of species: unknown levels of survey effort, unknown absences of a species from a region, and unknown level of repeated occurrence of a species in different samples collected at the same location. These limitations hinder our ability to distinguish between the actual absence of a species at a given location and its (erroneous) apparent absence as consequence of inadequate surveys. Good practice in biodiversity research requires knowledge of the number, location and degree of completeness of relatively well-surveyed inventories within territorial units. We herein present KnowBR, an application designed to simultaneously estimate the completeness of species inventories across an unlimited number of spatial units and different geographical extents, resolutions and unit expanses from any biodiversity database. We use the number of database records gathered in a territorial unit as a surrogate of survey effort, assuming that such number correlates positively with the probability of recording a species within such area. Consequently, KnowBR uses a “record-by-species” matrix to estimate the relationship between the accumulated number of species and the number of database records to characterize the degree of completeness of the surveys. The final slope of the species accumulation curves and completeness percentages are used to discriminate and map well-surveyed territorial units according to user criteria. The capacity and possibilities of KnowBR are demonstrated through two examples derived from data of varying geographic extent and numbers of records. Further, we identify the main advances that would improve the current functionality of KnowBR.}
}
@incollection{SLIWA2018121,
title = {Chapter 7 - Security, Privacy, and Ethical Issues in Smart Sensor Health and Well-Being Applications},
editor = {Miguel Wister and Pablo Pancardo and Francisco Acosta and José Adán Hernández},
booktitle = {Intelligent Data Sensing and Processing for Health and Well-Being Applications},
publisher = {Academic Press},
pages = {121-140},
year = {2018},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-812130-6},
doi = {https://doi.org/10.1016/B978-0-12-812130-6.00007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012812130600007X},
author = {Jan Sliwa},
keywords = {Privacy, Safety, Research, Internet of Things, Internet of Everything, Randomized control trials},
abstract = {The rapidly evolving domain of sensor-based smart medical devices offers new opportunities and creates new risks. An overly enthusiastic approach, concentrating principally on novel functions and reducing the time to market, may lead to forgetting about important safety issues. The risk may be caused by one's own poor design or by malicious actions of others. We try here to take a broad perspective and discuss security, privacy, and ethical issues regarding such devices.}
}
@article{HERRERASEMENETS2018272,
title = {A data reduction strategy and its application on scan and backscatter detection using rule-based classifiers},
journal = {Expert Systems with Applications},
volume = {95},
pages = {272-279},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.11.041},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417307972},
author = {Vitali Herrera-Semenets and Osvaldo {Andrés Pérez-García} and Raudel Hernández-León and Jan {van den Berg} and Christian Doerr},
keywords = {Data mining, Data reduction, Intrusion detection, Scan, Backscatter},
abstract = {In the last few years, the telecommunications scenario has experienced an increase in the volume of information generated, as well as in the execution of malicious activities. In order to complement Intrusion Detection Systems (IDSs), data mining techniques have begun to play a fundamental role in data analysis. On the other hand, the presence of useless information and the amount of data generated by telecommunication services (leading to a huge dimensional problem), can affect the performance of traditional IDSs. In this sense, a data preprocessing strategy is necessary to reduce data, but reducing data without affecting the accuracy of IDSs represents a challenge. In this paper, we propose a new data preprocessing strategy which reduces the number of features and instances in the training collection without greatly affecting the achieved accuracy of IDSs. Finally, our proposal is evaluated using four different rule-based classifiers, which are tested on real scan and backscatter data collected by a network telescope.}
}
@article{STURMLINGER2018232,
title = {Development of a wear model of a manufacturing system based on external smart production data on the example of a spring coiling machine},
journal = {Procedia CIRP},
volume = {72},
pages = {232-236},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.260},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304311},
author = {Tobias Stürmlinger and Christoph Haar and Julian Pandtle and Volker Niemeyer},
keywords = {Industry 4.0, data exchange, smart data, manufacturing system engineering, wear model},
abstract = {Industry 4.0 is mostly used to reduce production waste, lead times and costs and to increase flexibility. In this paper, the authors present an approach how to use Industry 4.0 data to support the development of a new generation of a manufacturing system based on production and sensor data acquired at an external, producing company. A wear model of forming tools is developed to support finding profiles for the next machine generation. In particular the production system developer can identify the customers load cases for the dimensioning of their next generation tools. Furthermore, predictive maintenance is supported by the acquired data.}
}
@incollection{KENT2018305,
title = {Chapter 7 - Data, information and the smart factory},
editor = {Robin Kent},
booktitle = {Cost Management in Plastics Processing (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
pages = {305-326},
year = {2018},
isbn = {978-0-08-102269-6},
doi = {https://doi.org/10.1016/B978-0-08-102269-6.50007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022696500070},
author = {Robin Kent}
}
@article{JACOB2017260,
title = {Mind the Gap: Analyzing the Impact of Data Gap in Millennium Development Goals’ (MDGs) Indicators on the Progress toward MDGs},
journal = {World Development},
volume = {93},
pages = {260-278},
year = {2017},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2016.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X15310433},
author = {Arun Jacob},
keywords = {MDG, SDGs, value of data, performance measurement, monitoring and evaluation, goal setting},
abstract = {Summary
This paper analyzes the impact of data gap in Millennium Development Goals’ (MDGs) performance indicators on actual performance success of MDGs. Performance success, within the MDG framework, is quantified using six different ways proposed in the existing literature, including both absolute and relative performance and deviation from historical transition paths of MDG indicators. The empirical analysis clearly shows that the data gap in performance measurement is a significant predictor of poor MDG performance in terms of any of the six progress measures. Larger the data gap or weaker the performance measurement system, lesser is the probability of MDG performance success. The empirical methodology used in the paper combines a Heckman correction and instrumental variable estimation strategies to simultaneously account for potential endogeneity of the key data gap variable and bias due to sample selection. This result holds true even after controlling for overall national statistical capacity and a variety of socioeconomic factors. The paper underlines the need to strengthen the performance measurement system attached to the 2030 agenda for sustainable development and the associated Sustainable Development Goals (SDGs). This paper is the first attempt at empirically evaluating the value of data in the context of international development goals and gives empirical evidence for the need to harness the “data revolution” for sustainable development.}
}
@article{COLAKOVIC201817,
title = {Internet of Things (IoT): A review of enabling technologies, challenges, and open research issues},
journal = {Computer Networks},
volume = {144},
pages = {17-39},
year = {2018},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618305243},
author = {Alem Čolaković and Mesud Hadžialić},
keywords = {IoT (Internet of Things), IoT vision, IoT features, IoT enabling technologies, Open issues and challenges, Future research direction},
abstract = {IoT (Internet of Things) is a new paradigm which provides a set of new services for the next wave of technological innovations. IoT applications are nearly limitless while enabling seamless integration of the cyber-world with the physical world. However, despite the enormous efforts of standardization bodies, alliances, industries, researchers and others, there are still numerous problems to deal with in order to reach the full potential of IoT. These issues should be considered from various aspects such as enabling technologies, applications, business models, social and environmental impacts. In focus of this paper are open issues and challenges considered from the technological perspective. Just for clarification, we put in light different visions that stand behind this paradigm in order to facilitate a better understanding of the IoT's features. Furthermore, this exhaustive survey provides insights into the state-of-the-art of IoT enabling and emerging technologies. The most relevant among them are addressed with some details. The main scope is to deliver a comprehensive overview of open issues and challenges to be tackled by future research. We provide some insights into specific emerging ideas in order to facilitate future research. Also, this paper brings order in the existing literature by classifying contributions according to different research topics.}
}
@article{LISMONT201813,
title = {Predicting tax avoidance by means of social network analytics},
journal = {Decision Support Systems},
volume = {108},
pages = {13-24},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300228},
author = {Jasmien Lismont and Eddy Cardinaels and Liesbeth Bruynseels and Sander {De Groote} and Bart Baesens and Wilfried Lemahieu and Jan Vanthienen},
keywords = {Board interlocks, Predictive analytics, Social network analytics, Social ties, Tax avoidance, Tax planning},
abstract = {This study predicts tax avoidance by means of social network analytics. We extend previous literature by being the first to build a predictive model including a larger variation of network features. We construct a network of firms connected through shared board membership. Then, we apply three analytical techniques, logistic regression, decision trees, and random forests; to create five models using either firm characteristics, network characteristics or different combinations of both. A random forest including firm characteristics, network characteristics of firms and network characteristics of board members provides the best performance with a minimal increase of 7 pp in AUC. Hence, including network effects significantly improves the predictive ability of tax avoidance models, implying that board members exhibit specific knowledge which can carry over across firms. We find that having board members with no connections to low-tax companies lowers the likelihood of being a low-tax firm. Similarly, the higher the average tax rate of the companies a board member is connected to, the lower the chance of being low-tax. On the other hand, being connected to more low-tax firms increases the probability of being low-tax. Consistent with prior literature on firm-specific variables, PP&E has a positive influence on the probability of being low-tax, while EBITDA has a negative effect. Our results are informative for companies as to the director expertise they want to attract in their boards. Additionally, financial analysts and regulatory agencies can use our insights to predict which firms are likely to be low-tax and potentially at risk.}
}
@article{AMANI201732,
title = {Data mining applications in accounting: A review of the literature and organizing framework},
journal = {International Journal of Accounting Information Systems},
volume = {24},
pages = {32-58},
year = {2017},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1467089515300488},
author = {Farzaneh A. Amani and Adam M. Fadlalla},
keywords = {Data mining, Accounting, Literature review, Framework, Prospective, Retrospective},
abstract = {This paper explores the applications of data mining techniques in accounting and proposes an organizing framework for these applications. A large body of literature reported on specific uses of the important data mining paradigm in accounting, but research that takes a holistic view of these uses is lacking. To organize the literature on the applications of data mining in accounting, we create a framework that combines the two well-known accounting reporting perspectives (retrospection and prospection), and the three well-accepted goals of data mining (description, prediction, and prescription). The framework encapsulates a taxonomy of four categories (retrospective-descriptive, retrospective-prescriptive, prospective-prescriptive, and prospective-predictive) of data mining applications in accounting. The proposed framework revealed that the area of accounting that benefited the most from data mining is assurance and compliance, including fraud detection, business health and forensic accounting. The clear gaps seem to be in the two prescriptive application categories (retrospective-prescriptive and prospective-prescriptive), indicating opportunities for benefiting from data mining in these application categories. The framework presents a holistic view of the literature and systematically organizes it in a structurally logical and thematically coherent manner.}
}
@article{KACHA20182789,
title = {Clinical Study Designs and Sources of Error in Medical Research},
journal = {Journal of Cardiothoracic and Vascular Anesthesia},
volume = {32},
number = {6},
pages = {2789-2801},
year = {2018},
issn = {1053-0770},
doi = {https://doi.org/10.1053/j.jvca.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1053077018301095},
author = {Aalok K. Kacha and Sarah L. Nizamuddin and Junaid Nizamuddin and Harish Ramakrishna and Sajid S. Shahul},
keywords = {error, bias, clinical research, trial design, study design, methodology, systematic review, meta-analysis, evidence-based medicine, Bayesian statistics, group sequential design, trial sequential analysis, data reporting}
}
@article{LI201751,
title = {Designing utilization-based spatial healthcare accessibility decision support systems: A case of a regional health plan},
journal = {Decision Support Systems},
volume = {99},
pages = {51-63},
year = {2017},
note = {Location Analytics and Decision Support},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167923617300891},
author = {Yan Li and Au Vo and Manjit Randhawa and Genia Fick},
keywords = {Two-step floating catchment area, Healthcare access, Spatial analytics, Spatial decision support, The Behavioral Model of Health Services Use},
abstract = {In the U.S., myriad healthcare reforms have begun to show some positive effects on enabling “potential access”. One facet of healthcare access, “having access”, which is the availability and accessibility of health services for the surrounding populations, has not been adequately addressed. Research regarding “having access” is presently championed by a family of methods called Floating Catchment Area (FCA). However, existing scholarship is limited in integrating non-spatial factors within the FCA methods. In this research, we propose a novel utilization-based framework as the first attempt to adopt the Behavioral Model of Health Services Use as a theoretical lens to integrate non-spatial factors in spatial healthcare accessibility research. The framework employs a unique approach to derive categorical and factor weights for different population subgroup's healthcare needs using predictive analytics. The proposed framework is evaluated using a case study of a regional health plan. A Spatial Decision Support System (SDSS) instantiates the framework and enables decision makers to explore physician shortage areas. The SDSS validates the practicality of the proposed utilization-based framework and subsequently allows other FCA methods to be implemented in real-world applications.}
}
@article{RAMONYCAJAL2017484,
title = {Cancer as an ecomolecular disease and a neoplastic consortium},
journal = {Biochimica et Biophysica Acta (BBA) - Reviews on Cancer},
volume = {1868},
number = {2},
pages = {484-499},
year = {2017},
issn = {0304-419X},
doi = {https://doi.org/10.1016/j.bbcan.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0304419X17300525},
author = {Santiago {Ramón y Cajal} and Claudia Capdevila and Javier Hernandez-Losa and Leticia {De Mattos-Arruda} and Abhishek Ghosh and Julie Lorent and Ola Larsson and Trond Aasen and Lynne-Marie Postovit and Ivan Topisirovic},
keywords = {Cancer, Ecomolecular, Consortium, Paradigms, Systems biology, Heterogeneity},
abstract = {Current anticancer paradigms largely target driver mutations considered integral for cancer cell survival and tumor progression. Although initially successful, many of these strategies are unable to overcome the tremendous heterogeneity that characterizes advanced tumors, resulting in the emergence of resistant disease. Cancer is a rapidly evolving, multifactorial disease that accumulates numerous genetic and epigenetic alterations. This results in wide phenotypic and molecular heterogeneity within the tumor, the complexity of which is further amplified through specific interactions between cancer cells and the tumor microenvironment. In this context, cancer may be perceived as an “ecomolecular” disease that involves cooperation between several neoplastic clones and their interactions with immune cells, stromal fibroblasts, and other cell types present in the microenvironment. This collaboration is mediated by a variety of secreted factors. Cancer is therefore analogous to complex ecosystems such as microbial consortia. In the present article, we comment on the current paradigms and perspectives guiding the development of cancer diagnostics and therapeutics and the potential application of systems biology to untangle the complexity of neoplasia. In our opinion, conceptualization of neoplasia as an ecomolecular disease is warranted. Advances in knowledge pertinent to the complexity and dynamics of interactions within the cancer ecosystem are likely to improve understanding of tumor etiology, pathogenesis, and progression. This knowledge is anticipated to facilitate the design of new and more effective therapeutic approaches that target the tumor ecosystem in its entirety.}
}
@article{RASMY201811,
title = {A study of generalizability of recurrent neural network-based predictive models for heart failure onset risk using a large and heterogeneous EHR data set},
journal = {Journal of Biomedical Informatics},
volume = {84},
pages = {11-16},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418301175},
author = {Laila Rasmy and Yonghui Wu and Ningtao Wang and Xin Geng and W. Jim Zheng and Fei Wang and Hulin Wu and Hua Xu and Degui Zhi},
keywords = {EHR, Deep learning, Predictive modeling, RNN},
abstract = {Recently, recurrent neural networks (RNNs) have been applied in predicting disease onset risks with Electronic Health Record (EHR) data. While these models demonstrated promising results on relatively small data sets, the generalizability and transferability of those models and its applicability to different patient populations across hospitals have not been evaluated. In this study, we evaluated an RNN model, RETAIN, over Cerner Health Facts® EMR data, for heart failure onset risk prediction. Our data set included over 150,000 heart failure patients and over 1,000,000 controls from nearly 400 hospitals. Convincingly, RETAIN achieved an AUC of 82% in comparison to an AUC of 79% for logistic regression, demonstrating the power of more expressive deep learning models for EHR predictive modeling. The prediction performance fluctuated across different patient groups and varied from hospital to hospital. Also, we trained RETAIN models on individual hospitals and found that the model can be applied to other hospitals with only about 3.6% of reduction of AUC. Our results demonstrated the capability of RNN for predictive modeling with large and heterogeneous EHR data, and pave the road for future improvements.}
}
@article{CHAN201887,
title = {Dynamic soft sensors with active forward-update learning for selection of useful data from historical big database},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {175},
pages = {87-103},
year = {2018},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2018.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0169743917303088},
author = {Lester Lik Teck Chan and Qing-Yang Wu and Junghui Chen},
keywords = {Active learning, Forward-update, Large database, Latent variable model, Model uncertainty, Soft sensor},
abstract = {Conventional static soft sensor is incapable of handling the dynamic of processes. With abundance of data, the problem of variable correlations and a large number of samples are encountered; moreover, the quality of the data for the construction of the soft sensors can be crucial for performance. An active learning strategy based on a latent variable model (LVM) to select representative data for efficient development of the dynamic soft sensor model is proposed. The uncertainty information for data selection is provided by the Gaussian process (GP) model. The developed LVM with the auxiliary GP model can handle the process dynamic. An active forward-update scheme which can update the soft sensor model in advance is proposed to reflect the current status of the process and improve the prediction performance without waiting for the quality measurements. Two case studies are done to demonstrate the features and the applicability of the proposed method.}
}
@article{POLIMENI20181,
title = {Neuroimaging with ultra-high field MRI: Present and future},
journal = {NeuroImage},
volume = {168},
pages = {1-6},
year = {2018},
note = {Neuroimaging with Ultra-high Field MRI: Present and Future},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.01.072},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918300727},
author = {Jonathan R. Polimeni and Kâmil Uludağ}
}
@article{LIU2017751,
title = {Trajectories of Emergent Central Sleep Apnea During CPAP Therapy},
journal = {Chest},
volume = {152},
number = {4},
pages = {751-760},
year = {2017},
issn = {0012-3692},
doi = {https://doi.org/10.1016/j.chest.2017.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0012369217310796},
author = {Dongquan Liu and Jeff Armitstead and Adam Benjafield and Shiyun Shao and Atul Malhotra and Peter A. Cistulli and Jean-Louis Pepin and Holger Woehrle},
keywords = {central sleep apnea, CPAP, telemonitoring},
abstract = {Background
The emergence of central sleep apnea (CSA) during positive airway pressure (PAP) therapy has been observed clinically in approximately 10% of obstructive sleep apnea titration studies. This study assessed a PAP database to investigate trajectories of treatment-emergent CSA during continuous PAP (CPAP) therapy.
Methods
U.S. telemonitoring device data were analyzed for the presence/absence of emergent CSA at baseline (week 1) and week 13. Defined groups were as follows: obstructive sleep apnea (average central apnea index [CAI] < 5/h in week 1, < 5/h in week 13); transient CSA (CAI ≥ 5/h in week 1, < 5/h in week 13); persistent CSA (CAI ≥ 5/h in week 1, ≥ 5/h in week 13); emergent CSA (CAI < 5/h in week 1, ≥ 5/h in week 13).
Results
Patients (133,006) used CPAP for ≥ 90 days and had ≥ 1 day with use of ≥ 1 h in week 1 and week 13. The proportion of patients with CSA in week 1 or week 13 was 3.5%; of these, CSA was transient, persistent, or emergent in 55.1%, 25.2%, and 19.7%, respectively. Patients with vs without treatment-emergent CSA were older, had higher residual apnea-hypopnea index and CAI at week 13, and more leaks (all P < .001). Patients with any treatment-emergent CSA were at higher risk of therapy termination vs those who did not develop CSA (all P < .001).
Conclusions
Our study identified a variety of CSA trajectories during CPAP therapy, identifying several different clinical phenotypes. Identification of treatment-emergent CSA by telemonitoring could facilitate early intervention to reduce the risk of therapy discontinuation and shift to more efficient ventilator modalities.}
}
@article{CURCIN20171,
title = {Templates as a method for implementing data provenance in decision support systems},
journal = {Journal of Biomedical Informatics},
volume = {65},
pages = {1-21},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2016.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S1532046416301599},
author = {Vasa Curcin and Elliot Fairweather and Roxana Danger and Derek Corrigan},
keywords = {D2.1 (Software Engineering) Requirements/specification J.3 (Life and Medical Sciences): Health data provenance, Model-driven architectures, Decision support systems},
abstract = {Decision support systems are used as a method of promoting consistent guideline-based diagnosis supporting clinical reasoning at point of care. However, despite the availability of numerous commercial products, the wider acceptance of these systems has been hampered by concerns about diagnostic performance and a perceived lack of transparency in the process of generating clinical recommendations. This resonates with the Learning Health System paradigm that promotes data-driven medicine relying on routine data capture and transformation, which also stresses the need for trust in an evidence-based system. Data provenance is a way of automatically capturing the trace of a research task and its resulting data, thereby facilitating trust and the principles of reproducible research. While computational domains have started to embrace this technology through provenance-enabled execution middlewares, traditionally non-computational disciplines, such as medical research, that do not rely on a single software platform, are still struggling with its adoption. In order to address these issues, we introduce provenance templates – abstract provenance fragments representing meaningful domain actions. Templates can be used to generate a model-driven service interface for domain software tools to routinely capture the provenance of their data and tasks. This paper specifies the requirements for a Decision Support tool based on the Learning Health System, introduces the theoretical model for provenance templates and demonstrates the resulting architecture. Our methods were tested and validated on the provenance infrastructure for a Diagnostic Decision Support System that was developed as part of the EU FP7 TRANSFoRm project.}
}
@article{GALPERN201837,
title = {Assessing urban connectivity using volunteered mobile phone GPS locations},
journal = {Applied Geography},
volume = {93},
pages = {37-46},
year = {2018},
issn = {0143-6228},
doi = {https://doi.org/10.1016/j.apgeog.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0143622817310731},
author = {Paul Galpern and Andrew Ladle and Francisco {Alaniz Uribe} and Beverly Sandalack and Patricia Doyle-Baker},
abstract = {The mode, timing and frequency of travel by residents of urban neighbourhoods is affected by sociodemographic and environmental factors. Among these, the layout and connectivity of the street network is amenable to design by urban planners and developers. Here we focus on street connectivity as a variable influencing mobility in cities by examining GPS-enabled mobile phone location data volunteered by a group of 127 university students (234,709 h of behavioural observation over a six-year period). We used a Bayesian beta regression framework to model the proportion of time spent inactive, walking and travelling at vehicle speeds, relative to street connectivity and other environmental attributes measured within a radius of home. Results indicated that lower street connectivity, measured using a simple measure we call network warp, is associated with more time spent inactive and more time travelling at vehicle speeds by students. The proportion of time spent walking was higher in areas with more street connectivity, and for student homes that were closer to campus. Our study confirms the importance of street connectivity as a factor influencing the walkability of neighborhoods and the selection of passive forms of transport, and builds on earlier studies of this relationship by incorporating longitudinal data with high spatial and temporal resolution. We conclude that crowdsourcing data that is recorded automatically by GPS-enabled mobile phones can provide an accessible and flexible evidence base to support the design of urban areas.}
}
@article{SOMERVILLE2018456,
title = {The Lifespan Human Connectome Project in Development: A large-scale study of brain connectivity development in 5–21 year olds},
journal = {NeuroImage},
volume = {183},
pages = {456-468},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.08.050},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918307481},
author = {Leah H. Somerville and Susan Y. Bookheimer and Randy L. Buckner and Gregory C. Burgess and Sandra W. Curtiss and Mirella Dapretto and Jennifer Stine Elam and Michael S. Gaffrey and Michael P. Harms and Cynthia Hodge and Sridhar Kandala and Erik K. Kastman and Thomas E. Nichols and Bradley L. Schlaggar and Stephen M. Smith and Kathleen M. Thomas and Essa Yacoub and David C. {Van Essen} and Deanna M. Barch},
keywords = {Neurodevelopment, Brain, MRI, Connectivity, Connectome, Network, Child, Adolescent, Development},
abstract = {Recent technological and analytical progress in brain imaging has enabled the examination of brain organization and connectivity at unprecedented levels of detail. The Human Connectome Project in Development (HCP-D) is exploiting these tools to chart developmental changes in brain connectivity. When complete, the HCP-D will comprise approximately ∼1750 open access datasets from 1300 + healthy human participants, ages 5–21 years, acquired at four sites across the USA. The participants are from diverse geographical, ethnic, and socioeconomic backgrounds. While most participants are tested once, others take part in a three-wave longitudinal component focused on the pubertal period (ages 9–17 years). Brain imaging sessions are acquired on a 3 T Siemens Prisma platform and include structural, functional (resting state and task-based), diffusion, and perfusion imaging, physiological monitoring, and a battery of cognitive tasks and self-reports. For minors, parents additionally complete a battery of instruments to characterize cognitive and emotional development, and environmental variables relevant to development. Participants provide biological samples of blood, saliva, and hair, enabling assays of pubertal hormones, health markers, and banked DNA samples. This paper outlines the overarching aims of the project, the approach taken to acquire maximally informative data while minimizing participant burden, preliminary analyses, and discussion of the intended uses and limitations of the dataset.}
}
@article{ZHANG2018914,
title = {Social media security and trustworthiness: Overview and new direction},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {914-925},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16303879},
author = {Zhiyong Zhang and Brij B. Gupta},
keywords = {Social media networks, Security, Trustworthiness, Measurement, Crowd computing},
abstract = {The emerging social media with inherent capabilities seems to be gaining edge over comprehensiveness, diversity and wisdom, nevertheless its security and trustworthiness issues have also become increasingly serious, which need to be addressed urgently. The available studies mainly aim at both social media content and user security, including model, protocol, mechanism and algorithm. Unfortunately, there is a lack of investigating on effective and efficient evaluations and measurements for security and trustworthiness of various social media tools, platforms and applications, thus has effect on their further improvement and evolution. To address the challenge, this paper firstly made a survey on the state-of-the-art of social media networks security and trustworthiness particularly for the increasingly growing sophistication and variety of attacks as well as related intelligence applications. And then, we highlighted a new direction on evaluating and measuring those fundamental and underlying platforms, meanwhile proposing a hierarchical architecture for crowd evaluations based on signaling theory and crowd computing, which is essential for social media ecosystem. Finally, we conclude our work with several open issues and cutting-edge challenges.}
}
@article{FAN20181123,
title = {Analytical investigation of autoencoder-based methods for unsupervised anomaly detection in building energy data},
journal = {Applied Energy},
volume = {211},
pages = {1123-1135},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306261917317166},
author = {Cheng Fan and Fu Xiao and Yang Zhao and Jiayuan Wang},
keywords = {Autoencoder, Unsupervised data analytics, Anomaly detection, Building operational performance, Building energy management},
abstract = {Practical building operations usually deviate from the designed building operational performance due to the wide existence of operating faults and improper control strategies. Great energy saving potential can be realized if inefficient or faulty operations are detected and amended in time. The vast amounts of building operational data collected by the Building Automation System have made it feasible to develop data-driven approaches to anomaly detection. Compared with supervised analytics, unsupervised anomaly detection is more practical in analyzing real-world building operational data, as anomaly labels are typically not available. Autoencoder is a very powerful method for the unsupervised learning of high-level data representations. Recent development in deep learning has endowed autoencoders with even greater capability in analyzing complex, high-dimensional and large-scale data. This study investigates the potential of autoencoders in detecting anomalies in building energy data. An autoencoder-based ensemble method is proposed while providing a comprehensive comparison on different autoencoder types and training schemes. Considering the unique learning mechanism of autoencoders, specific methods have been designed to evaluate the autoencoder performance. The research results can be used as foundation for building professionals to develop advanced tools for anomaly detection and performance benchmarking.}
}
@article{ZHENG2017267,
title = {Understanding the tourist mobility using GPS: Where is the next place?},
journal = {Tourism Management},
volume = {59},
pages = {267-280},
year = {2017},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2016.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0261517716301509},
author = {Weimin Zheng and Xiaoting Huang and Yuan Li},
keywords = {Tourist mobility, Prediction, GPS technology, Data mining},
abstract = {Understanding the mobility of tourists plays a fundamental role in the administration and design of tourist destinations, planning of on-site movement and marketing of attractions. In this paper, we focus on how to accurately predict the tourist's next location within a given attraction. A heuristic method based on data mining is proposed, which considers the trajectory of a focal tourist and the movements of past visitors. To evaluate the performance of the proposed method, a case study was conducted at the Summer Palace in Beijing, China. We collected movement information from tourists using GPS tracking technology, and the results of an independent samples t-test indicate that the proposed method indeed performs significantly better than existing methods. We further explore the potential applications of the proposed method. Our results significantly contribute to enhancing the level of personalized location-based service, tourist attraction administration, and real-time crowd control.}
}
@article{BAI201766,
title = {Toward a systematic exploration of nano-bio interactions},
journal = {Toxicology and Applied Pharmacology},
volume = {323},
pages = {66-73},
year = {2017},
issn = {0041-008X},
doi = {https://doi.org/10.1016/j.taap.2017.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0041008X17301151},
author = {Xue Bai and Fang Liu and Yin Liu and Cong Li and Shenqing Wang and Hongyu Zhou and Wenyi Wang and Hao Zhu and David A. Winkler and Bing Yan},
abstract = {Many studies of nanomaterials make non-systematic alterations of nanoparticle physicochemical properties. Given the immense size of the property space for nanomaterials, such approaches are not very useful in elucidating fundamental relationships between inherent physicochemical properties of these materials and their interactions with, and effects on, biological systems. Data driven artificial intelligence methods such as machine learning algorithms have proven highly effective in generating models with good predictivity and some degree of interpretability. They can provide a viable method of reducing or eliminating animal testing. However, careful experimental design with the modelling of the results in mind is a proven and efficient way of exploring large materials spaces. This approach, coupled with high speed automated experimental synthesis and characterization technologies now appearing, is the fastest route to developing models that regulatory bodies may find useful. We advocate greatly increased focus on systematic modification of physicochemical properties of nanoparticles combined with comprehensive biological evaluation and computational analysis. This is essential to obtain better mechanistic understanding of nano-bio interactions, and to derive quantitatively predictive and robust models for the properties of nanomaterials that have useful domains of applicability.}
}
@article{BRUNO2018256,
title = {Historic Building Information Modelling: performance assessment for diagnosis-aided information modelling and management},
journal = {Automation in Construction},
volume = {86},
pages = {256-276},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517301164},
author = {Silvana Bruno and Mariella {De Fino} and Fabio Fatiguso},
keywords = {BIM (Building Information Modelling), Historic building, HBIM (Historic Building Information Modelling), FM (Facility Management), Refurbishment, Energy retrofitting, Structural assessment, Diagnostics, Diagnosis-aided HBIMM (DA-HBIMM)},
abstract = {Building Information Modelling, new paradigm of digital design and management, shows great potential for the refurbishment process, as it represents a possible way out of criticalities that occur in documentation and preservation of existing assets, if connected to cognitive automation. The combination of BIM with automation systems improves the quality control during diagnosis, design and work execution, and the labour savings, which is particularly relevant for rapid intervention in case of hazardous conditions. Therefore, the paper is going to address a methodological discussion concerning complete “as-built” parametric models of historical buildings, supporting the design of refurbishment and conservation interventions. Although some reviews of the state of the art exist on the topic of Historic Building Information Modelling, the present research introduces a different perspective on HBIM modelling, with diagnosis and performance assessment as key-aspects, in terms of automating performance assessment. Specifically, from the data collection of contributions regarding HBIM/BIM, diagnostics and monitoring on existing buildings and infrastructures, a critical review by selected criteria is developed. Nevertheless, general methods and tools for information management and exchange tasks in BIM are briefly described as well, since they are considered useful for future developments of HBIM approach. The core of the critical analysis is focused on the scientific and technical relations among HBIM models, diagnosis and performance assessment features. In addition, the review identifies specific activities and relative tools and methods for knowledge acquisition and semantic enrichment. Finally, gaps in knowledge of the current literature are outlined and discussed, with specific focus on performance assessment in HBIM. In this regard, a new methodology toward Diagnosis-Aided Historic Building Information Modelling and Management (DA-HBIMM) is proposed as a framework to be developed in order to address smart knowledge acquisition, collection and notification of assessed performances and eventual risks, by cognitive automation and artificial intelligence, in the near future.}
}
@article{MONSEN2017e75,
title = {Empirical evaluation of the changes in public health nursing interventions after the implementation of an evidence-based family home visiting guideline},
journal = {Kontakt},
volume = {19},
number = {2},
pages = {e75-e85},
year = {2017},
issn = {1212-4117},
doi = {https://doi.org/10.1016/j.kontakt.2017.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1212411717300168},
author = {Karen A. Monsen and Sadie M. Swenson and Lisa V. Klotzbach and Michelle A. Mathiason and Karen E. Johnson},
keywords = {Family home visiting, Omaha System, Intervention, Guideline, Evaluation},
abstract = {The objective of this quality evaluation was to evaluate the changes in public health nursing (PHN) interventions after the implementation of an evidence-based family home visiting (EB-FHV) guideline encoded using the Omaha System.
Design and sample
This quality improvement evaluation was conducted using a secondary dataset of 27,910 PHN family home visiting interventions from visits to 129 adult clients enrolled in EB-FHV programs in a Midwestern PHN agency. The interventions were documented 12 months before and 14 months after EB-FHV Guideline implementation. The EB-FHV consisted of 94 PHN interventions for 10 Omaha System problems, with electronic health record (EHR) data generated by PHNs during routine clinical documentation. Standard descriptive and inferential statistics were employed in the analysis.
Measures
The Omaha System was used to compare PHN practice before and after the guideline implementation.
Results
Documentation patterns revealed that PHNs tailored interventions while also shifting toward the use of the EB-FHV guideline interventions. Ten EB-FHV problems accounted for 96.3% of interventions documented before and 98.5% of interventions documented after implementation. The proportion of interventions before and after EB-FHV by problem differed significantly for all problems except Substance use. Fewer interventions were provided after EB-FHV for the primary problems of Pregnancy and Postpartum, with a shift to more interventions for Caretaking/parenting.
Conclusion
The PHN documentation after guideline implementation demonstrated adherence to the EB-FHV guideline, while tailoring the evidence-based interventions differentially by problem. Further research is needed to extend this quality improvement approach to other guidelines and populations.}
}
@article{ESQUIVEL201836,
title = {Predictability of seasonal precipitation across major crop growing areas in Colombia},
journal = {Climate Services},
volume = {12},
pages = {36-47},
year = {2018},
issn = {2405-8807},
doi = {https://doi.org/10.1016/j.cliser.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405880718300177},
author = {Alejandra Esquivel and Lizeth Llanos-Herrera and Diego Agudelo and Steven D. Prager and Katia Fernandes and Alexander Rojas and Jhon Jairo Valencia and Julian Ramirez-Villegas},
keywords = {Seasonal forecast, Predictability, Sea surface temperatures, Climate services, Colombian agriculture},
abstract = {Agriculture is one of the sectors that has greatly benefitted from the establishment of climate services. In Colombia, interannual climate variability can disrupt agricultural production, lower farmers' incomes and increase market prices. Increasing demand thus exists for agro-climatic services in the country. Fulfilling such demand requires robust and consistent approaches for seasonal climate forecasting. Here, we assess seasonal precipitation predictability and forecast skill at agriculturally-relevant timescales for five departments that represent key growing areas of major staple crops (rice, maize, and beans). Analyses use Canonical Correlation Analysis, with both observed SSTs and modeled (NCEP-CFSv2) SSTs, as well as with CFSv2 predicted precipitation fields (through a Model-Output-Statistics analysis). Some 74.4% of the forecast situations analyzed (5 departments ∗ 4 seasons ∗ 3 predictors ∗ 3 lead times) showed correlation-based goodness index (Kendall’s tau, τ-) values above 0.1, 38.8% above 0.2, and 18.8% above 0.3. Predictability was limited towards eastern Colombia, and during wet periods of the year in the Inter-Andean Valleys. Importantly, results were consistent between ERSST and CFSv2-driven forecasts, implying that both can offer valuable outlooks for Colombia. While our study is a first important step toward the establishment of a sustainable and successful climate service for agriculture in Colombia, further work is required to (1) improve seasonal forecast skill; (2) link seasonal forecasts to agricultural modelling applications; (3) design appropriate delivery means; and (4) establish stakeholder-driven processes that allow two-way communication between forecast issuing institutions (e.g. IDEAM–Colombian Meteorological Service) and famers’ organizations and farming communities.}
}
@article{ZHANG2018126,
title = {Residual compensation extreme learning machine for regression},
journal = {Neurocomputing},
volume = {311},
pages = {126-136},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.05.057},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218306428},
author = {Jie Zhang and Wendong Xiao and Yanjiao Li and Sen Zhang},
keywords = {Extreme learning machine, Regression problem, Residual compensation extreme learning machine, Device-free localization, Gas utilization ratio prediction},
abstract = {Extreme learning machine (ELM) was proposed for training single hidden layer feedforward neural networks (SLFNs), and can provide an efficient learning solution for regression problem. However, the prediction error of ELM is unavoidable due to its limited modeling capability, and the nonlinear and stochastic nature of the regression problem. In this paper, a novel ELM, residual compensation ELM (RC-ELM), is proposed for regression problem by employing a multilayer structure with the baseline layer for building the feature mapping between the input and the output, and the other layers for residual compensation layer by layer iteratively. Two real world applications, device-free localization (DFL) and gas utilization ratio (GUR) prediction in blast furnace, are used for experimental testing of the proposed RC-ELM. Experimental results show that RC-ELM has better generalization performance and robustness than other machine learning approaches, including the classic ELM, weighted K-nearest neighbor (WKNN), support vector machine (SVM), and back propagation neural network (BPNN).}
}
@incollection{FRYMAN2017177,
title = {Chapter 7 - Playbook Deployment},
editor = {Lowell Fryman and Gregory Lampshire and Dan Meers},
booktitle = {The Data and Analytics Playbook},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {177-197},
year = {2017},
isbn = {978-0-12-802307-5},
doi = {https://doi.org/10.1016/B978-0-12-802307-5.00007-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023075000071},
author = {Lowell Fryman and Gregory Lampshire and Dan Meers},
keywords = {Data capabilities, Data cycle, Data governance, Deployment, Execution, Organizational models},
abstract = {This chapter focuses on getting the work done—real execution. Deployment is about deploying the Playbook methods, practices, and approaches to accomplish multiple goals. The first goal is always the improvement of enterprise or shared data. The second, which is tightly linked to the first, is the development and expansion of capabilities related to improving and governing data. The third goal is the overall course of progress that can be made across the scope of enterprise data. As we avoid the methods and build capabilities, more and more of the critical enterprise data is governed and improved in measurable ways.}
}
@article{CHENG2018186,
title = {Short-term traffic forecasting: An adaptive ST-KNN model that considers spatial heterogeneity},
journal = {Computers, Environment and Urban Systems},
volume = {71},
pages = {186-198},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518300140},
author = {Shifen Cheng and Feng Lu and Peng Peng and Sheng Wu},
keywords = {Short-term traffic forecasting, Adaptive spatiotemporal k-nearest neighbor model, Spatial heterogeneity, Traffic patterns},
abstract = {Accurate and robust short-term traffic forecasting is a critical issue in intelligent transportation systems and real-time traffic-related applications. Existing short-term traffic forecasting approaches adopt fixed model structures and assume traffic correlations between adjacent road segments within assigned time periods. Due to the inherent spatial heterogeneity of city traffic, it is difficult for these approaches to obtain stable and satisfying results. To overcome the problems of fixed model structures and quantitatively unclear spatiotemporal dependency relationships, this paper proposes an adaptive spatiotemporal k-nearest neighbor model (adaptive-STKNN) for short-term traffic forecasting. It comprehensively considers the spatial heterogeneity of city traffic based on adaptive spatial neighbors, time windows, spatiotemporal weights and other parameters. First, for each road segment, we determine the sizes of spatial neighbors and the lengths of time windows for traffic influence using cross-correlation and autocorrelation functions, respectively. Second, adaptive spatiotemporal weights are introduced into the distance functions to optimize the candidate neighbor search mechanism. Next, we establish adaptive spatiotemporal parameters to reflect continuous changes in traffic conditions, including the number of candidate neighbors and the weight allocation parameter in the predictive function. Finally, we evaluate the adaptive-STKNN model using two vehicular speed datasets collected on expressways in California, U.S.A., and on city roads in Beijing, China. Four traditional prediction models are compared with the adaptive-STKNN model in terms of forecasting accuracy and generalization ability. The results demonstrate that the adaptive-STKNN model outperforms those models during all time periods and especially the peak period. In addition, the results also show the generalization ability of the adaptive-STKNN model.}
}
@article{SUN201854,
title = {Local spatial obesity analysis and estimation using online social network sensors},
journal = {Journal of Biomedical Informatics},
volume = {83},
pages = {54-62},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300522},
author = {Qindong Sun and Nan Wang and Shancang Li and Hongyi Zhou},
keywords = {Online social networks, Internet of things (IoT), Obesity, Health informatics, Bioinformatics, Public health},
abstract = {Recently, the online social networks (OSNs) have received considerable attentions as a revolutionary platform to offer users massive social interaction among users that enables users to be more involved in their own healthcare. The OSNs have also promoted increasing interests in the generation of analytical, data models in health informatics. This paper aims at developing an obesity identification, analysis, and estimation model, in which each individual user is regarded as an online social network ‘sensor’ that can provide valuable health information. The OSN-based obesity analytic model requires each sensor node in an OSN to provide associated features, including dietary habit, physical activity, integral/incidental emotions, and self-consciousness. Based on the detailed measurements on the correlation of obesity and proposed features, the OSN obesity analytic model is able to estimate the obesity rate in certain urban areas and the experimental results demonstrate a high success estimation rate. The measurements and estimation experimental findings created by the proposed obesity analytic model show that the online social networks could be used in analyzing the local spatial obesity problems effectively.}
}
@article{LAMBIN2017131,
title = {Decision support systems for personalized and participative radiation oncology},
journal = {Advanced Drug Delivery Reviews},
volume = {109},
pages = {131-153},
year = {2017},
note = {Radiotherapy for cancer: present and future},
issn = {0169-409X},
doi = {https://doi.org/10.1016/j.addr.2016.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169409X16300084},
author = {Philippe Lambin and Jaap Zindler and Ben G.L. Vanneste and Lien Van {De Voorde} and Daniëlle Eekers and Inge Compter and Kranthi Marella Panth and Jurgen Peerlings and Ruben T.H.M. Larue and Timo M. Deist and Arthur Jochems and Tim Lustberg and Johan {van Soest} and Evelyn E.C. {de Jong} and Aniek J.G. Even and Bart Reymen and Nicolle Rekers and Marike {van Gisbergen} and Erik Roelofs and Sara Carvalho and Ralph T.H. Leijenaar and Catharina M.L. Zegers and Maria Jacobs and Janita {van Timmeren} and Patricia Brouwers and Jonathan A. Lal and Ludwig Dubois and Ala Yaromina and Evert Jan {Van Limbergen} and Maaike Berbee and Wouter {van Elmpt} and Cary Oberije and Bram Ramaekers and Andre Dekker and Liesbeth J. Boersma and Frank Hoebers and Kim M. Smits and Adriana J. Berlanga and Sean Walsh},
keywords = {Radiotherapy, Decision support systems, Prediction models, Shared decision making},
abstract = {A paradigm shift from current population based medicine to personalized and participative medicine is underway. This transition is being supported by the development of clinical decision support systems based on prediction models of treatment outcome. In radiation oncology, these models ‘learn’ using advanced and innovative information technologies (ideally in a distributed fashion — please watch the animation: http://youtu.be/ZDJFOxpwqEA) from all available/appropriate medical data (clinical, treatment, imaging, biological/genetic, etc.) to achieve the highest possible accuracy with respect to prediction of tumor response and normal tissue toxicity. In this position paper, we deliver an overview of the factors that are associated with outcome in radiation oncology and discuss the methodology behind the development of accurate prediction models, which is a multi-faceted process. Subsequent to initial development/validation and clinical introduction, decision support systems should be constantly re-evaluated (through quality assurance procedures) in different patient datasets in order to refine and re-optimize the models, ensuring the continuous utility of the models. In the reasonably near future, decision support systems will be fully integrated within the clinic, with data and knowledge being shared in a standardized, dynamic, and potentially global manner enabling truly personalized and participative medicine.}
}
@article{HAAG2018850,
title = {A Framework for Self-Evaluation and Increase of Resource-Efficient Production through Digitalization},
journal = {Procedia CIRP},
volume = {72},
pages = {850-855},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.304},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118304852},
author = {Sebastian Haag and Christoph Bauerdick and Alessio Campitelli and Reiner Anderl and Eberhard Abele and Liselotte Schebek},
keywords = {Digital Transformation, Industrie 4.0, Resource Efficiency, Data Analysis},
abstract = {Modern sensor technology and decreasing hardware costs enable the collection of a wide range of data. Nonetheless, the collection of data itself does not generate value. The collected data must be processed and analysed. Many small and medium-sized enterprises already collect a number of data. However, there is no definite strategy, which data needs to be collected in order to acquire relevant insights into processes. The enormous potential of data analysis and the current lack of its implementation caused the development of this framework. It will assist enterprises to evaluate their own level of digitalization to assess resource use.}
}
@incollection{PATRINOS2017353,
title = {Chapter 20 - Genomic Databases: Emerging Tools for Molecular Diagnostics},
editor = {George P. Patrinos},
booktitle = {Molecular Diagnostics (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {353-369},
year = {2017},
isbn = {978-0-12-802971-8},
doi = {https://doi.org/10.1016/B978-0-12-802971-8.00020-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128029718000201},
author = {G.P. Patrinos and T. Katsila and E. Viennas and G. Tzimas},
keywords = {Database management systems, Genomic databases, Genomic variants, Locus-specific databases, Microattribution, National/ethnic mutation databases},
abstract = {Genome informatics deals with informatics tools used in molecular biology, and it is an important scientific discipline that emerged in the postgenomic era from developments in the field of human genomics. Advances in the understanding of the genetic etiology of human disorders, coupled with advances in technology, have led to the identification of numerous genomic variants. These dictate the organization of this knowledge and these alterations in structured repositories that could eventually be useful not only for molecular diagnosis but also for clinicians and researchers. Genetic or mutation databases are referred to as online repositories of genomic variants, mainly described for one or more genes or specifically for a population or ethnic group, aiming to facilitate diagnosis at the DNA level and to correlate genomic variants with specific phenotypic patterns and clinical features. In this chapter we will summarize the key features of the main types of genetic databases that are frequently used in molecular diagnostics, namely locus-specific and national/ethnic genetic databases. In particular, the main activities relating to these genetic database types will be highlighted to describe the existing and emerging database types in this domain and emphasize their potential applications in modern medical genetics. We will also critically discuss and touch upon the key elements that are still missing and holding back the field.}
}
@article{GAO201841,
title = {Two novel lncRNAs discovered in human mitochondrial DNA using PacBio full-length transcriptome data},
journal = {Mitochondrion},
volume = {38},
pages = {41-47},
year = {2018},
issn = {1567-7249},
doi = {https://doi.org/10.1016/j.mito.2017.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1567724917301058},
author = {Shan Gao and Xiaoxuan Tian and Hong Chang and Yu Sun and Zhenfeng Wu and Zhi Cheng and Pengzhi Dong and Qiang Zhao and Jishou Ruan and Wenjun Bu},
keywords = {Mitochondrial transcriptome, Full-length transcriptome, lncRNA, psRNA, ltiRNA},
abstract = {In this study, we established a general framework to use PacBio full-length transcriptome sequencing for the investigation of mitochondrial RNAs. As a result, we produced the first full-length human mitochondrial transcriptome using public PacBio data and characterized the human mitochondrial genome with more comprehensive and accurate information. Other results included determination of the H-strand primary transcript, identification of the ND5/ND6AS/tRNAGluAS transcript, discovery of palindrome small RNAs (psRNAs) and construction of the “mitochondrial cleavage” model, etc. These results reported for the first time in this study fundamentally changed annotations of human mitochondrial genome and enriched knowledge in the field of animal mitochondrial studies. The most important finding was two novel long non-coding RNAs (lncRNAs) of MDL1 and MDL1AS exist ubiquitously in animal mitochondrial genomes.}
}
@incollection{KUMUTHINI2018179,
title = {Chapter 9 - Minimum Information Required for Pharmacogenomics Experiments},
editor = {Christophe G. Lambert and Darrol J. Baker and George P. Patrinos},
booktitle = {Human Genome Informatics},
publisher = {Academic Press},
pages = {179-193},
year = {2018},
isbn = {978-0-12-809414-3},
doi = {https://doi.org/10.1016/B978-0-12-809414-3.00009-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094143000097},
author = {J. Kumuthini and L. Zass and Emile R. Chimusa and Melek Chaouch and Collen Masimiremwa},
keywords = {Bioinformatics, Bioinformatics standardization, Data standardization, DMET, FAIR principles, MIDE, Minimum requirement reporting, Pharmacogenomics, Pharmacogenetics, Personalized medicine},
abstract = {Pharmacogenomics studies the impact of genetic variation on drug response. The discipline is crucial in order to improve healthcare worldwide and an important stepping stone toward a future of personalized medicine. Pharmacogenomics generally involves single- to multigene variation investigations, and, although a great deal of progress has been made in the last decades, significant barriers prevent the utilization of pharmacogenomics to its full potential. These barriers include both financial and technical/analytical considerations, which are further complicated by the varied reporting methods employed by pharmacogenomics researchers and clinicians. Therefore, standardizing the manner in which pharmacogenomics investigations are reported can significantly contribute to resolving the existing analytical concerns, and thus, the Minimum Information required for a DMET experiment (MIDE) pharmacogenomics was designed to challenge the reporting variance in experiments employing microarray technology, specifically with regard to pharmacogenomics. The following chapter provides a brief overview of pharmacogenomics research, standardization within the field, and an in-depth look into the MIDE pharmacogenomics standard. Notably, the MIDE standard is adaptable to other pharmacogenomics applications and is available along with several other pharmacogenomics standards on the FAIRsharing (Findable, Accessible, Interoperable, Reproducible Sharing) resource. Standardization across pharmacogenomics applications is crucial in order to enhance the findability, accessibility, interoperability, and reusability of pharmacogenomics studies, ultimately enhancing the quality of such studies and promoting collaboration between independent bodies involved in the science.}
}
@article{MEJRI201746,
title = {Crisis information to support spatial planning in post disaster recovery},
journal = {International Journal of Disaster Risk Reduction},
volume = {22},
pages = {46-61},
year = {2017},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2017.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S2212420916303144},
author = {Ouejdane Mejri and Scira Menoni and Kyla Matias and Negar Aminoltaheri},
keywords = {Crisis information, Spatial planning, Resilience, Knowledge management, Ontologies, Crowdsourcing},
abstract = {In this paper we propose to explore the complex node of post disaster reconstruction, knowledge and data necessary to support spatial planning, and new information technologies. The methodology that is illustrated assumes that post-event damage assessments are useful to verify to what extent hazard and risk assessments that were available to planners to make decisions before the disaster were correct and if they were actually used as a basis for locational and zoning choices. Our contribution is aimed at the creation and design of knowledge bases accounting for the dynamic evolution of disasters. New web based technologies provide the opportunity to collect and analyse dynamic territorial crisis data using crowdsourcing and crowdmapping platforms. The proposed methodology permits to sort and classify a very large set of different types of data generated through the web. Semantic conceptualization using ontologies is performed to identify and select the information produced during the emergency that can support spatial planning in the post disaster reconstruction. The city of Tacloban in the Philippines, affected by the Super Typhoon Haiyan in November 2013 constitutes the test case for applying the methodology that has been developed.}
}
@article{HAVERMANS201796,
title = {Forecasting European trade mark and design filings: An innovative approach including exogenous variables and IP offices' events},
journal = {World Patent Information},
volume = {48},
pages = {96-108},
year = {2017},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S017221901730008X},
author = {Quirinus A. Havermans and Samuel Gabaly and Antonio Hidalgo},
keywords = {Trade mark filings, Design filings, Artificial intelligent forecasting techniques, Advanced data analysis, Exogenous variables},
abstract = {Both national and international Intellectual Property (IP) offices need to adopt and use more reliable and efficient forecasting systems to improve their strategic planning and budgetary outlook. The European Union Intellectual Property Office (EUIPO), through the European Observatory on Infringements of Intellectual Property Rights, has conducted a research to evaluate the forecasting methodologies currently used at IP offices. Novel forecasting approaches for trade marks and designs have also been analysed. This paper discusses the classic forecasting techniques and shows the improved results that are obtained when innovative techniques based on artificial intelligence with the inclusion of exogenous variables are used.}
}
@article{MORAN2017e205,
title = {Development of a model web-based system to support a statewide quality consortium in radiation oncology},
journal = {Practical Radiation Oncology},
volume = {7},
number = {3},
pages = {e205-e213},
year = {2017},
issn = {1879-8500},
doi = {https://doi.org/10.1016/j.prro.2016.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S187985001630220X},
author = {Jean M. Moran and Mary Feng and Lisa A. Benedetti and Robin Marsh and Kent A. Griffith and Martha M. Matuszak and Michael Hess and Matthew McMullen and Jennifer H. Fisher and Teamour Nurushev and Margaret Grubb and Stephen Gardner and Daniel Nielsen and Reshma Jagsi and James A. Hayman and Lori J. Pierce},
abstract = {Purpose
A database in which patient data are compiled allows analytic opportunities for continuous improvements in treatment quality and comparative effectiveness research. We describe the development of a novel, web-based system that supports the collection of complex radiation treatment planning information from centers that use diverse techniques, software, and hardware for radiation oncology care in a statewide quality collaborative, the Michigan Radiation Oncology Quality Consortium (MROQC).
Methods and materials
The MROQC database seeks to enable assessment of physician- and patient-reported outcomes and quality improvement as a function of treatment planning and delivery techniques for breast and lung cancer patients. We created tools to collect anonymized data based on all plans.
Results
The MROQC system representing 24 institutions has been successfully deployed in the state of Michigan. Since 2012, dose-volume histogram and Digital Imaging and Communications in Medicine-radiation therapy plan data and information on simulation, planning, and delivery techniques have been collected. Audits indicated >90% accurate data submission and spurred refinements to data collection methodology.
Conclusions
This model web-based system captures detailed, high-quality radiation therapy dosimetry data along with patient- and physician-reported outcomes and clinical data for a radiation therapy collaborative quality initiative. The collaborative nature of the project has been integral to its success. Our methodology can be applied to setting up analogous consortiums and databases.}
}
@article{HENRIQUES2018638,
title = {Predictive Modelling: Flight Delays and Associated Factors, Hartsfield–Jackson Atlanta International Airport},
journal = {Procedia Computer Science},
volume = {138},
pages = {638-645},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.085},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918317319},
author = {Roberto Henriques and Inês Feiteira},
keywords = {Data Mining, Predictive Analysis, Flight Delays, Hartsfield–Jackson International Airport, Atlanta International Airport},
abstract = {Nowadays, a downside to traveling is the delays that are constantly being advertised to passengers resulting in a decrease in customer satisfaction and causing costs. Consequently, there is a need to anticipate and mitigate the existence of delays helping airlines and airports improving their performance or even take consumer-oriented measures that can undo or attenuate the effect that these delays have on their passengers. This study has as main objective to predict the occurrence of delays in arrivals at the international airport of Hartsfield-Jackson. A Knowledge Discovery Database (KDD) methodology was followed, and several Data Mining techniques were applied. Historical data of the flight and weather, information of the airplane and propagation of the delay were gathered to train the model. To overcome the problem of unbalanced datasets, we applied different sampling techniques. To predict delays in individual flights we used Decision Trees, Random Forest and Multilayer Perceptron. Finally, each model’s performance was evaluated and compared. The best model proved to be the Multilayer Perceptron with 85% of accuracy.}
}
@article{MEINECKE201713,
title = {Series: Pragmatic trials and real world evidence: Paper 8. Data collection and management},
journal = {Journal of Clinical Epidemiology},
volume = {91},
pages = {13-22},
year = {2017},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2017.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S089543561730776X},
author = {Anna-Katharina Meinecke and Paco Welsing and George Kafatos and Des Burke and Sven Trelle and Maria Kubin and Gaelle Nachbaur and Matthias Egger and Mira Zuidgeest},
keywords = {Pragmatic trial, Routinely collected data, Electronic health records, Registries, Claims databases, eCRF},
abstract = {Pragmatic trials can improve our understanding of how treatments will perform in routine practice. In a series of eight papers, the GetReal Consortium has evaluated the challenges in designing and conducting pragmatic trials and their specific methodological, operational, regulatory, and ethical implications. The present final paper of the series discusses the operational and methodological challenges of data collection in pragmatic trials. A more pragmatic data collection needs to balance the delivery of highly accurate and complete data with minimizing the level of interference that data entry and verification induce with clinical practice. Furthermore, it should allow for the involvement of a representative sample of practices, physicians, and patients who prescribe/receive treatment in routine care. This paper discusses challenges that are related to the different methods of data collection and presents potential solutions where possible. No one-size-fits-all recommendation can be given for the collection of data in pragmatic trials, although in general the application of existing routinely used data-collection systems and processes seems to best suit the pragmatic approach. However, data access and privacy, the time points of data collection, the level of detail in the data, and the lack of a clear understanding of the data-collection process were identified as main challenges for the usage of routinely collected data in pragmatic trials. A first step should be to determine to what extent existing health care databases provide the necessary study data and can accommodate data collection and management. When more elaborate or detailed data collection or more structured follow-up is required, data collection in a pragmatic trial will have to be tailor-made, often using a hybrid approach using a dedicated electronic case report form (eCRF). In this case, the eCRF should be kept as simple as possible to reduce the burden for practitioners and minimize influence on routine clinical practice.}
}
@article{DHAKAL2018145,
title = {Using CyclePhilly data to assess wrong-way riding of cyclists in Philadelphia},
journal = {Journal of Safety Research},
volume = {67},
pages = {145-153},
year = {2018},
issn = {0022-4375},
doi = {https://doi.org/10.1016/j.jsr.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022437517307338},
author = {Nirbesh Dhakal and Christopher R. Cherry and Ziwen Ling and Mojdeh Azad},
keywords = {Cycling behavior, Naturalistic data, Smartphones, Wrong-way riding, Bicycle safety},
abstract = {Problem
The increasing use of smartphones and low cost GPS have provided new sources for collecting data and using them to explain travel behavior. This study aims to use data collected from a smartphone application (CyclePhilly) to explain wrong-way riding behavior of cyclists on one-way segments to help better identify the demographic and network factors influencing the wrong-way riding decision making.
Methods
The data used in this study consist of two different sources: (a) Route trips data downloaded from the CyclePhilly Website contained trips detailed up to segment level, collected from May 2014 to April 2016 (12,202 trips by 300 unique users); and (b) Open Street Maps (OSM). Using ArcGIS, we calculate detour routes for each wrong way segment. We then built a mixed logistic regression model to identify the trip and riders' characteristics affecting wrong-way riding behavior. Next, we explore the characteristics of road facilities associated with wrong-way riding behavior.
Results and discussion
Only 2.7% of travel distance is wrong-way, yet 42% of trips include a wrong-way segment. Commute trips have a higher chance of wrong-way riding. The longer the trips also include more wrong-way riding. Segments with higher detour ratios (ratio of distance with a detour to the wrong-way distance) are found to be associated with more wrong-way behavior. Compared to roads with no bike lane, roads with sharrow markings and buffered bike lane discourage wrong way riding.
Practical applications
This study proposes new methods that can be adapted to use naturalistic and probe data and analyze city-wide aberrant riders' behavior. These help planners and engineers choose between various types of bike infrastructure. Wrong-way riding is one application that can be investigated, but probe bicycle datasets provide unprecedented resolution and volume of data that will allow for more sophisticated safety and planning analyses.}
}
@incollection{LOPES201835,
title = {Chapter 3 - Data Processing in Multivariate Analysis of Pharmaceutical Processes},
editor = {Ana Patricia Ferreira and José C. Menezes and Mike Tobyn},
booktitle = {Multivariate Analysis in the Pharmaceutical Industry},
publisher = {Academic Press},
pages = {35-51},
year = {2018},
isbn = {978-0-12-811065-2},
doi = {https://doi.org/10.1016/B978-0-12-811065-2.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128110652000023},
author = {João A. Lopes and Mafalda C. Sarraguça},
keywords = {Pharmaceutical process data, multivariate data analysis, pharmaceutical processes, quality-by-design, signal processing, statistical process control, process analytical technology, continuous manufacturing},
abstract = {With the recent guidelines promoted by the major health authorities, combined in the International Conference on Harmonization Guidelines Q8, Q9, and Q10, regulating the pharmaceutical development, risk management, and quality management systems, respectively, pharmaceutical process data and their appropriate handling become more relevant than never. The development of a drug product production process is steadily advancing toward the adoption of science-based approaches for better understanding of processes underlying mechanisms, aiming at minimizing product quality variability. The adoption of new technology for assessing the quality of pharmaceutical products in real time (process analytical technology tools) and new paradigms of production, such as continuous manufacturing, brings additional challenges for pharmaceutical data scientists. Finally, product conception and manufacturing processes development can no longer be considered apart from the concept of life-cycle management. This chapter provides an overview on the recent evolution of drug product manufacturing approaches, and implications in terms of data handling and processing.}
}
@article{ALRUITHE2017223,
title = {Analysis and Classification of Barriers and Critical Success Factors for Implementing a Cloud Data Governance Strategy},
journal = {Procedia Computer Science},
volume = {113},
pages = {223-232},
year = {2017},
note = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.352},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917317623},
author = {Majid Al-Ruithe and Elhadj Benkhelifa},
keywords = {Cloud computing, data governance, cloud data governance, critical success factors (CSFs), systematic literature review (SLR), barriers},
abstract = {The general consensus in literature seems to suggest that data governance refers to the entirety of decision rights and responsibilities concerning the management of data assets in organisations. These definitions do not however provide equal prominence for the data governance within the cloud computing technology context. As such, this deficit calls for in-depth understanding of data governance and cloud Computing. This trend contributes to changes in data governance strategy in the organisation, such as the organisation’s structure and regulations, people, technology, process, roles and responsibilities. This is one of the great challenges facing organizations today when they move their data to Cloud Computing environments, particularly how Cloud technology affects data governance. The authors’ general observation reveals that the area of data governance in general is under researched and not widely practiced by organisations, let alone when it is concerned with cloud computing, where research is really in its infancy and far from reaching maturity. This paper attempts to identify the possible common barriers and critical success factors for implementing cloud data governance in the hope that it helps the reader to be aware of these barriers and consider them in future developments in the field.}
}
@article{MCKINLEY201715,
title = {Citizen science can improve conservation science, natural resource management, and environmental protection},
journal = {Biological Conservation},
volume = {208},
pages = {15-28},
year = {2017},
note = {The role of citizen science in biological conservation},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2016.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0006320716301963},
author = {Duncan C. McKinley and Abe J. Miller-Rushing and Heidi L. Ballard and Rick Bonney and Hutch Brown and Susan C. Cook-Patton and Daniel M. Evans and Rebecca A. French and Julia K. Parrish and Tina B. Phillips and Sean F. Ryan and Lea A. Shanley and Jennifer L. Shirk and Kristine F. Stepenuck and Jake F. Weltzin and Andrea Wiggins and Owen D. Boyle and Russell D. Briggs and Stuart F. Chapin and David A. Hewitt and Peter W. Preuss and Michael A. Soukup},
keywords = {Citizen science, Public participation in scientific research, Conservation, Policymaking, Natural resource management, Public input, Public engagement},
abstract = {Citizen science has advanced science for hundreds of years, contributed to many peer-reviewed articles, and informed land management decisions and policies across the United States. Over the last 10years, citizen science has grown immensely in the United States and many other countries. Here, we show how citizen science is a powerful tool for tackling many of the challenges faced in the field of conservation biology. We describe the two interwoven paths by which citizen science can improve conservation efforts, natural resource management, and environmental protection. The first path includes building scientific knowledge, while the other path involves informing policy and encouraging public action. We explore how citizen science is currently used and describe the investments needed to create a citizen science program. We find that:1.Citizen science already contributes substantially to many domains of science, including conservation, natural resource, and environmental science. Citizen science informs natural resource management, environmental protection, and policymaking and fosters public input and engagement.2.Many types of projects can benefit from citizen science, but one must be careful to match the needs for science and public involvement with the right type of citizen science project and the right method of public participation.3.Citizen science is a rigorous process of scientific discovery, indistinguishable from conventional science apart from the participation of volunteers. When properly designed, carried out, and evaluated, citizen science can provide sound science, efficiently generate high-quality data, and help solve problems.}
}
@article{WERNER201729,
title = {Cloud identity management: A survey on privacy strategies},
journal = {Computer Networks},
volume = {122},
pages = {29-42},
year = {2017},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2017.04.030},
url = {https://www.sciencedirect.com/science/article/pii/S1389128617301664},
author = {Jorge Werner and Carla Merkle Westphall and Carlos Becker Westphall},
keywords = {Privacy, Identity management, Cloud computing},
abstract = {With the rise of cloud computing, thousands of users and multiple applications have sought to communicate with each other, exchanging sensitive data. Thus, for effectively managing applications and resources, the use of models and tools is essential for the secure management of identities and to avoid compromising data privacy. There are models and tools that address federated identity management, and it is important that they use privacy mechanisms to assist in compliance with current legislation. Therefore, this article aims to present a survey of privacy in cloud identity management, presenting and comparing main features and challenges described in the literature. At the end of this work there is a discussion of the use of privacy and future research directions.}
}
@article{ARAI2018e515s,
title = {Academic health centers: integration of clinical research with healthcare and education. Comments on a workshop},
journal = {Clinics},
volume = {73},
pages = {e515s},
year = {2018},
issn = {1807-5932},
doi = {https://doi.org/10.6061/clinics/2017/e515s},
url = {https://www.sciencedirect.com/science/article/pii/S1807593222011334},
author = {Roberto Jun Arai and Irene {de Lourdes Noronha} and José Carlos Nicolau and Charles Schmidt and Gustavo Moreira {de Albuquerque} and Kenneth W Mahaffey and Eduardo Moacyr Krieger and José Otávio Costa Auler Júnior}
}
@article{ZHOU2018139,
title = {Harnessing social media for health information management},
journal = {Electronic Commerce Research and Applications},
volume = {27},
pages = {139-151},
year = {2018},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1567422317300960},
author = {Lina Zhou and Dongsong Zhang and Christopher C. Yang and Yu Wang},
keywords = {Conceptual framework, Health information management, Data analytics, Social media},
abstract = {The remarkable upsurge of social media has dramatic impacts on health care research and practice. Social media are reshaping health information management in a variety of ways, ranging from providing cost-effective ways to improve clinician-patient communication and exchange health-related information and experience, to enabling the discovery of new medical knowledge and information. Despite some demonstrated initial success, social media use and analytics for improving health as a research field is still at its infancy. Information systems researchers can potentially play a key role in advancing the field. This study proposes a conceptual framework for social media-based health information management by drawing on multi-disciplinary research. With the guidance of the framework, this paper presents related research challenges, identifies important yet under-explored research issues, and discusses promising directions for future research.}
}
@article{LIU2017571,
title = {Adaptive just-in-time and relevant vector machine based soft-sensors with adaptive differential evolution algorithms for parameter optimization},
journal = {Chemical Engineering Science},
volume = {172},
pages = {571-584},
year = {2017},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2017.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0009250917304463},
author = {Yiqi Liu},
keywords = {Soft-sensors, Just-in-time, Relevant vector machine, Differential evolution, Parameters},
abstract = {Just-in-time (JIT) and Relevant vector machine (RVM) are two of commonly used models for soft-sensors modeling, the efficiency of which is governed by few critical parameters and hyper-parameters significantly. These parameters are routinely selected by trial and error or experience, thus leading to over- or under-fitting for the prediction. Adaptive differential evolution with optional external archive (JADE) has been used to optimize the parameters of JIT and RVM in this paper. The resulted JADE-JIT and JADE-RVM based soft-sensors are further enhanced into an adaptive format by the moving window (WM) technique. The proposed methodologies are applied to prediction of hard-to-measured variables in the wastewater treatment plants (WWTPs) and successful results are obtained.}
}
@article{KEENAN201813,
title = {The Library of Integrated Network-Based Cellular Signatures NIH Program: System-Level Cataloging of Human Cells Response to Perturbations},
journal = {Cell Systems},
volume = {6},
number = {1},
pages = {13-24},
year = {2018},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405471217304908},
author = {Alexandra B. Keenan and Sherry L. Jenkins and Kathleen M. Jagodnik and Simon Koplev and Edward He and Denis Torre and Zichen Wang and Anders B. Dohlman and Moshe C. Silverstein and Alexander Lachmann and Maxim V. Kuleshov and Avi Ma'ayan and Vasileios Stathias and Raymond Terryn and Daniel Cooper and Michele Forlin and Amar Koleti and Dusica Vidovic and Caty Chung and Stephan C. Schürer and Jouzas Vasiliauskas and Marcin Pilarczyk and Behrouz Shamsaei and Mehdi Fazel and Yan Ren and Wen Niu and Nicholas A. Clark and Shana White and Naim Mahi and Lixia Zhang and Michal Kouril and John F. Reichard and Siva Sivaganesan and Mario Medvedovic and Jaroslaw Meller and Rick J. Koch and Marc R. Birtwistle and Ravi Iyengar and Eric A. Sobie and Evren U. Azeloglu and Julia Kaye and Jeannette Osterloh and Kelly Haston and Jaslin Kalra and Steve Finkbiener and Jonathan Li and Pamela Milani and Miriam Adam and Renan Escalante-Chong and Karen Sachs and Alex Lenail and Divya Ramamoorthy and Ernest Fraenkel and Gavin Daigle and Uzma Hussain and Alyssa Coye and Jeffrey Rothstein and Dhruv Sareen and Loren Ornelas and Maria Banuelos and Berhan Mandefro and Ritchie Ho and Clive N. Svendsen and Ryan G. Lim and Jennifer Stocksdale and Malcolm S. Casale and Terri G. Thompson and Jie Wu and Leslie M. Thompson and Victoria Dardov and Vidya Venkatraman and Andrea Matlock and Jennifer E. {Van Eyk} and Jacob D. Jaffe and Malvina Papanastasiou and Aravind Subramanian and Todd R. Golub and Sean D. Erickson and Mohammad Fallahi-Sichani and Marc Hafner and Nathanael S. Gray and Jia-Ren Lin and Caitlin E. Mills and Jeremy L. Muhlich and Mario Niepel and Caroline E. Shamu and Elizabeth H. Williams and David Wrobel and Peter K. Sorger and Laura M. Heiser and Joe W. Gray and James E. Korkola and Gordon B. Mills and Mark LaBarge and Heidi S. Feiler and Mark A. Dane and Elmar Bucher and Michel Nederlof and Damir Sudar and Sean Gross and David F. Kilburn and Rebecca Smith and Kaylyn Devlin and Ron Margolis and Leslie Derr and Albert Lee and Ajay Pillai},
keywords = {L1000, MEMA, P100, systems pharmacology, systems biology, lincsproject, lincsprogram, data integration, BD2K, MCF10A},
abstract = {The Library of Integrated Network-Based Cellular Signatures (LINCS) is an NIH Common Fund program that catalogs how human cells globally respond to chemical, genetic, and disease perturbations. Resources generated by LINCS include experimental and computational methods, visualization tools, molecular and imaging data, and signatures. By assembling an integrated picture of the range of responses of human cells exposed to many perturbations, the LINCS program aims to better understand human disease and to advance the development of new therapies. Perturbations under study include drugs, genetic perturbations, tissue micro-environments, antibodies, and disease-causing mutations. Responses to perturbations are measured by transcript profiling, mass spectrometry, cell imaging, and biochemical methods, among other assays. The LINCS program focuses on cellular physiology shared among tissues and cell types relevant to an array of diseases, including cancer, heart disease, and neurodegenerative disorders. This Perspective describes LINCS technologies, datasets, tools, and approaches to data accessibility and reusability.}
}
@article{DELIMANETO2018225,
title = {A semiotic-inspired machine for personalized multi-criteria intelligent decision support},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {225-238},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300757},
author = {Fernando Buarque {de Lima Neto} and Denis Mayr {Lima Martins} and Gottfried Vossen},
keywords = {Multi-criteria decision support, Computational intelligence, Computational semiotics, Intelligent semiotic machine},
abstract = {The need for appropriate decisions to tackle complex problems increases every day. Selecting destinations for vacation, comparing and optimizing resources to create valuable products, or purchasing a suitable car are just a few examples of puzzling situations in which there is no standard form to find an appropriate solution. Such scenarios become arduous when the number of possibilities, restrictions, and factors affecting the decision rise, thereby turning decision makers into almost mere spectators. In such circumstances, decision support systems (DSS) can play an important role in guiding people and organizations towards more accurate decision making. However, conventional DSS lack the necessary adaptability to account for dynamic changes and are frequently inadequate to tackle the subjectivity inherent in decision-maker's preferences and intention. We argue that these shortcomings can be addressed by a suitable combination of Semiotic Theory and Computational Intelligence algorithms, which together can make up a new generation of DSS. In this article, a formal description of an Intelligent Semiotic Machine is provided and tried out in practical decision contexts. The results obtained show that our approach can provide well-suited decisions based on user preferences, achieving appropriateness while fanning out subjective options without losing decision context, objectivity, or accuracy.}
}
@article{YANG2018407,
title = {Towards sustainable and resilient high density cities through better integration of infrastructure networks},
journal = {Sustainable Cities and Society},
volume = {42},
pages = {407-422},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S2210670718300210},
author = {Yifan Yang and S. Thomas Ng and Frank J. Xu and Martin Skitmore},
keywords = {High density cities, Sustainability, Resilience, Infrastructure system, Asset management},
abstract = {Many developed high density cities around the world are facing unprecedented challenges as their infrastructure facilities are aging while citizen demands are ever surging. The concerted efforts of different infrastructure stakeholders are indispensable to elevate the quality, reliability, and capacity of infrastructure systems to make high-density cities more sustainable and resilient against increasing climate change and manmade threats. To address these challenges, this paper proposes an integrated framework for multisector infrastructure asset management. For deriving the framework, case studies are conducted first on the best infrastructure asset management (IAM) practices of different countries in such diverse aspects as core process integration, contingency management, climate change response and adaptation, program coordination and orchestration, social value creation and sharing, risk management, resilience and sustainability. By using the criteria and a list of questions obtained from the case studies, interviews with different stakeholders in selected infrastructure sectors in Hong Kong are subsequently carried out to identify the barriers and possible solutions to enhancing the integrated management of multisector infrastructure assets in high-density cities. To facilitate such municipalities in managing their infrastructure assets effectively and efficiently, the proposed multisector integrated IAM framework is established from the holistic perspectives of information integration, process integration, collective decision, and harmonization between interdependent infrastructure systems.}
}
@article{YANG2018861,
title = {Identifying household electricity consumption patterns: A case study of Kunshan, China},
journal = {Renewable and Sustainable Energy Reviews},
volume = {91},
pages = {861-868},
year = {2018},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2018.04.037},
url = {https://www.sciencedirect.com/science/article/pii/S136403211830265X},
author = {Ting Yang and Minglun Ren and Kaile Zhou},
keywords = {Electricity consumption patterns, Load profiling, Smart energy management, Case study, Smart grid},
abstract = {A case study of residential electricity consumption patterns mining and abnormal user identification using hierarchical clustering is presented in this paper. First, based on a brief introduction of hierarchical clustering, a process model and the specific steps of electricity consumption patterns mining in smart grid environment are proposed. Then, a case study using the daily electricity consumption data of 300 residential users in an eastern city of China, Kunshan, from November 16, 2014 to December 16, 2014, is presented. Through the implementation of hierarchical clustering, 9 abnormal users and 4 types of monthly electricity consumption patterns are successfully identified. The results show that most residential users in Kunshan city, nearly 81%, have a similar monthly electricity consumption pattern. Their average daily electricity consumption is about 7.73 kWh in the early winter with small fluctuations. Also, their daily electricity consumption is significantly associated with the temperature changes. However, it is worth noting that the special electricity consumption patterns of a small proportion of electricity users cannot be ignored, which is of great significance for the planning, operation, policy formulation and decision-making of smart grid.}
}
@article{WALKER201714,
title = {Redefining the incidents to learn from: Safety science insights acquired on the journey from black boxes to Flight Data Monitoring},
journal = {Safety Science},
volume = {99},
pages = {14-22},
year = {2017},
note = {Learning from Incidents},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517309086},
author = {Guy Walker},
keywords = {Leading and lagging indicators, Data recording, Flight Data Monitoring, Safety management systems, Risk triangle},
abstract = {The reason Flight Data and Cockpit Voice Recorders (FDRs and CVRs) exist is to learn from incidents. Probably no other single invention has yielded such significant improvements in aviation safety. Indeed, they have been so effective that we now need to redefine what is meant by the term ‘incident’ and the uses to which data recording technologies are now put. The paradox is that at no previous point in history have we collected so much data, yet safety performance is such that it is rarely used for its original purpose: as a lagging indicator of problems following an accident. In this paper the history of black boxes is briefly surveyed and connected to the underlying safety science knowledge base. Flight Data Monitoring (FDM) is then presented as an exemplar of the paradigm shift from lagging to leading indicators needed in order to continue learning from incidents. In many industries the pre-requisites for comparable Data Monitoring processes are already in place. The benefits to be accrued by following the example set by the aviation industry are considerable.}
}
@article{CITRIN2018197,
title = {Developing and deploying a community healthcare worker-driven, digitally- enabled integrated care system for municipalities in rural Nepal},
journal = {Healthcare},
volume = {6},
number = {3},
pages = {197-204},
year = {2018},
issn = {2213-0764},
doi = {https://doi.org/10.1016/j.hjdsi.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2213076417302609},
author = {David Citrin and Poshan Thapa and Isha Nirola and Sachit Pandey and Lal Bahadur Kunwar and Jasmine Tenpa and Bibhav Acharya and Hari Rayamazi and Aradhana Thapa and Sheela Maru and Anant Raut and Sanjaya Poudel and Diwash Timilsina and Santosh Kumar Dhungana and Mukesh Adhikari and Mukti Nath Khanal and Naresh {Pratap KC} and Bhim Acharya and Khem Bahadur Karki and Dipendra Raman Singh and Alex Harsha Bangura and Jeremy Wacksman and Daniel Storisteanu and Scott Halliday and Ryan Schwarz and Dan Schwarz and Nandini Choudhury and Anirudh Kumar and Wan-Ju Wu and S.P. Kalaunee and Pushpa Chaudhari and Duncan Maru},
keywords = {Community health workers, Delivery of healthcare, integrated, Electronic health records, Biometric identification, Health information systems, Nepal},
abstract = {Integrating care at the home and facility level is a critical yet neglected function of healthcare delivery systems. There are few examples in practice or in the academic literature of affordable, digitally-enabled integrated care approaches embedded within healthcare delivery systems in low- and middle-income countries. Simultaneous advances in affordable digital technologies and community healthcare workers offer an opportunity to address this challenge. We describe the development of an integrated care system involving community healthcare worker networks that utilize a home-to-facility electronic health record platform for rural municipalities in Nepal. Key aspects of our approach of relevance to a global audience include: community healthcare workers continuously engaging with populations through household visits every three months; community healthcare workers using digital tools during the routine course of clinical care; individual and population-level data generated routinely being utilized for program improvement; and being responsive to privacy, security, and human rights concerns. We discuss implementation, lessons learned, challenges, and opportunities for future directions in integrated care delivery systems.}
}
@article{BISASO2017366,
title = {A survey of machine learning applications in HIV clinical research and care},
journal = {Computers in Biology and Medicine},
volume = {91},
pages = {366-371},
year = {2017},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S001048251730361X},
author = {Kuteesa R. Bisaso and Godwin T. Anguzu and Susan A. Karungi and Agnes Kiragga and Barbara Castelnuovo},
keywords = {HIV, Machine learning, Clinical research, Application paradigms},
abstract = {A wealth of genetic, demographic, clinical and biomarker data is collected from routine clinical care of HIV patients and exists in the form of medical records available among the medical care and research communities. Machine learning (ML) methods have the ability to identify and discover patterns in complex datasets and predict future outcomes of HIV treatment. We survey published studies that make use of ML techniques in HIV clinical research and care. An advanced search relevant to the use of ML in HIV research was conducted in the PubMed biomedical database. The survey outcomes of interest include data sources, ML techniques, ML tasks and ML application paradigms. A growing trend in application of ML in HIV research was observed. The application paradigm has diversified to include practical clinical application, but statistical analysis remains the most dominant application. There is an increase in the use of genomic sources of data and high performance non-parametric ML methods with a focus on combating resistance to antiretroviral therapy (ART). There is need for improvement in collection of health records data and increased training in ML so as to translate ML research into clinical application in HIV management.}
}
@article{FAZEKAS201835,
title = {The extent and cost of corruption in transport infrastructure. New evidence from Europe},
journal = {Transportation Research Part A: Policy and Practice},
volume = {113},
pages = {35-54},
year = {2018},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2018.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0965856417311199},
author = {Mihály Fazekas and Bence Tóth},
keywords = {Corruption, Transport infrastructure, Public procurement, Europe, Spending structure, Price effect},
abstract = {Transport infrastructure provision from roads to waterways involves large amounts of public funds in very complex projects. It is hardly a surprise that all across Europe, but especially in high corruption risk countries, it is a primary target of corrupt elites. This article provides a state-of-the-art review of the literature on the cost of corruption and estimates the level of corruption risks and associated costs in European infrastructure development and maintenance in 2009–2014 using novel data on over 40,000 government contracts. Two forms of corruption costs are investigated in the empirical section: (1) distorting spending structure and project design, and (2) inflating prices. Findings indicate that corruption steers infrastructure spending towards high value as opposed to small value investment projects. It also inflates prices by 30–35% on average with largest excesses in high corruption risk regions. Contrary to perceptions, corruption risks in infrastructure are decoupled to a considerable extent from the national corruption environment. Source data and risk scores are made downloadable at digiwhist.eu/resources/data.}
}
@incollection{2017293,
title = {Subject Index},
editor = {Heimar {de Fátima Marin} and Eduardo Massad and Marco Antonio Gutierrez and Roberto J. Rodrigues and Daniel Sigulem},
booktitle = {Global Health Informatics},
publisher = {Academic Press},
pages = {293-301},
year = {2017},
isbn = {978-0-12-804591-6},
doi = {https://doi.org/10.1016/B978-0-12-804591-6.00027-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128045916000276}
}
@article{CAPALBO2017191,
title = {Next generation data systems and knowledge products to support agricultural producers and science-based policy decision making},
journal = {Agricultural Systems},
volume = {155},
pages = {191-199},
year = {2017},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2016.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16306898},
author = {Susan M. Capalbo and John M. Antle and Clark Seavert},
keywords = {Data systems, Knowledge products, AgBizLogic, TOA-MD, Next generation},
abstract = {Research on next generation agricultural systems models shows that the most important current limitation is data, both for on-farm decision support and for research investment and policy decision making. One of the greatest data challenges is to obtain reliable data on farm management decision making, both for current conditions and under scenarios of changed bio-physical and socio-economic conditions. This paper presents a framework for the use of farm-level and landscape-scale models and data to provide analysis that could be used in NextGen knowledge products, such as mobile applications or personal computer data analysis and visualization software. We describe two analytical tools - AgBiz Logic and TOA-MD - that demonstrate the current capability of farmlevel and landscape-scale models. The use of these tools is explored with a case study of an oilseed crop, Camelina sativa, which could be used to produce jet aviation fuel. We conclude with a discussion of innovations needed to facilitate the use of farm and policy-level models to generate data and analysis for improved knowledge products.}
}
@article{AHANI2017560,
title = {Forecasting social CRM adoption in SMEs: A combined SEM-neural network method},
journal = {Computers in Human Behavior},
volume = {75},
pages = {560-578},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.05.032},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217303539},
author = {Ali Ahani and Nor Zairah Ab. Rahim and Mehrbakhsh Nilashi},
keywords = {CRM adoption, Social CRM, Technology-organization-environment-process, SMEs, Social media, SEM-Neural network},
abstract = {The growth of social media usage questions the old-style idea of customer relationship management (CRM). Social CRM strategy is a novel version of CRM empowered by social media technology that offers a new way of managing relationships with customers effectively. This study aims to forecast the predictors of social CRM strategy adoption by small and medium enterprises (SMEs). The proposed model used in this study derived its theoretical support from IT/IS, marketing, and CRM literature. In the proposed Technology-Organization-Environment-Process (TOEP) adoption model, several hypotheses are developed which examine the role of Technological factors, such as Cost of Adoption, Relative Advantages, Complexity, and Compatibility; Organizational factors, such as IT/IS knowledge of employee, and Top management support; Environmental factors such as Competitive Pressure, and Customer Pressure; and Process factors such as Information Capture, Information Use, and Information Sharing; all having a positive relationship with social CRM adoption. This research applied a following two staged SEM-neural network method combining both structural equation modelling (SEM) and neural network analyses. The proposed hypothetical model is examined by using SEM on the collected data of SMEs in Kuala Lumpur, the central city of Malaysia. The SEM approach with a neural network method can be used to investigate the complicated relations involved in the adoption of social CRM. The study finds that compatibility, information capture, IT/IS knowledge of employee, top management support, information sharing, competitive pressure, cost, relative advantage, and customer pressure are the most important factors influencing social CRM adoption. Remarkably, the results of neural network analysis show that compatibility and information capture of social CRM are the most significant factors which affect SMEs' adoption of this form of customer relationship management. The outcomes of this research benefit executives' decision-making by identifying and ranking factors that enable them to discover how they can advance the usage of social CRM in their firms. Furthermore, the findings of this study can help the managers/owners of SMEs assign their resources, according to the ranking of social CRM adoption factors, when they are making plans to adopt social CRM. This study differs from previous studies as it proposes an innovative new approach to determine what influences the adoption of social CRM. By proposing the TOEP adoption model, additional information process factors advance the traditional TOE adoption model.}
}
@article{BEIER2017466,
title = {Multicenter data sharing for collaboration in sleep medicine},
journal = {Future Generation Computer Systems},
volume = {67},
pages = {466-480},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16300693},
author = {Maximilian Beier and Christoph Jansen and Geert Mayer and Thomas Penzel and Andrea Rodenbeck and René Siewert and Michael Witt and Jie Wu and Dagmar Krefting},
keywords = {Biosignal, Polysomnography, EDF, XNAT, OpenStack, Docker},
abstract = {Sleep is a fundamental biological process crucial for survival and health of (not only) humans. But many circumstances like physiological and mental disorders, environment and lifestyle may affect healthy sleep. To date, 88 different sleep disorders are internationally recognized. They cover a broad field of medical areas. Analysis of human sleep is typically based on multidimensional biosignal recordings, so called polysomnographies (PSG). Therefore research often includes digital signal processing. Clinical sleep research is an inherent multidisciplinary field. Inter-institutional and interdisciplinary collaborations are required to address the complexity of sleep regulation and disturbance. But to date, collaborative sleep research is poorly supported by IT systems. In particular, the management and processing of PSGs is challenging. A large variety of PSG devices, data formats, measurement procedures and quality variations impedes consistent biosignal data processing. In this manuscript we introduce a virtual research platform supporting inter-institutional data sharing and processing. The infrastructure is based on XNAT—a free and open source neuroimaging research platform, a loosely coupled service oriented architecture and scalable virtualization in the back end. The system is capable of local pseudonymization of biosignal data, mapping to a standardized set of parameters and automatic quality assessment. Terms and quality measures are derived from the “Manual for the Scoring of Sleep and Associated Events” of the American Academy of Sleep Medicine (AASM), the de facto standard for diagnostic biosignal analysis in sleep medicine.}
}
@article{EMMANOUILIDIS2018435,
title = {Internet of Things - Enabled Visual Analytics for Linked Maintenance and Product Lifecycle Management},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {435-440},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.339},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314630},
author = {C. Emmanouilidis and L. Bertoncelj and M. Bevilacqua and S. Tedeschi and C. Ruiz-Carcel},
keywords = {Internet of Things, Visual Analytics, Maintenance, Product Lifecycle Management},
abstract = {When closed loop product lifecycle management was first introduced, much effort focused on establishing ways to communicate data between different lifecycle phase activities. The concept of a smart product, able to communicate its own identity and status, had a key role to play to this end. Such a concept has further matured, benefiting from internet things-enabled product lifecycle management advancements. Product data exchanges can now be brought closer to the point of end use consumption, enabling users to become more proactive actors within the product lifecycle management process. This paper presents a conceptual approach and a pilot implementation of how this can be achieved by superimposing middle of life relevant product information to beginning of life product views, such as a 3D product CAD model. In this way, linked maintenance data and knowledge become visual features of a product design representation, facilitating a user’s understanding of middle-of life concepts, such as occurrence of failure modes. The proposed approach can be particularly useful when dealing with product data streams as a natural visual analytics add-in to closed loop product lifecycle management.}
}
@article{MURRAY2018130,
title = {Data challenges and opportunities for environmental management of North Sea oil and gas decommissioning in an era of blue growth},
journal = {Marine Policy},
volume = {97},
pages = {130-138},
year = {2018},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2018.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X18302355},
author = {Fiona Murray and Katherine Needham and Kate Gormley and Sally Rouse and Joop W.P. Coolen and David Billett and Jennifer Dannheim and Silvana N.R. Birchenough and Kieran Hyder and Richard Heard and Joseph S. Ferris and Jan M. Holstein and Lea-Anne Henry and Oonagh McMeel and Jan-Bart Calewaert and J. Murray Roberts},
keywords = {Decommissioning, Offshore energy, Environmental assessment, Blue economy, Open access, ROV survey},
abstract = {Maritime industries routinely collect critical environmental data needed for sustainable management of marine ecosystems, supporting both the blue economy and future growth. Collating this information would provide a valuable resource for all stakeholders. For the North Sea, the oil and gas industry has been a dominant presence for over 50 years that has contributed to a wealth of knowledge about the environment. As the industry begins to decommission its offshore structures, this information will be critical for avoiding duplication of effort in data collection and ensuring best environmental management of offshore activities. This paper summarises the outcomes of a Blue Growth Data Challenge Workshop held in 2017 with participants from: the oil and gas industry; the key UK regulatory and management bodies for oil and gas decommissioning; open access data facilitators; and academic and research institutes. Here, environmental data collection and archiving by oil and gas operators in the North Sea are described, alongside how this compares to other offshore industries; what the barriers and opportunities surrounding environmental data sharing are; and how wider data sharing from offshore industries could be achieved. Five primary barriers to data sharing were identified: 1) Incentives, 2) Risk Perception, 3) Working Cultures, 4) Financial Models, and 5) Data Ownership. Active and transparent communication and collaboration between stakeholders including industry, regulatory bodies, data portals and academic institutions will be key to unlocking the data that will be critical to informing responsible decommissioning decisions for offshore oil and gas structures in the North Sea.}
}
@article{DU201824,
title = {A machine learning based approach to identify protected health information in Chinese clinical text},
journal = {International Journal of Medical Informatics},
volume = {116},
pages = {24-32},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618303447},
author = {Liting Du and Chenxi Xia and Zhaohua Deng and Gary Lu and Shuxu Xia and Jingdong Ma},
keywords = {Protected health information, De-identification, Electronic health records, Conditional random fields},
abstract = {Background
With the increasing application of electronic health records (EHRs) in the world, protecting private information in clinical text has drawn extensive attention from healthcare providers to researchers. De-identification, the process of identifying and removing protected health information (PHI) from clinical text, has been central to the discourse on medical privacy since 2006. While de-identification is becoming the global norm for handling medical records, there is a paucity of studies on its application on Chinese clinical text. Without efficient and effective privacy protection algorithms in place, the use of indispensable clinical information would be confined.
Objectives
We aimed to (i) describe the current process for PHI in China, (ii) propose a machine learning based approach to identify PHI in Chinese clinical text, and (iii) validate the effectiveness of the machine learning algorithm for de-identification in Chinese clinical text.
Methods
Based on 14,719 discharge summaries from regional health centers in Ya'an City, Sichuan province, China, we built a conditional random fields (CRF) model to identify PHI in clinical text, and then used the regular expressions to optimize the recognition results of the PHI categories with fewer samples.
Results
We constructed a Chinese clinical text corpus with PHI tags through substantial manual annotation, wherein the descriptive statistics of PHI manifested its wide range and diverse categories. The evaluation showed with a high F-measure of 0.9878 that our CRF-based model had a good performance for identifying PHI in Chinese clinical text.
Conclusion
The rapid adoption of EHR in the health sector has created an urgent need for tools that can parse patient specific information from Chinese clinical text. Our application of CRF algorithms for de-identification has shown the potential to meet this need by offering a highly accurate and flexible solution to analyzing Chinese clinical text.}
}
@incollection{BERTHIER201885,
title = {5 - False Data and Fictitious Algorithmic Projections},
editor = {Thierry Berthier and Bruno Teboul},
booktitle = {From Digital Traces to Algorithmic Projections},
publisher = {Elsevier},
pages = {85-112},
year = {2018},
isbn = {978-1-78548-270-0},
doi = {https://doi.org/10.1016/B978-1-78548-270-0.50005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978178548270050005X},
author = {Thierry Berthier and Bruno Teboul},
keywords = {Anti-fragility, Attractive fictitious profiles, Digital security, Digital space, Fictitious data, Fictitious projections, Honeypot, Integrity, Natural language processing, Robin Sage},
abstract = {Abstract:
The creation of false data (data conveying false information) can be considered as a collateral effect of the algorithmization of the environment. The use of false or fictitious data always responds to an initial aim to fool or dissimulate that can be expressed in very varied contexts, such as protecting anonymity, economic or military inquiries, cyber-espionage, cyber-crime, financial fraud or the manipulation of stock for listed companies. Creators of false data seek to deceive a number of users or a calculation system with the aim of benefitting from doing so.}
}
@article{WANG2018536,
title = {Understanding the dynamic mechanism of interagency government data sharing},
journal = {Government Information Quarterly},
volume = {35},
number = {4},
pages = {536-546},
year = {2018},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18300728},
author = {Fang Wang},
keywords = {Interagency government data sharing, IGDS, Acting forces, Dynamic mechanism, Force-field theory of change},
abstract = {Interagency government data sharing plays an important role in promoting the coordination of government departments and improving public services. Under the guidance of a theoretical framework that combines the force-field theory of change and the theory of mechanism, this study conducted a case study on two Chinese urban governments and built a dynamic mechanism model for IGDS. The model consists of six forces acting on IGDS, as well as their activities, effects and interactions. Some effects of them are context-dependent. This model can be used to explain the reasons of various barriers to IGDS and thus to guide government departments and policy makers design more specific and targeted dynamic mechanisms to promote IGDS. Finally, several mechanisms were discussed in the context of policy making.}
}
@article{PAGOROPOULOS2017369,
title = {Assessing transformational change from institutionalising digital capabilities on implementation and development of Product-Service Systems: Learnings from the maritime industry},
journal = {Journal of Cleaner Production},
volume = {166},
pages = {369-380},
year = {2017},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617317341},
author = {Aris Pagoropoulos and Anja Maier and Tim C. McAloone},
keywords = {Product-Service Systems, Digitization, Customer, Maritime industry},
abstract = {Digitization is rapidly reshaping industries and economic sectors. It enables novel Product-Service Systems (PSS) that transform customer/supplier relationships and introduces new value propositions. However, while opportunities for novel types of PSS arise, it is not clear how digitization and the institutionalisation of digital capabilities, particularly within the customer organisations, may affect implementation of PSS, potentially leading to transformational changes in the customer organisation. This paper examines one such potential transformational change from three complementary viewpoints – the resource based, the dynamic, and the relational viewpoint. It does so through action research study in the context of the maritime industry, which is particularly attractive for PSS offerings. The research methodology comprised a two-step action research process, focusing on both digitization and PSS development and implementation. The main findings are that rather than facilitating procurement to co-development of PSS, institutionalisation of digital capabilities facilitated development of PSS by stakeholders internal to the company, and strategic co-development with external stakeholders. The new digital capabilities circumvented cost barriers associated with the procurement of services from external stakeholders, supported process standardisation - to the expense of process innovation-, and transformed the network that delivered PSS by closing opportunity gaps for externally procured services. Furthermore, the uptake of digital capabilities highlighted the importance of cost estimation in making the customer more responsive to threats and opportunities.}
}
@article{LEE201734,
title = {Online resources for studies of genome biology and epigenetics},
journal = {Current Opinion in Toxicology},
volume = {6},
pages = {34-41},
year = {2017},
note = {Genomic Toxicology: Epigenetics},
issn = {2468-2020},
doi = {https://doi.org/10.1016/j.cotox.2017.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S2468202017300621},
author = {Paul J. Lee and Mayank NK Choudhary and Ting Wang},
keywords = {Next generation sequencing, Online database, Epigenome, Methylome, Methylation, Chromatin accessibility, Histone modifications, 3D genome, TADs, ENCODE, ROADMAP},
abstract = {Environmental exposure to chemical toxins alters epigenetic modifications that culminate in altered cellular gene expression without changing the underlying DNA sequence. The complex interplay between the layers of epigenetic regulators ultimately results in observed cellular phenotype. This review highlights epigenetics annotations assayed in the Encyclopedia of DNA Elements (ENCODE) community resource project—a publically accessible database for understanding genomic function, development and disease etiologies. We outline the multiple levels of epigenetic control (DNA methylation, chromatin accessibility, histone modifications, genome topology) with their associated interrogation methodology. We explore the limitations and strengths of each methodology at every epigenetic checkpoint. This review points readers to epigenetic resources that have gathered focused scientific data and directs them toward data visualization tools that can help answer questions related to epigenetic controls. The purpose of this review is to highlight online resources available to toxicological epigenetic researchers that can help fast track novel insights using already curated reference epigenome datasets.}
}
@article{HAMPSON20171104,
title = {Improving the selection and development of influenza vaccine viruses – Report of a WHO informal consultation on improving influenza vaccine virus selection, Hong Kong SAR, China, 18–20 November 2015},
journal = {Vaccine},
volume = {35},
number = {8},
pages = {1104-1109},
year = {2017},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2017.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X17300373},
author = {Alan Hampson and Ian Barr and Nancy Cox and Ruben O. Donis and Hirve Siddhivinayak and Daniel Jernigan and Jacqueline Katz and John McCauley and Fernando Motta and Takato Odagiri and John S. Tam and Anthony Waddell and Richard Webby and Thedi Ziegler and Wenqing Zhang},
keywords = {Influenza vaccines, Surveillance, Antigenic drift, Pandemic influenza},
abstract = {Since 2010 the WHO has held a series of informal consultations to explore ways of improving the currently highly complex and time-pressured influenza vaccine virus selection and development process. In November 2015 experts from around the world met to review the current status of efforts in this field. Discussion topics included strengthening influenza surveillance activities to increase the availability of candidate vaccine viruses and improve the extent, timeliness and quality of surveillance data. Consideration was also given to the development and potential application of newer laboratory assays to better characterize candidate vaccine viruses, the potential importance of antibodies directed against influenza virus neuraminidase, and the role of vaccine effectiveness studies. Advances in next generation sequencing and whole genome sequencing of influenza viruses were also discussed, along with associated developments in synthetic genomics technologies, evolutionary analysis and predictive mathematical modelling. Discussions were also held on the late emergence of an antigenic variant influenza A(H3N2) virus in mid-2014 that could not be incorporated in time into the 2014–15 northern hemisphere vaccine. There was broad recognition that given the current highly constrained influenza vaccine development and production timeline it would remain impossible to incorporate any variant virus which emerged significantly long after the relevant WHO biannual influenza vaccine composition meetings. Discussions were also held on the development of pandemic and broadly protective vaccines, and on associated regulatory and manufacturing requirements and constraints. With increasing awareness of the health and economic burdens caused by seasonal influenza, the ever-present threat posed by zoonotic influenza viruses, and the significant impact of the 2014–15 northern hemisphere seasonal influenza vaccine mismatch, this consultation provided a very timely opportunity to share developments and exchange views. In all areas, a renewed and strengthened emphasis was placed on developing concrete and measurable actions and identifying the key stakeholders responsible for their implementation.}
}
@article{GALLINELLI2017391,
title = {CityFeel - micro climate monitoring for climate mitigation and urban design},
journal = {Energy Procedia},
volume = {122},
pages = {391-396},
year = {2017},
note = {CISBAT 2017 International ConferenceFuture Buildings & Districts – Energy Efficiency from Nano to Urban Scale},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.07.427},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217332605},
author = {Peter Gallinelli and Reto Camponovo and Victor Guillot},
keywords = {climate mitigation, micro climate monitoring, urban design, open data},
abstract = {While a significant part of the population is concentrated in urban areas, the influence of cityscape parameters on human heat stress remain poorly understood. Yet we agree to develop urban spaces (street, square, district ...) in a way to provide best possible quality of life. In order to do so, quantitative and qualitative references are required. To fill this gap the HES-SO††University of Applied Sciences and Arts of Western Switzerland - hepia/leea‡‡Haute école du paysage, d’ingénierie et d’architecture de Genève / Laboratory for energy, environment and architecture has developed an innovative portable monitoring system that can be easily deployed in various outdoor and indoor environments. The monitoring equipment is embedded into a backpack that is carried during ‘climatic urban walks’ that can be reproduced at different times of the day or seasons so to yield a detailed and dynamic description of the climatic context of a portion of the city from the pedestrian point of view.}
}
@article{AUCHTER20188,
title = {A description of the ABCD organizational structure and communication framework},
journal = {Developmental Cognitive Neuroscience},
volume = {32},
pages = {8-15},
year = {2018},
note = {The Adolescent Brain Cognitive Development (ABCD) Consortium: Rationale, Aims, and Assessment Strategy},
issn = {1878-9293},
doi = {https://doi.org/10.1016/j.dcn.2018.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1878929317302268},
author = {Allison M. Auchter and Margie {Hernandez Mejia} and Charles J. Heyser and Paul D. Shilling and Terry L. Jernigan and Sandra A. Brown and Susan F. Tapert and Gayathri J. Dowling},
keywords = {Adolescence, Development, Neuroimaging, Longitudinal, Organizational framework, Governance},
abstract = {The Adolescent Brain Cognitive Development (ABCD) study is designed to be the largest study of brain development and child health in the United States, performing comprehensive assessments of 11,500 children repeatedly for 10 years. An endeavor of this magnitude requires an organized framework of governance and communication that promotes collaborative decision-making and dissemination of information. The ABCD consortium structure, built upon the Matrix Management approach of organizational theory, facilitates the integration of input from all institutions, numerous internal workgroups and committees, federal partners, and external advisory groups to make use of a broad range of expertise to ensure the study’s success.}
}
@article{SHI20181390,
title = {Applying high-frequency surrogate measurements and a wavelet-ANN model to provide early warnings of rapid surface water quality anomalies},
journal = {Science of The Total Environment},
volume = {610-611},
pages = {1390-1399},
year = {2018},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2017.08.232},
url = {https://www.sciencedirect.com/science/article/pii/S0048969717322386},
author = {Bin Shi and Peng Wang and Jiping Jiang and Rentao Liu},
keywords = {Water quality, Surrogate parameters, Anomaly detection, Wavelet denoising, Back-propagation neural networks},
abstract = {It is critical for surface water management systems to provide early warnings of abrupt, large variations in water quality, which likely indicate the occurrence of spill incidents. In this study, a combined approach integrating a wavelet artificial neural network (wavelet-ANN) model and high-frequency surrogate measurements is proposed as a method of water quality anomaly detection and warning provision. High-frequency time series of major water quality indexes (TN, TP, COD, etc.) were produced via a regression-based surrogate model. After wavelet decomposition and denoising, a low-frequency signal was imported into a back-propagation neural network for one-step prediction to identify the major features of water quality variations. The precisely trained site-specific wavelet-ANN outputs the time series of residual errors. A warning is triggered when the actual residual error exceeds a given threshold, i.e., baseline pattern, estimated based on long-term water quality variations. A case study based on the monitoring program applied to the Potomac River Basin in Virginia, USA, was conducted. The integrated approach successfully identified two anomaly events of TP variations at a 15-minute scale from high-frequency online sensors. A storm event and point source inputs likely accounted for these events. The results show that the wavelet-ANN model is slightly more accurate than the ANN for high-frequency surface water quality prediction, and it meets the requirements of anomaly detection. Analyses of the performance at different stations and over different periods illustrated the stability of the proposed method. By combining monitoring instruments and surrogate measures, the presented approach can support timely anomaly identification and be applied to urban aquatic environments for watershed management.}
}
@article{WANG201832,
title = {Truthful incentive mechanism with location privacy-preserving for mobile crowdsourcing systems},
journal = {Computer Networks},
volume = {135},
pages = {32-43},
year = {2018},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618300756},
author = {Yingjie Wang and Zhipeng Cai and Xiangrong Tong and Yang Gao and Guisheng Yin},
keywords = {Mobile crowdsourcing, Incentive mechanism, Auction algorithm, -anonymity, Differential privacy},
abstract = {With the rapid development of mobile devices, mobile crowdsourcing has become an important research focus. In order to improve the efficiency and truthfulness of mobile crowdsourcing systems, this paper proposes a truthful incentive mechanism with location privacy-preserving for mobile crowdsourcing systems. The improved two-stage auction algorithm based on trust degree and privacy sensibility (TATP) is proposed. In addition, the k−ɛ-differential privacy-preserving is proposed to prevent users’ location information from being leaked. Through comparison experiments, the effectiveness of the proposed incentive mechanism is verified. The proposed incentive mechanism with location privacy-preserving can inspire users to participate sensing tasks, and protect users’ location privacy effectively.}
}
@article{MAGANHA2018120,
title = {Understanding reconfigurability of manufacturing systems: An empirical analysis},
journal = {Journal of Manufacturing Systems},
volume = {48},
pages = {120-130},
year = {2018},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2018.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612518302036},
author = {Isabela Maganha and Cristovao Silva and Luis Miguel D.F. Ferreira},
keywords = {Reconfigurable manufacturing system, Reconfigurability, Exploratory analysis, Questionnaire survey},
abstract = {The need for more responsive manufacturing systems to deal with high product variety and large fluctuations in market demand requires new approaches that enable the system to react to changes quickly and efficiently. Reconfigurability is an ability that allows the addition, removal or rearrangement of manufacturing system components and functions to better cope with high product variety and significant fluctuations in market demand in a cost effective way. This paper empirically investigates the understanding of reconfigurability in industrial manufacturing companies and tests and validates its core characteristics using a questionnaire survey, which was carried out with Portuguese companies. Findings show the existence of five core characteristics of reconfigurability. The implications of these characteristics, concerning the implementation of Reconfigurable Manufacturing Systems, are also analysed and discussed.}
}
@incollection{DONG2018409,
title = {Chapter 18 - Energy Disaggregation and the Utility-Privacy Tradeoff},
editor = {Reza Arghandeh and Yuxun Zhou},
booktitle = {Big Data Application in Power Systems},
publisher = {Elsevier},
pages = {409-444},
year = {2018},
isbn = {978-0-12-811968-6},
doi = {https://doi.org/10.1016/B978-0-12-811968-6.00018-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119686000188},
author = {Roy Dong and Lillian J. Ratliff},
keywords = {Energy disaggregation, Privacy metrics, Utility-privacy tradeoff, Estimation, Fundamental limits, Direct load control},
abstract = {CHAPTER OVERVIEW
The problem of energy disaggregation is the estimation of individual device usage patterns from available aggregate energy consumption measurements. In this work, we consider the fundamental limits of the energy disaggregation problem, and use these limits to quantify the tradeoff between the utilization of data for smart grid operations and the privacy provided to energy consumers. First, our fundamental limits build on a statistical testing framework to provide a theoretical bound to the accuracy of energy disaggregation that can be achieved by any algorithm. Then, we present a framework for understanding how variations in system design can affect the operational benefits of collecting data, as well as the privacy of users. We instantiate this framework in a direct load control example where we use thermostatically controlled loads and vary the frequency with which a centralized controller receives sensor measurements. Our work formalizes the process of incorporating privacy considerations into the design of modern energy systems.}
}
@article{FAN2018116,
title = {Discovering gradual patterns in building operations for improving building energy efficiency},
journal = {Applied Energy},
volume = {224},
pages = {116-123},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.04.118},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918306858},
author = {Cheng Fan and Yongjun Sun and Kui Shan and Fu Xiao and Jiayuan Wang},
keywords = {Gradual pattern mining, Motif discovery, Data mining, Building operational performance, Building energy efficiency},
abstract = {The development of information technologies has enabled real-time monitoring and controls over building operations. Massive amounts of building operational data are being collected and available for knowledge discovery. Advanced data analytics are urgently needed to fully realize the potentials of big building operational data in enhancing building energy efficiency. The rapid development of data mining has provided powerful tools for extracting insights in various knowledge representations. Gradual pattern mining is a promising technique for discovering useful patterns from building operational data. The knowledge discovered is represented as gradual relationships, i.e., “the more/less A, the more/less B”. It can bring special interests to building energy management by highlighting co-variations among numerical building variables. This study investigated the usefulness of gradual pattern mining for building energy management. A generic methodology was proposed to ensure the quality and applicability of the knowledge discovered. The methodology was validated through a case study. The results showed that the methodology could successfully extract valuable insights on building operation characteristics and provide opportunities for building energy efficiency enhancement.}
}
@article{ZENG20174,
title = {Energy finance data warehouse: Tracking revenues through the power sector},
journal = {The Electricity Journal},
volume = {30},
number = {3},
pages = {4-9},
year = {2017},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2017.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1040619017300131},
author = {Claire Zeng and Stephen Hendrickson and Sangkeun Matt Lee and Supriya Chinthavali and Jessica Lin and Eric Hsieh and Mallikarjun Shankar},
keywords = {Energy, Financial data, Power sector revenue, Sankey, Visualization},
abstract = {Reliable data is needed to understand financial relationships in the power sector. However, relevant data acquisition and visualization can be a challenge due to the fragmented nature of the power sector. The US DOE and ORNL leveraged a Sankey prototype to elucidate the ‘big picture’ of financial flows to understand the complex relationships between specific actors within the power sector. The continued incorporation of high quality data can improve the fidelity of such an approach and lead to an increasingly detailed understanding of financial relationships in the power sector and their implications for policymakers.}
}
@article{WASSERSTEIN2018483,
title = {Administrative Databases in Sports Medicine Research},
journal = {Clinics in Sports Medicine},
volume = {37},
number = {3},
pages = {483-494},
year = {2018},
note = {Sports Medicine Statistics},
issn = {0278-5919},
doi = {https://doi.org/10.1016/j.csm.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278591918300279},
author = {David Wasserstein and Ujash Sheth},
keywords = {Administrative database, Cohort study, Epidemiology, Incidence rate, Sports medicine}
}
@article{MA201814,
title = {Bike sharing and users’ subjective well-being: An empirical study in China},
journal = {Transportation Research Part A: Policy and Practice},
volume = {118},
pages = {14-24},
year = {2018},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2018.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0965856417307966},
author = {Liang Ma and Xin Zhang and Xiaoyan Ding and Gaoshan Wang},
keywords = {Perceived value, Social influence, Trust attitude, Personal accomplishment, Subjective well-being},
abstract = {The rise of bike sharing has been phenomenal in China. However, few studies have focused on it relation to subjective well-being. Here we develop an integrated model to investigate factors that affect the subjective well-being of shared bike users in China. An online survey of 908 users was conducted. The highlights are: (1) perceived value has a positive effect on users’ subjective well-being through users’ trust attitude. Hedonic value has the greatest impact on users’ subjective well-being, followed by social value and utilitarian value; (2) social influence has a positive effect on users’ trust attitude and hence to subjective well-being; (3) perceived ease of use and perceived usefulness of the system have positive effects on users’ trust attitude; (4) personal accomplishment and users’ trust attitude have a positive effect on users’ subjective well-being. Theoretical and practical implications are also discussed.}
}
@incollection{2017271,
title = {Index},
editor = {Martijn Groot},
booktitle = {A Primer in Financial Data Management},
publisher = {Academic Press},
pages = {271-282},
year = {2017},
isbn = {978-0-12-809776-2},
doi = {https://doi.org/10.1016/B978-0-12-809776-2.00015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097762000156}
}
@article{KAFFEE201866,
title = {The Human Face of the Web of Data: A Cross-sectional Study of Labels},
journal = {Procedia Computer Science},
volume = {137},
pages = {66-77},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316119},
author = {Lucie-Aimée Kaffee and Elena Simperl},
keywords = {Linked Data, Web of Data, Labels, Human Accessibility, Multilingual 2010 MSC: 00-01, 99-00},
abstract = {Labels in the web of data are the key element for humans to access the data. We introduce a framework to measure the coverage of information with labels. The framework is based on a set of metrics including completeness, unambiguity, multilinguality, labeled object usage, and monolingual islands. We apply this framework on seven diverse datasets, from the web of data, a collaborative knowledge base, open governmental and GLAM data. We gain an insight into the current state of labels and multilinguality on the web of data. Comparing a set of differently sourced datasets can help data publishers to understand what they can improve and what other ways of collecting and data can be adopted.}
}
@article{SAFADI20171684,
title = {Mapping for the Future: Business Intelligence Tool to Map Regional Housing Stock},
journal = {Procedia Engineering},
volume = {180},
pages = {1684-1694},
year = {2017},
note = {International High-Performance Built Environment Conference – A Sustainable Built Environment Conference 2016 Series (SBE16), iHBE 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.04.331},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817318386},
author = {Murad Safadi and Jun Ma and Rohan Wickramasuriya and Daniel Daly and Pascal Perez and Georgios Kokogiannakis},
keywords = {energy epidemiology, housing stock mapping, energy, visualisation, built environment},
abstract = {The amount of data available and the lack of data integration represent an increasing challenge to effective planning for government agencies. Integration of data from multiple sources has the potential to enable a user to draw valuable insights, which can be used to enhance service targeting and delivery, and to improve program evaluation. In recognition of the need to improve data integration the University of Wollongong and the NSW Office of Environment and Heritage (OEH) partnered to create an integrated housing stock database for the Illawarra region. The database serves as the backbone for an online and interactive Housing Stock Mapping Dashboard (HSMD). It assembled multilevel granular information (including at the Statistical Area Level 1 (SA1) and Local Government Area (LGA) level) collected from multiple historical programs by multiple agencies. This centralised, integrated data repository can help agencies understand the existing housing stock, and improve access to information to support evidence-based policy. This paper presents a model of how data can be integrated from multiple agencies to provide an online collaboration platform. The platform, HSMD, was designed to demonstrate to government, industry, and the research community the opportunity of data integration and advanced analytics. Potential applications of the HSMD include characterisation of the existing housing stock according to a range of building attributes, for instance the presence of ceiling insulation or rainwater tanks. Comparison of these attributes with energy consumption data can indicate the influence of the attribute, or the impact of a specific intervention. This can help policy makers understand uptake and penetration of previous rebate schemes.}
}
@article{REUTER2017487,
title = {Benefit Oriented Production Data Acquisition for the Production Planning and Control},
journal = {Procedia CIRP},
volume = {61},
pages = {487-492},
year = {2017},
note = {The 24th CIRP Conference on Life Cycle Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.11.142},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116313026},
author = {Christina Reuter and Felix Brambring and Thomas Hempel and Phil Kopp},
keywords = {data acquisition, production control, traceability},
abstract = {In order to stay competitive, many manufacturing companies, especially small and medium sized enterprises (SME), face the challenge to transform their production and the corresponding production planning and control (PPC) processes for the upcoming Internet of Things (IoT) Era. Since their time and cost budget is a constraint, SME need to focus particularly on relevant data types, data acquisition points and technologies that will be beneficial for their manufacturing processes. State of the art approaches are lacking in supporting SME sustainably, systematically and company-specific on their way to IoT from a PPC-perspective. Therefore, in this paper a systematic approach is described, which is providing a sustainable, benefit-oriented, and gradual guideline for companies which aim to build an IoT-supported production data acquisition for PPC processes, in order to enable sustainable manufacturing. The developed method is taking into account company-specific production structures through quantitative key performance indicators and qualitative morphological checklists. With the help of the approach proposed in this paper, SMEs are supported systematically on their transformation path of the production data acquisition for the PPC into the IoT Era in order to enable a sustainable production.}
}
@article{201883,
title = {Literature listing},
journal = {World Patent Information},
volume = {55},
pages = {83-98},
year = {2018},
note = {Advanced Analytics of Intellectual Property Information for TechMining},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0172219018301108}
}
@article{ZHANG20171412,
title = {Knowledge management of eco-industrial park for efficient energy utilization through ontology-based approach},
journal = {Applied Energy},
volume = {204},
pages = {1412-1421},
year = {2017},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.03.130},
url = {https://www.sciencedirect.com/science/article/pii/S0306261917303756},
author = {Chuan Zhang and Alessandro Romagnoli and Li Zhou and Markus Kraft},
keywords = {Eco-industrial park, Knowledge management, Ontology, Energy efficiency, Waste heat recovery systems, Industrial symbiosis},
abstract = {An ontology-based approach for Eco-Industrial Park (EIP) knowledge management is proposed in this paper. The designed ontology in this study is formalized conceptualization of EIP. Based on such an ontological representation, a Knowledge-Based System (KBS) for EIP energy management named J-Park Simulator (JPS) is developed. By applying JPS to the solution of EIP waste heat utilization problem, the results of this study show that ontology is a powerful tool for knowledge management of complex systems such as EIP. The ontology-based approach can increase knowledge interoperability between different companies in EIP. The ontology-based approach can also allow intelligent decision making by using disparate data from remote databases, which implies the possibility of self-optimization without human intervention scenario of Internet of Things (IoT). It is shown through this study that KBS can bridge the communication gaps between different companies in EIP, sequentially more potential Industrial Symbiosis (IS) links can be established to improve the overall energy efficiency of the whole EIP.}
}
@article{ISMAIL201833,
title = {Mining productive-periodic frequent patterns in tele-health systems},
journal = {Journal of Network and Computer Applications},
volume = {115},
pages = {33-47},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518301474},
author = {Walaa N. Ismail and Mohammad Mehedi Hassan and Hessah A. Alsalamah and Giancarlo Fortino},
keywords = {Tele-health, Data mining, Productive periodic frequent patterns, Periodic patterns, Incremental database, Fp-growth},
abstract = {Recently, tele-health systems have gained attention from vast research fields because they facilitate remote monitoring of patients (e.g. vital sign data, physical activities, etc.) by utlizing various technologies such as body sensor network, wireless communications, multimedia and human-computer interactions without interrupting the quality of lifestyle. As tele-health generates a huge amount of healthcare data consisting of much useful information, finding hidden information from the data is an important task. The purpose of this work is to facilitate a real-time warning alarm in the context of tele-health remote monitoring using data mining techniques. This can be utilized for the e-wellbeing applications, for example, rehabilitation, early identification of therapeutic issues and emergency warning. In particular, we focus on mining Productive Periodic frequent patterns from incremental databases (such as vital sign data of patients) for various decision makings. Exploring the correlations between periodic frequent vital sign data or items is important since the inherent relationships between the items of patterns are relevant. To mine the correlated periodic frequent patterns from incremental databases, we introduce the productive (i.e. useful) periodic frequent patterns (PPFP) as the set of periodic frequent patterns with periodicities that result from the occurrence of correlated items. We finally design and develop an efficient PPFP mining technique that can mine the complete set of useful periodically occurring patterns in incremental databases. Numerous experiments were performed on both real and synthetic data set to judge the effectiveness of the proposed pattern mining procedure when contrasted with existing best in class approaches.}
}