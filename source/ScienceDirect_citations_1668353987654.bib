@article{LEE20221728,
title = {Quality assurance of integrative big data for medical research within a multihospital system},
journal = {Journal of the Formosan Medical Association},
volume = {121},
number = {9},
pages = {1728-1738},
year = {2022},
issn = {0929-6646},
doi = {https://doi.org/10.1016/j.jfma.2021.12.024},
url = {https://www.sciencedirect.com/science/article/pii/S092966462100591X},
author = {Yi-Chia Lee and Ying-Ting Chao and Pei-Ju Lin and Yen-Yun Yang and Yu-Cih Yang and Cheng-Chieh Chu and Yu-Chun Wang and Chin-Hao Chang and Shu-Lin Chuang and Wei-Chun Chen and Hsing-Jen Sun and Hsin-Cheng Tsou and Cheng-Fu Chou and Wei-Shiung Yang},
keywords = {Big data, Electronic health record, Evidence based healthcare management, Validation study},
abstract = {Background
The need is growing to create medical big data based on the electronic health records collected from different hospitals. Errors for sure occur and how to correct them should be explored.
Methods
Electronic health records of 9,197,817 patients and 53,081,148 visits, totaling about 500 million records for 2006–2016, were transmitted from eight hospitals into an integrated database. We randomly selected 10% of patients, accumulated the primary keys for their tabulated data, and compared the key numbers in the transmitted data with those of the raw data. Errors were identified based on statistical testing and clinical reasoning.
Results
Data were recorded in 1573 tables. Among these, 58 (3.7%) had different key numbers, with the maximum of 16.34/1000. Statistical differences (P < 0.05) were found in 34 (58.6%), of which 15 were caused by changes in diagnostic codes, wrong accounts, or modified orders. For the rest, the differences were related to accumulation of hospital visits over time. In the remaining 24 tables (41.4%) without significant differences, three were revised because of incorrect computer programming or wrong accounts. For the rest, the programming was correct and absolute differences were negligible. The applicability was confirmed using the data of 2,730,883 patients and 15,647,468 patient-visits transmitted during 2017–2018, in which 10 (3.5%) tables were corrected.
Conclusion
Significant magnitude of inconsistent data does exist during the transmission of big data from diverse sources. Systematic validation is essential. Comparing the number of data tabulated using the primary keys allow us to rapidly identify and correct these scattered errors.}
}
@article{SOUZA2022104242,
title = {Multisource and temporal variability in Portuguese hospital administrative datasets: data quality implications},
journal = {Journal of Biomedical Informatics},
pages = {104242},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104242},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422002477},
author = {Júlio Souza and Ismael Caballero and João {Vasco Santos} and Mariana {Fernandes Lobo} and Andreia Pinto and João Viana and Carlos Sáez and Fernando Lopes and Alberto Freitas},
keywords = {Data Quality, Clinical Coding, Data Variability, Clinical Classification Software, International Classification of Diseases},
abstract = {Background
Unexpected variability across healthcare datasets may indicate data quality issues and thereby affect the credibility of these data for reutilization. No gold-standard reference dataset or methods for variability assessment are usually available for these datasets. In this study, we aim to describe the process of discovering data quality implications by applying a set of methods for assessing variability between sources and over time in a large hospital database.
Methods
We described and applied a set of multisource and temporal variability assessment methods in a large Portuguese hospitalization database, in which variation in condition-specific hospitalization ratios derived from clinically coded data were assessed between hospitals (sources) and over time. We identified condition-specific admissions using the Clinical Classification Software (CCS), developed by the Agency of Health Care Research and Quality. A Statistical Process Control (SPC) approach based on funnel plots of condition-specific standardized hospitalization ratios (SHR) was used to assess multisource variability, whereas temporal heat maps and Information-Geometric Temporal (IGT) plots were used to assess temporal variability by displaying temporal abrupt changes in data distributions. Results were presented for the 15 most common inpatient conditions (CCS) in Portugal. Main Findings Funnel plot assessment allowed the detection of several outlying hospitals whose SHRs were much lower or higher than expected. Adjusting SHR for hospital characteristics, beyond age and sex, considerably affected the degree of multisource variability for most diseases. Overall, probability distributions changed over time for most diseases, although heterogeneously. Abrupt temporal changes in data distributions for acute myocardial infarction and congestive heart failure coincided with the periods comprising the transition to the International Classification of Diseases, 10th revision, Clinical Modification, whereas changes in the Diagnosis-Related Groups software seem to have driven changes in data distributions for both acute myocardial infarction and liveborn admissions. The analysis of heat maps also allowed the detection of several discontinuities at hospital level over time, in some cases also coinciding with the aforementioned factors.
Conclusions
This paper described the successful application of a set of reproducible, generalizable and systematic methods for variability assessment, including visualization tools that can be useful for detecting abnormal patterns in healthcare data, also addressing some limitations of common approaches. The presented method for multisource variability assessment is based on SPC, which is an advantage considering the lack of gold standard for such process. Properly controlling for hospital characteristics and differences in case-mix for estimating SHR is critical for isolating data quality-related variability among data sources. The use of IGT plots provides an advantage over common methods for temporal variability assessment due its suitability for multitype and multimodal data, which are common characteristics of healthcare data. The novelty of this work is the use of a set of methods to discover new data quality insights in healthcare data.}
}
@incollection{GULERIA2022179,
title = {Chapter 15 - Predictions on diabetic patient datasets using big data analytics and machine learning techniques},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {179-199},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00018-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000182},
author = {Pratiyush Guleria},
keywords = {Analytics, Classification, Clustering, Decision, Diabetic, Healthcare},
abstract = {Big data analytics and machine learning are the promising fields of the present time and playing important role in the healthcare sector. Big data analytical techniques help in analyzing a huge volume of data which may be in structured, semistructured, or unstructured form, and extract meaningful information for effective decision-making. Machine learning techniques help in performing predictions with the trained models on the input datasets and perform classification, clustering of data. In this chapter, the author has performed data analysis on diabetic patients dataset categorical in nature using big data analytical techniques, i.e., MapReduce, Apache Pig, Apache Hive, Apache Spark, and their architectures are discussed. Apart from big data analytics, machine learning techniques, i.e., K-Nearest Neighbor, Decision Trees, Bagged Trees, are implemented on the female diabetic patient dataset which is categorical and numerical for performing predictions based on the attributes like Age, Body Mass Index, Glucose, Blood Pressure, etc. The sensitivity achieved by the decision tree is 61.2% which is higher compared to KNN and bagged tree, whereas the Specificity achieved by the KNN is 89.2% which is higher than the other two algorithms.}
}
@incollection{ZHANG20221681,
title = {Hashing-based just-in-time learning for big data quality prediction},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1681-1686},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50280-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596502803},
author = {Xinmin Zhang and Jiang Zhai and Zhihuan Song and Yuan Li},
keywords = {Virtual sensor, soft-sensor, big data quality prediction, hashing-based just-in-time modeling},
abstract = {In recent years, the just-in-time (JIT) predictive models have attracted considerable attention due to their ability to prevent degradation of prediction accuracy. However, one of their practical limitations is expensive computation, which becomes a major factor that prevents them from being used for big data quality prediction. This is because the JIT modeling methods need to update the local regression model using the relevant samples that are searched through the lineal scan of the database during online operation. To solve this issue, the present work proposes a novel hashing-based JIT (HbJIT) modeling method that is suitable for big data quality prediction. In HbJIT, a family of locality-sensitive hash functions is firstly used to hash big data into a set of buckets, in which similar samples are grouped on themselves. During online prediction, HbJIT looks up multiple buckets that have a high probability of containing similar samples of a query object through the intelligent probing scheme, uses the data objects in the buckets as the candidate set of the results, and then filters the candidate objects using a linear scan. After filtering, the most relevant samples are used to construct the local regression model to yield the prediction of the query object. By integrating the multi-probe hashing strategy into the JIT learning framework, HbJIT can not only deal with process nonlinearity and time-varying characteristics but also is applicable to large-scale industrial processes. Experimental results on real-world dataset have demonstrated that the proposed HbJIT is time-efficient in processing large-scale datasets, and greatly reduces the online prediction time without compromising on the prediction accuracy.}
}
@article{BRAVE2022481,
title = {The perils of working with big data, and a SMALL checklist you can use to recognize them},
journal = {Business Horizons},
volume = {65},
number = {4},
pages = {481-492},
year = {2022},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0007681321001178},
author = {Scott A. Brave and R. Andrew Butters and Michael Fogarty},
keywords = {Big data, Data analysis, Economic forecasting, Selection bias, Reporting lags, High-frequency data, Real-time forecasts, Leading indicator},
abstract = {The use of big data to help explain fluctuations in the broader economy and key business performance indicators is now so commonplace that in some instances it has even begun to rival more traditional measures. Big data sources can very often provide advantages when compared with these more traditional data sources, but with these advantages also come potential pitfalls. We lay out a checklist called SMALL that we have developed in order to help interested parties as they navigate the big data minefield. Based on a set of five questions, the SMALL checklist should help users of big data draw justifiable conclusions and avoid making mistakes in matters of interpretation. To demonstrate, we provide several case studies that demonstrate the subtle nuances of several of these new big data sets and show how the problems they face often closely relate to age-old concerns that more traditional data sources are also forced to tackle.}
}
@article{TANG2022e198,
title = {The National Inpatient Sample: A Primer for Neurosurgical Big Data Research and Systematic Review},
journal = {World Neurosurgery},
volume = {162},
pages = {e198-e217},
year = {2022},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2022.02.113},
url = {https://www.sciencedirect.com/science/article/pii/S1878875022002601},
author = {Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera Perla} and Robert J. Weil and Steven A. Toms},
keywords = {Big data, Disparities, Health care costs, Health policy, Hospital volume, Machine learning, National Inpatient Sample, Nationwide Inpatient Sample, NIS},
abstract = {Objective
The National Inpatient Sample (NIS) (the largest all-payer inpatient database in the United States) is an important instrument for big data analysis of neurosurgical inquiries. However, earlier research has determined that many NIS studies are limited by common methodological pitfalls. In this study, we provide the first primer of NIS methodological procedures in the setting of neurosurgical research and review all reported neurosurgical studies using the NIS.
Methods
We designed a protocol for neurosurgical big data research using the NIS, based on our subject matter expertise, NIS documentation, and input and verification from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive search strategy to identify all neurosurgical studies using the NIS in the PubMed and MEDLINE, Embase, and Web of Science databases from inception to August 2021. Studies underwent qualitative categorization (years of NIS studied, neurosurgical subspecialty, age group, and thematic focus of study objective) and analysis of longitudinal trends.
Results
We identified a canonical, 4-step protocol for NIS analysis: study population selection; defining additional clinical variables; identification and coding of outcomes; and statistical analysis. Methodological nuances discussed include identifying neurosurgery-specific admissions, addressing missing data, calculating additional severity and hospital-specific metrics, coding perioperative complications, and applying survey weights to make nationwide estimates. Inherent database limitations and common pitfalls of NIS studies discussed include lack of disease process–specific variables and data after the index admission, inability to calculate certain hospital-specific variables after 2011, performing state-level analyses, conflating hospitalization charges and costs, and not following proper statistical methodology for performing survey-weighted regression. In a systematic review, we identified 647 neurosurgical studies using the NIS. Although almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data after 2015. The average sample size of studies was 507,352 patients (standard deviation = 2,739,900). Most studies analyzed cranial procedures (58.1%) and adults (68.1%). The most prevalent topic areas analyzed were surgical outcome trends (35.7%) and health policy and economics (17.8%), whereas patient disparities (9.4%) and surgeon or hospital volume (6.6%) were the least studied.
Conclusions
We present a standardized methodology to analyze the NIS, systematically review the state of the NIS neurosurgical literature, suggest potential future directions for neurosurgical big data inquiries, and outline recommendations to improve the design of future neurosurgical data instruments.}
}
@article{CHO2022102477,
title = {What's driving the diffusion of next-generation digital technologies?},
journal = {Technovation},
pages = {102477},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102477},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222000244},
author = {Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik},
abstract = {The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.}
}
@article{MOSTEFAOUI2022374,
title = {Big data architecture for connected vehicles: Feedback and application examples from an automotive group},
journal = {Future Generation Computer Systems},
volume = {134},
pages = {374-387},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001492},
author = {Ahmed Mostefaoui and Mohammed Amine Merzoug and Amir Haroun and Anthony Nassar and François Dessables},
keywords = {Connected vehicles, V2I communication, Automotive big data, Big data architecture, Groupe PSA},
abstract = {Nowadays, using their onboard built-in sensors and communication devices, connected vehicles (CVs) can perform numerous measurements (speed, temperature, fuel consumption, etc.) and transmit them, in a real-time fashion, to dedicated infrastructure, usually via 4G/5G wireless communications. This raises many opportunities to develop new innovative telematics services, including, among others, driver safety, customer experience, location-based services and infotainment. Indeed, it is expected that there will be roughly 2 billion connected cars by the end of 2025 on the world’s roadways, where each of which can produce up to 30 terabytes of data per day. Managing this big automotive data, in real and batch modes, imposes tight constraints on the underlying data management platform. To contribute to this research area, in this paper, we report on a real, in-production automotive big data platform; specifically, the one deployed by Groupe PSA (a French car manufacturer known also as Peugeot-Citroën). In particular, we present the technologies and open-source products used within the different components of this CV platform to gather, store, process, and leverage big automotive data. The proposed architecture is then assessed through realistic experiments, and the obtained results are reported and analyzed. Finally, we also provide examples of deployed automotive applications and reveal the implementation details of one of them (an eco-driving service).}
}
@article{LI2022119292,
title = {Data cleaning and restoring method for vehicle battery big data platform},
journal = {Applied Energy},
volume = {320},
pages = {119292},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119292},
url = {https://www.sciencedirect.com/science/article/pii/S030626192200647X},
author = {Shuangqi Li and Hongwen He and Pengfei Zhao and Shuang Cheng},
keywords = {Big data, Internet of vehicle, Electric vehicles, Data cleaning, Battery management system, Battery state estimation},
abstract = {Battery is one of the most important and costly devices in electric vehicles (EVs). Developing an efficient battery management method is of great significance to enhancing vehicle safety and economy. Recently developed big-data and cloud platform computing technologies bring a bright perspective for efficient utilization and protection of vehicle batteries. However, a reliable data transmission network and a high-quality cloud battery dataset are indispensable to enable this benefit. This paper makes the first effort to systematically solve data quality problems in cloud-based vehicle battery monitoring and management by developing a novel integrated battery data cleaning framework. In the first stage, the outlier samples are detected by analyzing the temporal features in the battery data time series. The outlier data in the dataset can be accurately detected to avoid their impacts on battery monitoring and management. Then, the abnormal samples, including the noise polluted data and missing value, are restored by a novel future fusion data restoring model. The real electric bus operation data collected by a cloud-based battery monitoring and management platform are used to verify the performance of the developed data cleaning method. More than 93.3% of outlier samples can be detected, and the data restoring error can be limited to 2.11%, which validates the effectiveness of the developed methods. The proposed data cleaning method provides an effective data quality assessment tool in cloud-based vehicle battery management, which can further boost the practical application of the vehicle big data platform and Internet of vehicle.}
}
@incollection{BISWAS202263,
title = {Chapter 6 - Big data analytics in precision medicine},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {63-72},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000054},
author = {Saurabh Biswas and Yasha Hasija},
keywords = {Big data, EHR, Healthcare, Omics, Precision medicine},
abstract = {Precision medicine is a medical model that recommends custom-tailored products, techniques, treatments, and decisions for a subgroup of patients having the same biological basis of diseases. Due to the huge size and complexity of omics data and dataset of patient features, they cannot be analyzed directly by doctors. Big data is a term used for complex or large datasets that cannot be accurately processed or stored by traditional management tools. Omics and electronic health record (EHR) data are essential big biomedical data having a strong association with precision medicine. In this chapter, we review the importance of analyzing EHR and omics data in precision medicine. Big data analytics has been applied to healthcare in biomarker discovery and disease subtyping, drug repurposing, and integrating omics data into EHR. This will provide the most appropriate and efficient treatment to every patient on the basis of their subtyping data.}
}
@article{SHU2022102817,
title = {Knowledge Discovery: Methods from data mining and machine learning},
journal = {Social Science Research},
pages = {102817},
year = {2022},
issn = {0049-089X},
doi = {https://doi.org/10.1016/j.ssresearch.2022.102817},
url = {https://www.sciencedirect.com/science/article/pii/S0049089X22001284},
author = {Xiaoling Shu and Yiwan Ye},
keywords = {Knowledge discovery, data mining, machine learning, causal discovery, predition, big data},
abstract = {The interdisciplinary field of knowledge discovery and data mining emerged from a necessity of big data requiring new analytical methods beyond the traditional statistical approaches to discover new knowledge from the data mine. This emergent approach is a dialectic research process that is both deductive and inductive. The data mining approach automatically or semi-automatically considers a larger number of joint, interactive, and independent predictors to address causal heterogeneity and improve prediction. Instead of challenging the conventional model-building approach, it plays an important complementary role in improving model goodness of fit, revealing valid and significant hidden patterns in data, identifying nonlinear and non-additive effects, providing insights into data developments, methods, and theory, and enriching scientific discovery. Machine learning builds models and algorithms by learning and improving from data when the explicit model structure is unclear and algorithms with good performance are difficult to attain. The most recent development is to incorporate this new paradigm of predictive modeling with the classical approach of parameter estimation regressions to produce improved models that combine explanation and prediction.}
}
@article{MOSTAFA2022100363,
title = {Renewable energy management in smart grids by using big data analytics and machine learning},
journal = {Machine Learning with Applications},
volume = {9},
pages = {100363},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100363},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000597},
author = {Noha Mostafa and Haitham Saad Mohamed Ramadan and Omar Elfarouk},
keywords = {Energy internet, Renewable energy, Smart grid, Big data analytics, Machine learning, Predictive models},
abstract = {The application of big data in the energy sector is considered as one of the main elements of Energy Internet. Crucial and promising challenges exist especially with the integration of renewable energy sources and smart grids. The ability to collect data and to properly use it for better decision-making is a key feature; in this work, the benefits and challenges of implementing big data analytics for renewable energy power stations are addressed. A framework was developed for the potential implementation of big data analytics for smart grids and renewable energy power utilities. A five-step approach is proposed for predicting the smart grid stability by using five different machine learning methods. Data from a decentralized smart grid data system consisting of 60,000 instances and 12 attributes was used to predict the stability of the system through three different machine learning methods. The results of fitting the penalized linear regression model show an accuracy of 96% for the model implemented using 70% of the data as a training set. Using the random forest tree model has shown 84% accuracy, and the decision tree model has shown 78% accuracy. Both the convolutional neural network model and the gradient boosted decision tree model yielded 87% for the classification model. The main limitation of this work is that the amount of data available in the dataset is considered relatively small for big data analytics; however the cloud computing and real-time event analysis provided was suitable for big data analytics framework. Future research should include bigger datasets with variety of renewable energy sources and demand across more countries.}
}
@article{OESTERREICH2022103685,
title = {What translates big data into business value? A meta-analysis of the impacts of business analytics on firm performance},
journal = {Information & Management},
volume = {59},
number = {6},
pages = {103685},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103685},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000945},
author = {Thuy Duong Oesterreich and Eduard Anton and Frank Teuteberg},
keywords = {Business analytics, Business intelligence, IT business value, Firm performance, Meta-analysis, Moderator analysis},
abstract = {The main purpose of this study is to examine the factors that are critical to create business value from business analytics (BA). Therefore, we conduct a meta-analysis of 125 firm-level studies spanning ten years of research from across 26 countries. We found evidence that the social factors of BA, such as human resources, management capabilities, and organizational culture show a greater impact on business value, whereas technical aspects play a minor role in enhancing firm performance. Through these findings, we contribute to the ongoing debate concerning BA business value by synthesizing and validating the findings of the body of knowledge.}
}
@article{WEERASINGHE2022121222,
title = {Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121222},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121222},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006557},
author = {Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin},
keywords = {Analytics, Clinical decision-making, Big data, Healthcare, Social representation theory},
abstract = {The introduction and use of ‘big data and analytics’ is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.}
}
@article{OUAFIQ20224123,
title = {Data Architecture and Big Data Analytics in Smart Cities},
journal = {Procedia Computer Science},
volume = {207},
pages = {4123-4131},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.475},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922013710},
author = {El Mehdi Ouafiq and Mourad Raif and Abdellah Chehri and Rachid Saadane},
keywords = {Smart cities, big data analytics, Drone, IoT, Data Management},
abstract = {The smart city has become a persistent need and is no longer just a concept. The concept of smart cities heavily relies on collecting enormous amounts of data. This paper proposes a data-management-based solution for smart city, which is labeled Smart Systems Oriented Big Data Architecture. Big data technologies have become essential to the functioning of cities. The architecture includes complex components to be implemented based on the architectural requirements. A data migration strategy was proposed to handle the various data sources such as IoT devices, video cameras, and drones. The proposed approach also takes into account data processing and data storage. The technical constraints related to data processing in a big-data environment are also studied. We also consider data modeling from a business intelligence point of view and a data science perspective. Our main goal is to favor the facilitation of the daily life practices in the context of a smart city by providing the city administrators with a solution that helps them maintain their city smartly and effectively.}
}
@article{LI202245,
title = {Spatially gap free analysis of aerosol type grids in China: First retrieval via satellite remote sensing and big data analytics},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {193},
pages = {45-59},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622002374},
author = {Ke Li and Kaixu Bai and Mingliang Ma and Jianping Guo and Zhengqiang Li and Gehui Wang and Ni-Bin Chang},
keywords = {Aerosol types, Fine mode fraction, Haze reduction, Satellite remote sensing, Big data analytics},
abstract = {Spatially contiguous aerosol type grids were rarely available for air quality management in the past. To bridge the gap, we developed an integrated remote sensing and big data analytics framework to generate spatially gap-free aerosol type grids between 2000 and 2020 in China. The effect of emission control via environmental management on haze reduction was fully realized for the first time with the aid of satellite-based gap-free aerosol type data. Daily gap-free aerosol fine mode fraction (FMF) data were first derived via a data-driven regression model based on remote sensing big data. According to empirically determined FMF probability distributions over regions with typical emission sources, aerosols in China were classified into eight major types, including typical/atypical/mixed anthropogenic aerosols, typical/atypical/mixed dust, and typical mixed and multiple modes. The results indicated that the gridded FMF estimates derived in this study agreed well with FMF retrievals from AERONET, with correlation coefficient of 0.81 and root mean square error of 0.13. The long-term variations in major aerosol types showed that in China the territory covered by typical anthropogenic aerosols was reduced from 21.38% to 11.76% over the past two decades, while dust aerosols decreased from 6.99% to 2.15%. The declining trend in anthropogenic aerosols could be attributed to reduced coal consumption and/or favorable dispersion conditions, whereas decreasing dust aerosols were largely associated with increased vegetation cover and/or weakened wind speed in the west. Overall, such advancements provide fresh evidence to improve our understanding of the emission control effect on haze pollution variations in China.}
}
@incollection{SHANG2022203,
title = {Chapter 7 - Data mining technologies for Mobility-as-a-Service (MaaS)},
editor = {Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
booktitle = {Big Data and Mobility as a Service},
publisher = {Elsevier},
pages = {203-228},
year = {2022},
isbn = {978-0-323-90169-7},
doi = {https://doi.org/10.1016/B978-0-323-90169-7.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901697000087},
author = {Wen-Long Shang and Haoran Zhang and Yi Sui},
keywords = {MaaS, Big data, Data mining, Support vector machine, Linear regression, Decision tree, Clustering, Bike-sharing, COVID-19},
abstract = {This chapter mainly introduces big data technologies for MaaS. Firstly, the development, definition, and purpose of MaaS and the significance of data mining technologies for MaaS are introduced briefly. Following this, the definition of data mining, its processing objects, classical steps and processes, and types of traffic big data are reviewed. Afterward, data mining technologies such as support vector machine, linear regression, decision tree, and clustering analysis are introduced. Finally, a case study of data mining technology used for bike-sharing in Beijing during the Covid-19 pandemic is presented to demonstrate the role of data mining technologies in travel behaviors. This chapter mainly provides a clue or reference for the exploration of big data analysis of MaaS.}
}
@article{PENG2022102674,
title = {The relationship between soil microbial diversity and angelica planting based on network big data},
journal = {Sustainable Energy Technologies and Assessments},
volume = {53},
pages = {102674},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102674},
url = {https://www.sciencedirect.com/science/article/pii/S2213138822007238},
author = {Yinan Peng and Ze Ye and Peng Xi and Hongshan Qi and Bin Ji and Zhiye Wang},
keywords = {Big data, Soil microorganisms, Biodiversity analysis, Angelica cultivation, Hadoop systems, Metagenome research},
abstract = {Angelica sinensis is a kind of traditional Chinese medicine with very good blood nourishing effect, and it is cultivated in many regions of China. But with the increasingly severe climate, angelica cultivation has become a big problem. Therefore, this paper starts from the soil microorganisms of angelica planting, and studies the influence of soil biodiversity and angelica planting in the context of big data. This paper proposes a Hadoop system for big data analysis, combining the biocommunity characteristics and metagenomes of soil microorganisms. It then calculates the distance between samples and generates a dissimilarity matrix. Finally, this paper proposes a soil optimization method for angelica planting based on big data analysis of soil microorganisms. In order to optimize the Angelica planting soil designed in this paper, a soil microbial genome comparison experiment and a big data concurrent control test experiment were designed in this paper. It then analyzes the data obtained from the experiment, and the results of the analysis are used to optimize the soil for angelica planting. It finally compares the soil method of Angelica planting designed in this paper with the traditional Angelica planting method. The experimental results show that the survival rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms has increased by 16.09% compared with the traditional Angelica sinensis planting site. The growth rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms increased by 9.64% compared with the traditional Angelica sinensis planting area.}
}
@incollection{DOMINGUEZ2022,
title = {Big Data applications in power systems},
booktitle = {Reference Module in Materials Science and Materials Engineering},
publisher = {Elsevier},
year = {2022},
isbn = {978-0-12-803581-8},
doi = {https://doi.org/10.1016/B978-0-12-821204-2.00073-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212042000738},
author = {Xavier Dominguez and Alvaro Prado and Pablo Arboleya},
keywords = {Artificial Intelligence, Big Data, Big Data Analytics, Data mining, Machine learning, Power systems},
abstract = {Abstract
In the last years, the term Big Data (BD) is becoming ubiquitous in almost every scientific field given the information era we are living. In this respect, power systems is not the exception. This overflood of information can be overwhelming for the different energy stakeholders, nonetheless, numerous opportunities can also be derived from this BD. To do so, it is necessary to perform the corresponding knowledge extraction to this large data. This has been referred as Big Data Analytics (BDA) in the recent literature. In this respect, this work provides a comprehensive understanding of the attributes of BD in the power sector, along with the most representative tools, techniques, applications and challenges related to the incorporation of BDA in this field.}
}
@article{YIN2022104285,
title = {Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning},
journal = {Tunnelling and Underground Space Technology},
volume = {120},
pages = {104285},
year = {2022},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2021.104285},
url = {https://www.sciencedirect.com/science/article/pii/S0886779821004764},
author = {Xin Yin and Quansheng Liu and Xing Huang and Yucong Pan},
keywords = {TBM, Surrounding rock class, Perception model, Unsupervised learning, Supervised learning},
abstract = {The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability.}
}
@article{ULLAH2022103294,
title = {On the scalability of Big Data Cyber Security Analytics systems},
journal = {Journal of Network and Computer Applications},
volume = {198},
pages = {103294},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103294},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002897},
author = {Faheem Ullah and M. Ali Babar},
keywords = {Big data, Cyber security, Adaptation, Scalability, Configuration parameter, Spark},
abstract = {Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics.}
}
@article{CHEN2022157581,
title = {Quantifying on-road vehicle emissions during traffic congestion using updated emission factors of light-duty gasoline vehicles and real-world traffic monitoring big data},
journal = {Science of The Total Environment},
volume = {847},
pages = {157581},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.157581},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722046794},
author = {Xue Chen and Linhui Jiang and Yan Xia and Lu Wang and Jianjie Ye and Tangyan Hou and Yibo Zhang and Mengying Li and Zhen Li and Zhe Song and Jiali Li and Yaping Jiang and Pengfei Li and Xiaoye Zhang and Yang Zhang and Daniel Rosenfeld and John H. Seinfeld and Shaocai Yu},
keywords = {On-road vehicle emissions, Traffic congestion, Light-duty gasoline vehicles, Real-world, Big data, Emission factors},
abstract = {Light-duty gasoline vehicles (LDGVs) have made up >90 % of vehicle fleets in China since 2019, moreover, with a high annual growth rate (> 10 %) since 2017. Hence, accurate estimates of air pollutant emissions of these fast-changing LDGVs are vital for air quality management, human healthcare, and ecological protection. However, this issue is poorly quantified due to insufficient reserves of timely updated LDGV emission factors, which are dependent on real-world activity levels. Here we constructed a big dataset of explicit emission profiles (e.g., emission factors and accumulated mileages) for 159,051 LDGVs based on an official I/M database by matching real-time traffic dynamics via real-world traffic monitoring (e.g., traffic volumes and speeds). Consequently, we provide robust evidence that the emission factors of these LDGVs follow a clear heavy-tailed distribution. The top 10 % emitters contributed >60 % to the total fleet emissions, while the bottom 50 % contributed <10 %. Such emission factors were effectively reduced by 75.7–86.2 % as official emission standards upgraded gradually (i.e., from China 2 to China 5) within 13 years from 2004 to 2017. Nevertheless, such achievements would be offset once traffic congestion occurred. In the real world, the typical traffic congestions (i.e., vehicle speed <5 km/h) can lead to emissions 5– 9 times higher than those on non-congested roads (i.e., vehicle speed >50 km/h). These empirical analyses enabled us to propose future traffic scenarios that could harmonize emission standards and traffic congestion. Practical approaches on vehicle emission controls under realistic conditions are proposed, which would provide new insights for future urban vehicle emission management.}
}
@incollection{NOH2022639,
title = {20 - Big data analysis for civil infrastructure sensing},
editor = {Jerome P. Lynch and Hoon Sohn and Ming L. Wang},
booktitle = {Sensor Technologies for Civil Infrastructures (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {639-677},
year = {2022},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-08-102706-6},
doi = {https://doi.org/10.1016/B978-0-08-102706-6.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081027066000076},
author = {Hae Young Noh and Jonathon Fagert},
keywords = {Big data, Civil infrastructure sensing, Damage diagnosis, Machine learning, Occupant monitoring, Smart infrastructure, Structural health monitoring},
abstract = {With the growing scale and complexity of city infrastructures, the need for data analysis and machine learning is becoming more and more prominent in the field of civil infrastructure sensing. This coupled with the explosion of available sensing data in smart cities and smart infrastructures has offered new opportunities like never before. Using big data tools at a structure level, we can understand important information about structural properties and damage states, city environmental and operational conditions, as well as an individual user or group patterns. In this chapter, we explore and provide guidance for big data analytics and its application to civil infrastructure problems. Furthermore, we discuss future directions and trends that will enable large-scale monitoring of civil infrastructure and smart cities.}
}
@article{HERRMANN2022194,
title = {An ERP Data Quality Assessment Framework for the Implementation of an APS system using Bayesian Networks},
journal = {Procedia Computer Science},
volume = {200},
pages = {194-204},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002277},
author = {Jan-Phillip Herrmann and Sven Tackenberg and Elio Padoano and Jörg Hartlief and Jens Rautenstengel and Christine Loeser and Jörg Böhme},
keywords = {Data Quality Assessment, Advanced Planning, Scheduling, Bayesian Network, Enterprise Resource Planning},
abstract = {In today’s manufacturing industry, enterprise-resource-planning (ERP) systems reach their limit when planning and scheduling production subject to multiple objectives and constraints. Advanced planning and scheduling (APS) systems provide these capabilities and are an extension for ERP systems. However, when integrating an APS and ERP system, the ERP data frequently lacks quality, hindering the APS system from working as required. This paper introduces a data quality (DQ) assessment framework that employs a Bayesian Network (BN) to perform quick DQ assessments based on expert interviews and DQ measurements with actual ERP data. We explain the BN’s functionality, design, and validation and show how using the perceived DQ of experts and a semi-supervised learning algorithm improves the BN’s predictions over time. We discuss applying our framework in an APS system implementation project involving an APS system provider and a medium-sized manufacturer of hydraulic cylinders. Despite considering the DQ assessment framework in such a specific context, it is not restricted to a particular domain. We close by discussing the framework’s limits, particularly the BN as a DQ assessment methodology and future works to improve its performance.}
}
@article{MORANFERNANDEZ2022365,
title = {How important is data quality? Best classifiers vs best features},
journal = {Neurocomputing},
volume = {470},
pages = {365-375},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.05.107},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221011127},
author = {Laura Morán-Fernández and Verónica Bólon-Canedo and Amparo Alonso-Betanzos},
keywords = {Feature selection, Filters, Preprocessing, High dimensionality, Classification, Data analysis},
abstract = {The task of choosing the appropriate classifier for a given scenario is not an easy-to-solve question. First, there is an increasingly high number of algorithms available belonging to different families. And also there is a lack of methodologies that can help on recommending in advance a given family of algorithms for a certain type of datasets. Besides, most of these classification algorithms exhibit a degradation in the performance when faced with datasets containing irrelevant and/or redundant features. In this work we analyze the impact of feature selection in classification over several synthetic and real datasets. The experimental results obtained show that the significance of selecting a classifier decreases after applying an appropriate preprocessing step and, not only this alleviates the choice, but it also improves the results in almost all the datasets tested.}
}
@incollection{LV2022247,
title = {Chapter 19 - Privacy security risks of big data processing in healthcare},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {247-263},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00020-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000200},
author = {Zhihan Lv and Liang Qiao},
keywords = {Big data, Cloud services, Healthcare, Privacy measures, Privacy security risk},
abstract = {This chapter aims to discuss healthcare's development in China and the privacy and security risk factors in medical data under big data. First, the development status of China's healthcare sector is analyzed. The questionnaire is made to analyze the privacy and security risk factors of healthcare big data (HBD) and protection measures are proposed according to the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions and medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all increase annually. In 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The questionnaire results reveal that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system are all greater than 0.8 when HBD is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to emphasize data privacy protection and grasp using digital medical data to provide decision support for subsequent medical data analysis.}
}
@incollection{ANGADI2022151,
title = {Chapter Seven - Role of big data analytic and machine learning in power system contingency analysis},
editor = {Rakesh Sehgal and Neeraj Gupta and Anuradha Tomar and Mukund Dutt Sharma and Vigna Kumaran},
booktitle = {Smart Electrical and Mechanical Systems},
publisher = {Academic Press},
pages = {151-184},
year = {2022},
isbn = {978-0-323-90789-7},
doi = {https://doi.org/10.1016/B978-0-323-90789-7.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390789700004X},
author = {Ravi V. Angadi and Suresh Babu Daram and P.S. Venkataramu},
keywords = {Big data, Classification, Contingency analysis, Critical, Data collection, Data mining, Data processing, Data visualization, Decision tree, Different load conditions, Energy systems, Machine learning, Non critical, Powersystem, Semi critical, Severity prediction, Structured data, Testing data set, Training data set, Voltage stability, Voltage stability index, Volume of data},
abstract = {This work discusses the use of big data and machine learning to predict the severity of a system breakdown caused by an n−1 transmission line condition. The contingency analysis is a key part of traditional energy management systems. The severity of the line is identified by computing the Line Voltage Stability Index The large amount of data handling will be involved during the contingency study. These data need to be processed and analyzed properly by data handling technique and the use of machine learning tools arrive required information in the system. The severity of transmission lines is predicted and compared using classification approaches. To create large data, MATLAB simulation results will be used and the machine learning tool, Weka is used to analyze the data and forecast transmission line. The standard IEEE 30 bus system is considered to understand the proposed methodology.}
}
@article{MUHEIDAT202215,
title = {Emerging Concepts Using Blockchain and Big Data},
journal = {Procedia Computer Science},
volume = {198},
pages = {15-22},
year = {2022},
note = {12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024455},
author = {Fadi Muheidat and Dhaval Patel and Spandana Tammisetty and Lo’ai A. Tawalbeh and Mais Tawalbeh},
keywords = {Blockchain, Bigdata, Data analytics, Smart Cities applications, Healthcare},
abstract = {Blockchain and big data are the emerging technologies that are on the highest agendas of the firms. These are significantly expected to transform the ways in which the business as well as the firm run. These are on the verge of increasing the expectation of the distributed ledgers, which would keep the firms away from struggling challenges. The concept of big data and Blockchain has been used up by various other concepts that would help secure and interpret the information. The ideal solutions offered by these technologies shall address the challenges of big data management as well as for analytics. In addition to that, Blockchain provides its own consensus method, which is the primary means to create an audit trail. This enables users to verify all transactions. The audit trail is a means of verifying the correctness and integrity of every transaction, regardless of who owns the asset. The Blockchain can also verify that different parties of a transaction are following an agreement and not breaking the agreement. Moreover, there have been continuous arguments in the concept of Blockchain at which the bitcoin is fundamental and there are several popular blockchain approaches developed which would deliver performance, security as well as privacy. Apart from this, the use of Blockchain plays a major role in adding an extra data layer for the big data analytics process. Big data is considered secure which cannot be further forged with the network architecture. The current paper shall be discussing the emerging concepts that are using Blockchain as well as big data.}
}
@article{FERNANDES2022817,
title = {Big Data Analytics for Vehicle Multisensory Anomalies Detection},
journal = {Procedia Computer Science},
volume = {204},
pages = {817-824},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008377},
author = {Ana Xavier Fernandes and Pedro Guimarães and Maribel Yasmina Santos},
keywords = {Big Data Warehouse, Big Data, ETL},
abstract = {Autonomous driving is assisted by different sensors, each providing information about certain parameters. What we are looking for is an integrated perspective of all these parameters to drive us into better decisions. To achieve this goal, a system that can handle these Big Data issues regarding volume, velocity and variety is needed. This paper aims to design and develop a real-time Big Data Warehouse repository, integrating the data generated by the multiple sensors developed in the context of IVS (In-Vehicle Sensing) systems; the data to be stored in this repository should be merged, which will imply its processing, consolidation and preparation for the analytical mechanisms that will be required. This multisensory fusion is important because it allows the integration of different perspectives in terms of sensor data, since they complement each other. Therefore, it can enrich the entire analysis process at the decision-making level, for instance, understanding what is going on inside the cockpit.}
}
@article{WU2022129487,
title = {Machine learning in the identification, prediction and exploration of environmental toxicology: Challenges and perspectives},
journal = {Journal of Hazardous Materials},
volume = {438},
pages = {129487},
year = {2022},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2022.129487},
url = {https://www.sciencedirect.com/science/article/pii/S0304389422012808},
author = {Xiaotong Wu and Qixing Zhou and Li Mu and Xiangang Hu},
keywords = {Machine learning, Chemical, Toxicity, Environmental health, Big data},
abstract = {Over the past few decades, data-driven machine learning (ML) has distinguished itself from hypothesis-driven studies and has recently received much attention in environmental toxicology. However, the use of ML in environmental toxicology remains in the early stages, with knowledge gaps, technical bottlenecks in data quality, high-dimensional/heterogeneous/small-sample data analysis and model interpretability, and a lack of an in-depth understanding of environmental toxicology. Given the above problems, we review the recent progress in the literature and highlight state-of-the-art toxicological studies using ML (such as learning and predicting toxicity in complicated biosystems and multiple-factor environmental scenarios of long-term and large-scale pollution). Beyond predicting simple biological endpoints by integrating untargeted omics and adverse outcome pathways, ML development should focus on revealing toxicological mechanisms. The integration of data-driven ML with other methods (e.g., omics analysis and adverse outcome pathway frameworks) endows ML with widely promising application in revealing toxicological mechanisms. High-quality databases and interpretable algorithms are urgently needed for toxicology and environmental science. Addressing the core issues and future challenges for ML in this review may narrow the knowledge gap between environmental toxicity and computational science and facilitate the control of environmental risk in the future.}
}
@article{SALEM20225552,
title = {LODQuMa: A Free-ontology process for Linked (Open) Data quality management},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part A},
pages = {5552-5563},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001348},
author = {Samah Salem and Fouzia Benchikha},
keywords = {Linked Open Data, Quality assessment, Quality improvement, Synonym predicates, Profiling statistics, DBpedia},
abstract = {For many years, data quality is among the most commonly discussed issue in Linked Open Data (LOD) due to the huge volume of integrated datasets that are usually heterogeneous. Several ontology-based approaches dealing with quality problems have been proposed. However, when datasets lack a well-defined schema, these approaches become ineffective because of the lack of metadata. Moreover, the detection of quality problems based on an analysis between RDF (Resource Description Framework) triples without requiring ontology statistical and semantical information is not addressed. Keeping in mind that ontologies are not always available and they may be incomplete or misused. In this paper, a novel free-ontology process called LODQuMa is proposed to assess and improve the quality of LOD. It is mainly based on profiling statistics, synonym relationships between predicates, QVCs (Quality Verification Cases), and SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on the DBpedia dataset demonstrate that the proposed process is effective for increasing the intrinsic quality dimensions, resulting in correct and compact datasets.}
}
@article{RADONJIC2022,
title = {Artificial intelligence and HRM: HR managers’ perspective on decisiveness and challenges},
journal = {European Management Journal},
year = {2022},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0263237322000883},
author = {Aleksandar Radonjić and Henrique Duarte and Nádia Pereira},
keywords = {Human resources, HR, HRM, e-HRM, Decision-making, Big data (BD), Big data maturity models (BDMM), Artificial intelligence (AI)},
abstract = {Focus
The transformative power of today's big data (BD) has allowed many companies, i.e., decision-makers, to evolve at an unprecedented pace. With regard to decision-making, artificial intelligence (AI) takes task delegation to a new level, and by employing AI-assisted tools, companies can provide their HR departments with the means to manage the existing data and HR altogether.
Objectives
To determine how HR managers assess whether BD management is facilitated by AI, and how they frame the changes necessary to meet the trends related to AI and its implementation, namely their willingness to master its implementation and to meet the possible challenges.
Methodology
Content analysis was conducted on interviews held with a sample of 16 HR practitioners from a spectrum of areas, and the findings were analysed using the big data maturity model (BDMM) framework. Domains covered by this model allow the study of decision-making trends, in terms of preparedness and willingness to tackle disruptive technology with the aim of improving and gaining the competitive edge in decision-making.
Findings
The central potential of AI lies in faster data storage and processing power, thereby leading to more insightful and effective decision-making. This article contains closer insights into the challenges underlying the implementation of AI in decision-making processes, specifically in terms of strategic alignment, governance, and implementation. The results reflect the notions regarding the nature of AI – in assisting HR – and lay out the path that precedes the extraction of BD, through the delivery of advantageous intelligence, to augment decision-making in HR.}
}
@article{KESKAR2022532,
title = {Perspective of anomaly detection in big data for data quality improvement},
journal = {Materials Today: Proceedings},
volume = {51},
pages = {532-537},
year = {2022},
note = {CMAE'21},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.597},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321042243},
author = {Vinaya Keskar and Jyoti Yadav and Ajay Kumar},
keywords = {Credit card, Validation, LUHN, Big data, Bank},
abstract = {The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.}
}
@article{CHUNG2022100087,
title = {BLASTNet: A call for community-involved big data in combustion machine learning},
journal = {Applications in Energy and Combustion Science},
volume = {12},
pages = {100087},
year = {2022},
issn = {2666-352X},
doi = {https://doi.org/10.1016/j.jaecs.2022.100087},
url = {https://www.sciencedirect.com/science/article/pii/S2666352X22000309},
author = {Wai Tong Chung and Ki Sung Jung and Jacqueline H. Chen and Matthias Ihme},
keywords = {Big data, Deep learning, Direct numerical simulation, BLASTNet},
abstract = {Many state-of-the-art machine learning (ML) fields rely on large datasets and massive deep learning models (with O(109) trainable parameters) to predict target variables accurately without overfitting. Within combustion, a wealth of data exists in the form of high-fidelity simulation data and detailed measurements that have been accumulating since the past decade. Yet, this data remains distributed and can be difficult to access. In this work, we present a realistic and feasible framework which combines (i) community involvement, (ii) public data repositories, and (iii) lossy compression algorithms for enabling broad access to high-fidelity data via a network-of-datasets approach. This Bearable Large Accessible Scientific Training Network-of-Datasets (BLASTNet) is consolidated on a community-hosted web-platform (at https://blastnet.github.io/), and is targeted towards improving accessibility to diverse scientific data for deep learning algorithms. For datasets that exceed the storage limitations in public ML repositories, we propose employing lossy compression algorithms on high-fidelity data, at the cost of introducing controllable amounts of error to the data. This framework leverages the well-known robustness of modern deep learning methods to noisy data, which we demonstrate is also applicable in combustion by training deep learning models on lossy direct numerical simulation (DNS) data in two completely different ML problems — one in combustion regime classification and the other in filtered reaction rate regression. Our results show that combustion DNS data can be compressed by at least 10-fold without affecting deep learning models, and that the resulting lossy errors can even improve their training. We thus call on the research community to help contribute to opening a bearable pathway towards accessible big data in combustion.}
}
@article{CABALLERO2022102058,
title = {BR4DQ: A methodology for grouping business rules for data quality evaluation},
journal = {Information Systems},
volume = {109},
pages = {102058},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2022.102058},
url = {https://www.sciencedirect.com/science/article/pii/S0306437922000485},
author = {Ismael Caballero and Fernando Gualo and Moisés Rodríguez and Mario Piattini},
keywords = {Business rules, Data quality, Data quality evaluation, Data quality measurement, Data quality characteristics, Data quality properties, ISO/IEC 25012, ISO/IEC 25024},
abstract = {Data quality evaluation is built upon data quality measurement results. “Data quality evaluation” uses the “data quality rules” representing the risk appetite of the organization to decide on the usability of the data; “data quality measurement” uses the business rules describing the “data requirements” or “data specifications” to determine the validity of the data. Consequently, to conduct meaningful and useful data quality evaluations, business rules must be first completely identified and captured at the beginning of the evaluation to perform sound measurements. We propose that the evaluation leads to better and more interpretable and useful results when the potential contribution of these business rules to the measurement of the data quality characteristics is first evaluated, avoiding the inclusion in the evaluation of those not having potential contribution and the resulting waste of resources. Considering this, we feel that for a better management of business rules for data quality evaluation, it makes sense to group all business rules having an important contribution to the evaluation of data quality characteristics, something that other business rules management methodologies have not covered yet. Through our experiences in conducting industrial projects of data quality evaluations we identified six problems when collecting and grouping the business rules. These problems make data quality evaluation processes less efficient and more costly. The main contribution of this paper is a methodology to systematically collect, group and validate the business rules to avoid or to alleviate these problems. For the sake of generalization, comparability, and reusability, we propose to do the grouping for data quality characteristics and properties defined in ISO/IEC 25012 and ISO/IEC 25024, respectively. Lastly, we validate the methodology in three case studies of real projects. From this validation, it is possible to raise the conclusion that the methodology is useful, applicable in the real world, and valid to capture and group the business rules used as a basis for data quality evaluation.}
}
@article{BAVARESCO2022112197,
title = {Are years-long field studies about window operation efficient? a data-driven approach based on information theory and deep learning},
journal = {Energy and Buildings},
volume = {268},
pages = {112197},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112197},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822003681},
author = {Mateus Bavaresco and Ioannis Kousis and Ilaria Pigliautile and Anna {Laura Pisello} and Cristina Piselli and Enedir Ghisi},
keywords = {Information theory, Occupant behaviour, Energy efficiency, Machine learning, Data length, Big data, Indoor environmental quality, Continuous monitoring},
abstract = {Scientific literature about building occupants’ behaviour and the related energy performance analyses document about several strategies to monitor window operation, including different sensors and data series lengths. In this framework, the primary goal of this study is to propose effective guidelines for minimum experiment durations and their reliability. A six-year-long database from a living laboratory was used as a benchmark; and a recursive strategy enabled to split it into more than 2,500 subsets, supporting two main steps. First, information theory concepts were used to calculate uncertainty and subsets’ divergence were compared to the full database. Second, the subsets were used to train deep neural networks and evaluate the influence of monitoring lengths combined with different kinds of environmental data (i.e. indoor or outdoor). From the information-theoretic metrics, the results support that indoor-related variables can reduce most of the uncertainty related to window operation. Besides, subsets influenced by autumn and winter diverge the most compared to the full database. Considering the modelling approach, the results demonstrated that by including indoor-related variables, higher shares of reliably-performing models were achieved, and smaller subsets were needed. Seasonality has also played a major role along these lines. As a consequence, the conclusions supported the feasibility of nine-month-long field studies, starting in summer or spring, when indoor and outdoor variables are monitored.}
}
@article{GEORGIADIS2022105640,
title = {Towards a privacy impact assessment methodology to support the requirements of the general data protection regulation in a big data analytics context: A systematic literature review},
journal = {Computer Law & Security Review},
volume = {44},
pages = {105640},
year = {2022},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105640},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921001138},
author = {Georgios Georgiadis and Geert Poels},
keywords = {Big data analytics, Data protection, Data protection directive, General data protection regulation, Governance, Information security, Privacy, Privacy impact assessment, Systematic literature review},
abstract = {Big Data Analytics enables today's businesses and organisations to process and utilise the raw data that is generated on a daily basis. While Big Data Analytics has improved efficiency and created many opportunities, it has also increased the risk of personal data being compromised or breached. The General Data Protection Regulation (GDPR) mandates Data Protection Impact Assessment (DPIA) as a means of identifying appropriate controls to mitigate risks associated with the protection of personal data. However, little is currently known about how to conduct such a DPIA in a Big Data Analytics context. To this end, we conducted a systematic literature review with the aim of identifying privacy and data protection risks specific to the Big Data Analytics context that could negatively impact individuals' rights and freedoms when they occur. Based on a sample of 159 articles, we applied a thematic analysis to all identified risks which resulted in the definition of nine Privacy Touch Points that summarise the identified risks. The coverage of these Privacy Touch Points was then analysed for ten Privacy Impact Assessment (PIA) methodologies. The insights gained from our analysis will inform the next phase of our research, in which we aim to develop a comprehensive DPIA methodology that will enable data processors and data controllers to identify, analyse and mitigate privacy and data protection risks when storing and processing data involving Big Data Analytics.}
}
@article{LIU2022105219,
title = {Investigation of VRF system cooling operation and performance in residential buildings based on large-scale dataset},
journal = {Journal of Building Engineering},
volume = {61},
pages = {105219},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.105219},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222012256},
author = {Hua Liu and Yi Wu and Da Yan and Shan Hu and Mingyang Qian},
keywords = {Big data, VRF systems, Operational performance, Statistical analysis},
abstract = {With the increase in the energy consumption of air-conditioning (AC) systems in Chinese residential buildings, the realization of energy savings in AC systems has attracted increasing attention. The variable refrigerant flow (VRF) system is a common AC system for residential buildings in China. In most previous studies on VRF systems, onsite measurements or surveys were conducted to collect operational data. These traditional methods may face various data issues, such as limited sample sizes and invalid data, making them unable to capture the spatial and temporal performance features of VRF systems in residential buildings on a large scale. To fill this gap, with advances in data storage and transmission technology, Big Data methods have been widely used for data collection. In the present study, researchers adopted 16,985 sets of VRF system operation data from China as the database and conducted data analysis for both the spatial and temporal dimensions. Several key indicators were proposed from the two perspectives (spatial and temporal), including the part-space index (PSI), load ratio (LR), use duration (UD), and cooling energy consumption. The main findings were as follows: (1) The “part-time part-space” operation mode of residential VRF systems can be analyzed according to the statistical results of the UD and PSI. (2) An LR of <30% is the main operating condition for VRF systems in residential buildings. (3) Extracted typical LR patterns can reflect different user behavior. The statistical results obtained in this study provide a basis for VRF engineering projects.}
}
@article{OESTERREICH2022128,
title = {The role of the social and technical factors in creating business value from big data analytics: A meta-analysis},
journal = {Journal of Business Research},
volume = {153},
pages = {128-149},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.08.028},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322007111},
author = {Thuy Duong Oesterreich and Eduard Anton and Frank Teuteberg and Yogesh K Dwivedi},
keywords = {Big data analytics, IT business value, Firm performance, Meta-analysis, Moderator analysis, Sociotechnical theory},
abstract = {Big data analytics (BDA) has recently gained importance as an emerging technology for handling big data. The use of advanced techniques with differing levels of intelligence, such as descriptive, predictive, prescriptive, and autonomous analytics, is expected to create value for firms. By viewing BDA as a sociotechnical system, we conduct a meta-analysis of 107 individual studies to integrate prior evidence on the role of the technical and social factors of BDA in creating BDA business value. The findings underline the predominant role of the social components in enhancing firm performance, such as the BDA system’s human factors and a nurturing organizational structure, in contrast to the minor role of the technological factors. However, both the technical and social factors are found to be strong determinants of BDA business value. Through the combined lens of sociotechnical theory and the IS business value framework, we contribute to research and practice by enhancing the understanding of the main technical and social determinants of BDA business value at the firm level.}
}
@incollection{SEBASTIANCOLEMAN2022229,
title = {Chapter 10 - Dimensions of Data Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {229-256},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000109},
author = {Laura Sebastian-Coleman},
keywords = {Data quality dimensions, data completeness, data integrity, validity, data currency, metadata management, reference data management, data modeling},
abstract = {This chapter provides an in-depth discussion about a core concept in data quality management: data quality dimensions. Dimensions provide a framework through which we can understand the core capabilities. As the foundation for data quality rules and requirements, they play a critical role in helping answer the fundamental questions about data quality: “What do we mean by high-quality data?” “How do we detect low-quality data?” and “What action will we take when data does not meet quality standards?” This chapter will review a comprehensive set of dimensions (i.e., completeness, correctness, uniqueness, consistency, currency, validity, integrity, reasonability, precision, clarity, accessibility, timeliness, relevance, usability, trustworthiness) in the context of challenges associated with data structure and meaning, the processes for creating data, the influence of technology on quality, and the perceptions of data consumers.}
}
@article{GUO20229979,
title = {Addressing big data challenges in mass spectrometry-based metabolomics},
journal = {Chemical Communications},
volume = {58},
number = {72},
pages = {9979-9990},
year = {2022},
issn = {1359-7345},
doi = {https://doi.org/10.1039/d2cc03598g},
url = {https://www.sciencedirect.com/science/article/pii/S1359734522020882},
author = {Jian Guo and Huaxu Yu and Shipei Xing and Tao Huan},
abstract = {ABSTRACT
Advancements in computer science and software engineering have greatly facilitated mass spectrometry (MS)-based untargeted metabolomics. Nowadays, gigabytes of metabolomics data are routinely generated from MS platforms, containing condensed structural and quantitative information from thousands of metabolites. Manual data processing is almost impossible due to the large data size. Therefore, in the “omics” era, we are faced with new challenges, the big data challenges of how to accurately and efficiently process the raw data, extract the biological information, and visualize the results from the gigantic amount of collected data. Although important, proposing solutions to address these big data challenges requires broad interdisciplinary knowledge, which can be challenging for many metabolomics practitioners. Our laboratory in the Department of Chemistry at the University of British Columbia is committed to combining analytical chemistry, computer science, and statistics to develop bioinformatics tools that address these big data challenges. In this Feature Article, we elaborate on the major big data challenges in metabolomics, including data acquisition, feature extraction, quantitative measurements, statistical analysis, and metabolite annotation. We also introduce our recently developed bioinformatics solutions for these challenges. Notably, all of the bioinformatics tools and source codes are freely available on GitHub (https://www.github.com/HuanLab), along with revised and regularly updated content.}
}
@article{CHENG2022134380,
title = {Dirty skies lower subjective well-being},
journal = {Journal of Cleaner Production},
volume = {378},
pages = {134380},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134380},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203952X},
author = {Lu Cheng and Zhifu Mi and Yi-Ming Wei and Shidong Wang and Klaus Hubacek},
keywords = {Subjective well-being, Air pollution, Big data, Sentiment analysis},
abstract = {Self-reported life satisfaction of China's population has not improved as much as expected during the economic boom, which was accompanied by a significant decline in environmental performance. Is environmental pollution the culprit for the lagging subjective well-being? To explore this issue, this paper adopts sentiment analysis to construct a real-time daily subjective well-being metric at the city level based on the big data of online search traces. Using daily data from 13 Chinese cities centred on Beijing between August 2014 and December 2019, we look at the corelation between subjective well-being and air pollution and the heterogeneity in this relationship based on two separate identification strategies. We find that air pollutants are negatively correlated with subjective well-being, and well-being tends to decline more from pollution during hot seasons. In addition, residents in wealthier regions tend to be more sensitive to air pollution. This result may be explained by the differences in the subjective perception of air pollution and personal preferences at different levels of income. These findings provide information about concerns of the public, thereby helping the government to take appropriate actions to respond to the dynamics of subjective well-being.}
}
@article{NARAYANAN20223121,
title = {A novel system architecture for secure authentication and data sharing in cloud enabled Big Data Environment},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part B},
pages = {3121-3135},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820303700},
author = {Uma Narayanan and Varghese Paul and Shelbi Joseph},
keywords = {Big data outsourcing, Big data sharing, Big data management, SALSA encryption with MapReduce, Fractal index tree, SHA-3},
abstract = {With the rapid growth of data sources, Big data security in Cloud is a big challenge. Different issues have ascended in the area of Big data security such as infrastructure security, data privacy, data management and data integrity. Currently, Big data processing, analytics and storage is secured using cryptography algorithms, which are not appropriate for Big data protection over Cloud. In this paper, we present a solution for addressing the main issues in Big data security over Cloud. We propose a novel system architecture called the Secure Authentication and Data Sharing in Cloud (SADS-Cloud). There are three processes involved in this paper including (i). Big Data Outsourcing, (ii). Big Data Sharing and (iii). Big Data Management. In Big data outsourcing, the data owners are registered to a Trust Center using SHA-3 hashing algorithm. The MapReduce model is used to split the input file into fixed-size of blocks of data and SALSA20 encryption algorithm is applied over each block. In Big data sharing, data users participate in a secure file retrieval. For this purpose, user's credentials (ID, password, secure ID, and current timestamp, email id) are hashed and compared with that stored in a database. In Big data management, there are three important processes implemented to organize data. They are as follows: Compression using Lemperl Ziv Markow Algorithm (LZMA), Clustering using Density-based Clustering of Applications with Noise (DBSCAN), and Indexing using Fractal Index Tree. The proposed scheme for these processes are implemented using Java Programming and performance tested for the following metrics: Information Loss, Compression Ratio, Throughput, Encryption Time and Decryption Time.}
}
@incollection{CYCHOSZ20221,
title = {Chapter One - Using big data from long-form recordings to study development and optimize societal impact},
editor = {Rick O. Gilmore and Jeffrey J. Lockman},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {62},
pages = {1-36},
year = {2022},
booktitle = {New Methods and Approaches for Studying Child Development},
issn = {0065-2407},
doi = {https://doi.org/10.1016/bs.acdb.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065240721000434},
author = {Margaret Cychosz and Alejandrina Cristia},
keywords = {Big data, Wearable technology, Algorithm bias, Automatic measurement, Language, Audio recording, Children},
abstract = {Big data are everywhere. In this chapter, we focus on one source: long-form, child-centered recordings collected using wearable technologies. Because these recordings are simultaneously unobtrusive and encompassing, they may be a breakthrough technology for clinicians and researchers from several diverse fields. We demonstrate this possibility by outlining three applications for the recordings—clinical treatment, large-scale interventions, and language documentation—where we see the greatest potential. We argue that incorporating these recordings into basic and applied research will result in more equitable treatment of patients, more reliable measurements of the effects of interventions on real-world behavior, and deeper scientific insights with less observational bias. We conclude by outlining a proposal for a semistructured online platform where vast numbers of long-form recordings could be hosted and more representative, less biased algorithms could be trained.}
}
@article{VLAH2022101774,
title = {Data-driven engineering design: A systematic review using scientometric approach},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101774},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101774},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002324},
author = {Daria Vlah and Andrej Kastrin and Janez Povh and Nikola Vukašinović},
keywords = {Data-driven design, Engineering design, Product development, Big data analytics, Bibliographic analysis, Network analysis},
abstract = {In the last two decades, data regarding engineering design and product development has increased rapidly. Big data exploration and mining offer numerous opportunities for engineering design; however, owing to the multitude of data sources and formats coupled with the high complexity of the design process, these techniques are yet to be utilised to the best of their full potential. In this study, a comprehensive assessment of the state-of-the-art data-driven engineering design (DDED) in the last 20 years was conducted. A scientometric approach was employed wherein first, a systematic article acquisition procedure was performed, where a dataset of 3339 articles related to engineering design and big data analytics applications were extracted from Web of Science (WoS) and Scopus. Thereafter, this dataset was reduced to a dataset of 366 articles based on concise data screening. The resulting articles were used to analyse the dynamics of research in DDED throughout the last 20 years and to detect the primary research topics related to DDED, the most influential authors, and the papers with the highest impact in the DDED domain. Furthermore, the co-occurrence network of keywords/keyphrases and co-authorship networks were constructed and analysed to reveal the interconnection of the research topics and the collaboration between the most prolific authors. Finally, an insight how big data analytics is being applied through product development activities to support decision-making in engineering design was presented.}
}
@article{WANG2022e10312,
title = {Impact of big data resources on clinicians’ activation of prior medical knowledge},
journal = {Heliyon},
volume = {8},
number = {9},
pages = {e10312},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e10312},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022016000},
author = {Sufen Wang and Junyi Yuan and Changqing Pan},
keywords = {Big data resources, Activation of prior medical knowledge, Shared big data resources, Private big data resources},
abstract = {Background
Activating prior medical knowledge in diagnosis and treatment is an important basis for clinicians to improve their care ability. However, it has not been systematically explained whether and how various big data resources affect the activation of prior knowledge in the big data environment faced by clinicians.
Objective
The aim of this study is to contribute to a better understanding on how the activation of prior knowledge of clinicians is affected by a wide range of shared and private big data resources, to reveal the impact of big data resources on clinical competence and professional development of clinicians.
Method
Through the comprehensive analysis of extant research results, big data resources are classified as big data itself, big data technology and big data services at the public and institutional levels. A survey was conducted on clinicians and IT personnel in Chinese hospitals. A total of 616 surveys are completed, involving 308 medical institutions. Each medical institution includes a clinician and an IT personnel. SmartPLS version 2.0 software package was used to test the direct impact of big data resources on the activation of prior knowledge. We further analyze their indirect impact of those big data resources without direct impact.
Results
(1) Big data quality environment at the institutional level and the big data sharing environment at the public level directly affect activation of prior medical knowledge; (2) Big data service environment at the institutional level directly affects activation of prior medical knowledge; (3) Big data deployment environment at the institutional level and big data service environment at the public level have no direct impact on activation of prior knowledge of clinicians, but they have an indirect impact through big data quality environment and service environment at the institutional level and the big data sharing environment at the public level.
Conclusions
Big data technology, big data itself and big data service at the public level and institutional level interact and influence each other to activate prior medical knowledge. This study highlights the implications of big data resources on improvement of clinicians’ diagnosis and treatment ability.}
}
@article{WANG2022e97,
title = {Registries, Databases and Repositories for Developing Artificial Intelligence in Cancer Care},
journal = {Clinical Oncology},
volume = {34},
number = {2},
pages = {e97-e103},
year = {2022},
note = {Artificial Intelligence in Radiation Therapy},
issn = {0936-6555},
doi = {https://doi.org/10.1016/j.clon.2021.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S0936655521004593},
author = {J.W. Wang and M. Williams},
keywords = {Artificial intelligence, Big Data, database, deep learning, registries, repository},
abstract = {Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.}
}
@article{HORNG202222,
title = {Role of big data capabilities in enhancing competitive advantage and performance in the hospitality sector: Knowledge-based dynamic capabilities view},
journal = {Journal of Hospitality and Tourism Management},
volume = {51},
pages = {22-38},
year = {2022},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2022.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S1447677022000389},
author = {Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu},
keywords = {Knowledge-based dynamic capabilities view, Big data capabilities, Knowledge management, Sustainability marketing, Social media, Big data strategy},
abstract = {To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.}
}
@article{LI2022129190,
title = {Big data nanoindentation characterization of cross-scale mechanical properties of oilwell cement-elastomer composites},
journal = {Construction and Building Materials},
volume = {354},
pages = {129190},
year = {2022},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2022.129190},
url = {https://www.sciencedirect.com/science/article/pii/S095006182202846X},
author = {Yucheng Li and Yunhu Lu and Li Liu and Shengmin Luo and Li He and Yongfeng Deng and Guoping Zhang},
keywords = {Big data nanoindentation, Cross-scale characterization, Elastomer, Microstructure, Oilwell cement, Young’s modulus},
abstract = {Intensive big data nanoindentation (BDNi) characterization was performed to reveal the cross-scale mechanical properties of, and hence distinguish the different phases in, inorganic–organic hybrid oilwell cement-elastomer composites, hydrothermally cured at 160 °C and 20 MPa for 28 days. Totally-three emulsified and particulate elastomers, including styrene-butadiene latex (SBL) emulsion (6, 12, and 14 wt.%), polypropylene (PP) powder (12 wt.%), and nitrile rubber (NR) powder (6 wt.%), and a weighting agent, hematite (50 wt.%), were used as additives to finely adjust the mechanical properties and microstructure of the hybrid composites, which were respectively examined by the BDNi and mercury intrusion porosimetry and scanning electron microscopy. BDNi data were statistically deconvoluted by the Gaussian mixture modeling (GMM) to discern mechanically distinct phases and their Young’s moduli and hardness at the micro/nano scale and the bulk composites’ properties at the macro scale. Results show that the SBL emulsion can be more homogeneously dispersed into the cement matrix, due to its emulsified soft consistency and hydrophilicity, resulting in the formation of soft coatings on, and softer infills intermixed with, the cement hydration products (CHPs). In contrast, the two hydrophobic, inert, particulate elastomers, PP and NR powders, only act as isolated soft inclusions embedded in the hydrated cement matrix. The NR melts at high temperatures and permeates into the pores of the cement matrix, leading to the formation of complex intervened micromorphology and hence functions better than the PP. All elastomers can effectively reduce the composites’ Young’s moduli: with increasing the elastomer contents, while the modulus of a BDNi-identified major CHP phase decreases from 20.9 to 11.3 GPa, the bulk composites’ counterpart from 17.3 to 10.7 GPa. The BDNi enables the identification of multiple mechanically distinct phases in the hybrid composites and quantification of the property changes of these phases.}
}
@article{COZZINI2022133422,
title = {Computational methods on food contact chemicals: Big data and in silico screening on nuclear receptors family},
journal = {Chemosphere},
volume = {292},
pages = {133422},
year = {2022},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2021.133422},
url = {https://www.sciencedirect.com/science/article/pii/S0045653521038960},
author = {Pietro Cozzini and Francesca Cavaliere and Giulia Spaggiari and Gianluca Morelli and Marco Riani},
keywords = {Computational chemistry, Consensus prediction, Database, Nuclear receptors, Toxicology},
abstract = {According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner.}
}
@article{PENG2022104,
title = {Industrial big data-driven mechanical performance prediction for hot-rolling steel using lower upper bound estimation method},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {104-114},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S027861252200142X},
author = {Gongzhuang Peng and Yinliang Cheng and Yufei Zhang and Jian Shao and Hongwei Wang and Weiming Shen},
keywords = {Industrial big data, Mechanical performances prediction, Lower upper bound estimation, Broad learning system, Hot-rolling},
abstract = {Industrial big data technology has become one of the important driving forces to intelligent manufacturing in the steel industry. In this study, the characteristics of data in steel production are analyzed and an industrial big data platform for steeling process is developed to extract the quality-related parameters. A data-driven approach to construct prediction intervals (PIs) of mechanical performances for hot-rolling strips is proposed to represent the uncertainty and reliability of the prediction results. The proposed method employs a new manifold visualization method, SLISEMAP, to reduce the feature dimensions with interpretability, utilizes lower upper bound estimation (LUBE) method to obtain the PIs, in which the broad learning system (BLS) is used as the basic training network model and the artificial bee colony (ABC) algorithm is applied to optimize the weighting parameters of BLS under the LUBE framework. A hot-rolling steel coil dataset consisting of 39 variables and 1335 coil samples is used to validate the proposed method. Two Delta-based approaches, namely back propagation neural network (BPNN) and extreme learning machine (ELM); and three LUBE-based approaches, namely ABC-BPNN, ABC-ELM, and ABC-support vector regression (SVR) are compared with the proposed method. Results show that the proposed ABC-BLS in LUBE is effective and efficient in constructing the PIs with a higher coverage probability and a narrower interval width.}
}
@article{OUAFIQ2022102093,
title = {AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities},
journal = {Sustainable Energy Technologies and Assessments},
volume = {52},
pages = {102093},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102093},
url = {https://www.sciencedirect.com/science/article/pii/S221313882200145X},
author = {El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon},
keywords = {Smart Farming, Energy Harvesting Capabilities, IoT, Big Data, Agriculture 4.0, Water Management, Sustainability},
abstract = {The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always “cleaner” and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.}
}
@article{PALESI202293,
title = {MRI data quality assessment for the RIN - Neuroimaging Network using the ACR phantoms},
journal = {Physica Medica},
volume = {104},
pages = {93-100},
year = {2022},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2022.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S1120179722020695},
author = {Fulvia Palesi and Anna Nigri and Ruben Gianeri and Domenico Aquino and Alberto Redolfi and Laura Biagi and Irene Carne and Silvia {De Francesco} and Stefania Ferraro and Paola Martucci and Jean {Paul Medina} and Antonio Napolitano and Alice Pirastru and Francesca Baglio and Fabrizio Tagliavini and Maria {Grazia Bruzzone} and Michela Tosetti and Claudia A.M. {Gandini Wheeler-Kingshott}},
keywords = {ACR, Quality control, Multisite},
abstract = {Purpose
Generating big-data is becoming imperative with the advent of machine learning. RIN-Neuroimaging Network addresses this need by developing harmonized protocols for multisite studies to identify quantitative MRI (qMRI) biomarkers for neurological diseases. In this context, image quality control (QC) is essential. Here, we present methods and results of how the RIN performs intra- and inter-site reproducibility of geometrical and image contrast parameters, demonstrating the relevance of such QC practice.
Methods
American College of Radiology (ACR) large and small phantoms were selected. Eighteen sites were equipped with a 3T scanner that differed by vendor, hardware/software versions, and receiver coils. The standard ACR protocol was optimized (in-plane voxel, post-processing filters, receiver bandwidth) and repeated monthly. Uniformity, ghosting, geometric accuracy, ellipse’s ratio, slice thickness, and high-contrast detectability tests were performed using an automatic QC script.
Results
Measures were mostly within the ACR tolerance ranges for both T1- and T2-weighted acquisitions, for all scanners, regardless of vendor, coil, and signal transmission chain type. All measurements showed good reproducibility over time. Uniformity and slice thickness failed at some sites. Scanners that upgraded the signal transmission chain showed a decrease in geometric distortion along the slice encoding direction. Inter-vendor differences were observed in uniformity and geometric measurements along the slice encoding direction (i.e. ellipse’s ratio).
Conclusions
Use of the ACR phantoms highlighted issues that triggered interventions to correct performance at some sites and to improve the longitudinal stability of the scanners. This is relevant for establishing precision levels for future multisite studies of qMRI biomarkers.}
}
@article{DEVI20224980,
title = {Traffic management in smart cities using support vector machine for predicting the accuracy during peak traffic conditions},
journal = {Materials Today: Proceedings},
volume = {62},
pages = {4980-4984},
year = {2022},
note = {International Conference on Innovative Technology for Sustainable Development},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.03.722},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322022131},
author = {T. Devi and K. Alice and N. Deepa},
keywords = {Big data, Mobility, Linear and logistic regression, Support vector machine (SVM), Traffic management},
abstract = {Mobility is the main key for smart living, where navigation and automatic suggestions are also a strategy for a successful life in smart cities. Big Data analytics are behind urban changes in the mobility of smart cities to bring sustainable life. By the year 2025 all over Indian states can reach the expected lifestyle by providing high security and mobility which can grow the opportunities also high. As the population is rapidly increasing, the needs of people are also increasing such that necessitating real-time apps for daily needs, communication devices, and so on. We focus our idea on the benefits of traffic and safety measures which are becoming a huge challenge nowadays. Many are preferred with sophistication when traveling for short distances. In such a way the big data analytics tools R studio and weka are used on the dataset smart city from the Kaggle website for traffic patterns during the high traffic duration. Using the dataset, the data are classified using a Support vector machine (SVM) and applied with regression such as linear, logistic regression to find the accuracy of traffic peak situations. The proposed work aims to compare the efficiency of big data technologies which can be applied using various classification and regression that can be shown on various tools such as R, Weka, map-reduce which can produce accurate results to visualize the smart cities and their traffic analysis.}
}
@article{GOKALP2022103585,
title = {A process assessment model for big data analytics},
journal = {Computer Standards & Interfaces},
volume = {80},
pages = {103585},
year = {2022},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103585},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000805},
author = {Mert Onuralp Gökalp and Ebru Gökalp and Kerem Kayabay and Selin Gökalp and Altan Koçyiğit and P. Erhan Eren},
keywords = {Big data, Data analytics, Software development, Software process improvement, Software process assessment},
abstract = {Today, business success is essentially powered by data-centric software. Big data analytics (BDA) grasp the potential of generating valuable insights and empowering businesses to support their strategic decision-making. However, although organizations are aware of BDAs’ potential opportunities, they face challenges to satisfy the BDA-specific processes and integrate them into their daily software development lifecycle. Process capability/ maturity assessment models are used to assist organizations in assessing and realizing the value of emerging capabilities and technologies. However, as a result of the literature review and its analysis, it was observed that none of the existing studies in the BDA domain provides a complete, standardized, and objective capability maturity assessment model. To address this research gap, we focus on developing a BDA process capability assessment model grounded on the well-accepted ISO/IEC 330xx standard series. The proposed model comprises two main dimensions: process and capability. The process dimension covers six BDA-specific processes: business understanding, data understanding, data preparation, model building, evaluation, and deployment and use. The capability dimension has six levels, from not performed to innovating. We conducted case studies in two different organizations to validate the applicability and usability of the proposed model. The results indicate that the proposed model provides significant insights to improve the business value generated by BDA via determining the current capability levels of the organizations' BDA processes, deriving a gap analysis, and creating a comprehensive roadmap for continuous improvement in a standardized way.}
}
@incollection{KOLTAY202249,
title = {Chapter 3 - Data quality, the essential “ingredient”},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {49-75},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000047},
author = {Tibor Koltay},
keywords = {Research data quality, Stakeholders, Trust, Intrinsic and extrinsic data quality, Semiotic representation, Time-related dimensions, Data retrievability, Data reuse, Data governance},
abstract = {This chapter acquaints the reader with the general and often changing nature of research on data quality. It is emphasized that research data quality is closely related to business data; however, the goals of scholarly research have become different, especially as the environments shaping the two are different. From among data quality’s attributes, trust receives particular attention. Technical and scientific quality, the relationship of data quality to data reuse, and other quality factors are also examined, including big data quality, intrinsic and extrinsic data quality, and the semiotic representation of quality attributes, as well as their time-related dimensions and retrievability. Although data reuse was addressed in an earlier chapter, its relationship to data quality is touched on in this chapter as well. Sharing the previously mentioned origin with data quality and being closely associated with it, data governance is also portrayed.}
}
@article{GUO20221792,
title = {Measuring and evaluating SDG indicators with Big Earth Data},
journal = {Science Bulletin},
volume = {67},
number = {17},
pages = {1792-1801},
year = {2022},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2022.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S2095927322002997},
author = {Huadong Guo and Dong Liang and Zhongchang Sun and Fang Chen and Xinyuan Wang and Junsheng Li and Li Zhu and Jinhu Bian and Yanqiang Wei and Lei Huang and Yu Chen and Dailiang Peng and Xiaosong Li and Shanlong Lu and Jie Liu and Zeeshan Shirazi},
keywords = {Big Earth Data, Big data, Sustainable Development Goals (SDGs), Decision support, CASEarth, Digital Earth},
abstract = {The United Nations 2030 Agenda for Sustainable Development provides an important framework for economic, social, and environmental action. A comprehensive indicator system to aid in the systematic implementation and monitoring of progress toward the Sustainable Development Goals (SDGs) is unfortunately limited in many countries due to lack of data. The availability of a growing amount of multi-source data and rapid advancements in big data methods and infrastructure provide unique opportunities to mitigate these data shortages and develop innovative methodologies for comparatively monitoring SDGs. Big Earth Data, a special class of big data with spatial attributes, holds tremendous potential to facilitate science, technology, and innovation toward implementing SDGs around the world. Several programs and initiatives in China have invested in Big Earth Data infrastructure and capabilities, and have successfully carried out case studies to demonstrate their utility in sustainability science. This paper presents implementations of Big Earth Data in evaluating SDG indicators, including the development of new algorithms, indicator expansion (for SDG 11.4.1) and indicator extension (for SDG 11.3.1), introduction of a biodiversity risk index as a more effective analysis method for SDG 15.5.1, and several new high-quality data products, such as global net ecosystem productivity, high-resolution global mountain green cover index, and endangered species richness. These innovations are used to present a comprehensive analysis of SDGs 2, 6, 11, 13, 14, and 15 from 2010 to 2020 in China utilizing Big Earth Data, concluding that all six SDGs are on schedule to be achieved by 2030.}
}
@article{KIM2022100256,
title = {Organizational process maturity model for IoT data quality management},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100256},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100256},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000480},
author = {Sunho Kim and Ricardo Pérez-Castillo and Ismael Caballero and Downgwoo Lee},
keywords = {Data quality, Data quality management, IoT, ISO 8000, Process-centric, Process reference model, Maturity, Process maturity, process attribute},
abstract = {Data quality management (DQM) is one of the most critical aspects to ensure successful applications of the Internet of Things (IoT). So far, most of the approaches for assuring data quality are typically data-centric, i.e., mainly focus on fixing data issues for specific values. However, organizations can also benefit from improving their capabilities of their DQM processes by developing organizational best DQM practices. In this regard, our investigation addresses how well organizations perform their DQM processes in the IoT domain. The main contribution of this study is to establish a framework for IoT DQM maturity. This framework is compliant with ISO 8000-61 (DQM: process reference model) and ISO 8000-62 (DQM: organizational process maturity assessment) and can be used to assess and improve the capabilities of the DQM processes for IoT data. The framework is composed of two elements. First, a process reference model (PRM) for IoT DQM is proposed by extending the PRM for DQM defined in ISO 8000-61, tailoring some existing processes and adding new ones. Second, a maturity model suitable for IoT data is proposed based on the PRM for IoT DQM. The maturity model, named IoT DQM3, is proposed by extending the maturity model defined in ISO 8000-62. However, in order to increase the usability of IoT DQM3, we consider adequate the proposition of a simplification of the IoT DQM3, by introducing a lightweight version to reduce assessment indicators and facilitate its industrial adoption. A simplified method to measure the capability of a process is also suggested considering the relationship of process attributes with the measurement stack defined in ISO 8000-63. The empirical validation of the maturity model is twofold. First, the appropriateness of the two models is surveyed with data quality experts who are currently working in various organizations around the world. Second, in order to demonstrate the feasibility of the proposal, the light-weight version is applied to a manufacturing company as a case study.}
}
@incollection{CHAKRABORTY202273,
title = {Chapter 7 - Recent advances in processing, interpreting, and managing biological data for therapeutic intervention of human infectious disease},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {73-82},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000091},
author = {Pritha Chakraborty and Parth Sarthi {Sen Gupta} and Shankar Dey and Nabarun {Chandra Das} and Ritwik Patra and Suprabhat Mukherjee},
keywords = {Big Data, Biomedical, Electronic health records, Epidemiology, Surveillance},
abstract = {Big data has been a buzzword for academics and the healthcare industry, describing data accumulation, processing, and interpretation using analytical tools and techniques. Hitherto, identification of state of the art in big data has become a question of research for academicians due to lack of addressable studies in this regard. The potential of big data is lagging, as compared to other fields. Herein, in this chapter, we have described the potential of big data in combating infectious diseases involving personalized, participatory, predictive, and preventive models based on biomedical data also called the “omics data” and electronic health records from different authenticated sources. Our study indicates the use of various tools and techniques for data accumulation and management, thus providing an insight toward the revolution of the healthcare industry as well as the research community. Though the application of big data is still in the preliminary stage, growing Research and Development investment with its successful implementation will show enormous potential of growth in coming years. Considering all together, there is a need for the development of advanced technologies with inclusion of transparent ethical values to provide acceptance and with a moral foundation.}
}
@article{ROOS202218,
title = {Record linkage and big data—enhancing information and improving design},
journal = {Journal of Clinical Epidemiology},
volume = {150},
pages = {18-24},
year = {2022},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2022.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0895435622001524},
author = {Leslie L. Roos and Elizabeth Wall-Wieler and Charles Burchill and Naomi C. Hamm and Amani F. Hamad and Lisa M. Lix},
keywords = {Research possibilities, Covariates, Family data, Registries, Observational studies, Design},
abstract = {Background and Objectives
To highlight the potential of multiple file record linkage. Linkage increases the value of existing information by supplying missing data or correcting errors in existing data, through generating important covariates, and by using family information to control for unmeasured variables and expand research opportunities.
Methods
Recent Manitoba papers highlight the use of linkage to produce better studies. Specific ways in which linkage helps deal with different substantive issues are described.
Results
Wide data files—files containing considerable amounts of information on each individual—generated by linkage improve research by facilitating better design. Nonexperimental work in particular benefits from such linkages. Population registries are especially valuable in supplying family data to facilitate work across different substantive fields.
Conclusion
Several examples show how record linkage magnifies the value of information from individual projects. The results of observational studies become more defensible through the better designs facilitated by such linkage.}
}
@article{ZHANG2022101626,
title = {Big data analytics, resource orchestration, and digital sustainability: A case study of smart city development},
journal = {Government Information Quarterly},
volume = {39},
number = {1},
pages = {101626},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101626},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000629},
author = {Dan Zhang and L.G. Pee and Shan L. Pan and Lili Cui},
keywords = {Smart city, Big data, Digital sustainability, Resource orchestration, Socio-technical issues},
abstract = {Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data.}
}
@article{LIU2022103622,
title = {scenario modeling for government big data governance decision-making: Chinese experience with public safety services},
journal = {Information & Management},
volume = {59},
number = {3},
pages = {103622},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103622},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000349},
author = {Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU},
keywords = {Government big data governance, Scenario-based decision-making, Scenario modeling, Model-driven, Data link network, Public safety services},
abstract = {In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.}
}
@incollection{SEBASTIANCOLEMAN20223,
title = {Chapter 1 - The Importance of Data Quality Management},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {3-30},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000018},
author = {Laura Sebastian-Coleman},
keywords = {Quality management, process management, data quality management, data governance, costs of poor quality, big data, digital transformation, data privacy, data monetization},
abstract = {This chapter analyzes the role of data quality management in response to the rapid evolution of data in our world. It discusses the impact of poor-quality data on organizations, focusing on the costs and risks associated with poorly managed data. In many organizations, poor-quality data is tolerated to a degree that poor-quality products would not be. Data quality management reduces the costs and risks of poor-quality data and enables the benefits and opportunities of high-quality data, especially in an age of big data, digital transformation, and artificial intelligence.}
}
@article{YOUSSEF2022102827,
title = {Cross-national differences in big data analytics adoption in the retail industry},
journal = {Journal of Retailing and Consumer Services},
volume = {64},
pages = {102827},
year = {2022},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2021.102827},
url = {https://www.sciencedirect.com/science/article/pii/S0969698921003933},
author = {Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag},
keywords = {Big data analytics, Technology adoption, Diffusion of innovations model, Cross-national differences, Retail industry},
abstract = {Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a “one-size-fits-all” approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.}
}
@article{HE2022112372,
title = {A rule-based data preprocessing framework for chiller rooms inspired by the analysis of engineering big data},
journal = {Energy and Buildings},
volume = {273},
pages = {112372},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112372},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822005436},
author = {Ruikai He and Tong Xiao and Shunian Qiu and Jiefan Gu and Minchen Wei and Peng Xu},
keywords = {Data pre-processing, Big engineering data, Building energy management},
abstract = {The rapid development of building energy consumption monitoring platforms makes engineering data more diverse, which facilitates the goal of reducing emissions. It is increasingly acknowledged that data preprocessing deserves the same attention as intelligent algorithms. In this work, the data quality issue of the engineering big data from non-demonstration complexes in China are analyzed thoroughly, and the analysis is based on clustering-based algorithms. We can conclude that the data of the hourly power of equipment groups are quality and stable, which is suitable for the benchmark to check other data. The quality of the data about pipes is acceptable. The number of data types about cooling towers is less, and the quality is worse. Regarding other data, the quality is unstable, so researchers should deal with those case-by-case. According to the above analysis, we proposed a convenient, rule-based data preprocessing framework that utilizes the law of physics, ensuring the strong coupling of multi-variants. After the data preprocessing, these engineering data are more reliable and can be used to improve performance or train models. Additionally, the proposed framework is more suitable for preprocessing multi-variant engineering data.}
}
@incollection{SEBASTIANCOLEMAN2022187,
title = {Chapter 9 - Core Data Quality Management Capabilities},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {187-228},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000092},
author = {Laura Sebastian-Coleman},
keywords = {Data quality standards, assessing data quality, data quality monitoring, data quality reporting, data issue management, quality improvement methodology},
abstract = {This chapter describes the functions required to build organizational capacity to manage data for quality over time. They include: data quality standards, data quality assessment, data quality monitoring, data quality reporting, data quality issue management, and data quality improvement. These activities are likely to be executed more consistently and with greater impact if there is a data quality team specifically responsible for defining them and facilitating their adoption within the organization.}
}
@article{PICCAROZZI20221746,
title = {The role of Big Data in the business challenge of Covid-19: a systematic literature review in managerial studies},
journal = {Procedia Computer Science},
volume = {200},
pages = {1746-1755},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.375},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003842},
author = {Michela Piccarozzi and Barbara Aquilani},
keywords = {Big Data, Covid-19, systematic literature review, management},
abstract = {2020 was globally greatly affected by the Covid-19 pandemic caused by SARS-CoV-2, which is still today impacting and profoundly changing life globally for people but also for firms. In this context, the need for timely and accurate information has become vital in every area of business management. The spread of the Covid-19 global pandemic has generated an exponential increase and extraordinary volume of data. In this domain, Big Data is one of the digital innovation technologies that can support business organizations during these complex times. Based on these considerations, the aim of this paper is to analyze the managerial literature concerning the issue of Big Data in the management of the Covid-19 pandemic through a systematic literature review. The results show a fundamental role of Big Data in pandemic management for businesses. The paper also provides managerial and theoretical implications.}
}
@article{DHAND2022101010,
title = {Deep enriched salp swarm optimization based bidirectional -long short term memory model for healthcare monitoring system in big data},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101010},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101010},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001538},
author = {Geetika Dhand and Kavita Sheoran and Parul Agarwal and Siddhartha Sankar Biswas},
keywords = {Big data, Enriched sea salp optimization, Wearable sensors, Health care monitoring systems},
abstract = {The rapid development of communication technologies and expert systems have resulted in a large volume of medical data. Big data such as clinical data, omics data, and electronic health data are difficult to manage in real-time due to noise, large size, different formats, missing values and large features. Hence, it is more difficult for the health monitoring system to extract the correct information. Low quality and noisy data can lead to unnecessary treatment. To overcome these issues, we proposed Enriched Salp Swarm Optimization based Bidirectional Long Short Term Memory (ESSOBiLSTM) to monitor health. This method consists of four layers, such as the data collection layer, data storage layer, data analytics, and presentation layer. The initial layer handles a variety of information from main sources: wearable sensor devices (WSD), social network data, and medical records (MR). The second layer stores all the collected data from WSD, MR, and social network data to the cloud server through the wireless network. The proposed framework for performing big data analytics steps like preprocessing, filtering, dimensionality reduction, and classification is performed in the third layer. In the final layer, the doctor analyzes the patient's condition based on the classification results of the enriched SSO-BiLSTM. Based on the evaluation report, the proposed ESSOBiLSTM gives an accuracy of 85%, precision of 80%, RMSE of 0.6, MAE of 0.58, recall of 85% and F-measure of 79%. As a result, ESSOBiLSTM has proven to be more effective in monitoring health in large datasets.}
}
@incollection{MUHAMADIBRAHIM202233,
title = {Chapter 4 - Towards big data framework in government public open data (GPOD) for health},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {33-45},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00024-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000248},
author = {Najhan {Muhamad Ibrahim} and Nur Hidayah Ilham {Ahmad Azri} and Norbik Bashah Idris},
keywords = {Big data, Big data framework, Government public open data, Open data, Public data for health},
abstract = {Today, data is one of the most valuable assets on the planet. As a valuable resource, data may be used to develop a wide range of data applications, all of which are driven by creativity and innovation. In order to obtain information and provide services, data is also a critical component. In the recent years, big data has become a popular topic in global discussion. Big data is a new technology and knowledge generation phenomenon, that record, capture, and execute a significant amount of data for the usage in a variety of domains such as research, education, business, investing, health, and so on. The proliferation of data inspired by new methods of data gatherings such as via social media, wireless sensors, and data from government agencies which makes big data management an ultimate challenge. This study includes a thorough evaluation of existing theories and practical approach to address the public sector open data issues for determining the determinants of government public open data (GPOD) development of big data. To investigate the revolution of GPOD for health, the framework was dominantly used over architecture, infrastructures, followed by theoretical and conceptual framework, according to the review. This study revealed that most of the existing frameworks still lack consideration of the requirement for public open data in health. There is less number of existing research works that have sophisticated big data frameworks in GPOD for health. There also is still a lack of investment and adoption of big data in the public sector. The findings of this chapter will help academicians to empirically study the revealed requirement and provide decision-makers a better knowledge of how to leverage GPOD adoption in health by taking appropriate actions.}
}
@article{RIDZUAN2022685,
title = {Diagnostic analysis for outlier detection in big data analytics},
journal = {Procedia Computer Science},
volume = {197},
pages = {685-692},
year = {2022},
note = {Sixth Information Systems International Conference (ISICO 2021)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024133},
author = {Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}},
keywords = {Big data, data quality, outlier, Sustainable Development Goals},
abstract = {Recently, Big Data analytics has been one of the most emerging topics in the business field. Data is collected, processed and analyzed to gain useful insight for their organization. Big Data analytics has the potential to improve the quality of life and help to achieve Sustainable Development Goals (SDG). To ensure that SDG goals are achieved, we must utilize existing data to meet those targets and ensure accountability. However, data quality is often left out when dealing with data. Any types of errors presented in the dataset should be properly addressed to ensure the analysis provided is accurate and truthful. In this paper, we have addressed the concept of data quality diagnosis to identify the outlier presented in the dataset. The cause of the outlier is further discussed to identify potential improvements that can be done to the dataset. In addition, recommendations to improve the quality of data and data collection systems are provided.}
}
@article{EZERINS2022105569,
title = {Advancing safety analytics: A diagnostic framework for assessing system readiness within occupational safety and health},
journal = {Safety Science},
volume = {146},
pages = {105569},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105569},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521004112},
author = {Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yalçın Açıkgöz},
keywords = {Safety analytics, Data analytics, Readiness assessment, Occupational health},
abstract = {Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.}
}
@article{CAISSIE2022100925,
title = {Head and Neck Radiotherapy Patterns of Practice Variability Identified as a Challenge to Real-World Big Data: results from the Learning from Analysis of Multicentre Big Data Aggregation (LAMBDA) Consortium},
journal = {Advances in Radiation Oncology},
pages = {100925},
year = {2022},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2022.100925},
url = {https://www.sciencedirect.com/science/article/pii/S245210942200032X},
author = {Amanda Caissie and Michelle Mierzwa and Clifton David Fuller and Murali Rajaraman and Alex Lin and Andrew MacDonald and Richard Popple and Ying Xiao and Lisanne VanDijk and Peter Balter and Helen Fong and Heping Xu and Matthew Kovoor and Joonsang Lee and Arvind Rao and Mary Martel and Reid Thompson and Brandon Merz and John Yao and Charles Mayo},
abstract = {Purpose/Objective
Outside of randomized clinical trials, it is difficult to develop clinically relevant evidence-based recommendations for radiotherapy (RT) practice guidelines due to lack of comprehensive real-world data. To address this knowledge gap, we formed the Learning and Analytics from Multicenter Big Data Aggregation (LAMBDA) consortium to cooperatively implement RT data standardization, develop software solutions for data analysis and recommend clinical practice change based on real-world data analyzed. The first phase of this “Big Data” study aimed at characterizing variability in clinical practice patterns of dosimetric data for organs at risk (OAR), that would undermine subsequent use of large scale, electronically aggregated data to characterize associations with outcomes. Evidence from this study was used as the basis for practical recommendations to improve data quality.
Materials/Methods
Dosimetric details of patients with H&N cancer treated with RT between 2014 and 2019 were analyzed. Institutional patterns of practice were characterized including structure nomenclature, volumes and frequency of contouring. Dose volume histogram (DVH) distributions were characterized and compared to institutional constraints and literature values.
Results
Plans for 4664 patients treated to a mean plan dose of 64.4 ± 13.2 Gy in 32 ± 4 fractions were aggregated. Prior to implementation of TG263 guidelines in each institution, there was variability in OAR nomenclature across institutions and structures. With evidence from this study, we identified a targeted and practical set of recommendations aimed at improving the quality of real-world data.
Conclusion
Quantifying similarities and differences among institutions for OAR structures and DVH metrics is the launching point for next steps to investigate potential relationships between DVH parameters and patient outcomes.}
}
@article{KLEES202214,
title = {Building a smart database for predictive maintenance in already implemented manufacturing systems},
journal = {Procedia Computer Science},
volume = {204},
pages = {14-21},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922007402},
author = {Marina Klees and Safa Evirgen},
keywords = {predictive maintenance, smart maintenance, big data analytics, Sensor Data Analytics},
abstract = {Predictive analytics methods have become increasingly important in Manufacturing Organization in the context of Smart Maintenance. Standardized process models for data mining already known to search existing data stocks for patterns, trends and correlations. Sensors are progressively implemented in production machines to create a database for data mining processes. But the risk of Big Data, thus the risk of low quality data is probably high. For an economic consideration, the amount of investment in new measurement technology and infrastructure should be assessed. Organizations are confronted with the challenge of how much they have to invest to obtain a meaningful database. For this reason, it is important to research which existing approaches support the development of a sufficient database for predictive maintenance in manufacturing systems and provide a methodical framework.}
}
@article{RAMALLI2022140149,
title = {Automatic validation and analysis of predictive models by means of big data and data science},
journal = {Chemical Engineering Journal},
pages = {140149},
year = {2022},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2022.140149},
url = {https://www.sciencedirect.com/science/article/pii/S1385894722056297},
author = {E. Ramalli and T. Dinelli and A. Nobili and A. Stagni and B. Pernici and T. Faravelli},
keywords = {Model validation, Model analysis, Data science, Data mining, Big data, Knowledge extraction},
abstract = {Validation is an essential procedure in the development of a predictive model in several engineering fields. In addition, recent data analysis techniques and the increasing availability of data have the potential to provide a deeper understanding of experimental data and simulation models. This work proposes a systematic, objective, and automatic methodology to validate and analyze experiments and models from a high-level perspective. The proposed methodology exploits the opportunities offered by the ‘data ecosystem’ concept, combining data and model evaluation and providing an integrated set of techniques to produce synthetic but comprehensive insights about the experiment and the predictive model. The methodology focuses on data assessment of the experiments used in the process, the use of a trend similarity comparison index to measure the model performance, and data science techniques to systematically extract models’ behavior insight by analyzing a large number of validation results and linking them to the experiment characteristics. The automated proposed approach follows the generality principle and can be extended to different application domains in which predictive models are validated against big data in the chemical engineering domain. As a case study, the proposed methodology is applied with hundreds of experimental datasets to evaluate a kinetic model that describes the pyrolysis and combustion of hydrocarbons.}
}
@article{BACHECHI2022100292,
title = {Big Data Analytics and Visualization in Traffic Monitoring},
journal = {Big Data Research},
volume = {27},
pages = {100292},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100292},
url = {https://www.sciencedirect.com/science/article/pii/S221457962100109X},
author = {Chiara Bachechi and Laura Po and Federica Rollo},
keywords = {Traffic, Time series, Air quality maps, Real-time data, Spatio-temporal data, Smart cities},
abstract = {This paper presents a system that employs information visualization techniques to analyze urban traffic data and the impact of traffic emissions on urban air quality. Effective visualizations allow citizens and public authorities to identify trends, detect congested road sections at specific times, and perform monitoring and maintenance of traffic sensors. Since road transport is a major source of air pollution, also the impact of traffic on air quality has emerged as a new issue that traffic visualizations should address. Trafair Traffic Dashboard exploits traffic sensor data and traffic flow simulations to create an interactive layout focused on investigating the evolution of traffic in the urban area over time and space. The dashboard is the last step of a complex data framework that starts from the ingestion of traffic sensor observations, anomaly detection, traffic modeling, and also air quality impact analysis. We present the results of applying our proposed framework on two cities (Modena, in Italy, and Santiago de Compostela, in Spain) demonstrating the potential of the dashboard in identifying trends, seasonal events, abnormal behaviors, and understanding how urban vehicle fleet affects air quality. We believe that the framework provides a powerful environment that may guide the public decision-makers through effective analysis of traffic trends devoted to reducing traffic issues and mitigating the polluting effect of transportation.}
}
@incollection{VAIDYA2022409,
title = {Chapter 24 - Exploring performance and predictive analytics of agriculture data},
editor = {Ajith Abraham and Sujata Dash and Joel J.P.C. Rodrigues and Biswaranjan Acharya and Subhendu Kumar Pani},
booktitle = {AI, Edge and IoT-based Smart Agriculture},
publisher = {Academic Press},
pages = {409-436},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-823694-9},
doi = {https://doi.org/10.1016/B978-0-12-823694-9.00030-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823694900030X},
author = {Madhavi Vaidya and Shweta Katkar},
keywords = {Agriculture, Big data, Weka, J48, Talend, Crops, Analytics, Fertilizers, Prediction},
abstract = {The exponential growth and ubiquity of both structured and unstructured data have led us into the big data era. Big data analytics is increasingly becoming a trending practice that many organizations are adopting to construct valuable information from big data. This field has substantially attracted academics, practitioners, and industries. But there are some challenges for big data processing and analytics that include integration of data, volume ofdata, the rate of transformation of data, and the veracity and validity of data. The history of griculture in India dates back to the Indus Valley civilization. Due to variations in climatic conditions, it has become challenging to achieve the desired results in crop yields. The use of technology in agriculture has increased in recent years and data analytics is one such trend that has penetrated the agriculture field. The main challenge in using big data in agriculture is identifying the effectiveness of big data analytics. Big data analysis can be processed and analyzed using parallel databases such as Talend or analytical paradigms like MapReduce on a Hadoop distributed file system. There are other mechanisms such as Weka and R, which are two of the most popular data analytical and statistical computing tools produced by the open source community, but there are certain challenges compared to the other techniques mentioned. In this chapter, the comparative studies of various mechanisms will be provided that will give an insight to process and analyze big data generated from farms and the grains obtained from it according to the seasons, the soil health, and the location. In addition, various case studies are shown for data processing in context with planting, agricultural growth, and smart farming. From the experimentation, the authors have shown which is the right fertilizer for a specific state and soil. In addition, the authors have worked on the analysis of crop production per state and per year.}
}
@article{XIN2022100197,
title = {Review on A big data-based innovative knowledge teaching evaluation system in universities},
journal = {Journal of Innovation & Knowledge},
volume = {7},
number = {3},
pages = {100197},
year = {2022},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2022.100197},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X22000373},
author = {Xu Xin and Yu Shu-Jiang and Pang Nan and Dou ChenXu and Li Dan},
keywords = {Big data, Knowledge teaching evaluation, Performance management},
abstract = {With the widespread use of digital technologies such as big data, cloud computing and artificial intelligence in higher education, how to establish a scientific and systematic evaluation system to turn the traditional classroom with the one-way transmission of knowledge into an interactive space for exchanging ideas and inspiring wisdom has become an essential task for human resource management in universities, and a key to improving teaching quality. However, due to the debate between scientism and humanism in teaching evaluation, studies related to teaching performance have been isolated from human resource management, resulting in the lack of a systematic vision and framework for such studies. Relevant studies are still limited to the evaluation contents of different evaluation subjects. Evaluations also tend to focus only on the teaching process, ignoring the objectives of talent training, making it difficult for evaluations to play a goal-oriented role and hindering the further development of relevant studies. Therefore, this paper draws on human resource management methodologies and analyzes knowledge teaching evaluation system characteristics in colleges and universities in a big data context to construct a “multiple evaluations, trinity and four-step closed-loop” big data-based knowledge teaching evaluation system. “Trinity” represents evaluation from three performance dimensions: teaching effect, teaching behavior and teaching ability. “Multiple evaluations” represents the design of teaching performance indicators based on teaching data, breaking the barriers between different evaluation subjects. “Four-step closed-loop” draws on performance management theory to standardize the teaching performance management process from four aspects: planning, implementation, evaluation, and feedback. This evaluation system provides a systematic methodology for unifying the theory and practice of innovative knowledge teaching evaluation system in universities in a big data context.}
}
@article{SUN2022112331,
title = {Understanding building energy efficiency with administrative and emerging urban big data by deep learning in Glasgow},
journal = {Energy and Buildings},
volume = {273},
pages = {112331},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112331},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822005023},
author = {Maoran Sun and Changyu Han and Quan Nie and Jingying Xu and Fan Zhang and Qunshan Zhao},
keywords = {Building energy efficiency, Energy performance certificate, Deep learning, Google street view, SHapley additive explanations},
abstract = {With buildings consuming nearly 40% of energy in developed countries, it is important to accurately estimate and understand the building energy efficiency in a city. A better understanding of building energy efficiency is beneficial for reducing overall household energy use and providing guidance for future housing improvement and retrofit. In this research, we propose a deep learning-based multi-source data fusion framework to estimate building energy efficiency. We consider the traditional factors associated with the building energy efficiency from the Energy Performance Certificate (EPC) for 160,000 properties (30,000 buildings) in Glasgow, UK (e.g., property structural attributes and morphological attributes), as well as the Google Street View (GSV) building façade images as a complement. We compare the performance improvements between our data-fusion framework with traditional morphological attributes and image-only models. The results show that including the building façade images from GSV, the overall model accuracy increases from 79.7% to 86.8%. A further investigation and explanation of the deep learning model are conducted to understand the relationships between building features and building energy efficiency by using SHapley Additive exPlanations (SHAP). Our research demonstrates the potential of using multi-source data in building energy efficiency prediction with high accuracy and short inference time. Our paper also helps understand building energy efficiency at the city level to help achieve the net-zero target by 2050.}
}
@article{WANG2022643,
title = {Does city construction improve life quality?-evidence from POI data of China},
journal = {International Review of Economics & Finance},
volume = {80},
pages = {643-653},
year = {2022},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1059056022000041},
author = {Yang Wang and Hong Zhang and Libing Liu},
keywords = {Quality of life, Point of interest, Happiness},
abstract = {To explore the construction of a big data indicator system is conducive to a comprehensive, scientific, timely and accurate grasp of the quality of life of our residents and its evolutionary trends. This paper systematically sorts out the performance dimensions of the residents' quality of life, and integrates two types of methods of objective observation and subjective evaluation commercial POI(Point of Interest) data. From the aspects of life, entertainment, transportation, etc., preliminary development has been made including 8 first-level indicators, 16 second-level indicators, and 27 third-level indicators Big data indicator system, and measure the "clogging point" of the improvement of residents' quality of life, with a view to providing a scientific and feasible decision-making reference for "meeting the people's increasing needs for a better life".}
}
@article{LIN2022120320,
title = {Enhanced commercial cooking inventories from the city scale through normalized emission factor dataset and big data},
journal = {Environmental Pollution},
volume = {315},
pages = {120320},
year = {2022},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2022.120320},
url = {https://www.sciencedirect.com/science/article/pii/S0269749122015342},
author = {Pengchuan Lin and Jian Gao and Yisheng Xu and James J. Schauer and Jiaqi Wang and Wanqing He and Lei Nie},
keywords = {Point of interest data, Emission inventory, Cooking emission factors, Dataset, Beijing},
abstract = {Cooking emission inventories always have poor spatial resolutions when applying with traditional methods, making their impacts on ambient air and human health remain obscure. In this study, we created a systematic dataset of cooking emission factors (CEFs) and applied it with a new data source, cooking-related point of interest (POI) data, to build up highly spatial resolved cooking emission inventories from the city scale. Averaged CEFs of six particulate and gaseous species (PM, OC, EC, NMHC, OVOCs, VOCs) were 5.92 ± 6.28, 4.10 ± 5.50, 0.05 ± 0.05, 22.54 ± 20.48, 1.56 ± 1.44, and 7.94 ± 6.27 g/h normalized in every cook stove, respectively. A three-field CEF index containing activity and emission factor species was created to identify and further build a connection with cooking-related POI data. A total of 95,034 cooking point sources were extracted from Beijing, as a study city. In downtown areas, four POI types were overlapped in the central part of the city and radiated into eight distinct directions from south to north. Estimated PM/VOC emissions caused by cooking activities in Beijing were 4.81/9.85 t per day. A 3D emission map showed an extremely unbalanced emission density in the Beijing region. Emission hotspots were seen in Central Business District (CBD), Sanlitun, and Wangjing in Chaoyang District and Willow and Zhongguancun in Haidian District. PM/VOC emissions could be as high as 16.6/42.0 kg/d in the searching radius of 2 km. For PM, the total emissions were 417.4, 389.0, 466.9, and 443.0 t between Q1 and Q4 2019 in Beijing, respectively. The proposed methodology is transferrable to other Chinese cities for deriving enhanced commercial cooking inventories and potentially highlighting the further importance of cooking emissions on air quality and human health.}
}
@article{FANG2022104070,
title = {BIM-integrated portfolio-based strategic asset data quality management},
journal = {Automation in Construction},
volume = {134},
pages = {104070},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104070},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005215},
author = {Zigeng Fang and Yan Liu and Qiuchen Lu and Michael Pitt and Sean Hanna and Zhichao Tian},
keywords = {Strategic asset management (SAM), Building information modeling (BIM), Portfolio management, Data quality management},
abstract = {A building's strategic asset management (SAM) capability has traditionally been limited by its site-based management. With the emergence of needs from clients about delivering a long-term portfolio-based building asset management plan that minimizes the asset risk and optimizes the value of their asset portfolios, SAM Units have emerged as a new business form to provide various SAM services to their clients. However, the quality of their current data model is still hindered by many issues, such as missing important attributes and the lack of customized information flow guidance. In addition, there is a gap in integrating their existing data collection with various data sources and Building Information Modeling (BIM) to enhance their data quality. By evaluating a SAM Unit's portfolio case study, this paper identifies the factors limiting the quality of SAM Units' data model and develops a guide to integrating various data sources better. We develop a BIM-integrated portfolio-based SAM information flow framework and a detailed hierarchical portfolio-based non-geometric data structure. The proposed framework and data structure will help SAM professionals, building asset owners, and other facilities management professionals embrace the benefits of managing the portfolio-based SAM data.}
}
@article{ZHANG2022103231,
title = {Orchestrating big data analytics capability for sustainability: A study of air pollution management in China},
journal = {Information & Management},
volume = {59},
number = {5},
pages = {103231},
year = {2022},
note = {Big Data Analytics for Sustainability},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103231},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619302010},
author = {Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu},
keywords = {Big data, Big data analytics, Sustainability, Air pollution, Resource orchestration},
abstract = {Under rapid urbanization, cities are facing many societal challenges that impede sustainability. Big data analytics (BDA) gives cities unprecedented potential to address these issues. As BDA is still a new concept, there is limited knowledge on how to apply BDA in a sustainability context. Thus, this study investigates a case using BDA for sustainability, adopting the resource orchestration perspective. A process model is generated, which provides novel insights into three aspects: data resource orchestration, BDA capability development, and big data value creation. This study benefits both researchers and practitioners by contributing to theoretical developments as well as by providing practical insights.}
}
@article{JIA202255,
title = {Data Quality and Usability Assessment Methodology for Prognostics and Health Management: A Systematic Framework},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {55-60},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.183},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322013969},
author = {Xiaodong Jia and Da-Yan Ji and Takanobu Minami and Jay Lee},
keywords = {Intelligence Maintenance, Data Quality, Artificial Intelligence, Industry 4.0},
abstract = {Collecting useful and informative data play an essential role in ensuring the performance of data-driven solutions for intelligent maintenance. However, there is still a lack of methodology to systematically assess the data usefulness (or data suitability) for modeling. This lack of data suitability assessment becomes a more pressing issue in the big data environment where a large volume of machine data is generated at a high velocity. Therefore, there are imperative needs for standardized procedures and systematic solutions that can scan through a large amount of data to quantify the data suitability and locate the useful datasets for model development. To fill in this gap, this paper proposes a novel methodology to evaluate the data suitability for PHM modeling from the aspects of detectability assessment, diagnosability assessment, and prognosability assessment. In the discussion, new assessment procedures and algorithms are proposed by using a series of similarity metrics between data vectors or data distribution. Also, the proposed methods provide both visualization tools and quantitative metrics to assess the data suitability. The effectiveness of the methodology is demonstrated by using real-world examples about the ball screw degradation and boring tool degradation. The results successfully demonstrate the effectiveness and practicality of the proposed methodology and analytics.}
}
@article{GRIGG2022100619,
title = {DUG Insight: A software package for big-data analysis and visualisation, and its demonstration for passive radar space situational awareness using radio telescopes},
journal = {Astronomy and Computing},
volume = {40},
pages = {100619},
year = {2022},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2022.100619},
url = {https://www.sciencedirect.com/science/article/pii/S2213133722000452},
author = {D. Grigg and S.J. Tingay and M. Sokolowski and R.B. Wayth},
keywords = {Space situational awareness, , Radio astronomy, Interactive workflow creation, High performance computing},
abstract = {As the demand for software to support the processing and analysis of massive radio astronomy datasets increases in the era of the SKA, we demonstrate the interactive workflow building, data mining, processing, and visualisation capabilities of DUG Insight. We test the performance and flexibility of DUG Insight by processing almost 68,000 full sky radio images produced from the Engineering Development Array (EDA2) over the course of a three day period. The goal of the processing was to passively detect and identify known Resident Space Objects (RSOs: satellites and debris in orbit) and investigate how radio interferometry could be used to passively monitor aircraft traffic. These signals are observable due to both terrestrial FM radio signals reflected back to Earth and out-of-band transmission from RSOs. This surveillance of the low Earth orbit and airspace environment is useful as a contribution to space situational awareness and aircraft tracking technology. From the observations, we made 40 detections of 19 unique RSOs within a range of 1,500 km from the EDA2. This is a significant improvement on a previously published study of the same dataset and showcases the flexible features of DUG Insight that allow the processing of complex datasets at scale. Future enhancements of our DUG Insight workflow will aim to realise real-time acquisition, detect unknown RSOs, and continue to process data from SKA-relevant facilities.}
}
@article{MACHADO2022263,
title = {Data Mesh: Concepts and Principles of a Paradigm Shift in Data Architectures},
journal = {Procedia Computer Science},
volume = {196},
pages = {263-271},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022365},
author = {Inês Araújo Machado and Carlos Costa and Maribel Yasmina Santos},
keywords = {Big Data, Data Mesh, Data Architectures, Data Lake},
abstract = {Inherent to the growing use of the most varied forms of software (e.g., social applications), there is the creation and storage of data that, due to its characteristics (volume, variety, and velocity), make the concept of Big Data emerge. Big Data Warehouses and Data Lakes are concepts already well established and implemented by several organizations, to serve their decision-making needs. After analyzing the various problems demonstrated by those monolithic architectures, it is possible to conclude about the need for a paradigm shift that will make organizations truly data-oriented. In this new paradigm, data is seen as the main concern of the organization, and the pipelining tools and the Data Lake itself are seen as a secondary concern. Thus, the Data Mesh consists in the implementation of an architecture where data is intentionally distributed among several Mesh nodes, in such a way that there is no chaos or data silos, since there are centralized governance strategies and the guarantee that the core principles are shared throughout the Mesh nodes. This paper presents the motivation for the appearance of the Data Mesh paradigm, its features, and approaches for its implementation.}
}
@article{LIN2022103680,
title = {How big data analytics enables the alliance relationship stability of contract farming in the age of digital transformation},
journal = {Information & Management},
volume = {59},
number = {6},
pages = {103680},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103680},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000891},
author = {Shunzhi Lin and Jiabao Lin and Feiyun Han and Xin (Robert) Luo},
keywords = {Big data analytics, Risk management capability, Data quality, Alliance relationship stability},
abstract = {Notwithstanding the potential of big data analytics technology for alliance management, there is a lack of understanding of how such digital technology influences alliance relationship stability (ARS). Drawing on the information technology-enabled organizational capabilities (IT-enabled OCs) perspective, this study empirically verifies that big data analytics promotes ARS and risk management capability. Moreover, market risk management capability (MRM) enhances ARS, and data quality moderates the relationship between big data analytics usage (BDU) and MRM. This research reveals the impact mechanism of BDU on the ARS. Implications for management and future research are presented as well.}
}
@article{CHENG2022109585,
title = {Regional metal pollution risk assessment based on a big data framework: A case study of the eastern Tianshan mining area, China},
journal = {Ecological Indicators},
volume = {145},
pages = {109585},
year = {2022},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2022.109585},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X22010585},
author = {Yinyi Cheng and Kefa Zhou and Jinlin Wang and Shichao Cui and Jining Yan and Philippe {De Maeyer} and Tim {Van de Voorde}},
keywords = {Big data technology, Heavy metal pollution, Risk assessment, Index evaluation system, Eastern Tianshan},
abstract = {With the increasing maturity of big data technology, its application in the field of environmental assessment has become an important issue. The mining of mineral resource can affect the balance in the ecological environment surrounding mining areas and the normal life activities of humans through groundwater. To solve this problem, big data-based methods were used to evaluate the pollution risk in mining areas in this article. Based on a spatial big data management framework, in this paper, high-risk areas in eastern Tianshan were quantitatively analyzed and delineated via a new comprehensive heavy metal pollution assessment system. A distributed storage environment, unstructured management method and spatial index coding were used to uniformly manage spatial data in vector and raster formats retrieved from different sources. A system involving 18 geological environment indicators was used to evaluate the risk level in the study area and in delineated areas with a high risk of heavy metal pollution. The results indicated that the proposed framework could efficiently store and process spatial data and realize environmental pollution risk assessment in a big data environment. In addition, some of the high-pollution risk areas identified based on the assessment results were consistent with actual mine locations. Overall, the results show that integrating multi-source geological characteristics through a comprehensive evaluation system based on big data can have a positive effect on improving the accuracy of heavy metal pollution risk assessment. Heavy metal pollution assessment can provide a reference for environmental monitoring and governance of the eastern Tianshan region and offer new solutions to large-scale heavy metal pollution assessment.}
}
@incollection{CHANG202221,
title = {Chapter 2 - Investigation of COVID-19 and scientific analysis big data analytics with the help of machine learning},
editor = {Victor Chang and Mohamed Abdel-Basset and Muthu Ramachandran and Nicolas G. Green and Gary Wills},
booktitle = {Novel AI and Data Science Advancements for Sustainability in the Era of COVID-19},
publisher = {Academic Press},
pages = {21-66},
year = {2022},
isbn = {978-0-323-90054-6},
doi = {https://doi.org/10.1016/B978-0-323-90054-6.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323900546000076},
author = {Victor Chang and Mohamed Aleem Ali and Alamgir Hossain},
keywords = {COVID-19 review, AU methods for COVID-19, Machine learning for COVID-19},
abstract = {This book chapter presents the review of COVID-19 and its status, as well as the scientific Analysis big data analytics with the help of machine learning. We provide in-depth literature review, and provide a summary of the current AI and machine learning methods, which have become increasingly important to provide accurate analyses. Various conceptual diagrams have been used to illustrate how different technologies can contribute to effective analyses for COVID-19. We demonstrate our work from both theoretical contributions and practical implementations.}
}
@article{RUSSELL2022108709,
title = {Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring},
journal = {Mechanical Systems and Signal Processing},
volume = {168},
pages = {108709},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108709},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021010293},
author = {Matthew Russell and Peng Wang},
keywords = {Physics-informed deep learning, Prognostics and health management, Data compression, Big data},
abstract = {The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.}
}
@article{LI2022121355,
title = {Evaluating the impact of big data analytics usage on the decision-making quality of organizations},
journal = {Technological Forecasting and Social Change},
volume = {175},
pages = {121355},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121355},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521007861},
author = {Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo},
keywords = {Big data analytics usage, Data analytics capabilities, Decision-making quality, Agricultural firms},
abstract = {Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.}
}
@incollection{SEBASTIANCOLEMAN202231,
title = {Chapter 2 - Organizational Data and the Five Challenges of Managing Data Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {31-45},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821737500002X},
author = {Laura Sebastian-Coleman},
keywords = {Data, history of data, data quality, data management, data governance, data stewardship, data and technology, process improvement, technology strategy, culture/organization, data literacy},
abstract = {This chapter describes the five challenges in data quality management (data, process, technology, people, and culture/organization) and proposes that organizations that want to get more value and insight from their data should take a strategic approach to data quality management. This is because quality is not an accident, and it cannot be an afterthought, especially in today’s complex organizations. This chapter provides the context for Section 2 and introduces critical terminology used throughout the book.}
}
@article{CORALLO2022102331,
title = {Model-based Big Data Analytics-as-a-Service framework in smart manufacturing: A case study},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {76},
pages = {102331},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102331},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000205},
author = {Angelo Corallo and Anna Maria Crespino and Mariangela Lazoi and Marianna Lezzi},
keywords = {Big Data Analytics, BDA, MBDAaaS framework, Smart manufacturing, Industry 4.0, Anomaly detection},
abstract = {Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain.}
}
@article{SCHMUCKER2022100061,
title = {Measuring tourism with big data? Empirical insights from comparing passive GPS data and passive mobile data},
journal = {Annals of Tourism Research Empirical Insights},
volume = {3},
number = {2},
pages = {100061},
year = {2022},
issn = {2666-9579},
doi = {https://doi.org/10.1016/j.annale.2022.100061},
url = {https://www.sciencedirect.com/science/article/pii/S2666957922000295},
author = {Dirk Schmücker and Julian Reif},
keywords = {Big Data, Mobile Network Data, Passive GPS Data, Spatio-temporal behaviour, Tourist classification},
abstract = {In this paper we aim to classify digital data sources for the measurement of tourist mobility, to establish a set of assessment indicators, and to compare two Big Data sources to gain empirical insights into how we can measure tourism with Big Data. For three holiday destinations in Germany, passive mobile data and passive global positioning systems (GPS) data are compared with reference data from the destinations for twelve weeks in the summer of 2019. Results show that mobile network data are on a plausible level compared to the local reference data and are able to predict the temporal pattern to a very high degree. GPS app-based data also perform well, but are less plausible and precise than mobile network data.}
}
@article{LIU2022103138,
title = {Effects of governmental data governance on urban fire risk: A city-wide analysis in China},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103138},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103138},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003570},
author = {Zhao-Ge Liu and Xiang-Yang Li and Grunde Jomaas},
keywords = {Urban fire risk, Fire risk management, Big data technologies, Data governance, Socio-economic factors, City-wide analysis},
abstract = {The effects of data governance (as a means to maximize big data value creation in fire risk management) performance on fire risk was analyzed based on multi-source statistical data of 105 cities in China from 2016 to 2018. Specifically, data governance was first quantified with ten detailed indicators, which were then selected for explaining urban fire risk through correlation analysis. Next, the sample cities were clustered in terms of major socio-economic characteristics, and then the effects of data governance were examined by constructing multivariate regression models for each city cluster with ordinary least squares (OLS). The results showed that the constructed regression models produced good interpretation of fire risk in different types of cities, with coefficient of determination (R2) in each model exceeding 0.65. Among the indicators, the development of infrastructures (e.g. data collection devices and data analysis platforms), the level of data use, and the updating of fire risk related data were proved to produce significant effects on the reduction of fire frequency and fire consequence. Moreover, the organizational maturity of data governance was proved to be helpful in reducing fire frequency. For the cities with large population, the cross-department sharing of high-value data was found to be another important determinant of urban fire frequency. In comparison with existing statistical models which interpreted fire risk with general social factors (with the highest R2 = 0.60), these new regression models presented a better statistical performance (with the average R2 = 0.72). These findings are expected to provide decision support for the local governments of China and other jurisdictions to facilitate big data projects in improving fire risk management.}
}
@article{KURNI2022103324,
title = {MRPO-Deep maxout: Manta ray political optimization based Deep maxout network for big data intrusion detection using spark architecture},
journal = {Advances in Engineering Software},
volume = {174},
pages = {103324},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103324},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822002253},
author = {Muralidhar Kurni and Mujeeb S． Md and Bharath Bhushan Yannam and Arun Singh T},
keywords = {Intrusion detection system, Big data, Apache spark, Data augmentation, Deep maxout network},
abstract = {An intrusion detection system (IDS) is used to examine as well as analyze data for detecting intrusions in a network or system. This paper proposes an efficient intrusion detection method, named Manta Ray Political Optimization (MRPO)-based Deep maxout network. Here, the training procedure of the Deep maxout network is achieved by the proposed MRPO algorithm that is derived by integrating Manta Ray Foraging Optimization (MRFO) and Political Optimizer (PO). Initially, different subsets of data are formed by partitioning the input data and each data subset is processed by slave nodes to pre-process the data. The wrapper-based model and fisher score are used to select features such that the fusion of these features is based on Hellinger distance. With selected features, the dimensionality of data is increased using the data augmentation and based on data augmented output, and Deep maxout network is applied to detect normal and abnormal behavior. The proposed method attains maximum testing accuracy, True Positive Rate (TPR), and True Negative Rate (TNR) of 0.939, 0.943, and 0.910 by the Apache webserver dataset. When the training data is 90, the accuracy of the proposed method is 9.15%, 6.38%, 5.75%, 2.66%, 2.12%, 1.49%, 074%, 1.70%, and 4.79% higher when compared to the existing approaches namely, Hybrid deep learning, RCCRO-FCM, HHO-based DBN, CGO+ensemble SVM, Deep maxout network, PO-based Deep maxout network, MRFO- based Deep maxout network, STL-HDL, and ExpSLO enabled DRN.}
}
@article{SHEN2022102529,
title = {Personal big data pricing method based on differential privacy},
journal = {Computers & Security},
volume = {113},
pages = {102529},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102529},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821003539},
author = {Yuncheng Shen and Bing Guo and Yan Shen and Xuliang Duan and Xiangqian Dong and Hong Zhang and Chuanwu Zhang and Yuming Jiang},
keywords = {Personal big data, Data privacy, Privacy protection, Differential privacy, Positive pricing, Reverse pricing, Privacy budget, Privacy compensation},
abstract = {Personal big data can greatly promote social management, business applications, and personal services, and bring certain economic benefits to users. The difficulty with personal big data security and privacy protection lies in realizing the maximization of the value of personal big data and in striking a balance between data privacy protection and sharing on the premise of satisfying personal big data security and privacy protection. Thus, in this paper, we propose a personal big data pricing method based on differential privacy (PMDP). We design two different mechanisms of positive and reverse pricing to reasonbly price personal big data. We perform aggregate statistics on an open dataset and extensively evaluated its performance. The experimental results show that PMDP can provide reasonable pricing for personal big data and fair compensation to data owners, ensuring an arbitrage-free condition and finding a balance between privacy protection and data utility.}
}
@article{YANG2022108322,
title = {Data quality assessment and analysis for pest identification in smart agriculture},
journal = {Computers and Electrical Engineering},
volume = {103},
pages = {108322},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108322},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622005444},
author = {Jiachen Yang and Guipeng Lan and Yang Li and Yicheng Gong and Zhuo Zhang and Sezai Ercisli},
keywords = {Smart agriculture, Pest identification, Information entropy, Data-centric},
abstract = {Deep learning has played a crucial role in the field of smart agriculture and been widely used in various applications. However, the deep learning models are constrained by data quality, which means poor data quality and unreliable data annotation will seriously restrict the performance of smart applications. In this paper, we proposed two methods to assess data quality, named Bound-DE and Multi-Branch. In experiments, the IP06 dataset and the ResNet-18 backbone network were adopted. The results show that the redundancy of the used public dataset is so large that about 50% of the data can achieve the similar test accuracy. Furthermore, we also analyzed the high contributive samples and summarized the rules of those selected informative samples, which is significant for the design of high-efficiency datasets. In summary, this study guides and promotes the following data-centric research in the field of smart agriculture.}
}
@article{SOHRABI2022101122,
title = {Data validation techniques used in admission discharge and transfer systems: Necessity of use and effect on data quality},
journal = {Informatics in Medicine Unlocked},
volume = {34},
pages = {101122},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101122},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822002593},
author = {Atefe Sohrabi and Azam Sabahi and Ali Garavand and Leila Ahmadian},
keywords = {Hospital information system, Data quality, Data validity, Data control},
abstract = {Background
One of the most important functions of the admission unit is the speed of staff in receiving the information correctly and recording them on the computer, and providing suitable conditions for continuing the patient's treatment. The occurrence of errors in data recording affects the quality of care and may have unfavorable consequences. The application of data validation techniques can help users to record information correctly.
Objectives
Hence, the present study aims to identify the techniques and determine the level of compliance with them, their necessity in Admission Discharge Transfer (ADT) systems, and their effect on data quality.
Methods
This observational study investigating data quality in the ADT systems was carried out in two stages. In the first stage, a checklist of data control techniques was developed by reviewing the literature. The admission unit of hospital information systems in different units of the teaching hospitals of Kerman (in terms of application of data validation techniques) was evaluated. In the next stage, a researcher-made questionnaire was designed, and users' views on the necessity of using these techniques in HIS and their effect on data quality were evaluated. Data analysis was done using frequency, standard deviation, and mean and Friedman ranking test.
Results
In this study, six data techniques were investigated in the studied systems. Users gave the highest mean for the necessity of applying data control techniques for the first name, surname, and national code data items and the lowest mean for the data items related to vital signs. In the ranking of data validation techniques in terms of the importance of their application, the highest rank was obtained in the components of pattern control (17.29), and the lowest rank was obtained in delta control (2.91). In the general ranking of the effect of techniques on data quality, pattern control (5.95) and interval control (4.95), respectively, obtained the highest mean (out of 6).
Conclusions
The results revealed that the existing HISs had fewer benefits of using data control. However, users not only consider it necessary to use these controls in the systems but also believe that their application will improve their performance in recording information. Thus, for accurate documentation, HIS designers must implement these techniques in these systems, and their application should become a mandatory standard for system development.}
}
@article{JOSE2022100081,
title = {Integrating big data and blockchain to manage energy smart grids—TOTEM framework},
journal = {Blockchain: Research and Applications},
volume = {3},
number = {3},
pages = {100081},
year = {2022},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2022.100081},
url = {https://www.sciencedirect.com/science/article/pii/S2096720922000227},
author = {Dhanya Therese Jose and Jørgen Holme and Antorweep Chakravorty and Chunming Rong},
keywords = {Blockchain, Big data analytic, Hyperledger fabric, Hadoop, MapReduce, Docker, PIVT},
abstract = {The demand for electricity is increasing exponentially day by day, especially with the arrival of electric vehicles. In the smart community neighborhood project, electricity should be produced at the household or community level and sold or bought according to the demands. Since the actors can produce, sell, and buy according to the demands, thus the name prosumers. ICT solutions can contribute to this in several ways, such as machine learning for analyzing the household data for customer demand and peak hours for the usage of electricity, blockchain as a trustworthy platform for selling or buying, data hub, and ensuring data security and privacy of prosumers. TOTEM: Token for controlled computation is a framework that allows users to analyze the data without moving the data from the data owner's environment. It also ensures the data security and privacy of the data. Here, in this article, we will show the importance of the TOTEM architecture in the EnergiX project and how the extended version of TOTEM can be efficiently merged with the demands of the current and similar projects.}
}
@article{BALTI20221,
title = {Multidimensional architecture using a massive and heterogeneous data: Application to drought monitoring},
journal = {Future Generation Computer Systems},
volume = {136},
pages = {1-14},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001753},
author = {Hanen Balti and Ali Ben Abbes and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle},
keywords = {Big data, Data storage, Spatio-temporal querying, Decision-making, Earth observation, Disaster management},
abstract = {The rapid increase in the number of Earth Observation (EO) systems generates a massive amount of heterogeneous data. It has raised big issues in collecting, preprocessing, storing, and the visualization these data. However, traditional techniques are facing serious challenges when dealing with big EO data dimensions (i.e., Volume, Veracity, Variety, and Velocity), especially in natural hazards management. Therefore, big data techniques and tools attract more attention. In this paper we propose a multidimensional model framework for Big EO data warehousing. This framework includes 3 parts: (1) Data collection and preprocessing, being responsible for collecting data and improving their quality; (2) Data loading and storage, performing the ingestion task which consists of transferring the data from external resources to the Big data platform for storage; and (3) Visualization and interpretation, aiming to provide spatio-temporal analysis. This framework could be useful for decision-makers in monitoring the effects of drought disasters and, consequently, planning the mitigation and remediation measures. Experiments are carried out on drought monitoring in China along the period 2000–2020. The input data include remote sensing data, biophysical data, and climatological data. The results reveal that the proposed framework has a higher retrieval speed and a greater elasticity with different kinds (i.e. spatial, temporal, or spatiotemporal) of requests compared to traditional frameworks, indicating its superiority.}
}