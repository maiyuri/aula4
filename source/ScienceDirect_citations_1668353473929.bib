@article{MAO2021125081,
title = {How can bicycle-sharing have a sustainable future? A research based on life cycle assessment},
journal = {Journal of Cleaner Production},
volume = {282},
pages = {125081},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.125081},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620351258},
author = {Guozhu Mao and Tianyi Hou and Xi Liu and Jian Zuo and Abdul-Hakim Ibrahim Kiyawa and Pingping Shi and Sukhbir Sandhu},
keywords = {Life cycle assessment, Bicycle-sharing, Environmental impact, Sustainable},
abstract = {Bicycle-sharing is experiencing explosive growth in China, and it is expected to be an environmental friendly practice, however, mass production and insufficient recycling of shared bicycles may bring great negative environmental impact. In this study, we conduct a life cycle assessment (LCA) of bicycle-sharing in China to estimate the negative environmental impacts of the stages of the whole life cycle with nine environmental impact categories. The results show that the production stage contributes to the greatest negative environmental impacts by an average rate of 81.18% among different impact categories, much higher than other stages, i.e. the use stage, daily management and transportation stage and waste treatment and recycling stage. Specifically, the use of aluminum at the production stage contributes to the most in nine environmental impacts categories (55.43% on average). And rubber is another relatively important contributor to these nine environmental impact categories (16.27% on average). In addition, the cumulative production of excessive parts is expected to bring further environmental impacts at the production stage. To promote the sustainable development of bicycle-sharing, we discuss various potential opportunities at the production and maintenance stage, materials selection for better durability and longer service life, the structure design for the ease of maintenance. Besides, at the waste treatment and recycling stage, we promote that the recycling system is necessary for generating environmental benefits considering the potential huge supply and demand of the market. The construction of a bicycle-sharing industry chain may be an effective way for the sustainable development of bicycle-sharing in the future.}
}
@article{KRITTANAWONG202188,
title = {Social media and predictive analysis regarding dietary approaches to stop hypertension},
journal = {Progress in Cardiovascular Diseases},
volume = {68},
pages = {88-90},
year = {2021},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2021.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0033062021000748},
author = {Chayakrit Krittanawong and Scott Kaplin and W.H. Wilson Tang and Hani Jneid and Salim S. Virani and Franz H. Messerli},
keywords = {Social media, DASH, DASH diet, Social media analysis, Facebook, Twitter}
}
@article{THAKKAR202112,
title = {Comparative anatomization of data mining and fuzzy logic techniques used in diabetes prognosis},
journal = {Clinical eHealth},
volume = {4},
pages = {12-23},
year = {2021},
issn = {2588-9141},
doi = {https://doi.org/10.1016/j.ceh.2020.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2588914120300113},
author = {Harshil Thakkar and Vaishnavi Shah and Hiteshri Yagnik and Manan Shah},
keywords = {Data mining, Fuzzy logic, Diabetes, Health care},
abstract = {Diabetes is an ailment in which glucose level increase in at high rates in blood due to body’s inability to metabolize it. This happens when body does not produce sufficient amount of insulin or it does not respond to it properly. Critical and long-term health issues arise if diabetes is not handled or properly treated which includes: heart problems, disorders of the lungs, skin and liver complications, nerve damage, etc. With increasing number of diabetic patients, its early detection becomes essential. In this paper, our major focus areas are data mining and fuzzy logic techniques used in diabetes diagnosis. Data mining is used for locating patterns in huge datasets using a composition of different methods of machine learning, database manipulations and statistics. Data mining offers a lot of methods to inspect large data considering the expected outcome to find the hidden knowledge. Fuzzy logic is similar to human reasoning system and hence it can handle the uncertainties found in the data of medical diagnosis. These systems are called expert systems. The fuzzy expert systems (FES) analyze the knowledge from the available data which might be vague and suggests linguistic concept with huge approximation as its core to medical texts. In this paper, the methodology section delivers the pipeline of various tasks such as selecting the dataset, preprocessing the data by applying numerous methods such as standardization, normalization etc. After that, feature extraction technique is implemented on the dataset for improving the accuracy and finally dataset worked on data mining and fuzzy logic various classification algorithms. While analyzing different data mining methods, the accuracy computed through random forest classifiers as high as 99.7% and in case of numerous fuzzy logic approaches, high precision and low complexity was found to contribute a fairly high accuracy of 96%.}
}
@article{FISCHER2021268,
title = {Current applications of artificial intelligence in vascular surgery},
journal = {Seminars in Vascular Surgery},
volume = {34},
number = {4},
pages = {268-271},
year = {2021},
issn = {0895-7967},
doi = {https://doi.org/10.1053/j.semvascsurg.2021.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S089579672100065X},
author = {Uwe M. Fischer and Paula K. Shireman and Judith C. Lin},
abstract = {ABSTRACT
Basic foundations of artificial intelligence (AI) include analyzing large amounts of data, recognizing patterns, and predicting outcomes. At the core of AI are well-defined areas, such as machine learning, natural language processing, artificial neural networks, and computer vision. Although research and development of AI in health care is being conducted in many medical subspecialties, only a few applications have been implemented in clinical practice. This is true in vascular surgery, where applications are mostly in the translational research stage. These AI applications are being evaluated in the realms of vascular diagnostics, perioperative medicine, risk stratification, and outcome prediction, among others. Apart from the technical challenges of AI and research outcomes on safe and beneficial use in patient care, ethical issues and policy surrounding AI will present future challenges for its successful implementation. This review will give a brief overview and a basic understanding of AI and summarize the currently available and used clinical AI applications in vascular surgery.}
}
@article{RUMMENS2021125,
title = {The effect of spatiotemporal resolution on predictive policing model performance},
journal = {International Journal of Forecasting},
volume = {37},
number = {1},
pages = {125-133},
year = {2021},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2020.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169207020300558},
author = {Anneleen Rummens and Wim Hardyns},
keywords = {Predictive policing, Crime forecasting, Spatiotemporal forecasting, Decision making, Predictive modeling},
abstract = {Being able to anticipate crime such that new crime events can be dealt with effectively or prevented entirely, leads police forces worldwide to look at applying predictive policing, which provides predictions of times and places at risk for crime, such that proactive preventative measures can be taken. Ideally, predictive policing models predict crime at a high spatio-temporal level, while also providing optimal prediction performance. The main objective of this paper is therefore to evaluate the impact of varying grid resolution, temporal resolution and historical time frame on prediction performance. To investigate this, we analyse home burglary data from a large city in Belgium and predict new crime events using a range of parameter values, comparing the resulting prediction performances. Given the potential prediction performance costs associated with prediction at a high spatio-temporal resolution, consideration should be given to balance practical requirements with performance requirements.}
}
@article{ZHENG2021105610,
title = {Trilemma and tripartition: The regulatory paradigms of cross-border personal data transfer in the EU, the U.S. and China},
journal = {Computer Law & Security Review},
volume = {43},
pages = {105610},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105610},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000832},
author = {Guan Zheng},
keywords = {Trilemma, Tripartition, Personal data protection, Free cross-border flow, National jurisdiction},
abstract = {The regulation of the cross-border transfer of personal data is a major issue of globalization in the digital era. The key point for lawmakers is how to choose two of the following three elements in the trilemma: personal data protection, free transborder flow of information and the expansion of national jurisdiction. The EU, the U.S. and China adopt their own decisions, resulting in three inherently incompatible legislative paradigms, which has led to the restricted flow of personal data around the world as well as the free flow in three different regions, with the EU, the U.S. and China as the center of each region. In this way, the regulating paradigms of cross-border personal data transfer presents a pattern of tripartition.}
}
@article{XU2021102482,
title = {Application of training data affects success in broad-scale local climate zone mapping},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {103},
pages = {102482},
year = {2021},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102482},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421001896},
author = {Chunxue Xu and Perry Hystad and Rui Chen and Jamon {Van Den Hoek} and Rebecca A. Hutchinson and Steve Hankey and Robert Kennedy},
keywords = {Local climate zone, Machine learning, Training areas, Crowdsourced data, Spatial autocorrelation},
abstract = {Satellite imagery has been widely used to map urbanization processes. To address the urgent need for urban landscape mapping that goes beyond urban footprint analysis, the local climate zone (LCZ) scheme has been increasingly used to reveal the urban forms and functions important to urban heat islands and micro-climates across the globe. As with most supervised classification strategies, proper application of training data is critical for the success of LCZ classification models. However, the collection and application of LCZ training areas brings with it two challenges that may affect mapping success. First, because digitizing training areas is a time-consuming task, there is a broad effort in the LCZ mapping community to create a crowdsourced data collection among different experts. However, this strategy likely leads to inconsistencies in labels that could weaken models. Second, the LCZ labeling process typically involves the delineation of large zones from which multiple training samples are drawn, but those samples are likely spatially autocorrelated and lead to overly optimistic estimates of model accuracy. Although both effects -- inconsistent labeling and spatial autocorrelation -- are theoretically possible, it is unknown whether they substantially affect accuracy. We investigated both issues, specifically asking: (i) how do the discrepancies of LCZ labeling by different experts impact broad-scale LCZ mapping? (ii) to what extent does spatial correlation affect model prediction power? We used two classifiers (Random Forests and ResNets) to map eight metropolitan areas in the US into LCZs, comparing training areas drawn by different or consistent interpreters, and data splitting strategy using rules that allow or reduce spatial autocorrelation. We found large discrepancies among results built from crowdsourced training areas digitized by different experts; improving the consistency of labels can lead to substantial improvements in LCZ classification accuracy. Second, we found that spatial autocorrelation can boost the apparent accuracy of the classifier by 16% to 21%, leading to erroneous interpretation of mapping results. The two effects interplay as well: spatial autocorrelation in the raw data can lead to an underestimation of the model’s predictive error when modeling with crowdsourced training areas of high inconsistency. Due to the uncertainty in the labeling process and spatial autocorrelation in derived training data, broad-scale LCZ mapping results should be interpreted with caution.}
}
@article{FORGET2021100886,
title = {Is multi-source feedback the future of perioperative medicine?},
journal = {Anaesthesia Critical Care & Pain Medicine},
volume = {40},
number = {3},
pages = {100886},
year = {2021},
issn = {2352-5568},
doi = {https://doi.org/10.1016/j.accpm.2021.100886},
url = {https://www.sciencedirect.com/science/article/pii/S2352556821000904},
author = {Patrice Forget and Karuna Dahlberg}
}
@article{HANEL2021210,
title = {Impact of Cyber-physically enhanced manufacturing on the product requirement documentation in high-tech applications},
journal = {Procedia CIRP},
volume = {102},
pages = {210-215},
year = {2021},
note = {18th CIRP Conference on Modeling of Machining Operations (CMMO), Ljubljana, Slovenia, June 15-17, 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.09.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121007794},
author = {Albrecht Hänel and André Seidel and Carl Willy Mehling and Alexander Dementyev and Karol Kozak and Rudi Seidel and Uwe Teicher and Arvid Hellmich and Welf-Guntram Drossel and Steffen Ihlenfeldt},
keywords = {Digital Manufacturing System, Modelling, Process control, Cyber-physical production},
abstract = {Conventional machining of high-tech parts and components is often associated with complex processes, long lead times and small to medium batch sizes. Obviously, there is great interest to shorten process development periods and increase process reliability. The transformation of conventional machines into cyber-physical production systems (CPPS) promises significant improvements here. In fact, CPPS allows real-time data acquisition and processing, enabling to target integrated process improvement and quality assurance. In the present work, a methodical procedure for structured data aggregation is introduced for a Cyber-physical enhanced machining process while being explained from a high-tech application point of view. This includes data generation, extraction, transfer and storage, data-consolidation, linkage, visualization and interpretation. Finally, the paper illustrates these aspects in terms of the effects on the “digital product requirements” in order to achieve the presented “analytics-ready” data model using the example of a high-tech application.}
}
@article{AGHDAM2021105903,
title = {The Role of the Internet of Things in Healthcare: Future Trends and Challenges},
journal = {Computer Methods and Programs in Biomedicine},
volume = {199},
pages = {105903},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105903},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720317363},
author = {Zahra Nasiri Aghdam and Amir Masoud Rahmani and Mehdi Hosseinzadeh},
keywords = {Internet of Things, Healthcare, Future Trends, Systematic Review},
abstract = {Background and Objective
With the recent advances in the Internet of Things (IoT), the field has become more and more developed in healthcare. The Internet of things will help physicians and hospital staff perform their duties comfortably and intelligently. With the latest advanced technologies, most of the challenges of using IoT have been resolved, and this technology can be a great revolution and has many benefits in the future of digital. Healthcare is one of the most useful areas for IoT use. The most important application of IoT is to monitor and make quick decisions in critical situations. Thanks to this technology-based treatment approach, there is an unprecedented opportunity to better the quality and productivity of treatments and better the patient's well-being and better government funding.
Methods
In this paper, we provide a comprehensive overview of the primary uses of IoT in healthcare. We used the Systematic Literature Review (SLR) method to analyze and comparison articles published in this field between 2015 and March 2020.
Results
A comprehensive taxonomy is presented based on the contents of the articles under study. In this article, a brief overview of selected articles based on research questions is given and highlights the most critical challenges and case studies for the future use of IoT in healthcare.
Conclusions
According to a detailed study of the 89 articles and a glimpse into about 208 articles, challenges and future trends in healthcare have been identified.}
}
@incollection{2021321,
title = {Glossary},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {321-329},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.09986-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150099862}
}
@incollection{PLOTKIN202193,
title = {Chapter 5 - Training the Business Data Stewards},
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {93-104},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822132700005X},
author = {David Plotkin},
keywords = {Training, Data Quality, Data Stewards, skills, roles, responsibilities},
abstract = {The outline of a training guide for new data stewards is presented here, along with explanations of items that are not otherwise included in this book.}
}
@article{CENTOBELLI2021120463,
title = {Surfing blockchain wave, or drowning? Shaping the future of distributed ledgers and decentralized technologies},
journal = {Technological Forecasting and Social Change},
volume = {165},
pages = {120463},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120463},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520312890},
author = {Piera Centobelli and Roberto Cerchione and Emilio Esposito and Eugenio Oropallo},
keywords = {Bibliometric analysis, Block-chain, Decentralized technology, Distributed ledger, Literature review, Network analysis, Performance analysis, Traceability, Tracking, Transparency, trust},
abstract = {Blockchain is a promising technology whose four TRs (TRaceability, TRacking, TRansparency, TRrust) features are bound to revolutionize material, information, financial flows and transactions inside and outside organisations. Many studies have been published showing the potential of this disruptive technology in many fields and this number is growing exponentially in recent years. This enormous amount of papers calls for a more systematic approach to analyse the overall trend in this research field. A bibliometric approach based on performance analysis and network analysis techniques is used to examine the evolution of blockchain technology research. Firstly, this paper contributes to the body of literature by discussing the most influential countries, authors, subject areas and journals of the current blockchain research. Secondly, this paper identifies six main clusters of blockchain-related research contributions and, based on the analysis on centrality and density measures, it classifies research themes in motor themes, basic themes, emerging or disappearing themes, and specialised themes. Despite the majority of contributions belong to the computer science subject area, many papers belonging to the technology management subject area provide pivotal insights for practitioners and policy makers. Specifically, they may exploit the results of this research to rethink many traditional processes in the light of blockchain technology implementation, exploit the benefits of the four TRs to manage processes, automate common tasks, generate actionable results, and improve daily operations.}
}
@article{MILLECAM20211922,
title = {Coming of age of Allotrope: Proceedings from the Fall 2020 Allotrope Connect},
journal = {Drug Discovery Today},
volume = {26},
number = {8},
pages = {1922-1928},
year = {2021},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2021.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S1359644621001653},
author = {Todd Millecam and Austin J. Jarrett and Naomi Young and Dana E. Vanderwall and Dennis {Della Corte}},
keywords = {Precompetitive consortium, Semantics, Metadata, Standard, Harmonization, Laboratory IT, Allotrope, Digital Lab},
abstract = {The Allotrope Foundation (AF) is a group of pharmaceutical, device vendor, and software companies that develops and releases technologies [the Allotrope Data Format (ADF), the Allotrope Foundation Ontology (AFO), and the Allotrope Data Models (ADM)] to simplify the exchange of electronic data. We present here the first comprehensive history of the AF, its structure, a list of members and partners, and an introduction to the technologies. Finally, we provide current insights into the adoption and development of the technologies by summarizing the Fall 2020 Allotrope Connect virtual conference. This overview provides an easy access to the AF and highlights opportunities for collaboration.}
}
@incollection{FARHANHUSSAIN202319,
title = {Chapter 2 - Smart upstream sector: Smartness in upstream sector of the oil and gas industry},
editor = {Razin {Farhan Hussain} and Ali Mokhtari and Ali Ghalambor and Mohsen {Amini Salehi}},
booktitle = {IoT for Smart Operations in the Oil and Gas Industry},
publisher = {Gulf Professional Publishing},
pages = {19-56},
year = {2023},
isbn = {978-0-323-91151-1},
doi = {https://doi.org/10.1016/B978-0-32-391151-1.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911511000113},
author = {Razin {Farhan Hussain} and Ali Mokhtari and Ali Ghalambor and Mohsen {Amini Salehi}},
keywords = {Upstream, Exploration, Survey, Machine learning, Virtual reality, Drilling, Automation, SCADA system},
abstract = {Among three major sectors of O&G, upstream utilizes the most sophisticated and versatile technologies. The upstream sector mainly consists of exploration and production phases. In the exploration phase, various advanced computing systems and technologies are utilized to verify the existence and cost-efficiency of the hydrocarbon extraction from a reservoir. Moreover, the computing systems and technologies are becoming an indispensable part of the production phase to efficiently extract O&G with the minimum side-effects on the workforce and the environment. To understand the usage of IoT and smart technologies, the chapter first provides a computing taxonomy of the upstream sector that reflects the scope of computing systems adapted in the O&G industry. The taxonomy also serves as a roadmap for the whole upstream sector. Then, it traverses the taxonomy, specifically, explaining the computational aspects in detail, as this is the book's main objective. The upstream operations from exploration surveys to production wells and the scopes of using advanced state-of-the-art computing technologies to improve oil and gas production are explored and explained in a detailed manner. In addition, it provides various use cases where utilizing smart computing solutions and automation can improve the safe operation of the oil fields via minimizing its environmental impacts and the human interactions with the hazardous aspects.}
}
@article{HOJBRASEN20211918,
title = {Comparison between data maturity and maintenance strategy: A case sutdy},
journal = {Procedia CIRP},
volume = {104},
pages = {1918-1923},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.324},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121012221},
author = {Lucas Peter {Høj Brasen} and Oliver Fuglsan Groos and Torben Tambo},
keywords = {manufacturing, asset management, data maturity, sensor networks, predictive maintenance, Internet-of-Things, SME},
abstract = {With the rise of Industry 4.0, there has been a substantial drive towards sensor networks for enabling predictive maintenance as an essential component of asset management. This study analyses sensor data maturity and asset management strategy. A model is proposed for establishing a best-fit correlation between data maturity and maintenance strategy, both for the current situation and as a guide for future development. The findings are based on the literature and case studies for small and medium-sized enterprises. The research implication is to view enterprise strategy as a balance between the chosen maturity and operational needs. The practical implication is the possibility to sustain or improve and qualify investment planning.}
}
@article{SADRI2021102882,
title = {Fog data management: A vision, challenges, and future directions},
journal = {Journal of Network and Computer Applications},
volume = {174},
pages = {102882},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102882},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520303465},
author = {Ali Akbar Sadri and Amir Masoud Rahmani and Morteza Saberikamarposhti and Mehdi Hosseinzadeh},
keywords = {Fog computing, Internet of things, Data management, Data processing, Data analytics, Data storage, Data security, Systematic literature review},
abstract = {Cloud computing with its key facets and its inherent advantages still faces several challenges in the Internet of Things (IoT) ecosystem. The distance among the IoT end devices and cloud computing might be a problem for latency-sensitive applications such as catastrophe management and content transference applications. Fog computing is a novel paradigm to address such issues that playacts a significant role in massive and real-time data management systems in an IoT environment. Particularly IoT data management by fog computing is one important phase for latency reduction in latency-sensitive applications and necessary to generate more skilled knowledge and intelligent decisions. In this study, we used the SLR (systematic literature review) method to survey fog data management to understand the various topics and main contexts in this domain that have been newly offered. The target of this article is classifying and analyzing the researches about the fog data management domain which has been released from 2014 to 2019. A context-based taxonomy is offered for fog data management including data processing, data storage and data security based on the context of papers that are elected with the SLR method in our study. Based on presented technical taxonomy, the grouped papers in any context are compared with each other pursuant to some metrics of fog data management reference model. Then, for any selected research, the new findings, advantages, and weaknesses are debated. Finally, based on studies the open issues in fog data management and their related challenges for future researches are highlighted.}
}
@article{BANDARA2021103497,
title = {Construing online consumers’ information privacy decisions: The impact of psychological distance},
journal = {Information & Management},
volume = {58},
number = {7},
pages = {103497},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103497},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621000719},
author = {Ruwan J. Bandara and Mario Fernando and Shahriar Akter},
keywords = {Construal level theory, Psychological distance, Privacy concerns, Privacy behavior, Privacy empowerment},
abstract = {The role of subjective distance and mental representations in understanding consumers’ information privacy decisions is underexplored in the literature. This study draws on construal level theory and power-responsibility equilibrium framework of privacy to explain consumer privacy behavior based on the interplay between three psychological constructs, namely, privacy concerns, privacy empowerment, and the psychological distance of privacy. This study empirically validates the psychological distance of privacy construct and the results indicate the capability of psychological distance to explain privacy behavior and to moderate the relationship between privacy concerns and privacy behavior. The findings also suggest that empowered consumers’ privacy behavior does not vary despite the degree of psychological distance. Our findings have implications for the privacy scholarship, consumers and e-commerce system developers.}
}
@article{WU2021106593,
title = {Digitalization and decentralization driving transactive energy Internet: Key technologies and infrastructures},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {126},
pages = {106593},
year = {2021},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2020.106593},
url = {https://www.sciencedirect.com/science/article/pii/S0142061520328210},
author = {Ying Wu and Yanpeng Wu and Josep M. Guerrero and Juan C. Vasquez},
keywords = {Digitalization, Decentralization, Energy Internet, IoT, Blockchain, Microgrids, Energy router, Energy transaction},
abstract = {With the increasing access of renewable energy resources and fast ubiquitous connection of everything, the traditional one-way power flow from centralized generation to end consumers will give way to bidirectional-way power flow with multidirectional energy network among central grids and distributed prosumers. To empower the prosumer-centric Energy Internet (EI) and enhance the integration of energy-aware services, digitalization and decentralization are the key enablers to achieve transactive EI. This article presents a systematic overview on how Internet of Things (IoT) drives the digitalization of transactive EI and how blockchain empowers the decentralization of transactive EI. A comprehensive discussion on the key infrastructures is provided for presenting how to implement digitalization and decentralization of transactive EI, including the last mile “Advanced metering infrastructure” (AMI), renewables integrator “smart inverter”, energy flow adjuster “energy router”, and coordinator “Microgrid”. Challenges and future trends are discussed from an extensive point of view, including energy physical space, data cyber space and human social space.}
}
@article{RASHID20213,
title = {Artificial Intelligence Effecting a Paradigm Shift in Drug Development},
journal = {SLAS Technology},
volume = {26},
number = {1},
pages = {3-15},
year = {2021},
note = {Special Collection: Artificial Intelligence in Process Automation},
issn = {2472-6303},
doi = {https://doi.org/10.1177/2472630320956931},
url = {https://www.sciencedirect.com/science/article/pii/S2472630322010858},
author = {Masturah Bte Mohd Abdul Rashid},
keywords = {artificial intelligence, drug development, drug discovery, industry},
abstract = {The inverse relationship between the cost of drug development and the successful integration of drugs into the market has resulted in the need for innovative solutions to overcome this burgeoning problem. This problem could be attributed to several factors, including the premature termination of clinical trials, regulatory factors, or decisions made in the earlier drug development processes. The introduction of artificial intelligence (AI) to accelerate and assist drug development has resulted in cheaper and more efficient processes, ultimately improving the success rates of clinical trials. This review aims to showcase and compare the different applications of AI technology that aid automation and improve success in drug development, particularly in novel drug target identification and design, drug repositioning, biomarker identification, and effective patient stratification, through exploration of different disease landscapes. In addition, it will also highlight how these technologies are translated into the clinic. This paradigm shift will lead to even greater advancements in the integration of AI in automating processes within drug development and discovery, enabling the probability and reality of attaining future precision and personalized medicine.
摘要
薬剤開発にかかるコストと、薬剤の成果を市場に組み入れることとの間には逆相関があり、急速に拡大しているこの問題を克服するための革新的なソリューションが求められるようになってきた。この問題は、臨床試験の早期中止、規制要因、または薬剤開発プロセスの初期段階で下される判断など、複数の要因に起因している可能性がある。薬剤開発をより迅速化し補助するための人工知能（artificial intelligence：AI）の導入は、比較的安価なうえ効率的なプロセスをもたらし、最終的に臨床試験の成功率を向上させている。本レビューのねらいは、さまざまな疾患を取り巻く状況を探究しながら、薬剤開発の自動化を支援するとともに、薬剤開発の、特に新薬のターゲットの特定や設計、ドラッグリポジショニング、バイオマーカーの特定、有効な患者層別化などを首尾よく運ぶAI技術のさまざまなアプリケーションを示して比較することにある。また、これらの技術が臨床場面で活用される方法にも注目する。このパラダイムシフトは、薬剤開発や創薬の範疇のプロセスを自動化する際にAIを組み込むことの利点をさらに拡大することにつながり、ひいては将来の高精度医療やオーダメイド医療の実現の可能性を高める。
초록
약물 개발 비용과 약물의 성공적인 시장 통합 간의 역상관관계는 이러한 급증하는 문제들을 극복하기 위한 혁신적인 해법이 필요하다는 인식을 제기했다. 이러한 문제는 임상시험의 조기 종료, 규제 요인 또는 초기 약물 개발 과정에서 이루어진 결정을 포함한 여러 요인들에서 기인할 수 있다. 약물 개발을 가속화하고 지원하기 위한 인공지능(artificial intelligence, AI)의 도입으로 약물 개발 과정이 더욱 저렴하고 더욱 효율적이 되었으며 궁극적으로 임상시험의 성공률이 향상되었다. 본 종설의 목적은 다양한 질병 환경의 탐색을 통해 자동화를 지원하고 특히 신약의 표적 확인 및 설계, 약물의 재포지셔닝, 생체표지자 확인 및 효과적인 환자 층화 부분에서 약물 개발의 성공을 향상시키는 AI 기술의 다양한 적용을 보여주고 비교하는 것이다. 또한 이러한 기술들이 임상으로 전환되는 방식에 대해서도 강조할 것이다. 이러한 패러다임 전환은 신약 개발 및 발견의 자동화 과정에서 AI의 통합을 더욱 크게 발전시켜 향후 정밀의학 및 맞춤 의학 달성 가능성을 높이고 그 실현을 가능하게 할 것이다.
抄録
薬剤開発にかかるコストと、薬剤の成果を市場に組み入れることとの間には逆相関があり、急速に拡大しているこの問題を克服するための革新的なソリューションが求められるようになってきた。この問題は、臨床試験の早期中止、規制要因、または薬剤開発プロセスの初期段階で下される判断など、複数の要因に起因している可能性がある。薬剤開発をより迅速化し補助するための人工知能（artificial intelligence：AI）の導入は、比較的安価なうえ効率的なプロセスをもたらし、最終的に臨床試験の成功率を向上させている。本レビューのねらいは、さまざまな疾患を取り巻く状況を探究しながら、薬剤開発の自動化を支援するとともに、薬剤開発の、特に新薬のターゲットの特定や設計、ドラッグリポジショニング、バイオマーカーの特定、有効な患者層別化などを首尾よく運ぶAI技術のさまざまなアプリケーションを示して比較することにある。また、これらの技術が臨床場面で活用される方法にも注目する。このパラダイムシフトは、薬剤開発や創薬の範疇のプロセスを自動化する際にAIを組み込むことの利点をさらに拡大することにつながり、ひいては将来の高精度医療やオーダメイド医療の実現の可能性を高める。}
}
@incollection{2021vii,
title = {Contents},
editor = {Alan Godfrey and Sam Stuart},
booktitle = {Digital Health},
publisher = {Academic Press},
pages = {vii-xiii},
year = {2021},
isbn = {978-0-12-818914-6},
doi = {https://doi.org/10.1016/B978-0-12-818914-6.00031-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128189146000314}
}
@article{WOSIAK20212422,
title = {Using semantic enrichment methods in expert search system for recruitment process in IT corporation},
journal = {Procedia Computer Science},
volume = {192},
pages = {2422-2431},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921017488},
author = {Agnieszka Wosiak},
keywords = {expert search system, semantic enrichment, text data analysis, preprocessing and outlier detection;},
abstract = {The problem of intelligent information retrieval and semantic enrichment becomes more and more popular due to the difficulty of searching and analyzing large text datasets. The common approach assumes user manual queries in natural language. Various semantic enrichment methods and intelligent text searching allow obtaining more accurate results leading to broader knowledge and user satisfaction. This research presents state-of-the-art methods of searching with enrichment and building rankings of results for the expert recruitment process in IT industry. The proposed model implements full-text search, semantic enrichment, and machine learning to match experts with job offers. Different data sources on expert competencies were used, including curricula vitae, historical data, and Internet resources. The testing results confirm an improvement in the search quality compared to the existing systems in the recruitment company.}
}
@article{HAO2021127,
title = {Semi-supervised disentangled framework for transferable named entity recognition},
journal = {Neural Networks},
volume = {135},
pages = {127-138},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020304159},
author = {Zhifeng Hao and Di Lv and Zijian Li and Ruichu Cai and Wen Wen and Boyan Xu},
keywords = {Named entity recognition, Semi-supervised learning, Transfer learning, Disentanglement},
abstract = {Named entity recognition (NER) for identifying proper nouns in unstructured text is one of the most important and fundamental tasks in natural language processing. However, despite the widespread use of NER models, they still require a large-scale labeled data set, which incurs a heavy burden due to manual annotation. Domain adaptation is one of the most promising solutions to this problem, where rich labeled data from the relevant source domain are utilized to strengthen the generalizability of a model based on the target domain. However, the mainstream cross-domain NER models are still affected by the following two challenges (1) Extracting domain-invariant information such as syntactic information for cross-domain transfer. (2) Integrating domain-specific information such as semantic information into the model to improve the performance of NER. In this study, we present a semi-supervised framework for transferable NER, which disentangles the domain-invariant latent variables and domain-specific latent variables. In the proposed framework, the domain-specific information is integrated with the domain-specific latent variables by using a domain predictor. The domain-specific and domain-invariant latent variables are disentangled using three mutual information regularization terms, i.e., maximizing the mutual information between the domain-specific latent variables and the original embedding, maximizing the mutual information between the domain-invariant latent variables and the original embedding, and minimizing the mutual information between the domain-specific and domain-invariant latent variables. Extensive experiments demonstrated that our model can obtain state-of-the-art performance with cross-domain and cross-lingual NER benchmark data sets.}
}
@article{VALENTINETTI2021549,
title = {Internet of things: Emerging impacts on digital reporting},
journal = {Journal of Business Research},
volume = {131},
pages = {549-562},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.01.056},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321000667},
author = {Diego Valentinetti and Francisco {Flores Muñoz}},
keywords = {Digital reporting, Internet of Things (IoT), Corporate communication, Media richness},
abstract = {This paper develops a future research agenda for fostering a resurged interest in digital reporting through the emergence of the Internet of Things (IoT). Drawing upon the media richness and corporate communication frameworks, we enquire the evolving stages of digital reporting and review the contemporary academic literature on IoT to discuss the opportunities and practical concerns for developing future advances in the digitisation of accounting information. Our analysis explores how the media richness-related features of IoT fit, challenge and enhance the dynamics that constitute corporate communication, i.e.: communicator, message, medium/channel, audience, relationship and conversation. This paper opens new directions of research on advances in digital reporting and sheds light on the innovative ways accounting information is co-produced and shared through the IoT.}
}
@article{OCONNOR2021102934,
title = {Integrating informatics into undergraduate nursing education: A case study using a spiral learning approach},
journal = {Nurse Education in Practice},
volume = {50},
pages = {102934},
year = {2021},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2020.102934},
url = {https://www.sciencedirect.com/science/article/pii/S1471595320310209},
author = {Siobhan O'Connor and Elizabeth LaRue},
keywords = {Nursing education, Informatics, Technology, Digital health},
abstract = {A gap in informatics expertise amongst nursing students, practising staff and faculty has been noted globally, which reduces the potential for nurses to utilise technology to enhance patient care. National nursing education strategies and recommendations from professional associations have identified digital health as an area that needs investment. This case study describes how health informatics is being integrated into a Bachelor of Nursing programme in the United Kingdom. An international collaboration with a US-UK Fulbright Specialist Scholar enabled individual learning units corresponding to key health informatics competencies to be designed and incorporated into a pedagogic framework grounded in the spiral learning approach. This approach is proposed as one way to integrate informatics into nursing education, so students can become competent clinicians that are able to deliver technology enabled care in the health service.}
}
@article{ZHU2021102952,
title = {An improved convolution Merkle tree-based blockchain electronic medical record secure storage scheme},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102952},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102952},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621001642},
author = {Hegui Zhu and Yujia Guo and Libo Zhang},
keywords = {Electronic medical record, Blockchain, Convolution operation, Improved convolution Merkle tree},
abstract = {Presently, more and more electronic medical records (EMR) are used to replace traditional recording methods. However, there has potential safety hazard in the transmission of EMR because of the personal privacy disclosure. So, how to store, transmit and share EMR effectively and securely has become a research hotspot. In this paper, we propose an improved Merkle tree based-blockchain EMR storage scheme. The hallmark of the proposed scheme is that we employ the convolutional layer structure to replace the original binary tree structure in the proposed convolution Merkle tree, which can improve the efficiency effectively. Experiments show that the number of stored nodes has decreased significantly with the same amount of input data, and the layers number of the improved convolution Merkle tree and hash calculated amount are all reduced dramatically. The security and efficiency analysis also illustrate that the proposed scheme can provide a reliable choice for the further development of data storage security in the future.}
}
@incollection{OTT20211,
title = {1 - Information and Data Management},
editor = {Florence Ott},
booktitle = {Records Management At the Heart of Business Processes},
publisher = {ISTE},
pages = {1-50},
year = {2021},
isbn = {978-1-78548-043-0},
doi = {https://doi.org/10.1016/B978-1-78548-043-0.50001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480430500019},
author = {Florence Ott},
keywords = {Big data, Business processes, Data governance, Digital environment, Information, Open data, Protection of personal data, Records continuum, Three ages},
abstract = {Abstract:
While the digital world brings advantages by simplifying many processes, it also makes the context of records production more complex and difficult to understand according to traditional archival principles. The explosion in the volume of information leads to the multiplication of actors, the acceleration of exchanges, and the atomization and fragmentation of information with numerous digital files to replace what was formerly a paper document or the reproduction of several copies.}
}
@article{SCHRECKENBERG202131,
title = {Developing a maturity-based workflow for the implementation of ML-applications using the example of a demand forecast},
journal = {Procedia Manufacturing},
volume = {54},
pages = {31-38},
year = {2021},
note = {10th CIRP Sponsored Conference on Digital Enterprise Technologies (DET 2020) – Digital Technologies as Enablers of Industrial Competitiveness and Sustainability},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001396},
author = {Felix Schreckenberg and Nikolas Ulrich Moroff},
keywords = {artificial intelligenz, maturity-based workflow, challenges AI},
abstract = {The aim of the article is to present a guideline that has been developed in the form of a workflow to identify the capability of an organisation to implement machine learning (ML) applications on the one hand and, on the other hand, to describe a maturity-dependent procedure for the development of an ML application based on this knowledge. With the help of the guideline, application-specific requirements can be identified based on the phases of the development process of an ML application adapted to the corporate environment. The article begins with the motivation for using machine learning methods and presents the challenges in implementing these methods. Based on a literature review, a maturity-based approach is designed and the developed and adapted development phases from the literature are described in a more detailed way. The individual characteristics of certain phases are specified based on the maturity level. As well, the weighting of certain maturity dimensions of the respective phase is highlighted. The article ends with an outlook on the further development of the created guideline.}
}
@article{CHATTERJEE2021111051,
title = {Scientometric review of artificial intelligence for operations & maintenance of wind turbines: The past, present and future},
journal = {Renewable and Sustainable Energy Reviews},
volume = {144},
pages = {111051},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111051},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121003403},
author = {Joyjit Chatterjee and Nina Dethlefs},
keywords = {Wind turbines, Operations & maintenance, SCADA, Scientometric review, Artificial intelligence, Machine learning, Condition-based monitoring},
abstract = {Wind energy has emerged as a highly promising source of renewable energy in recent times. However, wind turbines regularly suffer from operational inconsistencies, leading to significant costs and challenges in operations and maintenance (O&M). Condition-based monitoring (CBM) and performance assessment/analysis of turbines are vital aspects for ensuring efficient O&M planning and cost minimisation. Data-driven decision making techniques have witnessed rapid evolution in the wind industry for such O&M tasks during the last decade, from applying signal processing methods in early 2010 to artificial intelligence (AI) techniques, especially deep learning in 2020. In this article, we utilise statistical computing to present a scientometric review of the conceptual and thematic evolution of AI in the wind energy sector, providing evidence-based insights into present strengths and limitations of data-driven decision making in the wind industry. We provide a perspective into the future and on current key challenges in data availability and quality, lack of transparency in black box-natured AI models, and prevailing issues in deploying models for real-time decision support, along with possible strategies to overcome these problems. We hope that a systematic analysis of the past, present and future of CBM and performance assessment can encourage more organisations to adopt data-driven decision making techniques in O&M towards making wind energy sources more reliable, contributing to the global efforts of tackling climate change.}
}
@article{GOYAL2021719,
title = {Internet of things: Architecture and enabling technologies},
journal = {Materials Today: Proceedings},
volume = {34},
pages = {719-735},
year = {2021},
note = {3rd International Conference on Science and Engineering in Materials},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.04.678},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320333253},
author = {Parul Goyal and Ashok Kumar Sahoo and Tarun Kumar Sharma},
keywords = {Enabling technologies, IoT, RFID, Sensors, ZigBee},
abstract = {Internet of Things is transforming real devices to smart intelligent virtual devices. In IoT day today devices of daily use are manufactured along with sensors which are capable for identification and sensing. They can be networked, are capable to process, can interact with other devices through Internet. IoT objective is to connect almost everything under a common infrastructure. This helps to control devices and will keep us informed about the status of devices. The paper aims to give Internet of Things overview, architectures, enabling technologies and their applications. It presents latest trends, current state, recent developments, challenges, security, privacy, applications of IoT and future research directions.}
}
@article{YIN2021106872,
title = {Knowledge discovery of geochemical patterns from a data-driven perspective},
journal = {Journal of Geochemical Exploration},
volume = {231},
pages = {106872},
year = {2021},
issn = {0375-6742},
doi = {https://doi.org/10.1016/j.gexplo.2021.106872},
url = {https://www.sciencedirect.com/science/article/pii/S0375674221001515},
author = {Bojun Yin and Renguang Zuo and Yihui Xiong and Yongsheng Li and Weigang Yang},
keywords = {Data-driven, Knowledge discovery, Data science, Geochemical exploration},
abstract = {We have entered the fourth research paradigm with the overwhelming availability of vast amounts of data. The processing and mining these data for a better understanding of earth systems and predicting mineral resources is challenging. This study discusses a data-driven knowledge discovery of geochemical patterns and presents a case study of geochemical data processing from a data-driven perspective. We employed local indicators of spatial association (LISA), principal component analysis (PCA), and deep autoencoder network (DAN) procedures to explore spatial association of geochemical patterns, extract elemental associations, and detect geochemical anomalies related to AuSb mineralization in the Daqiao district, Gansu Province, China. The results indicate the following: (1) both Au and Sb, and Pb and Zn have a close spatial correlation, indicating genetic connections among them; (2) the elemental association of Au, Sb, As, Hg and Ag can be adopted as a geochemical signature for the discovery of AuSb polymetallic mineralization in the study area; and (3) the geochemical anomalies identified by DAN exhibit a strong spatial relationship with locations of known mineral deposits and can provide a significant clue for further mineral exploration in this district. These findings indicate that data-driven procedures can help in the knowledge discovery of geochemical patterns in mineral exploration. Additional efforts are required for data-driven knowledge discovery in both geochemical prospecting and mineral exploration.}
}
@article{FLUKE2021104650,
title = {Child maltreatment data: A summary of progress, prospects and challenges},
journal = {Child Abuse & Neglect},
volume = {119},
pages = {104650},
year = {2021},
note = {30 Years of the Convention on the Rights of the Child: Challenges and progress in harm prevention},
issn = {0145-2134},
doi = {https://doi.org/10.1016/j.chiabu.2020.104650},
url = {https://www.sciencedirect.com/science/article/pii/S0145213420303057},
author = {John D. Fluke and Lil Tonmyr and Jenny Gray and Leonor {Bettencourt Rodrigues} and Flora Bolter and Scottye Cash and Andreas Jud and Franziska Meinck and Abigail {Casas Muñoz} and Melissa O’Donnell and Rhiannon Pilkington and Leemoy Weaver},
keywords = {Child maltreatment data, Linked data, Data collection, Data analysis, Self-report data, Administrative data, Sentinel data, Data collection ethics, International comparison, Child maltreatment data utilization, Evaluation, Decision-making},
abstract = {Background
In 1996, the ISPCAN Working Group on Child Maltreatment Data (ISPCAN-WGCMD) was established to provide an international forum in which individuals, who deal with child maltreatment data in their respective professional roles, can share concerns and solutions.
Objective
This commentary describes some of the key features and the status of child maltreatment related data collection addressed by the ISPCAN-WGCMD.
Methods
Different types of data collection methods including self-report, sentinel, and administrative data designs are described as well as how they address different needs for information to help understand child maltreatment and systems of prevention and intervention.
Results
While still lacking in many parts of the world, access to child maltreatment data has become much more widespread, and in many places a very sophisticated undertaking.
Conclusion
The ISPCAN-WGCMD has been an important forum for supporting the continued development and improvement in the global effort to understand and combat child maltreatment thus contributing to the long term goals of the UN Convention on the Rights of the Child. Nevertheless, based on what has been learned, even greater efforts are required to improve data in order to effectively combat child maltreatment.}
}
@article{WANG20211012,
title = {Consumers’ willingness to pay for ethical consumption initiatives on e-commerce platforms},
journal = {Journal of Integrative Agriculture},
volume = {20},
number = {4},
pages = {1012-1020},
year = {2021},
issn = {2095-3119},
doi = {https://doi.org/10.1016/S2095-3119(20)63584-5},
url = {https://www.sciencedirect.com/science/article/pii/S2095311920635845},
author = {Er-peng WANG and Ning AN and Xian-hui GENG and Zhifeng GAO and Emmanuel KIPROP},
keywords = {ethical consumption, apples from poverty-stricken areas, WTP, interval regression},
abstract = {Despite China’s fast-growing e-commerce and its great achievement in promoting poverty alleviation through consumption, little is known about Chinese consumers’ online ethical consumption. Using the payment card elicitation method, this paper designs a within-subject survey and a between-subject survey to investigate Chinese consumers’ quality perception and preference for apples from poverty-stricken areas. The results show that before “information shock”, emphasizing that taste and safety attributes of apples from poverty-stricken areas are the same as the conventional ones, Chinese consumers on average are willing to pay a 31% premium for apples from poverty-stricken areas. After “information shock”, both the within-subject and between-subject designs show a minimal drop of the premium, implying that the ethical attribute is the main motivation for buying apples from poverty-stricken areas. The regression results show that quality perception of private attributes has significant effect on consumers’ willingness to pay (WTP) for apples from poverty-stricken areas, and trust in government supervision of e-commerce plays an essential role in motivating online ethical consumption.}
}
@article{REN2021101457,
title = {An intelligent charging scheme maximizing the utility for rechargeable network in smart city},
journal = {Pervasive and Mobile Computing},
volume = {77},
pages = {101457},
year = {2021},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2021.101457},
url = {https://www.sciencedirect.com/science/article/pii/S1574119221000961},
author = {Yingying Ren and Anfeng Liu and Xingliang Mao and Fangfang Li},
keywords = {Mobile charging, Wireless energy transfer, Mobile chargers, Quality utility, Workload balance},
abstract = {The mobile charging scheme is a promising solution to extending the lifetime of the network by replenishing the energy for the sensing nodes, which has attracted more and more attention from the researchers. However, due to the limitation of energy storage both for sensing nodes and mobile chargers, not all the sensing nodes can be recharged in time by mobile chargers. Therefore, how to select appropriate sensing nodes and design the path for the mobile charger are the key to improve the system utility. This paper proposes an Intelligent Charging scheme Maximizing the Quality Utility (ICMQU) to design the charging path for the mobile charger. Comparing to the previous studies, we consider not only the utility of the data collected from the environment, but also the impact of sensing nodes with different quality. Quality Utility is proposed to optimize the charging path design. Besides, ICMQU designs the charging scheme for a single mobile charger and multiple mobile chargers simultaneously. For the charging scheme with multiple mobile chargers, the workload balance among different mobile chargers is also considered as well as the utility of the system. Extensive simulation results are provided, which demonstrates the proposed ICMQU scheme can significantly improve the utility of the system.}
}
@article{MARTINNOGUEROL2021317,
title = {Artificial intelligence in radiology: relevance of collaborative work between radiologists and engineers for building a multidisciplinary team},
journal = {Clinical Radiology},
volume = {76},
number = {5},
pages = {317-324},
year = {2021},
issn = {0009-9260},
doi = {https://doi.org/10.1016/j.crad.2020.11.113},
url = {https://www.sciencedirect.com/science/article/pii/S0009926020306024},
author = {T. Martín-Noguerol and F. Paulano-Godino and R. López-Ortega and J.M. Górriz and R.F. Riascos and A. Luna},
abstract = {The use of artificial intelligence (AI) algorithms in the field of radiology is becoming more common. Several studies have demonstrated the potential utility of machine learning (ML) and deep learning (DL) techniques as aids for radiologists to solve specific radiological challenges. The decision-making process, the establishment of specific clinical or radiological targets, the profile of the different professionals involved in the development of AI solutions, and the relation with partnerships and stakeholders are only some of the main issues that have to be faced and solved prior to starting the development of radiological AI solutions. Among all the players in this multidisciplinary team, the communication between radiologists and data scientists is essential for a successful collaborative work. There are specific skills that are inherent to radiological and medical training that are critical for identifying anatomical or clinical targets as well as for segmenting or labelling lesions. These skills would then have to be transferred, explained, and taught to the data science experts to facilitate their comprehension and integration into ML or DL algorithms. On the other hand, there is a wide range of complex software packages, deep neural-network architectures, and data transfer processes for which radiologists need the expertise of software engineers and data scientists in order to select the optimal manner to analyse and post-process this amount of data. This paper offers a summary of the top five challenges faced by radiologists and data scientists including tips and tricks to build a successful AI team.}
}
@article{AGUILAR2021111530,
title = {A systematic literature review on the use of artificial intelligence in energy self-management in smart buildings},
journal = {Renewable and Sustainable Energy Reviews},
volume = {151},
pages = {111530},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111530},
url = {https://www.sciencedirect.com/science/article/pii/S136403212100808X},
author = {J. Aguilar and A. Garces-Jimenez and M.D. R-Moreno and Rodrigo García},
keywords = {Energy management system, Autonomous management architecture, Smart building, Artificial intelligence, Systematic literature review, Smart grid},
abstract = {Buildings are one of the main consumers of energy in cities, which is why a lot of research has been generated around this problem. Especially, the buildings energy management systems must improve in the next years. Artificial intelligence techniques are playing and will play a fundamental role in these improvements. This work presents a systematic review of the literature on researches that have been done in recent years to improve energy management systems for smart building using artificial intelligence techniques. An originality of the work is that they are grouped according to the concept of “Autonomous Cycles of Data Analysis Tasks”, which defines that an autonomous management system requires specialized tasks, such as monitoring, analysis, and decision-making tasks for reaching objectives in the environment, like improve the energy efficiency. This organization of the work allows us to establish not only the positioning of the researches, but also, the visualization of the current challenges and opportunities in each domain. We have identified that many types of researches are in the domain of decision-making (a large majority on optimization and control tasks), and defined potential projects related to the development of autonomous cycles of data analysis tasks, feature engineering, or multi-agent systems, among others.}
}
@article{LAWSON2021100714,
title = {Detecting dirty data using SQL: Rigorous house insurance case},
journal = {Journal of Accounting Education},
volume = {55},
pages = {100714},
year = {2021},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2021.100714},
url = {https://www.sciencedirect.com/science/article/pii/S0748575121000014},
author = {James G. Lawson and Daniel A. Street},
keywords = {Data analytics, Accounting education, Dirty data, Structured query language (“SQL”), Data integrity},
abstract = {Proficiency with data analytics is an increasingly important skill within in the accounting profession. However, successful data analysis requires clean source data (i.e., source data without errors) in order to draw reliable conclusions. Although users often assume clean source data, this assumption is frequently incorrect. Therefore, identifying and remediating “dirty data” is a prerequisite to effective data analysis. You, an accountant working at a firm that specializes in data analytics, have been hired by Rigorous House Insurance to analyze the company’s claim insurance data. In addition to investigating specific issues mentioned by the company’s controller, you are tasked with identifying any other data integrity issues that you encounter and providing preventative information system internal control suggestions to the client to mitigate these issues in the future.}
}
@article{WU2021107792,
title = {Sentiment classification using attention mechanism and bidirectional long short-term memory network},
journal = {Applied Soft Computing},
volume = {112},
pages = {107792},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107792},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621007134},
author = {Peng Wu and Xiaotong Li and Chen Ling and Shengchun Ding and Si Shen},
keywords = {Attention mechanism, Bidirectional long short-term memory network, Sentiment classification, Social media, Word embedding},
abstract = {We propose a sentiment classification method for large scale microblog text based on the attention mechanism and the bidirectional long short-term memory network (SC-ABiLSTM). We use an experimental study to compare our proposed method with baseline methods using real world large-scale microblog data. Comparing the accuracy of the baseline methods to the accuracy of our model, we demonstrate the efficacy of our proposed method. While sentiment classification of social media data has been extensively studied, the main novelty of our study is the implementation of the attention mechanism in a deep learning network for analyzing large scale social media data.}
}
@incollection{PRADHAN2021285,
title = {Chapter 13 - The role of IoT in smart cities: Challenges of air quality mass sensor technology for sustainable solutions},
editor = {Sudhir Kumar Sharma and Bharat Bhushan and Narayan C. Debnath},
booktitle = {Security and Privacy Issues in IoT Devices and Sensor Networks},
publisher = {Academic Press},
pages = {285-307},
year = {2021},
series = {Advances in ubiquitous sensing applications for healthcare},
issn = {25891014},
doi = {https://doi.org/10.1016/B978-0-12-821255-4.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212554000134},
author = {Alok Pradhan and Bhuvan Unhelkar},
keywords = {Air quality monitoring, Smart and sustainable cities, Air quality sensors, Citizen science, Air quality management},
abstract = {Holistic management of air quality in cities is crucial to set a reasonable standard of living for its citizens. With the growing number of large cities across the world, the Internet of Things (IoT) is a technology that holds the promise to make cities smart and sustainable. A primary objective of a smart and sustainable city is to ensure urban air quality is clean and sustainably managed for future generations. Current regulation-based ambient air quality monitoring technology in urban environments involves sparsely distributed bulky and expensive equipment, which requires continuous calibration and maintenance. IoT devices can help measure, monitor, and abet the impact of poor air quality. This is so because of the ubiquitous nature of the devices, available connectivity in smart cities, and opportunities to analyze data within reasonable time to take corrective actions. This chapter discusses various aspects of air quality monitoring for smart cities capitalizing on IoT devices. Starting with a discussion on the background of air quality monitoring in the context of smart cities, this chapter outlines the challenges and the various ways in which IoT and Cloud-based applications can work together to overcome those challenges. Overall, this chapter proposes a three-tiered approach for effectively applying data to achieve better urban air quality. The first tier is a practical approach, the second tier is a strategic approach, and the third and final tier is a legislative approach.}
}
@article{GRASSOTORO2021100306,
title = {Brief overview of the future of metrology},
journal = {Measurement: Sensors},
volume = {18},
pages = {100306},
year = {2021},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100306},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421002695},
author = {Federico {Grasso Toro} and Hugo Lehmann},
keywords = {Digital metrology, Digitalization of metrology, Metrology of digitalization, Digitalization strategies, Ontology of knowledge},
abstract = {This position paper opens the discussion about the term digital metrology, the underlying digital trends and the activities related to the digitalization of metrology. Firstly, we present a clarification of terms between digitization and digitalization and the current digitalization strategies of two representative national metrology institutes, members of EURAMET. Subsequently, we describe the current METAS strategy towards digital metrology by its three main pillars and ongoing research and development efforts. Finally, we discuss the state of the digitalization of metrology, as well as presenting our suggestions for the next steps towards the digital transformation of metrology.}
}
@article{JOHANNA2021361,
title = {Predictive model for the identification of activities of daily living (ADL) in indoor environments using classification techniques based on Machine Learning},
journal = {Procedia Computer Science},
volume = {191},
pages = {361-366},
year = {2021},
note = {The 18th International Conference on Mobile Systems and Pervasive Computing (MobiSPC), The 16th International Conference on Future Networks and Communications (FNC), The 11th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.07.069},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921014721},
author = {García-Restrepo Johanna and Ariza-Colpas {Paola Patricia} and Oñate-Bowen {Alvaro Agustín} and Suarez-Brieva {Eydy del Carmen} and Urina-Triana Miguel and De-la-Hoz-Franco Emiro and Díaz-Martínez {Jorge Luis} and Butt {Shariq Aziz} and Molina_Estren Diego},
keywords = {HAR, Human Activity Recognition, Machine Learning, ADL, Activity Daily Living},
abstract = {AI-based techniques have included countless applications within the engineering field. These range from the automation of important procedures in Industry and companies, to the field of Process Control. Smart Home (SH) technology is designed to help house residents improve their daily activities and therefore enrich the quality of life while preserving their privacy. An SH system is usually equipped with a collection of software interrelated with hardware components to monitor the living space by capturing the behavior of the resident and their occupations. By doing so, the system can report risks, situations, and act on behalf of the resident to their satisfaction. This research article shows the experimentation carried out with the human activity recognition dataset, CASAS Kyoto, through preprocessing and cleaning processes of the data, showing the Vía Regression classifier as an excellent option to process this type of data with an accuracy 99.7% effective}
}
@article{PENG2023794,
title = {Observation-based sources evolution of non-methane hydrocarbons (NMHCs) in a megacity of China},
journal = {Journal of Environmental Sciences},
volume = {124},
pages = {794-805},
year = {2023},
issn = {1001-0742},
doi = {https://doi.org/10.1016/j.jes.2022.01.040},
url = {https://www.sciencedirect.com/science/article/pii/S1001074222000468},
author = {Yarong Peng and Hongli Wang and Qian Wang and Shengao Jing and Jingyu An and Yaqin Gao and Cheng Huang and Rusha Yan and Haixia Dai and Tiantao Cheng and Qiang Zhang and Meng Li and Jianlin Hu and Zhihao Shi and Li Li and Shengrong Lou and Shikang Tao and Qinyao Hu and Jun Lu and Changhong Chen},
keywords = {NMHCs, Characteristics, Source apportionment, Observation-based, Interannual trend, Shanghai},
abstract = {Both concentrations and emissions of many air pollutants have been decreasing due to implement of control measures in China, in contrast to the fact that an increase in emissions of non-methane hydrocarbons (NMHCs) has been reported. This study employed seven years continuous NMHCs measurements and the related activities data of Shanghai, a megacity in China, to explore evolution of emissions and effectiveness of air pollution control measures. The mixing ratio of NMHCs showed no statistical interannual changes, of which their compositions exhibited marked changes. This resulted in a decreasing trend of ozone formation potential by 3.8%/year (p < 0.05, the same below), which should be beneficial to ozone pollution mitigation as its production in Shanghai is in the NMHCs-limited regime. Observed alkanes, aromatics and acetylene changed by +3.7%/year, -5.9%/year and -7.4%/year, respectively, and alkenes showed no apparent trend. NMHCs sources were apportioned by a positive matrix factorization model. Accordingly, vehicular emissions (-5.9%/year) and petrochemical industry emissions (-7.1%/year) decreased significantly, but the decrease slowed down; significant reduction in solvent usage (-9.0%/year) appeared after 2010; however, emissions of natural gas (+12.6%/year) and fuel evaporation (with an increasing fraction) became more important. The inconsistency between observations and inventories was found in interannual trend and speciation as well as source contributions, emphasizing the need for further validation in NMHCs emission inventory. Our study confirms the effectiveness of measures targeting mobile and centralized emissions from industrial sources and reveals a need focusing on fugitive emissions, which provided new insights into future air policies in polluted region.}
}
@article{FRIEDERICH2021589,
title = {Towards Data-Driven Reliability Modeling for Cyber-Physical Production Systems},
journal = {Procedia Computer Science},
volume = {184},
pages = {589-596},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.073},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007067},
author = {Jonas Friederich and Sanja Lazarova-Molnar},
keywords = {Cyber-Physical Production Systems, Reliability Analysis, Data-Driven Reliability Modeling},
abstract = {Reliability is one of the most important performance indicators in contemporary production facilities. Increasing reliability of manufacturing systems results in their prolonged lifetimes, and reduced maintenance and repair costs. Reliability modeling is a common technique for deriving reliability measurements and illustrating relevant fault-dependencies. There is a significant body of research focusing on hardware- and software reliability models, such as Fault Trees, Petri Nets and Markov Chains. Up until now, development of reliability models has been a labor-intensive and expert-knowledge-driven process. To remedy that, through the prevalence of data stemming from the new and technologically advanced manufacturing systems, we propose that data generated in modern manufacturing lines could be used to either automate or at least to support development of reliability models. In this paper, we elaborate on the details of our proposed framework for data-driven reliability assessment of cyber-physical production systems. We, furthermore, introduce a case study that will aid the development and testing of the proposed novel data-driven approach.}
}
@article{DWIVEDI2021101994,
title = {Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy},
journal = {International Journal of Information Management},
volume = {57},
pages = {101994},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S026840121930917X},
author = {Yogesh K. Dwivedi and Laurie Hughes and Elvira Ismagilova and Gert Aarts and Crispin Coombs and Tom Crick and Yanqing Duan and Rohita Dwivedi and John Edwards and Aled Eirug and Vassilis Galanos and P. Vigneswara Ilavarasan and Marijn Janssen and Paul Jones and Arpan Kumar Kar and Hatice Kizgin and Bianca Kronemann and Banita Lal and Biagio Lucini and Rony Medaglia and Kenneth {Le Meunier-FitzHugh} and Leslie Caroline {Le Meunier-FitzHugh} and Santosh Misra and Emmanuel Mogaji and Sujeet Kumar Sharma and Jang Bahadur Singh and Vishnupriya Raghavan and Ramakrishnan Raman and Nripendra P. Rana and Spyridon Samothrakis and Jak Spencer and Kuttimani Tamilmani and Annie Tubadji and Paul Walton and Michael D. Williams},
keywords = {Artificial intelligence, AI, Cognitive computing, Expert systems, Machine learning, Research agenda},
abstract = {As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportunities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development.}
}
@article{ZHOU2021104955,
title = {A heterogeneous key performance indicator metadata model for air quality monitoring in sustainable cities},
journal = {Environmental Modelling & Software},
volume = {136},
pages = {104955},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104955},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220310124},
author = {Lianjie Zhou and Qingquan Li and Wei Tu and Chisheng Wang},
keywords = {KPI, Meta-model, Formulization framework, Heterogeneity, Geospatial sensor web, SDGs},
abstract = {Due to heterogeneous and inconsistent key performance indicators (KPIs) for the quantitative evaluation of a sustainable city's operational status, it is a great challenge to share multidimensional, multi-source and heterogeneous indicators. We propose a heterogeneous KPI capability representation model (HKPM) in our study. Based on the Meta Object Facility architecture, a nine-tuple multi-hierarchical meta-model is formulated to define the metadata components. Nine specific representation element datasets for specific KPIs are proposed to represent the meta-model. Besides, the KPI classification based on Sustainable Development Goals (SDGs) has been accomplished to support HKPM instantiated in concrete application. Experiments are conducted with the multi-type KPIs to validate the feasibility of HKPM, as shown in public service and Air Quality Index KPI instantiation example. Furthermore, the KPIs can be characterized in different dimensions, which can be modelled in a stereoscopic manner, promoting a comprehensive perception of sustainable cities.}
}
@article{GUO2021105522,
title = {Internet court's challenges and future in China},
journal = {Computer Law & Security Review},
volume = {40},
pages = {105522},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2020.105522},
url = {https://www.sciencedirect.com/science/article/pii/S0267364920301278},
author = {Meirong Guo},
keywords = {Internet court, Adjudication model, Civil jurisdiction, Internet technology},
abstract = {China established the world's first Internet court in Hangzhou in August 2017. Subsequently in 2018 Internet courts in Beijing and Guangzhou were established respectively. With the official establishment of these three Internet courts, China's electronic litigation advanced to a new stage.. Internet courts offer many advantages, and this innovative adjudication model has earned widespread approval for both its speedy acceptance of cases and speedy hearing of cases. This article analyzes the questions and challenges faced by Internet courts, proposes solutions such as compliance with three basic legal ethical principles, re-establishing the sense of presence and ritual of litigation, establishment of risk mitigation mechanisms between the legal system and technological systems to develop the ability for the construction of Internet courts in China.}
}
@article{AHUJA20214,
title = {Evolving term “accessibility” in spatial systems: Contextual evaluation of indicators},
journal = {Transport Policy},
volume = {113},
pages = {4-11},
year = {2021},
note = {Contemporary national and regional transport policy and planning},
issn = {0967-070X},
doi = {https://doi.org/10.1016/j.tranpol.2021.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0967070X21000688},
author = {Richa Ahuja and Geetam Tiwari},
keywords = {Accessibility, Evolution, Review, Measures, Indicator, Barriers},
abstract = {Access terminology is evolving since its inception by Hansen in 1959. Accounting accessibility is central to multiple disciplines such as geography, transportation, health, economics, social sciences, etc. Developing indicators to measure access is a common practice and usually favor specific dimensions of access based on application. Although measuring accessibility and developing related indicators is a common practice, there are missing links in the indicator development, planning process, its implementation and related policy-making. Due to many available indicators, each differing in context, the practicality of implementation and their transferability is generally lost. Current work focuses on extracting commonalities between indicators and understanding how the contextual focus of indicators’ have changed over time to measure access. Requirements for improved access related policies, developing realistic measures and future research directions based on gaps in the identified access measures are suggested.}
}
@article{BIMONTE2021101231,
title = {Collect and analysis of agro-biodiversity data in a participative context: A business intelligence framework},
journal = {Ecological Informatics},
volume = {61},
pages = {101231},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101231},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000224},
author = {Sandro Bimonte and Olivier Billaud and Benoît Fontaine and Thomy Martin and Frédéric Flouvat and Ali Hassan and Nora Rouillier and Lucile Sautot},
keywords = {Data warehouse, Data science, Biodiversity, Agriculture},
abstract = {In France and Europe, farmland represents a large fraction of land cover. The study and assessment of biodiversity in farmland is therefore a major challenge. To monitor biodiversity across wide areas, citizen science programs have demonstrated their effectiveness and relevance. The involvement of citizens in data collection offers a great opportunity to deploy extensive networks for biodiversity monitoring. But citizen science programs come with two issues: large amounts of data to manage and large numbers of participants with heterogeneous skills, needs and expectations about these data. In this article, we offer a solution to these issues, concretized by an information system. The study is based on a real life citizen science program tailored for farmers. This information system provides data and tools at several levels of complexity, to fit the needs and the skills of several users, from citizens with basic IT knowledge to scientists with strong statistical background. The proposed system is designed as follows. First, a data warehouse stores the data collected by citizens. This data warehouse is modelled depending on future data analysis. Secondly, associated with the data warehouse, a standard OLAP tool enables citizens and scientists to explore data. To complete the OLAP tool, we implement and compare four feature selection methods, in order to rank explanatory factors according to their relevance. Finally, for users with extended statistical skills, we use Generalized Linear Mixed Models to explore the temporal dynamics of invertebrate diversity in farmland ecosystems. The proposed system, a combination of business intelligence tools, data mining methods and advanced statistics, offers an example of complete exploitation of data by several user profiles. The proposition is supported by a real life citizen science program, and can be used as a guideline to design information systems in the same field.}
}
@article{CATO2021837,
title = {The Accuracy of Medication Administration Data in the Emergency Department: Why Does It Matter?},
journal = {Journal of Emergency Nursing},
volume = {47},
number = {6},
pages = {837-838},
year = {2021},
issn = {0099-1767},
doi = {https://doi.org/10.1016/j.jen.2021.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0099176721002464},
author = {Kenrick Cato}
}
@article{JAVAID2021100109,
title = {Significance of Quality 4.0 towards comprehensive enhancement in manufacturing sector},
journal = {Sensors International},
volume = {2},
pages = {100109},
year = {2021},
issn = {2666-3511},
doi = {https://doi.org/10.1016/j.sintl.2021.100109},
url = {https://www.sciencedirect.com/science/article/pii/S2666351121000309},
author = {Mohd Javaid and Abid Haleem and Ravi {Pratap Singh} and Rajiv Suman},
keywords = {Quality 4.0, Industry 4.0, Quality revolution, Quality control, Sensors, Technologies},
abstract = {Quality 4.0 corresponds to the growing digitisation of industry, which uses advanced technologies to enhance the quality of manufacturing and services. This fourth quality revolution is envisaged to digitise the entire quality systems and subsequently improve the existing quality approaches. Innovative industries adopt cloud-based quality 4.0 innovations in the controlled production process. It is used to resolve quality problems satisfactorily when they emerge and carry out real-time quality analyses to improve competitiveness and use them. Various ongoing challenges are take-over by Quality 4.0 technologies, such as automated root cause analysis, machine-to-machine connectivity to parameter auto adjustment, simulation of real-time processes and more. Quality 4.0 is a modern form of quality management. Digital technologies paired with more sophisticated methods and smarter processes will allow high-performance teams to provide consumers with high-performance and quality goods reliably. Sensors play an essential role in improving the quality of manufacturing and services. These can improve protection, increased internal productivity and sustainable operations. This paper provides how quality 4.0 will have a significant impact in the field of manufacturing. Various Key Aspects and enablers of Quality 4.0 for Manufacturing are discussed, finally, Identified and discussed eighteen significant applications of Quality 4.0 in the field of manufacturing. Quality 4.0 not only concerns the things happening inside a factory; it also includes the complete supply chain from Research and Development (R&D), manufacturing, development, distribution, sales, and service after-sales.}
}
@article{DELSO2021112,
title = {How to Design AI-Driven Clinical Trials in Nuclear Medicine},
journal = {Seminars in Nuclear Medicine},
volume = {51},
number = {2},
pages = {112-119},
year = {2021},
note = {Artificial Intelligence in Nuclear Medicine},
issn = {0001-2998},
doi = {https://doi.org/10.1053/j.semnuclmed.2020.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0001299820301082},
author = {Gaspar Delso and Davide Cirillo and Joshua D Kaggie and Alfonso Valencia and Ur Metser and Patrick Veit-Haibach},
abstract = {Artificial intelligence (AI) is an overarching term for a multitude of technologies which are currently being discussed and introduced in several areas of medicine and in medical imaging specifically. There is, however, limited literature and information about how AI techniques can be integrated into the design of clinical imaging trials. This article will present several aspects of AI being used in trials today and how imaging departments and especially nuclear medicine departments can prepare themselves to be at the forefront of AI-driven clinical trials. Beginning with some basic explanation on AI techniques currently being used and existing challenges of its implementation, it will also cover the logistical prerequisites which have to be in place in nuclear medicine departments to participate successfully in AI-driven clinical trials.}
}
@article{MAHAJAN2021102628,
title = {From Do-It-Yourself (DIY) to Do-It-Together (DIT): Reflections on designing a citizen-driven air quality monitoring framework in Taiwan},
journal = {Sustainable Cities and Society},
volume = {66},
pages = {102628},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102628},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720308453},
author = {Sachit Mahajan and Cyuan-Heng Luo and Dong-Yi Wu and Ling-Jyh Chen},
keywords = {Air pollution monitoring, Particulate matter exposure, Citizen science, Resilient cities, Low-cost sensors},
abstract = {Air pollution is a serious problem and has caused public health concerns all over the world. Despite the evidence, the preparedness and response of citizens has been limited. This underlines the importance of having sustainable air quality monitoring solutions that foster inclusion and multi-stakeholder partnerships for social-scientific interventions. This study illustrates how AirBox project has emerged in Taiwan, where makers and citizens use the sensors to sense air quality and provide the public with actionable data about their environments. The AirBox project includes elements of technology-innovation and citizen science: (1) Participatory Sensing – Static and mobile air quality sensing, (2) Open Data – Open hardware, software and access to data, (3) Co-creation Citizen Science – Citizen-led campaigns and forums, and (4) Outreach – Knowledge sharing, trust building and multi-stakeholder collaboration. The project uses a wide range of sensors to provide extendable solutions and data at fine spatio-temporal resolution. The results are highlighted using five cases studies that show how integrating social dimensions in an air quality monitoring framework can lead to public awareness, data-driven applications and environmentally sustainable cities. The multi-faceted approach highlights the effects of a bottom-up citizen science approach that considers local culture, practices and problems at grassroots.}
}
@article{IBRAHIM20211355,
title = {Is poor quality non-melanoma skin cancer data affecting high quality research and patient care?},
journal = {Journal of Plastic, Reconstructive & Aesthetic Surgery},
volume = {74},
number = {6},
pages = {1355-1401},
year = {2021},
issn = {1748-6815},
doi = {https://doi.org/10.1016/j.bjps.2020.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S1748681520307117},
author = {Nader Ibrahim and John Gibson and Stephen Ali and Thomas Dobbs and Iain S. Whitaker}
}
@article{KRISHEN2021183,
title = {A broad overview of interactive digital marketing: A bibliometric network analysis},
journal = {Journal of Business Research},
volume = {131},
pages = {183-195},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.03.061},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321002241},
author = {Anjala S. Krishen and Yogesh K. Dwivedi and N. Bindu and K. Satheesh Kumar},
keywords = {Digital marketing, Interactive marketing, Mobile marketing, e-marketing, e-advertising, e-Word-of-mouth},
abstract = {The widespread adoption of digital technologies and online social networks has revolutionized the way marketers engage with consumers. By deploying various digital platforms and information and communication technology (ICT) tools (e.g., smartphones, social media, mobile apps, electronic billboards, etc.), organizations can compete with more objective, relational, and interactive marketing techniques. The adoption of innovative devices and data-driven marketing, specifically in digital advertising, provides both a wide and efficient reach. Consequently, digital marketing (DM) triggered the creation of more informed, empowered, and connected groups of customers in both the real and virtual worlds. This paper tracks research dynamics in interactive digital marketing by identifying the stages of evolution of major topics, articles, citation and co-citation networks, using various computational techniques, including growth curve analysis and citation network analysis of bibliometric information. Finally, the study offers contributions to the field of interactive digital marketing as an international and interdisciplinary field of research.}
}
@article{CARVALHO2021499,
title = {On the relevance of data science for flight delay research: a systematic review},
journal = {Transport Reviews},
volume = {41},
number = {4},
pages = {499-528},
year = {2021},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2020.1861123},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722000666},
author = {Leonardo Carvalho and Alice Sternberg and Leandro {Maia Gonçalves} and Ana {Beatriz Cruz} and Jorge A. Soares and Diego Brandão and Diego Carvalho and Eduardo Ogasawara},
keywords = {Flight delay, data science, data management, data analytics, systematic review},
abstract = {ABSTRACT
Flight delays are a significant problem for society as they evenly impair airlines, transport companies, air traffic controllers, facility managers, and passengers. Studying prior flight data is an essential activity for every player involved in the air transportation system. Besides, developing accurate prediction models for flight delays is a crucial component of the decision-making process. Prescribing actions to solve on-going delays is an even challenging task due to the air transportation system complexity. In this regard, this paper presents a thorough literature review of data science techniques used for investigating flight delays. This work proposes a taxonomy and compiles the initiatives used to address the flight delay studies. It also offers a systematic literature review that describes the trends of the field and methods to analyse the applicability of newly proposed methods.}
}
@article{HEATH2021112372,
title = {Preface},
journal = {Food and Chemical Toxicology},
volume = {155},
pages = {112372},
year = {2021},
issn = {0278-6915},
doi = {https://doi.org/10.1016/j.fct.2021.112372},
url = {https://www.sciencedirect.com/science/article/pii/S0278691521004051},
author = {David Heath and Milena Horvat and Nives Ogrinc}
}
@article{YIN2021104274,
title = {Risk based arsenic rational sampling design for public and environmental health management},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {211},
pages = {104274},
year = {2021},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2021.104274},
url = {https://www.sciencedirect.com/science/article/pii/S0169743921000423},
author = {Lihao Yin and Huiyan Sang and Douglas J. Schnoebelen and Brian Wels and Don Simmons and Alyssa Mattson and Michael Schueller and Michael Pentella and Susie Y. Dai},
keywords = {Private well, Spatially clustered function model, Resource management},
abstract = {Groundwater contaminated with arsenic has been recognized as a global threat, which negatively impacts human health. Populations that rely on private wells for their drinking water are vulnerable to the potential arsenic-related health risks such as cancer and birth defects. Arsenic exposure through drinking water is among one of the primary arsenic exposure routes that can be effectively managed by active testing and water treatment. From the public and environmental health management perspective, it is critical to allocate the limited resources to establish an effective arsenic sampling and testing plan for health risk mitigation. We present a spatially adaptive sampling design approach based on an estimation of the spatially varying underlying contamination distribution. The method is different from traditional sampling design methods that often rely on a spatially constant or smoothly varying contamination distribution. In contrast, we propose a statistical regularization method to automatically detect spatial clusters of the underlying contamination risk from the currently available private well arsenic testing data in the USA, Iowa. This approach allows us to develop a sampling design method that is adaptive to the changes in the contamination risk across the identified clusters. We provide the spatially adaptive sample size calculation and sampling location determination at different acceptance precision and confidence levels for each cluster. The spatially adaptive sampling approach may effectively mitigate the arsenic risk from the resource management perspectives. The model presents a framework that can be widely used for other environmental contaminant monitoring and sampling for public and environmental health.}
}
@article{TAY2021101749,
title = {Industry 4.0: Current practice and challenges in Malaysian manufacturing firms},
journal = {Technology in Society},
volume = {67},
pages = {101749},
year = {2021},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101749},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X21002244},
author = {S.I. Tay and J. Alipal and T.C. Lee},
keywords = {Industry 4.0, Manufacturing firms, Industry 4.0 exploration, Internet of things, Policy, Data management},
abstract = {This research employed a qualitative approach to discuss the current practice and challenges of Malaysian manufacturing firms in the implementation of Industry 4.0. The study examined data from seven manufacturing companies pursuing Industry 4.0 initiatives to identify various options for their strategies. The study found that the implementation of Industry 4.0 in the manufacturing firms is still in the exploratory stage. The companies involved in this study were discovered to conduct exploration using an adaptive-like framework. That is, throughout the process, the majority of the subjects are 'trying and adding' Industry 4.0 to their operations. Their trial-and-error approach is based on what is feasible and effective in their manufacturing environment. Overall, the investigation determined that data management and integration, as well as personnel re-education, were the respondents' primary operational challenges.}
}
@article{HE2021116767,
title = {Development of the Leeuwin Current on the northwest shelf of Australia through the Pliocene-Pleistocene period},
journal = {Earth and Planetary Science Letters},
volume = {559},
pages = {116767},
year = {2021},
issn = {0012-821X},
doi = {https://doi.org/10.1016/j.epsl.2021.116767},
url = {https://www.sciencedirect.com/science/article/pii/S0012821X21000261},
author = {Yuxin He and Huanye Wang and Zhonghui Liu},
keywords = {Leeuwin Current, Indonesian Throughflow, northwest shelf of Australia, Pliocene-Pleistocene, temperatures, primary productivity},
abstract = {Although the Leeuwin Current (LC) is thought to play a pivotal role in climatic and oceanic systems of the western Australian region, how the LC developed through the Pliocene-Pleistocene period remains elusive. Here we used biomarker records to reconstruct variations of temperatures and primary productivity on the northwest shelf of Australia over the last 6 million years. Since ∼1.2 million years ago (Ma), our sea surface temperature record indicates progressive warming, with temperature values comparable to those in the Indo-Pacific Warm Pool, in contrast with the long-term global cooling trend. The regional surface warming was accompanied by suppressed primary productivity, together indicating prevailing warm, low-salinity, nutrient-deficient surface water, and thus a stronger LC since the Mid-Pleistocene Transition. During 4–1.2 Ma, greater surface temperature gradient between the Indo-Pacific Warm Pool and the northwest shelf of Australia and higher primary productivity seem to suggest a generally weaker LC. Warmer temperatures and lower productivity suggest a plausible existence of the LC during 6–4 Ma, but more work is required to confirm this. Impact of sea level and the Indonesian Throughflow on the LC strength may exist, but did not dominate through the Pliocene-Pleistocene period, considering different variation patterns among them. We propose the stronger LC after ∼1.2 Ma was more likely triggered by enhanced atmospheric circulation. Although the increased LC after ∼1.2 Ma may have potentially brought additional moisture to the Australian continent during the interglacial periods, it has not overturned the long-term drying trend through the Pliocene-Pleistocene period.}
}
@article{RUKANOVA2021101496,
title = {Identifying the value of data analytics in the context of government supervision: Insights from the customs domain},
journal = {Government Information Quarterly},
volume = {38},
number = {1},
pages = {101496},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2020.101496},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X20302756},
author = {Boriana Rukanova and Yao-Hua Tan and Micha Slegt and Marcel Molenhuis and Ben {van Rijnsoever} and Jonathan Migeotte and Mathieu L.M. Labare and Krunoslav Plecko and Bora Caglayan and Gavin Shorten and Otis {van der Meij} and Suzanne Post},
keywords = {Framework, Data analytics, Value, Government, Supervision, eCommerce, Capabilities, Collective capability},
abstract = {eCommerce, Brexit, new safety and security concerns are only a few examples of the challenges that government organisations, in particular customs administrations, face today when controlling goods crossing borders. To deal with the enormous volumes of trade customs administrations rely more and more on information technology (IT) and risk assessment, and are starting to explore the possibilities that data analytics (DA) can offer to support their supervision tasks. Driven by customs as our empirical domain, we explore the use of DA to support the supervision role of government. Although data analytics is considered to be a technological breakthrough, there is so far only a limited understanding of how governments can translate this potential into actual value and what are barriers and trade-offs that need to be overcome to lead to value realisation. The main question that we explore in this paper is: How to identify the value of DA in a government supervision context, and what are barriers and trade-offs to be considered and overcome in order to realise this value? Building on leading models from the information system (IS) literature, and by using case studies from the customs domain, we developed the Value of Data Analytics in Government Supervision (VDAGS) framework. The framework can help managers and policy-makers to gain a better understanding of the benefits and trade-offs of using DA when developing DA strategies or when embarking on new DA projects. Future research can examine the applicability of the VDAGS framework in other domains of government supervision.}
}
@article{DIAZDEARCAYA202318,
title = {Orfeon: An AIOps framework for the goal-driven operationalization of distributed analytical pipelines},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {18-35},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003223},
author = {Josu Díaz-de-Arcaya and Ana I. Torre-Bastida and Raúl Miñón and Aitor Almeida},
keywords = {Edge computing, Analytical pipelines, Machine learning operationalization, AIOps, MLOps},
abstract = {The use of Artificial Intelligence solutions keeps raising in the business domain. However, this adoption has not brought the expected results to companies so far. There are several reasons that make Artificial Intelligence solutions particularly complicated to adopt by businesses, such as the knowledge gap between the data science and operations teams. In this paper, we tackle the operationalization of distributed analytical pipelines in heterogeneous production environments, which span across different computational layers. In particular, we present a system called Orfeon, which can leverage different objectives and yields an optimized deployment for these pipelines. In addition, we offer the mathematical formulation of the problem alongside the objectives in hand (i.e. resilience, performance, and cost). Next, we propose a scenario utilizing cloud and edge infrastructural devices, in which we demonstrate how the system can optimize these objectives, without incurring scalability issues in terms of time nor memory. Finally, we compare the usefulness of Orfeon with a variety of tools in the field of machine learning operationalization and conclude that it is able to outperform these tools under the analyzed criteria, making it an appropriate system for the operationalization of machine learning pipelines.}
}
@article{WU2021102594,
title = {Multi-label active learning from crowds for secure IIoT},
journal = {Ad Hoc Networks},
volume = {121},
pages = {102594},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102594},
url = {https://www.sciencedirect.com/science/article/pii/S157087052100130X},
author = {Ming Wu and Qianmu Li and Muhammad Bilal and Xiaolong Xu and Jing Zhang and Jun Hou},
keywords = {Crowdsourcing, Secure IIoT, Annotation consensus, Multi-label learning, Active learning},
abstract = {With the development of IIoT (Industrial Internet of Things), Artificial Intelligence technology is widely used in many research areas, such as image classification, speech recognition, and information retrieval. Traditional supervised machine learning obtains labels from high-quality oracles, which is high cost and time-consuming and does not consider security. Since multi-label active learning becomes a hot topic, it is more challenging to train efficient and secure classification models, and reduce the label cost in the field of IIoT. To address this issue, this research focuses on the secure multi-label active learning for IIoT using an economical and efficient strategy called crowdsourcing, which involves querying labels from multiple low-cost annotators with various expertise on crowdsourcing platforms rather than relying on a high-quality oracle. To eliminate the effects of annotation noise caused by imperfect annotators, we propose the Multi-label Active Learning from Crowds (MALC) method, which uses a probabilistic model to simultaneously compute the annotation consensus and estimate the classifier’s parameters while also taking instance similarity into account. Then, to actively choose the most informative instances and labels, as well as the most reliable annotators, an instance-label-annotator triplets selection technique is proposed. Experimental results on two real-world data sets show that the performance of MALC is superior to existing methods.}
}
@article{LIU2021144649,
title = {Regional characteristics of children's blood lead levels in China: A systematic synthesis of national and subnational population data},
journal = {Science of The Total Environment},
volume = {769},
pages = {144649},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.144649},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720381808},
author = {Yang Liu and Feiyan Liu and Kylie Fei Dong and Yongning Wu and Xingfen Yang and Jintao Yang and Hong Tan and Xiaojun Niu and Xinyuan Zhao and Gexin Xiao and Shaoqi Zhou},
keywords = {Blood lead, Child health, Regional characteristics, China, Prefectures},
abstract = {The blood lead levels (BLLs) of children in China remain notably high in many areas. We aimed to summarise the relevant regional characteristics, identifying problematic areas and the causes of lead pollution. We searched the databases of PubMed, China National Knowledge Infrastructure (CNKI), and Wanfang Data, systematically reviewing 219 articles published from January 2010 to September 2020. In doing so, we assessed the BLLs noted in 220 prefectures across China. Data were organised using Geographic Information Systems (GIS) mapping. Out of a total of 629,627 children sampled, we found that the average blood lead level (BLL) of children included in our study is 50.61 ± 13.63 μg/L, which slightly exceeds the 50.00 μg/L US standard. Within the sample, 8.75% had BLLs higher than 100.00 μg/L. Children living in Liaoning, Hebei, Shanxi, Jiangxi, Anhui, Fujian, Guizhou, Yunnan, and Guangxi had notably high BLLs, at more than 60.00 μg/L. A total of 112 municipalities had an average children's BLL above 50.00 μg/L. Furthermore, Chenzhou, Linfen, Yuncheng, and Hechi had the highest children's BLLs, with average values above 100.00 μg/L. The leading contributors to lead pollution are lead mining, lead recovery and the smelting industry. Nonetheless, the lead-acid battery industry needs more attention. Although data suggest that BLLs are decreasing in China, many areas still have high BLLs that need to be monitored. Moreover, national standards must improve to decrease acceptable BLL thresholds for children.}
}
@incollection{CHAKRABORTY2021191,
title = {Chapter 9 - Industry sustainable supply chain management with data science},
editor = {Jennifer Dunn and Prasanna Balaprakash},
booktitle = {Data Science Applied to Sustainability Analysis},
publisher = {Elsevier},
pages = {191-202},
year = {2021},
isbn = {978-0-12-817976-5},
doi = {https://doi.org/10.1016/B978-0-12-817976-5.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128179765000103},
author = {Deboleena Chakraborty and Richard K. Helling},
keywords = {Sustainability, Supply chain, Carbon footprints, Life cycle assessment (LCA), Big data analytics, Data quality, GHG reduction, Multi-objective optimization, Geospatial information},
abstract = {Life cycle assessment (LCA) is a quantitative tool to bring environmental insights into decisions, supplementing consideration of cost, performance and social impact. Calculation of “carbon footprints” or other metrics derived from LCA typically requires data and information drawn from many sources, both within an organization and externally. Knowing LCA metrics for all products and companies could help society understand the most significant opportunities and trade-offs in the quest for sustainability. A barrier to this state of knowledge is the ability to create, access, manage and verify the extensive data required. New information technology and analytics can allow LCA to get closer to the ideal state, reducing the time required to do an assessment and increasing the quality of the results.}
}
@article{KRITTANAWONG2021780,
title = {Opportunities and challenges for artificial intelligence in clinical cardiovascular genetics},
journal = {Trends in Genetics},
volume = {37},
number = {9},
pages = {780-783},
year = {2021},
issn = {0168-9525},
doi = {https://doi.org/10.1016/j.tig.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0168952521000858},
author = {Chayakrit Krittanawong and Kipp W. Johnson and Benjamin S. Glicksberg},
abstract = {A combination of emerging genomic and artificial intelligence (AI) techniques may ultimately unlock a deeper understanding of heterogeneity and biological complexities in cardiovascular diseases (CVDs), leading to advances in prognostic guidance and personalized therapies. We discuss the state of AI in cardiovascular genetics, current applications, limitations, and future directions of the field.}
}
@article{XU2021103828,
title = {Spatiotemporal forecasting in earth system science: Methods, uncertainties, predictability and future directions},
journal = {Earth-Science Reviews},
volume = {222},
pages = {103828},
year = {2021},
issn = {0012-8252},
doi = {https://doi.org/10.1016/j.earscirev.2021.103828},
url = {https://www.sciencedirect.com/science/article/pii/S0012825221003299},
author = {Lei Xu and Nengcheng Chen and Zeqiang Chen and Chong Zhang and Hongchu Yu},
keywords = {Spatiotemporal forecasting, Artificial intelligence, Physical model, Uncertainty modeling, Predictability},
abstract = {Spatiotemporal forecasting (STF) extends traditional time series forecasting or spatial interpolation problem to space and time dimensions. Here, we review the statistical, physical and artificial intelligence (AI) methods, data and model uncertainties, predictability and future directions for STF problems. Statistical STF methods have limitations in high-level feature extractions and long-term memory modeling. Physical models are computationally intensive and are imperfect in model structure and parameterization. AI models lack the interpretability and require elaborate training but can model complex nonlinear and non-Gaussian problems. Integrating data-driven and physical model-driven methods could facilitate the improvement of interpretability and forecasting accuracy. The predictive uncertainty comes from data and models, which could be measured by probability distribution and Bayesian inference, respectively. The predictive uncertainty is generally missing in AI models and could be resolved by incorporating Bayesian frameworks. The predictability of dynamic earth systems is spatiotemporally heterogeneous and is generally examined by diagnostic and prognostic approaches. Diagnostic methods analyze the predictability empirically from a theoretical perspective, while prognostic methods investigate the predictability through real experiments. Unraveling the predictability in space and time and the predictability sources will greatly improve earth system understanding and operational forecasting development. Current STF systems are largely not user-friendly to provide probabilistic and understandable forecasting services in near real-time. Intelligent STF systems should automatically prepare various data sources, train the models in a self-adaptative way and provide timely predictive information services for users to make decisions. This review provides state-of-the-art advances in forecasting sciences and highlights new directions for new-generation STF systems.}
}
@article{MOURTZIS2021525,
title = {Equipment Design Optimization Based on Digital Twin Under the Framework of Zero-Defect Manufacturing},
journal = {Procedia Computer Science},
volume = {180},
pages = {525-533},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.271},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003203},
author = {Dimitris Mourtzis and John Angelopoulos and Nikos Panopoulos},
keywords = {Digital Twin, Machine Design, Zero-Defect Manufacturing},
abstract = {A digitalized Smart Factory can be considered as a data island. Moreover, engineers have focused on the development of new technologies and techniques not only for transforming information to data but also to achieve efficient data utilization to further optimize manufacturing processes. However, the Zero-Defect Manufacturing concept has emerged, where the main goal is production optimization. The cornerstone in achieving the factories of the future is to further optimize the design of new assets so as they comply with the unique requirements of the customers. Therefore, this paper proposes the conceptualization, design, and initial development of a platform for the utilization of data derived from industrial environments for the optimization of the equipment design. The main aspects of the proposed framework are the data acquisition, data processing and the simulation. The applicability of the proposed framework has been tested in a laboratory-based machine shop utilizing data from a real-life industrial scenario.}
}
@article{BELFIELD2021104956,
title = {Determination of “fitness-for-purpose” of quantitative structure-activity relationship (QSAR) models to predict (eco-)toxicological endpoints for regulatory use},
journal = {Regulatory Toxicology and Pharmacology},
volume = {123},
pages = {104956},
year = {2021},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2021.104956},
url = {https://www.sciencedirect.com/science/article/pii/S0273230021000969},
author = {Samuel J. Belfield and Steven J. Enoch and James W. Firman and Judith C. Madden and Terry W. Schultz and Mark T.D. Cronin},
keywords = { models, QSAR, Toxicity prediction, Uncertainty, Regulatory use},
abstract = {In silico models are used to predict toxicity and molecular properties in chemical safety assessment, gaining widespread regulatory use under a number of legislations globally. This study has rationalised previously published criteria to evaluate quantitative structure-activity relationships (QSARs) in terms of their uncertainty, variability and potential areas of bias, into ten assessment components, or higher level groupings. The components have been mapped onto specific regulatory uses (i.e. data gap filling for risk assessment, classification and labelling, and screening and prioritisation) identifying different levels of uncertainty that may be acceptable for each. Twelve published QSARs were evaluated using the components, such that their potential use could be identified. High uncertainty was commonly observed with the presentation of data, mechanistic interpretability, incorporation of toxicokinetics and the relevance of the data for regulatory purposes. The assessment components help to guide strategies that can be implemented to improve acceptability of QSARs through the reduction of uncertainties. It is anticipated that model developers could apply the assessment components from the model design phase (e.g. through problem formulation) through to their documentation and use. The application of the components provides the possibility to assess QSARs in a meaningful manner and demonstrate their fitness-for-purpose against pre-defined criteria.}
}
@article{WANG2021119279,
title = {Generalized models to predict the lower heating value (LHV) of municipal solid waste (MSW)},
journal = {Energy},
volume = {216},
pages = {119279},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.119279},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220323860},
author = {Dan Wang and Yu-Ting Tang and Jun He and Fei Yang and Darren Robinson},
keywords = {LHV prediction, Physical composition of municipal solid waste, Multiple regression, Artificial neural network},
abstract = {Accurately and efficiently predicting the LHV of MSW is vital for designing and operating a waste-to-energy plant. However, previous prediction models possess limited geographical applicability. In this paper, we employ multiple linear regression and artificial neural network (ANN) techniques to predict LHV. These data-driven models utilize 151 globally distributed datasets identified during a systematic literature review, describing the wet physical composition of MSW and measured LHV. The results show that models built via both methods exhibited acceptable and compatible levels of performance in predicting LHV, based on the multiple statistical indicators. However, the ANN model proved to be more robust in handling of datasets of diverse quality. Models developed from both methods demonstrate clearly that the wet proportion of food waste has a negative impact on LHV. Supported by the strong and significant correlation between food waste and moisture content, we concluded that the negative impact of high moisture content in food waste on LHV outweighed its calorific value. Separating food waste or any other waste with high moisture content from the MSW for incineration can significantly improve energy recovery efficiency. Contrary to expectation, the models also reveal a higher contribution of paper waste to the LHV of MSW than plastic waste.}
}
@article{STAVELIN2021101269,
title = {Applying object detection to marine data and exploring explainability of a fully convolutional neural network using principal component analysis},
journal = {Ecological Informatics},
volume = {62},
pages = {101269},
year = {2021},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101269},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121000601},
author = {Herman Stavelin and Adil Rasheed and Omer San and Arne Johan Hestnes},
keywords = {Neural networks, PCA, Object detection, XAI, Machine learning, YOLO},
abstract = {With the rise of focus on man made changes to our planet and wildlife therein, more and more emphasis is put on sustainable and responsible gathering of resources. In an effort to preserve maritime wildlife the Norwegian government decided to create an overview of the presence and abundance of various species of marine lives in the Norwegian fjords and oceans. The current work evaluates the possibility of utilizing machine learning methods in particular the You Only Look Once version 3 algorithm to detect fish in challenging conditions characterized by low light, undesirable algae growth and high noise. It was found that the algorithm trained on images collected during the day time under natural light could detect fish successfully in images collected during night under artificial lighting. The overall average precision score of 88% was achieved. Later principal component analysis was used to analyze the features learned in different layers of the network. It is concluded that for the purpose of object detection in specific application areas, the network can be considerably simplified since many of the feature detector turns our to be redundant.}
}
@article{LABONTE2021101870,
title = {Tweets and transitions: Exploring Twitter-based political discourse regarding energy and electricity in Ontario, Canada},
journal = {Energy Research & Social Science},
volume = {72},
pages = {101870},
year = {2021},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2020.101870},
url = {https://www.sciencedirect.com/science/article/pii/S221462962030445X},
author = {D. Labonte and I.H. Rowlands},
keywords = {Energy politics, Social media analysis, Twitter, Sustainability transitions, Ontario, Canada},
abstract = {The article explores how Twitter data can inform the study of the socio-political dimensions of sustainability transitions. Twitter is a widely used microblogging platform that allows users to share short comments, media, and links, and that offers researchers significant data collection opportunities. Twitter-based research has been growing in application in many disciplines but has not been prominently used in relation to sustainability transitions or sustainable energy research. This study aims to characterize the Twitter-based conversations regarding energy issues and politics in Ontario, Canada. The analysis in this article is based on 6946 tweets, from 2841 unique users, which were collected between September 2, 2017 and January 12, 2018. The Twitter-based discourse regarding energy issues in Ontario is described by a minority of very engaged users contributing disproportionately to the conversation, the most engaged users contributing different types of tweets to the conversation, and overall engagement that varies based on news events. Coding based on manual interpretation of the tweets by the most engaged users and those tweets that were highly retweeted identified a discourse that was highly partisan and often highlighted economic issues associated with electricity costs. Topics commonly associated with sustainable energy transitions were not prominent in the Twitter discourse. Additionally, the analysis suggests that users lacking traditional political empowerment can influence the political discourse on Twitter through high levels of retweets; however, savvy and strategic use of Twitter communication, rather than simply engagement with an issue, is important in generating consistent amplification from other users.}
}
@article{PLEY2021e739,
title = {Digital and technological innovation in vector-borne disease surveillance to predict, detect, and control climate-driven outbreaks},
journal = {The Lancet Planetary Health},
volume = {5},
number = {10},
pages = {e739-e745},
year = {2021},
issn = {2542-5196},
doi = {https://doi.org/10.1016/S2542-5196(21)00141-8},
url = {https://www.sciencedirect.com/science/article/pii/S2542519621001418},
author = {Caitlin Pley and Megan Evans and Rachel Lowe and Hugh Montgomery and Sophie Yacoub},
abstract = {Summary
Vector-borne diseases are particularly sensitive to changes in weather and climate. Timely warnings from surveillance systems can help to detect and control outbreaks of infectious disease, facilitate effective management of finite resources, and contribute to knowledge generation, response planning, and resource prioritisation in the long term, which can mitigate future outbreaks. Technological and digital innovations have enabled the incorporation of climatic data into surveillance systems, enhancing their capacity to predict trends in outbreak prevalence and location. Advance notice of the risk of an outbreak empowers decision makers and communities to scale up prevention and preparedness interventions and redirect resources for outbreak responses. In this Viewpoint, we outline important considerations in the advent of new technologies in disease surveillance, including the sustainability of innovation in the long term and the fundamental obligation to ensure that the communities that are affected by the disease are involved in the design of the technology and directly benefit from its application.}
}
@incollection{2021347,
title = {Index},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {347-355},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.09992-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150099928}
}
@article{BUSCHMANN2021313,
title = {Data-driven decision support for process quality improvements},
journal = {Procedia CIRP},
volume = {99},
pages = {313-318},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.047},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121003218},
author = {Daniel Buschmann and Chrismarie Enslin and Hannes Elser and Daniel Lütticke and Robert H. Schmitt},
keywords = {Data-Driven Decisions, Machine Learning, Defect Detection, Mass Production Manufacturing, Statistical Process Control, Random Forests},
abstract = {This paper presents a data-driven approach for improving the process quality of production systems. Therefore, the product quality is detected during the production process. The worker is provided with reasonable parameter recommendations about the production process as decision support to improve the process quality. To achieve this, a cross-process data analysis of the process and quality data is carried out using decision trees. The results are visualized in a comprehensible form for the worker. Based on a case study from mass production, the approach is evaluated and its performance is demonstrated in comparison to classical statistical methods.}
}
@article{SIRCAR2021379,
title = {Application of machine learning and artificial intelligence in oil and gas industry},
journal = {Petroleum Research},
volume = {6},
number = {4},
pages = {379-391},
year = {2021},
issn = {2096-2495},
doi = {https://doi.org/10.1016/j.ptlrs.2021.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S2096249521000429},
author = {Anirbid Sircar and Kriti Yadav and Kamakshi Rayavarapu and Namrata Bist and Hemangi Oza},
keywords = {Artificial intelligence, Machine learning, Upstream, Oil and gas industry, Petroleum exploration},
abstract = {Oil and gas industries are facing several challenges and issues in data processing and handling. Large amount of data bank is generated with various techniques and processes. The proper technical analysis of this database is to be carried out to improve performance of oil and gas industries. This paper provides a comprehensive state-of-art review in the field of machine learning and artificial intelligence to solve oil and gas industry problems. It also narrates the various types of machine learning and artificial intelligence techniques which can be used for data processing and interpretation in different sectors of upstream oil and gas industries. The achievements and developments promise the benefits of machine learning and artificial intelligence techniques towards large data storage capabilities and high efficiency of numerical calculations. In this paper a summary of various researchers work on machine learning and artificial intelligence applications and limitations is showcased for upstream and sectors of oil and gas industry. The existence of this extensive intelligent system could really eliminate the risk factor and cost of maintenance. The development and progress using this emerging technologies have become smart and makes the judgement procedure easy and straightforward. The study is useful to access intelligence of different machine learning methods to declare its application for distinct task in oil and gas sector.}
}
@incollection{CHINOY2021,
title = {Use of technology for real-world sleep and circadian research},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2021},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-822963-7.00200-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128229637002000},
author = {Evan D. Chinoy and Rachel R. Markwald},
keywords = {Actigraphy, Consumer sleep technology, Mobile EEG, Naturalistic settings, Nearables, Non-contact sensors, Polysomnography, Sleep monitoring, Wearables, Validation},
abstract = {Recent advances in technology and demand for biometric data have led to the creation of personal devices that track sleep and other physiological and behavioral patterns with increasing accuracy. Although such technologies have widespread use among the general population, applications for sleep and circadian research show much promise, but their current adoption has been slow in part due to the need for validation versus standard research methodologies. This article outlines the current state of sleep and circadian technologies for real-world research, their strengths and limitations, recommended standards for use, current operational use cases, and future directions for real-world applications.}
}
@article{MANTOUKA2021266,
title = {Smartphone sensing for understanding driving behavior: Current practice and challenges},
journal = {International Journal of Transportation Science and Technology},
volume = {10},
number = {3},
pages = {266-282},
year = {2021},
issn = {2046-0430},
doi = {https://doi.org/10.1016/j.ijtst.2020.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2046043020300460},
author = {Eleni Mantouka and Emmanouil Barmpounakis and Eleni Vlahogianni and John Golias},
keywords = {Driving, Behavior, Analytics, Smartphones, Maxinum likelihood, Profiling},
abstract = {Understanding driving behavior – even in the rapid emergence of automation - remains in the spotlight, for decomposing complex driving dynamics, enabling the development of user-friendly and acceptable autonomous vehicles and ensuring the safe co-existence of autonomous and conventional vehicles on the road. Mobile crowdsensing has emerged as a means to understand and model driving behavior. Although the advantages of collecting data through smartphones are many (speed, accuracy, low cost etc.), the challenges including, but do not limited to, the preparation rate, the processing needs, as well as the methodological, legislative and security issues, are significant. The present paper aims to review the research dedicated to analyzing driving behavior based on smartphone sensors’ data streams. We first establish an inclusive stepwise framework to describe the path from data collection to informed decision making. Next, the existing literature is thoroughly analyzed and challenges in relation to data collection and data mining practices are critically discussed placing particular emphasis on the limitations and concerns regarding the use of mobile phones for driving data collection, as well as using crowd sensed data for feature extraction. Subsequently, modeling driving behavior practices and end-to-end solutions for driver assistance and recommendation systems are also reviewed. The paper ends with a discussion on the most critical challenges arising from the literature and future research steps.}
}
@article{CASTILLO2021342,
title = {The productivity impact of the digitally connected 5 – layer stack in manufacturing enterprises},
journal = {Procedia CIRP},
volume = {104},
pages = {342-350},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.058},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009562},
author = {Adolfo Crespo del Castillo and John Patsavellas and Konstantinos Salonitis and Christos Emmanouilidis},
keywords = {Industry 4.0, Industrial Internet of Things (IIoT), MQTT, Manufacturing Execution System (MES), SCADA, Enterprise Scalable Data Architecture (ESDA), Overall Equipment Efficiency (OEE), Self-Cleaning-Data},
abstract = {This paper investigates the application of modern industrial internet protocols and network architecture in manufacturing companies, from the perspectives of productivity and sustainability, framed in the fourth industrial revolution paradigm. This is achieved by delving into the existing information systems and devices, their inter-operability and interconnections using industrial internet of things protocols. The paper details a study generating a standard model of data architecture to better unify the different layers of the information systems that typify most manufacturing companies, leveraging their existing digital infrastructure to establish a solid base for further digitalization.}
}
@article{LI2021123618,
title = {A data-driven reversible framework for achieving Sustainable Smart product-service systems},
journal = {Journal of Cleaner Production},
volume = {279},
pages = {123618},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123618},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620336635},
author = {Xinyu Li and Zuoxu Wang and Chun-Hsien Chen and Pai Zheng},
keywords = {Smart product-service system, Sustainability, Knowledge management, Reversible design, Context-awareness},
abstract = {Higher sustainability with extended product lifecycle is a tireless pursuit in companies’ product design/development endeavours. In this regard, two prevailing concepts, namely the smart circular system and smart product-service system (Smart PSS), have been introduced, respectively. However, most existing studies only focus on the sustainability of physical materials and components, without considering the cyber-physical resources as a whole, let alone an integrated strategy towards the so-called Sustainable Smart PSS. To fill the gap, this paper discusses the key features in Sustainable Smart PSS development from a broadened scope of cyber-physical resources management. A data-driven reversible framework is hereby proposed to sustainably exploit high-value and context-dependent information/knowledge in the development of Sustainable Smart PSS. A four-step context-aware process in the framework, including requirement elicitation, solution recommendation, solution evaluation, and knowledge evolvement, is further introduced to support the decision-making and optimization along the extended or circular lifecycle. An illustrative example is depicted in the sustainable development of a smart 3D printer, which validates the feasibility and advantages of the proposed framework. As an explorative study, it is hoped that this work provides useful insights for Smart PSS development with sustainability concerns in a cyber-physical environment.}
}
@article{TAMOLARRIEUX2021105541,
title = {Decision-making by machines: Is the ‘Law of Everything’ enough?},
journal = {Computer Law & Security Review},
volume = {41},
pages = {105541},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105541},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000145},
author = {Aurelia Tamò-Larrieux},
keywords = {Automated decision-making, Artificial intelligence, Data protection, Transparency, Fairness, Due process},
abstract = {Machines have moved from supporting decision-making processes of humans to making decisions for humans. This shift has been accompanied by concerns regarding the impact of decisions made by algorithms on individuals and society. Unsurprisingly, the delegation of important decisions to machines has therefore triggered a debate on how to regulate the automated decision-making practices. In Europe, policymakers have attempted to address these concerns through a combination of individual rights and due processes established in data protection law, which relies on other statutes, e.g., anti-discrimination law and restricting trade secret laws, to achieve certain goals. This article adds to the literature by disentangling the challenges arising from automated decision-making systems and focusing on ones arising without malevolence but merely as unwanted side-effects of increased automation. Such side-effects include ones arising from the internal processes leading to a decision, the impacts of decisions, as well as the responsibility for decisions and have consequences on an individual and societal level. Upon this basis the article discusses the redress mechanisms provided in data protection law. It shows that the approaches within data protection law complement one another, but do not fully remedy the identified side-effects. This is particularly true for side-effects that lead to systemic societal shifts. To that end, new paradigms to guide future policymaking discourse are being explored.}
}
@article{KUANG20211,
title = {Application and development trend of artificial intelligence in petroleum exploration and development},
journal = {Petroleum Exploration and Development},
volume = {48},
number = {1},
pages = {1-14},
year = {2021},
issn = {1876-3804},
doi = {https://doi.org/10.1016/S1876-3804(21)60001-0},
url = {https://www.sciencedirect.com/science/article/pii/S1876380421600010},
author = {Lichun KUANG and He LIU and Yili REN and Kai LUO and Mingyu SHI and Jian SU and Xin LI},
keywords = {artificial intelligence, logging interpretation, seismic exploration, reservoir engineering, drilling and completion, surface facility engineering},
abstract = {Aiming at the actual demands of petroleum exploration and development, this paper describes the research progress and application of artificial intelligence (AI) in petroleum exploration and development, and discusses the applications and development directions of AI in the future. Machine learning has been preliminarily applied in lithology identification, logging curve reconstruction, reservoir parameter estimation, and other logging processing and interpretation, exhibiting great potential. Computer vision is effective in picking of seismic first breaks, fault identification, and other seismic processing and interpretation. Deep learning and optimization technology have been applied to reservoir engineering, and realized the real-time optimization of waterflooding development and prediction of oil and gas production. The application of data mining in drilling, completion, and surface facility engineering etc. has resulted in intelligent equipment and integrated software. The potential development directions of artificial intelligence in petroleum exploration and development are intelligent production equipment, automatic processing and interpretation, and professional software platform. The highlights of development will be digital basins, fast intelligent imaging logging tools, intelligent seismic nodal acquisition systems, intelligent rotary-steering drilling, intelligent fracturing technology and equipment, real-time monitoring and control of zonal injection and production.}
}
@article{BENFER20211269,
title = {A Framework for Digital Twins for Production Network Management},
journal = {Procedia CIRP},
volume = {104},
pages = {1269-1274},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.213},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011112},
author = {Martin Benfer and Sina Peukert and Gisela Lanza},
keywords = {Digital Twin, Production Network, Modeling},
abstract = {The dynamic and highly complex task of production network management requires decision support through quantitative models. In the industrial praxis, these models are specifically designed and implemented for particular management decisions, requiring significant one-time effort for model creation. This contribution utilizes the digital twin concept to facilitate production network models that are continuously synchronized with the examined production network to support several different management decisions. The approach structures data from existing information systems as a synchronized generic base model, which is used to create problem-specific executable models, thereby saving costs through repeated model use and quicker decision making.}
}
@article{MAASS2021101909,
title = {Pairing conceptual modeling with machine learning},
journal = {Data & Knowledge Engineering},
volume = {134},
pages = {101909},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101909},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000367},
author = {Wolfgang Maass and Veda C. Storey},
keywords = {Conceptual modeling, Machine learning, Methodologies and tools, Models, Database management, Framework for incorporating conceptual modeling into data science projects, Artificial intelligence},
abstract = {Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.}
}
@article{CHANG2021101912,
title = {ArchNet: A data hiding design for distributed machine learning systems},
journal = {Journal of Systems Architecture},
volume = {114},
pages = {101912},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101912},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120301764},
author = {Kaiyan Chang and Wei Jiang and Jinyu Zhan and Zicheng Gong and Weijia Pan},
keywords = {Distributed machine learning, Data hiding, Neural networks, Embedded systems, Encryption},
abstract = {Integrating idle embedded devices into cloud computing is a promising approach to support Distributed Machine Learning (DML). In this paper, we approach to address the data hiding problem in such DML systems. For the purpose of the data encryption in DML systems, we introduce the tripartite asymmetric encryption theorem to provide theoretical support. Based on the theorem, we design a general image encryption scheme (called ArchNet), which can encrypt original images via the encoder to resist against illegal users. ArchNet encrypts the dataset by a specific neural network, which is especially trained for encryption. The encrypted data can be easily recognized by deep learning model. However, the encrypted data cannot be recognized by human, which makes the illegal attacker difficult to steal the encrypted data. We use MNIST, Fashion-MNIST and Cifar-10 datasets to evaluate efficiency of our design. We deploy certain base models on the encrypted datasets and compare them with the RC4 algorithm and differential privacy policy. Our design can improve the accuracy on MNIST up to 97.26% compared with RC4. The accuracies on these three datasets encrypted by ArchNet are similar to the base model. ArchNet can be deployed on DML systems with embedded devices.}
}
@article{LI2021106095,
title = {Stated acceptance and behavioral responses of drivers towards innovative connected vehicle applications},
journal = {Accident Analysis & Prevention},
volume = {155},
pages = {106095},
year = {2021},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106095},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521001263},
author = {Weixia Li and Guoyuan Wu and Danya Yao and Yi Zhang and Matthew J. Barth and Kanok Boriboonsomsin},
keywords = {Connected vehicle technology, Lane speed monitoring, High speed differential warning, Public acceptance, Behavioral response},
abstract = {This research is aimed at investigating drivers’ attitudes towards connected vehicle technology in general and two connected vehicle applications in particular—Lane Speed Monitoring and High Speed Differential Warning—which have been demonstrated via simulation to be effective in enhancing traffic mobility and safety, respectively. An online survey was sent to customers of an automobile manufacturer in the United States. Out of the 1453 survey responses that were received, 650 complete and valid responses were used to analyze the respondents’ stated acceptance of and expected behavioral responses to the two connected vehicle applications under a variety of scenarios. Statistical analyses were conducted to examine the influence of demographic and socioeconomic factors. The results reveal that the respondents express high willingness to use connected vehicle technology and the two applications under various circumstances, and the willingness is strongly associated with age, gender, education level, and income. Higher levels of acceptance are observed in older, male, higher-educated, or higher-income respondents, while the patterns of conditional acceptance and expected behavioral responses vary with specific scenarios. These results provide useful information for application developers, traffic operators, and policy makers to steer connected vehicle technology development and deployment in the direction that will benefit both the users and the society.}
}
@article{CHEN2021115699,
title = {Application of data-driven models to predictive maintenance: Bearing wear prediction at TATA steel},
journal = {Expert Systems with Applications},
volume = {186},
pages = {115699},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115699},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421010836},
author = {X. Chen and Jos {Van Hillegersberg} and E. Topan and S. Smith and M. Roberts},
keywords = {Predictive maintenance, Industry 4.0, Data-driven, Machine learning},
abstract = {Industries that are in transition to Industry 4.0 often face challenges in applying data-driven methods to improve performance. While ample methods are available in literature, knowledge on how to select and apply them is scarce. This study aims to address this gap reported on the design and implementation of data-driven models for predictive maintenance at TATA Steel, Shotton. The objective of the project is to predict the wearing behaviour of the components in the steel production line for maintenance activity decision support. To achieve the predictive maintenance goal, the approach applied can be summarized as follows: 1. business understanding and data collection, 2. literature review, 3. data preparation and exploration, 4. modelling and result analysis and 5. conclusion and recommendation. The data-driven methods that were analysed and compared are: Partial Least Squares Regression (PLSR), Artificial Neu- ral Network (ANN) and Random Forest(RF). After cleaning and analysing the production line data, predictive maintenance with the current available data in TATA Steel, Shotton is best feasible with PLSR. The study further concludes that, predictive maintenance is likely to be feasible in similar industries that are in transition to industry 4.0 and have growing volumes of production data with varying quality and detail. However, as illustrated in this case study, careful understanding of the industrial process, thorough modeling and cleaning of the data as well as careful method selection and tuning are required. Moreover, the resulting model needs to be packaged in a user friendly way to find its way to the job floor.}
}
@article{LI2021106888,
title = {Deep-learning-assisted power loss management for active distribution networks},
journal = {The Electricity Journal},
volume = {34},
number = {1},
pages = {106888},
year = {2021},
note = {Special Issue: Machine Learning Applications To Power System Planning And Operation},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2020.106888},
url = {https://www.sciencedirect.com/science/article/pii/S1040619020301809},
author = {Dairui Li and Jia Tan and Qun Yu and Zhiyi Li},
keywords = {Active distribution network, Deep learning, Data preprocessing, Power loss management, Cybersecurity},
abstract = {With widespread deployment of advanced information and communication techniques, power distribution networks have been undergoing a transition towards active distribution networks (ADNs). As ADNs feature multi-way interactions between power supply and demand as well as tight coupling between cyber and physical elements, conventional physical-driven energy management schemes are insufficient to tackle the challenges of increasingly complicated operating conditions. In this paper, a novel three-stage decision-making framework is proposed to manage the power losses of ADNs by taking advantage of state-of-the-art deep learning methods. The paper first sheds light on the opportunities and challenges brought by the immense amount of heterogeneous data in the context of ADNs. In particular, generative modeling methods such as generative adversarial networks are introduced to develop a data preprocessing scheme for fixing and enhancing data collected from heterogeneous field devices. The paper then employs recurrent neural networks such as long-short-term-memory networks to infer and formulate the intricate relations between preprocessed data with the objective of enhancing the observability of network topology and line losses. More specifically, an optimization-assisted deep learning method is proposed to facilitate the optimal decision making on power loss reduction under fast-changing operating conditions. Last, limitations of deep learning models such as cybersecurity challenges are discussed in depth in the context of ADNs.}
}
@article{ITCHHAPORIA2021412,
title = {Navigating the Path to Digital Transformation},
journal = {Journal of the American College of Cardiology},
volume = {78},
number = {4},
pages = {412-414},
year = {2021},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2021.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0735109721053845},
author = {Dipti Itchhaporia}
}
@article{HADJSASSI202129,
title = {Knowledge Management Process for Air Quality Systems based on Data Warehouse Specification},
journal = {Procedia Computer Science},
volume = {192},
pages = {29-38},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921014915},
author = {Mohamed Saifeddine {Hadj Sassi} and Lamia {Chaari Fourati} and Manel Zekri and Sadok {Ben Yahia}},
keywords = {Knowledge Management, Air Quality, Data Warehouse, Conceptual Data Model, Multidimensional Design, Ontology.},
abstract = {Even though several systems for Air Quality (AQ) monitoring have been in existence for over a decade, a research model for Knowledge Management (KM) of AQ data has to be created in order to enhance the decision-making and organize the air quality data collected from the Internet of Things (IoT) consumer devices. This model should be made more performant by ensuring greater flexibility and interoperability between devices and emerging technologies. In this context, we propose an approach for representing Data WareHouse (DWH) schema based on an ontology that captures the multidimensional knowledge of tools, techniques, and technologies used for novel AQ systems. This enhances decision-making by coping with potential problems such as data sources heterogeneity and covering the various phases of the decision-making life cycle.}
}
@incollection{BOUSQUET2023231,
title = {Chapter 11 - Asthma in the digital world},
editor = {Rachel Nadif},
booktitle = {Asthma in the 21st Century},
publisher = {Academic Press},
pages = {231-244},
year = {2023},
isbn = {978-0-323-85419-1},
doi = {https://doi.org/10.1016/B978-0-323-85419-1.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323854191000013},
author = {Jean Bousquet and Bernardo Sousa-Pinto and Francesca Puggioni and Aram Anto and Fabio Balli and Thomas Casale and Wienczyslawa Czarlewski and Anna Bedbrook and Luisa Brussino and Mina Gaga and Bilun Gemicioglu and Ludger Klimek and Violeta Kvedariene and Renaud Louis and Joaquin Sastre and Nicola Scichilone and Arunas Valiulis and Eleptherios Zervas and Arzu Yorgancioglu and Torsten Zuberbier and Josep M Anto and G Walter Canonica and Joao A Fonseca},
keywords = {App, Asthma, mHealth, Rhinitis},
abstract = {Digital health includes eHealth and areas such as the use of advanced computer sciences. eHealth comprises several components, including electronic health records, telehealth, and mHealth. mHealth is segmented into mHealth apps, mHealth services, and medical devices. The present paper is not intended to provide a systematic review of the current digital tools used in asthma but to provide a framework to better understand the potentials and drawbacks of some of these tools using, when available, systematic reviews.}
}
@incollection{AZEROUAL2021169,
title = {18 - Trustworthy or not? Research data on COVID-19 in data repositories},
editor = {David Baker and Lucy Ellis},
booktitle = {Libraries, Digital Information, and COVID},
publisher = {Chandos Publishing},
pages = {169-182},
year = {2021},
series = {Chandos Digital Information Review},
isbn = {978-0-323-88493-8},
doi = {https://doi.org/10.1016/B978-0-323-88493-8.00027-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884938000276},
author = {Otmane Azeroual and Joachim Schöpfel},
keywords = {Research data, Data repository, Data quality, Trustworthiness, Open access, Open science, COVID-19},
abstract = {The outburst of the COVID-19 pandemic has boosted the need for seamless, unrestricted, fast, and free access to the latest research results on the virus, on its treatment, prevention, protocols, and so on. Open access to publications and research data, suddenly, became self-evident, not only for researchers in life and medical sciences but also for politicians, journalists, and society as a whole. At the same time, this sudden awareness triggered another debate on the quality and, moreover, the trustworthiness of this mass of information made available most often without any form of quality control (peer review). Thousands of datasets from research on COVID-19 and related topics have already been deposited on data repositories. Our chapter discusses the issue of the quality and trustworthiness of research data in data repositories using examples from the ongoing pandemic. It offers insights into some fundamental concepts and summarizes recommendations for quality assurance and evaluation of research data.}
}
@article{LINKE2021579,
title = {Shared and Anxiety-Specific Pediatric Psychopathology Dimensions Manifest Distributed Neural Correlates},
journal = {Biological Psychiatry},
volume = {89},
number = {6},
pages = {579-587},
year = {2021},
note = {Early Developmental Antecedents of Mood Disorders},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2020.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S0006322320320436},
author = {Julia O. Linke and Rany Abend and Katharina Kircanski and Michal Clayton and Caitlin Stavish and Brenda E. Benson and Melissa A. Brotman and Olivier Renaud and Stephen M. Smith and Thomas E. Nichols and Ellen Leibenluft and Anderson M. Winkler and Daniel S. Pine},
keywords = {Anxiety, Disruptive behavior, Intrinsic brain connectivity, Irritability, Joint canonical correlation and independent component analysis, Youth},
abstract = {Background
Imaging research has not yet delivered reliable psychiatric biomarkers. One challenge, particularly among youth, is high comorbidity. This challenge might be met through canonical correlation analysis designed to model mutual dependencies between symptom dimensions and neural measures. We mapped the multivariate associations that intrinsic functional connectivity manifests with pediatric symptoms of anxiety, irritability, and attention-deficit/hyperactivity disorder (ADHD) as common, impactful, co-occurring problems. We evaluate the replicability of such latent dimensions in an independent sample.
Methods
We obtained ratings of anxiety, irritability, and ADHD, and 10 minutes of resting-state functional magnetic resonance imaging data, from two independent cohorts. Both cohorts (discovery: n = 182; replication: n = 326) included treatment-seeking youth with anxiety disorders, with disruptive mood dysregulation disorder, with ADHD, or without psychopathology. Functional connectivity was modeled as partial correlations among 216 brain areas. Using canonical correlation analysis and independent component analysis jointly we sought maximally correlated, maximally interpretable latent dimensions of brain connectivity and clinical symptoms.
Results
We identified seven canonical variates in the discovery and five in the replication cohort. Of these canonical variates, three exhibited similarities across datasets: two variates consistently captured shared aspects of irritability, ADHD, and anxiety, while the third was specific to anxiety. Across cohorts, canonical variates did not relate to specific resting-state networks but comprised edges interconnecting established networks within and across both hemispheres.
Conclusions
Findings revealed two replicable types of clinical variates, one related to multiple symptom dimensions and a second relatively specific to anxiety. Both types involved a multitude of broadly distributed, weak brain connections as opposed to strong connections encompassing known resting-state networks.}
}
@article{PENZEL2021619,
title = {New Paths in Respiratory Sleep Medicine: Consumer Devices, e-Health, and Digital Health Measurements},
journal = {Sleep Medicine Clinics},
volume = {16},
number = {4},
pages = {619-634},
year = {2021},
note = {Measuring Sleep},
issn = {1556-407X},
doi = {https://doi.org/10.1016/j.jsmc.2021.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1556407X2100062X},
author = {Thomas Penzel and Sarah Dietz-Terjung and Holger Woehrle and Christoph Schöbel},
keywords = {E-health, Out-of-center testing, Health apps, Longtime monitoring, Diagnostics, Sleep-disordered breathing}
}
@article{GEORGIOU2021111089,
title = {An empirical study of COVID-19 related posts on Stack Overflow: Topics and technologies},
journal = {Journal of Systems and Software},
volume = {182},
pages = {111089},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111089},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001862},
author = {Konstantinos Georgiou and Nikolaos Mittas and Alexandros Chatzigeorgiou and Lefteris Angelis},
keywords = {COVID-19, Pandemic, StackOverflow, Knowledge-sharing},
abstract = {The COVID-19 outbreak, also known as the coronavirus pandemic, has left its mark on every aspect of our lives and at the time of this writing is still an ongoing battle. Beyond the immediate global-wide health response, the pandemic has triggered a significant number of IT initiatives to track, visualize, analyze and potentially mitigate the phenomenon. For individuals or organizations interested in developing COVID-19 related software, knowledge-sharing communities such as Stack Overflow proved to be an effective source of information for tackling commonly encountered problems. As an additional contribution to the investigation of this unprecedented health crisis and to assess how fast and how well the community of developers has responded, we performed a study on COVID-19 related posts in Stack Overflow. In particular, we profiled relevant questions based on key post features and their evolution, identified the most prominent technologies adopted for developing COVID-19 software and their interrelations and focused on the most persevering problems faced by developers. For the analysis of posts we employed descriptive statistics, Association Rule Graphs, Survival Analysis and Latent Dirichlet Allocation. The results reveal that the response of the developers’ community to the pandemic was immediate and that the interest of developers on COVID-19 related challenges was sustained after its initial peak. In terms of the problems addressed, the results show a clear focus on COVID-19 data collection, analysis and visualization from/to the web, in line with the general needs for monitoring the pandemic.}
}
@article{ANTONOV202142,
title = {Façade deterioration prediction with the use of machine learning methods, based on objective parameters and e-participation data},
journal = {Procedia Computer Science},
volume = {193},
pages = {42-51},
year = {2021},
note = {10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020469},
author = {Aleksandr Antonov and Ivan Khodnenko and Sergei Kudinov},
keywords = {machine learning, logistic regression, predictive data, e-participation data, quality of urban environment, infrastructure deterioration},
abstract = {Condition monitoring and timely repair of residential buildings is an important task when ensuring a comfortable life in cities. In the case of large metropolitan areas, it is a difficult task to perform continuous objective condition monitoring for tens of thousands of residential buildings by efforts of experts. However, residential infrastructure health can be predicted on the basis of indirect data. These can be objective building parameters or subjective data on citizens’ complaints about deterioration. In cities today, it is possible to collect such data in machine-readable form from various information systems. This article proposes a method to predict external deterioration of buildings on the basis of indirect data, using machine learning and SMILE Low-coding platform. Based on the results of method approbation, which used data of a metropolis, the significance of electronic participation data and objective parameters of objects for façade deterioration forecast was assessed. Options for further research are proposed to improve the quality of deterioration predicting by using data on citizens’ complaints about infrastructure damage.}
}
@article{DONTHU2021102307,
title = {Forty years of the International Journal of Information Management: A bibliometric analysis},
journal = {International Journal of Information Management},
volume = {57},
pages = {102307},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102307},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220315061},
author = {Naveen Donthu and Satish Kumar and Nitesh Pandey and Prashant Gupta},
keywords = {Bibliometric analysis, International Journal of Information Management, Performance analysis, Science mapping, Negative binomial regression, Citation analysis},
abstract = {In 2019, the International Journal of Information Management (IJIM) celebrated its 40th year of publication. This study commemorates this event by presenting a retrospect of the journal. Using a range of bibliometric tools, we find that the journal has grown impressively in terms of publication and citation. The contributions come from all over the world, but the majority are from Europe and the United States. The journal has mostly published empirical articles, with its authors dominantly using quantitative methodology. Further, the culture of collaboration has increased among authors over the years. The journal publishes on a number of including managing information systems, information technologies and their application in business, technology acceptance among consumers, using information systems for decision making, social perspectives on knowledge management, and information research from the social science perspective. Regression analysis reveals that article attributes such as article order, methodology, presence of authors from Europe, number of references, number of keywords, and abstract length have a significant association with the citations. Finally, we find that conceptual and review articles have a positive association with citations.}
}
@article{SHAO2021102320,
title = {Exploring potential roles of academic libraries in undergraduate data science education curriculum development},
journal = {The Journal of Academic Librarianship},
volume = {47},
number = {2},
pages = {102320},
year = {2021},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2021.102320},
url = {https://www.sciencedirect.com/science/article/pii/S0099133321000112},
author = {Gang Shao and Jenny P. Quintana and Wei Zakharov and Senay Purzer and Eunhye Kim},
keywords = {Data science, Libraries, Curriculum, Education, Data management, College, Data ethics},
abstract = {Undergraduate data science education is receiving increasing interest in many higher education institutions in the U.S., with the proliferation of data and data related work and research. As an emerging interdisciplinary study field, data science curriculum is typically a collection of individual data science related courses from different schools and departments, most of which are teaching data science in a siloed fashion. Therefore, it is necessary to map the landscape of existing curricula and explore how academic libraries can collaborate and contribute to undergraduate data science education. In this study, we analyzed teaching content and topics of over 100 data science related courses at Purdue University to map the landscape and explore roles of academic libraries to support data science education curriculum. Our results indicate most existing courses focused on ‘hard-core’ scientific analytic principles, such as computer science, statistics, and domain-specific skills. Courses of data-oriented skills, such as data management, data ethics, and data communications were limited across disciplines. In addition, data science courses were more likely targeting STEM students at upper levels (3rd and 4th year students). Academic libraries can enrich data science education efforts, by supporting credit courses, certificate programs, and other co-curricular activities to provide learning opportunities to all students, particularly 1st and 2nd year students and non-STEM majors.}
}
@article{ULLAH2021120743,
title = {Risk management in sustainable smart cities governance: A TOE framework},
journal = {Technological Forecasting and Social Change},
volume = {167},
pages = {120743},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120743},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100175X},
author = {Fahim Ullah and Siddra Qayyum and Muhammad Jamaluddin Thaheem and Fadi Al-Turjman and Samad M.E. Sepasgozar},
keywords = {Smart city, Sustainability, Risk management, Smart city governance, Technology-organisation-environment (TOE) framework},
abstract = {Sustainable smart cities are confronted by technological, organisational and external risks, making their governance difficult and susceptible to manipulation. Based on a comprehensive literature review of 796 systematically retrieved articles, the current study proposes a multilayered technology-organisation-environment (TOE-based) risk management framework for sustainable smart city governance. A total of 56 risks are identified and grouped into TOE categories. There are 17 technological risks, including IoT networks, public internet management and user safety concerns, with a 38.7% contribution to smart city governance risks. With a 15.6% share, there are 11 organisational risks, including user data security and cloud management. There are 28 external risks with a contribution of 46.7% to the smart city governance and consist of smart city's environment, governance, integration and security risks. A multilayered TOE-based risk management framework is proposed to identify and manage the risks associated with smart city governance in the current study. The framework links smart citizens to each other through the smart city governance team and the integrated TOE layers. The iterative risk management process of identification, analysis, evaluation, monitoring and response planning is carried out in the TOE layers, both at the external layer levels and internal management levels. The proposed framework operationalises the risk management process for smart city governance by presenting the collection of pertinent risks and their thematic TOE categorisation. The criticality of the identified risks in line with the study's rankings can help researchers and practitioners understand the top risks of smart city governance. These risks present investment opportunities for city governance bodies to develop critical and effective responses as well as provide safety, security and enhanced privacy for citizens.}
}
@article{BOKADE2021113885,
title = {A cross-disciplinary comparison of multimodal data fusion approaches and applications: Accelerating learning through trans-disciplinary information sharing},
journal = {Expert Systems with Applications},
volume = {165},
pages = {113885},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113885},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420306886},
author = {Rohit Bokade and Alfred Navato and Ruilin Ouyang and Xiaoning Jin and Chun-An Chou and Sarah Ostadabbas and Amy V. Mueller},
keywords = {Multimodal data fusion, Machine learning, Complex systems, Big data, Trans-disciplinary},
abstract = {Multimodal data fusion (MMDF) is the process of combining disparate data streams (of different dimensionality, resolution, type, etc.) to generate information in a form that is more understandable or usable. Despite the explosion of data availability in recent decades, as yet there is no well-developed theoretical basis for multimodal data fusion, i.e., no way to determine a priori which approach is best suited to combine an arbitrary set of available data to achieve a stated goal for a given application. This has resulted in exploration of a wide variety of approaches across numerous domains but as yet very little integration of conclusions at a meta (cross-disciplinary) level. In response, this manuscript poses the following questions: (1) How convergent (or divergent) are approaches within single disciplines? (2) How similar are the challenges posed across different disciplines, i.e., might there be opportunity for successes in MMDF achieved in one field to inform progress in other areas as well? and (3) Where are the outstanding gaps in MMDF research, and what does this imply as targets for high impact research in the coming years? To begin to answer these questions, an apples-to-apples comparison of the literature of nine stakeholder-centric engineering domains (civil engineering, transportation, energy, environmental engineering, food engineering, critical care (healthcare), neuroscience, manufacturing/automation, and robotics) was created by quantifying the numbers and dimensionalities of modalities and sensors in each published project and classifying the algorithms used and purposes for which they are used. Within disciplines, it is shown there is often a tendency for use of similar methodologies, both in choice of level of fusion and data algorithm class. Yet this analysis also reveals that many problem types (defined by data dimensionality, modality number and type, and fusion purpose) are shared across different domains and are approached differently in those domains, e.g., transportation problems have similar characteristics to critical care, food science, robotics, and civil engineering. Of the disciplines studied, most (>75%) share problem characteristics with 3–5 others; to support leveraging these resources, lookup tables indexed by data dimensions, number of modalities, etc. are provided as a starting point for cross-disciplinary MMDF literature searches for new applications. Critical gaps identified are (1) a drop off of the number of published studies with increasing number of distinct modalities and (2) a dearth of publications tackling challenges with high dimensionality inputs, especially time-series 2D and 3D data. These gaps may point to topics where algorithm development will be fruitful to enable future solutions as video and other high-dimensionality sensors decrease in price. Finally, the lack of a shared vocabulary across disciplines makes analyses like the one conducted here challenging, as does the often implicit incorporation of expert knowledge into design; therefore progress toward a better leveraging of the current state of knowledge and toward a theoretical MMDF framework depends critically on improved cross-disciplinary communication and coordination on this topic.}
}
@article{OUNOUGHI2023267,
title = {Data fusion for ITS: A systematic literature review},
journal = {Information Fusion},
volume = {89},
pages = {267-291},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001087},
author = {Chahinez Ounoughi and Sadok {Ben Yahia}},
keywords = {Intelligent transportation system (ITS), Data fusion, Information fusion, Sensor fusion, Systematic literature review},
abstract = {In recent years, the development of intelligent transportation systems (ITS) has involved the input of various kinds of heterogeneous data in real time and from multiple sources, which presents several additional challenges. Studies on Data Fusion (DF) have delivered significant enhancements in ITS and demonstrated a substantial impact on its evolution. This paper introduces a systematic literature review on recent data fusion methods and extracts the main issues and challenges of using these techniques in intelligent transportation systems (ITS). It endeavors to identify and discuss the multi-sensor data sources and properties used for various traffic domains, including autonomous vehicles, detection models, driving assistance, traffic prediction, Vehicular communication, Localization, and management systems. Moreover, it attempts to associate abstractions of observation-level fusion, feature-level fusion, and decision-level fusion with different methods to better understand how DF is used in ITS applications. Consequently, the main objective of this paper is to review DF methods used for ITS studies to extract its trendy challenges. The review outcomes are (i) a description of the current Data fusion methods that adopt multi-sensor sources of heterogeneous data under different evaluation strategies, (ii) identifying several research gaps, current challenges, and new research trends.}
}