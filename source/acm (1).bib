@article{10.1145/3502771.3502781,
author = {Nguyen, Phu H. and Sen, Sagar and Jourdan, Nicolas and Cassoli, Beatriz and Myrseth, Per and Armendia, Mikel and Myklebust, Odd},
title = {Software Engineering and AI for Data Quality in Cyber- Physical Systems - SEA4DQ'21 Workshop Report},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3502771.3502781},
doi = {10.1145/3502771.3502781},
abstract = {Cyber-physical systems (CPS) have been developed in many industrial sectors and application domains in which the quality requirements of data acquired are a common factor. Data quality in CPS can deteriorate because of several factors such as sensor faults and failures due to operating in harsh and uncertain environments. How can software engineering and artificial intelligence (AI) help manage and tame data quality issues in CPS? This is the question we aimed to investigate in the SEA4DQ workshop. Emerging trends in software engineering need to take data quality management seriously as CPS are increasingly datacentric in their approach to acquiring and processing data along the edge-fog-cloud continuum. This workshop provided researchers and practitioners a forum for exchanging ideas, experiences, understanding of the problems, visions for the future, and promising solutions to the problems in data quality in CPS. Examples of topics include software/hardware architectures and frameworks for data quality management in CPS; software engineering and AI to detect anomalies in CPS data or to repair erroneous CPS data. SEA4DQ 2021, which took place on August 24th, 2021 was a satellite event of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC / FSE) 2021. The workshop attracted 35 international participants and was exciting with a great keynote, six excellent presentations, and concluded on a high note with a panel discussion. SEA4DQ was motivated by the common research interests from the EU projects for Zero-Defects Manufacturing such as InterQ and Dat4.Zero.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {26–29},
numpages = {4}
}

@article{10.1145/2935694.2935702,
author = {Wang, Hongzhi and Li, Mingda and Bu, Yingyi and Li, Jianzhong and Gao, Hong and Zhang, Jiacheng},
title = {Cleanix: A Parallel Big Data Cleaning System},
year = {2016},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2935694.2935702},
doi = {10.1145/2935694.2935702},
abstract = {For big data, data quality problem is more serious. Big data cleaning system requires scalability and the abilityof handling mixed errors. Motivated by this, we develop Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. In this paper, we show the organization, data cleaning algorithms as well as the design of Cleanix.},
journal = {SIGMOD Rec.},
month = {may},
pages = {35–40},
numpages = {6}
}

@inproceedings{10.1145/2627770.2627776,
author = {Smith, Ken and Seligman, Len and Rosenthal, Arnon and Kurcz, Chris and Greer, Mary and Macheret, Catherine and Sexton, Michael and Eckstein, Adric},
title = {"Big Metadata": The Need for Principled Metadata Management in Big Data Ecosystems},
year = {2014},
isbn = {9781450329972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627770.2627776},
doi = {10.1145/2627770.2627776},
abstract = {Current big data ecosystems lack a principled approach to metadata management. This impedes large organizations' ability to share data and data preparation and analysis code, to integrate data, and to ensure that analytic code makes compatible assumptions with the data it uses. This use-case paper describes the challenges and an in-progress effort to address them. We present a real application example, discuss requirements for "big metadata" drawn from that example as well as other U.S. government analytic applications, and briefly describe an effort to adapt an existing open source metadata manager to support the needs of big data ecosystems.},
booktitle = {Proceedings of Workshop on Data Analytics in the Cloud},
pages = {1–4},
numpages = {4},
keywords = {Big data analytics, data discovery, metadata, data integration},
location = {Snowbird, UT, USA},
series = {DanaC'14}
}

@article{10.1145/3005395,
author = {Bizer, Christian and Dong, Luna and Ilyas, Ihab and Vidal, Maria-Esther},
title = {Editorial: Special Issue on Web Data Quality},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3005395},
doi = {10.1145/3005395},
journal = {J. Data and Information Quality},
month = {nov},
articleno = {1},
numpages = {3}
}

@inproceedings{10.1145/2723372.2747646,
author = {Khayyat, Zuhair and Ilyas, Ihab F. and Jindal, Alekh and Madden, Samuel and Ouzzani, Mourad and Papotti, Paolo and Quian\'{e}-Ruiz, Jorge-Arnulfo and Tang, Nan and Yin, Si},
title = {BigDansing: A System for Big Data Cleansing},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2747646},
doi = {10.1145/2723372.2747646},
abstract = {Data cleansing approaches have usually focused on detecting and fixing errors with little attention to scaling to big datasets. This presents a serious impediment since data cleansing often involves costly computations such as enumerating pairs of tuples, handling inequality joins, and dealing with user-defined functions. In this paper, we present BigDansing, a Big Data Cleansing system to tackle efficiency, scalability, and ease-of-use issues in data cleansing. The system can run on top of most common general purpose data processing platforms, ranging from DBMSs to MapReduce-like frameworks. A user-friendly programming interface allows users to express data quality rules both declaratively and procedurally, with no requirement of being aware of the underlying distributed platform. BigDansing takes these rules into a series of transformations that enable distributed computations and several optimizations, such as shared scans and specialized joins operators. Experimental results on both synthetic and real datasets show that BigDansing outperforms existing baseline systems up to more than two orders of magnitude without sacrificing the quality provided by the repair algorithms.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1215–1230},
numpages = {16},
keywords = {distributed data repair, distributed data cleansing, cleansing abstraction, schema constraints},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/3359115.3359119,
author = {Wu, Jian and Kim, Kunho and Giles, C. Lee},
title = {CiteSeerX: 20 Years of Service to Scholarly Big Data},
year = {2019},
isbn = {9781450371841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359115.3359119},
doi = {10.1145/3359115.3359119},
abstract = {We overview CiteSeerX, the pioneer digital library search engine, that has been serving academic communities for more than 20 years (first released in 1998), from three perspectives. The system perspective summarizes its architecture evolution in three phases over the past 20 years. The data perspective describes how CiteSeerX has created searchable scholarly big datasets and made them freely available for multiple purposes. In order to be scalable and effective, AI technologies are employed in all essential modules. To effectively train these models, a sufficient amount of data has been labeled, which can then be reused for training future models. Finally, we discuss the future of CiteSeerX. Our ongoing work is to make CiteSeerX more sustainable. To this end, we are working to ingest all open access scholarly papers, estimated to be 30-40 million. Part of the plan is to discover dataset mentions and metadata in scholarly articles and make them more accessible via search interfaces. Users will have more opportunities to explore and trace datasets that can be reused and discover other datasets for new research projects. We summarize what was learned to make a similar system more sustainable and useful.},
booktitle = {Proceedings of the Conference on Artificial Intelligence for Data Discovery and Reuse},
articleno = {1},
numpages = {4},
keywords = {scholarly big data, CiteSeerX, search engines, digital libraries},
location = {Pittsburgh, Pennsylvania},
series = {AIDR '19}
}

@article{10.14778/3503585.3503602,
author = {Sinthong, Phanwadee and Patel, Dhaval and Zhou, Nianjun and Shrivastava, Shrey and Iyengar, Arun and Bhamidipaty, Anuradha},
title = {DQDF: Data-Quality-Aware Dataframes},
year = {2022},
issue_date = {December 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3503585.3503602},
doi = {10.14778/3503585.3503602},
abstract = {Data quality assessment is an essential process of any data analysis process including machine learning. The process is time-consuming as it involves multiple independent data quality checks that are performed iteratively at scale on evolving data resulting from exploratory data analysis (EDA). Existing solutions that provide computational optimizations for data quality assessment often separate the data structure from its data quality which then requires efforts from users to explicitly maintain state-like information. They demand a certain level of distributed system knowledge to ensure high-level pipeline optimizations from data analysts who should instead be focusing on analyzing the data. We, therefore, propose data-quality-aware dataframes, a data quality management system embedded as part of a data analyst's familiar data structure, such as a Python dataframe. The framework automatically detects changes in datasets' metadata and exploits the context of each of the quality checks to provide efficient data quality assessment on ever-changing data. We demonstrate in our experiment that our approach can reduce the overall data quality evaluation runtime by 40-80% in both local and distributed setups with less than 10% increase in memory usage.},
journal = {Proc. VLDB Endow.},
month = {apr},
pages = {949–957},
numpages = {9}
}

@inproceedings{10.1145/3495018.3501149,
author = {Tang, Yan},
title = {Computer Assisted Design and Practice of GNSS Survey Course of Typical Tasks through Cloud Computing and Big Data Technology},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3501149},
doi = {10.1145/3495018.3501149},
abstract = {Under the background of typical task orientation in higher vocational education, it is an inevitable trend to carry out teaching reform of GNSS Survey for engineering survey major in higher vocational education. This paper breaks the traditional theoretical teaching system, reconstructs it from the aspects of teaching content selection, combination of theory and practice teaching system, considers the actual situation of students' weak theoretical foundation in higher vocational colleges, and the core technical ability needed for employment and post, optimizes the curriculum structure, integrates the theoretical knowledge system and practice teaching links, and meets the requirements of project-oriented curriculum standards.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {2605–2608},
numpages = {4},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.5555/3400397.3400442,
author = {Vieira, Ant\'{o}nio A. C. and Dias, Lu\'{\i}s M. S. and Santos, Maribel Y. and Pereira, Guilherme A. B. and Oliveira, Jos\'{e} A.},
title = {Real-Time Supply Chain Simulation: A Big Data-Driven Approach},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Simulation of Supply Chains comprises huge amounts of data, resulting in numerous entities flowing in the model. These networks are highly dynamic systems, where entities' relationships and other elements evolve with time, paving the way for real-time Supply Chain decision-support tools capable of using real data. In light of this, a solution comprising of a Big Data Warehouse to store relevant data and a simulation model of an automotive plant, are being developed. The purpose of this paper is to address the modelling approach, which allowed the simulation model to automatically adapt to the data stored in a Big Data Warehouse and thus adapt to new scenarios without manual intervention. The main characteristics of the conceived solution were demonstrated, with emphasis to the real-time and the ability to allow the model to load the state of the system from the Big Data Warehouse.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {548–559},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/3291801.3291834,
author = {Deze, Wang},
title = {Application of Large Data Mining Technology in Colleges and Universities},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291834},
doi = {10.1145/3291801.3291834},
abstract = {Data mining is an advanced science and technology to process data and information. It can be extracted from a large number of complex data, or find some valuable data rules and models. Education has entered the era of big data. However, the way of data processing in university educational administration is relatively backward. Aiming at this problem, this paper applies data mining technology to college teaching management, extracts useful information from the data collected by educational administration management system, and provides correct and powerful data support and guarantee for college teaching managers to make relevant decisions.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {86–89},
numpages = {4},
keywords = {college student management, data mining, big data},
location = {Weihai, China},
series = {ICBDR 2018}
}

@article{10.1145/3174791,
author = {Geerts, Floris and Missier, Paolo and Paton, Norman},
title = {Editorial: Special Issue on Improving the Veracity and Value of Big Data},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3174791},
doi = {10.1145/3174791},
journal = {J. Data and Information Quality},
month = {mar},
articleno = {13},
numpages = {2}
}

@article{10.1145/3423321,
author = {Polese, Giuseppe and Deufemia, Vincenzo and Song, Shaoxu},
title = {Editorial: Special Issue on Metadata Discovery for Assessing Data Quality},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3423321},
doi = {10.1145/3423321},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {17},
numpages = {2}
}

@inproceedings{10.1145/3010089.3010095,
author = {Sindhu, C. S. and Hegde, Nagaratna P.},
title = {TAF: Temporal Analysis Framework for Handling Data Velocity in Healthcare Analytics},
year = {2016},
isbn = {9781450347792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3010089.3010095},
doi = {10.1145/3010089.3010095},
abstract = {We are inundated in a flood of data today. Data is being collected at a rapid scale from variety of sources like healthcare, e-commerce, social networking and so on. Decisions which were earlier made on assumptions can now be made on the data itself. It's a well known fact that volume, variety, velocity and veracity are the challenges associated in handling Big Data. The dynamic nature of the Internet and the velocity factor pose humongous challenges in retrieving patterns from the data. Coping up with noisy data which occurs at a rapid rate is still an open challenge. We have handled the issues associated with variety and veracity. After reviewing the existing system, it was found that there is no significant research model towards addressing data velocity problem exclusively taking case study of healthcare analytics.Hence, this paper presents a novel framework TAF or Temporal Analysis Framework that mainly targets at handling the incoming speed of data and redundancies in Healthcare Analytics. The proposed system uses real-time data analysis that significantly handles the data velocity along with retention of minimal error. The study outcome was assessed to find minimal algorithm complexities compared to any system that doesn't use this approach of self-adaptable real-time data analysis.},
booktitle = {Proceedings of the International Conference on Big Data and Advanced Wireless Technologies},
articleno = {10},
numpages = {11},
keywords = {Real-Time Analysis, Data Volume, Healthcare Analytics, Big Data, Medical Data},
location = {Blagoevgrad, Bulgaria},
series = {BDAW '16}
}

@inproceedings{10.1145/3386723.3387851,
author = {Tounsi, Youssef and Anoun, Houda and Hassouni, Larbi},
title = {CSMAS: Improving Multi-Agent Credit Scoring System by Integrating Big Data and the New Generation of Gradient Boosting Algorithms},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387851},
doi = {10.1145/3386723.3387851},
abstract = {Credit risk is one of the main risks facing banks and credit institutions, with the current progress in machine learning, artificial intelligence and big data. Recent research has proposed several systems for improving credit rating. In this paper, a new scalable credit scoring multi-agent system called "CSMAS" is introduced for the prediction of problems in data mining of credit scoring domain. This engine is built using a seven-layer multi-agent system architecture to generate a data mining process based on the coordination of intelligent agents. CSMAS performance is based on preprocessing and data forecasting. The first layer is designed to retrieve any data from various core banking systems, payment systems, credit Bureaus and external databases and data sources and to store it in big data platform. The second layer is devoted to three different subtasks; feature engineering, pre-processing data and integrating diverse datasets. While the third layer is dedicated to dealing with missing Values and treating outliers. In the fourth layer, the techniques of dimensionality reduction are used to reduce the number of features in the original set of features. The fifth layer is dedicated to build a model using the new generation of Gradient Boosting Algorithms (XGBoost, LightGBM and CatBoost) and make predictions. The sixth layer is designed for the model's evaluation. The seventh layer is made to perform the rating of new credit applicants. The performance of CSMAS is assessed using a large dataset of Home Credit Default Risk from Kaggle Challenge (307511 records) to evaluate the risk of a loan applicant as a major problem for banks. The results show that the CSMAS give relevant results. Therefore, the results indicated that the CSMAS can be further employed as a reliable tool to predict more complicated case in credit scoring.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {32},
numpages = {7},
keywords = {XgBoost, Credit Scoring, LightGBM, Big Data, Multi-Agent System, CatBoost},
location = {Marrakech, Morocco},
series = {NISS2020}
}

@article{10.14778/3137765.3137781,
author = {Zhang, Mingming and Wo, Tianyu and Xie, Tao and Lin, Xuelian and Liu, Yaxiao},
title = {CarStream: An Industrial System of Big Data Processing for Internet-of-Vehicles},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137781},
doi = {10.14778/3137765.3137781},
abstract = {As the Internet-of-Vehicles (IoV) technology becomes an increasingly important trend for future transportation, designing large-scale IoV systems has become a critical task that aims to process big data uploaded by fleet vehicles and to provide data-driven services. The IoV data, especially high-frequency vehicle statuses (e.g., location, engine parameters), are characterized as large volume with a low density of value and low data quality. Such characteristics pose challenges for developing real-time applications based on such data. In this paper, we address the challenges in designing a scalable IoV system by describing CarStream, an industrial system of big data processing for chauffeured car services. Connected with over 30,000 vehicles, CarStream collects and processes multiple types of driving data including vehicle status, driver activity, and passenger-trip information. Multiple services are provided based on the collected data. CarStream has been deployed and maintained for three years in industrial usage, collecting over 40 terabytes of driving data. This paper shares our experiences on designing CarStream based on large-scale driving-data streams, and the lessons learned from the process of addressing the challenges in designing and maintaining CarStream.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1766–1777},
numpages = {12}
}

@inproceedings{10.1145/3090354.3090358,
author = {Tikito, I. and Souissi, N.},
title = {Data Collect Requirements Model},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090358},
doi = {10.1145/3090354.3090358},
abstract = {In Big data era, managing data requires sufficient tools, last computer science evolution and developed methodologies. To be able to satisfy customer and the big need of information, multiple methods are developed to handle the complexity as well as the huge amount of data in different phases of data lifecycle. We notice for each complicated situation in data lifecycle we focus more particularly to develop storage or Analysis processes. For this reason in this paper, we try to have a different approach to resolve basic issues on targeting the first phase of data lifecycle, which is data collect. We present it as a System of systems, since the complexity of each phase of data lifecycle. In this research, we are interested by the collect system and particularly the process of Creation/Reception of data for which we model the requirements in order to manage smart data at the first level of the cycle. To build this model, we follow a methodology that required three major steps. Starting with requirement identification to defining criterion for each requirement, and in the last step will provide requirement modeling. This research highlight the importance of managing data collect to identify and restrict the issues of big data era.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {4},
numpages = {7},
keywords = {Collect system, Requirement Model, Creation/Reception Process, BPMN, Seven views, Data lifecycle, Big data, System of Systems},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3561801.3561814,
author = {Zhang, Cheng and Lin, Songbo and Zhang, Dang},
title = {A Data Cleaning Method for Industrial Data Flow Based on Multistage Combinational Optimization of Rule Set},
year = {2022},
isbn = {9781450390361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561801.3561814},
doi = {10.1145/3561801.3561814},
abstract = {With the development of the era of big data, the quality of data has become a growing concern of people. Improving data quality has become a very hot topic at present. In this paper, we propose a data cleaning method for industrial data flow based on multistage combinational optimization of rule set. According to the characteristics of the data, excellent cleaning algorithm is selected for the data. The data is evaluated and the cleaning rules are updated. In the first step, feature detection is carried out on the data, and high-quality data is selected as training samples to match the optimal data cleaning algorithm for them. In the second step, the model uses a random forest algorithm to learn the relationship between data features and data cleansing algorithms, and constructs multi-level filtering rules. In the third step, the data is cleansed and iterated to ensure that the rules are updated automatically. Finally, the model can automatically clean the data with a good cleaning effect. The results show that the method presented in this paper can achieve automatic cleaning effect on real industrial data sets, and the cleaning effect can reach 99% accuracy. This method effectively solves the problem of automatic data cleaning and can be used in the actual industrial data system.},
booktitle = {Proceedings of the 2022 5th International Conference on Big Data and Internet of Things},
pages = {77–81},
numpages = {5},
keywords = {Data cleaning. Big data. Industrial data flow. Cleaning model. Multi-dimensional},
location = {Chongqing, China},
series = {BDIOT '22}
}

@inproceedings{10.1145/3220199.3220206,
author = {Xie, Dingding and Li, Junyi and Yuan, Lening and Peng, Peng},
title = {Multi: Source Data Inconsistency Detection and Repair Based on CRC Algorithm},
year = {2018},
isbn = {9781450364263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220199.3220206},
doi = {10.1145/3220199.3220206},
abstract = {Most existing systems suffer from data quality problems. Data quality has been affected by many factors such as manual operation, software problems and hardware problems, especially data inconsistencies. As an important carrier of data, database system plays an important role in distributed systems. In order to reduce the impact of data inconsistency on data quality in distributed database systems, we design and implement a multi-source data inconsistency detection and repair method based on CRC algorithm.The idea of the proposed techniques is to use the rolling checksum in the rsync algorithm. In the process of data inconsistency detection, the method divides the table into chunks and calculates the checksums of data chunks in parallel for multiple data tables to detect and repair the inconsistent data. The experimental results show that the detection effect of this method is consistent with that of the traditional method which comparing source data with target data. The detection rate is as high as 99%, but it performs better than the traditional method, and the running time is reduced by about 20%.},
booktitle = {Proceedings of the 3rd International Conference on Big Data and Computing},
pages = {38–43},
numpages = {6},
keywords = {Data quality, Data inconsistency, CRC, Distributed database system},
location = {Shenzhen, China},
series = {ICBDC '18}
}

@article{10.14778/1687553.1687576,
author = {Cohen, Jeffrey and Dolan, Brian and Dunlap, Mark and Hellerstein, Joseph M. and Welton, Caleb},
title = {MAD Skills: New Analysis Practices for Big Data},
year = {2009},
issue_date = {August 2009},
publisher = {VLDB Endowment},
volume = {2},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/1687553.1687576},
doi = {10.14778/1687553.1687576},
abstract = {As massive data acquisition and storage becomes increasingly affordable, a wide variety of enterprises are employing statisticians to engage in sophisticated data analysis. In this paper we highlight the emerging practice of Magnetic, Agile, Deep (MAD) data analysis as a radical departure from traditional Enterprise Data Warehouses and Business Intelligence. We present our design philosophy, techniques and experience providing MAD analytics for one of the world's largest advertising networks at Fox Audience Network, using the Greenplum parallel database system. We describe database design methodologies that support the agile working style of analysts in these settings. We present dataparallel algorithms for sophisticated statistical techniques, with a focus on density methods. Finally, we reflect on database system features that enable agile design and flexible algorithm development using both SQL and MapReduce interfaces over a variety of storage mechanisms.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1481–1492},
numpages = {12}
}

@inproceedings{10.1145/2767109.2770014,
author = {Abiteboul, Serge and Dong, Luna and Etzioni, Oren and Srivastava, Divesh and Weikum, Gerhard and Stoyanovich, Julia and Suchanek, Fabian M.},
title = {The Elephant in the Room: Getting Value from Big Data},
year = {2015},
isbn = {9781450336277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2767109.2770014},
doi = {10.1145/2767109.2770014},
booktitle = {Proceedings of the 18th International Workshop on Web and Databases},
pages = {1–5},
numpages = {5},
location = {Melbourne, VIC, Australia},
series = {WebDB'15}
}

@techreport{10.5555/2582001,
author = {Marchionini, Gary and Lee, Christopher A. and Bowden, Heather and Lesk, Michael},
title = {Curating for Quality: Ensuring Data Quality to Enable New Science},
year = {2012},
publisher = {National Science Foundation},
address = {USA},
abstract = {Science is built on observations. If our observational data is bad, we are building a house on sand. Some of our data banks have quality measurements and maintenance, such as the National Climate Data Center and the National Center for Biotechnology Information; but others do not, and we do not even know which scientific data services have quality metrics or what they are.Data quality is an assertion about data properties, typically assumed within a context defined by a collection that holds the data. The assertion is made by the creator of the data. The collection context includes both metadata that describe provenance and representation information, and procedures that are able to parse and manipulate the data. However data quality from the perspective of users is defined based on the data properties that are required for use within their scientific research. The user believes data is of high quality when assertions about compliance can be shown to their research requirements.Digital data can accumulate rich contextual and derivative data as it is collected, analyzed, used, and reused, and planning for the management of this history requires new kinds of tools, techniques, standards, workflows, and attitudes. As science and industry recognize the need for digital curation, scientists and information professionals recognize that access and use of data depend on trust in the accuracy and veracity of data. In all data sets trust and reuse depend on accessible context and metadata that make explicit provenance, precision, and other traces of the datum and data life cycle. Poor data quality can be worse than missing data because it can waste resources and lead to faulty ideas and solutions, or at minimum challenges trust in the results and implications drawn from the data. Improvement in data quality can thus have significant benefits.}
}

@inproceedings{10.1145/3486011.3486555,
author = {Rocuts, Schweitzer and Alier, Marc},
title = {A Methodology Exploration to Motivate Teachers to Place Mathematics at the Center of a Transdisciplinary Experience Using Videogames and Big Data Combined with the 21st Century Skills},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486555},
doi = {10.1145/3486011.3486555},
abstract = {Human knowledge is highly connected and under continuous and collaborative evolution, where all of us can co-create and contribute. But the execution of our standardized educational systems mostly offers a standardized experience and teaching practices that do not reflect this point. This is particularly acute in teachers and students with mathematics. In this research, I will review some strategies to help teachers to effectively use connected mathematics with the real and daily world, promoting transdisciplinary work with an emphasis on creativity and collaboration, and using the Game-Based Assessment methodology combined with the unique environment surrounding each teacher.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {721–722},
numpages = {2},
keywords = {Connected mathematics, 21st Century Skills, creativity and collaborative work in education, transdisciplinary},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@article{10.1145/2629605,
author = {Rahm, Erhard},
title = {Discovering Product Counterfeits in Online Shops: A Big Data Integration Challenge},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1–2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2629605},
doi = {10.1145/2629605},
journal = {J. Data and Information Quality},
month = {sep},
articleno = {3},
numpages = {3}
}

@inproceedings{10.1145/3110025.3120958,
author = {Al-janabi, Samir and Hamid, Abubaker and Janicki, Ryszard},
title = {DatumPIPE: Data Generator and Corrupter for Multiple Data Quality Aspects},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3120958},
doi = {10.1145/3110025.3120958},
abstract = {Organizations use data to support different business processes. Data may become unclean because of corruptions in the central quality aspects due to factors such as duplicate records, outdated data, inconsistent values, incomplete information, or inaccurate values. Real datasets are usually not available for reasons such as privacy constraints. In the existing systems that generate or corrupt synthetic data, the intrinsic characteristics of data may not satisfy the quality aspects, and the injected types of errors do not corrupt multiple data quality aspects. Also, a lack of common datasets is a primary reason that representative comparisons between algorithms of different data quality management approaches are not possible. To address these issues, we present datumPIPE, a system that allows for the generation of data that satisfies a set of integrity constraints, including functional dependencies (FDs), conditional functional dependencies (CFDs), and inclusion dependencies (INDs). Also, datumPIPE provides the functionality to generate other types of attribute values such as sensors and personal data. It also allows for the corruption of the generated data through the introduction of quality issues in the central data quality aspects.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {589–592},
numpages = {4},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1145/3365109.3368778,
author = {Wang, Chun-Yu and Fuh, Shih-Hao and Lo, Ta-Chun and Cheng, Qi-Jun and Chen, Yu-Cheng and Cho, Feng-Min and Chang, Jyh-Biau and Shieh, Ce-Kuen},
title = {A Data Compacting Technique to Reduce the NetFlow Size in Botnet Detection with BotCluster},
year = {2019},
isbn = {9781450370165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365109.3368778},
doi = {10.1145/3365109.3368778},
abstract = {Big data analytics helps us to find potentially valuable knowledge, but as the size of the dataset increases, the computing cost also grows exponentially. In our previous work, BotCluster, we had designed a pre-processing filtering pipeline, including whitelist filter and flow loss-response rate (FLR) filter, for data reduction, which intended to wipe out irrelative noises and reduce the computing overhead. However, we still face a data redundancy phenomenon in which some of the same feature vectors repeatedly emerged. In this paper, we propose a data compacting approach aimed to reduce the input volume and keep enough representative feature vectors to fit DBSCAN's (Density-based spatial clustering of applications with noise) criteria. It purges the redundant vectors according to a purging threshold and keeps the primary representatives. Experimental results have shown that the average data reduction ratio is about 81.34%, while the precision has only slightly decreased by 1.6% on average, and the results still have 99.88% of IPs overlapped with the previous system.},
booktitle = {Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {81–84},
numpages = {4},
keywords = {p2p botnet, mapreduce framework, data compression, botnet, data reduction, netflow, big data, data compacting},
location = {Auckland, New Zealand},
series = {BDCAT '19}
}

@inproceedings{10.1145/3175684.3175719,
author = {Nguyen, Minh Chau and Won, Hee Sun},
title = {Advanced Multitenant Hadoop in Smart Open Data Platform},
year = {2017},
isbn = {9781450354301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3175684.3175719},
doi = {10.1145/3175684.3175719},
abstract = {Nowadays, there has been an immense amount of data coming from various devices sensors, social networks and IoT services. Among these data, open data is playing more and more important role in practice. Many individuals and organizations collect a broad range of different types of data in order to perform their analytic tasks. However, the current open data platforms still have many limitations. Among the drawbacks, data management, an important process of analytic service development, needs to be improved significantly. The main reason is that the emergence of massive data explosion coming from various sources has been making the process become more and more complicated and costly. Therefore, we propose here a system related to the field of data management to allow multitenant users to find and access easily their desired data as well as metadata. It also helps improve the performance of platform.},
booktitle = {Proceedings of the International Conference on Big Data and Internet of Thing},
pages = {48–51},
numpages = {4},
keywords = {Big data, Open data, Metadata, Advanced multitenant Hadoop, Data authorization},
location = {London, United Kingdom},
series = {BDIOT2017}
}

@article{10.1145/3464419,
author = {Zhang, Jie and Qu, Zhihao and Chen, Chenxi and Wang, Haozhao and Zhan, Yufeng and Ye, Baoliu and Guo, Song},
title = {Edge Learning: The Enabling Technology for Distributed Big Data Analytics in the Edge},
year = {2021},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3464419},
doi = {10.1145/3464419},
abstract = {Machine Learning (ML) has demonstrated great promise in various fields, e.g., self-driving, smart city, which are fundamentally altering the way individuals and organizations live, work, and interact. Traditional centralized learning frameworks require uploading all training data from different sources to a remote data server, which incurs significant communication overhead, service latency, and privacy issues.To further extend the frontiers of the learning paradigm, a new learning concept, namely, Edge Learning (EL) is emerging. It is complementary to the cloud-based methods for big data analytics by enabling distributed edge nodes to cooperatively training models and conduct inferences with their locally cached data. To explore the new characteristics and potential prospects of EL, we conduct a comprehensive survey of the recent research efforts on EL. Specifically, we first introduce the background and motivation. We then discuss the challenging issues in EL from the aspects of data, computation, and communication. Furthermore, we provide an overview of the enabling technologies for EL, including model training, inference, security guarantee, privacy protection, and incentive mechanism. Finally, we discuss future research opportunities on EL. We believe that this survey will provide a comprehensive overview of EL and stimulate fruitful future research in this field.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {151},
numpages = {36},
keywords = {edge computing, federated learning, Edge learning, security and privacy, machine learning}
}

@inproceedings{10.1145/3524458.3547246,
author = {Bottai, Carlo and Crosato, Lisa and Domenech, Josep and Guerzoni, Marco and Liberati, Caterina},
title = {Unconventional Data for Policy: Using Big Data for Detecting Italian Innovative SMEs},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524458.3547246},
doi = {10.1145/3524458.3547246},
abstract = {The paper explores the possibility to employ the source code of corporate websites as an information source for research in innovation studies. Research in this area is generally based on studies that collect data on patents or official data sources. Our paper links the standard economic information of the firm with web-based data and joins the ongoing debate with a threefold contribution. First, whereas the majority of the literature focused on the linguistic content of web-pages, we mostly use HTML tags. Second, we propose a method to assess the quality of the linkage of Web data to firm-level information. Third, we show that the data retrieved from corporate websites can aid to identify ‘innovative SMEs’.},
booktitle = {Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
pages = {338–344},
numpages = {7},
keywords = {HTML tags, Innovation, SMEs, Website data},
location = {Limassol, Cyprus},
series = {GoodIT '22}
}

@inproceedings{10.1145/2928294.2928306,
author = {Wu, Jian and Liang, Chen and Yang, Huaiyu and Giles, C. Lee},
title = {CiteSeerX Data: Semanticizing Scholarly Papers},
year = {2016},
isbn = {9781450342995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2928294.2928306},
doi = {10.1145/2928294.2928306},
abstract = {Scholarly big data is, for many, an important instance of Big Data. Digital library search engines have been built to acquire, extract, and ingest large volumes of scholarly papers. This paper provides an overview of the scholarly big data released by CiteSeerX, as of the end of 2015, and discusses various aspects such as how the data is acquired, its size, general quality, data management, and accessibility. Preliminary results on extracting semantic entities from body text of scholarly papers with Wikifier show biases towards general terms appearing in Wikipedia and against domain specific terms. We argue that the latter will play a more important role in extracting important facts from scholarly papers.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {2},
numpages = {6},
keywords = {semantic entity extraction, scholarly big data, citation graph, digital library search engine, CiteSeerX},
location = {San Francisco, California},
series = {SBD '16}
}

@inproceedings{10.1145/3277644.3277803,
author = {Filonik, Daniel and Bednarz, Tomasz},
title = {Visual Analytics of Big Networks: Novel Approaches for Exploring Complex Networks in Big Data},
year = {2018},
isbn = {9781450360265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277644.3277803},
doi = {10.1145/3277644.3277803},
abstract = {Four "Paradigms" of Science• Empirical Science• Theoretical Science• Computational Science• Data Science},
booktitle = {SIGGRAPH Asia 2018 Courses},
articleno = {18},
numpages = {96},
location = {Tokyo, Japan},
series = {SA '18}
}

@article{10.1145/3165713,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3165713},
doi = {10.1145/3165713},
abstract = {Although the ultimate objective of Linked Data is linking and integration, it is not currently evident how connected the current Linked Open Data (LOD) cloud is. In this article, we focus on methods, supported by special indexes and algorithms, for performing measurements related to the connectivity of more than two datasets that are useful in various tasks including (a) Dataset Discovery and Selection; (b) Object Coreference, i.e., for obtaining complete information about a set of entities, including provenance information; (c) Data Quality Assessment and Improvement, i.e., for assessing the connectivity between any set of datasets and monitoring their evolution over time, as well as for estimating data veracity; (d) Dataset Visualizations; and various other tasks. Since it would be prohibitively expensive to perform all these measurements in a na\"{\i}ve way, in this article, we introduce indexes (and their construction algorithms) that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and transitive closure of the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of URIs of any set of datasets. For enhancing scalability, we propose parallel index construction algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved speedup using either a single machine or a cluster of machines, and we provide insights regarding the factors that affect efficiency. Finally, we report measurements about the connectivity of the (billion triples-sized) LOD cloud that have never been carried out so far.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {15},
numpages = {49},
keywords = {connectivity, dataset selection, linked data, lattice of measurements, big data, spark, dataset discovery, mapreduce, Data quality}
}

@inproceedings{10.1145/3437075.3437087,
author = {Baijens, Jeroen and Helms, Remko and Kusters, Rob},
title = {Data Analytics Project Methodologies: Which One to Choose?},
year = {2021},
isbn = {9781450375061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437075.3437087},
doi = {10.1145/3437075.3437087},
abstract = {Developments in big data have led to an increase in data analytics projects conducted by organizations. Such projects aim to create value by improving decision making or enhancing business processes. However, many data analytics projects still fail to deliver the expected value. The use of process models or methodologies is recommended to increase the success rate of these projects. Nevertheless, organizations are hardly using them because they are considered too rigid and hard to implement. The existing methodologies often do not fit the specific project characteristics. Therefore, this research suggests grouping different project characteristics to identify the most appropriate project methodology for a specific type of project. More specifically, this research provides a structured description that helps to determine what type of project methodology works for different types of data analytics projects. The results of six different case studies show that continuous projects would benefit from an iterative methodology.},
booktitle = {Proceedings of the 2020 International Conference on Big Data in Management},
pages = {41–47},
numpages = {7},
keywords = {Project Methodologies, Project characteristics, Data Analytics},
location = {Manchester, United Kingdom},
series = {ICBDM 2020}
}

@inproceedings{10.1145/3538950.3538962,
author = {Zhou, Wei and Xue, Xiaorui and Luo, Danxue},
title = {Credit Card Fraud Detection Using Boundary Reconstruction and Integrated Classification},
year = {2022},
isbn = {9781450395632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538950.3538962},
doi = {10.1145/3538950.3538962},
abstract = {With the popularity of electronic payment, it not only brings great convenience, but also increases the risk of fraudulent transactions. At present, there are two problems in the identification of credit card fraud. The number of fraud and normal transactions is extremely unbalanced and the classification boundary is fuzzy. In order to solve these problems, this paper proposes an integrated classification framework for boundary reconstruction, which uses different machine learning algorithms as base learners to compare the original data with the data modeling after boundary reconstruction. The research shows that data boundary reconstruction can not only effectively alleviate the deviation caused by data imbalance to machine learning. It can also improve the data quality, so as to improve the accuracy of model classification; The integrated classification method can accurately identify credit card transactions, and the prediction effect of decision tree is the best. The proposed model is also applicable in other abnormal situations.},
booktitle = {Proceedings of the 4th International Conference on Big Data Engineering},
pages = {86–93},
numpages = {8},
keywords = {Credit card fraud detection, Integrated learning, Boundary reconstruction, Machine learning},
location = {Beijing, China},
series = {BDE '22}
}

@article{10.1145/3428154,
author = {Chirkova, Rada and Doyle, Jon and Reutter, Juan},
title = {Ensuring Data Readiness for Quality Requirements with Help from Procedure Reuse},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3428154},
doi = {10.1145/3428154},
abstract = {Assessing and improving the quality of data are fundamental challenges in Big-Data applications. These challenges have given rise to numerous solutions targeting transformation, integration, and cleaning of data. However, while schema design, data cleaning, and data migration are nowadays reasonably well understood in isolation, not much attention has been given to the interplay between standalone tools in these areas. In this article, we focus on the problem of determining whether the available data-transforming procedures can be used together to bring about the desired quality characteristics of the data in business or analytics processes. For example, to help an organization avoid building a data-quality solution from scratch when facing a new analytics task, we ask whether the data quality can be improved by reusing the tools that are already available, and if so, which tools to apply, and in which order, all without presuming knowledge of the internals of the tools, which may be external or proprietary.Toward addressing this problem, we conduct a formal study in which individual data cleaning, data migration, or other data-transforming tools are abstracted as black-box procedures with only some of the properties exposed, such as their applicability requirements, the parts of the data that the procedure modifies, and the conditions that the data satisfy once the procedure has been applied. As a proof of concept, we provide foundational results on sequential applications of procedures abstracted in this way, to achieve prespecified data-quality objectives, for the use case of relational data and for procedures described by standard relational constraints. We show that, while reasoning in this framework may be computationally infeasible in general, there exist well-behaved cases in which these foundational results can be applied in practice for achieving desired data-quality results on Big Data.},
journal = {J. Data and Information Quality},
month = {apr},
articleno = {15},
numpages = {15},
keywords = {frameworks, Data and information quality, Big Data quality management processes, data cleaning in Big Data, and models, data integration in Big Data, Big Data quality in business process, Big Data quality and analytics}
}

@inproceedings{10.1145/3523181.3523193,
author = {Yu, Jie and Li, Danning and Chen, Kai and Huang, Wei and Qin, Meiyuan and Qin, Xianjin},
title = {Research on Food Safety Data Sharing and Exchange Mechanism},
year = {2022},
isbn = {9781450387453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523181.3523193},
doi = {10.1145/3523181.3523193},
abstract = {With the development of the economy and the improvement of people's quality of life, the public demand for food taste has gradually changed to the demand for food safety. In order to better facilitate the government to strengthen food safety supervision and protect people's food safety, it is necessary for the government to realize information interaction with enterprises related to the food supply chain and ensure the traceability of food flowing into the market by exchanging and sharing the data of food safety information. With the rapid promotion and popularization of various mobile terminals in the Internet era, the data analysis technology based on artificial intelligence technology is more accurate, which makes the value contained in the data more and more important to people. At present, many fields need the opening and sharing of big data, but there is no reliable data sharing environment in the field of food supervision, and it is still difficult to ensure the traceability of data related to food safety. Blockchain has unique advantages of decentralization and distribution, which can help break the current obstacles of big data sharing and exchange and achieve a high degree of data sharing, interconnection and exchange. Based on the blockchain technology, this paper studies the food safety data sharing and exchange mechanism, combines the blockchain with the distributed file system, constructs the data connection model, stores the shared information on the blockchain, then introduces IPFs and zigzag coding, designs the corresponding control method, and establishes a reliable data sharing and exchange mechanism. The analysis shows that the data sharing and exchange mechanism proposed in this paper can meet the needs of food safety data sharing and exchange.},
booktitle = {2022 3rd Asia Service Sciences and Software Engineering Conference},
pages = {81–86},
numpages = {6},
location = {Macau, Macao},
series = {ASSE' 22}
}

@inproceedings{10.1145/3468920.3468942,
author = {Aljohani, Asmaa and Jones, James},
title = {Conducting Malicious Cybersecurity Experiments on Crowdsourcing Platforms},
year = {2021},
isbn = {9781450389426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468920.3468942},
doi = {10.1145/3468920.3468942},
abstract = {Evaluating the effectiveness of defense technologies mandates the inclusion of a human element, specifically if these technologies target human cognition and emotions. One of the biggest challenges that face researchers in the realm of behavioral cybersecurity is participant recruitment. Researchers often rely on college students, the general public, real-world hackers, or a hard-to-reach population (e.g., professional red teamers) to test the effectiveness of cybersecurity defense techniques. However, recruiting participants from these populations has drawbacks, including but not limited to: high cost, time constraints, and manageability and accessibility issues. This research explored the applicability of using two popular crowdsourcing platforms, Amazon Mechanical Turk and Prolific, to conduct web hacking experiments. Our study is the first to use crowdsourcing platforms to run hacking experiments for scientific purposes. While the recruitment is challenging, the paradigm of existing crowdsourcing platforms can be useful for understanding adversarial behavior, as it facilitates access to a diverse set of participants and allows researchers to conduct longitudinal and cross-cultural assessments. In particular, crowdsourcing platforms offer a great opportunity for cybersecurity researchers to investigate the Oppositional Human Factors (OHFs) in a manageable and flexible way.},
booktitle = {The 2021 3rd International Conference on Big Data Engineering},
pages = {150–161},
numpages = {12},
keywords = {Participants, Online experiments, Recruitment, Crowdsourcing, Cybersecurity},
location = {Shanghai, China},
series = {BDE 2021}
}

@inproceedings{10.1145/3335484.3335541,
author = {Liang, Yu and Duan, Xuliang and Ding, Yuanjun and Kou, Xifeng and Huang, Jingcheng},
title = {Data Mining of Students' Course Selection Based on Currency Rules and Decision Tree},
year = {2019},
isbn = {9781450362788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335484.3335541},
doi = {10.1145/3335484.3335541},
abstract = {The currency of data can ensure that data is not obsolete and outdated. As one of the important bases for evaluating data quality, it plays an important role in the availability of data. Data currency rules can effectively discriminate the temporal relationship between data sets. The decision tree can availably classify and predict the data, and can test the attribute values very well. In this paper, the currency rules are combined with the C4.5 algorithm in the decision tree, and the improved algorithm is applied to the college elective data in recent years. Through experiments, the algorithm used in this paper can extract the statute rules from the student elective database. According to the currency rules, the college teaching plan can be planned in advance and the curriculum resources can be allocated reasonably.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Computing},
pages = {247–252},
numpages = {6},
keywords = {course selection information, decision tree, currency rules, data mining},
location = {Guangzhou, China},
series = {ICBDC '19}
}

@inproceedings{10.1145/3545801.3545803,
author = {Liu, Junhong and Ren, Tianqi and Huang, Ziyue and Tu, Yan},
title = {Regional Water Resources Allocation Based on Fuzzy Data Mining},
year = {2022},
isbn = {9781450396097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545801.3545803},
doi = {10.1145/3545801.3545803},
abstract = {To focus on the problem of water shortage and the contradictory relationship between water supply and demand in China, the water supply case sets of each administrative region in Hubei Province were studied in order to provide reference for the policy formulation of regional water allocation. Based on the construction of a multi-dimensional data index system, this paper applied the fuzzy hierarchical clustering method to cluster and characterize the water distribution cases in administrative regions, and used the Xie_beni index to test the validity of the clustering results. The results show that water allocation in different administrative regions of Hubei province by industry is divided into four categories, showing different industry-oriented water allocation characteristics. By Xie_Beni clustering validity index test, the minimum value of Xie_Beni index is 12.26, and the relative error of irrigation area water demand prediction is less than 15%, which confirms the validity of the method. Xie_Beni index can better test the validity of clustering results. However, there are still some drawbacks. In the case of increasing the number of clusters, the Xie_beni index will gradually lose its judgment ability. Besides, the method requires high data quality of the research object, and requires each index to be quantifiable and rich historical data.},
booktitle = {Proceedings of the 7th International Conference on Big Data and Computing},
pages = {6–13},
numpages = {8},
keywords = {Fuzzy data mining, Correlation analysis, Water resources allocation, Xie_Beni index, Cluster analysis},
location = {Shenzhen, China},
series = {ICBDC '22}
}

@inproceedings{10.1145/3404687.3404703,
author = {Wu, Mingming},
title = {Multi-Task Representation Learning Network for Trajectory Recovery},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404703},
doi = {10.1145/3404687.3404703},
abstract = {Trajectory recovery can benefit many applications such as migration pattern studies of animal and finding hot routes in the urban city. It is necessary to recover trajectory with limited trajectory points to utilize collected trajectory data in a reasonable and efficient way and to provide the better location based service for users. However, the trajectory data involves complex and nonlinear spatial-temporal impacts which cannot be captured by traditional trajectory recovery methods. Moreover, the existing methods consider little about the correlations between trajectory and traffic pattern in the urban city. The superiority of deep neural network makes it possible to recover trajectory with low data quality. We propose a Multi-Task Representation Learning Network (MRL-Net) framework which models the complex nonlinear spatial-temporal correlations in trajectory data with representation learning technique and capture the dependencies of trajectory points with recurrent neural networks. To the best of our knowledge, it is the first paper to address the trajectory recovery problem with representation learning and multi-task learning. Experiments on real-world trajectory data show that our model is superior to state-of-the-art methods.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {79–83},
numpages = {5},
keywords = {Representation learning, Trajectory recovery, Multi-task learning},
location = {Chengdu, China},
series = {ICBDC '20}
}

@inproceedings{10.1145/3090354.3090382,
author = {El Bacha, Oussama and Jmad, Othmane and El Bouzekri El Idrissi, Younes and Hmina, Nabil},
title = {Exploiting Open Data to Improve the Business Intelligence &amp; Business Discovery Experience},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090382},
doi = {10.1145/3090354.3090382},
abstract = {The extent to which data mining tools are able to make efficient use of an open data oriented strategy in a smart city is limited. In a sense that it is not fully automated, incompatible or has to be supervised. These sets of tools may offer the possibility to import a dataset in a certain predefined standardized format, still, they do not make it a part of their workflow and algorithms in a fully unsupervised manner (i.e without ongoing human guidance). In a departure from previous research works, in this paper, we present a middleware architecture that exploits open data as background knowledge by acting as a bridge between data mining tools and open data resources.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {27},
numpages = {6},
keywords = {linked, smart city, knowledge discovery in databases, data mining, business intelligence, open data, middleware, business discovery},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@article{10.1145/3177873,
author = {Esteves, Diego and Rula, Anisa and Reddy, Aniketh Janardhan and Lehmann, Jens},
title = {Toward Veracity Assessment in RDF Knowledge Bases: An Exploratory Analysis},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3177873},
doi = {10.1145/3177873},
abstract = {Among different characteristics of knowledge bases, data quality is one of the most relevant to maximize the benefits of the provided information. Knowledge base quality assessment poses a number of big data challenges such as high volume, variety, velocity, and veracity. In this article, we focus on answering questions related to the assessment of the veracity of facts through Deep Fact Validation (DeFacto), a triple validation framework designed to assess facts in RDF knowledge bases. Despite current developments in the research area, the underlying framework faces many challenges. This article pinpoints and discusses these issues and conducts a thorough analysis of its pipeline, aiming at reducing the error propagation through its components. Furthermore, we discuss recent developments related to this fact validation as well as describing advantages and drawbacks of state-of-the-art models. As a result of this exploratory analysis, we give insights and directions toward a better architecture to tackle the complex task of fact-checking in knowledge bases.},
journal = {J. Data and Information Quality},
month = {feb},
articleno = {16},
numpages = {26},
keywords = {fact checking, trustworthiness, linked data, benchmark, DeFacto, exploratory data analysis, data quality}
}

@inproceedings{10.1145/3422713.3422718,
author = {Qi, Yajie and Guo, Chunwei},
title = {Deep Learning-Based Hourly Temperature Prediction: A Case Study of Mega-Cites in North China},
year = {2020},
isbn = {9781450387859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422713.3422718},
doi = {10.1145/3422713.3422718},
abstract = {Accurate prediction of temperature is an important part of fine weather forecast services (such as heating energy consumption in winter, Winter Olympic Games, etc.). Therefore, the accurate prediction of hourly temperature is very significant in the management of human health and the decision-making of government. In this study, a long short term (LSTM) memory model was proposed and used to predict the next hour's temperature in mega-cites in North China. It was fully considered for the historic temperature and meteorological condition. As a result, the predictor secured a fast and accurate prediction performance by fully reflecting the long-term historic process of input time series data through LSTM. The meteorological data from Beijing, Tianjin, Shijiazhuang and Taiyuan which represents the mega-cites of North China during October 1 to December 31 in 2016-2018 were used to verify the validity of the proposed method. In conclusion, the proposed method was proved to have a good prediction performance in cooling and turning warming processes, making up for the poor performance of turning weather prediction in the previous research. It confirmed that the forward supplement LSTM model has the best prediction ability for hourly temperature prediction in Beijing among mega-cites in North China. The results also indicate great potential of the machine learning method in improving local weather forecast and the potential to serve the 2022 Winter Olympics.},
booktitle = {Proceedings of the 2020 3rd International Conference on Big Data Technologies},
pages = {93–96},
numpages = {4},
keywords = {LSTM, Hourly temperature prediction, Deep learning, North China},
location = {Qingdao, China},
series = {ICBDT 2020}
}

@inproceedings{10.1145/3291801.3291826,
author = {Sun, Donglei and Zeng, Jun and Zhu, Yi and Cao, Xiangyang and Wang, Yiqun and Yang, Bo and Yang, Bin and Wang, Nan and Bo, Qibin and Fu, Yimu and Wei, Jia and Liu, Dong},
title = {Mechanism Design for Unified Management of Power Grid Planning Data},
year = {2018},
isbn = {9781450364768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291801.3291826},
doi = {10.1145/3291801.3291826},
abstract = {In order to deal with diversity of massive data structures and the variety of information formats, a novel mechanism is designed for unified management of power grid planning data. By integrating many business systems including production management system (PMS), geographic information system (GIS), energy management system (EMS), distribution network information acquisition system in the power supply company's system and adopting various technical means such as data warehouse technology (e.g. ETL, Extract-Transform-Load) and incremental capture, data structure that supports the whole process management of power grid business is designed, data correlation analysis and integration &amp; migration are carried out, and efficient access and deep fusion of massive relational data, file-type data, distributed data and spatial data are realized. Besides, through computing modes such as diagnostic analysis, load analysis and spatial analysis, the integrated database for power grid planning that integrates data fusion, storage, mining, modeling, computing, analysis and intelligent perception is finally constructed based on the designed data management mechanism, which could provide the comprehensive model and data support for power grid development. Field application shows the engineering benefit of the designed data management mechanism.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Research},
pages = {21–26},
numpages = {6},
keywords = {data fusion, data management, power grid planning, mechanism design},
location = {Weihai, China},
series = {ICBDR 2018}
}

@inproceedings{10.1145/3469968.3469992,
author = {Pan, Zhengjun and Zhao, Lianfen and Zhong, Xingyu and Xia, Zitong},
title = {Application of Collaborative Filtering Recommendation Algorithm in Internet Online Courses},
year = {2021},
isbn = {9781450389808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469968.3469992},
doi = {10.1145/3469968.3469992},
abstract = {Aiming at the problem that the overload of online education platform course resources leads to the difficulty of user selection, this paper mainly studies the improvement and application of collaborative filtering algorithm based on online course recommendation system, which organically combines personalized recommendation technology and online course system to meet the needs of users and online education platform. In the process of recommendation, firstly, user preferences are collected to establish a data model, and user login information and learning behavior information are used as implicit characteristics of user preferences. The loss rate of users in the computing platform is defined, the popularity of each course is calculated, and the relationship between users and courses is constructed, and the correlation and comparative analysis are carried out, Then, the traditional collaborative filtering algorithm is improved by introducing the implicit features after analysis, and the cosine similarity method is used to calculate the course similarity. Finally, the topN recommendation list is generated to get the recommendation results. Based on the desensitization data of an education platform, the experimental results show that the improved recommendation model can improve the precision of recommendation by introducing implicit features.},
booktitle = {Proceedings of the 6th International Conference on Big Data and Computing},
pages = {142–147},
numpages = {6},
keywords = {Collaborative filtering algorithm, education platform, recommendation system, online course},
location = {Shenzhen, China},
series = {ICBDC '21}
}

@inproceedings{10.1145/3322134.3322145,
author = {Al Fanah, Muna and Ansari, Muhammad Ayub},
title = {Understanding E-Learners' Behaviour Using Data Mining Techniques},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322145},
doi = {10.1145/3322134.3322145},
abstract = {The information from Higher Education Institutions (HEIs) is primarily relevant for decision maker and educators. This study tackles e-learners behaviour using machine learning, particularly association rules and classifiers. Learners are characterized by a set of behaviours and attitudes that determine their learning abilities and skills. Learning from data generated by online learners may have significant impacts, however, few studies cover this resource from machine learning perspectives. We examine different data mining techniques including Random Forests, Logistic Regressions and Bayesian Networks as classifiers used for predicting e-learners' classes (High, Medium and Low). The novelty of this study is that it explores and compares classifiers performance on the behaviour of online learners on four variables: raise hands, visiting IT resources, view announcement and discussion impact on e-learners. The results of this study indicate an 80% accuracy level obtained by Bayesian Networks; in contrast, the Random Forests have only 63% accuracy level and Logistic Regressions for 58%.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {59–65},
numpages = {7},
keywords = {Accuracy, Precision, Radom Forests, Association Rules, Bayesian Networks},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@inproceedings{10.1145/3090354.3090404,
author = {Ennajjar, Ibtissam and Tabii, Youness and Benkaddour, Abdelhamid},
title = {Securing Data in Cloud Computing by Classification},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090404},
doi = {10.1145/3090354.3090404},
abstract = {Cloud computing is a wide architecture based on diverse models for providing different services of software and hardware. Cloud computing paradigm attracts different users because of its several benefits such as high resource elasticity, expense reduction, scalability and simplicity which provide significant preserving in terms of investment and work force. However, the new approaches introduced by the cloud, related to computation outsourcing, distributed resources, multi-tenancy concept, high dynamism of the model, data warehousing and the nontransparent style of cloud increase the security and privacy concerns and makes building and handling trust among cloud service providers and consumers a critical security challenge. This paper proposes a new approach to improve security of data in cloud computing. It suggests a classification model to categorize data before being introduced into a suitable encryption system according to the category. Since data in cloud has not the same sensitivity level, encrypting it with the same algorithms can lead to a lack of security or of resources. By this method we try to optimize the resources consumption and the computation cost while ensuring data confidentiality.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {49},
numpages = {5},
keywords = {Data Security, Classification, Cloud Computing, Cloud Storage},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3207677.3278010,
author = {Zhang, Guilan and Wang, Jian and Zhou, Guomin and Liu, Jianping and Wei, Caoyuan},
title = {Scientific Data Relevance Criteria Classification and Usage},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3278010},
doi = {10.1145/3207677.3278010},
abstract = {In1 the big data era, scientific data plays a crucial role in scientific research. Data sharing, retrieval and usage has become an inevitable trend. We study how the users of scientific data select relevant data from the data sharing platform. The study was conducted in two stages. In the first stage, a total of 14 subjects were selected to obtain their relevance criteria and usage of scientific data through semi-structured interviews. In the second stage, 671 questionnaires were collected in order to classify criteria. Finally, we determined 9 relevance criteria for scientific data: topicality, availability, comprehensiveness, currency, authority, quality, convenience, standardization, and usability, and divided them to 5 groups. In order to truly make a better data search engine and improve its search efficiency, moving beyond the criteria often used by users, we need to determine those criteria that are not often used, but still very important. What's more, a more convenient data search platform needs to be considered.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {30},
numpages = {7},
keywords = {scientific data, relevance criteria, information carrier, Relevance},
location = {Hohhot, China},
series = {CSAE '18}
}

@inproceedings{10.1145/3372938.3372950,
author = {Chahidi, Hamza and Omara, Hicham and Lazaar, Mohamed and Al Achhab, Mohammed},
title = {Impact of Neural Network Architectures on Arabic Sentiment Analysis},
year = {2020},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3372950},
doi = {10.1145/3372938.3372950},
abstract = {Sentiment Analysis (SA), commonly known as opinion mining, during last couple of years, it becomes the fastest growing research areas in computer science. Conventionally, it helps to automatically detect if a text express is a positive, negative or neutral opinion. It enables us to identify and extract subjective information in a piece of writing, and this leads to gain an overview of wider public opinions or attitudes toward topics, products or services. Many researches have been done in this area, but most of them have focused on English and other Indo-European languages. Insufficient studies have actually accosted Sentiment Analysis in morphologically rich language such as Arabic. Regardless, given the increasing number of Arabic users and the exponential growth of online content, SA in this language has gained the attention of many researches last years, since Arabic raises many challenges because of its derivational, inflectional and agglutinative morphology. The objective of this paper is to promote the performance of Arabic Sentiment Analysis (ASA) by using Deep learning techniques. For that we implement Multi-Layer perceptron model in order to process and classify a dataset (Tweets). In fact, the experimental results prove that MLP as a deep learning model has a better performance for ASA than classical approaches.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {12},
numpages = {6},
keywords = {Sentiment Analysis, Arabic, Deep Learning, Machine Learning, Multi-layer Perceptron},
location = {Rabat, Morocco},
series = {BDIoT'19}
}

@inproceedings{10.1145/3372938.3372970,
author = {El Kafhali, Said and Chahir, Chorouk and Hanini, Mohamed and Salah, Khaled},
title = {Architecture to Manage Internet of Things Data Using Blockchain and Fog Computing},
year = {2020},
isbn = {9781450372404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372938.3372970},
doi = {10.1145/3372938.3372970},
abstract = {In this paper, we propose a novel architecture that utilizes features of Blockchain, fog computing, and cloud computing to manage IoT data. Blockchain allows to have a distributed peer-to-peer network in which non-trusting participants can interact with each other without a trusted intermediary or third party. We evaluate how this mechanism works to face the challenges of IoT with respect to multiple accessibility to IoT devises. We consider a Blockchain architecture in presence of edge computing layer. With fog or fog computing, the sensitive data can be analyzed locally instead of sending it to the cloud for analysis. Edge nodes can also keep track and control of the IoT devices that collect, analyze and store data. We show that this control can be better executed when Software Defined Network (SDN) and Network Functions Virtualization (NFV) are integrated into our process for optimal resource management. In this paper, we present our system architecture with a detailed description of the different interactions. We remark that the integration of Blockchain, IoT, and edge computing when coupled with SDN and NFV-enabled cloud infrastructure can bring to more superior and efficient platform for accessing, managing, and processing the huge influx of IoT data.},
booktitle = {Proceedings of the 4th International Conference on Big Data and Internet of Things},
articleno = {32},
numpages = {8},
keywords = {SDN, NFV, Internet of Things, Fog computing, Smart Contracts, Edge Computing, Blockchain},
location = {Rabat, Morocco},
series = {BDIoT'19}
}

@inproceedings{10.1145/3152723.3152730,
author = {Chaofan, Dai and Ran, Zhang and Pei, Li and Wenqian, Wang},
title = {Design of ETL Provenance Tool Based on Minimal Attribute Set},
year = {2017},
isbn = {9781450353564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152723.3152730},
doi = {10.1145/3152723.3152730},
abstract = {For the ETL process, this paper designs a provenance tool based on inversible transformation, and describes the meta-information of ETL and data provenance process in two ways: one is to take the database two-dimensional table to describe the relevant information in logical level, easy to record; the other is the use of PROV model information on the xml description, and shows the ETL and the provenance process in the directed acyclic graph, easy to understand.},
booktitle = {Proceedings of the 2017 International Conference on Big Data Research},
pages = {57–61},
numpages = {5},
keywords = {minimal attribute set, PROV, data provenance, Metadata, ETL},
location = {Osaka, Japan},
series = {ICBDR 2017}
}

@inproceedings{10.1145/3226116.3226135,
author = {Huang, Huijun and Zheng, Jiguang},
title = {Quality Earned Value Analysis Based on IFPUG Method in Software Project},
year = {2018},
isbn = {9781450364270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3226116.3226135},
doi = {10.1145/3226116.3226135},
abstract = {Earned value method is an important tool to evaluate and control the schedule and cost of the project. It is widely used in engineering construction projects, but rarely used in software projects. Due to the characteristics of the software project, the accuracy of the calculation of the basic parameters of the earned value analysis is low, which leads to the fact that the credibility of the result of the earned value analysis becomes very low or even meaningless. In order to make Earned value method applied to software projects better , IFPUG function points method is used to measure the actual completion of software projects, then used earned value method on the basis of IFPUG function point method. This can improved the accuracy of earned value analysis better. In order to analyze the cost and schedule of software projects better, the quality factors of software are also taken into account in the analysis of earned value, this can monitor the cost and schedule of software projects accurately.},
booktitle = {Proceedings of 2018 International Conference on Big Data Technologies},
pages = {101–108},
numpages = {8},
keywords = {function point method, earned value analysis, software quality, software project},
location = {Hangzhou, China},
series = {ICBDT '18}
}

@article{10.1145/3287168,
author = {Srivastava, Divesh and Scannapieco, Monica and Redman, Thomas C.},
title = {Ensuring High-Quality Private Data for Responsible Data Science: Vision and Challenges},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3287168},
doi = {10.1145/3287168},
abstract = {High-quality data is critical for effective data science. As the use of data science has grown, so too have concerns that individuals’ rights to privacy will be violated. This has led to the development of data protection regulations around the globe and the use of sophisticated anonymization techniques to protect privacy. Such measures make it more challenging for the data scientist to understand the data, exacerbating issues of data quality. Responsible data science aims to develop useful insights from the data while fully embracing these considerations.We pose the high-level problem in this article, “How can a data scientist develop the needed trust that private data has high quality?” We then identify a series of challenges for various data-centric communities and outline research questions for data quality and privacy researchers, which would need to be addressed to effectively answer the problem posed in this article.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {1},
numpages = {9},
keywords = {quality of private data, private data, Responsible data science, data trust}
}

@inproceedings{10.1145/3545801.3545816,
author = {Bei, Yijun and Gao, Kewei and Wang, Linxin},
title = {Supervised Information Extraction of Chinese Equipment Maintenance Documents},
year = {2022},
isbn = {9781450396097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545801.3545816},
doi = {10.1145/3545801.3545816},
abstract = {The service data of equipment maintenance is scattered, complex and uncorrelated, and depends on the experience of experts. This paper focuses on the knowledge extraction technology of equipment maintenance documents. However, equipment maintenance documents are characterized by no obvious boundary symbols, many professional terms, and rich context information. Therefore, Convolution Neural Network - Bi-directional Long Short-Term Memory - Conditional Random Field(CNN-BiLSTM-CRF) entity extraction model is designed in this paper on the basis of Bi-directional Long Short-Term Memory – Conditional Random Field(BiLSTM-CRF) model. Aiming at the normative and implicit characteristics of equipment documents, this paper designed a relationship extraction method based on the fusion of pattern and CNN. Experimental results show that both models have good results.},
booktitle = {Proceedings of the 7th International Conference on Big Data and Computing},
pages = {102–108},
numpages = {7},
keywords = {Knowledge Graph, Relation Extraction, Entity Extraction, Equipment Maintenance, Knowledge Extraction},
location = {Shenzhen, China},
series = {ICBDC '22}
}

@inproceedings{10.1145/3437075.3437091,
author = {Majthoub, Manar and Odeh, Yousra and Hijjawi, Mohammed},
title = {Non-Functional Requirements Classification for Aligning Business with Information Systems},
year = {2021},
isbn = {9781450375061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437075.3437091},
doi = {10.1145/3437075.3437091},
abstract = {Non-Functional Requirements (NFR) are defined as the desired quality requirements, such as availability, that restrict software product being developed where some external restrictions may apply. Since information systems have been introduced, organizations in the business world align their functional activities with systems without paying attention to quality-based alignment. Few research works have been conducted in order to classify and integrate the NFR with business or system models. But these classifications and integrations are only confined to either the business side or the system side, which in turn have caused in having a gap in mapping the classifications between the two sides. Because business models and system models mutually affect each other in many ways, their NFR integration and classification should be aligned with each other. Having a NFR alignment-based classification between business and information systems contributes to assist the stakeholders in reflecting the quality requirements at the business side for a particular task on the related tasks integrated with NFRs at the systems side. Also having an alignment-oriented classification contributes to trace quality/NFR-based changes from the business organization to its systems and vice versa.In this research, we propose a NFR classification for aligning quality requirements in business with their NFRs in information systems. The work in business side is represented through business process models designed using Business Process Model and Notation (BPMN) where the use case models represents the system side in this research. The proposed classification is demonstrated in both business and systems using the academic advising and registration case study at Applied Science University in Jordan.},
booktitle = {Proceedings of the 2020 International Conference on Big Data in Management},
pages = {84–89},
numpages = {6},
keywords = {System Model, Use Case, Business/IT Alignment, Quality Requirements, Non-Functional Requirements, Business Models, Business Process Model},
location = {Manchester, United Kingdom},
series = {ICBDM 2020}
}

@inproceedings{10.1145/3459637.3481905,
author = {Bian, Shuqing and Zhao, Wayne Xin and Zhou, Kun and Cai, Jing and He, Yancheng and Yin, Cunxiang and Wen, Ji-Rong},
title = {Contrastive Curriculum Learning for Sequential User Behavior Modeling via Data Augmentation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481905},
doi = {10.1145/3459637.3481905},
abstract = {Within online platforms, it is critical to capture the semantics of sequential user behaviors for accurately modeling user interests. However, dynamic characteristics and sparse behaviors make it difficult to train effective user representations for sequential user behavior modeling.Inspired by the recent progress in contrastive learning, we propose a novel Contrastive Curriculum Learning framework for producing effective representations for modeling sequential user behaviors. We make important technical contributions in two aspects, namely data quality and sample ordering. Firstly, we design a model-based data generator by generating high-quality samples confirming to users' attribute information. Given a target user, it can leverage the fused attribute semantics for generating more close-to-real sequences. Secondly, we propose a curriculum learning strategy to conduct contrastive learning via an easy-to-difficult learning process. The core component is a learnable difficulty evaluator, which can score augmented sequences, and schedule them in curriculums. Extensive results on both public and industry datasets demonstrate the effectiveness of our approach on downstream tasks.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3737–3746},
numpages = {10},
keywords = {user behavior modeling, contrastive curriculum learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3322134.3322155,
author = {Polpinij, Jantima and Namee, Khanista},
title = {Internet Usage Patterns Mining from Firewall Event Logs},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322155},
doi = {10.1145/3322134.3322155},
abstract = {Understanding users' behavior of internet usage is essential for the quality of service (QoS) analysis on the internet. If the internet providers can better understand their users, they may be able to provide better service, and also enhance the quality of the service. In general, the information about users' behavior is stored as the internet access log files, called event logs, on the server. To have the patterns of users' behavior from the event logs, this work aims to extract an interesting pattern of inappropriate user behaviors through the method of internet usage patterns mining. The primary mechanism of the proposed method is the Generalized Sequential Pattern (GSP) algorithm, which is an algorithm of sequential pattern mining. This study uses real event logs from an organization in Thailand. The results have identified exciting findings that have made possible to propose some improvements and increasing the QoS of the internet service.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {93–97},
numpages = {5},
keywords = {Event logs, Data mining, Inappropriate user pattern, Generalized Sequential Pattern, Sequential pattern mining, Internet usage},
location = {London, United Kingdom},
series = {ICBDE'19}
}

@inproceedings{10.1145/3513142.3513235,
author = {Zhang, Le and Ren, Junda and Yang, Zhi and Yin, Zenan and Chen, Yiting and Gu, Yiming},
title = {Analysis of The Advancement of Rpa Technology and Its Application in the Financial Field of Electric Power Enterprises},
year = {2022},
isbn = {9781450386494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3513142.3513235},
doi = {10.1145/3513142.3513235},
abstract = {Under the background of the new technology era of cloud, big things, mobile intelligence, RPA (RoboticsProcessAutomation) technology, as an important and mature application in the field of artificial intelligence, can help financial personnel to free themselves from a large number of simple and complex transactional work and invest in Financial analysis, scientific decision-making and other high value-added work. At present, financial robot products based on RPA technology can be extended to be compatible with OCR, voice, intelligent customer service, deep learning and other functions, supporting the establishment of risk management and control systems and intelligent application scenarios, and ultimately improve the cross-business collaboration capabilities and operation automation efficiency of financial management. Effectively control financial risks, improve the efficiency of data asset use and financial analysis and decision-making capabilities, and provide power companies with good management and economic benefits. This article first analyzes the advantages and technical characteristics of RPA technology, then summarizes the practical application of financial robotics technology in power companies, explores the role of RPA technology in financial digital transformation, and studies its risk management and control models, which are of great significance to improving the comprehensive management level of power grid companies.},
booktitle = {The 4th International Conference on Information Technologies and Electrical Engineering},
articleno = {91},
numpages = {5},
keywords = {financial robot, electric power enterprise, RPA, intelligence, digitization},
location = {Changde, Hunan, China},
series = {ICITEE2021}
}

@inproceedings{10.1145/2663715.2669610,
author = {Silva, M\'{a}rio J. and Rijo, Pedro and Francisco, Alexandre},
title = {Evaluating the Impact of Anonymization on Large Interaction Network Datasets},
year = {2014},
isbn = {9781450315838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663715.2669610},
doi = {10.1145/2663715.2669610},
abstract = {We address the publication of a large academic information dataset addressing privacy issues. We evaluate anonymization techniques achieving the intended protection, while retaining the utility of the anonymized data. The released data could help infer behaviors and subsequently find solutions for daily planning activities, such as cafeteria attendance, cleaning schedules or student performance, or study interaction patterns among an academic population. However, the nature of the academic data is such that many implicit social interaction networks can be derived from the anonymized datasets, raising the need for researching how anonymity can be assessed in this setting.},
booktitle = {Proceedings of the First International Workshop on Privacy and Secuirty of Big Data},
pages = {3–10},
numpages = {8},
keywords = {privacy of big data, academic data publishing, privacy-preserving data publishing, interaction network inference},
location = {Shanghai, China},
series = {PSBD '14}
}

@article{10.14778/2824032.2824136,
author = {Gao, Jing and Li, Qi and Zhao, Bo and Fan, Wei and Han, Jiawei},
title = {Truth Discovery and Crowdsourcing Aggregation: A Unified Perspective},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824136},
doi = {10.14778/2824032.2824136},
abstract = {In the era of Big Data, data entries, even describing the same objects or events, can come from a variety of sources, where a data source can be a web page, a database or a person. Consequently, conflicts among sources become inevitable. To resolve the conflicts and achieve high quality data, truth discovery and crowdsourcing aggregation have been studied intensively. However, although these two topics have a lot in common, they are studied separately and are applied to different domains. To answer the need of a systematic introduction and comparison of the two topics, we present an organized picture on truth discovery and crowdsourcing aggregation in this tutorial. They are compared on both theory and application levels, and their related areas as well as open questions are discussed.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {2048–2049},
numpages = {2}
}

@article{10.14778/3229863.3236224,
author = {Li, Huan and Lu, Hua and Shi, Feichao and Chen, Gang and Chen, Ke and Shou, Lidan},
title = {TRIPS: A System for Translating Raw Indoor Positioning Data into Visual Mobility Semantics},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3236224},
doi = {10.14778/3229863.3236224},
abstract = {The rapid accumulation of indoor positioning data is increasingly booming the interest in indoor mobility analyses. As a fundamental analysis, it is highly relevant to translate raw indoor positioning data into mobility semantics that describe what, where and when in a more concise and semantics-oriented way. Such a translation is challenging as multiple data sources are involved, raw indoor positioning data is of low quality, and translation results are hard to assess. We demonstrate a system TRIPS that streamlines the entire translation process by three functional components. The Configurator provides a standard but concise means to configure multiple input sources, including the indoor positioning data, indoor space information, and relevant contexts. The Translator cleans the indoor positioning data and exports reliable mobility semantics without manual interventions. The Viewer offers a suite of flexible operations to trace the input, output and intermediate data involved in the translation. Data analysts can interact with TRIPS to obtain the desired mobility semantics in a visual and convenient way.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1918–1921},
numpages = {4}
}

@inproceedings{10.1145/3460866.3461769,
author = {Nesen, Alina and Bhargava, Bharat},
title = {Towards Situational Awareness with Multimodal Streaming Data Fusion: Serverless Computing Approach},
year = {2021},
isbn = {9781450384650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460866.3461769},
doi = {10.1145/3460866.3461769},
abstract = {The availability of large quantities of data has given an impulse for methods and techniques to extract unseen useful knowledge and process it in a fast and scalable manner. In order to extract the most complete possible knowledge from the continuous data stream, it is necessary to use the heterogeneous data sources and process information from multiple modalities. The systems that utilize multimodal data must take advantage of the up-to-date approaches for data storage, usage, cleaning and storage. Neural networks and machine learning approaches are widely used for data-heavy software where pattern extractions and predictions need to be conducted while serverless computing frameworks are being increasingly used for machine learning solutions to optimize cost and speed of such systems. This work presents a framework for processing data from multimodal sources where the task of feature and pattern extraction is performed on a serverless computing platform. The use cases for public safety solutions to increase situational awareness are described and compared with other implementation approaches.},
booktitle = {Proceedings of the International Workshop on Big Data in Emergent Distributed Environments},
articleno = {5},
numpages = {6},
keywords = {multimodal machine learning, function-as-a-service, serverless computing},
location = {Virtual Event, China},
series = {BiDEDE '21}
}

@article{10.1145/3138806,
author = {Truong, Hong-Linh and Murguzur, Aitor and Yang, Erica},
title = {Challenges in Enabling Quality of Analytics in the Cloud},
year = {2018},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3138806},
doi = {10.1145/3138806},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {9},
numpages = {4},
keywords = {big data analytics, data quality, service management, Cloud computing}
}

@inproceedings{10.1145/2463676.2465309,
author = {Cao, Yang and Fan, Wenfei and Yu, Wenyuan},
title = {Determining the Relative Accuracy of Attributes},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465309},
doi = {10.1145/2463676.2465309},
abstract = {The relative accuracy problem is to determine, given tuples t1 and t2 that refer to the same entity e, whether t1[A] is more accurate than t2A, i.e., t1A is closer to the true value of the A attribute of e than t2A. This has been a longstanding issue for data quality, and is challenging when the true values of e are unknown. This paper proposes a model for determining relative accuracy. (1) We introduce a class of accuracy rules and an inference system with a chase procedure, to deduce relative accuracy. (2) We identify and study several fundamental problems for relative accuracy. Given a set Ie of tuples pertaining to the same entity e and a set of accuracy rules, these problems are to decide whether the chase process terminates, is Church-Rosser, and leads to a unique target tuple te composed of the most accurate values from Ie for all the attributes of e. (3) We propose a framework for inferring accurate values with user interaction. (4) We provide algorithms underlying the framework, to find the unique target tuple te whenever possible; when there is no enough information to decide a complete te, we compute top-k candidate targets based on a preference model. (5) Using real-life and synthetic data, we experimentally verify the effectiveness and efficiency of our method.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {565–576},
numpages = {12},
keywords = {data accuracy, data cleaning},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/3331453.3360954,
author = {Liu, Jianping and Wang, Jian and Zhou, Guomin and Zhang, Guilan and Cui, Yunpeng and Liu, Juan},
title = {The Cognitive Enhancement Process of Scientific Data Retrieval},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3360954},
doi = {10.1145/3331453.3360954},
abstract = {Is there a stable cognitive structure of scientific data retrieval process? Based on the theory and method of user relevance research, this study explores the cognitive characteristics of user scientific data query and retrieval. The semi-structured interview method used to collect relevant data, and the content analysis method used to encode and analyze the cognitive process of users' scientific data query and retrieval. The results show that (1) users scientific data relevance judgment not only depend on topicality, but also use accessibility, quality, authority and usefulness. (2) There are 7 combination patterns for the use of user's scientific data relevance criteria, and (3) different patterns correspond to different user relevance types and different user information need states. These 7 criteria usage patterns reveal the cognitive enhancement of user scientific data relevance judgment. The research results have a great inspiration for the development of interactive scientific data retrieval system based on user cognitive enhancement characteristics.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {1},
numpages = {7},
keywords = {Scientific data retrieval, Relevance criteria, User relevance},
location = {Sanya, China},
series = {CSAE 2019}
}

@inproceedings{10.1145/2808797.2809367,
author = {Tekieh, Mohammad Hossein and Raahemi, Bijan},
title = {Importance of Data Mining in Healthcare: A Survey},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2809367},
doi = {10.1145/2808797.2809367},
abstract = {In this survey, we collect the related information that demonstrate the importance of data mining in healthcare. As the amount of collected health data is increasing significantly every day, it is believed that a strong analysis tool that is capable of handling and analyzing large health data is essential. Analyzing the health datasets gathered by electronic health record (EHR) systems, insurance claims, health surveys, and other sources, using data mining techniques is very complex and is faced with very specific challenges, including data quality and privacy issues. However, the applications of data mining in healthcare, advantages of data mining techniques over traditional methods, special characteristics of health data, and new health condition mysteries have made data mining very necessary for health data analysis.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {1057–1062},
numpages = {6},
keywords = {predictive modelling, data mining, data mining applications, data quality, health big data, health data analysis},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1145/3445815.3445820,
author = {Jin, Liya and Wang, Ronghui and Wang, Xuan},
title = {Research on Architecture of Big Date Analysis Platform in Cloud Environment},
year = {2021},
isbn = {9781450388436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445815.3445820},
doi = {10.1145/3445815.3445820},
abstract = {The rapid development of big data has attracted extensive attention at home and abroad. Scientific and effective analysis and processing of big data is the core issue in the field of big data. The construction of a big data analysis platform in cloud environment can process complex data structures and highly correlated data, timely respond to user requests, realize intelligent and efficient data analysis, and mine more valuable data, providing technical support for the rapid construction of big data services.},
booktitle = {2020 4th International Conference on Computer Science and Artificial Intelligence},
pages = {29–33},
numpages = {5},
keywords = {Analysis platform, Big data, Cloud computing, Data mining},
location = {Zhuhai, China},
series = {CSAI 2020}
}

@inproceedings{10.1145/3532213.3532249,
author = {Liu, Haitao and Ye, Bo and Qin, Zhi and Zhang, Jing},
title = {The High-Performance Solution with Federated Learning in Supply Chain System},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532249},
doi = {10.1145/3532213.3532249},
abstract = {Supply chain is an important commercial pattern which used in industry, agriculture and service widely. However different parties have different information even though they are all in the same supply chain. That would results in information silo and the extra increasing on cost. On one hand, information could not transfer to other parties for itself benefits or intellectual property. On the other hand, other parties want to obtain supplementary information in order to make equitable trade and lower trade risk. A skillful method would be designed and employed for solve above problems in this paper. Federated Learning technology was introduced in order to solve the puzzle between information privacy and information sharing. In the concretely implement process, Vertical Federated Learning (VFL) model was constructed and trained for resolving several problems specially because the overlap of parties are more but the features are small in supply chain forward. Gradient descent methods and loss computation ways were also used in training process in order to advance the performance. A series of experiments were used to evaluate VFL in supply chain. Experimental results revealed that VFL used in supply chain was effective.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {241–245},
numpages = {5},
keywords = {High-performance, Supply Chain, Federated Learning, Block chain},
location = {Tianjin, China},
series = {ICCAI '22}
}

@inproceedings{10.1145/3414752.3414772,
author = {Qin, Haiqing and Liu, Xiaohan and Qin, Haiqi and Zhu, Jiang},
title = {How to Be an Enabler of Digital Transformation for Media Organizations?},
year = {2020},
isbn = {9781450388016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3414752.3414772},
doi = {10.1145/3414752.3414772},
abstract = {Big data brings opportunities for the development of the media industry and also poses serious challenges. During the digital transformation period, media organizations have used third-party big data to achieve digital transformation and build a media digital ecology, which has become the focus of strategic development at this stage. This article takes the application of the data assets of China Unicom Big Data Co., Ltd. (UBD) in the media industry as an example, and explores the difficulties and attempts made by UBD. through in-depth interviews. The countermeasures are summarized from different aspects such as technology and data governance, which provide a reference for the digital transformation of media organizations.},
booktitle = {2020 The 11th International Conference on E-Business, Management and Economics},
pages = {153–156},
numpages = {4},
keywords = {Case study, Big data, Media organization, Data empowerment},
location = {Beijing, China},
series = {ICEME 2020}
}

@inproceedings{10.1145/3500931.3500993,
author = {Wang, Yiyang},
title = {A Comparison of Machine Learning Algorithms in Blood Glucose Prediction for People with Type 1 Diabetes},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3500993},
doi = {10.1145/3500931.3500993},
abstract = {Diabetes is a metabolic disease with the characteristic of hyperglycemia. The pathogenic principle is derived from the defect of insulin secretion or the impairment of biological effects, or both. We use machine learning models and deep learning models for forecasting future blood glucose levels in this paper, and study the efficiency of detecting hypoglycemia and hyperglycemia events. The data set used is in-silico data generated from the UVA/PADOVA type 1 diabetes simulator. We aim to compare support vector machines, random forests, linear regression, K-Nearest Neighbors regression (KNN), XGBoosted trees and other deep learning models in terms of Root Mean Squared Error (RMSE), and other several evaluation metrics to study their effectiveness in predicting future blood sugar, and the accuracy rate of predicting hypoglycemia and hyperglycemia events. In this work, we found a bidirectional long-short term memory (LSTM) model with the best prediction effect, which can predict the blood glucose level of simulated patients with leading accuracy within 30 minutes (RMSE = 7.55±0.19 [mg/dl], R2-SCORE=0.96). The hopeful results show that this method could have practical application value for self-management of blood glucose in patients with type 1 diabetes.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {351–360},
numpages = {10},
keywords = {deep neural networks, deep learning, machine learning, reinforcement learning, type 1 diabetes, artificial intelligence},
location = {Beijing, China},
series = {ISAIMS 2021}
}

@inproceedings{10.1145/2882903.2899391,
author = {Farid, Mina and Roatis, Alexandra and Ilyas, Ihab F. and Hoffmann, Hella-Franziska and Chu, Xu},
title = {CLAMS: Bringing Quality to Data Lakes},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2899391},
doi = {10.1145/2882903.2899391},
abstract = {With the increasing incentive of enterprises to ingest as much data as they can in what is commonly referred to as "data lakes", and with the recent development of multiple technologies to support this "load-first" paradigm, the new environment presents serious data management challenges. Among them, the assessment of data quality and cleaning large volumes of heterogeneous data sources become essential tasks in unveiling the value of big data. The coveted use of unstructured and semi-structured data in large volumes makes current data cleaning tools (primarily designed for relational data) not directly adoptable.We present CLAMS, a system to discover and enforce expressive integrity constraints from large amounts of lake data with very limited schema information (e.g., represented as RDF triples). This demonstration shows how CLAMS is able to discover the constraints and the schemas they are defined on simultaneously. CLAMS also introduces a scale-out solution to efficiently detect errors in the raw data. CLAMS interacts with human experts to both validate the discovered constraints and to suggest data repairs.CLAMS has been deployed in a real large-scale enterprise data lake and was experimented with a real data set of 1.2 billion triples. It has been able to spot multiple obscure data inconsistencies and errors early in the data processing stack, providing huge value to the enterprise.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2089–2092},
numpages = {4},
keywords = {data lakes, data quality, RDF},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3331184.3331218,
author = {Lu, Shuqi and Dou, Zhicheng and Jun, Xu and Nie, Jian-Yun and Wen, Ji-Rong},
title = {PSGAN: A Minimax Game for Personalized Search with Limited and Noisy Click Data},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331218},
doi = {10.1145/3331184.3331218},
abstract = {Personalized search aims to adapt document ranking to user's personal interests. Traditionally, this is done by extracting click and topical features from historical data in order to construct a user profile. In recent years, deep learning has been successfully used in personalized search due to its ability of automatic feature learning. However, the small amount of noisy personal data poses challenges to deep learning models to learn the personalized classification boundary between relevant and irrelevant results. In this paper, we propose PSGAN, a Generative Adversarial Network (GAN) framework for personalized search. By means of adversarial training, we enforce the model to pay more attention to training data that are difficult to distinguish. We use the discriminator to evaluate personalized relevance of documents and use the generator to learn the distribution of relevant documents. Two alternative ways to construct the generator in the framework are tested: based on the current query or based on a set of generated queries. Experiments on data from a commercial search engine show that our models can yield significant improvements over state-of-the-art models.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {555–564},
numpages = {10},
keywords = {generative adversarial network, personalized web search},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3323878.3325806,
author = {Lehmberg, Oliver and Bizer, Christian},
title = {Profiling the Semantics of N-Ary Web Table Data},
year = {2019},
isbn = {9781450367660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323878.3325806},
doi = {10.1145/3323878.3325806},
abstract = {The Web contains millions of relational HTML tables, which cover a multitude of different, often very specific topics. This rich pool of data has motivated a growing body of research on methods that use web table data to extend local tables with additional attributes or add missing facts to knowledge bases. Nearly all existing approaches for these tasks build upon the assumption that web table data consists of binary relations, meaning that an attribute value depends on a single key attribute, and that the key attribute value is contained in the HTML table. Inspecting randomly chosen tables on the Web, however, quickly reveals that both assumptions are wrong for a large fraction of the tables. In order to better understand the potential of non-binary web table data for downstream applications, this papers analyses a corpus of 5 million web tables originating from 80 thousand different web sites with respect to how many web table attributes are non-binary, what composite keys are required to correctly interpret the semantics of the non-binary attributes, and whether the values of these keys are found in the table itself or need to be extracted from the page surrounding the table. The profiling of the corpus shows that at least 38% of the relations are non-binary. Recognizing these relations requires information from the title or the URL of the web page in 50% of the cases. We find that different websites use keys of varying length for the same dependent attribute, e.g. one cluster of websites presents employment numbers depending on time, another cluster presents them depending on time and profession. By identifying these clusters, we lay the foundation for selecting Web data sources according to the specificity of the keys that are used to determine specific attributes.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {5},
numpages = {6},
keywords = {data profiling, web tables, n-ary relations, key detection},
location = {Amsterdam, Netherlands},
series = {SBD '19}
}

@inproceedings{10.1145/2623330.2623716,
author = {Dasu, Tamraparni and Loh, Ji Meng and Srivastava, Divesh},
title = {Empirical Glitch Explanations},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623716},
doi = {10.1145/2623330.2623716},
abstract = {Data glitches are unusual observations that do not conform to data quality expectations, be they logical, semantic or statistical. By applying data integrity constraints, potentially large sections of data could be flagged as being noncompliant. Ignoring or repairing significant sections of the data could fundamentally bias the results and conclusions drawn from analyses. In the context of Big Data where large numbers and volumes of feeds from disparate sources are integrated, it is likely that significant portions of seemingly noncompliant data are actually legitimate usable data.In this paper, we introduce the notion of Empirical Glitch Explanations - concise, multi-dimensional descriptions of subsets of potentially dirty data - and propose a scalable method for empirically generating such explanatory characterizations. The explanations could serve two valuable functions: (1) Provide a way of identifying legitimate data and releasing it back into the pool of clean data. In doing so, we reduce cleaning-related statistical distortion of the data; (2) Used to refine existing data quality constraints and generate and formalize domain knowledge.We conduct experiments using real and simulated data to demonstrate the scalability of our method and the robustness of explanations. In addition, we use two real world examples to demonstrate the utility of the explanations where we reclaim over 99% of the suspicious data, keeping data repair related statistical distortion close to 0.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {572–581},
numpages = {10},
keywords = {data quality, glitch explanations, quantitative data cleaning, crossover subsampling},
location = {New York, New York, USA},
series = {KDD '14}
}

@article{10.1145/3396850,
author = {Luckner, Marcin and Grzenda, Maciej and Kunicki, Robert and Legierski, Jaroslaw},
title = {IoT Architecture for Urban Data-Centric Services and Applications},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3396850},
doi = {10.1145/3396850},
abstract = {In this work, we describe an urban Internet of Things (IoT) architecture, grounded in big data patterns and focused on the needs of cities and their key stakeholders. First, the architecture of the dedicated platform USE4IoT (Urban Service Environment for the Internet of Things), which gathers and processes urban big data and extends the Lambda architecture, is proposed. We describe how the platform was used to make IoT an enabling technology for intelligent transport planning. Moreover, key data processing components vital to provide high-quality IoT data streams in a near-real-time manner are defined. Furthermore, tests showing how the IoT platform described in this study provides a low-latency analytical environment for smart cities are included.},
journal = {ACM Trans. Internet Technol.},
month = {jul},
articleno = {29},
numpages = {30},
keywords = {public transport, data processing, Data stream, big data}
}

@inproceedings{10.1145/2896387.2900326,
author = {Cuzzocrea, Alfredo and Gaber, Mohamed Medhat and Lattimer, Staci and Grasso, Giorgio Mario},
title = {Clustering-Based Spatio-Temporal Analysis of Big Atmospheric Data},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900326},
doi = {10.1145/2896387.2900326},
abstract = {This paper proposes a comprehensive approach for supporting clustering-based spatio-temporal analysis of big atmospheric data via specializing on the interesting applicative setting represented by Greenhouse Gas Emissions (GGEs), a relevant instance of Big Data that empathize the Variety aspect of the well-known 3V Big Data axioms. In particular, in our research we consider GGEs from three EU countries, namely UK, France and Italy. The deriving Big Data Mining model turns to be useful for decision support processes in both the governmental and industrial contexts.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {74},
numpages = {8},
keywords = {Big Environmental and Atmospheric Data, Big Data Mining, Clustering-Based Spatio-Temporal Analysis of Big Data},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@inproceedings{10.1145/3446999.3447017,
author = {Wang, Mo and Wang, Jing and Song, Yulun},
title = {A Map Matching Method for Restoring Movement Routes with Cellular Signaling Data},
year = {2021},
isbn = {9781450388559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446999.3447017},
doi = {10.1145/3446999.3447017},
abstract = {Cellular signaling data is a valuable and abundant data source to explore human mobility. Yet challenges remain to restore movement routes from signaling data due to its coarse positioning information. We propose an efficient map matching method based on road network topology. First, a customized spatial-temporal clustering algorithm ST-DBSCAN was employed to find stationary point clusters, which were later used to segment trips into sub-trips. The search space was then clipped with a fixed buffer zone along the line that connects the whole trip. Two optional strategies were provided to find the best matching routes with distance costs. Experiments on real-world data showed that both strategies achieved high map matching accuracies (88.2% and 94.3%). With Deep Mode, the method reached higher accuracy, while with longer computation time. The proposed method has the potential in solving practical problems, in the sense that it could be easily parallelized to deal with mass data.},
booktitle = {2020 The 8th International Conference on Information Technology: IoT and Smart City},
pages = {94–99},
numpages = {6},
keywords = {map matching, signaling data, human mobility, road networks},
location = {Xi'an, China},
series = {ICIT 2020}
}

@inproceedings{10.1109/CCGrid.2016.79,
author = {Abdellaoui, Sabrina and Bellatreche, Ladjel and Nader, Fahima},
title = {A Quality-Driven Approach for Building Heterogeneous Distributed Databases: The Case of Data Warehouses},
year = {2016},
isbn = {9781509024520},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2016.79},
doi = {10.1109/CCGrid.2016.79},
abstract = {Data Warehouse (DW) is a collection of data, consolidated from several heterogeneous sources, used to perform data analysis and support decision-making in an organization. Extract-Transform-Load (ETL) phase plays a crucial role in designing DW. To overcome the complexity of the ETL phase, different studies have recently proposed the use of ontologies. Ontology-based ETL approaches have been used to reduce heterogeneity between data sources and ensure automation of the ETL process. Existing studies in semantic ETL have largely focused on fulfilling functional requirements. However, the ETL process quality dimension has not been sufficiently considered by these studies. As the amount of data has exploded with the advent of big data era, dealing with quality challenges in the early stages of designing the process become more important than ever. To address this issue, we propose to keep data quality requirements at the center of the ETL phase design. We present in this paper an approach, defining the ETL process at the ontological level. We define a set of quality indicators and quantitative measures that can anticipate data quality problems and identify causes of deficiencies. Our approach checks the quality of data before loading them into the target data warehouse to avoid the propagation of corrupted data. Finally, our proposal is validated through a case study, using Oracle Semantic DataBase sources (SDBs), where each source references the Lehigh University BenchMark ontology (LUBM).},
booktitle = {Proceedings of the 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {631–638},
numpages = {8},
keywords = {data warehouse, ontologies, data quality, ETL design, semantic database sources},
location = {Cartagena, Columbia},
series = {CCGRID '16}
}

@article{10.1145/3317573,
author = {Ding, Junhua and Li, Xinchuan and Kang, Xiaojun and Gudivada, Venkat N.},
title = {A Case Study of the Augmentation and Evaluation of Training Data for Deep Learning},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3317573},
doi = {10.1145/3317573},
abstract = {Deep learning has been widely used for extracting values from big data. As many other machine learning algorithms, deep learning requires significant training data. Experiments have shown both the volume and the quality of training data can significantly impact the effectiveness of the value extraction. In some cases, the volume of training data is not sufficiently large for effectively training a deep learning model. In other cases, the quality of training data is not high enough to achieve the optimal performance. Many approaches have been proposed for augmenting training data to mitigate the deficiency. However, whether the augmented data are “fit for purpose” of deep learning is still a question. A framework for comprehensively evaluating the effectiveness of the augmented data for deep learning is still not available. In this article, we first discuss a data augmentation approach for deep learning. The approach includes two components: the first one is to remove noisy data in a dataset using a machine learning based classification to improve its quality, and the second one is to increase the volume of the dataset for effectively training a deep learning model. To evaluate the quality of the augmented data in fidelity, variety, and veracity, a data quality evaluation framework is proposed. We demonstrated the effectiveness of the data augmentation approach and the data quality evaluation framework through studying an automated classification of biology cell images using deep learning. The experimental results clearly demonstrated the impact of the volume and quality of training data to the performance of deep learning and the importance of the data quality evaluation. The data augmentation approach and the data quality evaluation framework can be straightforwardly adapted for deep learning study in other domains.},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {20},
numpages = {22},
keywords = {convolutional neural network, diffraction image, support vector machine, machine learning, Data quality, deep learning}
}

@inproceedings{10.1145/2668930.2688055,
author = {Arlitt, Martin and Marwah, Manish and Bellala, Gowtham and Shah, Amip and Healey, Jeff and Vandiver, Ben},
title = {IoTAbench: An Internet of Things Analytics Benchmark},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688055},
doi = {10.1145/2668930.2688055},
abstract = {The commoditization of sensors and communication networks is enabling vast quantities of data to be generated by and collected from cyber-physical systems. This ``Internet-of-Things" (IoT) makes possible new business opportunities, from usage-based insurance to proactive equipment maintenance. While many technology vendors now offer ``Big Data" solutions, a challenge for potential customers is understanding quantitatively how these solutions will work for IoT use cases. This paper describes a benchmark toolkit called IoTAbench for IoT Big Data scenarios. This toolset facilitates repeatable testing that can be easily extended to multiple IoT use cases, including a user's specific needs, interests or dataset. We demonstrate the benchmark via a smart metering use case involving an eight-node cluster running the HP Vertica analytics platform. The use case involves generating, loading, repairing and analyzing synthetic meter readings. The intent of IoTAbench is to provide the means to perform ``apples-to-apples" comparisons between different sensor data and analytics platforms. We illustrate the capabilities of IoTAbench via a large experimental study, where we store 22.8 trillion smart meter readings totaling 727 TB of data in our eight-node cluster.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {133–144},
numpages = {12},
keywords = {big data, benchmarking, performance evaluation, internet of things},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@article{10.1145/3397462,
author = {Caruccio, Loredana and Cirillo, Stefano},
title = {Incremental Discovery of Imprecise Functional Dependencies},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3397462},
doi = {10.1145/3397462},
abstract = {Functional dependencies (fds) are one of the metadata used to assess data quality and to perform data cleaning operations. However, to pursue robustness with respect to data errors, it has been necessary to devise imprecise versions of functional dependencies, yielding relaxed functional dependencies (rfds). Among them, there exists the class of rfds relaxing on the extent, i.e., those admitting the possibility that an fd holds on a subset of data. In the literature, several algorithms to automatically discover rfds from big data collections have been defined. They achieve good performances with respect to the inherent problem complexity. However, most of them are capable of discovering rfds only by batch processing the entire dataset. This is not suitable in the era of big data, where the size of a database instance can grow with high-velocity, and the insertion of new data can invalidate previously holding rfds. Thus, it is necessary to devise incremental discovery algorithms capable of updating the set of holding rfds upon data insertions, without processing the entire dataset. To this end, in this article we propose an incremental discovery algorithm for rfds relaxing on the extent. It manages the validation of candidate rfds and the generation of possibly new rfd candidates upon the insertion of the new tuples, while limiting the size of the overall search space. Experimental results show that the proposed algorithm achieves extremely good performances on real-world datasets.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {19},
numpages = {25},
keywords = {tuple insertions, discovery algorithm, Functional dependency, parallelism, incremental discovery}
}

@inproceedings{10.1145/3320154.3320165,
author = {Lawrenz, Sebastian and Sharma, Priyanka and Rausch, Andreas},
title = {Blockchain Technology as an Approach for Data Marketplaces},
year = {2019},
isbn = {9781450362689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320154.3320165},
doi = {10.1145/3320154.3320165},
abstract = {In the digital Economy 'Data is the new oil. In the last decade technology has disrupted every filed imaginable. One such booming technology is Blockchain. A blockchain is essentially a distributed database of records or public ledger of all transactions or digital events that have been executed and shared among participating parties. And once entered, the information is immutable. Ongoing projects and prior work in the fields of big data, data mining and data science has revealed how relevant data can be used to enhance products and services. There are uncountable applications and advantages of relevant data. The most valuable companies of today treat data as a commodity, which they trade and earn revenues.But use of relevant data has also drawn attention by the other non-conventional organizations and domains. To facilitate such trading, data marketplaces have emerged. In this paper we present a global data marketplace for users to easily buy and share data. The main focus of this research is to have a central data sharing platform for the recycling industry. This paper is a part of the research project "Recycling 4.0" which is focusing on sustainably improving the recycling process through exchange of information. We identify providing secure platform, data integrity and data quality as some major challenges for a data marketplace. In this paper we also explore how global data marketplace could be implemented using blockchain and similar technologies.},
booktitle = {Proceedings of the 2019 International Conference on Blockchain Technology},
pages = {55–59},
numpages = {5},
keywords = {Data marketplaces, Blockchain, Data quality, Smart Contracts, Security},
location = {Honolulu, HI, USA},
series = {ICBCT 2019}
}

@inproceedings{10.1145/3357292.3357306,
author = {Min, Han Xue and Feng, Zheng Gao and Xi, Liu Peng and Dong, Wang Xu and Ping, Xu Zhong},
title = {Application Research of Power Grid Full-Business Monitoring and Analysis Based on Multi-Source Business and Data Fusion},
year = {2019},
isbn = {9781450371445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357292.3357306},
doi = {10.1145/3357292.3357306},
abstract = {With the development of power enterprise informationization after more than ten years of development, Achieve full coverage of the company's business, effectively supporting the full-business operation of the power grid, and the accumulated business data has exploded. However, there are still problems such as low data quality, insufficient integration of multi-source business and data fusion, which makes it difficult to monitor and analyze the full-business of the power grid. This paper will combine the big data technology to study how to conduct monitoring and analysis of power grid full-business operation based on multi-source business and data fusion, and realize the three-layer architecture of business and data combination layer, business and data integration layer and business and data aggregation layer. Different levels of analysis and application, such as indicator monitoring analysis, subject monitoring analysis, and special monitoring analysis, effectively support enterprise management analysis and analytical decision.},
booktitle = {Proceedings of the 2019 2nd International Conference on Information Management and Management Sciences},
pages = {49–53},
numpages = {5},
keywords = {Big data technology, mining analysis, business and data fusion},
location = {Chengdu, China},
series = {IMMS 2019}
}

@inproceedings{10.1145/3478905.3478999,
author = {Huang, Yongliang and Yang, Shulin and Li, Xiang and Peng, Jiao and Zhou, Meiqi},
title = {Research on Publisher Topic Selection Based on Data Mining},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478999},
doi = {10.1145/3478905.3478999},
abstract = {As one of the important traditional industries in China, the printing and publishing industry is facing the current situation of the Internet era with the explosion of information and people's demands tend to be personalized and diversified.How to achieve accurate topic selection is the key.In this context, this paper combines the most popular big data technology with the traditional printing industry, improves the quality of the original data of the publishing house through data preprocessing technology, classifies different types of data by decision tree classifier, and finally completes the data mining.It provides a new thought and method for the topic planning of publishing industry.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {448–452},
numpages = {5},
keywords = {Publishing topics, Decision tree, Data mining, Big data},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/2723372.2742800,
author = {Deshpande, Mukund and Ray, Dhruva and Dixit, Sameer and Agasti, Avadhoot},
title = {ShareInsights: An Unified Approach to Full-Stack Data Processing},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2742800},
doi = {10.1145/2723372.2742800},
abstract = {The field of data analysis seeks to extract value from data for either business or scientific benefit. This field has seen a renewed interest with the advent of big data technologies and a new organizational role called data scientist. Even with the new found focus, the task of analyzing large amounts of data is still challenging and time-consuming.The essence of data analysis involves setting up data pipe-lines which consists of several operations that are chained together - starting from data collection, data quality checks, data integration, data analysis and data visualization (including the setting up of interaction paths in that visualization).In our opinion, the challenges stem from from the technology diversity at each stage of the data pipeline as well as the lack of process around the analysis.In this paper we present a platform that aims to significantly reduce the time it takes to build data pipelines. The platform attempts to achieve this in following ways. Allow the user to describe the entire data pipeline with a single language and idioms - all the way from data ingestion to insight expression (via visualization and end-user interaction).Provide a rich library of parts that allow users to quickly assemble a data analysis pipeline in the language.Allow for a collaboration model that allows multiple users to work together on a data analysis pipeline as well as leverage and extend prior work with minimal effort.We studied the efficacy of the platform for a data hackathon competition conducted in our organization. The hackathon provided us with a way to study the impact of the approach. Rich data pipelines which traditionally took weeks to build were constructed and deployed in hours. Consequently, we believe that the complexity of designing and running the data analysis pipeline can be significantly reduced; leading to a marked improvement in the productivity of data analysts/data scientists.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1925–1940},
numpages = {16},
keywords = {data analysis, data visualization, big data, data pipeline},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.5555/2820489.2820507,
author = {Casale, G. and Ardagna, D. and Artac, M. and Barbier, F. and Nitto, E. Di and Henry, A. and Iuhasz, G. and Joubert, C. and Merseguer, J. and Munteanu, V. I. and P\'{e}rez, J. F. and Petcu, D. and Rossi, M. and Sheridan, C. and Spais, I. and Vladu\v{s}i\v{c}, D.},
title = {DICE: Quality-Driven Development of Data-Intensive Cloud Applications},
year = {2015},
publisher = {IEEE Press},
abstract = {Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.},
booktitle = {Proceedings of the Seventh International Workshop on Modeling in Software Engineering},
pages = {78–83},
numpages = {6},
keywords = {big data, model-driven engineering, quality assurance},
location = {Florence, Italy},
series = {MiSE '15}
}

@inproceedings{10.1145/3289100.3289123,
author = {Korachi, Zineb and Bounabat, Bouchaib},
title = {Data Driven Maturity Model for Assessing Smart Cities},
year = {2018},
isbn = {9781450365079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289100.3289123},
doi = {10.1145/3289100.3289123},
abstract = {Smart cities provide the ability to improve the quality of the citizen's life. Transformation into a smart city consists of defining the way ICT (Information and Communication Technologies) can be used to improve the weaker aspects of the city and improve the quality of services provided by public sectors (education, health, transportation...). The growth of the urban population implies the growing needs of urban services (health, education...) and resources (water, energy...). ICT can be used to meet the growing population needs and solve many of today's problems in the private and public sectors (health, transportation, school...). Using mobile phones all citizens produce data and information every day and everywhere, this data will be used to improve the quality of services provided by the city. The quality of the generated data presents the key element that will impact the success of the transformation into a smart city.This paper describes the proposed data quality driven smart cities model. The proposed model, called DQSC-MM (Data Quality Driven Smart Cities Maturity Model). DQSC-MM is used to evaluate the maturity of a smart city based on the quality of produced and consumed data. It suggests a way to measure the importance of data quality in a city's transformation into a smart city. The paper describes how the model was conceived, designed and developed. It describes also a JEE application conceived to support DQSC-MM. The developed application provides the ability to measure data quality and use these measurements for smart city evaluation.},
booktitle = {Proceedings of the 2nd International Conference on Smart Digital Environment},
pages = {140–147},
numpages = {8},
keywords = {smart city evaluation, data quality, Smart city, maturity model, DQSC-MM, data, data quality measurement},
location = {Rabat, Morocco},
series = {ICSDE'18}
}

@inproceedings{10.1145/2459976.2459991,
author = {Howes, Joshua and Solderitsch, James and Chen, Ignatius and Craighead, Jont\'{e}},
title = {Enabling Trustworthy Spaces via Orchestrated Analytical Security},
year = {2013},
isbn = {9781450316873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2459976.2459991},
doi = {10.1145/2459976.2459991},
abstract = {Cyberspaces require both the implementation of customized functional requirements and the enforcement of policy constraints to be trustworthy. In tailored, distributed and adaptive environments (spaces), monitoring to ensure this enforcement is especially difficult given the wide spectrum of activities performed and the evolving range of threats. Spaces must be monitored from a multitude of perspectives, each of which will generate a vast quantity of disparate information, including structured, semi-structured and unstructured data. However, existing security toolsets and offerings are not yet well equipped to analyze these kinds of data with the necessary speed and agility. Big Data technologies, such as Hadoop, enable the analysis of large and unstructured data sources. We propose security operations teams extend their existing security infrastructure with emerging Big Data analytics and Complex Event Processing platforms. To help them do so, we introduce a conceptual blueprint for the analytics solution. We also present an Orchestrated Analytical Security operational and organizational framework that helps organizations understand how analytical security not only provides monitoring but also creates actionable intelligence from data.},
booktitle = {Proceedings of the Eighth Annual Cyber Security and Information Intelligence Research Workshop},
articleno = {13},
numpages = {4},
keywords = {big data, Hadoop, trust, orchestration, complex event processing, security, intelligence, analytics},
location = {Oak Ridge, Tennessee, USA},
series = {CSIIRW '13}
}

@inproceedings{10.1145/3208352.3208357,
author = {Madkour, Amgad and Aref, Walid G. and Prabhakar, Sunil and Ali, Mohamed and Bykau, Siarhei},
title = {TrueWeb: A Proposal for Scalable Semantically-Guided Data Management and Truth Finding in Heterogeneous Web Sources},
year = {2018},
isbn = {9781450357791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208352.3208357},
doi = {10.1145/3208352.3208357},
abstract = {We envision a responsible web environment, termed TrueWeb, where a user should be able to find out whether any sentence he or she encounters in the web is true or false. The user should be able to track the provenance of any sentence or paragraph in the web. The target of TrueWeb is to compose factual knowledge from Internet resources about any subject of interest and present the collected knowledge in chronological order and distribute facts spatially and temporally as well as assign some belief factor for each fact. Another important target of TrueWeb is to be able to identify whether a statement in the Internet is true or false. The aim is to create an Internet infrastructure that, for each piece of published information, will be able to identify the truthfulness (or the degree of truthfulness) of that piece of information.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {5},
numpages = {6},
keywords = {Truth detection, RDF, Linked Data, Data Management},
location = {Houston, TX, USA},
series = {SBD'18}
}

@inproceedings{10.1145/3358961.3358970,
author = {Posadas, Brianna B. and Hanumappa, Mamatha and Gilbert, Juan E.},
title = {Opinions Concerning Crowdsourcing Applications in Agriculture in D.C.},
year = {2020},
isbn = {9781450376792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358961.3358970},
doi = {10.1145/3358961.3358970},
abstract = {As big data has become increasingly necessary in modern farming techniques, the dependence on high quality and quantity of ground truthing data has risen. Collecting ground truthing data is one of the most labor-intensive aspects of the research process. A crowdsourcing platform application to aid laypeople in completing ground truthing data can improve the quality and quantity of data for growers and agricultural researchers. Focus groups were conducted to gauge opinions on crowdsourcing initiatives in agriculture to inform the design of the platform. Preliminary results demonstrate that the greatest motivation for the participants was having opportunities to develop their skills and access to educational resources. They also discussed having a finite timeframe for collecting the data, feeling appreciated by the researchers, and being informed on the context and next steps of the research. The results of these focus groups will be used to develop design prototypes for the crowdsourcing platform.},
booktitle = {Proceedings of the IX Latin American Conference on Human Computer Interaction},
articleno = {3},
numpages = {4},
keywords = {urban agriculture, focus groups, big data, precision agriculture},
location = {Panama City, Panama},
series = {CLIHC '19}
}

@inproceedings{10.1145/3463677.3463687,
author = {Li, Hongqin and Zhai, Jun},
title = {Research on Suggestions of Improving Chinese Open Government Data in Innovation of Public Governance},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463687},
doi = {10.1145/3463677.3463687},
abstract = {This paper collects a large number of cases and makes a comparative analysis of the typical application of Chinese and American open government data for public governance. Through comparison, this paper finds the gap between China's open government data and the United States, and then analyzes the reasons. On this basis, through the investigation of advanced experience, this paper puts forward the suggestions of open government data to innovate public governance, including data catalogue compilation, data standard formulating, data quality assessment and open government data sharing cooperation, in order to improve Chinese open government data to innovate the public governance level.},
booktitle = {DG.O2021: The 22nd Annual International Conference on Digital Government Research},
pages = {142–152},
numpages = {11},
keywords = {Data quality, Sharing and cooperation},
location = {Omaha, NE, USA},
series = {DG.O'21}
}

@article{10.1145/3064173,
author = {Abdellaoui, Sabrina and Nader, Fahima and Chalal, Rachid},
title = {QDflows: A System Driven by Knowledge Bases for Designing Quality-Aware Data Flows},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3–4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3064173},
doi = {10.1145/3064173},
abstract = {In the big data era, data integration is becoming increasingly important. It is usually handled by data flows processes that extract, transform, and clean data from several sources, and populate the data integration system (DIS). Designing data flows is facing several challenges. In this article, we deal with data quality issues such as (1) specifying a set of quality rules, (2) enforcing them on the data flow pipeline to detect violations, and (3) producing accurate repairs for the detected violations. We propose QDflows, a system for designing quality-aware data flows that considers the following as input: (1) a high-quality knowledge base (KB) as the global schema of integration, (2) a set of data sources and a set of validated users’ requirements, (3) a set of defined mappings between data sources and the KB, and (4) a set of quality rules specified by users. QDflows uses an ontology to design the DIS schema. It offers the ability to define the DIS ontology as a module of the knowledge base, based on validated users’ requirements. The DIS ontology model is then extended with multiple types of quality rules specified by users. QDflows extracts and transforms data from sources to populate the DIS. It detects violations of quality rules enforced on the data flows, constructs repair patterns, searches for horizontal and vertical matches in the knowledge base, and performs an automatic repair when possible or generates possible repairs. It interactively involves users to validate the repair process before loading the clean data into the DIS. Using real-life and synthetic datasets, the DBpedia and Yago knowledge bases, we experimentally evaluate the generality, effectiveness, and efficiency of QDflows. We also showcase an interactive tool implementing our system.},
journal = {J. Data and Information Quality},
month = {jun},
articleno = {14},
numpages = {39},
keywords = {knowledge bases, data quality, Data flows, graph-based repairing}
}

@inproceedings{10.1145/2882903.2899389,
author = {Hai, Rihan and Geisler, Sandra and Quix, Christoph},
title = {Constance: An Intelligent Data Lake System},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2899389},
doi = {10.1145/2882903.2899389},
abstract = {As the challenge of our time, Big Data still has many research hassles, especially the variety of data. The high diversity of data sources often results in information silos, a collection of non-integrated data management systems with heterogeneous schemas, query languages, and APIs. Data Lake systems have been proposed as a solution to this problem, by providing a schema-less repository for raw data with a common access interface. However, just dumping all data into a data lake without any metadata management, would only lead to a 'data swamp'. To avoid this, we propose Constance, a Data Lake system with sophisticated metadata management over raw data extracted from heterogeneous data sources. Constance discovers, extracts, and summarizes the structural metadata from the data sources, and annotates data and metadata with semantic information to avoid ambiguities. With embedded query rewriting engines supporting structured data and semi-structured data, Constance provides users a unified interface for query processing and data exploration. During the demo, we will walk through each functional component of Constance. Constance will be applied to two real-life use cases in order to show attendees the importance and usefulness of our generic and extensible data lake system.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2097–2100},
numpages = {4},
keywords = {data quality, data lake, data integration},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/3469028,
author = {Tian, Haiman and Presa-Reyes, Maria and Tao, Yudong and Wang, Tianyi and Pouyanfar, Samira and Miguel, Alonso and Luis, Steven and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, Sundaraja Sitharama},
title = {Data Analytics for Air Travel Data: A Survey and New Perspectives},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3469028},
doi = {10.1145/3469028},
abstract = {From the start, the airline industry has remarkably connected countries all over the world through rapid long-distance transportation, helping people overcome geographic barriers. Consequently, this has ushered in substantial economic growth, both nationally and internationally. The airline industry produces vast amounts of data, capturing a diverse set of information about their operations, including data related to passengers, freight, flights, and much more. Analyzing air travel data can advance the understanding of airline market dynamics, allowing companies to provide customized, efficient, and safe transportation services. Due to big data challenges in such a complex environment, the benefits of drawing insights from the air travel data in the airline industry have not yet been fully explored. This article aims to survey various components and corresponding proposed data analysis methodologies that have been identified as essential to the inner workings of the airline industry. We introduce existing data sources commonly used in the papers surveyed and summarize their availability. Finally, we discuss several potential research directions to better harness airline data in the future. We anticipate this study to be used as a comprehensive reference for both members of the airline industry and academic scholars with an interest in airline research.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {167},
numpages = {35},
keywords = {revenue management, Airline, big data}
}

@article{10.1145/2874239.2874248,
author = {Gotterbarn, Don},
title = {The Creation of Facts in the Cloud: A Fiction in the Making},
year = {2016},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {3},
issn = {0095-2737},
url = {https://doi.org/10.1145/2874239.2874248},
doi = {10.1145/2874239.2874248},
abstract = {Like most significant changes in technology, Cloud Computing and Big Data along with their associated analytic techniques are claimed to provide us with new insights unattainable by any previous knowledge techniques. It is believed that the quantity of virtual data now available requires new knowledge production strategies. Although they have yielded significant results, there are problems with advocated processes and resulting facts. The primary process treats "pattern recognition" as a final result rather than using "pattern recognition" to lead to yet to be tested testable hypotheses. In data analytics, the discovery of a pattern is treated as knowledge rather than going further to understand the possible causes of those patterns. When this is used as the primary approach to knowledge acquisition unjustified inferences are made - "fact generation". These pseudo-facts are used to generate new pseudo-facts as those initial inferences are fed back into analytic engines as established facts. The approach of generating "facts from data analytics" is introducing highly risky scenarios where "fiction becomes fact" very quickly. These "facts" are then given elevated epistemic status and get used in decision making. This, misleading approach is inconsistent with the moral duty of computing professionals embodied in their Codes of Ethics. There are some ways to mitigate the problems generated by this single path approach to knowledge generation.},
journal = {SIGCAS Comput. Soc.},
month = {jan},
pages = {60–67},
numpages = {8},
keywords = {big data, data misuse, professional responsibility, data integrity}
}

@inproceedings{10.1145/2987386.2987413,
author = {Alabduljabbar, Reham and Al-Dossari, Hmood},
title = {A Task Ontology-Based Model for Quality Control in Crowdsourcing Systems},
year = {2016},
isbn = {9781450344555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987386.2987413},
doi = {10.1145/2987386.2987413},
abstract = {In the era of big data, a vast amount of data is created every day. Crowdsourcing systems have recently gained significance as an interesting practice in managing and performing big data operations. Crowdsourcing has facilitated the process of performing tasks that cannot be adequately solved by machines including image labeling, transcriptions, data validation and sentiment analysis. However, quality control remains one of the biggest challenges for crowdsourcing. Current crowdsourcing systems use the same quality control mechanism for evaluating different types of tasks. In this paper, we argue that quality mechanisms vary by task type. We propose a task ontology-based model to identify the most appropriate quality mechanism for a given task. The proposed model has been enriched by a reputation system to collect requesters' feedback on quality mechanisms. Accordingly, the reputation of each mechanism can be established and used for mapping between tasks and mechanisms. Description of the model's framework, algorithms, and its components' interaction are presented.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {22–28},
numpages = {7},
keywords = {HITs, Crowdsourcing, Reputation, Human Computation, Quality Control, Ontology, MTurk, Crowd Computing, Big Data},
location = {Odense, Denmark},
series = {RACS '16}
}

@inproceedings{10.1145/3004725.3004733,
author = {Werner, Martin and Kiermeier, Marie},
title = {A Low-Dimensional Feature Vector Representation for Alignment-Free Spatial Trajectory Analysis},
year = {2016},
isbn = {9781450345828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3004725.3004733},
doi = {10.1145/3004725.3004733},
abstract = {Trajectory analysis is a central problem in the era of big data due to numerous interconnected mobile devices generating unprecedented amounts of spatio-temporal trajectories. Unfortunately, datasets of spatial trajectories are quite difficult to analyse because of the computational complexity of the various existing distance measures. A significant amount of work in comparing two trajectories stems from calculating temporal alignments of the involved spatial points. With this paper, we propose an alignment-free method of representing spatial trajectories using low-dimensional feature vectors by summarizing the combinatorics of shape-derived string sequences. Therefore, we propose to translate trajectories into strings describing the evolving shape of each trajectory, and then provide a sparse matrix representation of these strings using frequencies of adjacencies of characters (n-grams). The final feature vectors are constructed by approximating this matrix with low-dimensional column space using singular value decomposition. New trajectories can be projected into this geometry for comparison. We show that this construction leads to low-dimensional feature vectors with surprising expressive power. We illustrate the usefulness of this approach in various datasets.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
pages = {19–26},
numpages = {8},
keywords = {multi-modal trajectory, moving objects, big data, trajectory},
location = {Burlingame, California},
series = {MobiGIS '16}
}

@inproceedings{10.1145/2675316.2675321,
author = {Dashdorj, Zolzaya and Sobolevsky, Stanislav and Serafini, Luciano and Ratti, Carlo},
title = {Human Activity Recognition from Spatial Data Sources},
year = {2014},
isbn = {9781450331425},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675316.2675321},
doi = {10.1145/2675316.2675321},
abstract = {Recent availability of big data of digital traces of human activity boosted research on human behavior. However, in most of the datasets such as mobile phone data or GPS traces, an important layer of information is typically missing: providing an extensive information of when and where people go typically does not allow understanding of what they do there. Predicting the context of human behavior in such cases where such information is not directly available from the data is a complex task that addresses context recognition problems. To fill in the contextual information for such data, we developed an ontological and stochastic model (HRBModel) that interprets semantic (high-level) human behaviors from geographical maps like OpenStreetMap, analyzing the distribution of Points of Interest(POIs), in a given region and time period. The semantic human behaviors are human activities that are accompanied by their likelihood, depending on their location and time. In this paper, we perform an extended evaluation of this model based on other qualitative data source, namely a country-wide anonymized bank card transaction data in Spain, which contains contextual information about the locations and the types of business categories where transactions occurred. This allows us to validate the model, by matching our predicted activity patterns with the actually observed ones, so that it can be later applied to the cases where such information is unavailable. This extended evaluation aimed to define the applicability of the predictive model, HRBModel, taking various type of spatial and temporal factors into account.},
booktitle = {Proceedings of the Third ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
pages = {18–25},
numpages = {8},
keywords = {urban and environmental planning, human activity recognition, big data, spatial data quality and uncertainty, context recognition, bank card transactions, geo-spatial data and knowledge, statistical matching},
location = {Dallas, Texas},
series = {MobiGIS '14}
}

@inproceedings{10.1145/3269206.3271809,
author = {Chen, Lihan and Liang, Jiaqing and Xie, Chenhao and Xiao, Yanghua},
title = {Short Text Entity Linking with Fine-Grained Topics},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271809},
doi = {10.1145/3269206.3271809},
abstract = {A wide range of web corpora are in the form of short text, such as QA queries, search queries and news titles. Entity linking for these short texts is quite important. Most of supervised approaches are not effective for short text entity linking. The training data for supervised approaches are not suitable for short text and insufficient for low-resourced languages. Previous unsupervised methods are incapable of handling the sparsity and noisy problem of short text. We try to solve the problem by mapping the sparse short text to a topic space. We notice that the concepts of entities have rich topic information and characterize entities in a very fine-grained granularity. Hence, we use the concepts of entities as topics to explicitly represent the context, which helps improve the performance of entity linking for short text. We leverage our linking approach to segment the short text semantically, and build a system for short entity text recognition and linking. Our entity linking approach exhibits the state-of-the-art performance on several datasets for the realistic short text entity linking problem.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {457–466},
numpages = {10},
keywords = {short text, concepts, entity linking, fine-grained topics},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/2818314.2818330,
author = {Grillenberger, Andreas and Romeike, Ralf},
title = {Bringing the Innovations in Data Management to CS Education: An Educational Reconstruction Approach},
year = {2015},
isbn = {9781450337533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818314.2818330},
doi = {10.1145/2818314.2818330},
abstract = {This paper describes the application of the research framework educational reconstruction for investigating the field data management under a CS education perspective. Like the many other innovations in CS, Big Data and the field data management have strong influences on students' daily lives. In contrast, school does not yet sufficiently prepare students to handle the arising challenges. In this paper we will describe how we apply an educational reconstruction approach to prepare the teaching of essential data management competencies. We will summarize the main goals and principles of educational reconstruction and discuss the application of the framework to the topic data management, as well as first outcomes. Just as educational reconstruction is suitable for finding the essential aspects for teaching data management and for designing classes/courses on this topic, it also seems promising for the curricular development of other CS innovations as well.},
booktitle = {Proceedings of the Workshop in Primary and Secondary Computing Education},
pages = {88–91},
numpages = {4},
keywords = {Educational Reconstruction, CS Education, Big Data, Secondary Schools, Data Management},
location = {London, United Kingdom},
series = {WiPSCE '15}
}

@inproceedings{10.1145/2513549.2513552,
author = {Wei, Qin},
title = {Information Fusion in Taxonomic Descriptions},
year = {2013},
isbn = {9781450324151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513549.2513552},
doi = {10.1145/2513549.2513552},
abstract = {Providing a single access point to an information system from multiple documents is helpful for biodiversity researchers as it is true in many fields. It not only saves the time for going back and forth from different sources but also provides the opportunity to generate new information out of the complementary information in different sources and levels of description. This paper investigates the potential of information fusion techniques in biodiversity area since the researchers in this domain desperately need information from different sources to verify their decision. In another sense, there are massive amounts of collections in this area. It is not easy or even possible for the researcher to manually collect information from different places. The proposed system contains 4 steps: Text segmentation and Taxonomic Name Identification, Organ-level and Sub-organ level Information Extraction, Relationship Identification, and Information fusion. Information fusion is based on the seven out of the twenty-four relationships in CST (Cross-document Sentence Theory). We argue that this kind of information fusion system might not only save the researchers the time for going back and forth from different sources but also provides the opportunity to generate new information out of the complementary information in different sources and levels.},
booktitle = {Proceedings of the 2013 International Workshop on Mining Unstructured Big Data Using Natural Language Processing},
pages = {11–14},
numpages = {4},
keywords = {information fusion, natural language processing},
location = {San Francisco, California, USA},
series = {UnstructureNLP '13}
}

@inproceedings{10.1145/3285957.3285962,
author = {Li, Xiao-Tong and Zhai, Jun and Zheng, Gui-Fu and Yuan, Chang-Feng},
title = {Quality Assessment for Open Government Data in China},
year = {2018},
isbn = {9781450364898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3285957.3285962},
doi = {10.1145/3285957.3285962},
abstract = {With the development in research of government open data, the issue of data quality becomes more prominent. It's important to accurately judge the data quality before using it. The microcosmic quality assessment not only provides criteria for users to pick up dataset, but also establishes standards for providers' data quality management. In this paper, it sums up 16 types of quality problems through the investigation of three Chinese local government datasets in Beijing, Guangzhou and Harbin, and then constructs 7 quality dimensions and metrics at different granular level to score three cities. The evaluation results reflect that overall score of completeness, accuracy and consistency is low, which will affect the availability of data and mislead users to make wrong decision. On the basis of this evaluation, government could take measures to overcome the weaknesses observed in the open data quality, addressing specific lower score quality aspects.},
booktitle = {Proceedings of the 2018 10th International Conference on Information Management and Engineering},
pages = {110–114},
numpages = {5},
keywords = {open government data, quality assessment, quality dimension, quality metric, Data quality},
location = {Salford, United Kingdom},
series = {ICIME 2018}
}

@inproceedings{10.1145/3314545.3314566,
author = {Mbah, Raymond Blanch K. and Rege, Manjeet and Misra, Bhabani},
title = {Using Spark and Scala for Discovering Latent Trends in Job Markets},
year = {2019},
isbn = {9781450366342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314545.3314566},
doi = {10.1145/3314545.3314566},
abstract = {Job markets are experiencing an exponential growth in data alongside the recent explosion of big data in various domains including health, security and finance. Staying current with job market trends entails collecting, processing and analyzing huge amounts of data. A typical challenge with analyzing job listings is that they vary drastically with regards to verbiage, for instance a given job title or skill can be referred to using different words or industry jargons. As a result, it becomes incumbent to go beyond words present in job listings and carry out analysis aimed at discovering latent structures and trends in job listings. In this paper, we present a systematic approach of uncovering latent trends in job markets using big data technologies (Apache Spark and Scala) and distributed semantic techniques such as latent semantic analysis (LSA). We show how LSA can uncover patterns/relationships/trends that will otherwise remain hidden if using traditional text mining techniques that rely only on word frequencies in documents.},
booktitle = {Proceedings of the 2019 3rd International Conference on Compute and Data Analysis},
pages = {55–62},
numpages = {8},
keywords = {Latent Semantic Analysis(LSA), Big Data, Spark, Scala, Singular Value Decomposition (SVD), Natural Language Processing(NLP)},
location = {Kahului, HI, USA},
series = {ICCDA 2019}
}

@article{10.1145/3076253,
author = {Cao, Longbing},
title = {Data Science: A Comprehensive Overview},
year = {2017},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3076253},
doi = {10.1145/3076253},
abstract = {The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {43},
numpages = {42},
keywords = {data analytics, data scientist, data profession, data innovation, data DNA, computing, advanced analytics, data industry, big data analytics, informatics, statistics, data engineering, data economy, data education, Big data, data service, data science, data analysis}
}

@inproceedings{10.1145/3357729.3357742,
author = {Belghait, Fodil and April, Alain and Hamet, Pavel and Tremblay, Johanne and Desrosiers, Christian},
title = {A Large-Scale and Extensible Platform for Precision Medicine Research},
year = {2019},
isbn = {9781450372084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357729.3357742},
doi = {10.1145/3357729.3357742},
abstract = {The massive adoption of high-throughput genomics, deep sequencing technologies and big data technologies have made possible the era of precision medicine. However, the volume of data and its complexity remain important challenges for precision medicine research, hindering development in this field. The literature on precision medicine research describes a few platforms to support specific types of studies, but none of these offer researchers the level of customization required to meet their specific needs [1]. Methods: We propose to design and develop a platform able to import and integrate a very large volume of genetics, clinical, demographical and environmental data in a cloud computing infrastructure. In our previous publication, we presented an approach that can customize existing data models to fit any precision medicine research data requirement [1] and the requirement for future large-scale precision medicine platforms, in terms of data extensibility and the scalability of processing on demand. We also proposed a framework to meet the specific requirement of any precision medicine research [2]. In this paper, we describe how this new framework was implemented and trialed by the precision medicine researchers at the Centre Hospitalier Universitaire de l'Universit\'{e} de Montr\'{e}al (CHUM). Results: The data analysis simulations showed that the random forest algorithm presents better accuracy results. We obtained an F1-Score of 72% for random forest, 69% using linear regression and 62% using the neural network algorithm. Conclusion: The results suggest that the proposed precision medicine data analysis platform allows researchers to configure, prepare the analysis environment and customize the platform data model to their specific research in very optimal delays, at very low cost and with minimal technical skills.},
booktitle = {Proceedings of the 9th International Conference on Digital Public Health},
pages = {47–54},
numpages = {8},
keywords = {genomics, clinical databases, bioinformatics, precision medicine, big data},
location = {Marseille, France},
series = {DPH2019}
}

@inproceedings{10.1145/3098593.3098601,
author = {Fiadino, Pierdomenico and Ponce-Lopez, Victor and Antonio, Juan and Torrent-Moreno, Marc and D'Alconzo, Alessandro},
title = {Call Detail Records for Human Mobility Studies: Taking Stock of the Situation in the "Always Connected Era"},
year = {2017},
isbn = {9781450350549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098593.3098601},
doi = {10.1145/3098593.3098601},
abstract = {The exploitation of cellular network data for studying human mobility has been a popular research topic in the last decade. Indeed, mobile terminals could be considered ubiquitous sensors that allow the observation of human movements on large scale without the need of relying on non-scalable techniques, such as surveys, or dedicated and expensive monitoring infrastructures. In particular, Call Detail Records (CDRs), collected by operators for billing purposes, have been extensively employed due to their rather large availability, compared to other types of cellular data (e.g., signaling). Despite the interest aroused around this topic, the research community has generally agreed about the scarcity of information provided by CDRs: the position of mobile terminals is logged when some kind of activity (calls, SMS, data connections) occurs, which translates in a picture of mobility somehow biased by the activity degree of users. By studying two datasets collected by a Nation-wide operator in 2014 and 2016, we show that the situation has drastically changed in terms of data volume and quality. The increase of flat data plans and the higher penetration of "always connected" terminals have driven up the number of recorded CDRs, providing higher temporal accuracy for users' locations.},
booktitle = {Proceedings of the Workshop on Big Data Analytics and Machine Learning for Data Communication Networks},
pages = {43–48},
numpages = {6},
keywords = {human mobility, call detail records, mobile networks},
location = {Los Angeles, CA, USA},
series = {Big-DAMA '17}
}

@inproceedings{10.1145/3105831.3105834,
author = {Ara\'{u}jo, Tiago Brasileiro and Cappiello, Cinzia and Kozievitch, Nadia Puchalski and Mestre, Demetrio Gomes and Pires, Carlos Eduardo Santos and Vitali, Monica},
title = {Towards Reliable Data Analyses for Smart Cities},
year = {2017},
isbn = {9781450352208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3105831.3105834},
doi = {10.1145/3105831.3105834},
abstract = {As cities are becoming green and smart, public information systems are being revamped to adopt digital technologies. There are several sources (official or not) that can provide information related to a city. The availability of multiple sources enables the design of advanced analyses for offering valuable services to both citizens and municipalities. However, such analyses would fail if the considered data were affected by errors and uncertainties: Data Quality is one of the main requirements for the successful exploitation of the available information. This paper highlights the importance of the Data Quality evaluation in the context of geographical data sources. Moreover, we describe how the Entity Matching task can provide additional information to refine the quality assessment and, consequently, obtain a better evaluation of the reliability data sources. Data gathered from the public transportation and urban areas of Curitiba, Brazil, are used to show the strengths and effectiveness of the presented approach.},
booktitle = {Proceedings of the 21st International Database Engineering &amp; Applications Symposium},
pages = {304–308},
numpages = {5},
keywords = {Entity Matching, Data Analysis, Data Quality, Smart Cities},
location = {Bristol, United Kingdom},
series = {IDEAS '17}
}

@inproceedings{10.1145/3424978.3425146,
author = {Si, Yaqing and Qin, Siyao and Su, Jing and Wang, Mingyue},
title = {Research on Factors Influencing the Value of Data Products and Pricing Models},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425146},
doi = {10.1145/3424978.3425146},
abstract = {The article focuses on the analysis of data product value influencing factors and establishes a data product pricing model based on value factors. The research reviews the existing research on the value evaluation of data assets, and summarizes the characteristics of data products in the data product trading system based on the alliance chain [1], and then obtains factors for data product value evaluation. Combined with the dynamics and personalized requirements of data products, a value-based three-stage dynamic pricing model for data products is proposed.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {161},
numpages = {5},
keywords = {Data product, Value factor, Pricing strategy, Pricing model},
location = {Sanya, China},
series = {CSAE 2020}
}

@inproceedings{10.1145/2536714.2536720,
author = {Meurisch, Christian and Planz, Karsten and Sch\"{a}fer, Daniel and Schweizer, Immanuel},
title = {Noisemap: Discussing Scalability in Participatory Sensing},
year = {2013},
isbn = {9781450324304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536714.2536720},
doi = {10.1145/2536714.2536720},
abstract = {Environmental pollutants are an ever increasing problem in dense urban environments. To assess the effect of these pollutants, an unprecedented density of data is needed for large areas (cities, states, countries). In the past, participatory sensing has been proposed as a mean to acquire large sets of data. Since the smartphone is ubiquitous, scalability seems to be no problem anymore.In reality this far from the truth. Measuring their environment, people need to invest their time. For Android and iOS the application needs to compete with more than 700,000 other applications. Measuring large amounts of data is only possible, if we can attract large amounts of casual users.Since 2011, we have been working with and on Noisemap. Noisemap is one of many applications that uses the microphone to measure sound pressure. It then uploads the captured data to our backend, where the data is processed and visualized. Noisemap is officially available since February 2012, has been downloaded over 2,500 times, and has more than 1,000 registered users, which have collected over 500,000 unique data points in 39 countries and 58 cities. We want to share the current state of Noisemap as a multi-platform tool on Android and iOS, as well as our experience in scaling the application.},
booktitle = {Proceedings of First International Workshop on Sensing and Big Data Mining},
pages = {1–6},
numpages = {6},
keywords = {Environmental Pollution Modeling, Sensing Campaign, Participatory Sensing},
location = {Roma, Italy},
series = {SENSEMINE'13}
}

@article{10.1145/3097570,
author = {Lin, Jimmy and Milligan, Ian and Wiebe, Jeremy and Zhou, Alice},
title = {Warcbase: Scalable Analytics Infrastructure for Exploring Web Archives},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3097570},
doi = {10.1145/3097570},
abstract = {Web archiving initiatives around the world capture ephemeral Web content to preserve our collective digital memory. However, unlocking the potential of Web archives for humanities scholars and social scientists requires a scalable analytics infrastructure to support exploration of captured content. We present Warcbase, an open-source Web archiving platform that aims to fill this need. Our platform takes advantage of modern open-source “big data” infrastructure, namely Hadoop, HBase, and Spark, that has been widely deployed in industry. Warcbase provides two main capabilities: support for temporal browsing and a domain-specific language that allows scholars to interrogate Web archives in several different ways. This work represents a collaboration between computer scientists and historians, where we have engaged in iterative codesign to build tools for scholars with no formal computer science training. To provide guidance, we propose a process model for scholarly interactions with Web archives that begins with a question and proceeds iteratively through four main steps: filter, analyze, aggregate, and visualize. We call this the FAAV cycle for short and illustrate with three prototypical case studies. This article presents the current state of the project and discusses future directions.},
journal = {J. Comput. Cult. Herit.},
month = {jul},
articleno = {22},
numpages = {30},
keywords = {Big data, WARC, ARC, Apache HBase, Apache Hadoop, Apache Spark}
}

@inproceedings{10.1145/3448016.3457250,
author = {Song, Jie and He, Yeye},
title = {Auto-Validate: Unsupervised Data Validation Using Data-Domain Patterns Inferred from Data Lakes},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457250},
doi = {10.1145/3448016.3457250},
abstract = {Complex data pipelines are increasingly common in diverse applications such as BI reporting and ML modeling. These pipelines often recur regularly (e.g., daily or weekly), as BI reports need to be refreshed, and ML models need to be retrained. However, it is widely reported that in complex production pipelines, upstream data feeds can change in unexpected ways, causing downstream applications to break silently that are expensive to resolve. Data validation has thus become an important topic, as evidenced by notable recent efforts from Google and Amazon, where the objective is to catch data quality issues early as they arise in the pipelines. Our experience on production data suggests, however, that on string-valued data, these existing approaches yield high false-positive rates and frequently require human intervention. In this work, we develop a corpus-driven approach to auto-validate machine-generated data by inferring suitable data-validation "patterns'' that accurately describe the underlying data-domain, which minimizes false-positives while maximizing data quality issues caught. Evaluations using production data from real data lakes suggest that sj is substantially more effective than existing methods. Part of this technology ships as an Auto-Tag feature in Microsoft Azure Purview.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1678–1691},
numpages = {14},
keywords = {data pipelines, data quality, data validation, data lake},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3230905.3230932,
author = {Maqboul, Jaouad and Bounabat, Bouchaib},
title = {An Approach of Data-Driven Framework Alignment to Knowledge Base},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230932},
doi = {10.1145/3230905.3230932},
abstract = {When we talk about quality, we cannot do without mentioning the cost of quality and non-quality, the cost increases if the quality also increases; to maintain quality in small data is easier than huge data like big data or knowledge base.Companies tend to use the knowledge base to perfect and facilitate their work, thus satisfying the end customer, however the non-quality of these bases will penalize the company, so it is necessary to improve the quality, the question is when and why to improve quality, our proposal is based on the cost and impact of this improvement, if the impact is greater than the cost then it is recommended to improve completeness in our case study.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {40},
numpages = {5},
keywords = {completeness, java EE, complexity, Business process, knowledge Base, data quality, Knowledge, prediction, impact, framework},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@article{10.1145/3422669,
author = {Fraihat, Salam and Salameh, Walid A. and Elhassan, Ammar and Tahoun, Bushra Abu and Asasfeh, Maisa},
title = {Business Intelligence Framework Design and Implementation: A Real-Estate Market Case Study},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3422669},
doi = {10.1145/3422669},
abstract = {This article builds on previous work in the area of real-world applications of Business Intelligence (BI) technology. It illustrates the analysis, modeling, and framework design of a BI solution with high data quality to provide reliable analytics and decision support in the Jordanian real estate market. The motivation is to provide analytics dashboards to potential investors about specific segments or units in the market. The article ekxplains the design of a BI solution, including background market and technology investigation, problem domain requirements, solution architecture modeling, design and testing, and the usability of descriptive and predictive features. The resulting framework provides an effective BI solution with user-friendly market insights for investors with little or no market knowledge. The solution features predictive analytics based on established Machine Learning modeling techniques, analyzed and contrasted to select the optimum methodology and model combination for predicting market behavior to empower inexperienced users.},
journal = {J. Data and Information Quality},
month = {jun},
articleno = {10},
numpages = {16},
keywords = {predictive analytics, real estate, Business intelligence, data quality}
}

@inproceedings{10.1145/3501247.3539504,
author = {Sen, Indira and Fr\"{o}hling, Leon and Weller, Katrin},
title = {Documenting Web Data for Social Research (#DocuWeb22): A Participatory Workshop for Developing Structured and Reusable Practices},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3539504},
doi = {10.1145/3501247.3539504},
abstract = {With this half-day, in-person workshop, we attempt to collaboratively discover best practices as well as frequent pitfalls encountered when working with Web data. Participants will first be presented with different perspectives on the significance of data quality in this specific context and familiarized with existing, structured approaches for the critical reflection on and documentation of data collection processes, before being invited to share their own experiences with the collection, use and documentation of Web data. We hope to thereby inspire participants to further integrate data documentation practices into their research processes, and for us to learn from the participants’ experiences in order to improve upon existing documentation frameworks for Web data. More details of the workshop, including the planned activities can be found at&nbsp;https://frohleon.github.io/DocuWeb22/.},
booktitle = {14th ACM Web Science Conference 2022},
pages = {478–479},
numpages = {2},
keywords = {dataset documentation, web data, data quality, data collection, guidelines},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@inproceedings{10.1145/2484712.2484715,
author = {de Mendon\c{c}a, Rogers Reiche and da Cruz, S\'{e}rgio Manuel Serra and De La Cerda, Jonas F. S. M. and Cavalcanti, Maria Cl\'{a}udia and Cordeiro, Kelli Faria and Campos, Maria Luiza M.},
title = {LOP: Capturing and Linking Open Provenance on LOD Cycle},
year = {2013},
isbn = {9781450321945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484712.2484715},
doi = {10.1145/2484712.2484715},
abstract = {The Web of Data has emerged as a means to expose, share, reuse, and connect information on the Web identified by URIs using RDF as a data model, following Linked Data Principles. However, the reuse of third party data can be compromised without proper data quality assessments. In this context, important questions emerge: how can one trust on published data and links? Which manipulation, modification and integration operations have been applied to the data before its publication? What is the nature of comparisons or transformations applied to data during the interlinking process? In this scenario, provenance becomes a fundamental element. In this paper, we describe an approach for generating and capturing Linked Open Provenance (LOP) to support data quality and trustworthiness assessments, which covers preparation and format transformation of traditional data sources, up to dataset publication and interlinking. The proposed architecture takes advantage of provenance agents, orchestrated by an ETL workflow approach, to collect provenance at any specified level and also link it with its corresponding data. We also describe a real use case scenario where the architecture was implemented to evaluate the proposal.},
booktitle = {Proceedings of the Fifth Workshop on Semantic Web Information Management},
articleno = {3},
numpages = {8},
keywords = {interoperability, data quality, linked open data, ETL, linked data, provenance},
location = {New York, New York},
series = {SWIM '13}
}

@inproceedings{10.1109/MICRO.2018.00056,
author = {Lv, Yirong and Sun, Bin and Luo, Qinyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai},
title = {CounterMiner: Mining Big Performance Data from Hardware Counters},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00056},
doi = {10.1109/MICRO.2018.00056},
abstract = {Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events1. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running ina"24/7/365" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance.In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark 2 version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {613–626},
numpages = {14},
keywords = {performance counters, data mining, computer architecture, big data},
location = {Fukuoka, Japan},
series = {MICRO-51}
}

@article{10.14778/3401960.3401965,
author = {Tan, Zijing and Ran, Ai and Ma, Shuai and Qin, Sheng},
title = {Fast Incremental Discovery of Pointwise Order Dependencies},
year = {2021},
issue_date = {June 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3401960.3401965},
doi = {10.14778/3401960.3401965},
abstract = {Pointwise order dependencies (PODs) are dependencies that specify ordering semantics on attributes of tuples. POD discovery refers to the process of identifying the set Σ of valid and minimal PODs on a given data set D. In practice D is typically large and keeps changing, and it is prohibitively expensive to compute Σ from scratch every time. In this paper, we make a first effort to study the incremental POD discovery problem, aiming at computing changes ΔΣ to Σ such that Σ ⊕ ΔΣ is the set of valid and minimal PODs on D with a set ΔD of tuple insertion updates. (1) We first propose a novel indexing technique for inputs Σ and D. We give algorithms to build and choose indexes for Σ and D, and to update indexes in response to ΔD. We show that POD violations w.r.t. Σ incurred by ΔD can be efficiently identified by leveraging the proposed indexes, with a cost dependent on log(|D|). (2) We then present an effective algorithm for computing ΔΣ, based on Σ and identified violations caused by ΔD. The PODs in Σ that become invalid on D + ΔD are efficiently detected with the proposed indexes, and further new valid PODs on D + ΔD are identified by refining those invalid PODs in Σ on D + ΔD. (3) Finally, using both real-life and synthetic datasets, we experimentally show that our approach outperforms the batch approach that computes from scratch, up to orders of magnitude.},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {1669–1681},
numpages = {13}
}

@inproceedings{10.1145/3357223.3362727,
author = {Teoh, Jason and Gulzar, Muhammad Ali and Xu, Guoqing Harry and Kim, Miryung},
title = {PerfDebug: Performance Debugging of Computation Skew in Dataflow Systems},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3362727},
doi = {10.1145/3357223.3362727},
abstract = {Performance is a key factor for big data applications, and much research has been devoted to optimizing these applications. While prior work can diagnose and correct data skew, the problem of computation skew---abnormally high computation costs for a small subset of input data---has been largely overlooked. Computation skew commonly occurs in real-world applications and yet no tool is available for developers to pinpoint underlying causes.To enable a user to debug applications that exhibit computation skew, we develop a post-mortem performance debugging tool. PerfDebug automatically finds input records responsible for such abnormalities in a big data application by reasoning about deviations in performance metrics such as job execution time, garbage collection time, and serialization time. The key to PerfDebug's success is a data provenance-based technique that computes and propagates record-level computation latency to keep track of abnormally expensive records throughout the pipeline. Finally, the input records that have the largest latency contributions are presented to the user for bug fixing. We evaluate PerfDebug via in-depth case studies and observe that remediation such as removing the single most expensive record or simple code rewrite can achieve up to 16X performance improvement.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {465–476},
numpages = {12},
keywords = {data intensive scalable computing, data provenance, fault localization, big data systems, Performance debugging},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@article{10.1145/3312750,
author = {Bertino, Elisa and Kundu, Ahish and Sura, Zehra},
title = {Data Transparency with Blockchain and AI Ethics},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3312750},
doi = {10.1145/3312750},
abstract = {Providing a 360° view of a given data item especially for sensitive data is essential toward not only protecting the data and associated privacy but also assuring trust, compliance, and ethics of the systems that use or manage such data. With the advent of General Data Protection Regulation, California Data Privacy Law, and other such regulatory requirements, it is essential to support data transparency in all such dimensions. Moreover, data transparency should not violate privacy and security requirements. In this article, we put forward a vision for how data transparency would be achieved in a de-centralized fashion using blockchain technology.},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {16},
numpages = {8},
keywords = {Big data, data provenance, accountability, privacy}
}

@inproceedings{10.1145/3185768.3186307,
author = {Cabrera, Anthony M. and Faber, Clayton J. and Cepeda, Kyle and Derber, Robert and Epstein, Cooper and Zheng, Jason and Cytron, Ron K. and Chamberlain, Roger D.},
title = {DIBS: A Data Integration Benchmark Suite},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3186307},
doi = {10.1145/3185768.3186307},
abstract = {As the generation of data becomes more prolific, the amount of time and resources necessary to perform analyses on these data increases. What is less understood, however, is the data preprocessing steps that must be applied before any meaningful analysis can begin. This problem of taking data in some initial form and transforming it into a desired one is known as data integration. Here, we introduce the Data Integration Benchmarking Suite (DIBS), a suite of applications that are representative of data integration workloads across many disciplines. We apply a comprehensive characterization to these applications to better understand the general behavior of data integration tasks. As a result of our benchmark suite and characterization methods, we offer insight regarding data integration tasks that will guide other researchers designing solutions in this area.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {25–28},
numpages = {4},
keywords = {data integration, big data, data wrangling},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1145/3159450.3159483,
author = {Saltz, Jeffrey S. and Dewar, Neil I. and Heckman, Robert},
title = {Key Concepts for a Data Science Ethics Curriculum},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3159483},
doi = {10.1145/3159450.3159483},
abstract = {Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {952–957},
numpages = {6},
keywords = {computing education, ethics, big data, data science},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/2957276.2957283,
author = {Verma, Nitya and Voida, Amy},
title = {On Being Actionable: Mythologies of Business Intelligence and Disconnects in Drill Downs},
year = {2016},
isbn = {9781450342766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2957276.2957283},
doi = {10.1145/2957276.2957283},
abstract = {We present results from a case study of the use of business intelligence systems in a human services organization. We characterize four mythologies of business intelligence that informants experience as shared organizational values and are core to their trajectory towards a "culture of data": data-driven, predictive and proactive, shared accountability, and inquisitive. Yet, for each mythology, we also discuss the ways in which being actionable is impeded by a disconnect between the aggregate views of data that allows them to identify areas of focus for decision making and the desired "drill down" views of data that would allow them to understand how to act in a data-driven context. These findings contribute initial empirical evidence for the impact of business intelligence's epistemological biases on organizations and suggest implications for the design of technologies to better support data-driven decision making.},
booktitle = {Proceedings of the 19th International Conference on Supporting Group Work},
pages = {325–334},
numpages = {10},
keywords = {mythology, data analytics, big data, business intelligence},
location = {Sanibel Island, Florida, USA},
series = {GROUP '16}
}

@inproceedings{10.1145/3329189.3329204,
author = {Rahman, Md Mahbubur and Nathan, Viswam and Nemati, Ebrahim and Vatanparvar, Korosh and Ahmed, Mohsin and Kuang, Jilong},
title = {Towards Reliable Data Collection and Annotation to Extract Pulmonary Digital Biomarkers Using Mobile Sensors},
year = {2019},
isbn = {9781450361262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329189.3329204},
doi = {10.1145/3329189.3329204},
abstract = {Proliferation of sensors embedded in smartphones and smartwatches helps capture rich dataset for machine learning algorithms to extract meaningful digital bio-markers on consumer devices for monitoring disease progression and treatment response. However, development and validation of machine learning algorithms depend on gathering high fidelity sensor data and reliable ground-truth. We conduct a study, called mLungStudy, with 131 subjects with varying pulmonary conditions to collect mobile sensor data including audio, accelerometer, gyroscope using a smartphone and a smartwatch, in order to extract pulmonary biomarkers such as breathing, coughs, spirometry, and breathlessness. Our study shows that commonly used breathing ground-truth data from chestband may not always be reliable as a gold-standard. Our analysis shows that breathlessness biomarkers such as pause time and pause frequency from 2.15 minutes of audio can be as reliable as those extracted from 5 minutes' worth of speech data. This finding can be useful for future studies to trade-off between the reliability of breathlessness data and patient comfort in generating continuous speech data. Furthermore, we use crowdsourcing techniques to annotate pulmonary sound events for developing signal processing and machine learning algorithms. In this paper, we highlight several practical challenges to collect and annotate physiological data and acoustic symptoms from chronic pulmonary patients and ways to improve data quality. We show that the waveform visualization of the audio signal improves annotation quality which leads to a 6.59% increase in cough classification accuracy and a 6% increase in spirometry event classification accuracy. Findings from this study inform future studies focusing on developing explainable machine learning models to extract pulmonary digital bio-markers using mobile sensors.},
booktitle = {Proceedings of the 13th EAI International Conference on Pervasive Computing Technologies for Healthcare},
pages = {179–188},
numpages = {10},
keywords = {Cough, Digital Biomarkers, Breathing, Crowdsourced Annotation, Breathlessness, Data Quality, mHealth},
location = {Trento, Italy},
series = {PervasiveHealth'19}
}

@inproceedings{10.1145/2896338.2896341,
author = {Vandervort, David},
title = {Medical Device Data Goes to Court},
year = {2016},
isbn = {9781450342247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896338.2896341},
doi = {10.1145/2896338.2896341},
abstract = {Advances in mobile and computer technology are combining to create massive changes in the way data about human health and well-being are gathered and used. As the trend toward wearable and ubiquitous health tracking devices moves forward, the sheer quantity of new data from a wide variety of devices presents challenges for analysts. In the coming years, this data will inevitably be used in the criminal and civil justice systems. However, the tools to make full use of it are currently lacking. This paper discusses scenarios where data collected from health and fitness related devices may intersect with legal requirements such as investigations into insurance fraud or even murder. The conclusion is that there is much work to be done to enable reliable investigations. This should include at least the establishment of an organization to promote development of the field, development of cross-disciplinary education materials, and the creation of an open data bank for information sharing.},
booktitle = {Proceedings of the 6th International Conference on Digital Health Conference},
pages = {23–27},
numpages = {5},
keywords = {big data, crime, ehealth, forensics, wearables, law},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {DH '16}
}

@inproceedings{10.1145/3131672.3131694,
author = {Hossain, Syed Monowar and Hnat, Timothy and Saleheen, Nazir and Nasrin, Nusrat Jahan and Noor, Joseph and Ho, Bo-Jhang and Condie, Tyson and Srivastava, Mani and Kumar, Santosh},
title = {MCerebrum: A Mobile Sensing Software Platform for Development and Validation of Digital Biomarkers and Interventions},
year = {2017},
isbn = {9781450354592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131672.3131694},
doi = {10.1145/3131672.3131694},
abstract = {The development and validation studies of new multisensory biomarkers and sensor-triggered interventions requires collecting raw sensor data with associated labels in the natural field environment. Unlike platforms for traditional mHealth apps, a software platform for such studies needs to not only support high-rate data ingestion, but also share raw high-rate sensor data with researchers, while supporting high-rate sense-analyze-act functionality in real-time. We present mCerebrum, a realization of such a platform, which supports high-rate data collections from multiple sensors with realtime assessment of data quality. A scalable storage architecture (with near optimal performance) ensures quick response despite rapidly growing data volume. Micro-batching and efficient sharing of data among multiple source and sink apps allows reuse of computations to enable real-time computation of multiple biomarkers without saturating the CPU or memory. Finally, it has a reconfigurable scheduler which manages all prompts to participants that is burden- and context-aware. With a modular design currently spanning 23+ apps, mCerebrum provides a comprehensive ecosystem of system services and utility apps. The design of mCerebrum has evolved during its concurrent use in scientific field studies at ten sites spanning 106,806 person days. Evaluations show that compared with other platforms, mCerebrum's architecture and design choices support 1.5 times higher data rates and 4.3 times higher storage throughput, while causing 8.4 times lower CPU usage.},
booktitle = {Proceedings of the 15th ACM Conference on Embedded Network Sensor Systems},
articleno = {7},
numpages = {14},
keywords = {mobile sensor big data, mHealth, software architecture},
location = {Delft, Netherlands},
series = {SenSys '17}
}

@inproceedings{10.1145/3078081.3078109,
author = {Kir\'{a}ly, P\'{e}ter},
title = {Towards an Extensible Measurement of Metadata Quality},
year = {2017},
isbn = {9781450352659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078081.3078109},
doi = {10.1145/3078081.3078109},
abstract = {This paper describes the structure of an extensible metadata quality assessment framework, which supports multiple metadata schemas, and is flexible enough to work with new schemas. The software has to be scalable to be able to process huge amount of metadata records within a reasonable time. Fundamental requirements that need to be considered during the design of such a software are i) the abstraction of the metadata schema (in the context of the measurement process), ii) how to address distinct parts within metadata records, iii) the workflow of the measurement, iv) a common and powerful interface for the individual metrics, and v) interoperability with Java and REST APIs.},
booktitle = {Proceedings of the 2nd International Conference on Digital Access to Textual Cultural Heritage},
pages = {111–115},
numpages = {5},
keywords = {REST API, metadata quality, big data, design patterns},
location = {G\"{o}ttingen, Germany},
series = {DATeCH2017}
}

@inproceedings{10.1145/2494091.2499223,
author = {Romualdo-Suzuki, Larissa and Finkelstein, Anthony and Gann, David},
title = {A Middleware Framework for Urban Data Management},
year = {2013},
isbn = {9781450322157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494091.2499223},
doi = {10.1145/2494091.2499223},
abstract = {The domain of inquiry of this research is the collection, organization, integration, distribution and consumption of knowledge derived from urban open data, and how it can be best offered to application cities' stakeholders through a software middleware. We argue that the extensive investigation proposed in this research will contribute to a growing body of knowledge about data integration and application in smart cities, and offer opportunities to re-think an integrated urban infrastructure.},
booktitle = {Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication},
pages = {1359–1362},
numpages = {4},
keywords = {big data, software architecture, value chain., smart cities},
location = {Zurich, Switzerland},
series = {UbiComp '13 Adjunct}
}

@inproceedings{10.1145/3194696.3194700,
author = {Palacio, Ana Le\'{o}n and L\'{o}pez, \'{O}scar Pastor},
title = {Towards an Effective Medicine of Precision by Using Conceptual Modelling of the Genome: Short Paper},
year = {2018},
isbn = {9781450357340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194696.3194700},
doi = {10.1145/3194696.3194700},
abstract = {The continuous improvement in our understanding of the human genome is leading to an increasing viable and effective Precision Medicine. Its intention is to provide a personalized solution to any individual health problem. Nevertheless, three main issues must be considered to make Precision Medicine a reality: i) the understanding of the huge amount of genomic data, spread out in hundreds of genome data sources, with different formats and contents, whose semantic interoperability is a must; ii) the development of information systems intended to guide the search of relevant genomic repositories related with a disease, the identification of significant information for its prevention, diagnosis and/or treatment and its management in an efficient software platform; iii) the high variability in the quality of the publicly available information. This paper presents a conceptual framework for solving these problems by i) using a precise conceptual schema of the human genome, and ii) introducing a method to search, identify, load and adequately interpret the required data, assuring its quality during the entire process.},
booktitle = {Proceedings of the International Workshop on Software Engineering in Healthcare Systems},
pages = {14–17},
numpages = {4},
keywords = {data quality, precision medicine, conceptual modelling},
location = {Gothenburg, Sweden},
series = {SEHS '18}
}

@inproceedings{10.1145/2790755.2790774,
author = {Hamdi, Sana and Bouazizi, Emna and Faiz, Sami},
title = {A New QoS Management Approach in Real-Time GIS with Heterogeneous Real-Time Geospatial Data Using a Feedback Control Scheduling},
year = {2015},
isbn = {9781450334143},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2790755.2790774},
doi = {10.1145/2790755.2790774},
abstract = {Geographic Information System (GIS) is a computer system designed to capture, store, manipulate, analyze, manage, and present all types of spatial data. Spatial data, whether captured through remote sensors or large scale simulations becomes more and big and heterogenous. As a result, structured data and unstructured content are simultaneously accessed via an integrated user interface. The issue of real-time and heterogeneity is extremely important for taking effective decision. Thus, heterogeneous real-time spatial data management is a very active research domain nowadays. Existing research are interested in querying of real-time spatial data and their updates without taking into account the heterogeneity of real-time geospatial data. In this paper, we propose the use of the real-time Spatial Big Data and we define a new architecture called FCSA-RTSBD (Feedback Control Scheduling Architecture for Real-Time Spatial Big Data). The main objectives of this architecture are the following: take in account the heterogeneity of data, guarantee the data freshness, enhance the deadline miss ratio even in the presence of conflicts and finally satisfy the requirements of users by the improving of the quality of service (QoS).},
booktitle = {Proceedings of the 19th International Database Engineering &amp; Applications Symposium},
pages = {174–179},
numpages = {6},
keywords = {Real-Time Spatial Big Data, Quality of Service, Geographic Information System, Transaction, Heterogeneous Real-Time Geospatial Data, Feedback Control Scheduling},
location = {Yokohama, Japan},
series = {IDEAS '15}
}

@inproceedings{10.1145/3286606.3286794,
author = {Noussair, Lazrak and Jihad, Zahir and Hajar, Mousannif},
title = {Responsive Cities and Data Gathering: Challenges and Opportunities},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286794},
doi = {10.1145/3286606.3286794},
abstract = {For the last two decades, data driven cities have emerged as an efficient way of improving the city performance, enhancing life quality, and providing more choices to city planners and decision makers. A significant change in data driven cities in recent years is that much more data are collected from a variety of sources and can be processed into various forms for different stakeholders. The availability of a large amount of data can potentially lead to a revolution in city development, changing the city operation system from a conventional technology-driven system into a more powerful multifunctional data-driven intelligent system. But with more data collected the more questions raised about the optimization and space saving methods, then the quality of data collected and the efficiency of the its treatment. In this paper, we provide a survey on the data driven cities requirements, and the tools made available for the responsive cities to maintain its data.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {17},
numpages = {8},
keywords = {Data gathering, Quality of data, Big Data, Responsive cities},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.1145/3549843.3549859,
author = {Lee, Angela S.H and Bengeri, Atul and Kan, Chong Chin},
title = {Artificial Intelligence Adoption in QSR Industry},
year = {2022},
isbn = {9781450397216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549843.3549859},
doi = {10.1145/3549843.3549859},
abstract = {According to research conducted by Allied Market Research (Kale and Deshmukh, 2020), the Quick-service restaurant (QSR) industry has been enjoying significant growth over the last several years and the trend is expected to continue at least over the next five years. This is due to the agility of most QSR brands in adapting to consumer needs, quality food products and service innovation. Offering the convenience of brands reach for their consumers such as a restaurant within the reach of the customer, as well as providing online ordering and food delivery convenience.The convenience of the omnichannel digital services offered by typical QSR restaurant brands to their customer provides convenience for service delivery and at the same time provides an opportunity to collect, collate, check, consume and analyse the data to examine, explore and provide hindsight, insight and foresight for the QSR industry to be ahead of the competition.Not only are these digital customer channels a great way of providing a better customer experience, but they also help QSR operators in capturing massive data such as sales transactions, customer details, product performance, and daypart (breakfast, lunch and dinner) sales insights.With the ability to analyse and perform analytics on the data and from a data strategy perspective, it is paramount to have a robust big data platform, data warehouse and ability to consolidate data from all the IT systems. This is to enable the development of BI/dashboards to provide better business insights, such as real-time and daypart product sales. Data could also be used to develop new digital products for more positive customer engagements such as upselling and loyalty programmes or for implementing of “smart kitchen” that is able to connect various datasets for maximum kitchen efficiency and cost optimisation.Due to a lack of data science approach and machine learning capability, most QSR brands are missing this sales uplifting opportunity.To develop an effective A.I. and machine learning in the QSR business, there is a need to evaluate and select the best machine learning techniques to address the business challenges.Our objective is to explore and recommend the various machine learning techniques that can be applied to the QSR industry to gain insights and provide the necessary analysis and analytics to ensure quality service and also address changing business challenges in the competitive and ever-changing market by deploying various machine learning models for prediction and pattern analysis through classification and association approaches.},
booktitle = {Proceedings of the 2022 6th International Conference on E-Education, E-Business and E-Technology},
pages = {102–110},
numpages = {9},
keywords = {Data Management, Prediction, Association, Data Warehousing, Classification, Big Data, Information systems applications Data Mining, Data Analytics, Machine Learning},
location = {Beijing, China},
series = {ICEBT '22}
}

@article{10.1145/3131611,
author = {Chen, Qingyu and Wan, Yu and Zhang, Xiuzhen and Lei, Yang and Zobel, Justin and Verspoor, Karin},
title = {Comparative Analysis of Sequence Clustering Methods for Deduplication of Biological Databases},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3131611},
doi = {10.1145/3131611},
abstract = {The massive volumes of data in biological sequence databases provide a remarkable resource for large-scale biological studies. However, the underlying data quality of these resources is a critical concern. A particular challenge is duplication, in which multiple records have similar sequences, creating a high level of redundancy that impacts database storage, curation, and search. Biological database deduplication has two direct applications: for database curation, where detected duplicates are removed to improve curation efficiency, and for database search, where detected duplicate sequences may be flagged but remain available to support analysis.Clustering methods have been widely applied to biological sequences for database deduplication. Since an exhaustive all-by-all pairwise comparison of sequences cannot scale for a high volume of data, heuristic approaches have been recruited, such as the use of simple similarity thresholds. In this article, we present a comparison between CD-HIT and UCLUST, the two best-known clustering tools for sequence database deduplication. Our contributions include a detailed assessment of the redundancy remaining after deduplication, application of standard clustering evaluation metrics to quantify the cohesion and separation of the clusters generated by each method, and a biological case study that assesses intracluster function annotation consistency to demonstrate the impact of these factors on a practical application of the sequence clustering methods. Our results show that the trade-off between efficiency and accuracy becomes acute when low threshold values are used and when cluster sizes are large. This evaluation leads to practical recommendations for users for more effective uses of the sequence clustering tools for deduplication.},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {17},
numpages = {27},
keywords = {Deduplication, validation, databases, clustering}
}

@article{10.1145/3428080,
author = {Wang, Guang and Fang, Zhihan and Xie, Xiaoyang and Wang, Shuai and Sun, Huijun and Zhang, Fan and Liu, Yunhuai and Zhang, Desheng},
title = {Pricing-Aware Real-Time Charging Scheduling and Charging Station Expansion for Large-Scale Electric Buses},
year = {2020},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3428080},
doi = {10.1145/3428080},
abstract = {We are witnessing a rapid growth of electrified vehicles due to the ever-increasing concerns on urban air quality and energy security. Compared to other types of electric vehicles, electric buses have not yet been prevailingly adopted worldwide due to their high owning and operating costs, long charging time, and the uneven spatial distribution of charging facilities. Moreover, the highly dynamic environment factors such as unpredictable traffic congestion, different passenger demands, and even the changing weather can significantly affect electric bus charging efficiency and potentially hinder the further promotion of large-scale electric bus fleets. To address these issues, in this article, we first analyze a real-world dataset including massive data from 16,359 electric buses, 1,400 bus lines, and 5,562 bus stops. Then, we investigate the electric bus network to understand its operating and charging patterns, and further verify the necessity and feasibility of a real-time charging scheduling. With such understanding, we design busCharging, a pricing-aware real-time charging scheduling system based on Markov Decision Process to reduce the overall charging and operating costs for city-scale electric bus fleets, taking the time-variant electricity pricing into account. To show the effectiveness of busCharging, we implement it with the real-world data from Shenzhen, which includes GPS data of electric buses, the metadata of all bus lines and bus stops, combined with data of 376 charging stations for electric buses. The evaluation results show that busCharging dramatically reduces the charging cost by 23.7% and 12.8% of electricity usage simultaneously. Finally, we design a scheduling-based charging station expansion strategy to verify our busCharging is also effective during the charging station expansion process.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {nov},
articleno = {13},
numpages = {26},
keywords = {charging pattern, data driven, charging scheduling, Electric bus, MDP}
}

@inproceedings{10.1145/3485447.3512104,
author = {Lin, Zihan and Tian, Changxin and Hou, Yupeng and Zhao, Wayne Xin},
title = {Improving Graph Collaborative Filtering with Neighborhood-Enriched Contrastive Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512104},
doi = {10.1145/3485447.3512104},
abstract = {Recently, graph collaborative filtering methods have been proposed as an effective recommendation approach, which can capture users’ preference over items by modeling the user-item interaction graphs. Despite the effectiveness, these methods suffer from data sparsity in real scenarios. In order to reduce the influence of data sparsity, contrastive learning is adopted in graph collaborative filtering for enhancing the performance. However, these methods typically construct the contrastive pairs by random sampling, which neglect the neighboring relations among users&nbsp;(or items) and fail to fully exploit the potential of contrastive learning for recommendation. To tackle the above issue, we propose a novel contrastive learning approach, named Neighborhood-enriched Contrastive Learning, named NCL, which explicitly incorporates the potential neighbors into contrastive pairs. Specifically, we introduce the neighbors of a user&nbsp;(or an item) from graph structure and semantic space respectively. For the structural neighbors on the interaction graph, we develop a novel structure-contrastive objective that regards users&nbsp;(or items) and their structural neighbors as positive contrastive pairs. In implementation, the representations of users&nbsp;(or items) and neighbors correspond to the outputs of different GNN layers. Furthermore, to excavate the potential neighbor relation in semantic space, we assume that users with similar representations are within the semantic neighborhood, and incorporate these semantic neighbors into the prototype-contrastive objective. The proposed NCL can be optimized with EM algorithm and generalized to apply to graph collaborative filtering methods. Extensive experiments on five public datasets demonstrate the effectiveness of the proposed NCL, notably with 26% and 17% performance gain over a competitive graph collaborative filtering base model on the Yelp and Amazon-book datasets, respectively. Our implementation code is available at: https://github.com/RUCAIBox/NCL.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2320–2329},
numpages = {10},
keywords = {Contrastive Learning, Recommender System, Graph Neural Network, Collaborative Filtering},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3437963.3441747,
author = {Wu, Jinze and Huang, Zhenya and Liu, Qi and Lian, Defu and Wang, Hao and Chen, Enhong and Ma, Haiping and Wang, Shijin},
title = {Federated Deep Knowledge Tracing},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441747},
doi = {10.1145/3437963.3441747},
abstract = {Knowledge tracing is a fundamental task in intelligent education for tracking the knowledge states of students on necessary concepts. In recent years, Deep Knowledge Tracing (DKT) utilizes recurrent neural networks to model student learning sequences. This approach has achieved significant success and has been widely used in many educational applications. However, in practical scenarios, it tends to suffer from the following critical problems due to data isolation: 1) Data scarcity. Educational data, which is usually distributed across different silos (e.g., schools), is difficult to gather. 2) Different data quality. Students in different silos have different learning schedules, which results in unbalanced learning records, meaning that it is necessary to evaluate the learning data quality independently for different silos. 3) Data incomparability. It is difficult to compare the knowledge states of students with different learning processes from different silos. Inspired by federated learning, in this paper, we propose a novel Federated Deep Knowledge Tracing (FDKT) framework to collectively train high-quality DKT models for multiple silos. In this framework, each client takes charge of training a distributed DKT model and evaluating data quality by leveraging its own local data, while a center server is responsible for aggregating models and updating the parameters for all the clients. In particular, in the client part, we evaluate data quality incorporating different education measurement theories, and we construct two quality-oriented implementations based on FDKT, i.e., FDKTCTT and FDKTIRT-where the means of data quality evaluation follow Classical Test Theory and Item Response Theory, respectively. Moreover, in the server part, we adopt hierarchical model interpolation to uptake local effects for model personalization. Extensive experiments on real-world datasets demonstrate the effectiveness and superiority of the FDKT framework.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {662–670},
numpages = {9},
keywords = {data isolation, intelligent education, data quality evaluation, knowledge tracing, federated learning},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/2659532.2659594,
author = {Jaakkola, Hannu and M\"{a}kinen, Timo and Etel\"{a}aho, Anna},
title = {Open Data: Opportunities and Challenges},
year = {2014},
isbn = {9781450327534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2659532.2659594},
doi = {10.1145/2659532.2659594},
abstract = {Open data is seen as a promising source of new business, especially in the SME sector, in the form of new products, services and innovative solutions. High importance is seen also in fostering citizens' participation in political and social life and increasing the transparency of public authorities. The forerunners of the open data movement in the public sector are the USA and the UK, which started to open their public data resources in 2009. The first European Union open data related directive was drawn up as early as 2003; however progress in putting the idea into practice has been slow and adoptions by the wider member states are placed in the early 2010s. The beneficial use of open data in real applications has progressed hand in hand with the improvement of other ICT-related technologies. The (raw) data itself has no high value. The economic value comes from a balanced combination of high quality open (data) resources combined with the related value chain. This paper builds up a "big picture" of the role of open data in current society. The approach is analytical and it clarifies the topic from the viewpoints of both opportunities and challenges. The paper covers both general aspects related to open data and results of the research and regional development project conducted by the authors.},
booktitle = {Proceedings of the 15th International Conference on Computer Systems and Technologies},
pages = {25–39},
numpages = {15},
keywords = {big data, public data, data analysis, open data, networking},
location = {Ruse, Bulgaria},
series = {CompSysTech '14}
}

@inproceedings{10.1145/3523111.3523119,
author = {Meng, Xianyu and Ma, Liangli and Zhou, Yingxue},
title = {Analysis and Example Implementation of Data Visualization Technology},
year = {2022},
isbn = {9781450395670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523111.3523119},
doi = {10.1145/3523111.3523119},
abstract = {Abstract: The development of visualization technology and data mining technology provides powerful means and tools for the visual analysis of diversified data. In this era of information and data explosion, various information resources are very rich, but due to the large amount of data, the characteristics of the data are not so obvious. This requires us to study the corresponding methods and means to extract the characteristics of data. Data visualization technology has developed rapidly under this background. This paper introduces the basic theory of data visualization technology, and selects the GDP data of the world's major economies over the years as a visualization example.},
booktitle = {2022 the 5th International Conference on Machine Vision and Applications (ICMVA)},
pages = {56–60},
numpages = {5},
keywords = {Data mining, Data analysis, Data space, Big data, Data visualization},
location = {Singapore, Singapore},
series = {ICMVA 2022}
}

@inproceedings{10.1145/3503928.3503930,
author = {Zeng, Xian and Han, Minglei and Li, Ning and Liu, Peng},
title = {Research on Real-Time Data Warehouse Technology for Sea Battlefield},
year = {2022},
isbn = {9781450385220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503928.3503930},
doi = {10.1145/3503928.3503930},
abstract = {Aiming at the data governance problems in the sea battlefield, this paper proposes a real-time data warehouse construction method for naval battlefields, which realizes the functions of storage, analysis and mining of battle data. This paper completes the construction of the data warehouse from the aspects of real-time data life cycle, real-time data application scenarios, data warehouse real-time safeguard measures, and data warehouse theme design. It can effectively provide data support for naval combat forces and provide auxiliary decision-making for commanders.},
booktitle = {2021 the 6th International Conference on Information Systems Engineering},
pages = {13–20},
numpages = {8},
keywords = {Big Data, Sea Battlefield, Real-time, Data Warehouse},
location = {Shanghai, China},
series = {ICISE 2021}
}

@inproceedings{10.1145/3411764.3445518,
author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
title = {“Everyone Wants to Do the Model Work, Not the Data Work”: Data Cascades in High-Stakes AI},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445518},
doi = {10.1145/3411764.3445518},
abstract = {AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades—compounding events causing negative, downstream effects from data issues—triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {39},
numpages = {15},
keywords = {data cascades, Uganda, data collectors, Data, high-stakes AI, ML, application-domain experts, Nigeria, raters, developers, India, AI, Kenya, data politics, data quality, Ghana, USA},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.5555/2857070.2857186,
author = {Blake, Catherine and Souden, Maria and Anderson, Caryn L. and Twidale, Michael and Stelmack, Jenifer E.},
title = {Online Question Answering Practices to Support Healthcare Data Re-Use},
year = {2015},
isbn = {087715547X},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Institutional data collection practices inevitably evolve over time, especially in a distributed clinical setting. Clinical and administrative data can improve health and healthcare, but only if researchers ensure that the data is well-aligned to their reuse goals and that they have adequately accounted for changes in data collection practices over time. Our goal is to understand information behaviors of health services data users as they bridge the gap between the historical data and their intended data reuse goals. This project leverages more than a decade of listserv posts related to the use of clinical and administrative data by US Department of Veterans Affairs (VA) employees, providing longitudinal insight into data reuse practices in both research and operational settings. In this paper we report the results of a pilot study that highlighted questions raised in the use of data and the knowledge engaged to answer them.},
booktitle = {Proceedings of the 78th ASIS&amp;T Annual Meeting: Information Science with Impact: Research in and for the Community},
articleno = {116},
numpages = {4},
keywords = {social question answering, big data, forums, health, communities of practice},
location = {St. Louis, Missouri},
series = {ASIST '15}
}

@inproceedings{10.1145/3047273.3047296,
author = {Choudhury, Pranab Ranjan and Behera, Manoj Kumar},
title = {Using Administrative Data for Monitoring and Improving Land Policy and Governance in India},
year = {2017},
isbn = {9781450348256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3047273.3047296},
doi = {10.1145/3047273.3047296},
abstract = {Demands for production and dissemination of reliable data is growing with increasing demand from public policies to monitor, compare and improve global and national developmental status and targets. Implementation of intentionally agreed commitments like Millennium Development Goals (MDGs), Sustainable development Goals (SDGs) are influencing data production and availability, and the development of national statistical capacities. They also trigger challenges and opportunities in production of internationally comparable data to induce fair comparability among nations. Being a signatory to major international treaties, India has considerably improved data production, accessibility and availability over the years to ensure proper alignment of national level statistics and induce international comparison. However, very little efforts have been made to assess India's progress around data production and dissemination around growingly important land governance. This assessment attempts to identify key opportunities and challenges at the country level to improve data availability, access, timeliness and quality.India has made many progressive reforms around land laws and institutions to make land governance more inclusive and equitable; however its assessment with respect to global best practices through World Bank's Land Governance Assessment Framework (LGAF) indicate the need of improvements around different land dimensions. Movement towards good land governance outcomes is incumbent upon robust and regular monitoring mechanism of land indicators across spatial (viz. administrative boundaries, land being a state subject in India) and temporal scales.India has traditions of collecting, maintaining and reporting land information through nation-wide surveys, census, administrative and judicial reports/ databases. Its flagship program Digital India Land Record Modernization Program (DILRMP), has been supporting universal digitization of spatial and textual land records by the states. Together, these administrative and survey-derived datasets provide seamless opportunity for routine generation of data on key land indicators at low cost on a regularbasis. Land is a state subject in India. Monitoring and reporting land-indicators at state levels would help in systematically discovering and identifying good practice that can then be documented and disseminated across states, manage change, and gradually move towards a more performance-based approach to improving land governance in India. However, there have been lack of institutionalized attempts, so far, to report land-indicators at national scale.We have tried to assess the state of data in India, particularly to track and report two critical land governance indicators viz. women land rights and forest rights, critical to ensure equity and sustainability in terms of public policy. With UN's SDG, defining similar indicators, we also attempt aligning them around SDG indicators. Status of these two parameters were analyzed using nation-wide datasets collecting whole population data, through legitimate institutions following robust processes and reporting them open access.Census (human population) data and Forest Survey of India (FSI) data were used to assess village-wise forest areas eligible for recognition of rights under India's historic Forest Rights Act, 2005. Using the FSI data and meta-analysis of census data, we calculated the estimated population (150 million including 90 million tribal) living in villages that have forest land within administrative revenue boundaries, potential area (40 million ha) that can be recognized under FRA and number of villages (0.17 million) that are eligible to initiate the claim. These data were made available across administrative boundaries of state, district and village, providing opportunities for relevant Government Ministries at Central and State level and civil society to expedite the forest rights recognition under India's largest land reform process.In order to assess women's land rights (WLR) in India in the context of the SDGs, after examining the existing data sets, we used Agricultural Census data, conducted by Government of India every fifth year following the guidelines of World Census on Agriculture (WCA). Using Agricultural census data, we have developed atlas of women land rights (based on operational holdings) in India with state and district wise granularity with further disaggregation across ethnicity (caste) and other socio-economic parameters. The study also attempted to analyze the link between the inter-regional and temporal variability of WLR and relevant policies and legal-institutional frameworks among the states to see if the correlations can better inform public policy and also induce healthy competition among states to appreciate and follow best practices. This paper presents the process, methodology and results of the data-analysis for these two land indicators while delving into the scope and challenges of dealing with existing and upcoming big datasets in India to report the land governance indicators and the potential policy spinoffs.},
booktitle = {Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance},
pages = {127–135},
numpages = {9},
keywords = {Big Data, Women Land Rights, Forest Rights, India, SDGs},
location = {New Delhi AA, India},
series = {ICEGOV '17}
}

@inproceedings{10.1145/3356991.3365474,
author = {Palumbo, Rachel and Thompson, Laura and Thakur, Gautam},
title = {SONET: A Semantic Ontological Network Graph for Managing Points of Interest Data Heterogeneity},
year = {2019},
isbn = {9781450369602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356991.3365474},
doi = {10.1145/3356991.3365474},
abstract = {Scalability, standardization, and management are important issues when working with very large Volunteered Geographic Information (VGI). VGI is a rich and valuable source of Points of Interest (POI) information, but its inherent heterogeneity in content, structure, and scale across sources present major challenges for interlinking data sources for analysis. To be useful at scale, the raw information needs to be transformed into a standardized schema that can be easily and reliably used by data analysts. In this work, we tackle the problem of unifying POI categories (e.g. restaurants, temple, and hotel) across multiple data sources to aid in improving land use maps and population distribution estimation as well as support data analysts wishing to fuse multiple data sources with the OpenStreetMap (OSM) mapping platform or working with projects that are already configured in the OSM schema and wish to add additional sources of information. Graph theory and its implementation through the SONET graph database, provides a programmatic way to organize, store, and retrieve standardized POI categories at multiple levels of abstraction. Additionally, it addresses category heterogeneity across data sources by standardizing and managing categories in a way that makes cross-domain analysis possible.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Geospatial Humanities},
articleno = {6},
numpages = {6},
keywords = {big data, graph database, points of interest, ontology, openstreetmap},
location = {Chicago, Illinois},
series = {GeoHumanities '19}
}

@inproceedings{10.1145/2882903.2912574,
author = {Chu, Xu and Ilyas, Ihab F. and Krishnan, Sanjay and Wang, Jiannan},
title = {Data Cleaning: Overview and Emerging Challenges},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2912574},
doi = {10.1145/2882903.2912574},
abstract = {Detecting and repairing dirty data is one of the perennial challenges in data analytics, and failure to do so can result in inaccurate analytics and unreliable decisions. Over the past few years, there has been a surge of interest from both industry and academia on data cleaning problems including new abstractions, interfaces, approaches for scalability, and statistical techniques. To better understand the new advances in the field, we will first present a taxonomy of the data cleaning literature in which we highlight the recent interest in techniques that use constraints, rules, or patterns to detect errors, which we call qualitative data cleaning. We will describe the state-of-the-art techniques and also highlight their limitations with a series of illustrative examples. While traditionally such approaches are distinct from quantitative approaches such as outlier detection, we also discuss recent work that casts such approaches into a statistical estimation framework including: using Machine Learning to improve the efficiency and accuracy of data cleaning and considering the effects of data cleaning on statistical analysis.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2201–2206},
numpages = {6},
keywords = {statistical cleaning, data quality, data cleaning, sampling, integrity constraints},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3378539.3393864,
author = {Birkel, Hendrik and Kopyto, Matthias and Lutz, Corinna},
title = {Challenges of Applying Predictive Analytics in Transport Logistics},
year = {2020},
isbn = {9781450371308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378539.3393864},
doi = {10.1145/3378539.3393864},
abstract = {The field of Predictive Analytics (PA) provides the possibility to utilize large amounts of data to improve forecasting, data-driven decision-making, and competitive advantage. Especially the transport logistics sector, which is characterized by high business-related uncertainties, time-sensitivity, and volatility, highly benefits from accurate resource and production planning. While success factors and framework conditions of applying PA are well-investigated on a theoretical SCM level, findings on internal and external challenges of transport logistics organizations remain scarce. Therefore, based on a multiple case approach, this study offers in-depth insights into six real-world cases of freight forwarders, ocean carriers, and air carriers. The results uncover both internal and external challenges. From the internal perspective, the biggest challenges are related to the technical implementation including the acquisition of globally generated, internal and external data and its harmonization. In addition, stakeholder management and target setting impede the development of PA. Regarding external challenges, relational and external conditions hamper the application. Therefore, especially actions of third-party institutions in terms of standardization and security enhancements are required. This study contributes to the existing literature in various ways as the systematic identification addresses real-world issues of PA in the neglected but crucial area of transport logistics, discussing urgent research needs and highlighting potential solutions. Additionally, the results offer valuable guidance for managers when implementing PA in transport logistics.},
booktitle = {Proceedings of the 2020 on Computers and People Research Conference},
pages = {144–151},
numpages = {8},
keywords = {transport logistics, supply chain management, predictive analytics, challenges, big data},
location = {Nuremberg, Germany},
series = {SIGMIS-CPR'20}
}

@article{10.1145/3436817,
author = {Harley, Kelsey and Cooper, Rodney},
title = {Information Integrity: Are We There Yet?},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3436817},
doi = {10.1145/3436817},
abstract = {The understanding and promotion of integrity in information security has traditionally been underemphasized or even ignored. From implantable medical devices and electronic voting to vehicle control, the critical importance of information integrity to our well-being has compelled review of its treatment in the literature. Through formal information flow models, the data modification view, and the relationship to data quality, information integrity will be surveyed. Illustrations are given for databases and information trustworthiness. Integrity protection is advancing but lacks standardization in terminology and application. Integrity must be better understood, and pursued, to achieve devices and systems that are beneficial and safe for the future.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {33},
numpages = {35},
keywords = {Biba’s model, Integrity, data quality, Clark-Wilson model, quality dimension, information integrity, noninterference, information flow, information trustworthiness, information security, security requirements, quality assessment, information quality}
}

@inproceedings{10.1145/2755492.2755494,
author = {Huang, Qunying and Cao, Guofeng and Wang, Caixia},
title = {From Where Do Tweets Originate? A GIS Approach for User Location Inference},
year = {2014},
isbn = {9781450331401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2755492.2755494},
doi = {10.1145/2755492.2755494},
abstract = {A number of natural language processing and text-mining algorithms have been developed to extract the geospatial cues (e.g., place names) to infer locations of content creators from publicly available information, such as text content, online social profiles, and the behaviors or interactions of users from social networks. These studies, however, can only successfully infer user locations at city levels with relatively decent accuracy, while much higher resolution is required for meaningful spatiotemporal analysis in geospatial fields. Additionally, geographical cues exploited by current text-based approaches are hidden in the unreliable, unstructured, informal, ungrammatical, and multilingual data, and therefore are hard to extract and make meaningful correctly. Instead of using such hidden geographic cues, this paper develops a GIS approach that can infer the true origin of tweets down to the zip code level by using and mining spatial (geo-tags) and temporal (timestamps when a message was posted) information recorded on user digital footprints. Further, individual major daily activity zones and mobility can be successfully inferred and predicted. By integrating GIS data and spatiotemporal clustering methods, this proposed approach can infer individual daily physical activity zones with spatial resolution as high as 20 m by 20 m or even higher depending on the number of digit footprints collected for social media users. The research results with detailed spatial resolution are necessary and useful for various applications such as human mobility pattern analysis, business site selection, disease control, or transportation systems improvement.},
booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on Location-Based Social Networks},
pages = {1–8},
numpages = {8},
keywords = {spatiotemporal clustering, spatial clustering, geography, big data, human mobility},
location = {Dallas/Fort Worth, Texas},
series = {LBSN '14}
}

@article{10.1145/3345551,
author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Large-Scale Semantic Integration of Linked Data: A Survey},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3345551},
doi = {10.1145/3345551},
abstract = {A large number of published datasets (or sources) that follow Linked Data principles is currently available and this number grows rapidly. However, the major target of Linked Data, i.e., linking and integration, is not easy to achieve. In general, information integration is difficult, because (a) datasets are produced, kept, or managed by different organizations using different models, schemas, or formats, (b) the same real-world entities or relationships are referred with different URIs or names and in different natural languages,<!--?brk?-->(c) datasets usually contain complementary information, (d) datasets can contain data that are erroneous, out-of-date, or conflicting, (e) datasets even about the same domain may follow different conceptualizations of the domain, (f) everything can change (e.g., schemas, data) as time passes. This article surveys the work that has been done in the area of Linked Data integration, it identifies the main actors and use cases, it analyzes and factorizes the integration process according to various dimensions, and it discusses the methods that are used in each step. Emphasis is given on methods that can be used for integrating several datasets. Based on this analysis, the article concludes with directions that are worth further research.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {103},
numpages = {40},
keywords = {Data integration, RDF, semantic web, data discovery, big data}
}

@inproceedings{10.1145/3433996.3434008,
author = {Guo, Xusheng and Liang, Likeng and Liu, Yuanxia and Weng, Heng and Hao, Tianyong},
title = {The Construction of a Diabetes-Oriented Frequently Asked Question Corpus for Automated Question-Answering Services},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434008},
doi = {10.1145/3433996.3434008},
abstract = {In recent years, the prevalence of diabetes has been increasing rapidly worldwide. With the advancement of information technology, automated question-answering services for healthcare, which are commonly based on annotated corpus in health domain, have positive effects on health knowledge spread and daily health management for high-risk populations. This paper proposes to construct a large scale diabetes corpus of frequently-asked questions for automated question-answering services and evaluations. Concentrating on the characteristics of diabetes-related factors that reflect conditions of diabetes, this work establishes an annotated dataset containing professional question &amp; answer pairs about diabetes and their annotated question target categories. The corpus is applicable for various question-answering applications, supporting users to retrieve needed information, arrange diets, adhere to scientific medication as well as prevent and control disease complications.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {60–66},
numpages = {7},
keywords = {corpus construction, frequently-asked questions, visualization, Diabetes},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/2882903.2899414,
author = {Agrawal, Divy and Ba, Lamine and Berti-Equille, Laure and Chawla, Sanjay and Elmagarmid, Ahmed and Hammady, Hossam and Idris, Yasser and Kaoudi, Zoi and Khayyat, Zuhair and Kruse, Sebastian and Ouzzani, Mourad and Papotti, Paolo and Quiane-Ruiz, Jorge-Arnulfo and Tang, Nan and Zaki, Mohammed J.},
title = {Rheem: Enabling Multi-Platform Task Execution},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2899414},
doi = {10.1145/2882903.2899414},
abstract = {Many emerging applications, from domains such as healthcare and oil &amp; gas, require several data processing systems for complex analytics. This demo paper showcases system, a framework that provides multi-platform task execution for such applications. It features a three-layer data processing abstraction and a new query optimization approach for multi-platform settings. We will demonstrate the strengths of system by using real-world scenarios from three different applications, namely, machine learning, data cleaning, and data fusion.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {2069–2072},
numpages = {4},
keywords = {data analytics, cross-platform execution, big data},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@article{10.1145/3447513,
author = {Deng, Song and Chen, Fulin and Dong, Xia and Gao, Guangwei and Wu, Xindong},
title = {Short-Term Load Forecasting by Using Improved GEP and Abnormal Load Recognition},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3447513},
doi = {10.1145/3447513},
abstract = {Load forecasting in short term is very important to economic dispatch and safety assessment of power system. Although existing load forecasting in short-term algorithms have reached required forecast accuracy, most of the forecasting models are black boxes and cannot be constructed to display mathematical models. At the same time, because of the abnormal load caused by the failure of the load data collection device, time synchronization, and malicious tampering, the accuracy of the existing load forecasting models is greatly reduced. To address these problems, this article proposes a Short-Term Load Forecasting algorithm by using Improved Gene Expression Programming and Abnormal Load Recognition (STLF-IGEP_ALR). First, the Recognition algorithm of Abnormal Load based on Probability Distribution and Cross Validation is proposed. By analyzing the probability distribution of rows and columns in load data, and using the probability distribution of rows and columns for cross-validation, misjudgment of normal load in abnormal load data can be better solved. Second, by designing strategies for adaptive generation of population parameters, individual evolution of populations and dynamic adjustment of genetic operation probability, an Improved Gene Expression Programming based on Evolutionary Parameter Optimization is proposed. Finally, the experimental results on two real load datasets and one open load dataset show that compared with the existing abnormal data detection algorithms, the algorithm proposed in this article have higher advantages in missing detection rate, false detection rate and precision rate, and STLF-IGEP_ALR is superior to other short-term load forecasting algorithms in terms of the convergence speed, MAE, MAPE, RSME, and R2.},
journal = {ACM Trans. Internet Technol.},
month = {jul},
articleno = {95},
numpages = {28},
keywords = {Gene expression programming, abnormal load recognition, power load forecasting, probability distribution, adaptive evolution}
}

@inproceedings{10.5555/2873021.2873031,
author = {Ferguson, Holly T. and Vardeman, Charles F. and Buccellato, Aimee P. C.},
title = {Capturing an Architectural Knowledge Base Utilizing Rules Engine Integration for Energy and Environmental Simulations},
year = {2015},
isbn = {9781510801042},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The era of "Big Data" presents new challenges and opportunities to impact how the built environment is designed and constructed. Modern design tools and material databases should be more scalable, reliable, and accessible to take full advantage of the quantity of available building data. New approaches providing well-structured information can lead to robust decision support for architectural simulations earlier in the design process; rule-based decision engines and knowledge bases are the link between current data and useful decision frameworks. Integrating distributed API-based systems means that material data silos existing in modern tools can become enriched and extensible for future use with additional data from building documents, other databases, and the minds of design professionals. The PyKE rules engine extension to the Green Scale (GS) Tool improves material searches, creates the opportunity for incorporating additional rules via a REST interface, and enables integration with the Semantic Web via Linked Data principles.},
booktitle = {Proceedings of the Symposium on Simulation for Architecture &amp; Urban Design},
pages = {67–74},
numpages = {8},
keywords = {REST, sustainable data, verification, algorithms, machine learning, OWL, PyKE, SWIRL, experimentation, ontological knowledge engine, reliability, SPARAQL, linked data, standardization, SPIN, knowledge based rules, design, HCI, RIF, semantic web, big data, green scale tool, expert systems, performance},
location = {Alexandria, Virginia},
series = {SimAUD '15}
}

@inproceedings{10.1145/3478905.3478920,
author = {Fei, Yiming and Yuan, Xiaoyue and Ren, Mengmeng and Fan, Shuhai},
title = {Research on Horizontal Integration Scheme for Mass Customization Data Quantity and Quality Problem: Horizontal Integration Scheme for MC},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478920},
doi = {10.1145/3478905.3478920},
abstract = {To solve the problem of Mass Customization Data Quantity and Quality Problem, a Horizontal Integration Scheme of MC is proposed. By using LiDAR technology to scan and identify parts information, the machining route and required parts of the workpiece are automatically planned by comparing and matching with the documents using STEP-NC standard, to realize the efficient acquisition and utilization of MC data and ensure the automation of enterprise production.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {69–73},
numpages = {5},
keywords = {Mass Customization, Horizontal Integration, LiDAR Camera Technology, Data Quality},
location = {Shanghai, China},
series = {DSIT 2021}
}

@inproceedings{10.1145/3341105.3373989,
author = {Alaa, Mostafa and Bolock, Alia El and Abas, Mostafa and Abdennadher, Slim and Herbert, Cornelia},
title = {AppGen: A Framework for Automatic Generation of Data Collection Apps},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373989},
doi = {10.1145/3341105.3373989},
abstract = {Data, and its collection, is one core aspect of technology and research, nowadays. Various scientific disciplines are interested in collecting human data in practically any context (at home, at work, during leisure time). For example, experts from the field of Psychology design studies for reliable and valid data collection in the laboratory and in the wild. We propose a generic platform for data-collection software development to be used by scientists without a programming background. This is done by adapting a basic Unity project through a configuration file provided by the platform users through an easy to use user interface. The scientific user can adapt and rearrange pre-defined data collection modules targeting a desired research question, implement it as application within the data collection platform and use and manage the application for data collection and later data analysis. As a proof of concept, the platform was embedded with build-in application modules for wide-spread Psychology data collection experiments. The versatility of the platform was tested by creating three diverse prototypical applications. Finally, the usability of the proposed platform evaluated using the System Usability Scale obtained high usability results. The robust module-based nature of the platform architecture makes is possible to create a various range of of psychologically-proven applications with different features to be decided by the researcher. This holds true for both the development phase of the applications, as well as, for after deployment.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1906–1913},
numpages = {8},
keywords = {big data, character computing, data collection, psychology, software framework},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3173574.3173710,
author = {Bowyer, Alex and Montague, Kyle and Wheater, Stuart and McGovern, Ruth and Lingam, Raghu and Balaam, Madeline},
title = {Understanding the Family Perspective on the Storage, Sharing and Handling of Family Civic Data},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173710},
doi = {10.1145/3173574.3173710},
abstract = {Across social care, healthcare and public policy, enabled by the "big data" revolution (which has normalized large-scale data-based decision-making), there are moves to "join up" citizen databases to provide care workers with holistic views of families they support. In this context, questions of personal data privacy, security, access, control and (dis-)empowerment are critical considerations for system designers and policy makers alike. To explore the family perspective on this landscape of what we call Family Civic Data, we carried out ethnographic interviews with four North-East families. Our design-game-based interviews were effective for engaging both adults and children to talk about the impact of this dry, technical topic on their lives. Our findings, delivered in the form of design guidelines, show support for dynamic consent: families would feel most empowered if involved in an ongoing co-operative relationship with state welfare and civic authorities through shared interaction with their data.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {social care, data privacy, ethnographic interviews, ubicomp, family design games, healthcare, family research, family, personal data, boundary objects, civic data, dynamic consent, user-centered design, design games, data security, big data, data sharing},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3219819.3219916,
author = {Xin, SHEN and Yang, Hongxia and Xian, Weizhao and Ester, Martin and Bu, Jiajun and Wang, Zhongyao and Wang, Can},
title = {Mobile Access Record Resolution on Large-Scale Identifier-Linkage Graphs},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219916},
doi = {10.1145/3219819.3219916},
abstract = {The e-commerce era is witnessing a rapid increase of mobile Internet users. Major e-commerce companies nowadays see billions of mobile accesses every day. Hidden in these records are valuable user behavioral characteristics such as their shopping preferences and browsing patterns. And, to extract these knowledge from the huge dataset, we need to first link records to the corresponding mobile devices. This Mobile Access Records Resolution (MARR) problem is confronted with two major challenges: (1) device identifiers and other attributes in access records might be missing or unreliable; (2) the dataset contains billions of access records from millions of devices. To the best of our knowledge, as a novel challenge industrial problem of mobile Internet, no existing method has been developed to resolve entities using mobile device identifiers in such a massive scale. To address these issues, we propose a SParse Identifier-linkage Graph (SPI-Graph) accompanied with the abundant mobile device profiling data to accurately match mobile access records to devices. Furthermore, two versions (unsupervised and semi-supervised) of Parallel Graph-based Record Resolution (PGRR) algorithm are developed to effectively exploit the advantages of the large-scale server clusters comprising of more than 1,000 computing nodes. We empirically show superior performances of PGRR algorithms in a very challenging and sparse real data set containing 5.28 million nodes and 31.06 million edges from 2.15 billion access records compared to other state-of-the-arts methodologies.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {886–894},
numpages = {9},
keywords = {mobile access record resolution, graph algorithms, scalable algorithms, big data},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/2910674.2935861,
author = {Bj\"{o}rk, Kaj-Mikael and Eirola, Emil and Miche, Yoan and Lendasse, Amaury},
title = {A New Application of Machine Learning in Health Care},
year = {2016},
isbn = {9781450343374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910674.2935861},
doi = {10.1145/2910674.2935861},
abstract = {In our ever more complex world, the field of analytics has dramatically increased its importance. Gut feeling is no longer sufficient in decision making, but intuition has to be combined with support from the huge amount of data available today. Even if the amount of data is enormous, the quality of the data is not always good. Problems arise in at least two situations: i) the data is imprecise by nature and ii) the data is incomplete (or there are missing parts in the data set). Both situations are problematic and need to be addressed appropriately. If these problems are solved, applications are to be found in various interesting fields. We aim at achieving significant methodology development as well as creative solutions in the domain of medicine, information systems and risk management. This paper sets focus especially on missing data problems in the field of medicine when presenting a new project in its very first phase.},
booktitle = {Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {49},
numpages = {4},
keywords = {Missing values, Huntington's disease, Health care, Big Data, Machine Learning},
location = {Corfu, Island, Greece},
series = {PETRA '16}
}

@inproceedings{10.1145/3468791.3468841,
author = {Bechny, Michal and Sobieczky, Florian and Zeindl, J\"{u}rgen and Ehrlinger, Lisa},
title = {Missing Data Patterns: From Theory to an Application in the Steel Industry},
year = {2021},
isbn = {9781450384131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468791.3468841},
doi = {10.1145/3468791.3468841},
abstract = {Missing data (MD) is a prevalent problem and can negatively affect the trustworthiness of data analysis. In industrial use cases, faulty sensors or errors during data integration are common causes for systematically missing values. The majority of MD research deals with imputation, i.e., the replacement of missing values with “best guesses”. Most imputation methods require missing values to occur independently, which is rarely the case in industry. Thus, it is necessary to identify missing data patterns (i.e., systematically missing values) prior to imputation (1) to understand the cause of the missingness, (2) to gain deeper insight into the data, and (3) to choose the proper imputation technique. However, in literature, there is a wide varity of MD patterns without a common formalization. In this paper, we introduce the first formal definition of MD patterns. Building on this theory, we developed a systematic approach on how to automatically detect MD patterns in industrial data. The approach has been developed in cooperation with voestalpine Stahl GmbH, where we applied it to real-world data from the steel industry and demonstrated its efficacy with a simulation study.},
booktitle = {33rd International Conference on Scientific and Statistical Database Management},
pages = {214–219},
numpages = {6},
keywords = {Data Quality, Pattern Detection, Missing Data, Steel Industry.},
location = {Tampa, FL, USA},
series = {SSDBM 2021}
}

@inproceedings{10.1145/3216122.3216148,
author = {Chabin, Jacques and Gomes-Jr., Luiz and Halfeld-Ferrari, Mirian},
title = {A Context-Driven Querying System for Urban Graph Analysis},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216148},
doi = {10.1145/3216122.3216148},
abstract = {This paper presents a context-driven query system for urban computing where users are responsible for defining their own restrictions over which datalog-like queries are built. Instead of imposing constraints on databases, our goal is to filter consistent data during the query process. Our query language is able to express aggregates in recursive rules, allowing it to capture network properties typical of graph analysis. This paper presents our query system and analyzes its capabilities using use cases in Urban Computing.},
booktitle = {Proceedings of the 22nd International Database Engineering &amp; Applications Symposium},
pages = {297–301},
numpages = {5},
keywords = {data quality, Query language, data graph, constraints, smart city},
location = {Villa San Giovanni, Italy},
series = {IDEAS '18}
}

@article{10.1145/2822898,
author = {Coletti, Paolo and Murgia, Maurizio},
title = {Design and Construction of a Historical Financial Database of the Italian Stock Market 1973--2011},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/2822898},
doi = {10.1145/2822898},
abstract = {This article presents the technical aspects of designing and building a historical database of the Italian Stock Market. The database contains daily market data from 1973 to 2011 and is constructed by merging two main digital sources and several other hand-collected data sources. We analyzed and developed semiautomatic tools to deal with problems related to time-series matchings, quality of data, and numerical errors. We also developed a concatenation structure to allow the handling of company name changes, mergers, and spin-offs without artificially altering numerical series. At the same time, we maintained the transparency of the historical information on each individual company listed. Thanks to the overlapping of digital and hand-collected data, the completed database has a very high level of detail and accuracy. The dataset is particularly suited for any empirical research in financial economics and for more practically oriented numerical applications and forecasting simulations.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {16},
numpages = {23},
keywords = {data quality, Financial database, stock market, data integration}
}

@inproceedings{10.1145/2948992.2949009,
author = {Martins, Pedro and Cec\'{\i}lio, Jos\'{e} and Abbasi, Maryam and Furtado, Pedro},
title = {GPII: A Benchmark for Generic Purpose Image Information},
year = {2016},
isbn = {9781450340755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948992.2949009},
doi = {10.1145/2948992.2949009},
abstract = {The growing number of different models and approaches for Geographic Information Systems (GIS) brings high complexity when we want to develop new approaches and compare a new GIS algorithm. In order to test and compare different processing models and approaches, in a simple way, we identified the need of defining uniform testing methods, able to compare processing algorithms in terms of performance and accuracy regarding large image processing, algorithms for GIS pattern-detection.Taking into account, for instance, images collected during a done flight or a satellite, it is important to know the processing cost to extract data when applying different processing models and approaches, as well as their accuracy (compare execution time vs. extracted data quality). In this work, we propose a GIS Benchmark (GPII), a benchmark that allows evaluating different approaches to detect/extract selected features from a GIS dataset. Considering a given dataset (or two data-sets, from different years, of the same region), it provides linear methods to compare different performance parameters regarding GIS information, making possible to access the most relevant information in terms of features and processing efficiency. Moreover, our approach to test algorithms makes possible to change the data-set in order to support different purpose algorithms.},
booktitle = {Proceedings of the Ninth International C* Conference on Computer Science &amp; Software Engineering},
pages = {119–122},
numpages = {4},
keywords = {Benchmark, Big-data, performance, GIS, spatio-temporal databases, algorithms, experimentation, pattern-detection},
location = {Porto, Portugal},
series = {C3S2E '16}
}

@inproceedings{10.1145/3340531.3414077,
author = {Ukil, Arijit and Marin, Leandro and Jara, Antonio and Farserotu, John},
title = {On the Knowledge-Driven Analytics and Systems Impacting Human Quality of Life},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3414077},
doi = {10.1145/3340531.3414077},
abstract = {The present scenario of Covid-19 pandemic has disrupted the human life to a larger extent. In such context, human-centric applications and systems that endeavor to positively impact the human quality of life is of utmost importance. Knowledge-driven analytics that help to build such intelligent systems play important role to construct the required eco-system on the macro-scale. It is worth mentioning that Knowledge-Driven Analytics and Systems Impacting Human Quality of Life (KDAH) workshop in ACM International Conference on Information and Knowledge Management (CIKM), attempts to bring out the intricate research direction for enabling a sustainable human society through the positive co-existence of human beings and intelligent systems.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3539–3540},
numpages = {2},
keywords = {security, human life, big data, knowledge, sensors, deep learning, privacy, artificial intelligence},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/2939672.2939799,
author = {Xu, Tong and Zhu, Hengshu and Zhao, Xiangyu and Liu, Qi and Zhong, Hao and Chen, Enhong and Xiong, Hui},
title = {Taxi Driving Behavior Analysis in Latent Vehicle-to-Vehicle Networks: A Social Influence Perspective},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939799},
doi = {10.1145/2939672.2939799},
abstract = {With recent advances in mobile and sensor technologies, a large amount of efforts have been made on developing intelligent applications for taxi drivers, which provide beneficial guide and opportunity to improve the profit and work efficiency. However, limited scopes focus on the latent social interaction within cab drivers, and corresponding social propagation scheme to share driving behaviors has been largely ignored. To that end, in this paper, we propose a comprehensive study to reveal how the social propagation affects for better prediction of cab drivers' future behaviors. To be specific, we first investigate the correlation between drivers' skills and their mutual interactions in the latent vehicle-to-vehicle network, which intuitively indicates the effects of social influences. Along this line, by leveraging the classic social influence theory, we develop a two-stage framework for quantitatively revealing the latent driving pattern propagation within taxi drivers. Comprehensive experiments on a real-word data set collected from the New York City clearly validate the effectiveness of our proposed framework on predicting future taxi driving behaviors, which also support the hypothesis that social factors indeed improve the predictability of driving behaviors.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1285–1294},
numpages = {10},
keywords = {social influence, mobile data mining, taxi trajectories},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/3290607.3313002,
author = {Hohmann, Matthias R. and Hackl, Michelle and Wirth, Brian and Zaman, Talha and Enficiaud, Raffi and Grosse-Wentrup, Moritz and Sch\"{o}lkopf, Bernhard},
title = {MYND: A Platform for Large-Scale Neuroscientific Studies},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3313002},
doi = {10.1145/3290607.3313002},
abstract = {We present a smartphone application for at-home participation in large-scale neuroscientific studies. Our goal is to establish user-experience design as a paradigm in basic neuroscientific research to overcome the limits of current studies, especially in rare neurological disorders.The presented application guides users through the fitting procedure of the EEG headset and automatically encrypts and uploads recorded data to a remote server. User-feedback and neurophysiological data from a pilot study with eighteen subjects indicate that the application can be used outside of a laboratory, without the need for external guidance. We hope to inspire future work on the intersection between basic neuroscience and human-computer interaction as a promising paradigm to accelerate research on rare neurological diseases and assistive neurotechnology.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {user-centered design, big data, smartphone application, neuroscience, wearable sensors, medical studies, electrophysiology},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@article{10.1007/s00778-021-00704-2,
author = {Alva Principe, Renzo Arturo and Maurino, Andrea and Palmonari, Matteo and Ciavotta, Michele and Spahiu, Blerina},
title = {ABSTAT-HD: A Scalable Tool for Profiling Very Large Knowledge Graphs},
year = {2021},
issue_date = {Sep 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00704-2},
doi = {10.1007/s00778-021-00704-2},
abstract = {Processing large-scale and highly interconnected Knowledge Graphs (KG) is becoming crucial for many applications such as recommender systems, question answering, etc. Profiling approaches have been proposed to summarize large KGs with the aim to produce concise and meaningful representation so that they can be easily managed. However, constructing profiles and calculating several statistics such as cardinality descriptors or inferences are resource expensive. In this paper, we present ABSTAT-HD, a highly distributed profiling tool that supports users in profiling and understanding big and complex knowledge graphs. We demonstrate the impact of the new architecture of ABSTAT-HD by presenting a set of experiments that show its scalability with respect to three dimensions of the data to be processed: size, complexity and workload. The experimentation shows that our profiling framework provides informative and concise profiles, and can process and manage very large KGs.},
journal = {The VLDB Journal},
month = {sep},
pages = {851–876},
numpages = {26},
keywords = {Data quality, Data profiling, Knowledge graph, Distributed processing engine, Data management}
}

@inproceedings{10.1145/3338906.3338931,
author = {Zhang, Xu and Xu, Yong and Lin, Qingwei and Qiao, Bo and Zhang, Hongyu and Dang, Yingnong and Xie, Chunyu and Yang, Xinsheng and Cheng, Qian and Li, Ze and Chen, Junjie and He, Xiaoting and Yao, Randolph and Lou, Jian-Guang and Chintalapati, Murali and Shen, Furao and Zhang, Dongmei},
title = {Robust Log-Based Anomaly Detection on Unstable Log Data},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338931},
doi = {10.1145/3338906.3338931},
abstract = {Logs are widely used by large and complex software-intensive systems for troubleshooting. There have been a lot of studies on log-based anomaly detection. To detect the anomalies, the existing methods mainly construct a detection model using log event data extracted from historical logs. However, we find that the existing methods do not work well in practice. These methods have the close-world assumption, which assumes that the log data is stable over time and the set of distinct log events is known. However, our empirical study shows that in practice, log data often contains previously unseen log events or log sequences. The instability of log data comes from two sources: 1) the evolution of logging statements, and 2) the processing noise in log data. In this paper, we propose a new log-based anomaly detection approach, called LogRobust. LogRobust extracts semantic information of log events and represents them as semantic vectors. It then detects anomalies by utilizing an attention-based Bi-LSTM model, which has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events. In this way, LogRobust is able to identify and handle unstable log events and sequences. We have evaluated LogRobust using logs collected from the Hadoop system and an actual online service system of Microsoft. The experimental results show that the proposed approach can well address the problem of log instability and achieve accurate and robust results on real-world, ever-changing log data.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {807–817},
numpages = {11},
keywords = {Data Quality, Deep Learning, Anomaly Detection, Log Analysis, Log Instability},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3386723.3387850,
author = {Maqboul, Jaouad and Jaouad, Bouchaib Bounabat},
title = {Contribution of Artificial Neural Network in Predicting Completeness Through the Impact and Complexity of Its Improvement},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387850},
doi = {10.1145/3386723.3387850},
abstract = {The technological evolution and the immensity of the data produced, circulated into company makes these data, the real capital of the companies to the detriment of the customers. The erroneous data put the knockout to relationships with customers, the company must address this problem and identify the quality projects on which it must make an effort. In this article, we will present an approach based on qualitative and quantitative analysis to help the decision-makers to target data by its impacts and complexities of process improvement. The Qualitative study will be a survey and a quantitative to learn from survey data to decide the prediction and the completeness of data.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {31},
numpages = {8},
keywords = {cost/benefit analysis, Data quality improvement project, cost of data quality, data quality assessment and improvement, artificial neural network},
location = {Marrakech, Morocco},
series = {NISS2020}
}

@article{10.1145/3483423,
author = {Shraga, Roee and Gal, Avigdor},
title = {PoWareMatch: A Quality-Aware Deep Learning Approach to Improve Human Schema Matching},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3483423},
doi = {10.1145/3483423},
abstract = {Schema matching is a core task of any data integration process. Being investigated in the fields of databases, AI, Semantic Web, and data mining for many years, the main challenge remains the ability to generate quality matches among data concepts (e.g., database attributes). In this work, we examine a novel angle on the behavior of humans as matchers, studying match creation as a process. We analyze the dynamics of common evaluation measures (precision, recall, and f-measure), with respect to this angle and highlight the need for unbiased matching to support this analysis. Unbiased matching, a newly defined concept that describes the common assumption that human decisions represent reliable assessments of schemata correspondences, is, however, not an inherent property of human matchers. In what follows, we design PoWareMatch that makes use of a deep learning mechanism to calibrate and filter human matching decisions adhering to the quality of a match, which are then combined with algorithmic matching to generate better match results. We provide an empirical evidence, established based on an experiment with more than 200 human matchers over common benchmarks, that PoWareMatch predicts well the benefit of extending the match with an additional correspondence and generates high-quality matches. In addition, PoWareMatch outperforms state-of-the-art matching algorithms.},
journal = {J. Data and Information Quality},
month = {may},
articleno = {16},
numpages = {27},
keywords = {deep learning, data quality, Human-in-the-loop}
}

@article{10.1145/3432247,
author = {Garriga, Martin and Aarns, Koen and Tsigkanos, Christos and Tamburri, Damian A. and Heuvel, Wjan Van Den},
title = {DataOps for Cyber-Physical Systems Governance: The Airport Passenger Flow Case},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3432247},
doi = {10.1145/3432247},
abstract = {Recent advancements in information technology have ushered a new wave of systems integrating Internet technology with sensing, wireless communication, and computational resources over existing infrastructures. As a result, myriad complex, non-traditional Cyber-Physical Systems (CPS) have emerged, characterized by interaction among people, physical facilities, and embedded sensors and computers, all generating vast amounts of complex data. Such a case is encountered within a contemporary airport hall setting: passengers roaming, information systems governing various functions, and data being generated and processed by cameras, phones, sensors, and other Internet of Things technology. This setting has considerable potential of contributing to goals entertained by the CPS operators, such as airlines, airport operators/owners, technicians, users, and more. We model the airport setting as an instance of such a complex, data-intensive CPS where multiple actors and data sources interact, and generalize a methodology to support it and other similar systems. Furthermore, this article instantiates the methodology and pipeline for predictive analytics for passenger flow, as a characteristic manifestation of such systems requiring a tailored approach. Our methodology also draws from DataOps principles, using multi-modal and real-life data to predict the underlying distribution of the passenger flow on a flight-level basis (improving existing day-level predictions), anticipating when and how the passengers enter the airport and move through the check-in and baggage drop-off process. This allows to plan airport resources more efficiently while improving customer experience by avoiding passenger clumping at check-in and security. We demonstrate results obtained over a case from a major international airport in the Netherlands, improving up to 60% upon predictions of daily passenger flow currently in place.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {36},
numpages = {25},
keywords = {DataOps, cyber-physical systems, big data, systems governance, Data-intensive systems, airport management}
}

@article{10.1145/3409473,
author = {Maiolo, Sof\'{\i}a and Etcheverry, Lorena and Marotta, Adriana},
title = {Data Profiling in Property Graph Databases},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3409473},
doi = {10.1145/3409473},
abstract = {Property Graph databases are being increasingly used within the industry as a powerful and flexible way to model real-world scenarios. With this flexibility, a great challenge appears regarding profiling tasks due to the need of adapting them to these new models while taking advantage of the Property Graphs’ particularities. This article proposes a set of data profiling tasks by integrating existing methods and techniques and an taxonomy to classify them. In addition, an application pipeline is provided while a formal specification of some tasks is defined.},
journal = {J. Data and Information Quality},
month = {oct},
articleno = {20},
numpages = {27},
keywords = {data profiling, Property Graph}
}

@inproceedings{10.1145/3404555.3404621,
author = {Zhu, Ruyi},
title = {Traffic Condition Prediction of Urban Roads Based on Neural Network},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404621},
doi = {10.1145/3404555.3404621},
abstract = {Real-time and reliable traffic flow estimation is the basis of urban traffic management and control. However, the existing research focuses on how to use the historical data of surveillance intersection to predict future traffic conditions. As we know, there are few effective algorithms to infer the real-time traffic state of non-surveillance intersections from limited road surveillance by using traffic information in the urban road system. In this paper, we introduce a new solution to solve the prediction task of traffic flow analysis by using traffic data, especially taxi historical data, traffic network data and intersection historical data. The proposed solution takes advantage of GCN and CGAN, and we improved the Unet to realize an important part of the generator. Then, we capture the relationship between the intersections with surveillance and the intersections without surveillance by floating taxi-cabs covered in the whole city. The framework of CGAN can adjust the weights and enhance the inference ability to generate complete traffic status under current conditions. The experimental results show that our method is superior to other methods on the accuracy of traffic volume inference.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {30–36},
numpages = {7},
keywords = {big data, Urban road system, forecasting, video surveillance system, real-time traffic condition},
location = {Tianjin, China},
series = {ICCAI '20}
}

@article{10.1007/s00778-021-00720-2,
author = {Sadiq, Shazia and Aryani, Amir and Demartini, Gianluca and Hua, Wen and Indulska, Marta and Burton-Jones, Andrew and Khosravi, Hassan and Benavides-Prado, Diana and Sellis, Timos and Someh, Ida and Vaithianathan, Rhema and Wang, Sen and Zhou, Xiaofang},
title = {Information Resilience: The Nexus of Responsible and Agile Approaches to Information Use},
year = {2022},
issue_date = {Sep 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00720-2},
doi = {10.1007/s00778-021-00720-2},
abstract = {The appetite for effective use of information assets has been steadily rising in both public and private sector organisations. However, whether the information is used for social good or commercial gain, there is a growing recognition of the complex socio-technical challenges associated with balancing the diverse demands of regulatory compliance and data privacy, social expectations and ethical use, business process agility and value creation, and scarcity of data science talent. In this vision paper, we present a series of case studies that highlight these interconnected challenges, across a range of application areas. We use the insights from the case studies to introduce Information Resilience, as a scaffold within which the competing requirements of responsible and agile approaches to information use can be positioned. The aim of this paper is to develop and present a manifesto for Information Resilience that can serve as a reference for future research and development in relevant areas of responsible data management.},
journal = {The VLDB Journal},
month = {jan},
pages = {1059–1084},
numpages = {26},
keywords = {Responsible data science, Effective information use, Information Resilience, Value creation, Data quality}
}

@inproceedings{10.1145/3495018.3495097,
author = {Chen, Zhangbin and Liu, Yang},
title = {Research and Construction of University Data Governance Platform Based on Smart Campus Environment},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495097},
doi = {10.1145/3495018.3495097},
abstract = {Campus data is a subset of education big data. It is a variety of data generated by teachers and students in the life, teaching, scientific research, management and service process, as well as various school affairs management status data. It has the characteristics of a wide variety of data. Contains great information value, and giving full play to its role is an indispensable part of achieving the school's strategic goals. Taking the opportunity of building a smart campus, using advanced technologies such as cloud computing, big data, Internet of Things, and artificial intelligence, through a big data management platform, the full collection of existing business data inside and outside the school is provided, and a normal data governance model is provided to eliminate data islands. Realize normal data sharing services. The article conducts research on the data governance platform, and builds a data governance platform system with functions such as a normalized data quality monitoring system, information resource catalog system and full-link data monitoring to realize university data integration, business collaboration, service upgrades and assistance Decision-making provides a solid foundation for the application and expansion of smart campuses in universities and provides a reference for smart campus builders in universities.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {450–455},
numpages = {6},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

@inproceedings{10.1145/3328905.3332513,
author = {Frischbier, Sebastian and Paic, Mario and Echler, Alexander and Roth, Christian},
title = {A Real-World Distributed Infrastructure for Processing Financial Data at Scale},
year = {2019},
isbn = {9781450367943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328905.3332513},
doi = {10.1145/3328905.3332513},
abstract = {Financial markets are event- and data-driven to an extremely high degree. For making decisions and triggering actions stakeholders require notifications about significant events and reliable background information that meet their individual requirements in terms of timeliness, accuracy, and completeness. As one of Europe's leading providers of financial data and regulatory solutions vwd: processes an average of 18 billion event notifications from 500+ data sources for 30 million symbols per day. Our large-scale distributed event-based systems handle daily peak rates of 1+ million event notifications per second and additional load generated by singular pivotal events with global impact.In this poster we give practical insights into our IT systems. We outline the infrastructure we operate and the event-driven architecture we apply at vwd. In particular we showcase the (geo)distributed publish/subscribe broker network we operate across locations and countries to provide market data to our customers with varying quality of information (QoI) properties.},
booktitle = {Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems},
pages = {254–255},
numpages = {2},
keywords = {publish/subscribe, Event-processing, quality of information, financial data, stream-processing, big data, infrastructure, broker network},
location = {Darmstadt, Germany},
series = {DEBS '19}
}

@inproceedings{10.1145/2757384.2757396,
author = {Cao, Paul Y. and Li, Gang and Chen, Guoxing and Chen, Biao},
title = {Mobile Data Collection Frameworks: A Survey},
year = {2015},
isbn = {9781450335249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757384.2757396},
doi = {10.1145/2757384.2757396},
abstract = {Mobile phones equipped with powerful sensors have become ubiquitous in recent years. Mobile sensing applications present an unprecedented opportunity to collect and analyze information from mobile devices. Much of the work in mobile sensing has been done on designing monolithic applications but inadequate attention has been paid to general mobile data collection frameworks. In this paper, we provide a survey on how to build a general purpose mobile data collection framework. We identify the basic requirements and present an architecture for such a framework. We survey existing works to summarize existing approaches to address the basic requirements. Eight major mobile data collection frameworks are compared with respect to the requirements as well as additional issues on privacy, energy and incentives.},
booktitle = {Proceedings of the 2015 Workshop on Mobile Big Data},
pages = {25–30},
numpages = {6},
keywords = {mobile data, data collection framework},
location = {Hangzhou, China},
series = {Mobidata '15}
}

@article{10.1145/2906149,
author = {Khan, Suleman and Gani, Abdullah and Wahab, Ainuddin Wahid Abdul and Bagiwa, Mustapha Aminu and Shiraz, Muhammad and Khan, Samee U. and Buyya, Rajkumar and Zomaya, Albert Y.},
title = {Cloud Log Forensics: Foundations, State of the Art, and Future Directions},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2906149},
doi = {10.1145/2906149},
abstract = {Cloud log forensics (CLF) mitigates the investigation process by identifying the malicious behavior of attackers through profound cloud log analysis. However, the accessibility attributes of cloud logs obstruct accomplishment of the goal to investigate cloud logs for various susceptibilities. Accessibility involves the issues of cloud log access, selection of proper cloud log file, cloud log data integrity, and trustworthiness of cloud logs. Therefore, forensic investigators of cloud log files are dependent on cloud service providers (CSPs) to get access of different cloud logs. Accessing cloud logs from outside the cloud without depending on the CSP is a challenging research area, whereas the increase in cloud attacks has increased the need for CLF to investigate the malicious activities of attackers. This paper reviews the state of the art of CLF and highlights different challenges and issues involved in investigating cloud log data. The logging mode, the importance of CLF, and cloud log-as-a-service are introduced. Moreover, case studies related to CLF are explained to highlight the practical implementation of cloud log investigation for analyzing malicious behaviors. The CLF security requirements, vulnerability points, and challenges are identified to tolerate different cloud log susceptibilities. We identify and introduce challenges and future directions to highlight open research areas of CLF for motivating investigators, academicians, and researchers to investigate them.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {7},
numpages = {42},
keywords = {integrity, big data, cloud log forensics, Cloud computing, correlation of cloud logs, confidentiality, authenticity}
}

@article{10.1145/3379445,
author = {Bonifati, Angela and Holubov\'{a}, Irena and Prat-P\'{e}rez, Arnau and Sakr, Sherif},
title = {Graph Generators: State of the Art and Open Challenges},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3379445},
doi = {10.1145/3379445},
abstract = {The abundance of interconnected data has fueled the design and implementation of graph generators reproducing real-world linking properties or gauging the effectiveness of graph algorithms, techniques, and applications manipulating these data. We consider graph generation across multiple subfields, such as Semantic Web, graph databases, social networks, and community detection, along with general graphs. Despite the disparate requirements of modern graph generators throughout these communities, we analyze them under a common umbrella, reaching out the functionalities, the practical usage, and their supported operations. We argue that this classification is serving the need of providing scientists, researchers, and practitioners with the right data generator at hand for their work. This survey provides a comprehensive overview of the state-of-the-art graph generators by focusing on those that are pertinent and suitable for several data-intensive tasks. Finally, we discuss open challenges and missing requirements of current graph generators along with their future extensions to new emerging fields.},
journal = {ACM Comput. Surv.},
month = {apr},
articleno = {36},
numpages = {30},
keywords = {Big data management, benchmarks, graph data, generators, synthetic data}
}

@article{10.5555/3546258.3546276,
author = {Jiang, Gaoxia and Wang, Wenjian and Qian, Yuhua and Liang, Jiye},
title = {A Unified Sample Selection Framework for Output Noise Filtering: An Error-Bound Perspective},
year = {2022},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {The existence of output noise will bring difficulties to supervised learning. Noise filtering, aiming to detect and remove polluted samples, is one of the main ways to deal with the noise on outputs. However, most of the filters are heuristic and could not explain the filtering in uence on the generalization error (GE) bound. The hyper-parameters in various filters are specified manually or empirically, and they are usually unable to adapt to the data environment. The filter with an improper hyper-parameter may overclean, leading to a weak generalization ability. This paper proposes a unified framework of optimal sample selection (OSS) for the output noise filtering from the perspective of error bound. The covering distance filter (CDF) under the framework is presented to deal with noisy outputs in regression and ordinal classification problems. Firstly, two necessary and sufficient conditions for a fixed goodness of fit in regression are deduced from the perspective of GE bound. They provide the unified theoretical framework for determining the filtering effectiveness and optimizing the size of removed samples. The optimal sample size has the adaptability to the environmental changes in the sample size, the noise ratio, and noise variance. It offers a choice of tuning the hyper-parameter and could prevent filters from overcleansing. Meanwhile, the OSS framework can be integrated with any noise estimator and produces a new filter. Then the covering interval is proposed to separate low-noise and high-noise samples, and the effectiveness is proved in regression. The covering distance is introduced as an unbiased estimator of high noises. Further, the CDF algorithm is designed by integrating the cover distance with the OSS framework. Finally, it is verified that the CDF not only recognizes noise labels correctly but also brings down the prediction errors on real apparent age data set. Experimental results on benchmark regression and ordinal classification data sets demonstrate that the CDF outperforms the state-of-the-art filters in terms of prediction ability, noise recognition, and efficiency.},
journal = {J. Mach. Learn. Res.},
month = {jul},
articleno = {18},
numpages = {66},
keywords = {covering distance filtering, generalization error bound, supervised learning, output noise, optimal sample selection}
}

@inproceedings{10.1145/3383583.3398539,
author = {Esteva, Maria and Xu, Weijia and Simone, Nevan and Gupta, Amit and Jah, Moriba},
title = {Modeling Data Curation to Scientific Inquiry: A Case Study for Multimodal Data Integration},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398539},
doi = {10.1145/3383583.3398539},
abstract = {Scientific data publications may include interactive data applications designed by scientists to explore a scientific problem. Defined as knowledge systems, their development is complex when data are aggregated from multiple sources over time. Multimodal data are created, encoded, and maintained differently, and even when reporting about identical phenomena, fields and their values may be inconsistent across datasets. To assure the validity and accuracy of the application, the data has to abide by curation requirements similar to those ruling digital libraries. We present a novel, inquiry-driven curation approach aimed to optimize multimodal datasets curation and maximize data reuse by domain researchers. We demonstrate the method through the ASTRIAGraph project, in which multiple data sources about near earth space objects are aggregated into a central knowledge system. The process involves multidisciplinary collaboration, resulting in the design of a data model as the backbone for both data curation and scientific inquiry. We demonstrate a) how data provenance information is needed to assess the uncertainty of the results of scientific inquiries involving multiple data sources, and b) that continuous curation of integrated datasets is facilitated when undertaken as integral to the research project. The approach provides flexibility to support expansion of scientific inquiries and data in the knowledge system, and allows for transparent and explainable results.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {235–242},
numpages = {8},
keywords = {data curation, knowledge system, big data, data integration, graph database, data model, space traffic management},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3340017.3340022,
author = {Wieczorkowski, Jundefineddrzej},
title = {Barriers to Using Open Government Data},
year = {2019},
isbn = {9781450362375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340017.3340022},
doi = {10.1145/3340017.3340022},
abstract = {The article describes the issues of Open Government Data (OGD) and problems with the use of such data. Good quality and proper publishing of OGD enable (apart from the control function) their business use. This affects the economic benefits. The author has identified the main problems of data publication based on Central Repositories for Public Information (CRPI) in Poland, the USA, the UK and Germany. The article focuses on the maturity of data formats, automated processing with Application Programming Interface (API), using the concept of Linked Open Data (LOD). The aim of the article is to identify barriers to the implementation of OGD-based solutions and to indicate recommendations to overcome these barriers. The research shows that the methods of sharing OGD differ significantly between countries despite common guidelines. The main problem is the use of unstructured data, unsuitable for the use of LOD.},
booktitle = {Proceedings of the 2019 3rd International Conference on E-Commerce, E-Business and E-Government},
pages = {15–20},
numpages = {6},
keywords = {E-government, Linked Data, Big Data, Open Data, Central Repository for Public Information, OGD, LOD, CRPI, Open Government Data, Linked Open Data},
location = {Lyon, France},
series = {ICEEG 2019}
}

@inproceedings{10.5555/2740769.2740814,
author = {Borgman, Christine L. and Darch, Peter T. and Sands, Ashley E. and Wallis, Jillian C. and Traweek, Sharon},
title = {The Ups and Downs of Knowledge Infrastructures in Science: Implications for Data Management},
year = {2014},
isbn = {9781479955695},
publisher = {IEEE Press},
abstract = {The promise of technology-enabled, data-intensive scholarship is predicated upon access to knowledge infrastructures that are not yet in place. Scientific data management requires expertise in the scientific domain and in organizing and retrieving complex research objects. The Knowledge Infrastructures project compares data management activities of four large, distributed, multidisciplinary scientific endeavors as they ramp their activities up or down; two are big science and two are small science. Research questions address digital library solutions, knowledge infrastructure concerns, issues specific to individual domains, and common problems across domains. Findings are based on interviews (n=113 to date), ethnography, and other analyses of these four cases, studied since 2002. Based on initial comparisons, we conclude that the roles of digital libraries in scientific data management often depend upon the scale of data, the scientific goals, and the temporal scale of the research projects being supported. Digital libraries serve immediate data management purposes in some projects and long-term stewardship in others. In small science projects, data management tools are selected, designed, and used by the same individuals. In the multi-decade time scale of some big science research, data management technologies, policies, and practices are designed for anticipated future uses and users. The need for library, archival, and digital library expertise is apparent throughout all four of these cases. Managing research data is a knowledge infrastructure problem beyond the scope of individual researchers or projects. The real challenges lie in designing digital libraries to assist in the capture, management, interpretation, use, reuse, and stewardship of research data.},
booktitle = {Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {257–266},
numpages = {10},
keywords = {data management, biology, small science, big data, big science, sensor networks, little science, digital libraries, astronomy, knowledge infrastructures},
location = {London, United Kingdom},
series = {JCDL '14}
}

@inproceedings{10.1145/3409501.3409542,
author = {Hongmeng, Zhang and Zhiqiang, Zhu and Lei, Sun and Xiuqing, Mao and Yuehan, Wang},
title = {A Detection Method for DeepFake Hard Compressed Videos Based on Super-Resolution Reconstruction Using CNN},
year = {2020},
isbn = {9781450375603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409501.3409542},
doi = {10.1145/3409501.3409542},
abstract = {The DeepFake video detection method based on convolutional neural networks has a poor performance in the dataset of hard compressed DeepFake video. And a large number of false tests will occur to the real data. To solve this problem, a networks model detection method for super-resolution reconstruction of DeepFake video is proposed. First of all, the face area of real data is processed by Gaussian blur, which is converted into negative data, and the real data and processing data are input into neural network for training. Then the residual network is used for super-resolution reconstruction of test data. Finally, the trained model is used to test the video after super-resolution reconstruction. Experiments show that the proposed method can reduce the false detection rate and improve the accuracy in detection of single frames.},
booktitle = {Proceedings of the 2020 4th High Performance Computing and Cluster Technologies Conference &amp; 2020 3rd International Conference on Big Data and Artificial Intelligence},
pages = {98–103},
numpages = {6},
keywords = {Hard compressed video, DeepFake detection, Super-resolution reconstruction, Deep Learning},
location = {Qingdao, China},
series = {HPCCT &amp; BDAI '20}
}

@article{10.14778/3007263.3007320,
author = {Chu, Xu and Ilyas, Ihab F.},
title = {Qualitative Data Cleaning},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007320},
doi = {10.14778/3007263.3007320},
abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions.Data cleaning exercise often consist of two phases: error detection and error repairing. Error detection techniques can either be quantitative or qualitative; and error repairing is performed by applying data transformation scripts or by involving human experts, and sometimes both.In this tutorial, we discuss the main facets and directions in designing qualitative data cleaning techniques. We present a taxonomy of current qualitative error detection techniques, as well as a taxonomy of current data repairing techniques. We will also discuss proposals for tackling the challenges for cleaning "big data" in terms of scale and distribution.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {1605–1608},
numpages = {4}
}

@article{10.1145/3301284,
author = {Silva, Thiago H. and Viana, Aline Carneiro and Benevenuto, Fabr\'{\i}cio and Villas, Leandro and Salles, Juliana and Loureiro, Antonio and Quercia, Daniele},
title = {Urban Computing Leveraging Location-Based Social Network Data: A Survey},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3301284},
doi = {10.1145/3301284},
abstract = {Urban computing is an emerging area of investigation in which researchers study cities using digital data. Location-Based Social Networks (LBSNs) generate one specific type of digital data that offers unprecedented geographic and temporal resolutions. We discuss fundamental concepts of urban computing leveraging LBSN data and present a survey of recent urban computing studies that make use of LBSN data. We also point out the opportunities and challenges that those studies open.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {17},
numpages = {39},
keywords = {city dynamics, big data, Urban computing, urban informatics, location-based social networks, urban societies, urban sensing}
}

@inproceedings{10.1145/3447548.3467129,
author = {Deng, Alex and Li, Yicheng and Lu, Jiannan and Ramamurthy, Vivek},
title = {On Post-Selection Inference in A/B Testing},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467129},
doi = {10.1145/3447548.3467129},
abstract = {When interpreting A/B tests, we typically focus only on the statistically significant results and take them by face value. This practice, termed post-selection inference in the statistical literature, may negatively affect both point estimation and uncertainty quantification, and therefore hinder trustworthy decision making in A/B testing. To address this issue, in this paper we explore two seemingly unrelated paths, one based on supervised machine learning and the other on empirical Bayes, and propose post-selection inferential approaches that combine the strengths of both. Through large-scale simulated and empirical examples, we demonstrate that our proposed methodologies stand out among other existing ones in both reducing post-selection biases and improving confidence interval coverage rates, and discuss how they can be conveniently adjusted to real-life scenarios.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2743–2752},
numpages = {10},
keywords = {big data, empirical Bayes, winner's curse, machine learning, online metrics, post-selection inference, randomization, A/B testing, regression, bias correction},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3410992.3411027,
author = {Kamel, Mohammed B. M. and Wallis, Kevin and Ligeti, Peter and Reich, Christoph},
title = {Distributed Data Validation Network in IoT: A Decentralized Validator Selection Model},
year = {2020},
isbn = {9781450387583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410992.3411027},
doi = {10.1145/3410992.3411027},
abstract = {The generated real-time data on the Internet of Things (IoT) and the ability to gather and manipulate them are positively affecting various fields. One of the main concerns in IoT is how to provide trustworthy data. The data validation network ensures that the generated data by data sources in the IoT are trustworthy. However, the existing data validation network depends on a centralized entity for the selection of data validators. In this paper, a decentralized validator selection model is proposed. The proposed model creates multiple clusters using the distributed hash table (DHT) technique. The selection process of data validators from different clusters in the model is done randomly in a decentralized scheme. It provides a global method of assignment, selection, and verification of the selected validators in the network.},
booktitle = {Proceedings of the 10th International Conference on the Internet of Things},
articleno = {12},
numpages = {8},
keywords = {data validation, internet of things, cluster-based data \^{A}\u{a}Validation, industrial internet of things, big data, distributed hash table, data validation network},
location = {Malm\"{o}, Sweden},
series = {IoT '20}
}

@inproceedings{10.1145/3442555.3442579,
author = {Maziku, Hellen},
title = {Improved Data Accuracy Assessment Tool for Information Management Systems},
year = {2021},
isbn = {9781450388092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442555.3442579},
doi = {10.1145/3442555.3442579},
abstract = {Developing countries are increasingly taking advantage of the rapid advancement in ICT to replace paper-based operations with Information Management Systems (IMS) such as District Health Information Software (DHIS). While the adoption of IMS presents significant benefits, challenges exist in the quality of IMS data. Inaccurate, incomplete or redundant data in IMS has misled organisations into making incorrect decisions leading to customer dissatisfaction and high cost implications. There is an urgent need for IMS stakeholders to have mechanisms of assessing the quality of data prior to data analysis, data sharing or decision making. Data Accuracy Assessment Tool (DAAT) assesses and identifies errors in a pair of context related datasets. DAAT provides ability for Data Managers to easily compare datasets by choosing attributes of their interest from a pool of diverse attributes that define the data. Through reports and visualization, the tool reveals the accuracy of data in real time using metrics such as validity, completeness and duplication of data. DAAT is scalable since it can be integrated with any IMS such as DHIS. The tool has been tested using four years Voluntary Medical Male Circumcision (VMMC) program data from JHPIEGO's AIDSFree project in Tanzania.},
booktitle = {2020 the 6th International Conference on Communication and Information Processing},
pages = {148–152},
numpages = {5},
keywords = {Information Management Systems, Human Centered Design, Data Quality Assessment, Accuracy},
location = {Tokyo, Japan},
series = {ICCIP 2020}
}

@inproceedings{10.1145/2457317.2457382,
author = {Jiang, Yu and Deng, Dong and Wang, Jiannan and Li, Guoliang and Feng, Jianhua},
title = {Efficient Parallel Partition-Based Algorithms for Similarity Search and Join with Edit Distance Constraints},
year = {2013},
isbn = {9781450315999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2457317.2457382},
doi = {10.1145/2457317.2457382},
abstract = {The quantity of data in real-world applications is growing significantly while the data quality is still a big problem. Similarity search and similarity join are two important operations to address the poor data quality problem. Although many similarity search and join algorithms have been proposed, they did not utilize the abilities of modern hardware with multi-core processors. It calls for new parallel algorithms to enable multi-core processors to meet the high performance requirement of similarity search and join on big data. To this end, in this paper we propose parallel algorithms to support efficient similarity search and join with edit-distance constraints. We adopt the partition-based framework and extend it to support parallel similarity search and join on multi-core processors. We also develop two novel pruning techniques. We have implemented our algorithms and the experimental results on two real datasets show that our parallel algorithms achieve high performance and obtain good speedup.},
booktitle = {Proceedings of the Joint EDBT/ICDT 2013 Workshops},
pages = {341–348},
numpages = {8},
keywords = {similarity join, content filter, similarity search, parallel algorithms},
location = {Genoa, Italy},
series = {EDBT '13}
}

@inproceedings{10.1109/CCGrid.2015.24,
author = {Yan, Cairong and Song, Yalong and Wang, Jian and Guo, Wenjing},
title = {Eliminating the Redundancy in MapReduce-Based Entity Resolution},
year = {2015},
isbn = {9781479980062},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2015.24},
doi = {10.1109/CCGrid.2015.24},
abstract = {Entity resolution is the basic operation of data quality management, and the key step to find the value of data. The parallel data processing framework based on MapReduce can deal with the challenge brought by big data. However, there exist two important issues, avoiding redundant pairs led by the multi-pass blocking method and optimizing candidate pairs based on the transitive relations of similarity. In this paper, we propose a multi-signature based parallel entity resolution method, called multi-sig-er, which supports unstructured data and structured data. Two redundancy elimination strategies are adopted to prune the candidate pairs and reduce the number of similarity computation without affecting the resolution accuracy. Experimental results on real-world datasets show that our method tends to handle large datasets and it is more suitable for complex similarity computation than simple object matching.},
booktitle = {Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {1233–1236},
numpages = {4},
keywords = {MapReduce, entity resolution, redundancy elimination, blocking},
location = {Shenzhen, China},
series = {CCGRID '15}
}

@inproceedings{10.1145/3340531.3414073,
author = {Bowles, Juliana and Broccia, Giovanna and Nanni, Mirco},
title = {DataMod2020: 9th International Symposium "From Data to Models and Back"},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3414073},
doi = {10.1145/3340531.3414073},
abstract = {DataMod 2020 aims to bring together practitioners and researchers from academia, industry and research institutions interested in the combined application of computational modelling methods with data-driven techniques from the areas of knowledge management, data mining and machine learning. Modelling methodologies of interest include automata, agents, Petri nets, process algebras and rewriting systems. Application domains include social systems, ecology, biology, medicine, smart cities, governance, security, education, software engineering, and any other field that deals with complex systems and large amounts of data. Papers can present research results in any of the themes of interest for the symposium as well as application experiences, tools and promising preliminary ideas. Papers dealing with synergistic approaches that integrate modelling and knowledge management/discovery or that exploit knowledge management/discovery to develop/syntesise system models are especially welcome.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3531–3532},
numpages = {2},
keywords = {process calculi, processing mining, deep learning, machine learning, big data analytics, formal methods, text mining},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3397536.3422232,
author = {Soliman, Aiman and Terstriep, Jeffrey},
title = {Leveraging Geospatial Data Gateways to Support the Operational Application of Deep Learning Models: Vision Paper},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422232},
doi = {10.1145/3397536.3422232},
abstract = {Geospatial data providers have adopted a variety of science gateways as the primary method for accessing remote geospatial data. Early systems provided little more than a simple file transfer mechanism but over the past decade, advanced features were incorporated to allow users to retrieve data seamlessly without concern for native file formats, data resolution, or even spatial projections. However, the recent growth in Deep Learning models in the geospatial domains has exposed additional requirements for accessing geospatial repositories. In this paper we discussed the major data accessibility challenges faced by the Deep Learning community namely: (1) reproducibility of data preprocessing workflows, (2) optimizing data transfer between gateways and computational environments, and (3) minimizing local storage requirements using on-the-fly augmentation. In this paper, we present our vision of spatial data generators to act as middleware between geospatial data gateways and Deep Learning models. We propose advanced features for spatial data generators and describe how they could satisfy the data accessibility requirements of the geospatial Deep Learning community. Lastly, we argue that satisfying these data accessibility requirements will not only enhance the reproducibility of Deep Learning workflows and speed their development but will also improve the quality of training and prediction of operational Deep Learning models.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {593–596},
numpages = {4},
keywords = {Geospatial Big Data, Geospatial Data Gateway, Image Preprocessing, Remote Sensing, Deep Learning, Scientific Reproducibility},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{10.1145/2806416.2806418,
author = {Zhou, Xiaofang and Zheng, Kai and Jueng, Hoyoung and Xu, Jiajie and Sadiq, Shazia},
title = {Making Sense of Spatial Trajectories},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806418},
doi = {10.1145/2806416.2806418},
abstract = {Spatial trajectory data is widely available today. Over a sustained period of time, trajectory data has been collected from numerous GPS devices, smartphones, sensors and social media applications. Daily increases of real-time trajectory data have also been phenomenal in recent years. More and more new applications have emerged to derive business values from both trajectory data warehouses and real-time trajectory data. Due to their very large volumes, their nature of streaming, their highly variable levels of data quality, as well as many possible links with other types of data, making sense of spatial trajectory data becomes one of the crucial areas for big data analytics. In this paper we will present a review of the extensive work in spatiotemporal data management and trajectory mining, and discuss new challenges and new opportunities in the context of new applications, focusing on recent advances in trajectory data management and trajectory mining from their foundations to high performance processing with modern computing infrastructure.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {671–672},
numpages = {2},
keywords = {trajectory mining, spatiotemporal database, trajectory data management},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@article{10.1145/3328747,
author = {Colborne, Adrienne and Smit, Michael},
title = {Characterizing Disinformation Risk to Open Data in the Post-Truth Era},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3328747},
doi = {10.1145/3328747},
abstract = {Curated, labeled, high-quality data is a valuable commodity for tasks such as business analytics and machine learning. Open data is a common source of such data—for example, retail analytics draws on open demographic data, and weather forecast systems draw on open atmospheric and ocean data. Open data is released openly by governments to achieve various objectives, such as transparency, informing citizen engagement, or supporting private enterprise. Critical examination of ongoing social changes, including the post-truth phenomenon, suggests the quality, integrity, and authenticity of open data may be at risk. We introduce this risk through various lenses, describe some of the types of risk we expect using a threat model approach, identify approaches to mitigate each risk, and present real-world examples of cases where the risk has already caused harm. As an initial assessment of awareness of this disinformation risk, we compare our analysis to perspectives captured during open data stakeholder consultations in Canada.},
journal = {J. Data and Information Quality},
month = {jun},
articleno = {13},
numpages = {13},
keywords = {risk identification, risk mitigation, fake news, post-truth, Open data, data quality assurance}
}

@inproceedings{10.1145/3511808.3557498,
author = {Demartini, Gianluca and Yang, Jie and Sadiq, Shazia},
title = {Workshop on Human-in-the-Loop Data Curation},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557498},
doi = {10.1145/3511808.3557498},
abstract = {Although data quality is a long-standing and enduring problem, it has recently received a resurgence of attention due to the fast proliferation of data analytics, machine learning, and decision-support applications built upon the wide-scale availability and accessibility of (big) data. The success of such applications heavily relies on not only the quantity, but also the quality of data. Data curation, which may include annotation, cleaning, transformation, integration, etc., is a critical step to provide adequate assurances on the quality of analytics and machine learning results. Such data preparation activities are recognised as time and resource intensive for data scientists as data often comes with a number of challenges that need to be tackled before it can be used in practice. Data re-purposing and the resulting distance between design and use intentions of the data, is a fundamental issue behind many of these challenges. These challenges include a variety of data issues such as noise and outliers, incompleteness, representativeness or biases, heterogeneity of format or semantics, etc. Mishandling these challenges can lead to negative and sometimes damaging effects, especially in critical domains like healthcare, transport, and finance. An observable distinct feature of data quality in these contexts is the increasingly important role played by humans, being often the source of data generation and the active players in data curation. This workshop will provide an opportunity to explore the interdisciplinary overlap between manual, automated, and hybrid human-machine methods of data curation.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {5161–5162},
numpages = {2},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3316416.3316425,
author = {Singh, Lisa and Deshpande, Amol and Zhou, Wenchao and Banerjee, Arindam and Bowers, Alex and Friedler, Sorelle and Jagadish, H.V. and Karypis, George and Obradovic, Zoran and Vullikanti, Anil and Zuo, Wangda},
title = {NSF BIGDATA PI Meeting - Domain-Specific Research Directions and Data Sets},
year = {2019},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3316416.3316425},
doi = {10.1145/3316416.3316425},
abstract = {In March 2017, PIs and co-PIs funded through the NSF BIGDATA program were brought together along with selected industry and government invitees to discuss current research, identify current challenges, discuss promising future directions, foster new collaborations, and share accomplishments, at BDPI-2017. Given that two recent NITRD [2] and NSF [1] meeting reports contained a set of recommendations, grand challenges, and high impact priorities for Big Data, the organizers of this meeting shifted the focus of the breakout sessions to discuss problems and available data sets that exist in five application domains - policy, health, education, economy &amp; finance, and environment &amp; energy. These domains were selected based on a survey of the PIs/co-PIs and should not be interpreted as being more important than others. Slides that were presented by the different breakout group leaders are available at https://www.bi.vt.edu/ nsf-big-data/. We hope this report will serve as a blueprint for promising big data research in five application domains.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {32–35},
numpages = {4}
}

@inproceedings{10.1145/2567574.2567582,
author = {Piety, Philip J. and Hickey, Daniel T. and Bishop, M. J.},
title = {Educational Data Sciences: Framing Emergent Practices for Analytics of Learning, Organizations, and Systems},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567582},
doi = {10.1145/2567574.2567582},
abstract = {In this paper, we develop a conceptual framework for organizing emerging analytic activities involving educational data that can fall under broad and often loosely defined categories, including Academic/Institutional Analytics, Learning Analytics/Educational Data Mining, Learner Analytics/Personalization, and Systemic Instructional Improvement. While our approach is substantially informed by both higher education and K-12 settings, this framework is developed to apply across all educational contexts where digital data are used to inform learners and the management of learning. Although we can identify movements that are relatively independent of each other today, we believe they will in all cases expand from their current margins to encompass larger domains and increasingly overlap. The growth in these analytic activities leads to the need to find ways to synthesize understandings, find common language, and develop frames of reference to help these movements develop into a field.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {193–202},
numpages = {10},
keywords = {learning analytics, educational data science, data-driven decisions, learner analytics, educational data mining, theories and theoretical concepts for understanding learning, methods, tools for sense-making in learning analytics, big data, analytic approaches},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@article{10.14778/3415478.3415562,
author = {Whang, Steven Euijong and Lee, Jae-Gil},
title = {Data Collection and Quality Challenges for Deep Learning},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415562},
doi = {10.14778/3415478.3415562},
abstract = {Software 2.0 refers to the fundamental shift in software engineering where using machine learning becomes the new norm in software with the availability of big data and computing infrastructure. As a result, many software engineering practices need to be rethought from scratch where data becomes a first-class citizen, on par with code. It is well known that 80--90% of the time for machine learning development is spent on data preparation. Also, even the best machine learning algorithms cannot perform well without good data or at least handling biased and dirty data during model training. In this tutorial, we focus on data collection and quality challenges that frequently occur in deep learning applications. Compared to traditional machine learning, there is less need for feature engineering, but more need for significant amounts of data. We thus go through state-of-the-art data collection techniques for machine learning. Then, we cover data validation and cleaning techniques for improving data quality. Even if the data is still problematic, hope is not lost, and we cover fair and robust training techniques for handling data bias and errors. We believe that the data management community is well poised to lead the research in these directions. The presenters have extensive experience in developing machine learning platforms and publishing papers in top-tier database, data mining, and machine learning venues.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3429–3432},
numpages = {4}
}

@inproceedings{10.1109/WI-IAT.2014.139,
author = {Ba, Huafeng and Gao, Xiaoming and Zhang, Xiaofeng and He, Zhenyu},
title = {Protecting Data Privacy from Being Inferred from High Dimensional Correlated Data},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.139},
doi = {10.1109/WI-IAT.2014.139},
abstract = {In the era of big data, privacy becomes a challenging issue which already attracts a good number of research efforts. In the literature, most of existing privacy preserving algorithms focus on protecting users' privacy from being disclosed by making the set of designated semi-id features indiscriminate. However, how to automatically determine the appropriate semi-id features from high-dimensional correlated data is seldom studied. Therefore, in this paper we first theoretically study the problem and propose the IPFS algorithm to find all possible features forming the candidate semi-id feature set which can infer users' privacy. Then, the KIPFS algorithm is proposed to find the key features from the candidate semi-id feature set. By anonymizing the key feature set, called as key inferring privacy features (KIPFS), users' privacy is protected. To evaluate the effectiveness and the efficacy of the proposed approach, two state-of-the-art algorithms, i.e., K-anonymity and t-closeness, applied on the designated semi-id feature set are chose as the baseline algorithms and their revised versions are applied on the KIPFS for the performance comparison. The promising results showed that by anonymizing the identified KIPFS, both aforementioned algorithms can achieve better performance than the original ones in terms of efficiency and data quality.},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 02},
pages = {495–502},
numpages = {8},
keywords = {data publishing, algorithm, privacy preserving data mining},
series = {WI-IAT '14}
}

@inproceedings{10.1145/2335484.2335488,
author = {Artikis, Alexander and Etzion, Opher and Feldman, Zohar and Fournier, Fabiana},
title = {Event Processing under Uncertainty},
year = {2012},
isbn = {9781450313155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2335484.2335488},
doi = {10.1145/2335484.2335488},
abstract = {Big data is recognized as one of the three technology trends at the leading edge a CEO cannot afford to overlook in 2012. Big data is characterized by volume, velocity, variety and veracity ("data in doubt"). As big data applications, many of the emerging event processing applications must process events that arrive from sources such as sensors and social media, which have inherent uncertainties associated with them. Consider, for example, the possibility of incomplete data streams and streams including inaccurate data. In this tutorial we classify the different types of uncertainty found in event processing applications and discuss the implications on event representation and reasoning. An area of research in which uncertainty has been studied is Artificial Intelligence. We discuss, therefore, the main Artificial Intelligence-based event processing systems that support probabilistic reasoning. The presented approaches are illustrated using an example concerning crime detection.},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
pages = {32–43},
numpages = {12},
keywords = {pattern matching, artificial intelligence, uncertainty, event recognition, event processing},
location = {Berlin, Germany},
series = {DEBS '12}
}

@inproceedings{10.1145/3442381.3450066,
author = {Fang, Minghong and Sun, Minghao and Li, Qi and Gong, Neil Zhenqiang and Tian, Jin and Liu, Jia},
title = {Data Poisoning Attacks and Defenses to Crowdsourcing Systems},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450066},
doi = {10.1145/3442381.3450066},
abstract = {A key challenge of big data analytics is how to collect a large volume of (labeled) data. Crowdsourcing aims to address this challenge via aggregating and estimating high-quality data (e.g., sentiment label for text) from pervasive clients/users. Existing studies on crowdsourcing focus on designing new methods to improve the aggregated data quality from unreliable/noisy clients. However, the security aspects of such crowdsourcing systems remain under-explored to date. We aim to bridge this gap in this work. Specifically, we show that crowdsourcing is vulnerable to data poisoning attacks, in which malicious clients provide carefully crafted data to corrupt the aggregated data. We formulate our proposed data poisoning attacks as an optimization problem that maximizes the error of the aggregated data. Our evaluation results on one synthetic and two real-world benchmark datasets demonstrate that the proposed attacks can substantially increase the estimation errors of the aggregated data. We also propose two defenses to reduce the impact of malicious clients. Our empirical results show that the proposed defenses can substantially reduce the estimation errors of the data poisoning attacks.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {969–980},
numpages = {12},
keywords = {Data poisoning attacks, crowdsourcing, truth discovery},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3520084.3520099,
author = {Li, Yunze and Wu, Yuxuan and Tang, Ruisen},
title = {Data Aggregation and Anomaly Detection System for Isomerism and Heterogeneous Data},
year = {2022},
isbn = {9781450395519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520084.3520099},
doi = {10.1145/3520084.3520099},
abstract = {With the development of big data technology, data accessed by big data platforms maintain the features of mass, isomerism, heterogeneous, and streaming. Therefore, how to access the varied data sources of isomerism and heterogeneous data and how to process and analyze the data become the current challenges. In this paper, we design and implement a data aggregation and anomaly detection system for isomerism and heterogeneous data. The system proposes a novel isomerism and heterogeneous data access sub-system. The sub-system applies improved Avro as the unified data description format and presents different storage algorithms for data serialization to raise the data adaption efficiency. The system adopts Kafka as the message middleware for data aggregation and distribution. Also, we design the anomaly detection and alarming sub-system for detecting the anomalies of streaming data on time and notifying the users. The data aggregation and anomaly detection system has passed all the tests and applied in small and medium-sized enterprises.},
booktitle = {2022 The 5th International Conference on Software Engineering and Information Management (ICSIM)},
pages = {95–99},
numpages = {5},
keywords = {serialization and deserialization, isomerism and heterogeneous data, anomaly detection, Kafka},
location = {Yokohama, Japan},
series = {ICSIM 2022}
}

@inproceedings{10.1145/3495018.3495398,
author = {Zhang, Mingjie and Sheng, Yan and Tian, Nuo and Liu, Wei and Wang, Hui and Zhu, Longzhu and Xu, Qing},
title = {Exploring and Analyzing Data Mining Algorithm Technology in Internet Customer Ranking},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3495398},
doi = {10.1145/3495018.3495398},
abstract = {Based on the characteristics of online customers, such as user characteristics, interaction behavior, frequency of visits and business queries, this paper uses big data analysis mining algorithm to conduct exploratory analysis on each business data, and builds a weight division model (Entropy Value Method) to achieve online customer ranking.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {1352–1360},
numpages = {9},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}

