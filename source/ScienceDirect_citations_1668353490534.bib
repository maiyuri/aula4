@article{LI2021181,
title = {High-Throughput physiology-based stress response phenotyping: Advantages, applications and prospective in horticultural plants},
journal = {Horticultural Plant Journal},
volume = {7},
number = {3},
pages = {181-187},
year = {2021},
note = {Abiotic Stresses in Horticultural Plants},
issn = {2468-0141},
doi = {https://doi.org/10.1016/j.hpj.2020.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2468014120300996},
author = {Yanwei Li and Xinyi Wu and Wenzhao Xu and Yudong Sun and Ying Wang and Guojing Li and Pei Xu},
keywords = {Phenomics, Physiolomics, Isohydric/anisohydric, Abiotic stress},
abstract = {Phenomics is a new branch of science that provides high-throughput quantification of plant and animal traits at systems level. The last decade has witnessed great successes in high-throughput phenotyping of numerous morphological traits, yet major challenges still exist in precise phenotyping of physiological traits such as transpiration and photosynthesis. Due to the highly dynamic nature of physiological traits in responses to the environment, appropriate selection criteria and efficient screening systems at the physiological level for abiotic stress tolerance have been largely absent in plants. In this review, the current status of phenomics techniques was briefly summarized in horticultural plants. Specifically, the emerging field of high-throughput physiology-based phenotyping, which is referred to as “physiolomics”, for drought stress responses was highlighted. In addition to analyzing the advantages of physiology-based phenotyping over morphology-based approaches, recent examples that applied high-throughput physiological phenotyping to model and non-model horticultural plants were revisited and discussed. Based on the collective findings, we propose that high-throughput, non-destructive, and automatic physiological assays can and should be used as routine methods for phenotyping stress response traits in horticultural plants.}
}
@article{KICKBUSCH20211727,
title = {The Lancet and Financial Times Commission on governing health futures 2030: growing up in a digital world},
journal = {The Lancet},
volume = {398},
number = {10312},
pages = {1727-1776},
year = {2021},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(21)01824-9},
url = {https://www.sciencedirect.com/science/article/pii/S0140673621018249},
author = {Ilona Kickbusch and Dario Piselli and Anurag Agrawal and Ran Balicer and Olivia Banner and Michael Adelhardt and Emanuele Capobianco and Christopher Fabian and Amandeep {Singh Gill} and Deborah Lupton and Rohinton P Medhora and Njide Ndili and Andrzej Ryś and Nanjira Sambuli and Dykki Settle and Soumya Swaminathan and Jeanette Vega Morales and Miranda Wolpert and Andrew W Wyckoff and Lan Xue and Aferdita Bytyqi and Christian Franz and Whitney Gray and Louise Holly and Micaela Neumann and Lipsa Panda and Robert D Smith and Enow Awah {Georges Stevens} and Brian Li Han Wong}
}
@incollection{USHA2021281,
title = {Chapter 17 - Deciphering the animal genomics using bioinformatics approaches},
editor = {Sukanta Mondal and Ram Lakhan Singh},
booktitle = {Advances in Animal Genomics},
publisher = {Academic Press},
pages = {281-297},
year = {2021},
isbn = {978-0-12-820595-2},
doi = {https://doi.org/10.1016/B978-0-12-820595-2.00017-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205952000175},
author = {Talambedu Usha and Prachurjya Panda and Arvind Kumar Goyal and Shivani Sukhralia and Sarah Afreen and H.P. {Prashanth Kumar} and Dhivya Shanmugarajan and Sushil Kumar Middha},
keywords = {Animal genomes, Food, India, Sequencing techniques},
abstract = {Animal genomics is gaining popularity among researchers due to its utility-driven approaches. The major advantage lies in the understanding of how genes function and get expressed within various animal populations. Genomic understanding can propionate the thriving yield in farm animals to bioengineer innovative materials, enhanced productivity of livestock, xenotransplantation, and several other animal-based populous items for consumptions and even nonconsumption-based products like fabrics, silk. In this chapter, we present an introductory commentary on techniques and databases available to deduce animal genomes, a rapidly developing genome project resource, completed genomes summary of various domestic animals such as buffalo, sheep, and goat and the latest progress in the field. We have also flagged a concern with regard to resources and updates concerning farm livestock genome projects, especially in India, as compared to growing population and food demands across the globe within subsequent decades.}
}
@article{KLAPWIJK2021100902,
title = {Opportunities for increased reproducibility and replicability of developmental neuroimaging},
journal = {Developmental Cognitive Neuroscience},
volume = {47},
pages = {100902},
year = {2021},
issn = {1878-9293},
doi = {https://doi.org/10.1016/j.dcn.2020.100902},
url = {https://www.sciencedirect.com/science/article/pii/S1878929320301511},
author = {Eduard T. Klapwijk and Wouter {van den Bos} and Christian K. Tamnes and Nora M. Raschle and Kathryn L. Mills},
keywords = {Development, Open science, Sample size, Cognitive neuroscience, Transparency, Preregistration},
abstract = {Many workflows and tools that aim to increase the reproducibility and replicability of research findings have been suggested. In this review, we discuss the opportunities that these efforts offer for the field of developmental cognitive neuroscience, in particular developmental neuroimaging. We focus on issues broadly related to statistical power and to flexibility and transparency in data analyses. Critical considerations relating to statistical power include challenges in recruitment and testing of young populations, how to increase the value of studies with small samples, and the opportunities and challenges related to working with large-scale datasets. Developmental studies involve challenges such as choices about age groupings, lifespan modelling, analyses of longitudinal changes, and data that can be processed and analyzed in a multitude of ways. Flexibility in data acquisition, analyses and description may thereby greatly impact results. We discuss methods for improving transparency in developmental neuroimaging, and how preregistration can improve methodological rigor. While outlining challenges and issues that may arise before, during, and after data collection, solutions and resources are highlighted aiding to overcome some of these. Since the number of useful tools and techniques is ever-growing, we highlight the fact that many practices can be implemented stepwise.}
}
@article{SHEN2021258,
title = {Discovery of marageing steels: machine learning vs. physical metallurgical modelling},
journal = {Journal of Materials Science & Technology},
volume = {87},
pages = {258-268},
year = {2021},
issn = {1005-0302},
doi = {https://doi.org/10.1016/j.jmst.2021.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S1005030221002504},
author = {Chunguang Shen and Chenchong Wang and Pedro E.J. Rivera-Díaz-del-Castillo and Dake Xu and Qian Zhang and Chi Zhang and Wei Xu},
keywords = {Machine learning, Physical metallurgy, Small sample problem, Marageing steel},
abstract = {Physical metallurgical (PM) and data-driven approaches can be independently applied to alloy design. Steel technology is a field of physical metallurgy around which some of the most comprehensive understanding has been developed, with vast models on the relationship between composition, processing, microstructure and properties. They have been applied to the design of new steel alloys in the pursuit of grades of improved properties. With the advent of rapid computing and low-cost data storage, a wealth of data has become available to a suite of modelling techniques referred to as machine learning (ML). ML is being emergingly applied in materials discovery while it requires data mining with its adoption being limited by insufficient high-quality datasets, often leading to unrealistic materials design predictions outside the boundaries of the intended properties. It is therefore required to appraise the strength and weaknesses of PM and ML approach, to assess the real design power of each towards designing novel steel grades. This work incorporates models and datasets from well-established literature on marageing steels. Combining genetic algorithm (GA) with PM models to optimise the parameters adopted for each dataset to maximise the prediction accuracy of PM models, and the results were compared with ML models. The results indicate that PM approaches provide a clearer picture of the overall composition-microstructure-properties relationship but are highly sensitive to the alloy system and hence lack on exploration ability of new domains. ML conversely provides little explicit physical insight whilst yielding a stronger prediction accuracy for large-scale data. Hybrid PM/ML approaches provide solutions maximising accuracy, while leading to a clearer physical picture and the desired properties.}
}
@article{CHEN2021111375,
title = {A novel short-term load forecasting framework based on time-series clustering and early classification algorithm},
journal = {Energy and Buildings},
volume = {251},
pages = {111375},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111375},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821006599},
author = {Zhe Chen and Yongbao Chen and Tong Xiao and Huilong Wang and Pengwei Hou},
keywords = {Short-term load forecasting, Light gradient boosting machine (LightGBM), Time series clustering, Early classification, Feature engineering},
abstract = {With the development of data-driven models, extracting information from historical data for better energy forecasting is critically important for energy planning and optimization in buildings. Feature engineering is a key factor in improving the performance of forecasting models. Adding load pattern labels for different daily energy consumption patterns resulting from different time schedules and weather conditions can help improve forecasting accuracy. Traditionally, pattern labeling focuses mainly on finding a day similar to the forecasting day based on calendar or other information, such as weather conditions. The most intuitive approach for dividing historical time-series load into patterns is clustering; however, the pattern cannot be determined before the load is known. To address this problem, this study proposes a novel short-term load forecasting framework integrating an early classification algorithm that uses a stochastic algorithm to predetermine the load pattern of a forecasting day. In addition, a hybrid multistep method combining the strengths of single-step forecasting and recursive multistep forecasting is integrated into the framework. The proposed framework was validated through a case study using actual metered data. The results demonstrate that the early classification and proposed labeling strategy produce satisfactory forecasting accuracy and significantly improve the forecasting performance of the LightGBM model.}
}
@article{BANKER2021741,
title = {International Committee for Monitoring Assisted Reproductive Technologies (ICMART): world report on assisted reproductive technologies, 2013},
journal = {Fertility and Sterility},
volume = {116},
number = {3},
pages = {741-756},
year = {2021},
issn = {0015-0282},
doi = {https://doi.org/10.1016/j.fertnstert.2021.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0015028221002442},
author = {Manish Banker and Silke Dyer and Georgina M. Chambers and Osamu Ishihara and Markus Kupka and Jacques {de Mouzon} and Fernando Zegers-Hochschild and G. David Adamson},
keywords = {Assisted reproductive technology, IVF/ICSI outcome, frozen embryo transfer, ICMART, cumulative live birth rate, registry},
abstract = {Objective
To report the utilization, effectiveness, and safety of practices in assisted reproductive technology (ART) globally in 2013 and assess global trends over time.
Design
Retrospective, cross-sectional survey on the utilization, effectiveness, and safety of ART procedures performed globally during 2013.
Setting
Seventy-five countries and 2,639 ART clinics.
Patient(s)
Women and men undergoing ART procedures.
Intervention(s)
All ART.
Main Outcome Measure(s)
The ART cycles and outcomes on country-by-country, regional, and global levels. Aggregate country data were processed and analyzed based on methods developed by the International Committee for Monitoring Assisted Reproductive Technology (ICMART).
Result(s)
A total of 1,858,500 ART cycles were conducted for the treatment year 2013 across 2,639 clinics in 75 participating countries with a global participation rate of 73.6%. Reported and estimated data suggest 1,160,474 embryo transfers (ETs) were performed resulting in >344,317 babies. From 2012 to 2013, the number of reported aspiration and frozen ET cycles increased by 3% and 16.4%, respectively. The proportion of women aged >40 years undergoing nondonor ART increased from 25.2% in 2012 to 26.3% in 2013. As a percentage of nondonor aspiration cycles, intracytoplasmic sperm injection (ICSI) was similar to results for 2012. The in vitro fertilization (IVF)/ICSI combined delivery rates per fresh aspiration and frozen ET cycles were 24.2% and 22.8%, respectively. In fresh nondonor cycles, single ET increased from 33.7% in 2012 to 36.5% in 2013, whereas the average number of transferred embryos was 1.81—again with wide country variation. The rate of twin deliveries after fresh nondonor transfers was 17.9%; the triplet rate was 0.7%. In frozen ET cycles performed in 2013, single ET was used in 57.6%, with an average of 1.49 embryos transferred and twin and triplet rates of 10.8% and 0.4%, respectively. The cumulative delivery rate per aspiration was 30.4%, similar to that in 2012. Perinatal mortality rate per 1,000 births was 22.2% after fresh IVF/ICSI and 16.8% after frozen ET. The data presented depended on the quality and completeness of the data submitted by individual countries. This report covers approximately two-thirds of world ART activity. Continued efforts to improve the quality and consistency of reporting ART data by registries are still needed.
Conclusion(s)
Reported ART cycles, effectiveness, and safety increased between 2012 and 2013 with adoption of a better method for estimating unreported cycles.
Comité Internacional para la monitorización de las Tecnologías de Reproducción Asistida (ICMART): informe mundial.
Objetivo
Informar sobre la utilización, la eficacia y la seguridad de las prácticas de la tecnología de reproducción asistida (TRA) a nivel mundial en 2013 y evaluar las tendencias mundiales a lo largo del tiempo.
Diseño
Encuesta retrospectiva y transversal sobre la utilización, la eficacia y la seguridad de los procedimientos de TRA realizados a nivel mundial durante el año 2013.
Entorno
Setenta y cinco países y 2639 clínicas de TRA.
Pacientes
Mujeres y hombres sometidos a procedimientos de TRA.
Intervención(es)
Todas las TRA.
Medida(s) principal(es) del resultado
Los ciclos de TRA y los resultados a nivel de país, regional y mundial. Los datos agregados de los países se procesaron y analizaron según los métodos desarrollados por el Comité Internacional para la Vigilancia de las Tecnologías de Reproducción Asistida (ICMART). Tecnología de Reproducción Asistida (ICMART).
Resultados
En el año de tratamiento 2013 se realizaron un total de 1.858.500 ciclos de TRA en 2.639 clínicas de 75 países participantes con una tasa de participación global del 73,6%. Los datos informados y estimados sugieren que se realizaron 1.160.474 transferencias de embriones (TE) que dieron lugar a >344.317 bebés. De 2012 a 2013, el número de ciclos de aspiración y de TE congelados notificados aumentó un 3% y un 16,4%, respectivamente. La proporción de mujeres de >40 años que se sometieron a TRA autóloga aumentó del 25,2% en 2012 al 26,3% en 2013. Como porcentaje de ciclos de aspiración autóloga, la inyección intracitoplasmática de espermatozoides (ICSI) fue similar a los resultados de 2012. Las tasas de parto combinadas de fecundación in vitro (FIV)/ICSI por ciclos de aspiración en fresco y TE congelada fueron del 24,2% y el 22,8%, respectivamente. En los ciclos autólogos en fresco,la TE única aumentó del 33,7% en 2012 al 36,5% en 2013, mientras que el número medio de embriones transferidos fue de 1,81 -de nuevo, con una amplia variación entre países. La tasa de partos gemelares tras transferencias autólogas en fresco fue del 17,9%; la tasa de trillizos fue del 0,7%. En los ciclos de TE de congelados realizados en 2013, se utilizó la TE simple en el 57,6%, con una media de 1,49 embriones transferidos y unas tasas de gemelos y trillizos del 10,8% y 0,4%, respectivamente. La tasa acumulada de partos por aspiración fue del 30,4%, similar a la de 2012. La tasa de mortalidad perinatal por cada 1.000 nacimientos fue del 22,2% tras la FIV/ICSI en fresco y del 16,8% tras la TE de congelados. Los datos presentados dependían de la calidad y la exhaustividad de los datos presentados por cada país. Este informe abarca aproximadamente dos tercios de la actividad mundial de TRA. Los esfuerzos continuos para mejorar la calidad y la coherencia de los datos presentados por los registros sobre la TRA deben seguir siendo objeto de esfuerzos continuos.
Conclusión(es)
Los ciclos de TRA notificados, la eficacia y la seguridad aumentaron entre 2012 y 2013 con la adopción de un mejor método para estimar los ciclos no reportados.}
}
@article{CHEN2021101713,
title = {Evaluation of occupational stress management for improving performance and productivity at workplaces by monitoring the health, well-being of workers},
journal = {Aggression and Violent Behavior},
pages = {101713},
year = {2021},
issn = {1359-1789},
doi = {https://doi.org/10.1016/j.avb.2021.101713},
url = {https://www.sciencedirect.com/science/article/pii/S1359178921001671},
author = {Ming Chen and Bin Ran and Xiaoying Gao and Guilan Yu and Jing Wang and J. Jagannathan},
keywords = {Occupational stress, Improving performance, Productivity, Stress, Health, Information technology},
abstract = {Competence lack, inadequate social support at work leads to the inability of workers since they are suffering from occupational stress. This will cause distress, burnout or psychosomatic difficulties, decreases in quality of life and service provision. Some of them may connect to work in an individual's personal life, both as managers, recognize stressors in their department, and respond on a departmental basis or individually. Many workers say that their employee utilization monitoring is not sufficient until computer counting involves. In addition, the systems are associated with higher stress, health hazards, and work unhappiness among supervised personnel. Monitoring these problems can increase employee awareness of personal productivity, providing performance information more promptly and frequently. Interventions are based on an examination of the variables that impact the performance of health workers. The article for employee stress management and health monitoring using information technology (SMHM-IT) gives better working conditions, motivation, retention, etc. Evaluation of occupational risks is a framework introduced to manage health and safety implications associated with preventative measures for improving and protecting the highest physical, social, or emotional working skills. Statistical data analysis is introduced to compare a medical specialty which includes analysis of employee's details. Results are compared with assessments shows that architecture offers successful in-time accessibility of performance 98.12% is achieved.}
}
@incollection{CASASROMA2021111,
title = {Chapter 6 - A literature review on artificial intelligence and ethics in online learning},
editor = {Santi Caballé and Stavros N. Demetriadis and Eduardo Gómez-Sánchez and Pantelis M. Papadopoulos and Armin Weinberger},
booktitle = {Intelligent Systems and Learning Data Analytics in Online Education},
publisher = {Academic Press},
pages = {111-131},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-823410-5},
doi = {https://doi.org/10.1016/B978-0-12-823410-5.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234105000061},
author = {Joan Casas-Roma and Jordi Conesa},
keywords = {artificial intelligence in education, artificial intelligence, data science, learning analytics, ethics, artificial morality, online learning},
abstract = {In recent years, artificial intelligence (AI) has been used in online learning to improve teaching and learning, with the aim of providing a more efficient, purposeful, adaptive, ubiquitous, and fair learning experiences. However, and as it has been seen in other contexts, the integration of AI can have unforeseen consequences with detrimental effects which can result in unfair and discriminatory decisions. Therefore it is worth thinking about potential risks that learning environments integrating AI systems might pose. This work explores the intersections between AI, online learning, and ethics in order to understand the ethical concerns surrounding this crossroads. We review the main ethical challenges identified in the literature and distill a set of guidelines to support the ethical design and integration of AI systems in online learning environments. This should help ensure that online learning is how is meant to be: accessible, inclusive, fair, and beneficial to society.}
}
@article{CABITZA2021104510,
title = {The need to separate the wheat from the chaff in medical informatics: Introducing a comprehensive checklist for the (self)-assessment of medical AI studies},
journal = {International Journal of Medical Informatics},
volume = {153},
pages = {104510},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104510},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621001362},
author = {Federico Cabitza and Andrea Campagner},
keywords = {Medical artificial intelligence, Machine learning, Checklist, Quality auditing},
abstract = {This editorial aims to contribute to the current debate about the quality of studies that apply machine learning (ML) methodologies to medical data to extract value from them and provide clinicians with viable and useful tools supporting everyday care practices. We propose a practical checklist to help authors to self assess the quality of their contribution and to help reviewers to recognize and appreciate high-quality medical ML studies by distinguishing them from the mere application of ML techniques to medical data.}
}
@article{DOBBELAERE20211201,
title = {Machine Learning in Chemical Engineering: Strengths, Weaknesses, Opportunities, and Threats},
journal = {Engineering},
volume = {7},
number = {9},
pages = {1201-1211},
year = {2021},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2021.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S2095809921002010},
author = {Maarten R. Dobbelaere and Pieter P. Plehiers and Ruben {Van de Vijver} and Christian V. Stevens and Kevin M. {Van Geem}},
keywords = {Artificial intelligence, Machine learning, Reaction engineering, Process engineering},
abstract = {Chemical engineers rely on models for design, research, and daily decision-making, often with potentially large financial and safety implications. Previous efforts a few decades ago to combine artificial intelligence and chemical engineering for modeling were unable to fulfill the expectations. In the last five years, the increasing availability of data and computational resources has led to a resurgence in machine learning-based research. Many recent efforts have facilitated the roll-out of machine learning techniques in the research field by developing large databases, benchmarks, and representations for chemical applications and new machine learning frameworks. Machine learning has significant advantages over traditional modeling techniques, including flexibility, accuracy, and execution speed. These strengths also come with weaknesses, such as the lack of interpretability of these black-box models. The greatest opportunities involve using machine learning in time-limited applications such as real-time optimization and planning that require high accuracy and that can build on models with a self-learning ability to recognize patterns, learn from data, and become more intelligent over time. The greatest threat in artificial intelligence research today is inappropriate use because most chemical engineers have had limited training in computer science and data analysis. Nevertheless, machine learning will definitely become a trustworthy element in the modeling toolbox of chemical engineers.}
}
@article{GE2021107394,
title = {Improved adaptive gray wolf genetic algorithm for photovoltaic intelligent edge terminal optimal configuration},
journal = {Computers and Electrical Engineering},
volume = {95},
pages = {107394},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107394},
url = {https://www.sciencedirect.com/science/article/pii/S004579062100361X},
author = {Leijiao Ge and Jiaheng Liu and Bo Wang and Yue Zhou and Jun Yan and Ming Wang},
keywords = {Distributed photovoltaic, Photovoltaic intelligent edge terminal, Optimal configuration, Improved adaptive genetic algorithm},
abstract = {Photovoltaic (PV) intelligent edge terminals (IETs) integrate data acquisition, processing, storage and upload functions for intelligent operations of PV power stations. However, the cost of installing a PV IET at one PV station is relatively high. In order to achieve the goal of multiple distributed PV stations sharing one PV IET on the premise of ensuring reliability, the paper proposes a method for the optimal configuration of PV IETs. First of all, considering the economy and reliability of optimizing configuration of PV IET, a two-layer optimization model is established. After that, to solve the nonlinearity of the proposed model, an improved adaptive genetic algorithm and gray wolf optimization (IAGA-GWO) is proposed. Finally, through two application cases of PV IETs, it is proved that the optimized configuration method in this paper can reduce the cost under the premise of ensuring the reliability.}
}
@article{DAVIDOVIC2021109533,
title = {Application of artificial intelligence for detection of chemico-biological interactions associated with oxidative stress and DNA damage},
journal = {Chemico-Biological Interactions},
volume = {345},
pages = {109533},
year = {2021},
issn = {0009-2797},
doi = {https://doi.org/10.1016/j.cbi.2021.109533},
url = {https://www.sciencedirect.com/science/article/pii/S0009279721001691},
author = {Lazar M. Davidovic and Darko Laketic and Jelena Cumic and Elena Jordanova and Igor Pantic},
keywords = {Reactive oxygen species, Radiation, Machine learning, Non-coding DNA, Aging},
abstract = {In recent years, various AI-based methods have been developed in order to uncover chemico-biological interactions associated with DNA damage and oxidative stress. Various decision trees, bayesian networks, random forests, logistic regression models, support vector machines as well as deep learning tools, have great potential in the area of molecular biology and toxicology, and it is estimated that in the future, they will greatly contribute to our understanding of molecular and cellular mechanisms associated with DNA damage and repair. In this concise review, we discuss recent attempts to build machine learning tools for assessment of radiation – induced DNA damage as well as algorithms that can analyze the data from the most frequently used DNA damage assays in molecular biology. We also review recent works on the detection of antioxidant proteins with machine learning, and the use of AI-related methods for prediction and evaluation of noncoding DNA sequences. Finally, we discuss previously published research on the potential application of machine learning tools in aging research.}
}
@article{GONZALEZ2021100858,
title = {Urban climate and resiliency: A synthesis report of state of the art and future research directions},
journal = {Urban Climate},
volume = {38},
pages = {100858},
year = {2021},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2021.100858},
url = {https://www.sciencedirect.com/science/article/pii/S2212095521000882},
author = {Jorge E. González and Prathap Ramamurthy and Robert D. Bornstein and Fei Chen and Elie R. Bou-Zeid and Masoud Ghandehari and Jeffrey Luvall and Chandana Mitra and Dev Niyogi},
keywords = {Urban climate resiliency, Extreme urban weather, Climate adaptation, Modeling and observations of extreme urban weather, Knowledge transfer of urban climate data, Cyber-systems for urban climate and weather},
abstract = {The Urban Climate and Resiliency-Science Working Group (i.e., The WG) was convened in the summer of 2018 to explore the scientific grand challenges related to climate resiliency of cities. The WG leveraged the presentations at the 10th International Conference on Urban Climate (ICUC10) held in New York City (NYC) on 6–10 August 2018 as input forum. ICUC10 was a collaboration between the International Association of Urban Climate, American Meteorological Society, and World Meteorological Organization. It attracted more than 600 participants from more than 50 countries, resulting in close to 700 oral and poster presentations under the common theme of “Sustainable & Resilient Urban Environments”. ICUC10 covered topics related to urban climate and weather processes with far-reaching implications to weather forecasting, climate change adaptation, air quality, health, energy, urban planning, and governance. This article provides a synthesis of the analysis of the current state of the art and of the recommendations of the WG for future research along each of the four Grand Challenges in the context of urban climate and weather resiliency; Modeling, Observations, Cyber-Informatics, and Knowledge Transfer & Applications.}
}
@article{HUANG2021103115,
title = {Privacy protection among three antithetic-parties for context-aware services},
journal = {Journal of Network and Computer Applications},
volume = {191},
pages = {103115},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103115},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001351},
author = {Yan Huang and Wei Li and Jinbao Wang and Zhipeng Cai and Anu G. Bourgeois},
keywords = {Privacy protection, Game theory, Context-aware services},
abstract = {The popularity of context-aware services is improving the quality of life, while raising serious privacy issues. In order for users to receive quality service, they are at risk of leaking private information by adversaries that are possibly eavesdropping on the data and/or by the untrusted service platform selling off its data to adversaries. Game theory has been utilized as a powerful tool to achieve privacy preservation by strategically balancing the trade-off between profit (service) and cost (data leakage) for the user. However, most of the existing schemes cannot fully exploit the power of game theory, as they fail to depict the mutual relationship between any two (of the three) parties involved: user, platform, and adversary. Existing schemes are also not always able to provide specific guidance for a user to reduce the impact of the joint threats from the platform and adversary. In this paper, we design a privacy-preserving game to quantify the three parties’ concerns and capture interactions between any two of them. We also identify the best strategy for each party at a fine-grained level, i.e. specific settings, not simply binary choices. We validate the performance of our proposed game model through both a theoretical analysis and experiments.}
}
@article{FINN20211021,
title = {Is it time to put rest to rest?},
journal = {Trends in Cognitive Sciences},
volume = {25},
number = {12},
pages = {1021-1032},
year = {2021},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2021.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364661321002345},
author = {Emily S. Finn},
keywords = {resting state, task-based, functional connectivity, naturalistic tasks, brain–behavior prediction},
abstract = {The so-called resting state, in which participants lie quietly with no particular inputs or outputs, represented a paradigm shift from conventional task-based studies in human neuroimaging. Our foray into rest was fruitful from both a scientific and methodological perspective, but at this point, how much more can we learn from rest on its own? While rest still dominates in many subfields, data from tasks have empirically demonstrated benefits, as well as the potential to provide insights about the mind in addition to the brain. I argue that we can accelerate progress in human neuroscience by de-emphasizing rest in favor of more grounded experiments, including promising integrated designs that respect the prominence of self-generated activity while offering enhanced control and interpretability.}
}
@article{WANG2021112665,
title = {A method for land surface temperature retrieval based on model-data-knowledge-driven and deep learning},
journal = {Remote Sensing of Environment},
volume = {265},
pages = {112665},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112665},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721003850},
author = {Han Wang and Kebiao Mao and Zijin Yuan and Jiancheng Shi and Mengmeng Cao and Zhihao Qin and Sibo Duan and Bohui Tang},
keywords = {Land surface temperature (LST), Model-data-knowledge-driven, Deep learning, Geophysical logical reasoning, Expert knowledge},
abstract = {Most algorithms for land surface temperature (LST) retrieval depend on acquiring prior knowledge. To overcome this drawback, we propose a novel LST retrieval method based on model-data-knowledge-driven and deep learning, called the MDK-DL method. Based on the expert knowledge and radiation transfer model, we deduce LST retrieval mechanism and determine the best combination of the thermal infrared (TIR) bands of the sensor. Then, we use the radiation transfer model simulation and reliable satellite-ground data to establish a training and test database, and finally use the deep learning neural network for optimal computation. Three typical high-, medium- and low-spatial-resolution TIR remote sensing datasets (from Gaofen, the Moderate Resolution Imaging Spectroradiometer (MODIS), and Fengyun) are used for theoretical simulation and application analysis. The simulation shows that the minimum mean absolute error (MAE) is less than 0.1 K (standard deviation: 0.04 K; correlation coefficient: 1.000) at a small viewing direction (<7.5°) and less than 0.8 K at a large viewing direction (<65°). The in situ validation shows that the minimum MAE obtained by the optimal band combination is approximately 1 K (root mean square error (RMSE) = 1.12 K; coefficient of determination (R2) = 0.902). The retrieval accuracy is improved by increasing the number of TIR bands in the atmospheric window, and adding accurate atmospheric water vapor information produces better results. In general, four TIR bands in the atmospheric window bands are sufficient to retrieve the LST with high accuracy. Likewise, three TIR bands plus atmospheric water vapor information are sufficient for the retrieval requirements. All analyses indicate that our method is feasible and reliably accurate and can also be used to help design the instrument band to retrieve the LST with high precision.}
}
@incollection{MLADENOVIC202112,
title = {Mobility as a Service},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {12-18},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10607-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717106074},
author = {Miloš N. Mladenović},
keywords = {Emerging technology, Mobility governance, Mobility on demand, Mobility system, Responsible innovation, Social justice, Transportation network company, Transport policy},
abstract = {Mobility as a Service (MaaS) is a relatively fast-growing emerging technology based on the vision of integration (incl., policy, operational, informational, and transactional levels) and customization in transport systems. The user is expected to receive information, book, and pay for a choice of different mobility services by accessing a “one-stop-shop” or “mobility platform” via digital interfaces. Although highly uncertain, MaaS has significant potential to exert a considerable impact on the socio-technical domains in and beyond mobility. Such implications are also in terms of the composition of actors, institutions, and patterns of interactions among those, along with the associated innovation processes. Technological transition from niche to regime for MaaS depends on associated rhetoric as well as a wider set of converging socio-technical factors of societal automation, digitalization, and reregulation. Ultimately, transport policy and governance institutions will have to reflect and act on the potential undesired consequences from the depolitization of MaaS technological development.}
}
@incollection{YADAV2021123,
title = {Chapter 8 - A research review on semantic interoperability issues in electronic health record systems in medical healthcare},
editor = {Sanjay Kumar Singh and Ravi Shankar Singh and Anil Kumar Pandey and Sandeep S. Udmale and Ankit Chaudhary},
booktitle = {IoT-Based Data Analytics for the Healthcare Industry},
publisher = {Academic Press},
pages = {123-138},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-821472-5},
doi = {https://doi.org/10.1016/B978-0-12-821472-5.00009-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128214725000090},
author = {Rimmy Yadav and Saniksha Murria and Anil Sharma},
keywords = {Medical healthcare, Electronic health record, Interoperability, Semantic interoperability},
abstract = {With constantly diminishing costs and prolonged effectiveness of wireless communication and transmission techniques and, importantly, the Internet of Things (IoT) emerging as a powerful technology, some aspects of our lives have changed and broadened. The healthcare sector in particular is a developing and highly demanding application sector. IoT contributions to the medical healthcare domain include remote health and monitoring services, care for the elderly, recognition as well as tactical management of chronic illnesses, and offering of adaptive and self-regulated medical facilities. In medical healthcare, electronic health record (EHR) systems provide efficient management of clinical records in today’s clinical healthcare organizations. However, medical records are generating huge amounts of data, with every medical record having its own standard pattern, schema, and level of abstraction and interoperability. To interact with EHRs, medical stakeholders must use standard and well-structured methods and ontology-based languages to analyze and mine the useful information from huge data records. Much research has been done on interoperability issues, particularly syntactic interoperability and technical interoperability. After reviewing the research articles and chapters from respected medical databases such as IEEE Xplore, Elsevier, and Science Direct, the authors noted that semantic interoperability is, in the EHR framework, one of the critical issues. To achieve full semantic interoperability, researchers and scholars have developed and structured numerous methodologies, tools, and techniques. This research review thus includes methodologies, frameworks, tools, and models, along with their advantages and limitations, developed by researchers to cope with semantic interoperability issues in medical healthcare. Furthermore, in this chapter, the authors have focused on searching the papers related to the semantic web with a model-driven architecture (MDA) approach for semantic interoperability. Applications of the MDA approach and advanced features of the semantic web may be able to resolve the issue of semantic interoperability.}
}
@article{ZAMORA2021138,
title = {Massive data screening is a second opportunity to improve the management of patients with familial hypercholesterolemia phenotype},
journal = {Clínica e Investigación en Arteriosclerosis (English Edition)},
volume = {33},
number = {3},
pages = {138-147},
year = {2021},
issn = {2529-9123},
doi = {https://doi.org/10.1016/j.artere.2020.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S2529912321000231},
author = {Alberto Zamora and Guillem Paluzie and Joan García-Vilches and Oriol Alonso Gisbert and Ana Inés {Méndez Martínez} and Núria Plana and Cèlia Rodríguez-Borjabad and Daiana Ibarretxe and Anabel Martín-Urda and Luis Masana},
keywords = {Familial hypercholesterolemia, Massive data screening, Profiling of patients, Hipercolesterolemia familiar, Rastreo masivo de datos, Perfilado de pacientes},
abstract = {Introduction
Familial Hypercholesterolemia (FH) is an autosomal dominant disease with an estimated prevalence between 1/200–250. It is under-treated and underdiagnosed. Massive data screening can increase the detection of patients with FH.
Methods
Study population: Residents in the health coverage area (N: 195.000 inhabitants) and with at least one determination of cholesterol linked to low-density lipoproteins (LDLC) carried out between January 1, 2010 and December 30, 2019. The highest LDL-C values were selected. Exclusion criteria: nephrotic syndrome, hypothyroidism, Hypothyroid treatment or triglycerides > 400 mg/dL. Seven algorithms suggestive of Familial Hypercholesterolemia Phenotype (HF-P) were analyzed, selecting the most efficient algorithm that could easily be translated into clinical practice.
Results
Based on 6.264.877 assistances and 288.475 patients, after applying the inclusionexclusion criteria, 504.316 tests were included, corresponding to 106.382 adults and 10.509 < 18 years. The selected algorithm presented a prevalence of 0.62%. 840 patients with HF-P were detected, 55.8% being women and 178 < 18 years old, 9.3% had a history of cardiovascular disease (CVD) and 16.4% had died. 65% of the patients in primary prevention had LDL-C values > 130 mg/dL and 83% in secondary prevention values > 70 mg/dL. A ratio of 7.64 (1–18) patients with HF-P per analytical requesting physician was obtained.
Conclusions
Massive data screening and patient profiling are effective tools and easily applicable in clinical practice for the detection of patients with FH.
Resumen
Introducción
La Hipercolesterolemia Familiar (HF) es una enfermedad autósómica dominante con una prevalencia estimada entre 1/200–250. Se encuentra infratratada e infradiagnosticada. El rastreo masivo de datos puede incrementar la detección de pacientes con HF.
Métodos
Población a estudio: Residentes en la zona sanitaria de cobertura (N: 195.000 habitantes) y con al menos una determinación de colesterol ligado a lipoproteínas de baja densidad (C-LDL) realizada entre el 1 de Enero de 2010 y el 30 de Diciembre de 2019. Se seleccionaron los valores más altos de C-LDL. Criterios de exclusión: síndrome nefrótico, hipotiroidismo, tratamiento hipotiroideo o triglicéridos > 400 mg/dL. Se analizaron 7 algoritmos sugestivos de fenotipo de Hipercolesterolemia Familiar (FHF). Se seleccionó el algoritmo más eficaz y de fácil traslación a la práctica clínica.
Resultados
Partiendo de 6.264.877 asistencias y 288.475 pacientes tras aplicar los criterios de inclusión-exclusión se incluyeron 504.316 analíticas correspondiendo a 106.382 adultos y 10.509 < 18 años.El algoritmo seleccionado presentó una prevalencia de 0.62%.Se detectaron 840 pacientes con fenotipo de Hipercolestereolemia Familiar (FHF) siendo el 55.8% mujeres y 178 < 18 años, El 9.3% tenían antecedentes de enfermedad cardio-vascular (ECV) y 16.4% habían fallecido.El 65% de los pacientes en prevención primaria presentaron valores de C-LDL > 130 mg/dL y el 83% en prevención secundaria valores > 70 mg/dL.Se obtuvo una ratio de 7.64 (1–18) pacientes con HF-P por médico solicitante de analítica.
Conclusiones
El rastreo masivo de datos y el perfilado de pacientes son herramientas eficaces y fácilmente aplicables en práctica clínica para la detección de pacientes con HF.}
}
@article{PAUDEL2021103016,
title = {Machine learning for large-scale crop yield forecasting},
journal = {Agricultural Systems},
volume = {187},
pages = {103016},
year = {2021},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2020.103016},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X20308775},
author = {Dilli Paudel and Hendrik Boogaard and Allard {de Wit} and Sander Janssen and Sjoukje Osinga and Christos Pylianidis and Ioannis N. Athanasiadis},
keywords = {Crop yield prediction, Machine learning, Modularity, Reusability, Large-scale crop yield forecasting},
abstract = {Many studies have applied machine learning to crop yield prediction with a focus on specific case studies. The data and methods they used may not be transferable to other crops and locations. On the other hand, operational large-scale systems, such as the European Commission's MARS Crop Yield Forecasting System (MCYFS), do not use machine learning. Machine learning is a promising method especially when large amounts of data are being collected and published. We combined agronomic principles of crop modeling with machine learning to build a machine learning baseline for large-scale crop yield forecasting. The baseline is a workflow emphasizing correctness, modularity and reusability. For correctness, we focused on designing explainable predictors or features (in relation to crop growth and development) and applying machine learning without information leakage. We created features using crop simulation outputs and weather, remote sensing and soil data from the MCYFS database. We emphasized a modular and reusable workflow to support different crops and countries with small configuration changes. The workflow can be used to run repeatable experiments (e.g. early season or end of season predictions) using standard input data to obtain reproducible results. The results serve as a starting point for further optimizations. In our case studies, we predicted yield at regional level for five crops (soft wheat, spring barley, sunflower, sugar beet, potatoes) and three countries (the Netherlands (NL), Germany (DE), France (FR)). We compared the performance with a simple method with no prediction skill, which either predicted a linear yield trend or the average of the training set. We also aggregated the predictions to the national level and compared with past MCYFS forecasts. The normalized RMSE (NRMSE) for early season predictions (30 days after planting) were comparable for NL (all crops), DE (all except soft wheat) and FR (soft wheat, spring barley, sunflower). For example, NRMSE was 7.87 for soft wheat (NL) (6.32 for MCYFS) and 8.21 for sugar beet (DE) (8.79 for MCYFS). In contrast, NRMSEs for soft wheat (DE), sugar beet (FR) and potatoes (FR) were twice as much compared to MCYFS. NRMSEs for end of season were still comparable to MCYFS for NL, but worse for DE and FR. The baseline can be improved by adding new data sources, designing more predictive features and evaluating different machine learning algorithms. The baseline will motivate the use of machine learning in large-scale crop yield forecasting.}
}
@article{WANG2021331,
title = {A data driven approach to assessing the reliability of using taxicab as probes for Real-Time route selections},
journal = {Journal of Intelligent Transportation Systems},
volume = {25},
number = {4},
pages = {331-342},
year = {2021},
issn = {1547-2450},
doi = {https://doi.org/10.1080/15472450.2019.1617142},
url = {https://www.sciencedirect.com/science/article/pii/S1547245022002961},
author = {Zheng Wang and Wei-Hua Lin and Wangtu Xu},
keywords = {Data driven approach, route choice, taxi service, travel time estimation},
abstract = {Taxi service is one of the most important modes for urban transportation. In recent years, many taxi companies have been routinely collecting data to track the movement of each taxi for improving security, coordination, and service performance. This paper is intended to use the GPS vehicle positioning data to assess the route choice behavior of taxi drivers and explore if the routes selected by taxi drivers can be incorporated into a traveler information system. It is often perceived that taxi drivers have the ability to select quality routes assuming that: (1) they tend to be more knowledgeable about alternative routes and time-dependent traffic conditions than general public, including some publicly available route guidance systems due to the nature of their profession; and (2) they are typically more motivated to incorporate their knowledge about traffic conditions into their route choice decisions. An experimental study is conducted to examine the validity of these two assumptions. We have developed a framework that can effectively process the data into information about routes selected by taxi drivers and their associated travel times. The performance of the routes selected by taxi drivers is compared with the performance of those recommended by e-maps. Our results indicate that the routes selected by taxi drivers are generally more efficient than the routes recommended by some major e-maps, suggesting that taxi drivers are more active in selecting routes to avoid congestion.}
}
@article{LI2021102974,
title = {Inferring the trip purposes and uncovering spatio-temporal activity patterns from dockless shared bike dataset in Shenzhen, China},
journal = {Journal of Transport Geography},
volume = {91},
pages = {102974},
year = {2021},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2021.102974},
url = {https://www.sciencedirect.com/science/article/pii/S0966692321000272},
author = {Shaoying Li and Caigang Zhuang and Zhangzhi Tan and Feng Gao and Zhipeng Lai and Zhifeng Wu},
keywords = {Activity inference, Gravity model, Bayesian rules, Travel patterns, Dockless shared bikes},
abstract = {Trip purpose is closely related to travel patterns and plays an important role in urban planning and transportation management. Recently, there has been a growing interest in investigating the spatio-temporal patterns of dockless shared-bike usage and its influencing mechanisms. Few, however, have focused on revealing the travel patterns by inferring the purpose of dockless shared-bike trips at the individual level. We present a framework for inferring the purpose of dockless shared-bike users, based on gravity model and Bayesian rules, and conduct it in Shenzhen, China. We consider the comprehensive factors including distance, time, environment, activity type proportion, and service capacity of points of interest (POIs), the last two factors of which were usually neglected in previous transport studies. Especially, we integrated areas of interest (AOIs) and Tencent User density (TUD) social media data characterize the service capacity of POIs, which reflect the area and scale differences of different POI categories. Through the comparison between two improved models and the basic model, it is demonstrated that the introduction of activity type proportion and service capacity of POIs can improve the effectiveness of model for inferring the purposes of dockless shared-bike trips. Based on the obtained trip purposes, we further explore the spatio-temporal patterns of different activities and gain some insights into bike travel demand, which can inform scientific decisions for bicycle infrastructure planning and dockless shared- bike management.}
}
@incollection{GRANTMULLER2021135,
title = {Technology Enabled Data for Sustainable Transport Policy},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {135-141},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10627-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102671710627X},
author = {Susan M. Grant-Muller and Mahmoud Abdelrazek and Hannah Budnitz and Caitlin D. Cottrill and Fiona Crawford and Charisma F. Choudhury and Teddy Cunningham and Gillian Harrison and Frances C. Hodgson and Jinhyun Hong and Adam Martin and Oliver O’Brien and Claire Papaix and Panagiotis Tsoleridis},
keywords = {Big data, Ethics, Influencing technologies, New and emerging data forms, Policy, Transport modeling},
abstract = {The explosive growth of New and Emerging Data Forms (NEDF) has enabled profound new insights into human behavior, especially related to mobility. NEDF are facilitated by technologies such as smartphones, sensor networks and distributed computing architectures, which are all becoming increasingly advanced and widespread. NEDF, which may offer large sample sizes of high resolution data, offer great potential for informing sustainable transport policy, as well as the development of crosssectoral policies, covering public-health, environment, land-use, and social equity. However, many challenges in exploiting NEDF exist including accessing data in public/private ownership; understanding the representativeness, measuring/accommodating biases/missing data; and the integration of traditional data with new forms to maximize overall utility. Questions remain on whether NEDF can be used to actively influence travel choice/behavior, the new skills and additional resources needed by stakeholders to realize data potential and the ethical challenges for all engaging with the data.}
}
@article{OGRADY2021328,
title = {Service design for climate-smart agriculture},
journal = {Information Processing in Agriculture},
volume = {8},
number = {2},
pages = {328-340},
year = {2021},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2020.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214317320301906},
author = {Michael O'Grady and David Langton and Francesca Salinari and Peter Daly and Gregory O'Hare},
keywords = {Smart agriculture, Climate services, Agrometeorology, Precision agriculture},
abstract = {Holistic information systems for climate-smart agriculture demands the seamless integration of various categories of climate, meteorological and weather data. Any actor in the agricultural value chain may harness weather forecasts at the short and medium-range, local weather history, and prevailing climatic conditions, to inform decision-making. Weather is fundamental to many day-to-day operations, especially at farm-level, influencing decision-making at various spatial and temporal scales. Many operational decisions ideally require hyper-localized service provision. In practice, integrating weather information into decision-support services demands a comprehensive understanding of various categories of weather-related data, their genesis, as well as the specific standards and data formats used by the meteorological community. This paper considers the weather as a crucial context for the delivery of farm-level operational services in smart agriculture, highlighting critical issues for reflection by system designers during the service design and implementation phases.}
}
@article{YANG2021100175,
title = {COSMOS next generation – A public knowledge base leveraging chemical and biological data to support the regulatory assessment of chemicals},
journal = {Computational Toxicology},
volume = {19},
pages = {100175},
year = {2021},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2021.100175},
url = {https://www.sciencedirect.com/science/article/pii/S2468111321000232},
author = {C. Yang and M.T.D. Cronin and K.B. Arvidson and B. Bienfait and S.J. Enoch and B. Heldreth and B. Hobocienski and K. Muldoon-Jacobs and Y. Lan and J.C. Madden and T. Magdziarz and J. Marusczyk and A. Mostrag and M. Nelms and D. Neagu and K. Przybylak and J.F. Rathman and J. Park and A-N Richarz and A.M. Richard and J.V. Ribeiro and O. Sacher and C. Schwab and V. Vitcheva and P. Volarath and A.P. Worth},
keywords = {Toxicity, Database, Public database, Knowledge hub, Study reliability, Analogue selection, Guided workflow},
abstract = {The COSMOS Database (DB) was originally established to provide reliable data for cosmetics-related chemicals within the COSMOS Project funded as part of the SEURAT-1 Research Initiative. The database has subsequently been maintained and developed further into COSMOS Next Generation (NG), a combination of database and in silico tools, essential components of a knowledge base. COSMOS DB provided a cosmetics inventory as well as other regulatory inventories, accompanied by assessment results and in vitro and in vivo toxicity data. In addition to data content curation, much effort was dedicated to data governance – data authorisation, characterisation of quality, documentation of meta information, and control of data use. Through this effort, COSMOS DB was able to merge and fuse data of various types from different sources. Building on the previous effort, the COSMOS Minimum Inclusion (MINIS) criteria for a toxicity database were further expanded to quantify the reliability of studies. COSMOS NG features multiple fingerprints for analysing structure similarity, and new tools to calculate molecular properties and screen chemicals with endpoint-related public profilers, such as DNA and protein binders, liver alerts and genotoxic alerts. The publicly available COSMOS NG enables users to compile information and execute analyses such as category formation and read-across. This paper provides a step-by-step guided workflow for a simple read-across case, starting from a target structure and culminating in an estimation of a NOAEL confidence interval. Given its strong technical foundation, inclusion of quality-reviewed data, and provision of tools designed to facilitate communication between users, COSMOS NG is a first step towards building a toxicological knowledge hub leveraging many public data systems for chemical safety evaluation. We continue to monitor the feedback from the user community at support@mn-am.com.}
}
@article{GREGORIADES2021115546,
title = {Supporting digital content marketing and messaging through topic modelling and decision trees},
journal = {Expert Systems with Applications},
volume = {184},
pages = {115546},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115546},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421009532},
author = {Andreas Gregoriades and Maria Pampaka and Herodotos Herodotou and Evripides Christodoulou},
keywords = {Topic modelling, Cultural and economic distance, Decision trees, Shapley additive explanation, Tourists’ reviews},
abstract = {This paper presents a machine learning approach involving tourists’ electronic word of mouth (eWOM) to support destination marketing campaigns. This approach enhances optimisation of a critical aspect of marketing campaigns, that is, the communication of the right content to the right consumers. The proposed method further considers aggregate cultural and economic-related information of the tourists’ country of origin with topic modelling and Decision Tree (DT) models. Each DT addresses different dimensions of culture and purchasing power and the way these dimensions are associated with the topics discussed in eWOM, thus revealing patterns relating tourists’ experiences with potential explanations for their dissatisfaction/satisfaction. The method is implemented in a case study in the context of tourism in Cyprus focusing on two hotel groups (2/3 and 4/5 stars) to account for their differences. Patterns emerged from the extraction of rules from DTs illuminate combinations of variables associated with tourist experience (negative or positive) for each of the two hotel categories and verify the asymmetric relationship between service performance and satisfaction. The approach can be used by management during marketing campaigns to design messages to better address the desires and needs of tourists from different cultural and economic backgrounds, as these emerge from the data analysis.}
}
@article{HUAHU2023118762,
title = {An exploration of the key determinants for the application of AI-enabled higher education based on a hybrid Soft-computing technique and a DEMATEL approach},
journal = {Expert Systems with Applications},
volume = {212},
pages = {118762},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118762},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422017808},
author = {Kuang {Hua Hu}},
keywords = {Artificial intelligence, Higher education, Fuzzy rough set theory (FRST), Ant colony optimization (ACO), Decision making trial and evaluation laboratory (DEMATEL)},
abstract = {The application of AI in higher education has greatly increased globally in the dynamic digital age. The adoption of developmentally appropriate practices using AI-enabled techniques for facilitating the performance of teaching and learning in the higher education domain is thus a necessary task, especially in the COVID 19 pandemic era. The development and implementation of such techniques involve many factors and are related to the classical multiple criteria decision-making (MCDM) issue; however, these factors surrounding supervisors will confuse them and may result in misjudgment. To clarify the relevant issues and illustrate the cause-and-effect relationships among factors, a hybrid soft-computing technique (i.e., the fuzzy rough set theory (FRST) with ant colony optimization (ACO)) and a DEMATEL approach was proposed in this study, which can help decision makers capture the best model necessary for achieving aspiration-level in a higher education management strategy. In the results submitted, the improvement priority for dimensions is based on the measurement of the influences, running in order of tutors for learners (A), skills and competences (B), interaction data to support learning (C), and universal access to global classrooms (D), and which can serve as a reference for the plan of AI-enabled teaching/learning for higher education.}
}
@article{LU2021103816,
title = {Exploring smart construction objects as blockchain oracles in construction supply chain management},
journal = {Automation in Construction},
volume = {129},
pages = {103816},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103816},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002673},
author = {Weisheng Lu and Xiao Li and Fan Xue and Rui Zhao and Liupengfei Wu and Anthony G.O. Yeh},
keywords = {Blockchain, Oracles, Smart contract, Supply chain management, Smart construction objects, Prefabricated construction},
abstract = {Blockchain technology has attracted the interest of the global construction industry for its potential to enhance the transparency, traceability, and immutability of construction data and enables collaboration and trust throughout the supply chain. However, such potential cannot be achieved without blockchain “oracles” needed to bridge the on-chain (i.e., blockchain system) and off-chain (i.e., real-life physical project) worlds. This study presents an innovative solution that exploits smart construction objects (SCOs). It develops a SCOs-enabled blockchain oracles (SCOs-BOs) framework. To instantiate this framework, the system architecture of a blockchain-enabled construction supply chain management (BCSCM) system is developed and validated using a case study, whereby four primary smart contracts are examined in the context of off-site logistics and on-site assembly services. The validation results show that accurate data is retrieved against malicious data in each request, and the corresponding reputation scores are successfully recorded. The innovativeness of the research lies in two aspects. In addition to mobilizing SCOs as blockchain oracles to bridge the on-chain and off-chain worlds, it develops a decentralized SCO network to avoid the single point of failure (SPoF) problem widely existing in blockchain systems. This study contributes to existing research and practice to harness the power of blockchain in construction.}
}
@article{ELBADAWI2021745,
title = {Disrupting 3D printing of medicines with machine learning},
journal = {Trends in Pharmacological Sciences},
volume = {42},
number = {9},
pages = {745-757},
year = {2021},
issn = {0165-6147},
doi = {https://doi.org/10.1016/j.tips.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S016561472100119X},
author = {Moe Elbadawi and Laura E. McCoubrey and Francesca K.H. Gavins and Jun J. Ong and Alvaro Goyanes and Simon Gaisford and Abdul W. Basit},
keywords = {additive manufacturing, 3D Printed drug products and formulations, Industry 4.0 and digital health, personalized oral drug delivery systems and medical devices, biomedical engineering and pharmaceutical sciences, translational pharmaceutics},
abstract = {3D printing (3DP) is a progressive technology capable of transforming pharmaceutical development. However, despite its promising advantages, its transition into clinical settings remains slow. To make the vital leap to mainstream clinical practice and improve patient care, 3DP must harness modern technologies. Machine learning (ML), an influential branch of artificial intelligence, may be a key partner for 3DP. Together, 3DP and ML can utilise intelligence based on human learning to accelerate drug product development, ensure stringent quality control (QC), and inspire innovative dosage-form design. With ML’s capabilities, streamlined 3DP drug delivery could mark the next era of personalised medicine. This review details how ML can be applied to elevate the 3DP of pharmaceuticals and importantly, how it can expedite 3DP’s integration into mainstream healthcare.}
}
@article{WANG2021112622,
title = {Multi-factor and multi-level predictive models of building natural period},
journal = {Engineering Structures},
volume = {242},
pages = {112622},
year = {2021},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2021.112622},
url = {https://www.sciencedirect.com/science/article/pii/S0141029621007720},
author = {Zetao Wang and Jun Chen and Jiaxu Shen},
keywords = {Natural period of building, Database, Maximal information coefficient, Kruskal–Wallis ANOVA, Empirical formula, Confidence interval, Rational scope},
abstract = {A comprehensive database containing data on approximately 2700 buildings and 6000 full-scale measured period samples was constructed through massive literature searching and stringent data filtering. The newly emerged maximal information coefficient method, which is suitable for large data set statistical analysis, was adopted in conjunction with Kruskal–Wallis analysis of variance to identify factors that significantly affect a building’s fundamental period. It was quantitatively verified that height, predominant structural material, and lateral-force resisting system are the three most important influencing factors. Subsequently, height was used as the dominant regression variable, and material and lateral-force resisting system were used as categorical variables, predictive models in combination with confidence intervals of the fundamental period are provided for multi-factors, including four material types and three structural types. In addition, multi-level empirical formulas of the natural period in other five modes (two translational and three torsional) are provided on the basis of the regression results of the fundamental period. All these predictive models can effectively reflect the tendency of the median and the rational scope of variability of the natural period of buildings.}
}
@article{SPANAKI2021102350,
title = {AI applications of data sharing in agriculture 4.0: A framework for role-based data access control},
journal = {International Journal of Information Management},
volume = {59},
pages = {102350},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102350},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000438},
author = {Konstantina Spanaki and Erisa Karafili and Stella Despoudi},
keywords = {Agriculture 4.0, Design science, Artificial intelligence, Data sharing, Role-based access control},
abstract = {Industry 4.0 and the associated IoT and data applications are evolving rapidly and expand in various fields. Industry 4.0 also manifests in the farming sector, where the wave of Agriculture 4.0 provides multiple opportunities for farmers, consumers and the associated stakeholders. Our study presents the concept of Data Sharing Agreements (DSAs) as an essential path and a template for AI applications of data management among various actors. The approach we introduce adopts design science principles and develops role-based access control based on AI techniques. The application is presented through a smart farm scenario while we incrementally explore the data sharing challenges in Agriculture 4.0. Data management and sharing practices should enforce defined contextual policies for access control. The approach could inform policymaking decisions for role-based data management, specifically the data-sharing agreements in the context of Industry 4.0 in broad terms and Agriculture 4.0 in specific.}
}
@article{KUFNER2021103389,
title = {Vertical data continuity with lean edge analytics for industry 4.0 production},
journal = {Computers in Industry},
volume = {125},
pages = {103389},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103389},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306230},
author = {Thomas Küfner and Stefan Schönig and Richard Jasinski and Andreas Ermer},
keywords = {Edge analytics, Industry 4.0, Smart sensors, Machine learning},
abstract = {Industry 4.0 is characterized by the digitization and networking of machines and systems in production. The amount of data in production is increasing, providing information about processes and thus enables the autonomous monitoring, control and optimization of value creation processes. However, there have been several open challenges and current research questions identified. In particular, new solutions need to be scalable and high-performing to deal with the growing volumes of data close to real-time. The work at hand tackles these research gaps by presenting an approach to realize vertical data continuity by combining signal acquisition and simultaneous data evaluation in a decentralized system without the use of time-consuming external cloud solutions. The approach has been evaluated in laboratory as well as in industrial settings.}
}
@article{MANTELERO2021105561,
title = {An evidence-based methodology for human rights impact assessment (HRIA) in the development of AI data-intensive systems},
journal = {Computer Law & Security Review},
volume = {41},
pages = {105561},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105561},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000340},
author = {Alessandro Mantelero and Maria Samantha Esposito},
keywords = {Artificial intelligence, Human rights, Human Rights Impact Assessment, Data protection, AI regulation, Data ethics},
abstract = {Different approaches have been adopted in addressing the challenges of Artificial Intelligence (AI), some centred on personal data and others on ethics, respectively narrowing and broadening the scope of AI regulation. This contribution aims to demonstrate that a third way is possible, starting from the acknowledgement of the role that human rights can play in regulating the impact of data-intensive systems. The focus on human rights is neither a paradigm shift nor a mere theoretical exercise. Through the analysis of more than 700 decisions and documents of the data protection authorities of six countries, we show that human rights already underpin the decisions in the field of data use. Based on empirical analysis of this evidence, this work presents a methodology and a model for a Human Rights Impact Assessment (HRIA). The methodology and related assessment model are focused on AI applications, whose nature and scale require a proper contextualisation of HRIA methodology. Moreover, the proposed models provide a more measurable approach to risk assessment which is consistent with the regulatory proposals centred on risk thresholds. The proposed methodology is tested in concrete case-studies to prove its feasibility and effectiveness. The overall goal is to respond to the growing interest in HRIA, moving from a mere theoretical debate to a concrete and context-specific implementation in the field of data-intensive applications based on AI.}
}
@article{FAVA2021124,
title = {The bioeconomy in Italy and the new national strategy for a more competitive and sustainable country},
journal = {New Biotechnology},
volume = {61},
pages = {124-136},
year = {2021},
issn = {1871-6784},
doi = {https://doi.org/10.1016/j.nbt.2020.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1871678420302041},
author = {Fabio Fava and Lucia Gardossi and Patrizia Brigidi and Piergiuseppe Morone and Daniela A.R. Carosi and Andrea Lenzi},
keywords = {Bioeconomy strategy, Circular bioeconomy, Italian bioeconomy},
abstract = {Italy has the third largest bioeconomy in Europe (€330 billion annual turnover, 2 million employees), making it a core pillar of the national economy. Its sectors of excellence are food and biobased products, and it is a consistent presence in research and innovation projects funded by the EU Horizon 2020 programme (Societal Challenges 2) and the European Public Private Partnership “Biobased industry” (BBI-JU). The bioeconomy reduces dependence on fossil fuels and finite materials, loss of biodiversity and changing land use. It contributes to environmental regeneration, spurs economic growth and supports jobs in rural, coastal and abandoned industrial areas, leveraging local contexts and traditions. In 2017 the Italian government promoted the development of a national Bioeconomy Strategy (BIT), recently updated (BIT II) to interconnect more efficiently the pillars of the national bioeconomy: production of renewable biological resources, their conversion into valuable food/feed, biobased products and bio-energy, and transformation and valorization of bio-waste streams. BIT II aims to improve coordination between Ministries and Italian regions in alignment of policies, regulations, R&I funding programmes and infrastructures investment. The goal is a 15 % increase in turnover and employment in the Italian bioeconomy by 2030. Based on Italy’s strategic geopolitical position in the Mediterranean basin, BIT II also includes actions to improve sustainable productivity, social cohesion and political stability through the implementation of bioeconomy strategies in this area. This paper provides an insight into these strategies and discusses the strengths and weaknesses of the sectors involved and the measures, regulatory initiatives and monitoring actions undertaken.}
}
@article{EACHEMPATI2021120903,
title = {Validating the impact of accounting disclosures on stock market: A deep neural network approach},
journal = {Technological Forecasting and Social Change},
volume = {170},
pages = {120903},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120903},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521003358},
author = {Prajwal Eachempati and Praveen Ranjan Srivastava and Ajay Kumar and Kim Hua Tan and Shivam Gupta},
keywords = {Disclosures, Data intelligence, Analytics, Finance, Machine learning, Deep learning, Stock market, Forecasts, Private decision-making},
abstract = {Firms disclose information either voluntarily or due to the regulator's mandatory requirements, and such disclosures form good sources to know the prospects of a firm. Information in the disclosures and analysts' opinions influence investor-trading behavior, and consequently, affects the asset prices. As sentiments factored in disclosures are a source of market action, this study aims to capture the sentiments from disclosure information to assess asset prices' impact. The paper adopts a deep neural network-based prediction model for conducting sentiment analysis on heterogeneous datasets. We construct a sentiment simulation model of voluntary disclosures to know whether the managers can use the market sentiment as a strategic input to boost market performance by suitably drafting the tone and content of disclosures without compromising their quality and veracity. The Deep Neural Networks with LSTM algorithm is found to outperform the Deep Neural Networks with RNN and other baseline machine learning classifiers in terms of predictive accuracy of the NSE NIFTY50. The variable importance computed also validates that market news, combined with historical indicators, predicts the stock market trend closer to the actual trend.}
}
@article{SUJON2021103844,
title = {Application of weigh-in-motion technologies for pavement and bridge response monitoring: State-of-the-art review},
journal = {Automation in Construction},
volume = {130},
pages = {103844},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103844},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002958},
author = {Mohhammad Sujon and Fei Dai},
keywords = {Weigh-in-motion, Review, Response monitoring, Structural health monitoring, Weight enforcement},
abstract = {Overweight vehicles may cause damage and premature deterioration of pavement and bridge structures. Weigh-in-Motion (WIM) is efficient in avoiding structural damage and ensuring successful weight enforcement by measuring a vehicle's weight in a dynamic state. WIM additionally provides information such as traffic volume, vehicle's speed, axle spacing, equivalent single axle load (ESAL), individual axle and gross vehicle weight (GVW), which is of value to planning, design, construction, and operations of transportation infrastructures. This paper reviewed the state of practice and research in WIM with focuses on its potential, limitations, cost-effectiveness, and data usage. Discussion was made on identifying needs and challenges for further development. This review provides the research community with a holistic view of available WIM techniques, their limitations, cost-effectiveness, and the need for future research on usage of the WIM data that might lead to wider adoption of WIM in transportation applications.}
}
@article{SAMAL2021100800,
title = {Multi-directional temporal convolutional artificial neural network for PM2.5 forecasting with missing values: A deep learning approach},
journal = {Urban Climate},
volume = {36},
pages = {100800},
year = {2021},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2021.100800},
url = {https://www.sciencedirect.com/science/article/pii/S2212095521000304},
author = {K. Krishna Rani Samal and Korra Sathya Babu and Santos Kumar Das},
keywords = {Deep learning, Time series forecasting, Temporal Convolutional Network, Artificial Neural Network, PM2.5},
abstract = {Data imputation and forecasting are the major research areas in environmental data engineering. Solving those critical issues has an immense impact on air pollution management, consequently improving social, economic growth, and public health. Missing data is a common issue for all the domains, especially for environmental data analysis. Most of the research study tries to solve all these problems of time series data using different models. This research study presents a novel deep learning-based hybrid model architecture to solve these issues in a single training process. We come up with Multi-directional Temporal Convolutional Artificial Neural Network (MTCAN) model to impute and forecast PM2.5 pollutant concentration in a single training process. The main idea of the multi-directional properties of MTCAN is to interpolate the PM2.5 pollutant feature matrix to impute its value. Ultimately, it maintains the temporal correlation within the features' measurement and meteorological and pollutant variables to impute PM2.5 missing values. The MTCAN model performs feature learning and sequential modeling simultaneously with a wide range of past observations for long-term forecasting, minimizing memory size requirement and training cost. Experimental results indicate that the proposed model is superior to baseline pollution forecasting models, which prove its effectiveness in air quality modeling.}
}
@article{DURRANT2021100493,
title = {How might technology rise to the challenge of data sharing in agri-food?},
journal = {Global Food Security},
volume = {28},
pages = {100493},
year = {2021},
issn = {2211-9124},
doi = {https://doi.org/10.1016/j.gfs.2021.100493},
url = {https://www.sciencedirect.com/science/article/pii/S2211912421000031},
author = {Aiden Durrant and Milan Markovic and David Matthews and David May and Georgios Leontidis and Jessica Enright},
keywords = {Data Trusts, Data sharing, AI Technologies, Agri-food supply chains},
abstract = {Data sharing is often hindered by a number of real word challenges caused by a mixture of technological and social factors. To date, the agri-food sector significantly lags behind other sectors in overcoming these challenges. However, the benefits of data sharing are too great to be ignored as they have a potential to address many historical failings such as issues related to food safety, traceability and transparency, and must be carefully considered as the sector is undergoing a widespread digitalisation. In this article, we explore the potential of different technologies in addressing the challenges presented by data sharing in the agri-food sector, and how the use of these technologies in the narrative of a Data Trust may address many of these obstacles. We argue the importance of utilising semantic web technologies, distributed ledger technologies, machine learning, and privacy preserving technologies to enable future transformative data sharing infrastructures in the agri-food sector. The utilisation of holistic statistical analysis of the shared data is also discussed, vital in supporting many of the sectors optimisation and sustainability goals.}
}
@article{YANG20211000,
title = {Research and applications of artificial neural network in pavement engineering: A state-of-the-art review},
journal = {Journal of Traffic and Transportation Engineering (English Edition)},
volume = {8},
number = {6},
pages = {1000-1021},
year = {2021},
issn = {2095-7564},
doi = {https://doi.org/10.1016/j.jtte.2021.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2095756421001008},
author = {Xu Yang and Jinchao Guan and Ling Ding and Zhanping You and Vincent C.S. Lee and Mohd Rosli {Mohd Hasan} and Xiaoyun Cheng},
keywords = {Pavement engineering, Pavement design, Artificial neural network, Deep learning, Pavement life cycle, Health inspection and monitoring},
abstract = {Given the great advancements in soft computing and data science, artificial neural network (ANN) has been explored and applied to handle complicated problems in the field of pavement engineering. This study conducted a state-of-the-art review for surveying the recent progress of ANN application at different stages of pavement engineering, including pavement design, construction, inspection and monitoring, and maintenance. This study focused on the papers published over the last three decades, especially the studies conducted since 2013. Through literature retrieval, a total of 683 papers in this field were identified, among which 143 papers were selected for an in-depth review. The ANN architectures used in these studies mainly included multi-layer perceptron neural network (MLPNN), convolutional neural network (CNN) and recurrent neural network (RNN) for processing one-dimensional data, two-dimensional data and time-series data. CNN-based pavement health inspection and monitoring attracted the largest research interest due to its potential to replace human labor. While ANN has been proved to be an effective tool for pavement material design, cost analysis, defect detection and maintenance planning, it is facing huge challenges in terms of data collection, parameter optimization, model transferability and low-cost data annotation. More attention should be paid to bring multidisciplinary techniques into pavement engineering to tackle existing challenges and widen future opportunities.}
}
@article{NEMOTO2021160,
title = {A Pitfall of Learning from User-generated Data: In-Depth Analysis of Subjective Class Problem},
journal = {Procedia Computer Science},
volume = {185},
pages = {160-169},
year = {2021},
note = {Big Data, IoT, and AI for a Smarter Future},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101098X},
author = {Kei Nemoto and Shweta Jain},
keywords = {Text classification, Supervised learning, Subjective class, Class noise},
abstract = {Research in the supervised learning algorithms field implicitly assumes that training data is labeled by domain experts or at least semi-professional labelers accessible through crowdsourcing services like Amazon Mechanical Turk. With the advent of the Internet, data has become abundant and a large number of machine learning based systems are being trained with user-generated data, where categorical data is used as labels. However, little work has been done in the area of supervised learning with user-defined labels where users are not necessarily experts and might be unable to provide correct labels to some data or the labels might contain significant human bias. In this article, we propose two types of classes in user-defined labels: subjective class and objective class - showing that the objective classes are as reliable as if they were provided by domain experts, whereas the subjective classes are subject to error and bias. We name this a subjective class problem and propose a Normalized Feature Indicative Score that can be effective in detecting subjective classes in a dataset without querying oracle. This score provides early detection of subjective classes in the data, saving time for data mining practitioners working with data that might contain errors and biases.}
}
@article{ZHENG2021103831,
title = {Research on the strategy of mobile short video in product sales based on 5G network and embedded system},
journal = {Microprocessors and Microsystems},
volume = {82},
pages = {103831},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.103831},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121000119},
author = {Lu Zheng and Shikun Liu},
keywords = {Recurrent Neural Network (RNN), Mobile short video, Product sales data upload bandwidth size, Embedded systems, 5G technology},
abstract = {Mobile short video-based product sales sharing sites like YouTube and Tudor have many established user content for creating and distributing shares. The increasing number of mobile devices for product sales leads to the upcoming new 5G technology roadmap for embedded systems and 5G network connectivity. As these are the main sources of 5G information and online activities for consumers, mobile phone short films are rapidly being replaced by embedded systems. As the demand for more embedded system devices and applications continues to grow, supported bandwidth is also essential to meet this growing connection demand. The existing system does not allocate the product sales data upload bandwidth size. The system proposed here focuses on user upload bandwidth allocation, one of the basic resources of a short video sharing system with product details. Its allocation upload bandwidth Recurrent Neural Network (RNN) algorithm is proposed in a centralized or decentralized way and evaluated for balancing widely used strategies (equal allocation) and a mobile short video. Embedded systems are responsible for running professional product sales and control applications consistently and predictably. Development while using the microprocessor is also important. It increases the need to process product sales to handle the bandwidth, latency requirements, product sales data and data generated from multiple connected devices. It's a big challenge for the industry to data and data capabilities.}
}
@article{GUO2021109256,
title = {Improved kinematic interpolation for AIS trajectory reconstruction},
journal = {Ocean Engineering},
volume = {234},
pages = {109256},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109256},
url = {https://www.sciencedirect.com/science/article/pii/S002980182100682X},
author = {Shaoqing Guo and Junmin Mou and Linying Chen and Pengfei Chen},
keywords = {AIS, Ship trajectory, Trajectory reconstruction, Kinematic interpolation},
abstract = {Ship trajectory information has made a significant contribution to the data-based research in analyzing maritime transportation and has facilitated the improvement of maritime safety. However, the AIS data, which consists of ship trajectory, inevitably contains noises or missing data that can interfere with the conclusion. In this paper, an improved kinematic interpolation is presented for AIS trajectory reconstruction, which integrates data preprocessing and interpolation that considers the ships' kinematic information. The improved kinematic reconstruction method includes four steps: (1) data preprocessing, (2) analysis of time interval distribution, (3) abnormal data detection and removal, (4) kinematic interpolation that takes the kinematic feature of ships (i.e., velocity and acceleration) into account, adding forward and backward track points to help correct the acceleration function of reconstruction points. The proposed method is tested on the AIS dataset of Zhoushan Port and was compared with traditional ship trajectory reconstruction methods. The comparison indicates that the proposed method can effectively reconstruct the ship trajectory with higher performance on a single ship trajectory and a large AIS data set of certain water areas.}
}
@incollection{TURCHI2023474,
title = {Massive Parallel Sequencing in Forensic Genetics},
editor = {Max M. Houck},
booktitle = {Encyclopedia of Forensic Sciences, Third Edition (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {474-484},
year = {2023},
isbn = {978-0-12-823678-9},
doi = {https://doi.org/10.1016/B978-0-12-823677-2.00094-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128236772000945},
author = {Chiara Turchi and Federica Alessandrini and Valerio Onofri},
keywords = {DNA phenotyping, Forensic epigenetics, Forensic genetics, Forensic transcriptome, Individual identification, Massive Parallel Sequencing (MPS), MtDNA polymorphisms, Next Generation Sequencing (NGS), Short Tandem Repeats (STR), Single Nucleotide Polymorphisms (SNP)},
abstract = {The introduction of Massive Parallel Sequencing technology (MPS) in the forensic genetics field has opened new possibilities in forensic DNA genotyping. The advantage of MPS is multifold, including: high throughput sequencing; production of millions of DNA molecules in parallel; simultaneous analysis of large number of markers; as well as different type of markers; and a high number of sample in a single experimental run. Beside genotyping traditional forensic markers for identification, i.e., Short Tandem Repeat (STR), Single Nucleotide Polymorphism (SNP), and mitochondrial DNA (mtDNA), MPS offers the potential to genotype a new type of genetic marker, known as microhaplotypes. Moreover, MPS makes it possible to explore the potential of forensic DNA phenotyping and of the forensic transcriptomic. This article discusses different MPS approaches used in forensic, the applications in forensic field, and benefits and drawbacks are discussed.}
}
@article{FRIKHA202198,
title = {Reinforcement and deep reinforcement learning for wireless Internet of Things: A survey},
journal = {Computer Communications},
volume = {178},
pages = {98-113},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002681},
author = {Mohamed Said Frikha and Sonia Mettali Gammar and Abdelkader Lahmadi and Laurent Andrey},
keywords = {Internet of Things, Reinforcement learning, Deep reinforcement learning, Wireless Networks},
abstract = {Nowadays, many research studies and industrial investigations have allowed the integration of the Internet of Things (IoT) in current and future networking applications by deploying a diversity of wireless-enabled devices ranging from smartphones, wearables, to sensors, drones, and connected vehicles. The growing number of IoT devices, the increasing complexity of IoT systems, and the large volume of generated data have made the monitoring and management of these networks extremely difficult. Numerous research papers have applied Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) techniques to overcome these difficulties by building IoT systems with effective and dynamic decision-making mechanisms, dealing with incomplete information related to their environments. The paper first reviews pre-existing surveys covering the application of RL and DRL techniques in IoT communication technologies and networking. The paper then analyzes the research papers that apply these techniques in wireless IoT to resolve issues related to routing, scheduling, resource allocation, dynamic spectrum access, energy, mobility, and caching. Finally, a discussion of the proposed approaches and their limits is followed by the identification of open issues to establish grounds for future research directions proposal.}
}
@article{ZUO2021101032,
title = {Reference-free video-to-real distance approximation-based urban social distancing analytics amid COVID-19 pandemic},
journal = {Journal of Transport & Health},
volume = {21},
pages = {101032},
year = {2021},
issn = {2214-1405},
doi = {https://doi.org/10.1016/j.jth.2021.101032},
url = {https://www.sciencedirect.com/science/article/pii/S2214140521000268},
author = {Fan Zuo and Jingqin Gao and Abdullah Kurkcu and Hong Yang and Kaan Ozbay and Qingyu Ma},
keywords = {Social distancing, COVID-19, Close contact, Pedestrian, Deep learning, Computer vision},
abstract = {Introduction
The rapidly evolving COVID-19 pandemic has dramatically reshaped urban travel patterns. In this research, we explore the relationship between “social distancing,” a concept that has gained worldwide familiarity, and urban mobility during the pandemic. Understanding social distancing behavior will allow urban planners and engineers to better understand the new norm of urban mobility amid the pandemic, and what patterns might hold for individual mobility post-pandemic or in the event of a future pandemic.
Methods
There are still few efforts to obtain precise information on social distancing patterns of pedestrians in urban environments. This is largely attributed to numerous burdens in safely deploying any effective field data collection approaches during the crisis. This paper aims to fill that gap by developing a data-driven analytical framework that leverages existing public video data sources and advanced computer vision techniques to monitor the evolution of social distancing patterns in urban areas. Specifically, the proposed framework develops a deep-learning approach with a pre-trained convolutional neural network to mine the massive amount of public video data captured in urban areas. Real-time traffic camera data collected in New York City (NYC) was used as a case study to demonstrate the feasibility and validity of using the proposed approach to analyze pedestrian social distancing patterns.
Results
The results show that microscopic pedestrian social distancing patterns can be quantified by using a generalized real-distance approximation method. The estimated distance between individuals can be compared to social distancing guidelines to evaluate policy compliance and effectiveness during a pandemic. Quantifying social distancing adherence will provide decision-makers with a better understanding of prevailing social contact challenges. It also provides insights into the development of response strategies and plans for phased reopening for similar future scenarios.}
}
@incollection{2021237,
title = {Index},
editor = {Stephanie Kay Ashenden},
booktitle = {The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry},
publisher = {Academic Press},
pages = {237-245},
year = {2021},
isbn = {978-0-12-820045-2},
doi = {https://doi.org/10.1016/B978-0-12-820045-2.09997-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200452099979}
}
@article{RUMSON2021109214,
title = {The application of fully unmanned robotic systems for inspection of subsea pipelines},
journal = {Ocean Engineering},
volume = {235},
pages = {109214},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109214},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821006442},
author = {Alexander G. Rumson},
keywords = {Automation, Inspection, Marine robotics, Underwater structures, Unmanned underwater technology, Underwater vehicles},
abstract = {This paper focuses on recent innovations in the methods used for external remote subsea pipeline inspection. An unmanned method is revealed, in which an Autonomous Underwater Vehicle (AUV), paired with an Unmanned Surface Vessel (USV) was utilised to complete inspections. Results are presented from a recent project, in which existing hardware and software were integrated and combined with automated workflows to form a solution, involving an AUV operated from a USV, with operations remotely controlled from shore. As inspection data was acquired, self-actuating workflows ran onboard the AUV, enabling data processing tasks to commence and QC messages/alerts to be transmitted to the control centre, this allowed execution of in-water mission adjustments. The primary focus of this paper is on the development and implementation of an automated, fully unmanned system for subsea inspection operations. Initially, a brief review is presented of recent developments in this field. Links are drawn between these wider developments and progress made within the project, and areas are highlighted where further work is required for realisation of a comprehensive unmanned pipeline inspection solution.}
}
@incollection{2021467,
title = {Index},
editor = {Cynthia J. Girman and Mary Elizabeth Ritchey},
booktitle = {Pragmatic Randomized Clinical Trials},
publisher = {Academic Press},
pages = {467-476},
year = {2021},
isbn = {978-0-12-817663-4},
doi = {https://doi.org/10.1016/B978-0-12-817663-4.20001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128176634200019}
}
@article{IPPOLITO2021108300,
title = {Improving facies prediction by combining supervised and unsupervised learning methods},
journal = {Journal of Petroleum Science and Engineering},
volume = {200},
pages = {108300},
year = {2021},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2020.108300},
url = {https://www.sciencedirect.com/science/article/pii/S0920410520313541},
author = {Marco Ippolito and John Ferguson and Fred Jenson},
keywords = {Facies classification, Machine learning, Supervised learning, Unsupervised learning, Bias, Joint PDF},
abstract = {Facies classification from well logs is an indispensable part of seismic interpretation and is important in the determination of sequence stratigraphy and ultimately reservoir characterization. Although there have been improvements in the tools used to perform this task, it remains laborious, subjective, and error-prone. Achieving a proper classification is complicated by increasing dataset sizes as well as the need for correlated multidisciplinary models. Recent developments in machine learning provide an opportunity to assist interpreters in accomplishing this task while also improving the accuracy of classification results. Applications of machine learning methods for automating facies classification from well logs have previously been explored, however these have largely focused on evaluations or comparisons of individual algorithms or of ensembles of homogeneous agents. The proposed method combines heterogeneous agents to enhance prediction accuracy. Specifically, supervised learning, which provides a direct mapping between the data domain and the solution domain while introducing bias to generalize the mapping, is combined with unsupervised learning, which does not depend on similar generalization bias or training data but also does not provide a direct mapping between the data and solution domains. The combination is accomplished via the joint probability density function (PDF) of the supervised classification, which is used to guide identification of clusters delineated by unsupervised learning. This multi-agent approach can reduce bias introduced during training and provides a basis for generating a probability distribution for each sample rather than a discrete classification. The distribution, in turn, can be used to more accurately model the continuous nature of well log signals, which reflects continuity in lithological regimes.}
}
@article{LIN2021108362,
title = {Uncertainty quantification and software risk analysis for digital twins in the nearly autonomous management and control systems: A review},
journal = {Annals of Nuclear Energy},
volume = {160},
pages = {108362},
year = {2021},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2021.108362},
url = {https://www.sciencedirect.com/science/article/pii/S0306454921002383},
author = {Linyu Lin and Han Bao and Nam Dinh},
keywords = {Digital twin, Autonomous control, Uncertainty quantification, Software risk analysis},
abstract = {A nearly autonomous management and control (NAMAC) system is designed to furnish recommendations to operators for achieving particular goals based on NAMAC’s knowledge base. As a critical component in a NAMAC system, digital twins (DTs) are used to extract information from the knowledge base to support decision-making in reactor control and management during all modes of plant operations. With the advancement of artificial intelligence and data-driven methods, machine learning algorithms are used to build DTs of various functions in the NAMAC system. To evaluate the uncertainty of DTs and its impacts on the reactor digital instrumentation and control systems, uncertainty quantification (UQ) and software risk analysis is needed. As a comprehensive overview of prior research and a starting point for new investigations, this study selects and reviews relevant UQ techniques and software hazard and software risk analysis methods that may be suitable for DTs in the NAMAC system.}
}
@article{ZHENG202110,
title = {Characterizing urban land changes of 30 global megacities using nighttime light time series stacks},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {173},
pages = {10-23},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2021.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0924271621000022},
author = {Qiming Zheng and Qihao Weng and Ke Wang},
keywords = {Nighttime light imagery, Land use and land cover changes, Urbanization, Time series analysis, Megacities, Urban areas, Sustainability},
abstract = {Worldwide urbanization has brought about diverse types of urban land use and land cover (LULC) changes. The diversity of urban land changes, however, have been greatly under studied, since the major focus of past research has been on urban growth. In this study, we proposed a framework to characterize diverse urban land changes of 30 global megacities using monthly nighttime light time series from VIIRS data. First, we developed a Logistic-Harmonic model to fit VIIRS time series. Second, by leveraging the uniqueness of urban land change and nighttime light data, we incorporated temporal information of VIIRS time series and proposed a new classification scheme to produce monthly maps of built-up areas and to disentangle urban land changes into five categories. Third, we provided an in-depth analysis and comparison of urban land change patterns of the selected megacities. Results demonstrated that the Logistic-Harmonic model yielded a robust performance in fitting VIIRS time series. Temporal features based classification can not only significantly improve the mapping accuracy of built-up areas, especially for regions with heterogeneous built-up and non-built-up landscapes, but also promoted temporal consistency and classification efficiency. Urban land changes occurred in 51% of the built-up pixels of the megacities. Compared with urban growth, other types of urban land change, particularly land use intensification, contributed to an unexpectedly large proportion of the changes (83%). The findings of this study offer an insightful understanding on global urbanization processes in megacities, and evoke further investigation on the environmental and ecological implications of urban land changes.}
}
@article{REICHARDT2021968,
title = {Procedure model for the development and launch of intelligent assistance systems},
journal = {Procedia Computer Science},
volume = {180},
pages = {968-977},
year = {2021},
note = {Proceedings of the 2nd International Conference on Industry 4.0 and Smart Manufacturing (ISM 2020)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.348},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921004026},
author = {Paul Reichardt and Sebastian Lang and Tobias Reggelin},
keywords = {Production planning, control, Machine learning, Planning assistance system, Practical implementation, Generic procedure model, Digital twin},
abstract = {The paper analyses the current state of knowledge on approaches for the practical implementation of machine learning based assistance systems for production planning and control. A concept of a procedure model for application-oriented projects in the field of industrial series production is proposed. It focusses on order sequencing and machine allocation in a real time production environment. As part of an application-oriented research project, a use case is referenced. In this paper, a first conceptual approach is presented, using the example of an industrial production of printed circuit boards. In the following steps, practical suitability is checked on the basis of the practical reference, conclusions are drawn and the methodology will be developed further. The aim is a generally valid procedure model for industrial series production.}
}
@article{EBITU2021105326,
title = {Citizen science for sustainable agriculture – A systematic literature review},
journal = {Land Use Policy},
volume = {103},
pages = {105326},
year = {2021},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2021.105326},
url = {https://www.sciencedirect.com/science/article/pii/S0264837721000491},
author = {Larmbert Ebitu and Helen Avery and Khaldoon A. Mourad and Joshua Enyetu},
keywords = {Sustainable agriculture, Citizen science, Farmers, Methodologies},
abstract = {Farmers as volunteers in research could potentially provide a rich resource for exploring sustainable agricultural research questions. To discern emerging patterns in citizen science-based studies on topics with relevance for sustainable agriculture and reveal salient challenges and opportunities for conducting such studies, we conducted a literature review of 27 articles from the period 2004–2019 of 250 publications screened from Google Scholar. These articles were thematically grouped under the topics: Soil health, climate adaptation, pest/pathogen monitoring, invasive species, inputs and outputs and pollination. Participants’ characteristics, motivations, study design and project outcomes in the reviewed articles were summarized and discussed. Both observational and experimental studies were represented in the articles, while emerging trends point towards field experimentation and ‘Large-N′ trials by lay farmers. Crowdsourcing lends itself to projects where the main role of the public is local visual observations and reporting, such as in pest/pathogen monitoring. Challenges included methodological issues such as validation procedures, but above all motivation, recruitment, and retention of volunteers. Despite the importance of participatory approaches for deeper citizen involvement for sustainability transitions and for the quality of knowledge outcomes, the role of citizens was overall restricted to data collection. Several of the methodologies proposed would be difficult to implement in low-income countries, and relatively few studies pertained to agricultural concerns of the global South. To lend value to farmers' time, we recommend projects relevant to livelihoods, health issues or local farming problems, accompanied by well-structured data feedback protocols, routing study results back to farmers.}
}
@article{YANG2021e690,
title = {Associations between HIV infection and clinical spectrum of COVID-19: a population level analysis based on US National COVID Cohort Collaborative (N3C) data},
journal = {The Lancet HIV},
volume = {8},
number = {11},
pages = {e690-e700},
year = {2021},
issn = {2352-3018},
doi = {https://doi.org/10.1016/S2352-3018(21)00239-3},
url = {https://www.sciencedirect.com/science/article/pii/S2352301821002393},
author = {Xueying Yang and Jing Sun and Rena C Patel and Jiajia Zhang and Siyuan Guo and Qulu Zheng and Amy L Olex and Bankole Olatosi and Sharon B Weissman and Jessica Y Islam and Christopher G Chute and Melissa Haendel and Gregory D Kirk and Xiaoming Li and Richard Moffitt and Hana Akelsrod and Keith A Crandall and Nora Francheschini and Evan French and Teresa {Po-Yu Chiang} and G Caleb-Alexander and Kathleen M Andersen and Amanda J Vinson and Todd T Brown and Roslyn B Mannon},
abstract = {Summary
Background
Evidence of whether people living with HIV are at elevated risk of adverse COVID-19 outcomes is inconclusive. We aimed to investigate this association using the population-based National COVID Cohort Collaborative (N3C) data in the USA.
Methods
We included all adult (aged ≥18 years) COVID-19 cases with any health-care encounter from 54 clinical sites in the USA, with data being deposited into the N3C. The outcomes were COVID-19 disease severity, hospitalisation, and mortality. Encounters in the same health-care system beginning on or after January 1, 2018, were also included to provide information about pre-existing health conditions (eg, comorbidities). Logistic regression models were employed to estimate the association of HIV infection and HIV markers (CD4 cell count, viral load) with hospitalisation, mortality, and clinical severity of COVID-19 (multinomial). The models were initially adjusted for demographic characteristics, then subsequently adjusted for smoking, obesity, and a broad range of comorbidities. Interaction terms were added to assess moderation effects by demographic characteristics.
Findings
In the harmonised N3C data release set from Jan 1, 2020, to May 8, 2021, there were 1 436 622 adult COVID-19 cases, of these, 13 170 individuals had HIV infection. A total of 26 130 COVID-19 related deaths occurred, with 445 among people with HIV. After adjusting for all the covariates, people with HIV had higher odds of COVID-19 death (adjusted odds ratio 1·29, 95% CI 1·16–1·44) and hospitalisation (1·20, 1·15–1·26), but lower odds of mild or moderate COVID-19 (0·61, 0·59–0·64) than people without HIV. Interaction terms revealed that the elevated odds were higher among older age groups, male, Black, African American, Hispanic, or Latinx adults. A lower CD4 cell count (<200 cells per μL) was associated with all the adverse COVID-19 outcomes, while viral suppression was only associated with reduced hospitalisation.
Interpretation
Given the COVID-19 pandemic's exacerbating effects on health inequities, public health and clinical communities must strengthen services and support to prevent aggravated COVID-19 outcomes among people with HIV, particularly for those with pronounced immunodeficiency.
Funding
National Center for Advancing Translational Sciences, National Institute of Allergy and Infectious Diseases, National Institutes of Health, USA.}
}
@incollection{MARTYR2021469,
title = {Chapter 14 - Data handling and modeling},
editor = {Anthony J. Martyr and David R. Rogers},
booktitle = {Engine Testing (Fifth Edition)},
publisher = {Butterworth-Heinemann},
edition = {Fifth Edition},
address = {Oxford},
pages = {469-509},
year = {2021},
isbn = {978-0-12-821226-4},
doi = {https://doi.org/10.1016/B978-0-12-821226-4.00014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212264000140},
author = {Anthony J. Martyr and David R. Rogers},
keywords = {Data-science, machine-learning, artificial-intelligence, clustering, regression, prediction, model, data handling},
abstract = {In this chapter an overview of the industry standard for powertrain test data analytics will be provided, along with the emergence of the application of data science for engineering data. This is in order to provide the facility engineering with information of the value of modern data science in powertrain test and development environments. The material serves to provide those who are new to powertrain test data analysis mechanisms, the basic practices of data acquisition, analysis, and utilization as they currently exist within the industry. In addition, providing information for those seeking deeper information on sophisticated statistical and machine-learning tools within the context of powertrain engineering, along with the types of problems to which they are suited. As a summary, the chapter compares and contrasts conventional data practice and emerging processes, making observations on the possible future directions for the industry in this rapidly developing field.}
}
@article{CISNEROSCABRERA2021103391,
title = {An approach and decision support tool for forming Industry 4.0 supply chain collaborations},
journal = {Computers in Industry},
volume = {125},
pages = {103391},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103391},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306254},
author = {Sonia Cisneros-Cabrera and Grigory Pishchulov and Pedro Sampaio and Nikolay Mehandjiev and Zixu Liu and Sophia Kununka},
keywords = {Digitalization, Supply chain collaboration, Industry 4.0, Decision support systems, Interoperability, Ontology},
abstract = {Industry 4.0 technologies, process digitalisation and automation can be applied to support the formation of supply chain collaborations in manufacturing. Underpinned by information and communication technologies, collaborations of independent companies can dynamically pool production capacities and capabilities to jointly react to new business opportunities. These collaborations may involve a wide range of enterprises with different sizes and scope that individually would not be able to tender for such new business opportunities. To form these collaborative teams, assistive processes and technologies can underpin the effort towards exploring the tender requirements, unbundling the tender into smaller tasks and finding a suitable supplier for each task. In this paper, we present an approach and a tool to support decision making concerning forming supply chain collaborations in Industry 4.0. The approach proposed is unique in integrating industry domain ontologies, assistive human-computer interaction tools and multi-criteria decision support techniques to form team compositions speeding-up the collaboration process whilst maximising the chances of forming a viable team to fulfil the tender requirements. We also show evaluation results involving stakeholders from the supply chain function pointing to the effectiveness of the proposed solution, available online as a demo11http://130.88.97.225:4200 (username: TDMS@uniman.eu; password: uniman)..}
}
@article{XIANG2021114989,
title = {High-end equipment data desensitization method based on improved Stackelberg GAN},
journal = {Expert Systems with Applications},
volume = {180},
pages = {114989},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114989},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421004309},
author = {Nan Xiang and Xiongtao Zhang and Yajie Dou and Xiangqian Xu and Kewei Yang and Yuejin Tan},
keywords = {High-end equipment, Data desensitization, Generative adversarial networks},
abstract = {High-end equipment refers to a type of technical equipment with high technical content, large capital investment, and long development cycle. Therefore, high-end equipment data has extraordinary significance and its desensitization is an urgent problem in data analysis. Traditional data desensitization principles are processing original data such as substitution and adding noise. These methods may not only damage data correlation information, but also result in data disclosure and high computing cost. Given the aforementioned reasons, the study proposes a high-end equipment data desensitization method based on improved Stackelberg Generative Adversarial Networks (GAN). When compared with the normal GAN, the structure proposed in the study includes more generators and discriminators. By inputting the original data, the trained GAN can output indistinguishable data from the original data which helps data mining and also ensures the privacy of data. We experimented on two datasets: optimal improvement was determined by Gaussian dataset experiments, i.e. Stackelberg GAN with eight discriminators. The second experiment results on real-world datasets proved that the 8-discriminator Stackelberg GAN better fits the original data and significantly aids data desensitization.}
}
@article{MOTHUKURI2021619,
title = {A survey on security and privacy of federated learning},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {619-640},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20329848},
author = {Viraaji Mothukuri and Reza M. Parizi and Seyedamin Pouriyeh and Yan Huang and Ali Dehghantanha and Gautam Srivastava},
keywords = {Artificial intelligence, Machine learning, Distributed learning, Federated learning, Federated machine learning, Security, Privacy},
abstract = {Federated learning (FL) is a new breed of Artificial Intelligence (AI) that builds upon decentralized data and training that brings learning to the edge or directly on-device. FL is a new research area often referred to as a new dawn in AI, is in its infancy, and has not yet gained much trust in the community, mainly because of its (unknown) security and privacy implications. To advance the state of the research in this area and to realize extensive utilization of the FL approach and its mass adoption, its security and privacy concerns must be first identified, evaluated, and documented. FL is preferred in use-cases where security and privacy are the key concerns and having a clear view and understanding of risk factors enable an implementer/adopter of FL to successfully build a secure environment and gives researchers a clear vision on possible research areas. This paper aims to provide a comprehensive study concerning FL’s security and privacy aspects that can help bridge the gap between the current state of federated AI and a future in which mass adoption is possible. We present an illustrative description of approaches and various implementation styles with an examination of the current challenges in FL and establish a detailed review of security and privacy concerns that need to be considered in a thorough and clear context. Findings from our study suggest that overall there are fewer privacy-specific threats associated with FL compared to security threats. The most specific security threats currently are communication bottlenecks, poisoning, and backdoor attacks while inference-based attacks are the most critical to the privacy of FL. We conclude the paper with much needed future research directions to make FL adaptable in realistic scenarios.}
}
@article{LI2023109044,
title = {Rethinking referring relationships from a perspective of mask-level relational reasoning},
journal = {Pattern Recognition},
volume = {133},
pages = {109044},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109044},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005246},
author = {Chengyang Li and Liping Zhu and Gangyi Tian and Yi Hou and Heng Zhou},
keywords = {Referring relationship, Multimodal learning, Image and text, Visual grounding, Deep learning},
abstract = {Referring relationship aims at localizing subject and object entities in an image, according to a triple text <subject, predicate, object>. Previous methods use iterative attention to shift between image regions for modeling predicate. However, predicate sometimes is implicit and difficult to be represented in the image domain. Convolution modeling method to express predicate is simple and inappropriate. Besides, relational reasoning information in the text itself is not fully utilized. To this end, we rethink referring relationship from a mask-level relational reasoning perspective to improve model interpretability. For text-to-image reasoning, we design Mask Generate and Mask Transfer modules, so as to fully integrate the text priors into the reasoning and prediction of masks. For image-to-text reasoning, we propose an unsupervised triple reconstruction method to guide text-to-image reasoning and improve multimodal generalization. By bi-directional reasoning between image and text, the proposed method MRR fully conforms to the multimodal relational reasoning process. Experiments show that MRR achieves state-of-the-art performance on two datasets of referring relationships, VRD and Visual Genome.}
}
@article{GARG2021120407,
title = {Measuring the perceived benefits of implementing blockchain technology in the banking sector},
journal = {Technological Forecasting and Social Change},
volume = {163},
pages = {120407},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120407},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520312336},
author = {Poonam Garg and Bhumika Gupta and Ajay Kumar Chauhan and Uthayasankar Sivarajah and Shivam Gupta and Sachin Modgil},
keywords = {Blockchain, Banking sector, Perceived benefits, Instrument development, AMOS},
abstract = {This study aims to measure the perceived business benefits of blockchain technology implementation in the banking sector and establish factors to measure these benefits. Concerns regarding security, values, and standards are essential to banking operations. Data was collected from 291 respondents who are either blockchain consultants, blockchain marketing experts, or CEOs/business heads of banks that are in the process of advising, consulting, or implementing blockchain technology. Confirmatory factor analysis (CFA) was carried out to assess the reliability and validity of the proposed instrument. The results support the proposed instrument and its five constructs. The scale emerging from this study indicates a good degree of reliability, validity and unidimensionality in each of its constructs. Technologies like blockchain are in their initial stages, and recent advances in blockchain technology may impact our findings. The developed instrument could help give decision makers a foundational view to measure the benefits of implementing blockchain technology before they choose to integrate it in their existing system. The scientific and societal significance of the study based on its practical and theoretical applications is presented at the end.}
}
@article{DETERWANGNE2021105497,
title = {Council of Europe convention 108+: A modernised international treaty for the protection of personal data},
journal = {Computer Law & Security Review},
volume = {40},
pages = {105497},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2020.105497},
url = {https://www.sciencedirect.com/science/article/pii/S0267364920301023},
author = {Cécile {de Terwangne}},
keywords = {Data protection, Council of Europe Convention 108, Modernised Convention 108, Personal data, Informational autonomy, Data subject’s rights, Data security, Transborder data flows, Supervisory authority, Convention Committee},
abstract = {Summary
The Council of Europe has modernized its Convention 108 for the protection of individuals with regard to automatic processing of personal data: in 2018 it adopted Convention 108+. The modernised version of Convention 108 seeks to respond to the challenges posed, in terms of human rights, by the use of new information and communication technologies. This article presents a detailed analysis of this new international text. Convention 108+ contains important innovations: it proclaims the importance of protecting the right to informational autonomy and human dignity in the face of technological developments. It consolidates the proportionality requirement for data processing and strengthens the arsenal of rights of the data subjects. It reinforces the responsibility of those in charge of data processing as well as its transparency. It requires notification of security breaches. It strengthens the independence, powers and means of action of the supervisory authorities. It also strengthens the mechanism to ensure its effective implementation by entrusting the Committee set up by the Convention with the task of verifying compliance with the commitments made by Parties.}
}
@article{OLDHAM20212040,
title = {NHLBI-CMREF Workshop Report on Pulmonary Vascular Disease Classification: JACC State-of-the-Art Review},
journal = {Journal of the American College of Cardiology},
volume = {77},
number = {16},
pages = {2040-2052},
year = {2021},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2021.02.056},
url = {https://www.sciencedirect.com/science/article/pii/S0735109721005660},
author = {William M. Oldham and Anna R. Hemnes and Micheala A. Aldred and John Barnard and Evan L. Brittain and Stephen Y. Chan and Feixiong Cheng and Michael H. Cho and Ankit A. Desai and Joe G.N. Garcia and Mark W. Geraci and Susan D. Ghiassian and Kathryn T. Hall and Evelyn M. Horn and Mohit Jain and Rachel S. Kelly and Jane A. Leopold and Sara Lindstrom and Brian D. Modena and William C. Nichols and Christopher J. Rhodes and Wei Sun and Andrew J. Sweatt and Rebecca R. Vanderpool and Martin R. Wilkins and Beth Wilmot and Roham T. Zamanian and Joshua P. Fessel and Neil R. Aggarwal and Joseph Loscalzo and Lei Xiao},
keywords = {drug repurposing, integrative omics, master clinical trial protocol, precision medicine, pulmonary hypertension, systems biology},
abstract = {The National Heart, Lung, and Blood Institute and the Cardiovascular Medical Research and Education Fund held a workshop on the application of pulmonary vascular disease omics data to the understanding, prevention, and treatment of pulmonary vascular disease. Experts in pulmonary vascular disease, omics, and data analytics met to identify knowledge gaps and formulate ideas for future research priorities in pulmonary vascular disease in line with National Heart, Lung, and Blood Institute Strategic Vision goals. The group identified opportunities to develop analytic approaches to multiomic datasets, to identify molecular pathways in pulmonary vascular disease pathobiology, and to link novel phenotypes to meaningful clinical outcomes. The committee suggested support for interdisciplinary research teams to develop and validate analytic methods, a national effort to coordinate biosamples and data, a consortium of preclinical investigators to expedite target evaluation and drug development, longitudinal assessment of molecular biomarkers in clinical trials, and a task force to develop a master clinical trials protocol for pulmonary vascular disease.}
}
@article{SEE2021736881,
title = {Aquaculture efficiency and productivity: A comprehensive review and bibliometric analysis},
journal = {Aquaculture},
volume = {544},
pages = {736881},
year = {2021},
issn = {0044-8486},
doi = {https://doi.org/10.1016/j.aquaculture.2021.736881},
url = {https://www.sciencedirect.com/science/article/pii/S0044848621005445},
author = {Kok Fong See and Rabiatul Adawiyah Ibrahim and Kim Huat Goh},
keywords = {Aquaculture, Bibliometric analysis, Data envelopment analysis, Efficiency, Productivity, Stochastic frontier analysis},
abstract = {The scientific research on aquaculture efficiency and productivity has been increasing over the years. This study aims to identify the publication trends and growth potential of aquaculture efficiency and productivity studies. A bibliometric analysis was employed for a sample of 85 scientific articles published during the 1998–2020 period. The findings show that authors and institutions have close groups in collaboration networks. Through the citation analysis, three clusters were obtained that were related to the use of stochastic frontier analysis in an empirical application, Norwegian salmon aquaculture, and efficiency studies associated to freshwater aquaculture. For the temporal evolution of the keywords, earlier existing studies adopted a stochastic translog production function to assess the technical efficiency of aquaculture production, whereas later studies used data envelopment analysis, which focused on more diverse research objectives. The farms or subsectors of aquaculture in Norway, Bangladesh, and Vietnam have been analyzed in-depth for the efficiency and productivity in the existing studies. Education, experience, and age of farmers are often used as determinants to explain the variations in technical efficiency. The present study concludes that aquaculture efficiency and productivity research is not moving toward a mature stage. Several of the discovered issues are only focused on specific countries, and there is still room for methodological improvement in assessing aquaculture efficiency and productivity. Nevertheless, research collaborations are growing, and new research trends that are related to environmental regulation and pollution show great interest in aquaculture efficiency and productivity. This study provides a clear roadmap for researchers and practitioners to understand the publication patterns and hotspots in the research field.}
}
@article{YAN2021100011,
title = {Emerging approaches applied to maritime transport research: Past and future},
journal = {Communications in Transportation Research},
volume = {1},
pages = {100011},
year = {2021},
issn = {2772-4247},
doi = {https://doi.org/10.1016/j.commtr.2021.100011},
url = {https://www.sciencedirect.com/science/article/pii/S2772424721000111},
author = {Ran Yan and Shuaian Wang and Lu Zhen and Gilbert Laporte},
keywords = {Maritime transport, Shipping, Port, Data-driven modeling, Digitalization in the maritime industry},
abstract = {Maritime transport is the backbone of international trade and globalization. Maritime transport research can be roughly divided into two categories, namely the shipping side and the port side. Most of the classic approaches adopted to address practical problems in these research topics are based on long-term observations and expert knowledge, while few of them are based on historical data accumulated from practice. In recent years, emerging approaches, which we refer to as machine learning and deep learning techniques in this essay, have been receiving a wider attention to solve practical problems. As a relatively conservative industry, there are some initial trials of applying the emerging approaches to solve practical problems in the maritime sector. The objective of this essay is to review the application of emerging approaches to maritime transport research. The main research topics in maritime transport and classic methods developed to solve them are first presented. The introduction of emerging approaches and their suitability to be applied in maritime transport research is then discussed. Related existing studies are then reviewed according to problem settings, main data sources, and emerging approaches adopted. Challenges and solutions in the process are also discussed from the perspectives of data, model, users, and targets. Finally, promising future research directions are identified. This essay is the first to give a comprehensive review of existing studies on developing machine learning and deep learning models together with popular data sources used to address practical problems in maritime transport.}
}
@article{YAO2023109084,
title = {Regularizing autoencoders with wavelet transform for sequence anomaly detection},
journal = {Pattern Recognition},
volume = {134},
pages = {109084},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.109084},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322005647},
author = {Yueyue Yao and Jianghong Ma and Yunming Ye},
keywords = {Sequence anomaly detection, Autoencoder, Discrete wavelet transform, Frequency domain regularization, Sample-adaptive regularization weight},
abstract = {Nowadays, systems or entities are usually monitored by devices, generating large amounts of time series. Detecting anomalies in them help prevent potential losses, thus arousing much research interest. Existing studies have adopted autoencoders to detect anomalies, where reconstruction errors are used to indicate outliers. However, sometimes autoencoders may also reconstruct anomalies well due to the learned general features in latent spaces. To solve the above problem, we propose to regularize autoencoders to grasp specific features of normal sequences. Specifically, spectral unique patterns are captured by statistical analysis on discrete wavelet transform (DWT) coefficients of input sequences, restricting latent spaces to reflect unique patterns of normal sequences in both time and frequency domains. Furthermore, a Weight Controller calculating sample-adaptive regularization weights is designed to fully utilize the regularization effect. Extensive experiments on three public benchmarks demonstrate the effectiveness and superiority of the proposed model compared with state-of-the-art algorithms.}
}
@article{YANG2021102384,
title = {Digital transformation solutions of entrepreneurial SMEs based on an information error-driven T-spherical fuzzy cloud algorithm},
journal = {International Journal of Information Management},
pages = {102384},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102384},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000773},
author = {Zaoli Yang and Jinping Chang and Lucheng Huang and Abbas Mardani},
keywords = {Digital transformation, Entrepreneurial SMEs, Evaluation and selection, T-spherical fuzzy cloud, T-spherical fuzzy cloud weighted Heronian mean operator},
abstract = {The digital transformation of enterprises has become an inevitable development trend and one of the key driving forces that promotes the sustainable development of enterprises. However, due to the many obstacles of financial burdens, technical thresholds, and talent shortages, digital transformation has become a challenging task for entrepreneurial Small and Medium-Sized Enterprises (SMEs). Additionally, many competitive digital transformation solutions on the market cause confusion when enterprises must choose one. This study drew a new information error-driven T-spherical fuzzy cloud algorithm to evaluate digital transformation solutions of entrepreneurial SMEs and support its selection. First, an evaluation index system for the digital transformation solution of entrepreneurial SMEs was established from four aspects. Then, a new concept of a T-spherical fuzzy cloud was defined to represent the evaluation information of the indicators. Additionally, a T-spherical Fuzzy Cloud Weighted Heronian Mean (T-SFCWHM) operator was used to aggregate the evaluation information. Afterward, an evaluation and selection decision framework for the digital transformation solution of entrepreneurial SMEs based on the T-SFCWHM operator was developed. Further, a practical example was given to illustrate the effectiveness of the proposed method. Finally, a discussion of the findings in our study was conducted, and the conclusions were summarized.}
}
@article{ZHANG2021114261,
title = {Using CatBoost algorithm to identify middle-aged and elderly depression, national health and nutrition examination survey 2011–2018},
journal = {Psychiatry Research},
volume = {306},
pages = {114261},
year = {2021},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2021.114261},
url = {https://www.sciencedirect.com/science/article/pii/S0165178121005564},
author = {Chenyang Zhang and Xiaofei Chen and Song Wang and Junjun Hu and Chunpeng Wang and Xin Liu},
keywords = {Depression, Machine learning, Middle-aged and elderly, NHANES},
abstract = {Depression is one of the most common mental health problems in middle-aged and elderly people. The establishment of risk factor-based depression risk assessment model is conducive to early detection and early treatment of high-risk groups of depression. Five machine learning models (logistic regression (LR); back propagation (BP); random forest (RF); support vector machines (SVM); category boosting (CatBoost) were used to evaluate the depression among 8374 middle-aged people and 4636 elderly people in the NHANES database from 2011 to 2018. In the 2011–2018 cycle, the estimated prevalence of depression was 8.97% in the middle-aged participants and 8.02% in the elderly participants. Among the middle-aged and elderly participants, CatBoost was the best model to identify depression, and its area under the working characteristic curve (AUC) reaches the highest. The second is LR model and SVM model, while the performance of BP and RF model was slightly worse. The primary influencing factor of depression in middle-aged male is alanine aminotransferase. All five machine learning models can identify the occurrence of depression in the NHANES data set through social demographics, lifestyle, laboratory data and other data of middle-aged and elderly people, and among five models, the CatBoost model performed best.}
}
@article{KOLAR2021105392,
title = {On interdisciplinarity in the humanities: A comment on Fanta et al. (2020) on the bias in dating obtained from historical sources},
journal = {Journal of Archaeological Science},
volume = {132},
pages = {105392},
year = {2021},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2021.105392},
url = {https://www.sciencedirect.com/science/article/pii/S0305440321000625},
author = {Jan Kolář and Péter Szabó},
keywords = {Settlement history, Written records, Middle ages, Time lag, Archaeological method, Bohemia, Central Europe},
abstract = {Medieval settlement history in Europe is a common topic in several scientific disciplines. Recently, Fanta et al. (2020) examined colonization processes in Bohemia through the comparison of archaeological evidence and historical records. They concluded that the first mentions of settlements in historical documents are not reliable sources for settlement dating and should always be verified and preferably superseded by archaeological data, which are, in contrast, mostly unproblematic. We argue that this conclusion is controversial from several aspects. Firstly, it neglects the disciplinary constraints of archaeological evidence for medieval settlement development, as regards quality and chronology. Secondly, there are several legitimate perspectives from which to analyse the data. Our reanalysis of the original dataset showed that – in partial contrast to the conclusions of Fanta et al. (2020) – when viewed from the point of view of historical evidence, the time lag between the historical and archaeological dating increased with time and that the historical dating of most of the settlements between the 10th and 13th centuries was supported by archaeological evidence. Lastly, we demonstrated how research combining different disciplines (archaeology, history, palaeoecology, geography) and types evidence can reveal the manifold processes of human settlement dynamics. In our view each type of evidence has advantages as well as drawbacks, therefore strictly prioritising one at the expense of others hardly furthers the understanding of complex social phenomena.}
}
@article{BROWN2021107694,
title = {Opportunities for improving recognition of coastal wetlands in global ecosystem assessment frameworks},
journal = {Ecological Indicators},
volume = {126},
pages = {107694},
year = {2021},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2021.107694},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X21003599},
author = {Christopher J. Brown and Maria F. Adame and Christina A. Buelow and Marieke A. Frassl and Shing Yip Lee and Brendan Mackey and Eva C. McClure and Ryan M. Pearson and Anusha Rajkaran and Thomas S. Rayner and Michael Sievers and Chantal A. {Saint Ange} and Ana I. Sousa and Vivitskaia J.D. Tulloch and Mischa P. Turschwell and Rod M. Connolly},
keywords = {Seagrass, Saltmarsh, Mangrove, Fish nursery, Ecosystem condition, System of environmental-economic accounting indicators, Biodiversity, Health index},
abstract = {Vegetated coastal wetlands, including seagrass, saltmarsh and mangroves, are threatened globally, yet the need to avert these losses is poorly recognized in international policy, such as in the Convention on Biological Diversity and the United Nations (UN) Sustainable Development Goals. Identifying the impact of overlooking coastal wetlands in ecosystem assessment frameworks could help prioritize research efforts to fill these gaps. Here, we examine gaps in the recognition of coastal wetlands in globally applicable ecosystem assessments. We address both shortfalls in assessment frameworks when it comes to assessing wetlands, and gaps in data that limit widespread application of assessments. We examine five assessment frameworks that track fisheries, greenhouse gas emissions, ecosystem threats, and ecosystem services. We found that these assessments inform management decisions, but that the functions provided by coastal wetlands are incompletely represented. Most frameworks had sufficient complexity to measure wetland status, but limitations in data meant they were incompletely informed about wetland functions and services. Incomplete representation of coastal wetlands may lead to them being overlooked by research and management. Improving the coverage of coastal wetlands in ecosystem assessments requires improving global scale mapping of wetland trends, developing global-scale indicators of wetland function and synthesis to quantitatively link animal population dynamics to wetland trends. Filling these gaps will help ensure coastal wetland conservation is properly informed to manage them for the outstanding benefits they bring humanity.}
}
@article{YUAN2021401,
title = {Spatiotemporal attention mechanism-based deep network for critical parameters prediction in chemical process},
journal = {Process Safety and Environmental Protection},
volume = {155},
pages = {401-414},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2021.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S0957582021004900},
author = {Zhuang Yuan and Zhe Yang and Yiqun Ling and Chuanpeng Wu and Chuankun Li},
keywords = {Chemical processes, Parameters prediction, Deep networks, Spatiotemporal attention mechanism, Feature representation},
abstract = {In chemical processes, grasping the changing trend of critical parameters can help field operators take appropriate adjustments to eliminate potential fluctuations. Thus, deep networks, renowned for its revolutionary feature representation capability, have been gradually exploited for building reliable prediction models from massive data embraced tremendously nonlinearities and dynamics. Because of the inherent complexity, the process trajectories over the whole running duration make distinctive contributions to the ultimate targets. Specifically, features extracted from different secondary variables at different previous instants have diverse impacts on the current state of primary variables. However, this spatiotemporal relevance discrepancy is rarely considered, which may lead to deterioration of prediction performance. Therefore, this paper seamlessly integrates the spatiotemporal attention (STA) mechanism with convolutional neural networks (CNN) and bi-directional long short-term memory (BiLSTM), and proposes a novel predictive model, namely STA-ConvBiLSTM. Using the deep framework composed of CNN and BiLSTM, the integrated model can, not only automatically explore the esoteric spatial correlations among high-dimensional variables at each time step, but also adaptively excavate beneficial temporal characteristics across all time steps. Meanwhile, STA is further introduced to assign corresponding weights to information with dissimilar importance, so as to prevent high target-relevant interactions from being discarded due to overlong sequences and excessive features. STA-ConvBiLSTM is applied in the case of furnace tube temperature prediction of a delayed coking unit, which exhibits a significant improvement of the prediction accuracy.}
}
@article{WU20213318,
title = {Iterative tomography with digital adaptive optics permits hour-long intravital observation of 3D subcellular dynamics at millisecond scale},
journal = {Cell},
volume = {184},
number = {12},
pages = {3318-3332.e17},
year = {2021},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2021.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0092867421005328},
author = {Jiamin Wu and Zhi Lu and Dong Jiang and Yuduo Guo and Hui Qiao and Yi Zhang and Tianyi Zhu and Yeyi Cai and Xu Zhang and Karl Zhanghao and Hao Xie and Tao Yan and Guoxun Zhang and Xiaoxu Li and Zheng Jiang and Xing Lin and Lu Fang and Bing Zhou and Peng Xi and Jingtao Fan and Li Yu and Qionghai Dai},
keywords = {long-term high-speed imaging, adaptive optics, light-field microscopy, phototoxicity, intravital, migrasome, retraction fiber, tumor metastasis, calcium imaging},
abstract = {Summary
Long-term subcellular intravital imaging in mammals is vital to study diverse intercellular behaviors and organelle functions during native physiological processes. However, optical heterogeneity, tissue opacity, and phototoxicity pose great challenges. Here, we propose a computational imaging framework, termed digital adaptive optics scanning light-field mutual iterative tomography (DAOSLIMIT), featuring high-speed, high-resolution 3D imaging, tiled wavefront correction, and low phototoxicity with a compact system. By tomographic imaging of the entire volume simultaneously, we obtained volumetric imaging across 225 × 225 × 16 μm3, with a resolution of up to 220 nm laterally and 400 nm axially, at the millisecond scale, over hundreds of thousands of time points. To establish the capabilities, we investigated large-scale cell migration and neural activities in different species and observed various subcellular dynamics in mammals during neutrophil migration and tumor cell circulation.}
}
@article{WANG20211,
title = {TCM network pharmacology: A new trend towards combining computational, experimental and clinical approaches},
journal = {Chinese Journal of Natural Medicines},
volume = {19},
number = {1},
pages = {1-11},
year = {2021},
issn = {1875-5364},
doi = {https://doi.org/10.1016/S1875-5364(21)60001-8},
url = {https://www.sciencedirect.com/science/article/pii/S1875536421600018},
author = {Xin WANG and Zi-Yi WANG and Jia-Hui ZHENG and Shao LI},
keywords = {Network pharmacology, Traditional Chinese medicine, Network target, Computation, Experiment, Clinical approach},
abstract = {Traditional Chinese medicine (TCM) is a precious treasure of the Chinese nation and has unique advantages in the prevention and treatment of diseases. The holistic view of TCM coincides with the new generation of medical research paradigm characterized by network and system. TCM gave birth to a new method featuring holistic and systematic “network target”, a core theory and method of network pharmacology. TCM is also an important research object of network pharmacology. TCM network pharmacology, which aims to understand the network-based biological basis of complex diseases, TCM syndromes and herb treatments, plays a critical role in the origin and development process of network pharmacology. This review introduces new progresses of TCM network pharmacology in recent years, including predicting herb targets, understanding biological foundation of diseases and syndromes, network regulation mechanisms of herbal formulae, and identifying disease and syndrome biomarkers based on biological network. These studies show a trend of combining computational, experimental and clinical approaches, which is a promising direction of TCM network pharmacology research in the future. Considering that TCM network pharmacology is still a young research field, it is necessary to further standardize the research process and evaluation indicators to promote its healthy development.}
}
@article{THOMAS2021100427,
title = {SPARTAN: Maximizing the use of spectro-photometric observational data during template fitting},
journal = {Astronomy and Computing},
volume = {34},
pages = {100427},
year = {2021},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2020.100427},
url = {https://www.sciencedirect.com/science/article/pii/S2213133720300810},
author = {R. Thomas},
keywords = {Galaxy, Fitting, Observations, Spectroscopy, Photometry},
abstract = {SPARTAN [Spectroscopic And photometRic fitting Tool for Astronomical aNalysis] is a tool designed to perform the fitting of galaxy observations either using photometry and low resolution spectroscopy separately or simultaneously. Based on a grid search χ2 fitting method, SPARTAN was tailored to UV-to-NIR data and designed for well calibrated data. The first version of this tool allows the use of the low resolution models of Bruzual & Charlot (2003) and include the treatment of the intergalactic medium as a free parameter. It has been designed to be an user-friendly environment where people do not need to know how to code to perform the fit. SPARTAN includes everything needed to perform the fit, from the galaxy models creation, to the visualization of the results through the graphical interface. SPARTAN is a fully open source software made with Python 3. It is published under the GNU general public license (v3) and is available in a Github repository. It can be installed directly from the python official repository (pypi) and the documentation is available through a github repository.}
}
@article{RAGHUNATHAN2023105626,
title = {Factors in integrating academic electronic medical records in nursing curricula: A qualitative multiple case studies approach},
journal = {Nurse Education Today},
volume = {120},
pages = {105626},
year = {2023},
issn = {0260-6917},
doi = {https://doi.org/10.1016/j.nedt.2022.105626},
url = {https://www.sciencedirect.com/science/article/pii/S0260691722003628},
author = {Kalpana Raghunathan and Lisa McKenna and Monica Peddle},
keywords = {Electronic medical records, Curriculum, Education, Technology, Nursing, Students},
abstract = {Background
Academic electronic medical records are useful simulation-based educational tools that assist health professional students develop their skill sets for digital health practice. Despite this, their utilisation in pre-registration nursing curricula is uncommon in Australia and New Zealand.
Aim
To explore factors surrounding integration of academic electronic medical records into pre-registration nursing curricula in Australia and New Zealand.
Design
Exploratory qualitative multiple case studies approach with purposive sampling set within a larger research project.
Methods
Semi-structured interviews conducted with course leaders from six nursing schools. Data were analysed in an iterative content-driven deductive and inductive process using open-coding and categories. Case analysis involved within case and cross-case analysis.
Results
Findings revealed different factors that impacted the utilisation of academic electronic medical records in nursing curricula including factors influencing adoption, barriers and challenges with implementation, enablers for integration and perceived benefits for students' clinical practice preparation. Reasons for not using academic electronic medical records, barriers for implementation, and preparation of students for clinical practice in the absence of these simulation tools were also highlighted.
Conclusion
Our findings suggest that use of academic electronic medical records in nursing curricula is still evolving and that their adoption and application within programs is not straightforward. While there are many factors unique to the schools using such resources, factors including decisions around curriculum incorporation, optimising available resources to support students' learning, and developing faculty capability to teach with academic electronic medical records were common considerations. Lack of funding and access to local educational tools were ongoing barriers for adoption. Further research examining curriculum timing and preparation, possibilities of partnerships to share resources, and evaluation in meeting students' needs is necessary.}
}
@article{AUGUSTE2021102053,
title = {Heterogeneity in head and neck cancer incidence among black populations from Africa, the Caribbean and the USA: Analysis of cancer registry data by the AC3},
journal = {Cancer Epidemiology},
volume = {75},
pages = {102053},
year = {2021},
issn = {1877-7821},
doi = {https://doi.org/10.1016/j.canep.2021.102053},
url = {https://www.sciencedirect.com/science/article/pii/S1877782121001703},
author = {Aviane Auguste and Samuel Gathere and Paulo S. Pinheiro and Clement Adebamowo and Adeola Akintola and Kellie Alleyne-Mike and Simon G. Anderson and Kimlin Ashing and Fred Kwame Awittor and Baffour Awuah and Bernard Bhakkan and Jacqueline Deloumeaux and Maira du Plessis and Ima-Obong A. Ekanem and Uwemedimbuk Ekanem and Emmanuel Ezeome and Nkese Felix and Andrew K. Gachii and Stanie Gaete and Tracey Gibson and Robert Hage and Sharon Harrison and Festus Igbinoba and Kufre Iseh and Evans Kiptanui and Ann Korir and Heather-Dawn Lawson-Myers and Adana Llanos and Daniele Luce and Dawn McNaughton and Michael Odutola and Abidemi Omonisi and Theresa Otu and Jessica Peruvien and Nasiru Raheem and Veronica Roach and Natasha Sobers and Nguundja Uamburu and Camille Ragin},
keywords = {Head and neck cancer, Incidence, Blacks, Tobacco smoking, Alcohol drinking, HPV, Caribbean, Africa, USA, Population-based cancer registry},
abstract = {Background
Africa and the Caribbean are projected to have greater increases in Head and neck cancer (HNC) burden in comparison to North America and Europe. The knowledge needed to reinforce prevention in these populations is limited. We compared for the first time, incidence rates of HNC in black populations from African, the Caribbean and USA.
Methods
Annual age-standardized incidence rates (IR) and 95% confidence intervals (95%CI) per 100,000 were calculated for 2013–2015 using population-based cancer registry data for 14,911 HNC cases from the Caribbean (Barbados, Guadeloupe, Trinidad & Tobago, N = 443), Africa (Kenya, Nigeria, N = 772) and the United States (SEER, Florida, N = 13,696). We compared rates by sub-sites and sex among countries using data from registries with high quality and completeness.
Results
In 2013–2015, compared to other countries, HNC incidence was highest among SEER states (IR: 18.2, 95%CI = 17.6–18.8) among men, and highest in Kenya (IR: 7.5, 95%CI = 6.3–8.7) among women. Nasopharyngeal cancer IR was higher in Kenya for men (IR: 3.1, 95%CI = 2.5–3.7) and women (IR: 1.5, 95%CI = 1.0–1.9). Female oral cavity cancer was also notably higher in Kenya (IR = 3.9, 95%CI = 3.0–4.9). Blacks from SEER states had higher incidence of laryngeal cancer (IR: 5.5, 95%CI = 5.2–5.8) compared to other countries and even Florida blacks (IR: 4.4, 95%CI = 3.9–5.0).
Conclusion
We found heterogeneity in IRs for HNC among these diverse black populations; notably, Kenya which had distinctively higher incidence of nasopharyngeal and female oral cavity cancer. Targeted etiological investigations are warranted considering the low consumption of tobacco and alcohol among Kenyan women. Overall, our findings suggest that behavioral and environmental factors are more important determinants of HNC than race.}
}
@article{WANG202141,
title = {3D Printing, Computational Modeling, and Artificial Intelligence for Structural Heart Disease},
journal = {JACC: Cardiovascular Imaging},
volume = {14},
number = {1},
pages = {41-60},
year = {2021},
issn = {1936-878X},
doi = {https://doi.org/10.1016/j.jcmg.2019.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X20305155},
author = {Dee Dee Wang and Zhen Qian and Marija Vukicevic and Sandy Engelhardt and Arash Kheradvar and Chuck Zhang and Stephen H. Little and Johan Verjans and Dorin Comaniciu and William W. O’Neill and Mani A. Vannan},
keywords = {3D printing, artificial intelligence, computational modeling, computed tomography, left atrial appendage, structural heart disease, transcatheter aortic valve replacement, transcatheter mitral valve replacement, transesophageal echocardiogram},
abstract = {Structural heart disease (SHD) is a new field within cardiovascular medicine. Traditional imaging modalities fall short in supporting the needs of SHD interventions, as they have been constructed around the concept of disease diagnosis. SHD interventions disrupt traditional concepts of imaging in requiring imaging to plan, simulate, and predict intraprocedural outcomes. In transcatheter SHD interventions, the absence of a gold-standard open cavity surgical field deprives physicians of the opportunity for tactile feedback and visual confirmation of cardiac anatomy. Hence, dependency on imaging in periprocedural guidance has led to evolution of a new generation of procedural skillsets, concept of a visual field, and technologies in the periprocedural planning period to accelerate preclinical device development, physician, and patient education. Adaptation of 3-dimensional (3D) printing in clinical care and procedural planning has demonstrated a reduction in early-operator learning curve for transcatheter interventions. Integration of computation modeling to 3D printing has accelerated research and development understanding of fluid mechanics within device testing. Application of 3D printing, computational modeling, and ultimately incorporation of artificial intelligence is changing the landscape of physician training and delivery of patient-centric care. Transcatheter structural heart interventions are requiring in-depth periprocedural understanding of cardiac pathophysiology and device interactions not afforded by traditional imaging metrics.}
}
@article{EMMENS2021114975,
title = {The promises and perils of Automatic Identification System data},
journal = {Expert Systems with Applications},
volume = {178},
pages = {114975},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114975},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421004164},
author = {Ties Emmens and Chintan Amrit and Asad Abdi and Mayukh Ghosh},
keywords = {AIS data, Data mining, Navigation safety, Ship behavior analysis, Environmental evaluation, Advanced applications of AIS data},
abstract = {Automatic Identification System (AIS) is used to identify vessels in maritime navigation. Currently, it is used for various commercial purposes. However, the abundance and lack of quality of AIS data make it difficult to capitalize on its value. Therefore, an understanding of both the limitations of AIS data and the opportunities is important to maximize its value, but these have not been clearly stated in the existing literature. This study aims to help researchers and practitioners understand AIS data by identifying both the promises and perils of AIS data. We identify the different applications and limitations of AIS data in the literature and build upon them in a sequential mixed-design study. We first identify the promises and perils that exist in the literature. We then analyze AIS data from the port of Amsterdam quantitatively to detect noise and to find the perils researchers and practitioners could encounter. Our results incorporate quantitative findings with qualitative insights obtained from interviewing domain experts. This study extends the literature by considering multiple limitations of AIS data across different domains at the same time. Our results show that the amount of noise in AIS data depends on factors such as the equipment used, external factors, humans, dense traffic etc. The contribution that our paper makes is in combining and making a comprehensive list of both the promises and perils of AIS data. Consequently, this study helps researchers and practitioners to (i) identify the sources of noise, (ii) to reduce the noise in AIS data and (iii) use it for the benefits of their research or the optimization of their operations.}
}
@incollection{2021xxxiii,
title = {Author biographies},
editor = {David Baker and Lucy Ellis},
booktitle = {Libraries, Digital Information, and COVID},
publisher = {Chandos Publishing},
pages = {xxxiii-xliii},
year = {2021},
series = {Chandos Digital Information Review},
isbn = {978-0-323-88493-8},
doi = {https://doi.org/10.1016/B978-0-323-88493-8.00030-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884938000306}
}
@article{KHALLAF2021103760,
title = {Classification and analysis of deep learning applications in construction: A systematic literature review},
journal = {Automation in Construction},
volume = {129},
pages = {103760},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103760},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002119},
author = {Rana Khallaf and Mohamed Khallaf},
keywords = {Systematic literature review, Deep learning, Construction, Damage detection},
abstract = {In recent years, the construction industry has experienced an expansion in the multitude of projects and emergent information. With the advent of deep learning, new opportunities have emerged for utilizing this vast amount of data to solve construction-related issues. While the use of deep learning has been increasing in construction, there has been no review on these applications to date. Therefore, this paper presents a Systematic Literature Review on the use of deep learning applications in construction. A total of 80 journal papers were identified and analyzed. Among these papers, six application-based topics were identified: equipment tracking, crack detection, construction work management, sewer assessment, 3D point cloud enhancement, and miscellaneous topics. Analysis shows that deep learning has been beneficial in leveraging data in areas such as crack detection and segmentation of infrastructure and sewers; equipment and worker detection and; and analysis and reporting on construction-related operations. Additionally, a discussion of the various deep learning techniques is provided as well as a contrast between deep learning, machine learning, and artificial intelligence.}
}
@article{SHAKORSHAHABI2021102337,
title = {Application of data mining in Iran's Artisanal and Small-Scale mines challenges analysis},
journal = {Resources Policy},
volume = {74},
pages = {102337},
year = {2021},
issn = {0301-4207},
doi = {https://doi.org/10.1016/j.resourpol.2021.102337},
url = {https://www.sciencedirect.com/science/article/pii/S0301420721003469},
author = {Reza ShakorShahabi and Ali Nouri Qarahasanlou and Seyed Reza Azimi and Adel Mottahedi},
keywords = {Data mining, Artisanal and small-scale mines, Clustering, Decision tree},
abstract = {Most of the mines operating in Iran are classified into Artisanal and Small-scale mines (ASM). ASM accounts for 98.3% of the country's 10,000 mines, more than 80% of employment, and about 65% of the mining sector production. However, these mines face liquidity, legal and administrative issues, sales market, infrastructure, and investment. Though, their activation and restoration require many limited resources compared to large mines. Therefore, it is undeniable to use this sector's capacity to create sustainable employment and development in deprived areas of the country (due to ASM's geographical extent) and help supply raw materials. Hence, in this paper, in the first step, identifying and troubleshooting in these mines was done based on field information and organ documents such as Ministry of Industry, Mine and Trade, Iranian Mines and Mining Industries Development and Renovation Organization (IMIDRO), Iran Minerals Procurement and Production Company, etc. A database consisting of 313 mines from 29 provinces of the country was formed and evaluated using a data mining approach. In this study, two data mining methods, including clustering and decision tree, were used. As a result, appropriate divisions were presented based on available information without any previous hypotheses or backgrounds. The purpose of these divisions was to provide an appropriate classification of mines by applying different estimators to make strategic decisions. Because at present, in most decisions, mines are divided solely based on an estimator such as geographical distance, mineral genus, annual production.}
}
@article{DWIVEDI2021102168,
title = {Setting the future of digital and social media marketing research: Perspectives and research propositions},
journal = {International Journal of Information Management},
volume = {59},
pages = {102168},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102168},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220308082},
author = {Yogesh K. Dwivedi and Elvira Ismagilova and D. Laurie Hughes and Jamie Carlson and Raffaele Filieri and Jenna Jacobson and Varsha Jain and Heikki Karjaluoto and Hajer Kefi and Anjala S. Krishen and Vikram Kumar and Mohammad M. Rahman and Ramakrishnan Raman and Philipp A. Rauschnabel and Jennifer Rowley and Jari Salo and Gina A. Tran and Yichuan Wang},
keywords = {Artificial intelligence, Augmented reality marketing, Digital marketing, Ethical issues, eWOM, Mobile marketing, Social media marketing},
abstract = {The use of the internet and social media have changed consumer behavior and the ways in which companies conduct their business. Social and digital marketing offers significant opportunities to organizations through lower costs, improved brand awareness and increased sales. However, significant challenges exist from negative electronic word-of-mouth as well as intrusive and irritating online brand presence. This article brings together the collective insight from several leading experts on issues relating to digital and social media marketing. The experts’ perspectives offer a detailed narrative on key aspects of this important topic as well as perspectives on more specific issues including artificial intelligence, augmented reality marketing, digital content management, mobile marketing and advertising, B2B marketing, electronic word of mouth and ethical issues therein. This research offers a significant and timely contribution to both researchers and practitioners in the form of challenges and opportunities where we highlight the limitations within the current research, outline the research gaps and develop the questions and propositions that can help advance knowledge within the domain of digital and social marketing.}
}
@article{BRANDT2021379,
title = {Prescriptive analytics in public-sector decision-making: A framework and insights from charging infrastructure planning},
journal = {European Journal of Operational Research},
volume = {291},
number = {1},
pages = {379-393},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.09.034},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720308389},
author = {Tobias Brandt and Sebastian Wagner and Dirk Neumann},
keywords = {Decision support systems, Public value, Prescriptive analytics, Smart city, Electric mobility},
abstract = {In this work, we investigate the challenges public-sector organizations face when seeking to leverage prescriptive analytics and provide insights into the public value such data-driven tools and methods can provide. Using the strategic triangle of value, legitimacy, and operational capacity as a starting point, we derive a framework to assess public-sector prescriptive analytics initiatives, along with six guiding questions that structure the assessment process. We present a case study applying prescriptive analytics to the placement of charge points in urban areas, a critical challenge many municipalities are currently facing in the transition towards electric mobility. Reflecting on the analytics application as well as its development and implementation process through the guiding questions, we derive key lessons for public-sector organizations seeking to apply prescriptive analytics.}
}
@article{RUNESON2021111088,
title = {Open Data Ecosystems — An empirical investigation into an emerging industry collaboration concept},
journal = {Journal of Systems and Software},
volume = {182},
pages = {111088},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111088},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001850},
author = {Per Runeson and Thomas Olsson and Johan Linåker},
keywords = {Open data, Open data ecosystem, Open innovation, Empirical study},
abstract = {Software systems are increasingly depending on data, particularly with the rising use of machine learning, and developers are looking for new sources of data. Open Data Ecosystems (ODE) is an emerging concept for data sharing under public licenses in software ecosystems, similar to Open Source Software (OSS). It has certain similarities to Open Government Data (OGD), where public agencies share data for innovation and transparency. We aimed to explore open data ecosystems involving commercial actors. Thus, we organized five focus groups with 27 practitioners from 22 companies, public organizations, and research institutes. Based on the outcomes, we surveyed three cases of emerging ODE practice to further understand the concepts and to validate the initial findings. The main outcome is an initial conceptual model of ODEs’ value, intrinsics, governance, and evolution, and propositions for practice and further research. We found that ODE must be value driven. Regarding the intrinsics of data, we found their type, meta-data, and legal frameworks influential for their openness. We also found the characteristics of ecosystem initiation, organization, data acquisition and openness be differentiating, which we advise research and practice to take into consideration.}
}
@article{HUSSAIN2021e156,
title = {First American College of Surgeons National Surgical Quality Improvement Program Report from a Low-Middle-Income Country: A 1-Year Outcome Analysis of Neurosurgical Cases},
journal = {World Neurosurgery},
volume = {155},
pages = {e156-e167},
year = {2021},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2021.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S1878875021011967},
author = {Mustafa Mushtaq Hussain and Farida Bibi and Shafqat Shah and Rida Mitha and Muhammad Shahzad Shamim and Afsheen Ziauddin and Hasnain Zafar},
keywords = {30-Day complications, ACS-NSQIP, Developing world, Low-middle-income country, Neurosurgery, Pakistan, Postoperative outcomes},
abstract = {Background
Low-middle-income countries (LMICs) share a substantial proportion of global surgical complications. This is compounded by the seemingly deficient documentation of postsurgical complications and the lack of a national average for comparison. In this context, the implementation of the American College of Surgeons (ACS) National Surgical Quality Improvement Program (NSQIP) that compares hospital performance based on postsurgical complication data provided by a wide array of centers, could be a major initiative in a resource-challenged setting. Implementation of the NSQIP has provenly mitigated postoperative morbidity and mortality across many centers all over the world. To our knowledge, this report is the first from an LMIC to report its postoperative neurosurgical complications in comparison with international benchmarks.
Methods
Our hospital joined the NSQIP in 2019. Through a standardized ACS protocol, ACS-trained surgical clinical reviewers (SCRs) reviewed and extracted data from randomly assigned neurosurgical patients’ medical records from preoperative to postoperative (30-day) data using validated, standardized data definitions. SCRs entered deidentified data in an online Health Insurance Portability and Accountability Act web-based secure platform. The validated data were then consigned to the ACS NSQIP head office in the United States where the data were analyzed and compared with similar data from other centers registered with the NSQIP. In this way, our hospital was rated for each of the variables related to postsurgical complications after both spinal and cranial procedures, and the results were sent back to us in the form of text, tables, and graphs.
Results
Our initial report suggested a relatively higher odds ratio for sepsis and readmissions after spinal procedures at our hospital, and a similarly higher odds ratio for morbidity, sepsis, urinary tract infection, and surgical site infection for cranial procedures. For these variables, our hospital fell in the needs improvement category of the NSQIP. For the rest of the variables studied for both spinal and cranial procedures, the hospital fell in the as expected category of the NSQIP.
Conclusions
Implementation of the NSQIP is an important first step in creating a culture of transparency, safety, and quality. This is the first report of NSQIP implementation in an LMIC, and we have shown comparable results to developed countries.}
}
@article{LU2021117446,
title = {Review of meta-heuristic algorithms for wind power prediction: Methodologies, applications and challenges},
journal = {Applied Energy},
volume = {301},
pages = {117446},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117446},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921008369},
author = {Peng Lu and Lin Ye and Yongning Zhao and Binhua Dai and Ming Pei and Yong Tang},
keywords = {Meta-heuristic algorithms, Wind power forecasting, Combined approach, Multiple time horizons, Multiple error evaluation metrics},
abstract = {The integration of large-scale wind power introduces issues in modern power systems operations due to its strong randomness and volatility. These issues can be resolved via wind power forecasting that can provide comprehensive future information about wind power uncertainties. This paper presents a timely and comprehensive review of meta-heuristic algorithms in the framework of wind power forecasting. The framework is based on the auxiliary layer, forecasting base layer, and core layer. The auxiliary layer, such as the data-decomposition layer, decomposes the wind power time series into many relatively stationary subseries, and uses prediction models, including artificial neural networks (ANNs) and machine learning (ML). The core layer is based on meta-heuristic algorithms, which include evolutionary-based algorithms, physics-based algorithms, human-based algorithms, swarm-based algorithms, hybrid algorithms, and multi-objective optimization algorithms. These algorithms aim to search for the optimal solutions under constraints, which is highly significant for optimizing the key parameters of the prediction models. Besides, multiple error evaluation metrics, e.g., deterministic, uncertainty, and testing methods used in the field of wind power prediction are described. A quantitative analysis focusing on their advantages, disadvantages, forecasting accuracy, and computational costs are also provided. Finally, a few open research issues and trends related to the topic are discussed, which can contribute to improving the understanding of each wind power forecasting method. In general, this review paper provides valuable insights to wind power engineers.}
}
@article{WANG2021127042,
title = {Using sustainable performance prediction in data-scarce scenarios: A study of park-level integrated microgrid projects in Tianjin, China},
journal = {Journal of Cleaner Production},
volume = {304},
pages = {127042},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.127042},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621012610},
author = {Yuan Wang and Shuquan Li and Xiuyu Wu and Yan Zhang and Baoluo Li and Lei Gao},
keywords = {Integrated energy system, Sustainable performance prediction, Conditional generative adversarial networks, Long short-term memory, Park level integrated energy system},
abstract = {Time series data of project performance in park-level integrated energy system projects are non-linear, difficult to collect and store, and scarce. Thus, it is difficult to carry out real-time prediction of project performance. In the context of energy and environmental crises, a real-time prediction method for the sustainable development performance of IES projects based on conditional generative adversarial networks-long short-term memory neural networks was proposed after a full study of the park-level IES projects in Tianjin, China. In this study, an evaluation system for the sustainable development performance of IES projects, such as “integrated energy efficiency,” was established to collect the monthly performance index values of 638 IES projects in Tianjin in 2017. The monthly performance evaluation index was calculated using the entropy weight method. After sorting, a binary method was applied to form the monthly performance evaluation label values,"1″ corresponding to the top 50% of the project, “0″ corresponding to the bottom 50% projects, establishing a database of historical project performance. The generator in CGAN game training was initially used to learn the mapping relationship between the noise distribution under the predicted conditions and the historical IES project performance data set, resulting in 6220 project data with similar distribution, with improved generalization ability of online data mining and accuracy of the stabilization algorithm. LSTM was then used to capture the time dependence in IES project performance data characteristics to predict monthly sustainable performance after 12 months of project operation. Compared with other machine learning models, this method is time-adaptive and the model structure is simple. The average response time of performance prediction for the same park was shortened to 2.76 months, and the prediction accuracy increased to 98.75%. Three schemes were designed to verify the effectiveness of the proposed method by comparing the real data of the park-level IES project in Tianjin with the predicted results. These results have practical significance for strengthening the real-time control of integrated energy projects and for effectively promoting the sustainable development of the integrated energy industry.}
}
@article{STINGONE2021111019,
title = {Interdisciplinary data science to advance environmental health research and improve birth outcomes},
journal = {Environmental Research},
volume = {197},
pages = {111019},
year = {2021},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2021.111019},
url = {https://www.sciencedirect.com/science/article/pii/S0013935121003133},
author = {Jeanette A. Stingone and Sofia Triantafillou and Alexandra Larsen and Jay P. Kitt and Gary M. Shaw and Judit Marsillach},
keywords = {Preterm birth, Environmental mixtures, Multiple exposures, Public health data science},
abstract = {Rates of preterm birth and low birthweight continue to rise in the United States and pose a significant public health problem. Although a variety of environmental exposures are known to contribute to these and other adverse birth outcomes, there has been a limited success in developing policies to prevent these outcomes. A better characterization of the complexities between multiple exposures and their biological responses can provide the evidence needed to inform public health policy and strengthen preventative population-level interventions. In order to achieve this, we encourage the establishment of an interdisciplinary data science framework that integrates epidemiology, toxicology and bioinformatics with biomarker-based research to better define how population-level exposures contribute to these adverse birth outcomes. The proposed interdisciplinary research framework would 1) facilitate data-driven analyses using existing data from health registries and environmental monitoring programs; 2) develop novel algorithms with the ability to predict which exposures are driving, in this case, adverse birth outcomes in the context of simultaneous exposures; and 3) refine biomarker-based research, ultimately leading to new policies and interventions to reduce the incidence of adverse birth outcomes.}
}
@article{DEVILLIERS2021598,
title = {A (new) role for business – Promoting the United Nations’ Sustainable Development Goals through the internet-of-things and blockchain technology},
journal = {Journal of Business Research},
volume = {131},
pages = {598-609},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.11.066},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320308262},
author = {Charl {de Villiers} and Sanjaya Kuruppu and Dinithi Dissanayake},
keywords = {Internet-of-things, Blockchain, Sustainable development goals, Innovation},
abstract = {We outline the business opportunity for the provision of measurement technology, linked to the internet, i.e. the internet-of-things (IoT), which feeds information into blockchains, providing reliable and trusted data and an incentive for others to contribute towards progress on the United Nations’ Sustainable Development Goals (SDGs). Both existing businesses and start-ups could exploit these new opportunities, which could inspire the participation of employees, volunteers, donors, and other participants. We provide a conceptual framework for the different ways business can play a role in facilitating measurement of SDGs, and trust in these measurements, by harnessing technology.}
}
@article{ZHANG2023108919,
title = {An interpretable knowledge-based decision support method for ship collision avoidance using AIS data},
journal = {Reliability Engineering & System Safety},
volume = {230},
pages = {108919},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108919},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022005348},
author = {Jinfen Zhang and Jiongjiong Liu and Spyros Hirdaris and Mingyang Zhang and Wuliu Tian},
keywords = {Automatic identification system (AIS), Ship collision avoidance behavior, Scenario similarity measurement, Trajectory fusion, Collision avoidance path planning},
abstract = {AIS data include ship spatial-temporal and motion parameters which can be used to excavate the deep-seated information. In this article, an interpretable knowledge-based decision support method is established to guide the ship to make collision avoidance decisions with good seamanship and ordinary practice of seamen using AIS data. First, AIS data is preprocessed and trajectory reconstructed to restore the ship historical navigation state, and a ship encounter identification model is constructed according to the encounter characteristics; Second, a two-stage collision avoidance behavior extraction algorithm is formed to build a behavior knowledge base, and the scenario similarity model is constructed to measure and match similar scenarios based on ship position, motion tendency and collision risk. Then, the Delaunay Triangulation Network is used to fuse ship trajectories of similar scenario to form the collision avoidance path. Finally, a case study is performed using the real AIS data outside Ningbo-Zhoushan Port waters, China, and the effectiveness of the planned path is verified by setting the head-on and crossing situations and comparison between the planned and real paths. Results indicate that the proposed model can extract the ship collision avoidance behavior accurately, and the planned path can ensure navigation safety.}
}
@incollection{KARADUZOVICHADZIABDIC2021327,
title = {Chapter 15 - Artificial intelligence in clinical decision-making for diagnosis of cardiovascular disease using epigenetics mechanisms},
editor = {Yvan Devaux and Emma Louise Robinson},
booktitle = {Epigenetics in Cardiovascular Disease},
publisher = {Academic Press},
pages = {327-345},
year = {2021},
volume = {24},
series = {Translational Epigenetics},
isbn = {978-0-12-822258-4},
doi = {https://doi.org/10.1016/B978-0-12-822258-4.00020-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222584000201},
author = {Kanita Karađuzović-Hadžiabdić and Antje Peters},
keywords = {Artificial intelligence, Machine learning, Computational biology, Data mining, Cardiovascular disease, Epigenetics},
abstract = {This chapter provides an overview of machine learning, a mainstream discipline of artificial intelligence. Machine learning is discussed in the context of medical research in general and epigenetics research in cardiology and cardiovascular research in particular. The chapter begins with an overview of machine learning concepts. Main stages of the machine learning workflow including the description of the most popular machine learning techniques used in cardiovascular medicine are presented. In order to reflect the importance of machine learning in biomedical research, selected machine learning applications for disease prediction and diagnosis are reviewed.}
}
@article{GANDHI2023424,
title = {Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions},
journal = {Information Fusion},
volume = {91},
pages = {424-444},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001634},
author = {Ankita Gandhi and Kinjal Adhvaryu and Soujanya Poria and Erik Cambria and Amir Hussain},
keywords = {Affective computing, Sentiment analysis, Multimodal fusion, Fusion techniques},
abstract = {Sentiment analysis (SA) has gained much traction In the field of artificial intelligence (AI) and natural language processing (NLP). There is growing demand to automate analysis of user sentiment towards products or services. Opinions are increasingly being shared online in the form of videos rather than text alone. This has led to SA using multiple modalities, termed Multimodal Sentiment Analysis (MSA), becoming an important research area. MSA utilises latest advancements in machine learning and deep learning at various stages including for multimodal feature extraction and fusion and sentiment polarity detection, with aims to minimize error rate and improve performance. This survey paper examines primary taxonomy and newly released multimodal fusion architectures. Recent developments in MSA architectures are divided into ten categories, namely early fusion, late fusion, hybrid fusion, model-level fusion, tensor fusion, hierarchical fusion, bi-modal fusion, attention-based fusion, quantum-based fusion and word-level fusion. A comparison of several architectural evolutions in terms of MSA fusion categories and their relative strengths and limitations are presented. Finally, a number of interdisciplinary applications and future research directions are proposed.}
}
@article{GRAHAM2021113061,
title = {Use of spatio-temporal habitat suitability modelling to prioritise areas for common carp biocontrol in Australia using the virus CyHV-3},
journal = {Journal of Environmental Management},
volume = {295},
pages = {113061},
year = {2021},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.113061},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721011233},
author = {K. Graham and D. Gilligan and P. Brown and R.D. {van Klinken} and K.A. McColl and P.A. Durr},
keywords = {Bayesian networks, Damage thresholds, Expert opinion, Invasive freshwater fish, Murray-Darling basin, Vertebrate pest species},
abstract = {Common carp (Cyprinus carpio) are an invasive species of the rivers and waterways of south-eastern Australia, implicated in the serious decline of many native fish species. Over the past 50 years a variety of control options have been explored, all of which to date have proved either ineffective or cost prohibitive. Most recently the use of cyprinid herpesvirus 3 (CyHV-3) has been proposed as a biocontrol agent, but to assess the risks and benefits of this, as well as to develop a strategy for the release of the virus, a knowledge of the fundamental processes driving carp distribution and abundance is required. To this end, we developed a novel process-based modelling framework that integrates expert opinion with spatio-temporal datasets via the construction of a Bayesian Network. The resulting weekly networks thus enabled an estimate of the habitat suitability for carp across a range of hydrological habitats in south-eastern Australia, covering five diverse catchment areas encompassing in total a drainage area of 132,129 km2 over a period of 17–27 years. This showed that while suitability for adult and subadult carp was medium-high across most habitats throughout the period, nevertheless the majority of habitats were poorly suited for the recruitment of larvae and young-of-year (YOY). Instead, high population abundance was confirmed to depend on a small number of recruitment hotspots which occur in years of favourable inundation. Quantification of the underlying ecological drivers of carp abundance thus makes possible detailed planning by focusing on critical weaknesses in the population biology of carp. More specifically, it permits the rational planning for population reduction using the biocontrol agent, CyHV-3, targeting areas where the total population density is above a “damage threshold” of approximately 100 kg/ha.}
}
@article{LUO202183,
title = {A multi-task deep learning model for short-term taxi demand forecasting considering spatiotemporal dependences},
journal = {Journal of Traffic and Transportation Engineering (English Edition)},
volume = {8},
number = {1},
pages = {83-94},
year = {2021},
issn = {2095-7564},
doi = {https://doi.org/10.1016/j.jtte.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S209575641830521X},
author = {Huimin Luo and Jianming Cai and Kunpeng Zhang and Ruihang Xie and Liang Zheng},
keywords = {Traffic engineering, Short-term traffic prediction, Deep learning, Multi-task model, Spatiotemporal dependences},
abstract = {Short-term taxi demand forecasting is of great importance to incentivize vacant cars moving from over-supply regions to over-demand regions, which can minimize the wait time for passengers and drivers. With the consideration of spatiotemporal dependences, this study proposes a multi-task deep learning (MTDL) model to predict short-term taxi demand in multi-zone level. The nonlinear Granger causality test is applied to explore the causality relationships among various traffic zones, and long short-term memory (LSTM) is used as the core neural unit to construct the framework of the multi-task deep learning model. In addition, several hyperparameter optimization methods (e.g., grid search, random search, Bayesian optimization, hyperopt) are used to tune the model. Using the taxi trip data in New York City for validation, the multi-task deep learning model considering spatiotemporal dependences (MTDL∗) is compared with the single-task deep learning model (STDL), the full-connected multi-task deep learning model (MTDL#) and other benchmark algorithms (such as LSTM, support vector machine (SVM) and k-nearest neighbors (k-NN)). The experiment results show that the proposed MTDL model is promising to predict short-term taxi demand in multi-zone level, the nonlinear Granger causality analysis is able to capture the spatiotemporal correlations among various traffic zones, and the Bayesian optimization is superior to the other three methods, which verified the feasibility and adaptability of the proposed method.}
}
@article{CRAMER2021586,
title = {Towards a flexible process-independent meta-model for production data},
journal = {Procedia CIRP},
volume = {99},
pages = {586-591},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.112},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004157},
author = {Simon Cramer and Max Hoffmann and Peter Schlegel and Marco Kemmerling and Robert H. Schmitt},
keywords = {Process data, Meta-model, Predictive quality, Smart production, Production data, Digital shadow, Data analytics},
abstract = {Data integration is a considerable challenge when investigating information sources from a multi-step manufacturing process. The interpretation of the process data profoundly depends on the incurred meta-data. However, during most data aggregating processes along the production chain, accompanying meta-information of vital importance is lost. To address this shortcoming, we propose a flexible and process-independent meta-model for efficient data integration for multi-step manufacturing processes. The product-oriented model unites process- and meta-data to reflect their mutual relationships within the manufacturing process. The context provided by the meta-information enables automatic data analysis for Predictive Quality applications in a cross-company setting.}
}
@article{WANG2023103694,
title = {An intelligent blockchain-based access control framework with federated learning for genome-wide association studies},
journal = {Computer Standards & Interfaces},
volume = {84},
pages = {103694},
year = {2023},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2022.103694},
url = {https://www.sciencedirect.com/science/article/pii/S0920548922000617},
author = {Huanhuan Wang and Xiao Zhang and Youbing Xia and Xiang Wu},
keywords = {Automated quality control, Federated learning, Differential privacy, Blockchain},
abstract = {Genome-Wide Association Studies (GWAS) aim to find various variations in human disorders and have become one of the most commonly-used methods to find the pathogenesis and genetic mechanisms of complex diseases. However, the GWAS process needs to frequently search the genome-wide data, especially in the calculation process of multi-party participation. The statistical value calculation and interactive search of Single Nucleotide Polymorphisms (SNPs) and model training processes might easily disclose personal information. Therefore, to solve these problems, we propose a Blockchain-based access control Framework for GWAS with Federated Learning-BFGF. Specifically, before training local models, this framework implements Automated Quality Control (AQC) to guarantee the quality of training data. Design the authentication mechanism in blockchain to filter out users who are malicious attackers to protect the security of other users' information initially. To accelerate the speed of cloud model training and resist multiple attacks in federated learning, propose a periodic aggregation method combining differential privacy mechanisms. Finally, simulated experiments have shown that the BFGF framework can protect the security of genetic data and balance availability and accuracy.}
}
@article{AHAMMAD2021102292,
title = {QoS Performance Enhancement Policy through Combining Fog and SDN},
journal = {Simulation Modelling Practice and Theory},
volume = {109},
pages = {102292},
year = {2021},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2021.102292},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X21000216},
author = {Ishtiaq Ahammad and Md. Ashikur Rahman Khan and Zayed Us Salehin},
keywords = {Internet of Things, Software-Defined Networking, Fog Computing, Quality of Service, Modeling and Simulation, iFogSim Simulator},
abstract = {The goal of Internet of Things (IoT) is to bring any object online, thereby creating a massive volume of data that can overwhelm the existing computing and networking technologies. Therefore, centered cloud isn't ideal for rapidly expanding IoT environmental requirements. Fog computing (FC) moves some portion of the computing load (related to real-time services) from the cloud into edge fog devices. FC is expected to become the subsequent major computing transition and this one has ability to overcome existing cloud limitations. However the key obstacles facing FC are: wide distribution, isolated coupling, quality-of-service (QoS) regulation, adaptability to conditions, and particularly the standardization and normalization is still in phase of development. Software defined networking (SDN) will help fog to solve these obstacles. SDN means unified network control plane (which is separated from data plane), allowing the introduction for advanced traffic control and the orchestration mechanisms of networks and resources. On the grounds of SDN concept, and then combining it with FC, the network type can be modified to resolve all those cloud drawbacks and improve IoT system's QoS. Within this paper, architecture is developed through the combination of independently researched areas of SDN and FC to enhance the QoS in an IoT system. An algorithm (which is dependent on partition the SDN virtually) is presented to support the architecture whose purpose is to select the optimal access point and optimal place to process the data. The main objective of this algorithm is to provide improved QoS by partitioning the corresponding fog devices through the SDN controller. A use case dependent on the presented architecture and algorithm is then provided and assessed this use case's QoS parameter values (network usage, cost, latency and power consumption) using the iFogSim simulator. In contrast to cloud-only deployment, the result indicates a major enhancement of the mentioned QoS parameter values in the deployment of fog with SDN. In addition, once compared to a relative former identical use case; the findings of this paper show improved results for power consumption, network usage and latency. In fact, when compared to a former identical use case, the outcome of this paper shows around 3 times less latency and 2 times less network usage. Finally the ground (IoMT, Industry 4.0, Green IoT, and 5G) that is influenced by this QoS improvement is broadly illustrated in this paper.}
}
@article{KRELLENBERG2021126986,
title = {What to do in, and what to expect from, urban green spaces – Indicator-based approach to assess cultural ecosystem services},
journal = {Urban Forestry & Urban Greening},
volume = {59},
pages = {126986},
year = {2021},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2021.126986},
url = {https://www.sciencedirect.com/science/article/pii/S161886672100011X},
author = {Kerstin Krellenberg and Martina Artmann and Celina Stanley and Robert Hecht},
keywords = {Demand and supply, Indicator conceptualization and operationalization, Multi-step assessment approach, Open data, Recreational urban ecosystem services, Site-level},
abstract = {Literature on urban ecosystem services (UESS) is vast, particularly on cultural ecosystem services. However, due to a lack of knowledge on individual urban green spaces on the site level, further research on enhanced methods is needed to underpin existing assumptions about the reasons why people are visiting urban green spaces and what kinds of ecosystem services they can expect, with a focus on recreational activities. We argue for enhanced methods to assess supply of and demand on cultural UESS that should include the direct work with urban green space users. With the overall aim of developing a Spatial Decision Support System for visiting urban green spaces, we are applying a set of different quantitative methods to gather information on peoples’ needs and perceptions as well as data on what existing green spaces offer them. We present a two-step approach 1) linking green space criteria with recreational activities (demand-side) based on a linear series of three online surveys and 2) conducting a spatial mapping of urban green space criteria based on activity-driven indicators (supply-side). In the course of exemplified indicators operationalized by using open and local authorities′ geospatial data in an explorative study in Dresden and Heidelberg (Germany), we discuss the strengths and weaknesses of the approach.}
}
@article{PETERSEN20213594,
title = {CellExplorer: A framework for visualizing and characterizing single neurons},
journal = {Neuron},
volume = {109},
number = {22},
pages = {3594-3608.e2},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321006565},
author = {Peter C. Petersen and Joshua H. Siegle and Nicholas A. Steinmetz and Sara Mahallati and György Buzsáki},
keywords = {electrophysiology, extracellular electrodes, framework, graphical interface, standardized processing and data structure, single cell analysis},
abstract = {Summary
The large diversity of neuron types provides the means by which cortical circuits perform complex operations. Neuron can be described by biophysical and molecular characteristics, afferent inputs, and neuron targets. To quantify, visualize, and standardize those features, we developed the open-source, MATLAB-based framework CellExplorer. It consists of three components: a processing module, a flexible data structure, and a powerful graphical interface. The processing module calculates standardized physiological metrics, performs neuron-type classification, finds putative monosynaptic connections, and saves them to a standardized, yet flexible, machine-readable format. The graphical interface makes it possible to explore the computed features at the speed of a mouse click. The framework allows users to process, curate, and relate their data to a growing public collection of neurons. CellExplorer can link genetically identified cell types to physiological properties of neurons collected across laboratories and potentially lead to interlaboratory standards of single-cell metrics.}
}
@article{LIAO2021127132,
title = {A comparison of global and regional open datasets for urban greenspace mapping},
journal = {Urban Forestry & Urban Greening},
volume = {62},
pages = {127132},
year = {2021},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2021.127132},
url = {https://www.sciencedirect.com/science/article/pii/S1618866721001576},
author = {Yiming Liao and Qi Zhou and Xuanqiao Jing},
keywords = {FROM-GLC10, Land-use, Land-cover, OpenStreetMap, Park, Urban atlas, Vegetation},
abstract = {Greenspace has positive influences on urban environment and human health, and thus it is desirable to acquire data for (urban) greenspace mapping. Nowadays, global and regional open land-use/land-cover datasets have become essential sources for greenspace mapping, but few studies have quantitatively compared them. To fill this gap, this study carries out a quantitative comparison of six global and regional open datasets (CGLS-LC100, CLC, GLC30, UA, FROM-GLC10 and OSM) for greenspace mapping. First of all, the most appropriate land-use/land-cover classes selected as greenspace are analyzed for each open dataset; then, different open datasets are evaluated and compared in terms of five measures (accuracy, precision, recall, F1-score and green coverage rate). Five urban areas in UK are chosen as study areas. Two categories of reference datasets are used for evaluation, including an Ordnance Survey (OS) greenspace dataset in UK and a number of sampling points classified by referring to Google Earth. Results show that: the OSM dataset performs the best, while comparing with the OS dataset (characterized by a narrowly interpreted greenspace); and the FROM-GLC10 dataset performs the best, while comparing with the sampling points (characterized by a broadly interpreted greenspace). Moreover, by using these two open datasets, most quantitative results are close to or higher than 80 %, in terms of the accuracy, precision, recall and F1-score; in most cases there also is the smallest difference between using these two open datasets and corresponding reference datasets, in terms of the green coverage rate. These findings have benefits for researchers and planners to choose an appropriate open dataset for greenspace mapping.}
}