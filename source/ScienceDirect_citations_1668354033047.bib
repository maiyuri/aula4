@article{MIRBAGHERI2022101011,
title = {Developing the required data set for the integration of breast cancer registry systems in Iran},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101011},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101011},
url = {https://www.sciencedirect.com/science/article/pii/S235291482200154X},
author = {Esmat Mirbagheri and Mohsen Shafiee and Mostafa Shanbezadeh and Hadi Kazemi-Arpanahi},
keywords = {Breast neoplasms, Registries, Common data elements, Information management, Information systems},
abstract = {Background
Breast cancer is a major public health concern due to its increasing incidence and mortality rates. A large volume of data is generated from different healthcare settings with inconsistent and heterogeneous data frameworks. There is an increasing demand to integrate breast cancer data between various information systems to answer specific research questions and future clinical trials. Thus, this study aimed to develop a minimum data set (MDS) for integrating breast cancer registry systems as a prerequisite for multi-center data exchange and research cooperation.
Methods
The proposed MDS was developed using a multi-stage process. First, a systematic search was performed in scientific databases. Available data sets and registries related to breast cancer were also reviewed until data saturation. Then, a two-round Delphi survey was performed to reach an agreement on the primary data items. Finally, an additional Delphi stage was carried out to validate the content of the final MDS by calculating the individual item content validity index (CVI), overall scale CVI (S-CVI), and face validity.
Results
After the literature review, the primary data set for breast cancer including 309 data items was identified. After the Delphi phase and calculation of I-CVI, S-CVI, and face validity, the breast cancer MDS was finalized with 14 classes and 205 data items.
Conclusions
This agreed-upon MDS enables accurate, consistent, and comparable inter-organizational data collection between breast cancer care centers. This data homogeneity enhances the analytic power and depth of variables, thereby contributing to multicenter, large-scale, and more generalizable epidemiological and predictive studies on breast cancer.}
}
@article{YU2022101698,
title = {How does intelligent manufacturing reconcile the conflict between process standards and technological innovation?},
journal = {Journal of Engineering and Technology Management},
volume = {65},
pages = {101698},
year = {2022},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2022.101698},
url = {https://www.sciencedirect.com/science/article/pii/S0923474822000285},
author = {Kangkang Yu and Cheng Qian and Jinliang Chen},
keywords = {Process standards, Intelligent manufacturing, Technological innovation, Case study},
abstract = {Understanding whether process standards foster or hinder technological innovation lacks consensus in the literatures. From the process management perspective, this study aimes at exploring the mechanisms underlying this relationship under a specific situation of intelligent manufacturing. By analyzing interview data from four case companies in the environmental instrument industry in China and secondary data at the firm level, we find that (i) process standards have a positive effect on technological innovation and (ii) process standards in companies with relatively high levels of intelligent manufacturing raise technological innovation. In particular, these four companies are differentiated with respect to intelligent manufacturing context: two companies with relatively high levels of intelligent manufacturing; and the other two with relatively low levels of intelligent manufacturing. Based on the case analysis, we find that there is an open logic, tightening both internal and external communication linkages, between process standards and technological innovation, so process standards positively affect technological innovation. Further, we find that for those with a higher level of intelligent manufacturing, firms typically benefit more organizational learning competency, which enhances the impact of process standards on technological innovation. The findings are also confirmed by a post hoc survey. Above all, this study contributes to the literatures on process management and technological innovation as well as the new trend of intelligent manufacturing in Industry 4.0.}
}
@article{FLECHSIG2022100718,
title = {Robotic Process Automation in purchasing and supply management: A multiple case study on potentials, barriers, and implementation},
journal = {Journal of Purchasing and Supply Management},
volume = {28},
number = {1},
pages = {100718},
year = {2022},
issn = {1478-4092},
doi = {https://doi.org/10.1016/j.pursup.2021.100718},
url = {https://www.sciencedirect.com/science/article/pii/S1478409221000522},
author = {Christian Flechsig and Franziska Anslinger and Rainer Lasch},
keywords = {Robotic process automation, Digital procurement, Implementation, Barriers, Digital readiness, Public sector},
abstract = {Robotic Process Automation (RPA) has received growing attention within the digital transformation as this cutting-edge technology automates human behavior and promises high potentials. However, the adoption in purchasing and supply management (PSM) is still in its infancy and has hardly been explored, particularly in the public sector. Based on a multiple case study including 19 organizations of the public and private sector, this paper narrows that gap and presents comprehensive insights into potentials, barriers, suitable processes, and best practices and components for RPA implementation. The findings indicate that adoption depends on the organizations’ digital procurement readiness and maturity. Application areas of RPA enlarge with increasing experience and range from transactional and operative tasks within the procure-to-pay process to more strategic use cases in sourcing and supply relationship management. Potentials mainly comprise employee reliefs, cost savings, and increased operational efficiency and quality. We uncover multiple technical, organizational, and environmental barriers related to IT infrastructure and human resources, internal communication, financial resources, top management support, organizational structures, supplier-related issues, and government regulations. Furthermore, our study indicates several differences between the private and public sectors for RPA implementation. We outline implications for the emerging research on RPA and pivotal directions for organizational practice.}
}
@article{KARIM2022101760,
title = {Development of “Biosearch System” for biobank management and storage of disease associated genetic information},
journal = {Journal of King Saud University - Science},
volume = {34},
number = {2},
pages = {101760},
year = {2022},
issn = {1018-3647},
doi = {https://doi.org/10.1016/j.jksus.2021.101760},
url = {https://www.sciencedirect.com/science/article/pii/S1018364721004225},
author = {Sajjad Karim and Mona Al-Kharraz and Zeenat Mirza and Hend Noureldin and Heba Abusamara and Nofe Alganmi and Adnan Merdad and Saddig Jastaniah and Sudhir Kumar and Mahmood Rasool and Adel Abuzenadah and Mohammed Al-Qahtani},
keywords = {Biosearch system, LIMS database, Biobank, Genomics, Microarray, Bioinformatics},
abstract = {Objective
Databases and softwares are important to manage modern high-throughput laboratories and store clinical and genomic information for quality assurance. Commercial softwares are expensive with proprietary code issue while academic versions have adaptation issue. Our aim was to develop an adaptable in-house software that can stores specimen and disease-associated genetic information in biobank to facilitate translational research.
Methods
Prototype was designed as per the research requirements and computational tools were used to develop software under three tiers; Visual Basic and ASP.net for presentation tier, SQL server for data tier, and Ajax and JavaScript for business tier. We retrieved specimens from biobank using this software and performed microarray based transcriptomic analysis to detect differentialy expressed genes (DEGs) with FC ±2 and P-value <0.05 in triple negative breast cancer cases. Ingenuity pathway analysis tool was used to predict canonical molecular pathways associated with disease. Overall performance and utility of software was evaluated by JMeter software, CRUD function test and set of feedback questioners.
Results
We developed “Biosearch System”, a web-based software enabling management of biobank samples (tissue, blood, FTTP slides) and their extracts (DNA, RNA and proteins) with clinical and experimental details. The client satisfaction feedback was excellent with score 4.7/5. We identified a total of 1181 DEGs including both upregulated (IFI6, LEF1, FANCI, CASC5, PLXNA3 etc.) and down-regulated (ADH1B, LYVE1, ADH1C, ADH1B, ADIPOQ, PLIN1, LYVE1 etc.) genes in triple negative breast cancer. Pathway analysis of DEGs revealed significant activation of interferon signaling (z-score 2.646) and kinetochore metaphase signaling pathway (z-score 2.138) in cancer.
Conclusion
Biosearch System is a user friendly LIMS for collection, storage and retrieval of specimen and clinical information. It is secure, efficient, and very convenient in sample tracking and data analysis. We illustrated its utility in transcriptomic study of breast cancer. Additionally, it can facilitate and speed up any genomic study and translational research publications.}
}
@article{RAFFIN2022136,
title = {Qualitative assessment of the impact of manufacturing-specific influences on Machine Learning Operations},
journal = {Procedia CIRP},
volume = {115},
pages = {136-141},
year = {2022},
note = {10th CIRP Global Web Conference – Material Aspects of Manufacturing Processes},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.10.063},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122014986},
author = {Tim Raffin and Tobias Reichenstein and Dennis Klier and Alexander Kühl and Jörg Franke},
keywords = {Machine Learning, Deep Learning, MLOps, IIoT},
abstract = {Machine Learning Operations (MLOps) enables the streamlining of the development and deployment processes of machine learning models; thus, manufacturers can utilize the inherent flexibility and adaptability of Deep Learning at scale to further optimize their processes. This publication provides insights into the challenges that companies face while striving for the efficient operationalization of machine learning algorithms. Moreover, a mapping of capabilities and requirements is presented to provide a baseline for the qualitative analysis of the current state of the art. In conclusion, this article discusses the shortcomings of the existing literature and provides novel implications for MLOps systems in manufacturing.}
}
@article{RASOVSKA20221938,
title = {Learning factory FleXtory: Interactive loops between real and virtual factory through digital twin},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1938-1943},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.682},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322020006},
author = {I. Rasovska and I. Deniaud and F. Marmier and J.-L. Michalak},
keywords = {Industry 4.0, Learning factory architecture, Virtual factory, Digital twin},
abstract = {The digitalization increase in industrial processes is perceived as an opportunity to grow up the competitiveness of companies. Data is more and more accessible, potentially allowing making better decisions at all levels of the company. Job profiles are then changing and requiring new skills, more focused on new technologies and information systems. The most effective way to acquire necessary skills is a “learning by doing” way in industrial projects and processes. The learning factory FleXtory was designed and produced in this objective and at the crossroad of the academic and the industrial environment. It allows running combinations of theoretical and applied tools in the context of industry 4.0. This is based on interactive loops between the real and the virtual world passing through the digital twin of the learning factory. The pedagogical modules developed within this learning factory address the evolution of professional competencies and skills in according to the transition to industry 4.0. This transformation supposes the development of the ability for professionals to work within a digital environment. In this paper we propose an architecture model of FleXtory favoring the return on experience/information loop within the digital transformation of the learning factory. The originality of our work is to consider this architecture from the point of view of the pedagogical specifications of the proposed learning models and the future perspective of their use.}
}
@article{SON2022108879,
title = {Integrated framework for estimating remaining useful lifetime through a deep neural network},
journal = {Applied Soft Computing},
volume = {122},
pages = {108879},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108879},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002587},
author = {Seho Son and Ki-Yong Oh},
keywords = {Deep neural network, Genetic algorithm, Moving-time window, Feature extraction, Feature reasoning, Remaining useful life, Step differential method},
abstract = {This paper proposes an integrated framework for a deep neural network to estimate the remaining useful life (RUL) to ensure the reliability and safety of complex mechanical systems and enable proactive maintenance for intelligent operation. This data-driven method can predict complex and highly nonlinear degradation characteristics that are difficult to predict using physics-based prognostics and health management. In particular, this study focused on feature preprocessing and hyperparameter optimization, whereas previous studies had focused on the neural network architecture to improve prediction accuracy and robustness. The proposed integrated framework comprises four phases: feature preprocessing, feature reasoning using a deep neural network, hyperparameter optimization using a genetic algorithm, and RUL estimation. In the first phase, sensor measurements sensitive to degradation are selected and separated into primary and dynamic degradation trends. In addition, step differential values are extracted to account for multiple operational modes using an unsupervised clustering method. In the second phase, feature reasoning is performed using a deep neural network to characterize hidden complex and highly nonlinear degradation features. The health indicators manipulated in the first phase are trained using the proposed deep neural network. In the third phase, a genetic algorithm is introduced to optimize the hyperparameters used in feature preprocessing and reasoning. The final phase estimates the RUL using the proposed deep neural network with optimized hyperparameters. The proposed method was validated on the C-MAPSS dataset. The results show that the proposed integrated framework outperformed other state-of-the-art machine learning and deep learning methods under different operational conditions, suggesting that efficient feature preprocessing and hyperparameter optimization significantly improve the prediction accuracy and robustness of RUL for data-driven prognostics and health management.}
}
@article{VRAIN2022122051,
title = {The discontinuance of low carbon digital products and services},
journal = {Technological Forecasting and Social Change},
volume = {185},
pages = {122051},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.122051},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522005728},
author = {Emilie Vrain and Charlie Wilson and Barnaby Andrews},
keywords = {Climate change, Diffusion of innovations, Covid-19, Post-adoption, Attributes},
abstract = {Digital consumer innovations offer low-carbon alternatives to mainstream consumption practices. We address a lack of research on the factors influencing post-adoption decisions of discontinuance for this important class of innovations. We conducted a repeat survey with UK consumers (n = 995) in 2019 and 2020 to investigate 16 digital products and services across mobility, food, homes, and energy domains. Our survey captured temporal changes in adoption, personal and contextual characteristics, social influences, innovation experiences and perceived attributes. We also provide a unique contribution by assessing the impacts of Covid-19 on post-adoption processes. Our results indicate that discontinuance is associated with: 1) services more than products; 2) perceived functional attributes not met by experienced attributes; 3) a lack of positive social influence, including word-of-mouth; 4) a lack of social network connections to other adopters; and 5) a decline in an individual's financial situation. Covid-19 was not found to be a significant factor influencing innovation discontinuance. Findings highlight generalisable insights regarding issues that need addressing to overcome discontinuance. For example, while digital services offer low-carbon promise, continued adoption is sensitive to their strong performance attributes. There is a need for continued innovation to sustain market position relative to more familiar incumbents.}
}
@article{WANG2022102759,
title = {Business Innovation based on artificial intelligence and Blockchain technology},
journal = {Information Processing & Management},
volume = {59},
number = {1},
pages = {102759},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102759},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002405},
author = {Zeyu Wang and Mingyu Li and Jia Lu and Xin Cheng},
keywords = {Artificial intelligence, Blockchain, business},
abstract = {The growing business evolution and the latest Artificial Intelligence (AI) make the different business practices to be enhanced by the ability to create new means of collaboration. Such growing technology helps to deliver brand services and even some new kinds of corporate interactions with customers and staff. AI digitization simultaneously emphasized businesses to focus on the existing strategies and regularly and early pursue new market opportunities. While digital technology research in the framework of business innovation is gaining greater interest and the privacy of data can be maintained by Blockchain technology. Therefore in this paper, Business Innovation based on artificial intelligence and Blockchain technology (BI-AIBT) has been proposed to enhance the business practices and maintain the secured interaction among the various clients. The collection of qualitative empirical data is made up of few primary respondents from two distinct business sectors. BI-AIBT has been evaluated by undertaking and exploring the difference and similarities between digitalization's impact on value development, proposal, and business capture. Besides, organizational capacities and staff skills interaction issues can be improved by BT. The experimental result suggests that digital transformation is usually regarded as essential and improves business innovation strategies. The numerical result proposed BI-AIBT improves the demand prediction ratio (97.1%), product quality ratio (98.3%), Business development ratio (98.9%), customer behavior analysis ratio (96.3%), and customer satisfaction ratio (97.2%).}
}
@article{YIM2022118835,
title = {Rise and fall of lung cancers in relation to tobacco smoking and air pollution: A global trend analysis from 1990 to 2012},
journal = {Atmospheric Environment},
volume = {269},
pages = {118835},
year = {2022},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2021.118835},
url = {https://www.sciencedirect.com/science/article/pii/S1352231021006579},
author = {Steve H.L. Yim and T. Huang and Jason M.W. Ho and Amy S.M. Lam and Sarah T.Y. Yau and Thomas W.H. Yuen and G.H. Dong and Kelvin K.F. Tsoi and Joseph J.Y. Sung},
keywords = {Air quality, Lung cancer, Smoking, Lung adenocarcinoma, Lung squamous cell carcinoma},
abstract = {Lung cancer remains the leading cancer in incidence and mortality in both genders and most countries. Global cancer statistics show a declining trend of lung squamous cell carcinoma (LSCC) but an uprising trend of lung adenocarcinoma (LADC). The reasons behind their opposite trends are unclear. This study aims to analyze the global trends of LSCC and LADC during 1990–2012 in relation to tobacco consumption and air pollution. Results show a 1% decline of smoking prevalence of 7 years ago is associated with a 9% (95% conﬁdence interval: 8%, 10%) drop in the LSCC incidence globally, whereas a 0.1 μg/m3 increment of BC of 7 years ago is associated with a 12% (9%, 16%) increase in LADC incidence globally. Association between BC and LSCC (or LADC) is more prominent in females, with a 14% (7%, 20%) increase in LSCC [or 14% (11%, 19%) increase in LADC] incidence for a 0.1 μg/m3 increment of BC of 8 (or 6) years ago. Associations vary with different genders across different continents. For instance, concentration of BC is positively associated with incidence of both LSCC and LADC in Europe and North America, whereas concentration of sulfate is positively associated with LSCC incidence in Europe and Oceania, and with LADC incidence in Asia, Oceania and South America. We conclude global decreasing LSCC incidence is associated with the reduced tobacco consumption, whereas the global increasing LADC incidence is likely associated with air pollution. Various particulate species have divergent effects on LADC incidence in different continents.}
}
@incollection{ALEMANY202297,
title = {5 - Data management and processing of 3D body scans},
editor = {Norsaadah Zakaria},
booktitle = {Digital Manufacturing Technology for Sustainable Anthropometric Apparel},
publisher = {Woodhead Publishing},
pages = {97-116},
year = {2022},
series = {The Textile Institute Book Series},
isbn = {978-0-12-823969-8},
doi = {https://doi.org/10.1016/B978-0-12-823969-8.00007-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239698000071},
author = {Sandra Alemany and Alfredo Remon and Alfredo Ballester and Juan {Vicente Durá} and Beatriz Nácher and Eduardo Parrilla and Juan {Carlos González}},
keywords = {Anthropometry, 3D body scanning, 3D body avatar, 3D body model, anthropometry standardization, 3D body template, 3D body processing, rigging, data-driven models, shape analysis, fitting, sizing tables, mannequins},
abstract = {Since the appearance of 3D body scanning technology until today, the progress of research has delivered several solutions to be offered to the customers’ clothing needs as to what fits better the body sizes and shapes. These solutions range from sizing tables and 3D body mannequins that may improve the garment development process to new mobile apps that scan the body using a smartphone and provide to the user new services of size selection or customization. All this progress is underpinned by databases of body measurements or 3D body scans and reveals the importance of the access to this data. This chapter includes a review of the value chain of 3D body data: processing, 3D body model creation, standardization, and anonymization described as key elements to build a future sustainable ecosystem used to grow and update 3D body databases.}
}
@article{DAVIDSON2022126,
title = {The crossroads of digital phenotyping},
journal = {General Hospital Psychiatry},
volume = {74},
pages = {126-132},
year = {2022},
issn = {0163-8343},
doi = {https://doi.org/10.1016/j.genhosppsych.2020.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0163834320301614},
author = {Brittany I. Davidson},
abstract = {The term ‘Digital Phenotyping’ has started to appear with increasing regularity in medical research, especially within psychiatry. This aims to bring together digital traces (e.g., from smartphones), medical data (e.g., electronic health records), and lived experiences (e.g., daily activity, location, social contact), to better monitor, intervene, and diagnose various psychiatric conditions. However, is this notion any different from digital traces or the quantified self? While digital phenotyping has the potential to transform and revolutionize medicine as we know it; there are a number of challenges that must be addressed if research is to blossom. At present, these issues include; (1) methodological issues, for example, the lack of clear theoretical links between digital markers (e.g., battery life, interactions with smartphones) and condition relapses, (2) the current tools being employed, where they typically have a number of security or privacy issues, and are invasive by nature, (3) analytical methods and approaches, where I question whether research should start in larger-scale epidemiological scale or in smaller (and potentially highly vulnerable) patient populations as is the current norm, (4) the current lack of security and privacy regulation adherence of apps used, and finally, (5) how do such technologies become integrated into various healthcare systems? This aims to provide deep insight into how the Digital Phenotyping could provide huge promise if we critically reflect now and gather clinical insights with a number of other disciplines such as epidemiology, computer- and the social sciences to move forward.}
}
@article{LEI2022288,
title = {Will tourists take mobile travel advice? Examining the personalization-privacy paradox},
journal = {Journal of Hospitality and Tourism Management},
volume = {50},
pages = {288-297},
year = {2022},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2022.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1447677022000195},
author = {Soey Sut Ieng Lei and Irene Cheng Chu Chan and Jingyi Tang and Shun Ye},
keywords = {Experiment, Mobile travel advice, Perceived personalization, Personalization-privacy paradox, Personalization strategy},
abstract = {Recognizing the penetration of mobile devices and its transformational impact on travel behavior, tourism and hospitality businesses have been investing on various mobile initiatives to connect with travelers, hoping to influence their decision-making behaviors at different stages throughout the trip. One of these strategies is to provide personalized contents that are more attractive and persuasive. As research regarding the impact of mobile-driven personalization practices on travel behavior is limited, this study investigates the factors affecting travelers' adoption of personalized mobile travel advice. Based on personalization-privacy paradox and self-referencing effect, a 2 (self-reference: high vs. low) × 2 (relevance: high vs. low) between-subjects experiment was carried out. The results demonstrate the mechanism underlying the effects of personalization cues (self-reference and content relevance) on travelers’ intention to adopt personalized mobile travel advice. The competitive mediating roles of perceived personalization and privacy concern on the relationship between personalization cues and adoption intention are highlighted.}
}
@article{WU2022108456,
title = {Consortium blockchain-enabled smart ESG reporting platform with token-based incentives for corporate crowdsensing},
journal = {Computers & Industrial Engineering},
volume = {172},
pages = {108456},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108456},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222004909},
author = {Wei Wu and Yelin Fu and Zicheng Wang and Xinlai Liu and Yuxiang Niu and Bing Li and George Q. Huang},
keywords = {ESG reporting, Consortium blockchain, Internet of Things, Incentive mechanism, Corporate crowdsensing, Shapley value},
abstract = {Environmental, social and governance (ESG) issues arouse wide concern in both industry and academia to promote sustainable development. Listed companies assume the responsibility to submit annual reports to disclose their ESG outcomes genuinely. However, the challenge is that companies may overstate or slur their actual sustainable performance, and the entire ESG reporting process is hidden backward, thereby making the report untrustworthy. Accordingly, this paper proposes an architecture of smart ESG reporting platform leveraging the Internet of Things (IoT) and blockchain technologies to enable corporate crowdsensing for environmental data and enhance the security, transparency and creditability of ESG reporting process. In addition, with the aim of motivating firms to upload massive ESG raw data of high quality, we devise an incentive mechanism that grants them crypto tokens as a reputation for sustainable performance disclosure, exposed to industry and investors for reference. The maximum tokens settlement for each environmental key performance indicator (KPI) is regarded as a cooperative game for premiums, and Shapley value is applied to fairly distribute the tokens in line with the disclosure significance scored by experienced investors. Here, we take the ESG reporting guide issued by the Hong Kong exchange (HKEX) and the practice in the Hong Kong apparel industry as the context. An experimental simulation is conducted to illustrate the feasibility and effectiveness of the proposed platform architecture and approach of token allocation. This study holds the promise of providing a precedent for adopting the advanced technologies to address the greenwashing in firms and actualize intelligent and trustable ESG reporting.}
}
@article{MIKLAUTSCH20221483,
title = {Harmonizing “Smart” Life Cycle Assessment in Manufacturing Companies: Literature Review and Preliminary Morphological Analysis},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1483-1490},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.600},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322019097},
author = {Philipp Miklautsch and Mario Hoffelner and Manuel Woschank},
keywords = {Life cycle assessment/analysis, Industry 4.0, Smart LCA, Manufacturing Enterprises},
abstract = {One of the major challenges for manufacturing companies is finding opportunities to implement green and transparent operational processes to meet a set of macro-economic targets which are defined in strategic initiatives such as the 2030 Agenda for Sustainable Development, the European Green Deal, and the Paris Agreement. Although, in this context, Life Cycle Assessment (LCA) has become established as a common method for assessing the environmental impact of products or processes, in practice problems often arise due to a large amount of data and information to be processed and the resources required. In this regard, recent Industry 4.0 technologies can be seen as a potential enabler of an automated “smart” life cycle assessment in manufacturing companies. Therefore, this paper systematically reviews the current literature on “smart” LCA implementations and presents a comprised overview of the generic properties of “smart” LCA implementations using a general morphological analysis. The research results of this paper can be used for the further development of a “smart” LCA for science and practical applications, as well.}
}
@article{JASIULEWICZKACZMAREK2022223,
title = {Assessing the Barriers to Industry 4.0 Implementation From a Maintenance Management Perspective - Pilot Study Results},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {223-228},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.197},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001987},
author = {Malgorzata Jasiulewicz-Kaczmarek and Katarzyna Antosz and Chao Zhang and Robert Waszkowski},
keywords = {Industry 4.0 technologies, Maintenance 4.0, barriers I4.0 technologies implementation},
abstract = {The purpose of this paper is to identify the barriers of I4.0 technologies implementation in maintenance. Based on literature analysis twenty two barriers of I4.0 technologies implementation are determined and classified into five main groups such as “Strategy and Organization”; “Maintenance staff knowledge and training”; “Resources”; “Technology and infrastructure” and “Security and confidentiality”. Experts’ opinions were taken to finalize the identified barriers. The data for the study were collected from academia experts and have been further analyzed. The results show that from twenty two identified I4.0 technologies implementation barriers only nine are the most representative ones.}
}
@article{PRITCHARD2022100282,
title = {Monitoring populations at increased risk for SARS-CoV-2 infection in the community using population-level demographic and behavioural surveillance},
journal = {The Lancet Regional Health - Europe},
volume = {13},
pages = {100282},
year = {2022},
issn = {2666-7762},
doi = {https://doi.org/10.1016/j.lanepe.2021.100282},
url = {https://www.sciencedirect.com/science/article/pii/S2666776221002684},
author = {Emma Pritchard and Joel Jones and Karina-Doris Vihta and Nicole Stoesser and Prof Philippa C. Matthews and David W. Eyre and Thomas House and John I Bell and Prof John N Newton and Jeremy Farrar and Prof Derrick Crook and Susan Hopkins and Duncan Cook and Emma Rourke and Ruth Studley and Prof Ian Diamond and Prof Tim Peto and Koen B. Pouwels and Prof A. Sarah Walker},
keywords = {SARS-CoV-2, community, monitoring},
abstract = {Summary
Background
The COVID-19 pandemic is rapidly evolving, with emerging variants and fluctuating control policies. Real-time population screening and identification of groups in whom positivity is highest could help monitor spread and inform public health messaging and strategy.
Methods
To develop a real-time screening process, we included results from nose and throat swabs and questionnaires taken 19 July 2020-17 July 2021 in the UK's national COVID-19 Infection Survey. Fortnightly, associations between SARS-CoV-2 positivity and 60 demographic and behavioural characteristics were estimated using logistic regression models adjusted for potential confounders, considering multiple testing, collinearity, and reverse causality.
Findings
Of 4,091,537 RT-PCR results from 482,677 individuals, 29,903 (0·73%) were positive. As positivity rose September-November 2020, rates were independently higher in younger ages, and those living in Northern England, major urban conurbations, more deprived areas, and larger households. Rates were also higher in those returning from abroad, and working in healthcare or outside of home. When positivity peaked December 2020-January 2021 (Alpha), high positivity shifted to southern geographical regions. With national vaccine roll-out from December 2020, positivity reduced in vaccinated individuals. Associations attenuated as rates decreased between February-May 2021. Rising positivity rates in June-July 2021 (Delta) were independently higher in younger, male, and unvaccinated groups. Few factors were consistently associated with positivity. 25/45 (56%) confirmed associations would have been detected later using 28-day rather than 14-day periods.
Interpretation
Population-level demographic and behavioural surveillance can be a valuable tool in identifying the varying characteristics driving current SARS-CoV-2 positivity, allowing monitoring to inform public health policy.
Funding
Department of Health and Social Care (UK), Welsh Government, Department of Health (on behalf of the Northern Ireland Government), Scottish Government, National Institute for Health Research.}
}
@article{ULRICH2022907,
title = {A Literature Review on the Impact of Modern Technologies on Management Reporting},
journal = {Procedia Computer Science},
volume = {207},
pages = {907-915},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.146},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010286},
author = {Patrick ULRICH and Vanessa FRANK and Ricardo BUETTNER and Wolfgang BECKER},
keywords = {digitization, IT tools, management accounting, management reporting, role theoryIntroduction},
abstract = {In this paper, two systematic literature searches are used to pursue questions about digitization in management reporting on the one hand and role-specific questions on the other. We assume that greater digitization in terms of efficiency and effectiveness will change the role of management accountants in general and in reporting in particular, as this should lead to more automation and possibly more time for consulting activities from management accountants/controllers. After a strictly documented filtering process, a total of 51 papers remained for the analysis of the research questions posed. As a result, in addition to research gaps in management reporting, the most important information technology (IT) tools in management accounting and management reporting can be identified. Concerning corporate performance and management accounting performance, the research also reveals a connection with digitization/IT trends in controlling, controller roles, and role conflicts. The systematic literature evaluations serve as a basis for further research to verify the possible causal relationships found.}
}
@article{WANG2022116236,
title = {Multi-classification assessment of bank personal credit risk based on multi-source information fusion},
journal = {Expert Systems with Applications},
volume = {191},
pages = {116236},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116236},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015475},
author = {Tianhui Wang and Renjing Liu and Guohua Qi},
keywords = {Personal credit risk, Multi-classification assessment, Information fusion, D-S evidence theory},
abstract = {There have been many studies on machine learning and data mining algorithms to improve the effect of credit risk assessment. However, there are few methods that can meet its universal and efficient characteristics. This paper proposes a new multi-classification assessment model of personal credit risk based on the theory of information fusion (MIFCA) by using six machine learning algorithms. The MIFCA model can simultaneously integrate the advantages of multiple classifiers and reduce the interference of uncertain information. In order to verify the MIFCA model, dataset collected from a real data set of commercial bank in China. Experimental results show that MIFCA model has two outstanding points in various assessment criteria. One is that it has higher accuracy for multi-classification assessment, and the other is that it is suitable for various risk assessments and has universal applicability. In addition, the results of this research can also provide references for banks and other financial institutions to strengthen their risk prevention and control capabilities, improve their credit risk identification capabilities, and avoid financial losses.}
}
@article{FRAISL202281,
title = {Demonstrating the potential of Picture Pile as a citizen science tool for SDG monitoring},
journal = {Environmental Science & Policy},
volume = {128},
pages = {81-93},
year = {2022},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121003208},
author = {D. Fraisl and L. See and T. Sturn and S. MacFeely and A. Bowser and J. Campbell and I. Moorthy and O. Danylo and I. McCallum and S. Fritz},
keywords = {Citizen science, Crowdsourcing, SDGs, Citizen science tools, SDG monitoring, Earth Observation, Sustainable Development Goals},
abstract = {The SDGs are a universal agenda to address the world’s most pressing societal, environmental and economic challenges. The supply of timely, relevant and reliable data is essential in guiding policies and decisions for successful implementation of the SDGs. Yet official statistics cannot provide all of the data needed to populate the SDG indicator framework. Citizen science offers a novel solution and an untapped opportunity to complement traditional sources of data, such as household surveys, for monitoring progress towards the SDGs, while at the same time mobilizing action and raising awareness for their achievement. This paper presents the potential offered by one specific citizen science tool, Picture Pile, to complement and enhance official statistics to monitor several SDGs and targets. Designed to be a generic and flexible tool, Picture Pile is a web-based and mobile application for ingesting imagery from satellites, orthophotos, unmanned aerial vehicles or geotagged photographs that can then be rapidly classified by volunteers. The results show that Picture Pile could contribute to the monitoring of fifteen SDG indicators under goals 1, 2, 11, 13, 14 and 15 based on the Picture Pile campaigns undertaken to date. Picture Pile could also be modified to support other SDGs and indicators in the areas of ecosystem health, eutrophication and built-up areas, among others. In order to leverage this particular tool for SDG monitoring, its potential must be showcased through the development of use cases in collaboration with governments, NSOs and relevant custodian agencies. Additionally, mutual trust needs to be built among key stakeholders to agree on common goals that would facilitate the use of Picture Pile or other citizen science tools and data for SDG monitoring and impact.}
}
@article{SILERYTE2022131767,
title = {European Waste Statistics data for a Circular Economy Monitor: Opportunities and limitations from the Amsterdam Metropolitan Region},
journal = {Journal of Cleaner Production},
volume = {358},
pages = {131767},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.131767},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622013786},
author = {Rusne Sileryte and Arnout Sabbe and Vasileios Bouzas and Kozmo Meister and Alexander Wandl and Arjan {van Timmeren}},
keywords = {Circular Economy Monitor, European Waste Statistics, Amsterdam Metropolitan Region, Circular Economy Action Plan, Waste mapping},
abstract = {As appointed in the EU Circular Economy Action Plan, cities and regions in EU member countries start accompanying their circular economy strategies by monitoring frameworks, often called Circular Economy Monitors (CEM). Having the task to assess the performance towards the achievement of set targets and to steer decision-making, CEMs need to rely on a multitude of statistics and datasets. Waste statistics play an important role in circular economy monitoring as they provide insights into the remaining linear part of the economy. The collection of waste statistics is mandated by the European Commission which provides general guidelines on data collection and processing. The Netherlands has one of the most detailed waste registries among the EU countries. The country’s largest metropolitan region, Amsterdam, is currently building a CEM which tracks progress over time towards the set goals, highlights which areas need improvement and estimates target feasibility. This paper uses the Amsterdam CEM as a case-study to explore how the existing system of waste registration in the Netherlands is able to support decision-making. The data is explored with the help of four queries that relate to the CEM’s goals and require data mapping to be answered. The data mapping and analysis process has revealed several limitations present in the waste data collection and a number of gaps present in current circular economy research and data analysis. At the same time, the available data already supports significant insights into the status quo of the current waste system and provides opportunities for circular economy monitoring.}
}
@article{GUENTHER2022251,
title = {AI-Based Failure Management: Value Chain Approach in Commercial Vehicle Industry},
journal = {Procedia CIRP},
volume = {109},
pages = {251-256},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122006941},
author = {Robin Guenther and Sebastian Beckschulte and Martin Wende and Hendrik Mende and Robert H. Schmitt},
keywords = {Artificial intelligence, failure detection, failure management, machine learning, production, value chain},
abstract = {This paper describes an artificial intelligence (AI) based failure management approach across the value chain for the commercial vehicle industry by integrating and utilizing lifecycle data for product and production optimization. The amount of available data throughout a product lifecycle has increased significantly in previous years, primarily driven by the development and deployment of cyber-physical systems. While data from a single entity in the value chain already enables failure management-related analysis and services, including AI-based methods such as predictive maintenance, there remains a lack of systematic approaches to utilize data across the entire value chain. This paper proposes an AI-based failure management approach, which relies on integrating a variety of diverse data sources along the value chain. At first, three so-called application areas were defined: process and product optimization, availability optimization, and performance optimization. Consequently, practice-relevant use cases are identified for each area, for which it is shown how failures in the value chain can be proactively eliminated with the support of AI. Methods for predictive analytics are adapted for cross-value chain failure management to derive correlations between different stages of the production process and product usage. Based on these results and human expert knowledge, proactive measures are recommended by a decision support system (DSS) to resolve failures before arising. The commercial vehicle industry serves as an overarching validation case study for the practice-relevant verification of the targeted applications. The paper gives an outlook on the envisaged research work for the realization of holistic failure management.}
}
@article{JUNG202219,
title = {Comparative analysis of network-based approaches and machine learning algorithms for predicting drug-target interactions},
journal = {Methods},
volume = {198},
pages = {19-31},
year = {2022},
note = {Network-Based Approaches in Bioinformatics and Biomedicine},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2021.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1046202321002474},
author = {Yi-Sue Jung and Yoonbee Kim and Young-Rae Cho},
keywords = {Drug-target interactions, DTIs, DTI networks, Network-based approaches},
abstract = {Computational prediction of drug–target interactions (DTIs) is of particular importance in the process of drug repositioning because of its efficiency in selecting potential candidates for DTIs. A variety of computational methods for predicting DTIs have been proposed over the past decade. Our interest is which methods or techniques are the most advantageous for increasing prediction accuracy. This article provides a comprehensive overview of network-based, machine learning, and integrated DTI prediction methods. The network-based methods handle a DTI network along with drug and target similarities in a matrix form and apply graph-theoretic algorithms to identify new DTIs. Machine learning methods use known DTIs and the features of drugs and target proteins as training data to build a predictive model. Integrated methods combine these two techniques. We assessed the prediction performance of the selected state-of-the-art methods using two different benchmark datasets. Our experimental results demonstrate that the integrated methods outperform the others in general. Some previous methods showed low accuracy on predicting interactions of unknown drugs which do not exist in the training dataset. Combining similarity matrices from multiple features by data fusion was not beneficial in increasing prediction accuracy. Finally, we analyzed future directions for further improvements in DTI predictions.}
}
@article{ZHANG2022119510,
title = {Spatiotemporal neural network for estimating surface NO2 concentrations over north China and their human health impact},
journal = {Environmental Pollution},
volume = {307},
pages = {119510},
year = {2022},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2022.119510},
url = {https://www.sciencedirect.com/science/article/pii/S0269749122007242},
author = {Chengxin Zhang and Cheng Liu and Bo Li and Fei Zhao and Chunhui Zhao},
keywords = {Exposure assessment, Surface nitrogen dioxide, Deep learning, Health impact, Satellite remote sensing, Air quality prediction},
abstract = {Atmospheric nitrogen dioxide (NO2) is an important reactive gas pollutant harmful to human health. The spatiotemporal coverage provided by traditional NO2 monitoring methods is insufficient, especially in the suburban and rural areas of north China, which have a high population density and experience severe air pollution. In this study, we implemented a spatiotemporal neural network (STNN) model to estimate surface NO2 from multiple sources of information, which included satellite and in situ measurements as well as meteorological and geographical data. The STNN predicted NO2 with high accuracy, with a coefficient of determination (R2) of 0.89 and a root mean squared error of 5.8 μg/m3 for sample-based 10-fold cross-validation. Based on the surface NO2 concentration determined by the STNN, we analyzed the spatial distribution and temporal trends of NO2 pollution in north China. We found substantial drops in surface NO2 concentrations ranging between 9.1% and 33.2% for large cities during the 2020 COVID-19 lockdown when compared to those in 2019. Moreover, we estimated the all-cause deaths attributed to NO2 exposure at a high spatial resolution of about 1 km, with totals of 6082, 4200, and 18,210 for Beijing, Tianjin, and Hebei Provinces in 2020, respectively. We observed remarkable regional differences in the health impacts due to NO2 among urban, suburban, and rural areas. Generally, the STNN model could incorporate spatiotemporal neighboring information and infer surface NO2 concentration with full coverage and high accuracy. Compared with machine learning regression techniques, STNN can effectively avoid model overfitting and simultaneously consider both spatial and temporal correlations of input variables using deep convolutional networks with residual blocks. The use of the proposed STNN model, as well as the surface NO2 dataset, can benefit air quality monitoring, forecasting, and health burden assessments.}
}
@article{RUI2022123927,
title = {High-accuracy transient fuel consumption model based on distance correlation analysis},
journal = {Fuel},
volume = {321},
pages = {123927},
year = {2022},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2022.123927},
url = {https://www.sciencedirect.com/science/article/pii/S0016236122007864},
author = {Ding Rui and Jin Hui},
keywords = {Transient fuel consumption model, Model optimization, Distance correlation analysis},
abstract = {With the gradual aggravation of energy shortage, automobile energy saving has gained widespread attention from scholars. However, due to the lack of a high-accuracy practical fuel consumption model, it is difficult to estimate transient fuel consumption and evaluate the actual effect of real-time fuel consumption control strategies. Therefore, it is necessary to establish a more accurate and practical model according to the transient motion characteristics of the vehicles. To ensure the accuracy of the model, an integrated structure of the steady-state base module and the transient correction module is determined as the overall structure of the model. Based on the steady-state fuel consumption data, the steady-state base module is established. Then, based on the easily obtained vehicle and engine state parameters, principal component analysis and cluster analysis are used to reasonably classify different driving conditions of the vehicles. Following that, the distance correlation analysis is applied to find the combination of state parameters with the strongest correlation with the estimation error of the steady-state module, and a transient correction module is established according to the optimal state parameter combination obtained. After that, the optimal transient correction module structure is determined based on the Bayesian criterion. Finally, the model is tested, and the results show that the mean absolute percentage error (MAPE) of the fuel consumption estimation of the new model is about 15%, while that of the classical VT-Micro model and the VT-CPFM model are about 28% and 20%, respectively. It can be seen that the new model has a higher accuracy. On the other hand, compared with structured physical fuel consumption models such as VT-CPEM model, the new model has a simpler structure, a shorter computation time, and a higher computational speed. In addition, the new model has high practicability due to its clear structure and easy access to parameters.}
}
@article{DING2022116163,
title = {A study on data-driven hybrid heating load prediction methods in low-temperature district heating: An example for nursing homes in Nordic countries},
journal = {Energy Conversion and Management},
volume = {269},
pages = {116163},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116163},
url = {https://www.sciencedirect.com/science/article/pii/S019689042200944X},
author = {Yiyu Ding and Thomas Ohlson Timoudas and Qian Wang and Shuqin Chen and Helge Brattebø and Natasa Nord},
keywords = {Nursing homes, District heating load prediction, Linear regression, Artificial neural network, Low-temperature district heating},
abstract = {In the face of green energy initiatives and progressively increasing shares of more energy-efficient buildings, there is a pressing need to transform district heating towards low-temperature district heating. The substantially lowered supply temperature of low-temperature district heating broadens the opportunities and challenges to integrate distributed renewable energy, which requires enhancement on intelligent heating load prediction. Meanwhile, to fulfill the temperature requirements for domestic hot water and space heating, separate energy conversion units on user-side, such as building-sized boosting heat pumps shall be implemented to upgrade the temperature level of the low-temperature district heating network. This study conducted hybrid heating load prediction methods with long-term and short-term prediction, and the main work consisted of four steps: (1) acquisition and processing of district heating data of 20 district heating supplied nursing homes in the Nordic climate (2016–2019); (2) long-term district heating load prediction through linear regression, energy signature curve in hourly resolution, providing an overall view and boundary conditions for the unit sizing; (3) short-term district heating load prediction through two Artificial Neural Network models, f72 and g120, with different prediction input parameters; (4) evaluation of the predicted load profiles based on the measured data. Although the three prediction models met the quality criteria, it was found that including the historical hourly heating loads as the input to the forecasting model enhanced the prediction quality, especially for the peak load and low-mild heating season. Furthermore, a possible application of the heating load profiles was proposed by integrating two building-sized heat pumps in low-temperature district heating, which may be a promising heat supply method in low-temperature district heating.}
}
@article{LIU2022102503,
title = {Real-time multiscale prediction of structural performance in material extrusion additive manufacturing},
journal = {Additive Manufacturing},
volume = {49},
pages = {102503},
year = {2022},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.102503},
url = {https://www.sciencedirect.com/science/article/pii/S2214860421006503},
author = {Xin Liu and Chen Kan and Zehao Ye},
keywords = {Multiscale modeling, In-situ monitoring, Material extrusion, Honeycomb structure, Geometric defects},
abstract = {The material extrusion additive manufacturing (AM) has been extensively used in fabricating structures with complex geometries. However, geometric defects often exist in an AM structure, which could compromise its final performance. In this paper, a real-time multiscale performance evaluation method is developed for material extrusion-based honeycomb structures. The representative cell boundary is extracted from three-dimensional (3D) point clouds obtained via an in-situ monitoring approach. The cell boundary is then used to generate the digital twin of the unit cell of the printed layer based on the finite element (FE) method. A physics-based multiscale modeling approach called mechanics of structure genome (MSG) is then employed to predict the effective material properties of the printed layer and plate stiffness matrix of the final structure. The proposed approach provides a highly efficient way to predict the real-time performance of the as-manufactured products. Moreover, the numerical example shows that the geometric defects could result in complex mechanical behaviors in the defected parts, which cannot be captured by the conventional approaches based on the shape deviations. The numerical results are validated by the three-point bending tests. The proposed method can be used in the closed-loop control of material extrusion-based manufacturing systems.}
}
@incollection{CORTES2022219,
title = {Chapter 11 - Societal and ethical impact of technologies for health and biomedicine},
editor = {Davide Cirillo and Silvina Catuara-Solarz and Emre Guney},
booktitle = {Sex and Gender Bias in Technology and Artificial Intelligence},
publisher = {Academic Press},
pages = {219-238},
year = {2022},
isbn = {978-0-12-821392-6},
doi = {https://doi.org/10.1016/B978-0-12-821392-6.00002-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128213926000029},
author = {Atia Cortés and Nataly Buslón and Liliana Arroyo},
keywords = {Artificial intelligence, Ethics, AI ethics, Social impact AI, Health AI, Responsible AI},
abstract = {The healthcare sector has been an early adopter of new technologies such as artificial intelligence, nanotechnology, or genome sequencing. They are expected to improve healthcare systems and augment practitioners’ skills. The deployment of wearable sensors and healthcare trackers are empowering individuals, making them self-aware of their wellbeing but also turning them into data donors. Personal data are essential to train machine learning models used to support healthcare professionals in decision making. However, it is extremely relevant to consider the power of the (mis-)represented population in the data analyzed. Artificial intelligent systems used in precision medicine need to be robust, not only technically but also socially by tackling gender imbalance, technology access, or other issues that may affect vulnerable groups in healthcare. This chapter offers an overview on the opportunities of digital health ecosystems while highlighting some social, ethical, and technical challenges. It also provides a review of the relation of the traditional ethical principles used in health and biomedicine and those defined for the design, deployment, and use of a trustworthy AI in Europe.}
}
@article{BUMAN2022112984,
title = {Towards consistent assessments of in situ radiometric measurements for the validation of fluorescence satellite missions},
journal = {Remote Sensing of Environment},
volume = {274},
pages = {112984},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.112984},
url = {https://www.sciencedirect.com/science/article/pii/S0034425722000980},
author = {Bastian Buman and Andreas Hueni and Roberto Colombo and Sergio Cogliati and Marco Celesti and Tommaso Julitta and Andreas Burkart and Bastian Siegmann and Uwe Rascher and Matthias Drusch and Alexander Damm},
keywords = {Sun-induced chlorophyll fluorescence, Spectroradiometer, Uncertainty, Bias, Measurement variability, Spectral shift, FLEX, FloX},
abstract = {The upcoming Fluorescence Explorer (FLEX) satellite mission aims to provide high quality radiometric measurements for subsequent retrieval of sun-induced chlorophyll fluorescence (SIF). The combination of SIF with other observations stemming from the FLEX/Sentinel-3 tandem mission holds the potential to assess complex ecosystem processes. The calibration and validation (cal/val) of these radiometric measurements and derived products are central but challenging components of the mission. This contribution outlines strategies for the assessment of in situ radiometric measurements and retrieved SIF. We demonstrate how in situ spectrometer measurements can be analysed in terms of radiometric, spectral and spatial uncertainties. The analysis of more than 200 k spectra yields an average bias between two radiometric measurements by two individual spectrometers of 8%, with a larger variability in measurements of downwelling radiance (25%) compared to upwelling radiance (6%). Spectral shifts in the spectrometer relevant for SIF retrievals are consistently below 1 spectral pixel (up to 0.75). Found spectral shifts appear to be mostly dependent on temperature (as measured by a temperature probe in the instrument). Retrieved SIF shows a low variability of 1.8% compared with a noise reduced SIF estimate based on APAR. A combination of airborne imaging and in situ non-imaging fluorescence spectroscopy highlights the importance of a homogenous sampling surface and holds the potential to further uncover SIF retrieval issues as here shown for early evening acquisitions. Our experiments clearly indicate the need for careful site selection, measurement protocols, as well as the need for harmonized processing. This work thus contributes to guiding cal/val activities for the upcoming FLEX mission.}
}
@article{CHONDRODIMA2022100086,
title = {Particle swarm optimization and RBF neural networks for public transport arrival time prediction using GTFS data},
journal = {International Journal of Information Management Data Insights},
volume = {2},
number = {2},
pages = {100086},
year = {2022},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2022.100086},
url = {https://www.sciencedirect.com/science/article/pii/S2667096822000295},
author = {Eva Chondrodima and Harris Georgiou and Nikos Pelekis and Yannis Theodoridis},
keywords = {Estimated time of arrival (ETA), Fuzzy means, General transit feed specification (GTFS), Intelligent transportation systems, Neural networks (NN), Particle swarm optimization (PSO), Public transport},
abstract = {Accurate prediction of Public Transport (PT) mobility is important for intelligent transportation. Nowadays, mobility data have become increasingly available with the General Transit Feed Specification (GTFS) being the format for PT agencies to disseminate such data. Estimated Time of Arrival (ETA) of PT is crucial for the public, as well as the PT agency for logistics, route-optimization, maintenance, etc. However, prediction of PT-ETA is a challenging task, due to the complex and non-stationary urban traffic. This work introduces a novel data-driven approach for predicting PT-ETA based on RBF neural networks, using a modified version of the successful PSO-NSFM algorithm for training. Additionally, a novel pre-processing pipeline (CR-GTFS) is designed for cleansing and reconstructing the GTFS data. The combination of PSO-NSFM and CR-GTFS introduces a complete framework for predicting PT-ETA accurately with real-world data feeds. Experiments on GTFS data verify the proposed approach, outperforming state-of-the-art in prediction accuracy and computational times.}
}
@article{BORCH2022101852,
title = {Machine learning, knowledge risk, and principal-agent problems in automated trading},
journal = {Technology in Society},
volume = {68},
pages = {101852},
year = {2022},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101852},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X21003274},
author = {Christian Borch},
keywords = {Automated trading, Financial markets, Knowledge risk, Machine learning, Principal-agent problems},
abstract = {Present-day securities trading is dominated by fully automated algorithms. These algorithmic systems are characterized by particular forms of knowledge risk (adverse effects relating to the use or absence of certain forms of knowledge) and principal-agent problems (goal conflicts and information asymmetries arising from the delegation of decision-making authority). Where automated trading systems used to be based on human-defined rules, increasingly, machine-learning (ML) techniques are being adopted to produce machine-generated strategies. Drawing on 213 interviews with market participants involved in automated trading, this study compares the forms of knowledge risk and principal-agent relations characterizing both human-defined and ML-based automated trading systems. It demonstrates that certain forms of ML-based automated trading lead to a change in knowledge risks, particularly concerning dramatically changing market settings, and that they are characterized by a lack of insight into how and why trading rules are being produced by the ML systems. This not only intensifies but also reconfigures principal-agent problems in financial markets.}
}
@incollection{LEMUSALARCON2022159,
title = {Chapter 6 - Cloud-based data pipeline orchestration platform for COVID-19 evidence-based analytics},
editor = {Victor Chang and Mohamed Abdel-Basset and Muthu Ramachandran and Nicolas G. Green and Gary Wills},
booktitle = {Novel AI and Data Science Advancements for Sustainability in the Era of COVID-19},
publisher = {Academic Press},
pages = {159-180},
year = {2022},
isbn = {978-0-323-90054-6},
doi = {https://doi.org/10.1016/B978-0-323-90054-6.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323900546000039},
author = {Mauro {Lemus Alarcon} and Roland Oruche and Ashish Pandey and Prasad Calyam},
keywords = {Research data sharing, Cloud-hosted health-care data, Data access control, Data science tools interface},
abstract = {Identifying high-quality publications remains a critical challenge for health-care data consumers (e.g., immunologists, clinical researchers) who seek to make timely decisions related to the COVID-19 pandemic response. Currently, researchers perform a manual literature review process to compile and analyze publications from disparate medical journal databases. Such a process is cumbersome, inefficient, and increases the time to complete research tasks. In this book chapter, we describe a cloud-based, intelligent data pipeline orchestration platform, viz., “OnTimeEvidence” that provides health-care consumers with easy access to publication archives and analytics tools for rapid pandemic-related knowledge discovery tasks. This platform aims to reduce the burden and expensive time to find, sort, and analyze publications in terms of their level of evidence. We also present a case study of how OnTimeEvidence platform can be configured to help health-care data consumers to combine and analyze multiple data sources using interactive interfaces featuring workspaces equipped with analytics tools.}
}
@article{JADHAV2022100328,
title = {A review study of the blockchain-based healthcare supply chain},
journal = {Social Sciences & Humanities Open},
volume = {6},
number = {1},
pages = {100328},
year = {2022},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2022.100328},
url = {https://www.sciencedirect.com/science/article/pii/S2590291122000821},
author = {Jayendra S. Jadhav and Jyoti Deshmukh},
keywords = {Blockchain, Blockchain technology, Healthcare supply chain, Healthcare, Review, Hybrid reinforcement, IPFS},
abstract = {Technological acclimatization in today's healthcare industry is a subject of new inventions. The worldwide Covid-19 epidemic has led to increase in the use of technology for healthcare supply chain, patient data management, and claims settlement. Data management in healthcare industry is a complex structure where multiple organizations provide proper supply chain services in day to day life. Improper data management disrupts the supply chain, which has a long-term impact on the healthcare sector. Various issues in the present supply chain must be addressed. Blockchain-based crypto-currencies are well-known nowadays for their ability to create safe and traceable solutions. With the growing use of crypto-currencies, it also governs new range of applications and opportunities, including healthcare applications. Blockchain-based solutions are effective in the health sector for secure data retrieval and storage, resulting in more effectual product creation and tracking. Such system can provide data provenance, promotes genuine healthcare sector demands, and ensures the immutability of multi-direction transactions. In this study, we contribute a thorough overview of the literature on how Blockchain technology is changing the way healthcare supply chains operate. We looked at 61 papers from 2019 to 2021 that highlighted various difficulties with the traditional healthcare supply chain. We scrutinized different barriers and opportunity of Blockchain-based healthcare supply chain at the end of the research.}
}
@article{WU2022103421,
title = {Amenity, firm agglomeration, and local creativity of producer services in Shanghai},
journal = {Cities},
volume = {120},
pages = {103421},
year = {2022},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2021.103421},
url = {https://www.sciencedirect.com/science/article/pii/S0264275121003206},
author = {Yangyi Wu and Yehua Dennis Wei and Han Li and Meitong Liu},
keywords = {Producer services, Creativity, Urban amenity, Shanghai},
abstract = {Studies of creativity in urban China are heavily confined at the interurban level and have been criticized for unclear spatial mechanisms and missing local context. This study constructs a theoretical framework to understand the role of urban amenity on the local attractiveness to producer services and further analyzes such attractiveness in Shanghai in terms of agglomeration and creativity using open data. We find that creative firms are more clustered than other producer service firms and urban amenities in Shanghai. The regression results show that urban amenity is strong in explaining local attractiveness to creativity rather than firm agglomeration at the 1-km scale. The attractiveness of the local urban area to creativity may be affected by urban amenity in various ways, including co-location, accessibility, and high-density clusters. Such relationships also follow Shanghai's monocentric structure. The importance of urban amenity decays as the distance to the central business district (CBD) increases in regards to firm agglomeration but persists in terms of creativity level. These findings highlight the importance of considering the co-existence of different spatial relationships and accentuate the differentiated applicability of industrial agglomeration and creativity theories.}
}
@article{SANDEEPA2022100405,
title = {A survey on privacy for B5G/6G: New privacy challenges, and research directions},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100405},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100405},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000723},
author = {Chamara Sandeepa and Bartlomiej Siniarski and Nicolas Kourtellis and Shen Wang and Madhusanka Liyanage},
keywords = {Beyond 5G, 6G, Privacy issues, Privacy solutions, Artificial intelligence, Machine learning, Explainable AI, Survey},
abstract = {Massive developments in mobile wireless telecommunication networks have been made during the last few decades. At present, mobile users are getting familiar with the latest 5G networks, and the discussion for the next generation of Beyond 5G (B5G)/6G networks has already been initiated. It is expected that B5G/6G will push the existing network capabilities to the next level, with higher speeds, enhanced reliability and seamless connectivity. To make these expectations a reality, research is progressing on new technologies, architectures, and intelligence-based decision-making processes related to B5G/6G. Privacy considerations are a crucial aspect that requires further attention in such developments, as billions of people and devices will be transmitting data through the upcoming network. However, the main recognition remains biased towards the network security. A discussion focused on privacy of B5G/6G is lacking at the moment. To address the gap, this paper provides a comprehensive survey on privacy-related aspects of B5G/6G networks. First, it discusses a taxonomy of different privacy perspectives. Based on the taxonomy, the paper then conceptualizes a set of challenges that appear as barriers to reach privacy preservation. Next, this work provides a set of solutions applicable to the proposed architecture of B5G/6G networks to mitigate the challenges. It also provides an overview of standardization initiatives for privacy preservation. Finally, the paper concludes with a roadmap of future directions, which will be an arena for new research towards privacy-enhanced B5G/6G networks. This work provides a basis for privacy aspects that will significantly impact peoples’ daily lives when using these future networks.}
}
@article{ZANTI2022102093,
title = {Leveraging integrated data for program evaluation: Recommendations from the field},
journal = {Evaluation and Program Planning},
volume = {95},
pages = {102093},
year = {2022},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2022.102093},
url = {https://www.sciencedirect.com/science/article/pii/S0149718922000477},
author = {Sharon Zanti and Emily Berkowitz and Matthew Katz and Amy Hawn Nelson and T.C. Burnett and Dennis Culhane and Yixi Zhou},
keywords = {Cross-sector data linkage, Integrated data systems, Administrative data reuse},
abstract = {Use of administrative data to inform decision making is now commonplace throughout the public sector, including program and policy evaluation. While reuse of these data can reduce costs, improve methodologies, and shorten timelines, challenges remain. This article informs evaluators about the growing field of Integrated Data Systems (IDS), and how to leverage cross-sector administrative data in evaluation work. This article is informed by three sources: a survey of current data integration efforts in the United States (U.S.) (N=63), informational interviews with experts, and internal knowledge cultivated through Actionable Intelligence for Social Policy’s (AISP) 12+ years of work in the field. A brief discussion of the U.S. data integration context and history is provided, followed by discussion of tangible recommendations for evaluators, examples of evaluations relying on integrated data, and a list of U.S. IDS sites with publicly available processes for external data requests. Despite the challenges associated with reusing administrative data for program evaluation, IDS offer evaluators a new set of tools for leveraging data across institutional silos.}
}
@article{GUENDUEZ2022101719,
title = {Strategically constructed narratives on artificial intelligence: What stories are told in governmental artificial intelligence policies?},
journal = {Government Information Quarterly},
pages = {101719},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101719},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22000521},
author = {Ali A. Guenduez and Tobias Mettler},
keywords = {Artificial intelligence (AI), Policy research, Structural topic modeling (STM), Narrative policy framework (NPF), Role of government},
abstract = {What stories are told in national artificial intelligence (AI) policies? Combining the novel technique of structural topic modeling (STM) and qualitative narrative analysis, this paper examines the policy narratives in 33 countries’ AI policies. We uncover six common narratives that are dominating the political agenda concerning AI. Our findings show that the policy narratives' saliences vary across time and countries. We make several contributions. First, our narratives describe well-grounded, supportable conceptions of AI among governments, and show that AI is still a fairly novel, multilayered, and controversial phenomenon. Building on the premise that human sensemaking is best represented and supported by narration, we address the applied rhetoric of governments to either minimize the risks or exalt the opportunities of AI. Second, we uncover the four prominent roles governments seek  to take concerning AI implementation: enabler, leader, regulator, and/or user. Third, we make a methodological contribution toward data-driven, computationally-intensive theory development. Our methodological approach and the identified narratives present key starting points for further research.}
}
@article{SINGH20225021,
title = {Blockchain with cloud for handling healthcare data: A privacy-friendly platform},
journal = {Materials Today: Proceedings},
volume = {62},
pages = {5021-5026},
year = {2022},
note = {International Conference on Innovative Technology for Sustainable Development},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.04.910},
url = {https://www.sciencedirect.com/science/article/pii/S221478532203098X},
author = {Suruchi Singh and Bhatt Pankaj and K. Nagarajan and Neha {P. Singh} and Veer Bala},
keywords = {Data management, Cloud computing, e-Health, Cloud E-health, E-health security},
abstract = {In addition to its unique characteristics, health care data make it an attractive target for criminals. Additionally, healthcare data is highly regulated by US & EU privacy and security laws and international laws governing data storage in the cloud. The value of healthcare data and these regulatory requirements have motivated organizations to use blockchain to protect data. Blockchain will facilitate effective data exchange while maintaining patient privacy and data protection. Cloud data has long been a draw for cyber attackers. Today, cloud health data has been their latest interest. The cloud-based application that theoretically transforms the operation and communication of existing healthcare networks is EHR. The EHR is a digital file comprising the personal details of citizens/patients (name, photo, address, age, etc.) and health (existing diseases, previous surgeries, allergies, blood type, allergies, vaccinations, etc.). Maximizing cost efficiency is one of the drivers for integrating cloud services in health care. The return on investment for healthcare services is better when using cloud solutions. The cloud infrastructure includes a centralized medical record entry point, allowing many clinicians to display laboratory reports or consult patient notes. Loud technologies would allow researchers to explore these data and analyses public health more accurately. Cloud technology is not the best way to handle the medical services and records of the healthcare system. Nevertheless, this technology allows patients and physicians access to software that will enhance patient care. The Cloud also provides an excellent opportunity to utilize patient data on a health computer scale to draw insights on health and allow patients to manage personal details quickly and efficiently. There are hurdles in introducing emerging technologies and threatening privacy violations, but health services will utilize this increasingly growing and valuable technology with the appropriate training and framework. As part of this paper, we present a patient-centric healthcare data management solution that uses blockchain technology as a storage mechanism that helps to ensure patient privacy.}
}
@article{YAN2022103663,
title = {Weak celestial source fringes detection based on channel attention shrinkage networks and cluster-based anchor boxes generation algorithm},
journal = {Digital Signal Processing},
volume = {129},
pages = {103663},
year = {2022},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103663},
url = {https://www.sciencedirect.com/science/article/pii/S1051200422002809},
author = {Ruiqing Yan and Rong Ma and Wei Liu and Zongyao Yin and Zhengang Zhao and Siying Chen and Sheng Chang and Hui Zhu and Dan Hu and Xianchuan Yu},
keywords = {Deep learning, Weak signal detection, Celestial source fringe, Soft thresholding, Astronomical image processing},
abstract = {Detecting weak celestial source signals from massive radio data is a very challenging task because the radiation received by radio telescope is very weak and prone to disturbances. In order to detect these weak signals, we propose a two-stage object detection method that performs more finely in computer vision tasks. The novelty of the proposed method is to combine traditional soft thresholding denoising methods with attention mechanisms in deep neural networks. We propose a channel attention shrinkage network as the backbone of the object detection model to extract the features of weak signals from celestial sources by removing noise-related information. Moreover, targeting the characteristics of celestial source fringes in phase images, we propose a cluster-based anchor boxes generation algorithm to improve the accuracy of fringes position detection. We also introduce the CIoU loss function to improve the performance of the model because of the large aspect ratio of the celestial source fringes in the phase image. We generate simulated celestial source fringes data based on the parameters of the observation system to train our model and conduct experiments to evaluate the performance of the proposed algorithm. Our model obtains satisfactory detection accuracy and accurate for the location of celestial source fringes.}
}
@article{SHI2022106836,
title = {Real-time driving risk assessment using deep learning with XGBoost},
journal = {Accident Analysis & Prevention},
volume = {178},
pages = {106836},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2022.106836},
url = {https://www.sciencedirect.com/science/article/pii/S0001457522002718},
author = {Liang Shi and Chen Qian and Feng Guo},
keywords = {Crash prediction, High frequency kinematic driving data, Deep learning, Convolutional neural network, Gated recurrent unit, XGBoost, Naturalistic driving study},
abstract = {Traffic crashes typically occur in a few seconds and real-time prediction can significantly benefit traffic safety management and the development of safety countermeasures. This paper presents a novel deep learning model for crash identification based on high-frequency, high-resolution continuous driving data. The method consists of feature engineering based on Convolutional Neural Network (CNN) and Gated Recurrent Unit (GRU) and classification based on Extreme Gradient Boosting (XGBoost). The CNN-GRU architecture captures the time series characteristics of driving kinematics data. Compared to normal driving segments, safety-critical events (SCEs)—i.e., crashes and near-crashes (CNC)—are rare. The weighted categorical cross-entropy loss and oversampling methods are utilized to address this imbalance issue. An XGBoost classifier is utilized instead of the multi-layer perceptron (MLP) to achieve a high precision and recall rate. The proposed approach is applied to the Second Strategic Highway Research Program Naturalistic Driving Study (SHRP 2 NDS) data with 1,820 crashes, 6,848 near-crashes, and 59,997 normal driving segments. The results show that in a 3-class classification system (crash, near-crash, normal driving segments), the accuracy for the overall model is 97.5%, and the precision and recall for crashes are 84.7%, and 71.3% respectively, which is substantially better than benchmarks models. Furthermore, the recall of the most severe crashes is 98.0%. The proposed crash identification approach provides an accurate, highly efficient, and scalable way to identify crashes based on high frequency, high-resolution continuous driving data and has broad application prospects in traffic safety applications.}
}
@article{GRENYER202237,
title = {Multistep prediction of dynamic uncertainty under limited data},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {37},
pages = {37-54},
year = {2022},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2022.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1755581722000025},
author = {Alex Grenyer and Oliver Schwabe and John A. Erkoyuncu and Yifan Zhao},
keywords = {Forecast, Limited data, Long-short term memory (LSTM), Multistep, Prediction, Spatial geometry, Uncertainty},
abstract = {Engineering systems are growing in complexity, requiring increasingly intelligent and flexible methods to account for and predict uncertainties in service. This paper presents a framework for dynamic uncertainty prediction under limited data (UPLD). Spatial geometry is incorporated with LSTM networks to enable real-time multistep prediction of quantitative and qualitative uncertainty over time. Validation is achieved through two case studies. Results demonstrate robust prediction of trends in limited and dynamic uncertainty data with parallel determination of geometric symmetry at each time unit. Future work is recommended to explore alternative network architectures suited to limited data scenarios.}
}
@article{NEUNZIG2022320,
title = {Model Selection for Predictive Quality in Hydraulic Testing},
journal = {Procedia CIRP},
volume = {107},
pages = {320-325},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.04.052},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002682},
author = {Christian Neunzig and Simon Fahle and Jürgen Schulz and Matthias Möller and Bernd Kuhlenkötter},
keywords = {Hydraulic Testing, Machine Learning, Predictive Quality, Supervised Learning},
abstract = {Manufacturing companies are confronted with enormous challenges such as increasing product complexity, shorter product life cycles and growing product diversity. Politically and socially, increased demands regarding sustainability and resource consumption are streaming into the focus of companies. One solution strategy to increase the productivity of existing production systems while ensuring existing quality standards is the application of data-driven analytical methods such as machine learning. Due to the frequent changes in production conditions, the analysis of real manufacturing data is linked to sophisticated data pre-processing. Changes in production data are manifested in trends and systematic shifts over time. Data pre-processing includes rule-based data cleaning, the application of dimension reduction techniques and the identification of comparable data subsets. Within the used dataset of hydraulic valves by Bosch, the comparability of the same production conditions in the manufacturing of hydraulic valves can be identified within certain periods. Machine learning methods can process large amounts of data, unfavorable row-column ratios and discover dependencies between the input data and the specified target variable as well as evaluate the multidimensional influence of all input variables on the target variable. For use cases in manufacturing, neural networks, support vector machines and tree-based methods have so far proved to be very successful. The use of cross-process production data along the value chain of hydraulic valves is a promising approach to predict the quality characteristics of workpieces. Within this research, machine learning methods with deep and shallow structures are applied to predict the internal leakage of hydraulic valves based on geometric gauge blocks from machining, mating data from assembly and hydraulic measurement data from end-of-line testing. Moreover, the most suitable methods are selected, and accurate quality predictions are obtained.}
}
@article{JADHAV2022186,
title = {An enhanced and secured predictive model of Ada-Boost and Random-Forest techniques in HCV detections},
journal = {Materials Today: Proceedings},
volume = {51},
pages = {186-195},
year = {2022},
note = {CMAE'21},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.071},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321036476},
author = {Dhaval A Jadhav},
keywords = {Ada-Boost, Random-Forest, ECC, Hepatitis, HCC, SHA, HCV- predictions},
abstract = {The Evolution of HCV-Hepatitis C-virus plays the vital cause for liver-related complications in humans, global-wide. But also, specific tools were employed to eliminate the impacts of virus in humans. This phenomena does not rely in the prior stage diagnosis of the virus and in the treatment phases. In this study, the implementation of Ada-Boost algorithm and Random-forest algorithm in the framing the HCV-Hepatitis-C-Virus prediction design prevailing in humans. According to the methodology, Random-forest algorithm were utilized as the weak-learner for choosing the instances of the weight to enhance the stability factors, accuracy-factors and to decrease the outfitting complications. The paper aims to design the secured framework in the improvisation of the user’s privacy. Hence for this purpose, The techniques PPDM-Privacy-Preserving-Data mining approach were developed to keep preserve of the personal data of the userscattered in Distributed-data-mining operational system and also enhancing the security in centralized systems of data-mining as well. The data-set were subjected to the comparison of the ECC-encryption algorithm and the RSA-based encryption along with the blend of SHA-algorithm. The algorithms were evaluated with various sizes of key and proceeded with the decryption process. The Decrypted data, applied with Ada-boost and Random-Forest algorithm in qualitative predictions. Data were partitioned as the test and training sets by the classifiers. The overall-efficiency of the proposed hybrid framework were assessed by the aid of performance-metrics such as accuracy factor, time-factor and specificity-factor. The experimental analysis of the framework, establishes the classifier and various merged classifiers in the predictions of HCV-Stains. The outcomes of the framework exhibited higher accuracy rate in comparison with the other approaches, thus enabling the efficient predictions of HCV.}
}
@article{SI2022103384,
title = {Can government regulation, carbon-emission reduction certification and information publicity promote carpooling behavior?},
journal = {Transportation Research Part D: Transport and Environment},
volume = {109},
pages = {103384},
year = {2022},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2022.103384},
url = {https://www.sciencedirect.com/science/article/pii/S1361920922002127},
author = {Hongyun Si and Yangyue Su and Guangdong Wu and Wenxiang Li and Long Cheng},
keywords = {Carpooling, Carbon-emission reduction, Information publicity, Theory of planned behavior, Shared mobility, Ridesharing},
abstract = {This study constructs a novel theoretical framework to uncover the effects of government regulation, carbon-emission reduction certification and information publicity on people’s carpooling behavior. Survey data on 1056 potential users from China were empirically examined using partial least squares structural equation modeling and multigroup analysis. The results reveal that government regulation and carbon-emission reduction certification can significantly improve users’ carpooling intention. Information publicity not only positively influences carpooling intention but also increases carpooling behavior. Interestingly, government regulation has a more significant effect on female users, users with high educational levels and young users. Carbon-emission reduction certification may negatively affect female users’ carpooling behavior. Information publicity is even effective at improving the older users’ carpooling intention. This research provides new evidence and serves as an insightful decision-making reference for policymakers and operators worldwide seeking to encourage people’s carpooling behavior.}
}
@article{DESILVA2022100489,
title = {An artificial intelligence life cycle: From conception to production},
journal = {Patterns},
volume = {3},
number = {6},
pages = {100489},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100489},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922000745},
author = {Daswin {De Silva} and Damminda Alahakoon},
keywords = {artificial intelligence, AI, AI life cycle, machine learning, AI design, AI development, AI deployment, AI operationalization},
abstract = {Summary
This paper presents the “CDAC AI life cycle,” a comprehensive life cycle for the design, development, and deployment of artificial intelligence (AI) systems and solutions. It addresses the void of a practical and inclusive approach that spans beyond the technical constructs to also focus on the challenges of risk analysis of AI adoption, transferability of prebuilt models, increasing importance of ethics and governance, and the composition, skills, and knowledge of an AI team required for successful completion. The life cycle is presented as the progression of an AI solution through its distinct phases—design, develop, and deploy—and 19 constituent stages from conception to production as applicable to any AI initiative. This life cycle addresses several critical gaps in the literature where related work on approaches and methodologies are adapted and not designed specifically for AI. A technical and organizational taxonomy that synthesizes the functional value of AI is a further contribution of this article.}
}
@article{DWIVEDI2022102456,
title = {Climate change and COP26: Are digital technologies and information management part of the problem or the solution? An editorial reflection and call to action},
journal = {International Journal of Information Management},
volume = {63},
pages = {102456},
year = {2022},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102456},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221001493},
author = {Yogesh K. Dwivedi and Laurie Hughes and Arpan Kumar Kar and Abdullah M. Baabdullah and Purva Grover and Roba Abbas and Daniela Andreini and Iyad Abumoghli and Yves Barlette and Deborah Bunker and Leona {Chandra Kruse} and Ioanna Constantiou and Robert M. Davison and Rahul De’ and Rameshwar Dubey and Henry Fenby-Taylor and Babita Gupta and Wu He and Mitsuru Kodama and Matti Mäntymäki and Bhimaraya Metri and Katina Michael and Johan Olaisen and Niki Panteli and Samuli Pekkola and Rohit Nishant and Ramakrishnan Raman and Nripendra P. Rana and Frantz Rowe and Suprateek Sarker and Brenda Scholtz and Maung Sein and Jeel Dharmeshkumar Shah and Thompson S.H. Teo and Manoj Kumar Tiwari and Morten Thanning Vendelø and Michael Wade},
keywords = {Climate change, COP26, Digital world, Information management, Information systems, Information technology, Sustainability, Sustainable Development Goals (SDGs)},
abstract = {The UN COP26 2021 conference on climate change offers the chance for world leaders to take action and make urgent and meaningful commitments to reducing emissions and limit global temperatures to 1.5 °C above pre-industrial levels by 2050. Whilst the political aspects and subsequent ramifications of these fundamental and critical decisions cannot be underestimated, there exists a technical perspective where digital and IS technology has a role to play in the monitoring of potential solutions, but also an integral element of climate change solutions. We explore these aspects in this editorial article, offering a comprehensive opinion based insight to a multitude of diverse viewpoints that look at the many challenges through a technology lens. It is widely recognized that technology in all its forms, is an important and integral element of the solution, but industry and wider society also view technology as being part of the problem. Increasingly, researchers are referencing the importance of responsible digitalization to eliminate the significant levels of e-waste. The reality is that technology is an integral component of the global efforts to get to net zero, however, its adoption requires pragmatic tradeoffs as we transition from current behaviors to a more climate friendly society.}
}
@article{BATCHU2022109269,
title = {An integrated approach explaining the detection of distributed denial of service attacks},
journal = {Computer Networks},
volume = {216},
pages = {109269},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109269},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622003334},
author = {Raj Kumar Batchu and Hari Seetha},
keywords = {DDoS attacks, Data preprocessing, Feature selection, CICDDoS2019 dataset, SHAP, LIME},
abstract = {In recent years, several machine learning and deep learning models have been designed to detect various DDoS attacks, but the presence of irrelevant features, lack of transparency and class imbalance make these models less efficient. In this paper, we developed a novel efficient model to address these issues in detecting DDOS attacks. To begin with, data preprocessing is performed to improve the quality of the training data. The minority class samples are then generated using the Adaptive Synthetic oversampling technique to overcome the class imbalance. Following that, feature selection is performed by embedding SHAP feature importance within recursive feature elimination with five base classifiers. In addition, the hyperparameter of these classifiers is tuned to determine the most contributed features. Furthermore, global and local explanations for extracted features are provided to ensure transparency. Finally, these features are fed to the dynamic ensemble selection techniques such as KNORA-E and KNORA-U for classification by varying k values. These evaluations are analyzed using the CICDDoS2019 dataset. The evaluations are carried out in balanced and imbalanced data scenarios. The results indicate that the balanced data scenario outperformed the imbalanced data scenario as well as existing approaches. An accuracy of 99.9878% using KNORA-E and 99.9886% using KNORA-U is obtained utilizing the five most contributed features.}
}
@article{ASCHENBRENNER20221455,
title = {FlexiCell: 5G location-based context-aware agile manufacturing},
journal = {Procedia CIRP},
volume = {107},
pages = {1455-1460},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.174},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004589},
author = {Doris Aschenbrenner and Marvin Scharle and Stephan Ludwig},
keywords = {5G, agile manufacturing, human-robot co-production, context awareness, location-based system},
abstract = {Manufacturing machines need to be re-tooled approximately 15 times per week and in the future even more often because of decreasing batch sizes and increasing short-cyclic demands. Collaborative robots promise to offer a versatile automation approach for priorly manual tasks in small and medium-sized enterprises. However, their configuration needs to change at least as often as the re-tooling rate because different parts are produced by the machines or might require different handling in general. Therefore, it would be great if robots and autonomous factory systems, in general, would automatically adjust to these changes in an intelligent way. In our approach, we propose a context-aware and location-based approach for agile manufacturing, in which the manufacturing plant parts, especially the collaborative robots, store i) their constellation, ii) their configuration, and iii) their adaptation strategy, and can react to re-tooling changes and even re-location changes adaptively. For example, moving one collaborative robot to a different location next to the production machine will automatically load its new configuration and consult with the operator on the adaptation strategy (i.e., the safety requirements). Such an approach requires precise location information. To realize the localization and the network capabilities, we propose to use localization based on heterogeniclocalization technologies like ultrasound and wireless communications. We suggest a wireless small-cell-based approach around a nomadic 5G core network, which integrates multiple wireless and wired communication technologies as well as localization support combined with an intelligent asset management strategy. Such nomadic cells can operate on an island without a heavy operator backend and optimize end-to-end communication. Furthermore, these small cells can federate with each other and thus extend their coverage when getting into each other's range.}
}
@article{GARCIA2022108463,
title = {Towards a connected Digital Twin Learning Ecosystem in manufacturing: Enablers and challenges},
journal = {Computers & Industrial Engineering},
volume = {171},
pages = {108463},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108463},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222004922},
author = {Álvaro García and Anibal Bregon and Miguel A. Martínez-Prieto},
keywords = {Digital twin, Learning ecosystem, Manufacturing, Human–machine collaboration, Learning factory, Cyber–physical system},
abstract = {The evolution of digital twin, leveraged by the progressive physical–digital convergence, has provided smart manufacturing systems with knowledge-generation ecosystems based on new models of collaboration between the workforce and industrial processes. Digital twin is expected to be a decision-making solution underpinned by real-time communication and data-driven enablers, entailing close cooperation between workers, systems and processes. But industry will need to face the challenges of building and supporting new technical and digital infrastructures, while workers’ skills development eventually manages to include the increased complexity of industrial processes. This paper is intended to reach a better understanding of learning opportunities offered by emerging Industry 4.0 digital twin ecosystems in manufacturing. Diverse learning approaches focused on the potential application of the digital twin concept in theoretical and real manufacturing ecosystems are reviewed. In addition, we propose an original definition of Digital Twin Learning Ecosystem and the conceptual layered architecture. Existing key enablers of the digital twin physical–digital convergence, such as collaborative frameworks, data-driven approaches and augmented interfaces, are also described. The role of the Learning Factory concept is highlighted, providing a common understanding between academia and industry. Academic applications and complex demonstration scenarios are combined in line with the enablement of connected adaptive systems and the empowerment of workforce skills and competences. The adoption of digital twin in production is still at an initial stage in the manufacturing industry, where specific human and technological challenges must be addressed. The research priorities presented in this work are considered as a recognised basis in industry, which should help digital twin with the objective of its progressive integration as a future learning ecosystem.}
}
@article{JIANG2022104601,
title = {Building demolition estimation in urban road widening projects using as-is BIM models},
journal = {Automation in Construction},
volume = {144},
pages = {104601},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104601},
url = {https://www.sciencedirect.com/science/article/pii/S092658052200471X},
author = {Feng Jiang and Ling Ma and Tim Broyd and Ke Chen and Hanbin Luo and Muzi Du},
keywords = {As-is BIM, Building information modelling (BIM), Road widening, Road engineering, Alignment fitting, Building demolition, Cost estimation},
abstract = {Building demolition caused by urban road widening projects can lead to engineering, economic, and environmental issues and should be planned at the design stage. Based on as-is BIM, this paper proposes a method to estimate the building demolition caused by urban road widening using online map data and statistics on government websites. The as-is BIM models of the existing old road and its surrounding buildings are created, and the BIM models of the newly widened road are built based on the as-is BIM models considering road components in accordance with road engineering expressions to assist building demolition estimation using clash detection. This paper presents a cost-effective building demolition estimation in urban road widening projects without field surveys. It was tested on the M4 Motorway project in London. It has been proved to be a very practical approach to facilitate urban road planning and decision making.}
}
@article{LOUCA2022104243,
title = {Machine learning integration of multimodal data identifies key features of blood pressure regulation},
journal = {eBioMedicine},
volume = {84},
pages = {104243},
year = {2022},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2022.104243},
url = {https://www.sciencedirect.com/science/article/pii/S235239642200425X},
author = {Panayiotis Louca and Tran Quoc Bao Tran and Clea du Toit and Paraskevi Christofidou and Tim D. Spector and Massimo Mangino and Karsten Suhre and Sandosh Padmanabhan and Cristina Menni},
keywords = {Blood pressure, Machine learning, Genomics, Metabolomics, Diet},
abstract = {Summary
Background
Association studies have identified several biomarkers for blood pressure and hypertension, but a thorough understanding of their mutual dependencies is lacking. By integrating two different high-throughput datasets, biochemical and dietary data, we aim to understand the multifactorial contributors of blood pressure (BP).
Methods
We included 4,863 participants from TwinsUK with concurrent BP, metabolomics, genomics, biochemical measures, and dietary data. We used 5-fold cross-validation with the machine learning XGBoost algorithm to identify features of importance in context of one another in TwinsUK (80% training, 20% test). The features tested in TwinsUK were then probed using the same algorithm in an independent dataset of 2,807 individuals from the Qatari Biobank (QBB).
Findings
Our model explained 39·2% [4·5%, MAE:11·32 mmHg (95%CI, +/- 0·65)] of the variance in systolic BP (SBP) in TwinsUK. Of the top 50 features, the most influential non-demographic variables were dihomo-linolenate, cis-4-decenoyl carnitine, lactate, chloride, urate, and creatinine along with dietary intakes of total, trans and saturated fat. We also highlight the incremental value of each included dimension. Furthermore, we replicated our model in the QBB [SBP variance explained = 45·2% (13·39%)] cohort and 30 of the top 50 features overlapped between cohorts.
Interpretation
We show that an integrated analysis of omics, biochemical and dietary data improves our understanding of their in-between relationships and expands the range of potential biomarkers for blood pressure. Our results point to potentially key biological pathways to be prioritised for mechanistic studies.
Funding
Chronic Disease Research Foundation, Medical Research Council, Wellcome Trust, Qatar Foundation.}
}
@article{OCHELLA2022108332,
title = {An RUL-informed approach for life extension of high-value assets},
journal = {Computers & Industrial Engineering},
volume = {171},
pages = {108332},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108332},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222003849},
author = {Sunday Ochella and Mahmood Shafiee and Chris Sansom},
keywords = {Remaining useful life (RUL), Life extension (LE), Prognostics and health management (PHM), Machine learning (ML), Reliability centered maintenance (RCM), Turbofan engines},
abstract = {The conventional approaches for life-extension (LE) of industrial assets are largely qualitative and focus only on a few indicators at the end of an asset’s design life. However, an asset may consist of numerous individual components with different useful lives and therefore applying a single LE strategy to every component will not result in an efficient outcome. In recent years, many advanced analytics techniques have been proposed to estimate the remaining useful life (RUL) of the assets equipped with sensor technology. This paper proposes a data-driven model for LE decision-making based on RUL values predicted on a real-time basis during the asset’s operational life. Our proposed LE model is conceptually targeted at the component, unit, or subsystem level; however, an asset-level decision is made by aggregating information across all components. Consequently, LE is viewed and assessed as a series of ongoing activities, albeit carefully orchestrated in a manner similar to operation and maintenance (O&M). The application of the model is demonstrated using the publicly available NASA C-MAPSS dataset for large commercial turbofan engines. This approach will be very beneficial to asset owners and maintenance engineers as it seamlessly weaves LE strategies into O&M activities, thus optimizing resources.}
}
@article{NUNAN2022451,
title = {Value creation in an algorithmic world: Towards an ethics of dynamic pricing},
journal = {Journal of Business Research},
volume = {150},
pages = {451-460},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322005689},
author = {Daniel Nunan and MariaLaura {Di Domenico}},
keywords = {Dynamic pricing, Pricing ethics, Algorithms, AI ethics},
abstract = {Choice of pricing strategy plays a central role in value creation and the effective functioning of markets. Shifts in technology and the growing availability of data are facilitating ever more innovative forms of pricing strategy. Within the emerging literature on pricing ethics, there is a gap in our understanding of the specific challenges of algorithmically generated dynamic pricing. Increasing pricing automation shifts the managerial focus from the selection of prices to the choice of algorithms. This paper expands the literature on pricing ethics by conceptualizing the ethical challenges raised by the contemporary use of dynamic pricing. We propose a governance model for algorithmically generated dynamic pricing, taking into account the role of the customer as a stakeholder in value generation.}
}
@article{JIANG2022100117,
title = {Static-shift suppression and anti-interference signal processing for CSAMT based on Guided Image Filtering},
journal = {Earthquake Research Advances},
volume = {2},
number = {1},
pages = {100117},
year = {2022},
issn = {2772-4670},
doi = {https://doi.org/10.1016/j.eqrea.2022.100117},
url = {https://www.sciencedirect.com/science/article/pii/S2772467022000057},
author = {Enhua Jiang and Rujun Chen and Debin Zhu and Weiqiang Liu and Regean Pitiya},
keywords = {CSAMT, Static shift, Guided image filtering, Anti-interference},
abstract = {Shallow conductive heterogeneity can lead to static shifts ain the apparent resistivity sounding curve of controlled-source audio-frequency magnetotellurics (CSAMT). The static effect will shift the apparent resistivity curves along with axial log-log coordinates. Such an effect, if not properly processed, can distort the resistivity of rock formation and the depth of interfaces, and even make the geological structures unrecognizable. In this paper, we discuss the reasons and characteristics of the static shift and summarize the previous studies regarding static shift correction. Then, we propose the Guided Image Filtering algorithm to suppress static shifts in CSAMT. In detail, we use the multi-window superposition method to superimpose 1D signals into a 2D matrix image, which is subsequently processed with Guided Image Filtering. In the synthetic model study and field examples, the Guided Image Filtering algorithm has effectively corrected and suppressed static shifts, and finally improved the precision of data interpretation.}
}
@article{CANEPARO2022104012,
title = {Semantic knowledge in generation of 3D layouts for decision-making},
journal = {Automation in Construction},
volume = {134},
pages = {104012},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104012},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521004635},
author = {Luca Caneparo},
keywords = {Ontology (computer science), Semantic knowledge, Generative design, Layout generation, Layout planning, Site management, Multiobjective performance optimisation, Pareto set, AEC, Smart City},
abstract = {Generative computation has the potential to enhance the accuracy, effectiveness, and creativity of spatial layout in design and planning. The paper proposes a methodology to separate the knowledge about objects, spatial relationships, and constraints from the generative process. The separation between the knowledge in a domain and its possible practical uses is an important achievement of semantic technologies, because it grants access to a large body of knowledge, spanning various aspects and processes across buildings and cities, which is being codified into formal ontologies. The present study has reused existing knowledge from two established ontologies. An illustrative case-project demonstrates the suitability of the methodology for a complex layout planning problem, involving a large number of decision-makers, with multiple competing objectives and criteria. The system implements multidimensional visual interactive tools to assist designers, planners, and decision-makers in exploring the layouts and the criteria, to develop their confidence in what qualifies as a good and effective solution.}
}
@article{ZEFRI2022102652,
title = {Developing a deep learning-based layer-3 solution for thermal infrared large-scale photovoltaic module inspection from orthorectified big UAV imagery data},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {106},
pages = {102652},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102652},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421003597},
author = {Yahya Zefri and Imane Sebari and Hicham Hajji and Ghassane Aniba},
keywords = {Digital photogrammetry, Unmanned Aerial Vehicle, Thermography, Photovoltaics, Big imagery data, Deep learning},
abstract = {The increasing adoption of photovoltaic(PV) technology highlights the need for efficient and large-scale deployment-ready inspection solutions. In the thermal infrared imagery-based inspection framework, we develop a robust and versatile deep learning model for the classification of defect-related patterns on PV modules. The model is developed from big UAV imagery data, and designed as a layer-3 building block that can be implemented on top of any two-stage PV inspection workflow comprising: (1)An aerial Structure from Motion– MultiView Stereo (SfM-MVS) photogrammetric acquisition/processing stage, at which a georeferenced thermal orthomosaic of an inspected PV site is generated, and which enables to locate precisely defective modules on field; then (2)an instance segmentation stage that extracts the images of modules. Orthomosaics from 28 different PV sites were produced, comprising 93220 modules with various types, layouts and thermal patterns. Modules were extracted through a developed semi-automatic workflow, then labeled into six classes. Data augmentation and balancing techniques were used to prepare a highly representative and balanced deep learning-ready dataset. The dataset was used to train, cross-validate and test the developed classifier, as well as benchmarking with the VGG16 architecture. The developed model achieves the state-of-art performance and versatility on the addressed classification problem, with a mean F1-score of94.52%. The proposed three-layer solution resolves the issues of conventional imagery-based workflows. It ensures highly accurate and versatile defect detection, and can be efficiently deployed to real-world large-scale applications.}
}
@article{CUI2022131208,
title = {A robust approach for the decomposition of high-energy-consuming industrial loads with deep learning},
journal = {Journal of Cleaner Production},
volume = {349},
pages = {131208},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.131208},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622008393},
author = {Jia Cui and Yonghui Jin and Renzhe Yu and Martin Onyeka Okoye and Yang Li and Junyou Yang and Shunjiang Wang},
keywords = {Smart grid, Load awareness, Non-intrusive load decomposition, Deep learning},
abstract = {The knowledge of the users’ electricity consumption pattern is an important coordinating mechanism between the utility company and the electricity consumers in terms of key decision makings. The load decomposition is therefore crucial to reveal the underlying relationship between the load consumption and its characteristics. However, load decomposition is conventionally performed on the residential and commercial loads, and adequate consideration has not been given to the high-energy-consuming industrial loads leading to inefficient results. This paper thus focuses on the load decomposition of the industrial park loads (IPL). The commonly used parameters in a conventional method are however inapplicable in high-energy-consuming industrial loads. Therefore, a more robust approach is developed comprising a three-algorithm model to achieve this goal on the IPL. First, the improved variational mode decomposition (IVMD) algorithm is introduced to denoise the training data of the IPL and improve its stability. Secondly, the convolutional neural network (CNN) and simple recurrent units (SRU) joint algorithms are used to achieve a non-intrusive and non-invasive decomposition process of the IPL using a double-layer deep learning network based on the IPL characteristics. Specifically, CNN is used to extract the IPL data characteristics while the improved long and short-term memory (LSTM) network, SRU, is adopted to develop the decomposition model and further train the load data. Through the robust decomposition process, the underlying relationship in the load consumption is extracted. The results obtained from the numerical examples show that this approach outperforms the state-of-the-art in the conventional decomposition process.}
}
@incollection{MACCARTHY20223,
title = {Chapter 1 - The Digital Supply Chain—emergence, concepts, definitions, and technologies},
editor = {Bart L. MacCarthy and Dmitry Ivanov},
booktitle = {The Digital Supply Chain},
publisher = {Elsevier},
pages = {3-24},
year = {2022},
isbn = {978-0-323-91614-1},
doi = {https://doi.org/10.1016/B978-0-323-91614-1.00001-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323916141000010},
author = {Bart L. MacCarthy and Dmitry Ivanov},
keywords = {Blockchain, Digital supply chain, Digital twins, Internet of things, Smart factory, Supply chain analytics, Cloud computing},
abstract = {Advances in technology, rapid globalization, trade liberalization, and increased regulation have shaped supply chains in the last four decades. We examine the impact of digitalization on contemporary and future supply chains. Digitalization potentially enables a strong digital thread connecting and mirroring an entire physical supply chain. We provide an overview of the principal technologies and systems enabling the Digital Supply Chain, including Smart Factories, Smart Warehouses, Smart Logistics, Cloud-based systems, and digital platforms. We discuss the computational engines enabled by Analytics, Data Science, and Artificial Intelligence and the emerging technologies likely to influence future supply chains—Blockchain, Digital Twins, Internet of Things, 5G, Edge, and Fog computing. The technologies offering the most promise in linking the virtual and physical worlds to improve supply chain performance are noted. We describe an evolving spectrum from digitally immature to digitally enabled and digitally transformed supply chains. We provide both narrow and broad definitions for future Digital Supply Chains. The transformative effects of the digitalization of supply chains will affect supply systems in diverse ways. Data-rich supply chain ecosystems will provide many new opportunities but will also give rise to many challenges that require continued analysis and evaluation by researchers and practitioners.}
}
@article{ZHANG2022105255,
title = {Heat wave tracker: A multi-method, multi-source heat wave measurement toolkit based on Google Earth Engine},
journal = {Environmental Modelling & Software},
volume = {147},
pages = {105255},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105255},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221002978},
author = {Mingxi Zhang and Xihua Yang and Jamie Cleverly and Alfredo Huete and Hong Zhang and Qiang Yu},
keywords = {Extreme heat wave, Google Earth Engine, Climate datasets, Risk analysis, GCM, Australia},
abstract = {Under ongoing global warming due to climate change, heat waves in Australia are expected to become more frequent and severe. Extreme heat waves have devastating impacts on both terrestrial and marine ecosystems. A multi-characteristic heat wave framework is used to estimate historical and future projected heat waves across Australia. A Google Earth Engine-based toolkit named heat wave tracker (HWT) is developed, which can be used for dynamic visualization, extraction, and processing of complex heat wave events. The toolkit exploits the public long-term high-resolution climate datasets to developed nine heat wave datasets across Australia for extreme heat wave value analysis. To examine climate change on heat waves and how they vary in time and space, we also explore the probability and return periods of extreme heat waves over a period of 100 years. The datasets, toolkit and findings we developed contribute to global studies on heat waves under accelerated global warming.}
}
@article{NGUYEN2022109636,
title = {Predicting the opening state of a group of windows in an open-plan office by using machine learning models},
journal = {Building and Environment},
volume = {225},
pages = {109636},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.109636},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322008666},
author = {Thi Hao Nguyen and Anda Ionescu and Evelyne Géhin and Olivier Ramalho},
keywords = {Indoor air quality, Windows opening state, Machine learning model, Time series, Autocorrelation functions, Open-plan office},
abstract = {Window operation is among one of the most influencing factors on the indoor air quality (IAQ). The opening state of the windows can modify the air exchange rate and as such the pollutant transfer between indoor and outdoor environments. In this paper, we focus on the modeling of the windows opening state in a real open-plan office with five windows. For this purpose, three machine learning-based models were implemented: (i) Decision Tree, (ii) k-Nearest Neighbors and (iii) Kernel Approximation. IAQ, climatic parameters and the opening state of the windows have been monitored during an entire period of 18 months. The information about: (i) the environmental factors from the previous 24th hour and (ii) the current time (month, day of the week, hour of the day) was used to predict the current state of the windows. The predictor importance estimation and the calculated autocorrelation functions showed that the three most relevant factors were: the previous 24th hour of the windows status, the current time and the previous 24th hour of the prevailing mean outdoor air temperature. The three models perform well with the testing sets according to the different evaluation indicators. The developed methods can be helpful for understanding occupant behavior and also for controlling indoor air pollutants levels in buildings, either as a standalone model or a part of a real-time IAQ monitoring system.}
}
@article{RAUH2022576,
title = {Towards AI Lifecycle Management in Manufacturing Using the Asset Administration Shell (AAS)},
journal = {Procedia CIRP},
volume = {107},
pages = {576-581},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122003122},
author = {Lukas Rauh and Sascha Gärtner and David Brandt and Michael Oberle and Daniel Stock and Thomas Bauernhansl},
keywords = {AI Lifecycle, AI Asset, Industry 4.0, Digital Twin, Asset Administration Shell},
abstract = {Driven by the digital transformation, manufacturing companies face the challenge of managing, but more importantly, enabling rapid operationalization of AI to achieve the full advantage of the exponential data growth. Heterogeneous data structures, the continuously growing variety of implementation frameworks, and the lack of standards for the semantic description of AI solution components, the effort required to manage and share datasets and models between stakeholders impedes efficient and reproducible progression. This paper addresses the current challenges in industrial AI applications currently hindering their acceptance and widespread adoption in manufacturing. Based on an overview of the AI application lifecycle, we present our approach for AI asset meta-data management utilizing the technical concept of the Asset Administration Shell (AAS). Following the definition of the AAS as a reference implementation of the digital twin that provides a digital representation of physical assets and their properties, we propose an AAS for AI assets. The AI AAS maps relevant properties of an AI model together with properties of the corresponding dataset and learning algorithm in order to integrate the AI lifecycle in the Industry4.0 ecosphere.}
}
@article{MUEHLBAUER2022364,
title = {Data driven logistics-oriented value stream mapping 4.0: A guideline for practitioners},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {16},
pages = {364-369},
year = {2022},
note = {18th IFAC Workshop on Control Applications of Optimization CAO 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.051},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322012277},
author = {K. Muehlbauer and M. Wuennenberg and S. Meissner and J. Fottner},
keywords = {Data Maturity, Data Science, Inconsistencies, Logistics in Manufacturing, Machine Learning, Optimization, Value Stream Management},
abstract = {The use of data-oriented approaches like data mining or machine learning has an increasing potential for application in the planning and control of production and logistics systems. The growing amount of digital process information helps to expand the existing process understanding in order to determine weaknesses in the process landscape. Due to the extensive complexity within production and logistics systems, a comprehensive approach is required to ensure a systematic analysis. This article presents an extension of the value stream method based on the existing approaches that is intended to support operators of logistics systems in the company. This methodology collects all relevant process information and validates the data maturity. Hence, indications for the use of data-oriented approaches can be given and potential machine learning-based analysis scenarios can be derived.}
}
@article{BRACCONI2022109148,
title = {Intensification of catalytic reactors: A synergic effort of Multiscale Modeling, Machine Learning and Additive Manufacturing},
journal = {Chemical Engineering and Processing - Process Intensification},
volume = {181},
pages = {109148},
year = {2022},
issn = {0255-2701},
doi = {https://doi.org/10.1016/j.cep.2022.109148},
url = {https://www.sciencedirect.com/science/article/pii/S025527012200352X},
author = {Mauro Bracconi},
keywords = {Catalytic reactors, Process intensification, Multiscale modeling, Additive manufacturing, Machine learning},
abstract = {The intensification of catalytic reactors is expected to play a crucial role to address the challenges that the chemical industry is facing in the transition to more sustainable productions. An advanced design paradigm is necessary to develop customized and process-tailored reactor solutions able to provide the optimal operating conditions, transport properties and geometry. This can be achieved by a detailed understanding of the catalyst functionality in the reactive environment. Multiscale Modeling provides such in-depth insights into the complex physical-chemical phenomena enabling to achieve a first-principles-based understanding and design of the most suitable reactor geometry and configuration. To overcome the intrinsic complexity of the approach, Machine Learning can be synergically employed to reduce the computational cost fostering the inclusion of detailed numerical simulations since the early stage of the design process. Moreover, hybrid machine learning models trained with the data and enforced by the physics are envisioned to assist the work of designers facilitating the development of disruptive intensified solutions. The manufacturing of these unconventional systems requires adequate techniques. Additive Manufacturing is showing enormous potential in this direction and their future developments are expected to make it possible to routinely fabricate intensified reactors.}
}
@article{DONG2022300,
title = {The market effectiveness of regulatory certification for sustainable food supply: A conjoint analysis approach},
journal = {Sustainable Production and Consumption},
volume = {34},
pages = {300-309},
year = {2022},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2022.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S2352550922002603},
author = {Xuemei Dong and Baichen Jiang},
keywords = {Sustainable food supply, Intermediary, Policy integration, Consumer decision, Heuristic-systematic model},
abstract = {The commitment to sustainability follows the consensus of adopting integrated governance to mitigate uncertainty about the whole food system. Thus, a coherent intermediary framework emerges in response to the food system crisis, pledging a “farm to fork” regulatory certification with enhanced credibility and reduced search cost compared with the fragmented intermediation. Yet, the institutional practice is scarce, and the market effectiveness remains underexamined. Drawing upon the heuristic-systematic model from the dual-process theory, this research builds the view of sustainable choice as a hierarchical decision process. The regulatory certification enacts an easy but reliable heuristic judgment, establishing an adaptive base for further systematic decisions. Specifically, common barriers (such as the status quo bias and price sensitivity) to sustainable action are not firmly established as typically assumed; instead, consumer reluctance to action arises from the lacking of a dominant heuristic. Consumers with an available regulatory certification experience less difficult trade-offs and have stronger preferences for technology innovation, such as digital production for food industry 4.0. Potentially, the segment that values regulatory certification as the crucial heuristic accounts for approximately 50%. This research provides insightful implications on how governance reforms spark a shift to sustainable food supply and the market effectiveness of inviting consumers to join the initiatives.}
}
@incollection{FREMIN2022673,
title = {Chapter 19 - Technical (engineering) advancements enabling deepwater exploration and production},
editor = {Jon R. Rotzien and Cindy A. Yeilding and Richard A. Sears and F. Javier Hernández-Molina and Octavian Catuneanu},
booktitle = {Deepwater Sedimentary Systems},
publisher = {Elsevier},
pages = {673-692},
year = {2022},
isbn = {978-0-323-91918-0},
doi = {https://doi.org/10.1016/B978-0-323-91918-0.00019-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919180000190},
author = {Lori Fremin and Richard A. Sears and Charlie Williams},
keywords = {Deepwater, Well engineering, Production engineering, Offshore structure, Development planning, Pipelines, Flow assurance, Surveillance, Remote monitoring, Emerging technology},
abstract = {The oil and gas industry has a long history of developing the technologies necessary to exploit discovered resources. In this regard, deepwater is no exception. Since first stepping offshore in the early 20th century, the industry has had to solve a variety of technical challenges to drill in deeper water depths and economically develop and produce the discovered resources. This chapter looks at some of these important engineering advancements, describing them in the context of the deepwater reservoirs and fluids that are characterized in the other chapters of this book. Without these technology advancements, deepwater oil and gas resources could not have been successfully exploited and much of the detailed geological understanding of deepwater reservoirs and petroleum systems would never have been developed. This chapter is dedicated to the engineering pioneers in the oil and gas industry who were willing to step off the beach into the sea in the early 1900s, move over the horizon and out of sight of land in the 1940s, and 50years later, explore for and develop oil and gas resources in nearly 10,000ft of water.}
}
@article{DANTRACCOLI2022101512,
title = {Maps of relative floristic ignorance and virtual floristic lists: An R package to incorporate uncertainty in mapping and analysing biodiversity data},
journal = {Ecological Informatics},
volume = {67},
pages = {101512},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2021.101512},
url = {https://www.sciencedirect.com/science/article/pii/S1574954121003034},
author = {Marco D'Antraccoli and Gianni Bedini and Lorenzo Peruzzi},
keywords = {Algorithms, Flora, GIS, Occurrence records, Spatial and temporal uncertainties},
abstract = {The vast amount of occurrence records currently available offers increasing opportunities for biodiversity data analyses. This amount of data poses new challenges for the reliability and correct interpretation of the results. Indeed, to safely deal with occurrence records, their uncertainty and associated biases should be taken into account. We developed an R package to explicitly include spatial and temporal uncertainties during the mapping and listing of plant occurrence records for a given study area. Our workflow returns two objects: (a) a ‘Map of Relative Floristic Ignorance’ (MRFI), which represents the spatial distribution of the lack of floristic knowledge; (b) a ‘Virtual Floristic List’ (VFL), i.e. a list of taxa potentially occurring in the area with an associated probability of occurrence. The method implemented in the package can manage a large amount of occurrence data and represents relative floristic ignorance across a study area with a sustainable computational effort. Several parameters can be set by the user, conferring high flexibility to the method. Uncertainty is not avoided, but incorporated into biodiversity analyses through appropriate methodological approaches and innovative spatial representations. Our study introduces a workflow that pushes forward the analytical capacities to deal with uncertainty in biological occurrence records, allowing to produce more accurate outputs.}
}
@article{LEE2022102590,
title = {When does AI pay off? AI-adoption intensity, complementary investments, and R&D strategy},
journal = {Technovation},
volume = {118},
pages = {102590},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102590},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222001377},
author = {Yong Suk Lee and Taekyun Kim and Sukwoong Choi and Wonjoon Kim},
keywords = {Artificial intelligence, High-tech ventures, Firm performance, Complementary investment, R&D strategy},
abstract = {This paper examines how high-tech venture performance varies with AI-adoption intensity. We find that firm revenue increases only after sufficient investment in AI, and the benefits of AI adoption are greater at firms that also invest in complementary technologies and pursue internal R&D strategy. Specifically, AI adoption at low levels does not suggest significant revenue growth, but, as the intensity of AI adoption increases revenue growth occurs. We find that such performance gains from adoption is larger among firms that invest in complementary technologies such as cloud computing and database systems. Moreover, the positive relationship between AI adoption intensity and revenue growth is stronger among firms that pursue a more exclusive R&D strategy specific to the venture.}
}
@article{BLANKENBERG2022314,
title = {Using a graph database for the ontology-based information integration of business objects from heterogenous Business Information Systems},
journal = {Procedia Computer Science},
volume = {196},
pages = {314-323},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022420},
author = {Carolin Blankenberg and Berit Gebel-Sauer and Petra Schubert},
keywords = {IS Integration, Enterprise Knowledge Graph, Ontology, Graph Database},
abstract = {This paper reports on findings from a project on information integration from multiple Business Information Systems with the help of a user-specific Enterprise Knowledge Graph. Most ERP systems currently in use store information objects in relational databases. Research in Web Sciences has shown that graph structures present information in a more intuitive way that is easier to interpret for humans. Following a DSR approach, we developed a concept for storing an ontology in a graph database that allows us to map ERP objects and load them at runtime. This allows the end user to navigate through the graph structure, thus providing an intuitive and quick access to essential job-related information. We evaluated the suggested concept with a prototype following the paradigm of polyglot persistence; the prototype was equipped with a graph database to store the company-specific ontology in its native form. The program code was encapsulated into a separate module following a service-oriented software design.}
}
@article{WANG202224,
title = {DeepCS: Training a deep learning model for cervical spondylosis recognition on small-labeled sensor data},
journal = {Neurocomputing},
volume = {472},
pages = {24-34},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016829},
author = {Nana Wang and Chunjie Luo and Xi Huang and Yunyou Huang and Jianfeng Zhan},
keywords = {Cervical spondylosis recognition, High-dimensional time series sensor data, Convolutional neural network, Network architecture search, Feature extraction},
abstract = {Cervical spondylosis (CS) recognition systems provide regular screening services outside of a hospital and promote early detection and treatment of CS. However, in this paper, we propose a deep learning-based CS recognition system. Concerning the state-of-the-art and state-of-the-practice systems, the innovations of our approaches and algorithms are as follows: First, to elevate the reliance upon the sample number required for training the high-quality model, we reduce sample dimension and find optimal neural network architectures to reduce the number of model parameters to fit. Second, we incorporate multi-stream parallel network architecture search with multi-view feature extraction by converting time series classification into an image classification task. Specifically, five feature extraction methods (time-domain, frequency-domain, time–frequency domain, model-based, nonlinear feature extraction) are firstly utilized to extract features from multiple perspectives and form low-dimensional data set with multi-properties. Third, we reorganize low-dimensional data into image one representing the spatio-temporal relationship of muscle activity pattern. Finally, a multi-stream parallel network architecture search is proposed to use a bypass mechanism for optimal neural network architecture, each of which processes a kind of features mentioned above with an idea of the sparse connection of convolution neural network. The results on the real-world data set show that our CS recognition system achieves the average accuracy of 95.54%, average sensitivity of 99.09%, and average specificity of 90.00%, outperforming the state-of-the-art ones.}
}
@article{LV2022114746,
title = {Causal effect of PM1 on morbidity of cause-specific respiratory diseases based on a negative control exposure},
journal = {Environmental Research},
pages = {114746},
year = {2022},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2022.114746},
url = {https://www.sciencedirect.com/science/article/pii/S0013935122020734},
author = {Shiyun Lv and Xiangtong Liu and Zhiwei Li and Feng Lu and Moning Guo and Mengmeng Liu and Jing Wei and Zhiyuan Wu and Siqi Yu and Shihong Li and Xia Li and Wenkang Gao and Lixin Tao and Wei Wang and Jinyuan Xin and Xiuhua Guo},
keywords = {PM, Morbidity, Causal effect, Cause-specific respiratory diseases},
abstract = {Background
Extensive studies have linked PM2.5 and PM10 with respiratory diseases (RD). However, few is known about causal association between PM1 and morbidity of RD. We aimed to assess the causal effects of PM1 on cause-specific RD.
Methods
Hospital admission data were obtained for RD during 2014 and 2019 in Beijing, China. Negative control exposure and extreme gradient boosting with SHapley Additive exPlanation was used to explore the causality and contribution between PM1 and RD. Stratified analysis by gender, age, and season was conducted.
Results
A total of 1,183,591 admissions for RD were recorded. Per interquartile range (28 μg/m3) uptick in concentration of PM1 corresponded to a 3.08% [95% confidence interval (CI): 1.66%–4.52%] increment in morbidity of total RD. And that was 4.47% (95% CI: 2.46%–6.52%) and 0.15% (95% CI: 1.44%-1.78%), for COPD and asthma, respectively. Significantly positive causal associations were observed for PM1 with total RD and COPD. Females and the elderly had higher effects on total RD, COPD, and asthma only in the warm months (Z = 3.03, P = 0.002; Z = 4.01, P < 0.001; Z = 3.92, P < 0.001; Z = 2.11, P = 0.035; Z = 2.44, P = 0.015). Contribution of PM1 ranked first, second and second for total RD, COPD, and asthma among air pollutants.
Conclusion
PM1 was causally associated with increased morbidity of total RD and COPD, but not causally associated with asthma. Females and the elderly were more vulnerable to PM1-associated effects on RD.}
}
@incollection{FRANCHAK202261,
title = {Chapter Three - Beyond screen time: Using head-mounted eye tracking to study natural behavior},
editor = {Rick O. Gilmore and Jeffrey J. Lockman},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {62},
pages = {61-91},
year = {2022},
booktitle = {New Methods and Approaches for Studying Child Development},
issn = {0065-2407},
doi = {https://doi.org/10.1016/bs.acdb.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065240721000409},
author = {John M. Franchak and Chen Yu},
keywords = {Eye movements, Head-mounted eye tracking, Mobile eye tracking, Ecological validity, Perceptual-motor development, Joint attention, Language development, Computer vision, Social attention},
abstract = {Head-mounted eye tracking is a new method that allows researchers to catch a glimpse of what infants and children see during naturalistic activities. In this chapter, we review how mobile, wearable eye trackers improve the construct validity of important developmental constructs, such as visual object experiences and social attention, in ways that would be impossible using screen-based eye tracking. Head-mounted eye tracking improves ecological validity by allowing researchers to present more realistic and complex visual scenes, create more interactive experimental situations, and examine how the body influences what infants and children see. As with any new method, there are difficulties to overcome. Accordingly, we identify what aspects of head-mounted eye-tracking study design affect the measurement quality, interpretability of the results, and efficiency of gathering data. Moreover, we provide a summary of best practices aimed at allowing researchers to make well-informed decisions about whether and how to apply head-mounted eye tracking to their own research questions.}
}
@article{DOORNENBAL2022101515,
title = {Opening the black box: Uncovering the leader trait paradigm through machine learning},
journal = {The Leadership Quarterly},
volume = {33},
number = {5},
pages = {101515},
year = {2022},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2021.101515},
url = {https://www.sciencedirect.com/science/article/pii/S1048984321000205},
author = {Brian M. Doornenbal and Brian R. Spisak and Paul A. {van der Laken}},
keywords = {Leader trait paradigm, Machine learning, Complexity, Interpretability, Personality},
abstract = {Understanding the traits that define a leader is a perennial quest. An ongoing debate surrounds the complexity required to unravel the leader trait paradigm. With the advancement of machine learning, scholars are now better equipped to model leadership as an outcome of complex patterns in traits. However, interpreting those models is often harder. In this paper, we guide researchers in the application of machine learning techniques to uncover complex relationships. Specifically, we demonstrate how applying machine learning can help to assess the complexity of a relationship and show techniques that help interpret the outcomes of “black box” machine learning algorithms. While demonstrating techniques to uncover complex relationships, we are using the Big Five Inventory and need for cognition to predict leadership role occupancy. Among our sample (n = 3385), we find that the leader trait paradigm can benefit from modeling complexity beyond linear effects and generate several interpretable results.}
}
@article{SUN202290,
title = {Informed Graph Convolution Networks for Multilingual Short Text Understanding},
journal = {Procedia Computer Science},
volume = {207},
pages = {90-99},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.041},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009140},
author = {Yaru Sun and Ying Yang and Dawei Yang},
keywords = {Sub-word embedding, Graph neural network, Attention mechanism, Few samples learning, Multilingual text classification},
abstract = {In the open domain environment, the state-of-the-art models cannot process analyze insufficient training data correctly. We propose an adaptive graph convolution network with informed machine learning for multilingual short text understanding to tackle these problems. Specifically, The prior knowledge to guide the graph neural network to extract sentence topics. We construct category anchor words as prior category keywords, prior category keywords and training data as independent information sources, and prior knowledge participates in the training of graph neural network. Moreover, we integrate the attention mechanism in the training process, so that the model can pay attention to task-related information adaptively. We explain the build blocks and present the integrated knowledge representation. The experimental results on the Multilingual Short Text (MST), THUCNews and AGNews datasets show that our method outperforms most of the existing methods.}
}
@article{ERHART2022106686,
title = {Application of North European characterisation factors, population density and distance-to-coast grid data for refreshing the Swedish human toxicity and ecotoxicity footprint analysis},
journal = {Environmental Impact Assessment Review},
volume = {92},
pages = {106686},
year = {2022},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2021.106686},
url = {https://www.sciencedirect.com/science/article/pii/S0195925521001360},
author = {Szilárd Erhart and Kornél Erhart},
keywords = {Chemical footprint, Hazardous chemicals, -PRTR, Human toxicity, Ecotoxicity, USEtox, ESG rating, Sustainability rating, European Green Deal, Sustainable Devoelopment Goals (SDGs)},
abstract = {Here, we develop further the national chemical footprint assessment methods using Sweden as an example to enhance the precision of calculations. First, we integrate grid data on population density and distance-to-seacoast into the analytical framework to better match the European Pollutant Release and Transfer Register on the sub-compartment level with USEtox toxicity characterisation factors. Second, we use the latest USEtox 2.12 model version and its more punctual North European characterisation factors. Third, we conduct trend and geographic analysis and rank Swedish facilities in terms of toxicity potential. We show that total human toxicity potential in Sweden was smaller than previously estimated when using the North European USEtox landscape settings and sloped downwards over time. We confirm toxicity potential of major pollutants in previous research papers (Zn, Hg, Pb, Ni) and find that Hg’s relative human toxicity potential in a longer period can be larger than previously estimated on shorter periods. Human toxicity is estimated to be mostly non-cancer type in Sweden. Results are largely invariant to the choice of air sub-compartments. Companies in the metals manufacturing sector are estimated to have the largest human toxicity potential in Sweden in the period between 2001 and 2017 and companies in the paper manufacturing industry have the largest ecotoxicity potential.}
}
@article{LIOTTA2022103832,
title = {Testing the monocentric standard urban model in a global sample of cities},
journal = {Regional Science and Urban Economics},
volume = {97},
pages = {103832},
year = {2022},
issn = {0166-0462},
doi = {https://doi.org/10.1016/j.regsciurbeco.2022.103832},
url = {https://www.sciencedirect.com/science/article/pii/S0166046222000710},
author = {Charlotte Liotta and Vincent Viguié and Quentin Lepetit},
keywords = {Urbanization, Standard urban model, Urban spatial structure, Between-country comparisons},
abstract = {Using a unique dataset containing gridded data on population densities, rents, housing sizes, and transportation in 192 cities worldwide, we investigate the empirical relevance of the monocentric standard urban model (SUM). Overall, the SUM seems surprisingly capable of capturing the inner structure of cities, both in developed and developing countries. As expected, cities spread out when they are richer, more populated, and when transportation or farmland is cheaper. Respectively 100% and 87% of the cities exhibit the expected negative density and rent gradients: on average, a 1% decrease in income net of transportation costs leads to a 21% decrease in densities and a 3% decrease in rents per m2. We also investigate the heterogeneity between cities of different characteristics in terms of monocentricity, informality, and amenities.}
}
@article{BRIDGEWOOD20222660,
title = {T Helper 2 IL-4/IL-13 Dual Blockade with Dupilumab Is Linked to Some Emergent T Helper 17‒Type Diseases, Including Seronegative Arthritis and Enthesitis/Enthesopathy, but Not to Humoral Autoimmune Diseases},
journal = {Journal of Investigative Dermatology},
volume = {142},
number = {10},
pages = {2660-2667},
year = {2022},
issn = {0022-202X},
doi = {https://doi.org/10.1016/j.jid.2022.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0022202X22002561},
author = {Charlie Bridgewood and Miriam Wittmann and Tom Macleod and Abdulla Watad and Darren Newton and Kanchan Bhan and Howard Amital and Giovanni Damiani and Sami Giryes and Nicola Luigi Bragazzi and Dennis McGonagle},
abstract = {Dupilumab, an IL-4/IL-13 receptor blocker, has been linked to emergent seronegative inflammatory arthritis and psoriasis that form part of the spondyloarthropathy spectrum. We systematically investigated patterns of immune disorders, including predominantly T helper 17‒(spondyloarthropathy pattern) and T helper 2‒mediated disorders and humoral autoimmune pattern diseases, using VigiBase, the World Health Organization’s global pharmacovigilance of adverse drug reactions. Several bioinformatics databases and repositories were mined to couple dupilumab-related immunopharmacovigilance with molecular cascades relevant to reported findings. A total of 37,848 dupilumab adverse drug reaction cases were reported, with skin, eye, and musculoskeletal systems most affected. Seronegative arthritis (OR = 9.61), psoriasis (OR = 1.48), enthesitis/enthesopathy (OR = 12.65), and iridocyclitis (OR = 3.77) were highly associated. However, ankylosing spondylitis and inflammatory bowel disease were not conclusively associated. Overall, classic polygenic humoral‒mediated autoimmune diseases such as rheumatoid arthritis and systemic lupus erythematosus were not associated with dupilumab use. Pathway analysis identified several biological pathways potentially involved in dupilumab‒associated adverse drug reactions, including the fibroblast GF receptor (in particular, FGFR2) pathway. MicroRNAs analysis revealed the potential involvement of hsa-miR-21-5p and hsa-miR-335-5p. In conclusion, IL-4/IL-13 blockers are not unexpectedly protective against humoral autoimmune diseases but dynamically skew immune responses toward some IL-23/IL-17 cytokine pathway‒related diseases. IL-4/13 axis also plays a role in homeostatic tissue repair and we noted evidence for a link with ocular and arterial pathology.}
}
@article{SASIKUMAR2022,
title = {Blockchain-based trust mechanism for digital twin empowered Industrial Internet of Things},
journal = {Future Generation Computer Systems},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003636},
author = {A. Sasikumar and Subramaniyaswamy Vairavasundaram and Ketan Kotecha and V. Indragandhi and Logesh Ravi and Ganeshsree Selvachandran and Ajith Abraham},
keywords = {Digital twin, Industry 5.0, Industrial Internet of Things, Blockchain technology, Proof of authority},
abstract = {Emerging technologies such as blockchain and digital twins are essential for the rapid development and employment in Industry 5.0 revolution. With the growing number of industrial IoT nodes, optimizing the network and availing of limited resources to enable secure transmission is difficult. A digital twin is the virtual representation of a physical device that completely depends on sensor data for the critical decision-making simulation process. To this end, we combine a blockchain-based distributed network with a digital twin for the Industrial Internet of Things (IIoT) applications. This paper proposes a blockchain-based Proof of Authority (PoA) trust mechanism to provide high-quality services such as security and data privacy in IIoT. Furthermore, to enhance the authority of decentralized digital twin combined blockchain networks, we introduce a Deterministic Pseudo-Random Generation (DPRG) to generate a genesis block. To evaluate the trustworthiness of the proposed system, we simulate the blockchain network with a digital twin using IIoT sensor nodes. The simulation result shows that the PoA-based authority master nodes can reduce energy consumption and enhance data security.}
}
@article{SIALA2022114782,
title = {SHIFTing artificial intelligence to be responsible in healthcare: A systematic review},
journal = {Social Science & Medicine},
volume = {296},
pages = {114782},
year = {2022},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2022.114782},
url = {https://www.sciencedirect.com/science/article/pii/S0277953622000855},
author = {Haytham Siala and Yichuan Wang},
keywords = {Systematic literature review, Responsible artificial intelligence (AI), Health-medicine, AI ethics, Digital health, Virtue ethics},
abstract = {A variety of ethical concerns about artificial intelligence (AI) implementation in healthcare have emerged as AI becomes increasingly applicable and technologically advanced. The last decade has witnessed significant endeavors in striking a balance between ethical considerations and health transformation led by AI. Despite a growing interest in AI ethics, implementing AI-related technologies and initiatives responsibly in healthcare settings remains a challenge. In response to this topical challenge, we reviewed 253 articles pertaining to AI ethics in healthcare published between 2000 and 2020, summarizing the coherent themes of responsible AI initiatives. A preferred reporting items for systematic review and meta-analysis (PRISMA) approach was employed to screen and select articles, and a hermeneutic approach was adopted to conduct systematic literature review. By synthesizing relevant knowledge from AI governance and ethics, we propose a responsible AI initiative framework that encompasses five core themes for AI solution developers, healthcare professionals, and policy makers. These themes are summarized in the acronym SHIFT: Sustainability, Human centeredness, Inclusiveness, Fairness, and Transparency. In addition, we unravel the key issues and challenges concerning responsible AI use in healthcare, and outline avenues for future research.}
}
@article{BARBIERI2022100396,
title = {Decentralized federated learning for extended sensing in 6G connected vehicles},
journal = {Vehicular Communications},
volume = {33},
pages = {100396},
year = {2022},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2021.100396},
url = {https://www.sciencedirect.com/science/article/pii/S2214209621000656},
author = {Luca Barbieri and Stefano Savazzi and Mattia Brambilla and Monica Nicoli},
keywords = {Cooperative sensing, Connected automated driving, Distributed computing, Federated learning, 6G V2X, Consensus},
abstract = {Research on smart connected vehicles has recently targeted the integration of vehicle-to-everything (V2X) networks with Machine Learning (ML) tools and distributed decision making. Among these convergent paradigms, Federated Learning (FL) allows the vehicles to train a deep ML model collaboratively, by exchanging model parameters (i.e., neural network weights and biases), rather than raw sensor data, via V2X links. Early FL approaches resorted to a server-client architecture, where a Parameter Server (PS) acts as edge device to orchestrate the learning process. Novel FL tools, on the other hand, target fog architectures where the model parameters are mutually shared by vehicles and synchronized in a distributed manner via consensus algorithms. These tools do not rely on the PS, but take advantage of low-latency V2X links. In line with this recent research direction, in this paper we investigate distributed FL methods for augmenting the capability of road user/object classification based on Lidar data. More specifically, we propose a new modular, decentralized approach to FL, referred to as consensus-driven FL (C-FL), suitable for PointNet compliant deep ML architectures and Lidar point cloud processing for road actor classification. The C-FL process is evaluated by simulating a realistic V2X network, based on the Collective Perception Service (CPS), for mutual sharing of the PointNet model parameters. The performance validation considers the impact of the degree of connectivity of the vehicular network, the benefits of continual learning over heterogeneous training data, convergence time and loss/accuracy tradeoffs. Experimental results indicate that C-FL complies with the extended sensors use cases for high levels of driving automation, it provides a low-latency training service, compared with existing distributed ML approaches, and it outperforms ego learning with minimal bandwidth usage.}
}
@article{WEI2022466,
title = {Artificial intelligence and SMEs: How can B2B SMEs leverage AI platforms to integrate AI technologies?},
journal = {Industrial Marketing Management},
volume = {107},
pages = {466-483},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2022.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0019850122002474},
author = {Ruiqi Wei and Catherine Pardo},
keywords = {Artificial intelligence, Digital platforms, SME AI adoption, Service dominant logic, Case study research},
abstract = {Drawing on the SDL perspective whereby platforms are seen as multi-layered modular structures, our work adopts a case study approach to investigate how small and medium enterprises (SMEs) can leverage AI platforms to integrate AI Technologies. A digital platform headquartered in China was selected for our research. Twenty-one interviews were conducted with the managers of the platform company, platform user companies, and external module providers. Our findings identify three layers of the AI platform where interactions between platform users and the platform take place. We further identify six roles adopted by these users and highlight the mechanisms of their interactions with the platform. Our findings further explore the conditional factors relating to these interactions, namely, knowledge (operational, functional, and technological), organizational processes, and access to external data. With these findings, our study contributes to the platform literature by highlighting its multi-layer architecture. Our study also contributes to the literature on service platforms by describing five types of interactions experienced by platform users when leveraging platforms. Finally, we contribute to the literature on SME AI adoption by revealing the role of platforms in the process.}
}
@article{CIFUENTESGONZALEZ2022349,
title = {Colombian Ocular Infectious Epidemiology Study (COIES): Ocular Toxoplasmosis Incidence and Sociodemographic Characterization, 2015-2019},
journal = {International Journal of Infectious Diseases},
volume = {117},
pages = {349-355},
year = {2022},
issn = {1201-9712},
doi = {https://doi.org/10.1016/j.ijid.2022.02.028},
url = {https://www.sciencedirect.com/science/article/pii/S1201971222001035},
author = {Carlos Cifuentes-González and Estefanía Zapata-Bravo and María Camila Sierra-Cote and Laura Boada-Robayo and Ángela Paola Vargas-Largo and Juliana Reyes-Guanes and Alejandra de-la-Torre},
keywords = {Incidence Study, Ocular Toxoplasmosis, Ophthalmology, Toxoplasmosis},
abstract = {Objectives
This study aims to describe the incidence of ocular involvement in patients with toxoplasmosis and describe the sociodemographic characteristics by age, sex, and region in Colombia, based on the National Health Registry of data between January 1, 2015, and December 31, 2019.
Methods
We conducted a cross-sectional study using the Integrated Social Protection Information System database from the Colombian Ministry of Health, the unique official database in the country. We used the International Classification of Diseases for all codes of toxoplasmosis with a specific filter for ocular toxoplasmosis (OT) from 2015 to 2019 to estimate the incidence and the demographic status of the disease in Colombia.
Results
During the 5 years of study, the crude unadjusted incidence of OT was 42.02 (Confidence Interval 30.29-56.19) cases in 1,000 patients with toxoplasmosis per year, showing a significant increase of incidence when comparing the year 2019 to the year 2015. There was a predominance of female patients (58% of the cases). The distribution by age shows an increase in cases of the disease in subjects aged 15 to 49 years (65.2%). The geographic analysis showed a higher proportion of cases in the Andean region, followed by the Pacific and the Atlantic regions.
Conclusion
This is the first study that determines the epidemiological characteristics of OT based on a National Health database in Colombia, showing a public health problem and evidencing the neediness of solidifying preventive and screening strategies in the Colombian population.}
}
@article{TOP2022106909,
title = {Cultivating FAIR principles for agri-food data},
journal = {Computers and Electronics in Agriculture},
volume = {196},
pages = {106909},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106909},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922002265},
author = {Jan Top and Sander Janssen and Hendrik Boogaard and Rob Knapen and Görkem Şimşek-Şenel},
keywords = {Agriculture, Food supply chain, Data sharing, FAIR principles, Ontology, Controlled vocabulary},
abstract = {Data generated by the global food system is crucial in the transformation towards sustainable, resilient, and high-quality food production. Although the amount of potentially useful data is growing rapidly, its (re)use is still limited. The FAIR-principles have been developed for making data findable, accessible, interoperable, and reusable both by humans and machines. This paper explores the further operationalization of the FAIR principles in agriculture and food. Experience shows that several conditions must be fulfilled before data can be effectively shared and reused. First, automated tools must be available for data providers and users. Secondly, we need a community-based approach in developing tools and vocabularies. Thirdly, data cannot be shared by an open-by-default policy only. Finally, scientific insight is needed in how data is actually (re)used in scientific communities. We conclude that bringing the FAIR-principles to full maturity requires a fair balance of efforts within the agri-food communities, supported by a proper infrastructure.}
}
@article{HIMEUR2022129786,
title = {Techno-economic assessment of building energy efficiency systems using behavioral change: A case study of an edge-based micro-moments solution},
journal = {Journal of Cleaner Production},
volume = {331},
pages = {129786},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129786},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621039615},
author = {Yassine Himeur and Abdullah Alsalemi and Faycal Bensaali and Abbes Amira and Iraklis Varlamis and George Bravos and Christos Sardianos and George Dimitrakopoulos},
keywords = {Energy efficiency, Behavioral change, Anomaly detection, Recommendation generation, Business model, Market drivers/barriers},
abstract = {Energy efficiency based on behavioral change has attracted increasing interest in recent years, although, solutions in this area lack much needed techno-economic analysis. That is due to the absence of both prospective studies and consumer awareness. To close such gap, this paper proposes the first techno-economic assessment of a behavioral change-based building energy efficiency solution, to the best of the authors’ knowledge. From the one hand, the technical assessment is conducted through (i) introducing a novel edge-based energy efficiency solution; (ii) analyzing energy data using machine learning tools and micro-moments, and producing intelligent, personalized, and explainable action recommendations; and (iii) proceeding with a technical evaluation of four application scenarios, i.e., data collection, data analysis and anomaly detection, recommendation generation, and data visualization. On the other hand, economic assessment is performed by examining the marketability potential of the proposed solution via a market and research analysis of behavioral change-based systems for energy efficiency applications. Also, various factors impacting the commercialization of the final product are investigated before providing recommended actions to ensure its potential marketability via conducting a Go/No-Go evaluation. In conclusion, the proposed solution is designed at a low cost and can save up to 28%–68% of the consumed energy, which results in a Go decision to commercialize the technology.}
}
@article{MERTES2022578,
title = {Evaluation of 5G-capable framework for highly mobile, scalable human-machine interfaces in cyber-physical production systems},
journal = {Journal of Manufacturing Systems},
volume = {64},
pages = {578-593},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001376},
author = {Jan Mertes and Daniel Lindenschmitt and Masoud Amirrezai and Nima Tashakor and Moritz Glatt and Christian Schellenberger and Swati Matwankar Shah and Ali Karnoub and Christopher Hobelsberger and Li Yi and Stefan Götz and Jan C. Aurich and Hans D. Schotten},
keywords = {Human-machine interface, Framework, 5G, Human-centricity, CPPS, Brain-computer interface},
abstract = {Human cyber-physical production systems (HCPS) - as an extension of cyber-physical production systems (CPPS) - focus on the human being in the system and the development of socio-technical systems. Humans and therefore anthropogenic behavior have to be integrated into the manufacturing system and its processes. Due to the shift towards customized products, CPPS require high reconfigurability, flexibility and individual manufacturing processes. As a result, the role of and requirements for humans are fundamentally changing, creating the necessity for new interfaces to interact with machines within reconfigurable manufacturing process. Scalable human machine interfaces (HMI) are needed that incorporate emerging technologies as well as allowing mobility for the operator while interacting with different machines. Therefore, new approaches for HMI in the context of CPPS and HCPS are needed, which are simultaneously sufficiently mobile, scalable, and modular as well as human centered. High mobility of HMIs can be ensured by using the 5G communication standard that enables wireless migration of computational resources to an edge server with high reliability, low latency, and high data rates. This paper develops, implements, and evaluates a 5G-based, framework for highly mobile, scalable HMI in CPPS by utilizing the new 5G communication technology. The capabilities of the framework and of the new communication standard are demonstrated and evaluated for a use case, where a brain-computer interface (BCI) is used to control a robot arm. For better accuracy, the BCI is supported by an eye tracker and visual feedback is received via an augmented reality environment, and all devices are embedded via 5G communication. In particular, the influence of 5G communication on the system performance is examined, evaluated, and discussed. For this purpose, experiments are designed and conducted with different network configurations.}
}
@article{JAMSHIDI2022101672,
title = {Detecting outliers in a univariate time series dataset using unsupervised combined statistical methods: A case study on surface water temperature},
journal = {Ecological Informatics},
volume = {69},
pages = {101672},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101672},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122001224},
author = {Ehsan Jolous Jamshidi and Yusri Yusup and John Stephen Kayode and Mohamad Anuar Kamaruddin},
keywords = {Outlier detection, Ocean temperature, Modified -Score, Exponential moving average method, Univariate data},
abstract = {The surface water temperature is a vital ecological and climate variable, and its monitoring is critical. An extensive sensor network measures the ocean, but outliers pervade the monitoring data due to the sudden change in the water surface level. No single algorithm can identify the outlier efficiently. Hence, this work aims to propose and evaluate the performance of three statistical-based outlier detection algorithms for the water surface temperature: 1) the Standard Z-Score method, 2) the Modified Z-Score coupled with decomposition, and 3) the Exponential Moving Average with the Coupled Modified Z-Score and decomposition. A threshold was set to flag the outlier values. The models' performance was evaluated using the F-score method. Results showed that an increase in outlier detection might reduce the precision of identifying the actual outlier. Based on the results, the Exponential Moving Average with the Modified Z-Score gave the highest F-score value (= 0.83) compared to the other two individual methods. Therefore, this proposed algorithm is recommended to detect outliers efficiently in large surface water temperature datasets.}
}
@article{SHON2022104222,
title = {Autonomous condition monitoring-based pavement management system},
journal = {Automation in Construction},
volume = {138},
pages = {104222},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104222},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522000954},
author = {Heeseung Shon and Chung-Suk Cho and Young-Ji Byon and Jinwoo Lee},
keywords = {Autonomous condition monitoring, Pavement management system, Connected autonomous vehicles, Social cost, Real-time data collection, Prediction, Inspection, Condition-based policies},
abstract = {Due to high operation cost of dedicated inspection vehicles, conventional pavement management systems (PMS) suffer from limited data quantity collected from periodic inspections. However, increasing market penetration of connected autonomous vehicles (CAVs) offers opportunities to monitor pavement conditions more frequently through sensors, including vision cameras and accelerometers, originally installed for autonomous driving. In this paper, we proposed an autonomous condition monitoring-based pavement management system (ACM-PMS) with real-time data collection using CAVs traveling voluntarily. We presented a novel mathematical framework to evaluate potential benefits of ACM-PMS in reducing social costs for both users and agency, systematically accounting for its unique three advantages: (i) large amount of condition data increases prediction model accuracy; (ii) aggregated measurement of current facility condition improves inspection accuracy; (iii) agency can perform maintenance activities at optimal timings, achieving continuous-time and condition-based policies. Results of numerical examples confirm that ACM-PMS significantly reduces the social cost of conventional PMS.}
}
@article{WU2022119191,
title = {Proximal Policy Optimization Algorithm for Dynamic Pricing with Online Reviews},
journal = {Expert Systems with Applications},
pages = {119191},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.119191},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422022096},
author = {Chao Wu and Wenjie Bi and Haiying Liu},
keywords = {Social Learning, Proximal Policy Optimization algorithm, Advantage Actor-Critic algorithm, Dynamic Pricing, Quality-based Review, Value-based Review},
abstract = {This study investigates whether the presence of both quality- and value-based online reviews help firms make decisions. To adapt to a complex real-world environment, we construct two simulated environments with high and low initial consumer-perceived quality and employ a Proximal Policy Optimization algorithm (PPO) to derive optimal pricing strategies. The simulation results show that retailers can gain higher revenue by considering quality-based reviews only when the consumers' initial perceived quality is low. In addition, retailers must choose an appropriate promotion method based on the social learning speed of the consumer group. When the social learning speed is slow, retailers should invest more in promotion costs to improve the initial perceived quality of consumers and thus increase revenue. Compared to the Advantage Actor-Critic algorithm, the PPO algorithm exhibits better performance, provides a new approach for complex and continuous revenue management problems, and can be applied to a wider range of areas.}
}
@article{GONG2022101777,
title = {Designing boundary resources in digital government platforms for collaborative service innovation},
journal = {Government Information Quarterly},
pages = {101777},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101777},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22001137},
author = {Yiwei Gong and Xinkai Li},
keywords = {Digital platforms, Digital government, Boundary resources, Collaboration, Collaborative innovation, Design science, Action design research},
abstract = {On the path to creating digital platforms, governments are opening organizational boundaries to cultivate cross-agency collaboration in public service innovation. This collaborative innovation takes place in the context of paradoxical tensions between openness and closeness, stability and flexibility, and generativity and control. The concept of boundary resources provides an instrument to balance these platform design paradoxes. Yet, little is known about designing boundary resources in digital government platforms. We contextualize the concept of boundary resources in digital government platforms to develop design theories. These design theories guide an action design research, where a national tax service platform was redesigned to enable cross-agency collaboration for service innovation. Design knowledge was thus generated into three design principles for the design of boundary resources in government platforms. These principles help in making design decisions by which government platform owners use different boundary resources to address the three paradoxes. We suggest that government platform owners cultivate collaborative platform ecosystems and define unified data standards to address the openness, design data modularity and interfaces for resourcing complementors, and use data relationships and accessibility as control points for securing the platform.}
}
@article{YANG2022113813,
title = {Social influence-based contrast language analysis framework for clinical decision support systems},
journal = {Decision Support Systems},
volume = {159},
pages = {113813},
year = {2022},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113813},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622000847},
author = {Xingwei Yang and Alexandra Joukova and Anteneh Ayanso and Morteza Zihayat},
keywords = {Contrast language analysis, Clinical Decision Support System (CDSS), Depression detection, Social network},
abstract = {Depression is a leading mental health problem affecting 300 million people globally. Recent studies show that social networks provide a tremendous potential for mental health professionals as a source of supplemental information about their patients. This study presents a methodological framework for clinical decision support systems (CDSSs) through analysis of social network data to distinguish the language usage of individuals with early signs of depression (i.e., contrast language analysis). By analyzing the contrast language patterns of different user groups, we are able to uncover constructive and actionable insights into the pain points and characteristics of users with signs of depression as decision support mechanisms for clinicians during intervention, (early) diagnosis and treatment plans. First, we discover terms that represent contrasting language by analyzing the percentage difference of terms in two user groups, labeled as”depressed” and”non-depressed” for ease of reference. Second, by building topic models based on social network contents, the topic-level contrast features are discovered. Finally, we consider the structure of the social network to discover the network-level contrast features. To illustrate the effectiveness of the proposed framework, we present a case study on early depression detection using a real-world dataset. The proposed framework has methodological contributions in enhancing the features and functionalities of CDSS for clinicians. It also contributes to evidence-based health research through cost-effective data and analytical insights that can supplement or improve the traditional survey and time-consuming interview methods.}
}
@article{LIN2022149,
title = {Data fusion and transfer learning empowered granular trust evaluation for Internet of Things},
journal = {Information Fusion},
volume = {78},
pages = {149-157},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001780},
author = {Hui Lin and Sahil Garg and Jia Hu and Xiaoding Wang and Md. Jalil Piran and M. Shamim Hossain},
keywords = {Data fusion, Trust evaluation, Transfer learning, Deep reinforcement learning, Privacy preservation, Internet of Things},
abstract = {In the Internet of Things (IoT), a huge amount of valuable data is generated by various IoT applications. As the IoT technologies become more complex, the attack methods are more diversified and can cause serious damages. Thus, establishing a secure IoT network based on user trust evaluation to defend against security threats and ensure the reliability of data source of collected data have become urgent issues, in this paper, a Data Fusion and transfer learning empowered granular Trust Evaluation mechanism (DFTE) is proposed to address the above challenges. Specifically, to meet the granularity demands of trust evaluation, time–space empowered fine/coarse grained trust evaluation models are built utilizing deep transfer learning algorithms based on data fusion. Moreover, to prevent privacy leakage and task sabotage, a dynamic reward and punishment mechanism is developed to encourage honest users by dynamically adjusting the scale of reward or punishment and accurately evaluating users’ trusts. The extensive experiments show that: (i) the proposed DFTE achieves high accuracy of trust evaluation under different granular demands through efficient data fusion; (ii) DFTE performs excellently in participation rate and data reliability.}
}
@article{MAHARANA202291,
title = {A review: Data pre-processing and data augmentation techniques},
journal = {Global Transitions Proceedings},
volume = {3},
number = {1},
pages = {91-99},
year = {2022},
note = {International Conference on Intelligent Engineering Approach(ICIEA-2022)},
issn = {2666-285X},
doi = {https://doi.org/10.1016/j.gltp.2022.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S2666285X22000565},
author = {Kiran Maharana and Surajit Mondal and Bhushankumar Nemade},
keywords = {Data augmentation, Data cleaning, Data oversampling, Data pre-processing, Data wraping},
abstract = {This review paper provides an overview of data pre-processing in Machine learning, focusing on all types of problems while building the machine learning problems. It deals with two significant issues in the pre-processing process (i). issues with data and (ii). Steps to follow to do data analysis with its best approach. As raw data are vulnerable to noise, corruption, missing, and inconsistent data, it is necessary to perform pre-processing steps, which is done using classification, clustering, and association and many other pre-processing techniques available. Poor data can primarily affect the accuracy and lead to false prediction, so it is necessary to improve the dataset's quality. So, data pre-processing is the best way to deal with such problems. It makes the knowledge extraction from the data set much easier with cleaning, Integration, transformation, and reduction methods. The issue with Data missing and significant differences in the variety of data always exists as the information is collected through multiple sources and from a real-world application. So, the data augmentation approach generates data for machine learning models. To decrease the dependency on training data and to improve the performance of the machine learning model. This paper discusses flipping, rotating with slight degrees and others to augment the image data and shows how to perform data augmentation methods without distorting the original data.}
}
@article{PANG2022105400,
title = {Geothermal regime and implications for basin resource exploration in the Qaidam Basin, northern Tibetan Plateau},
journal = {Journal of Asian Earth Sciences},
volume = {239},
pages = {105400},
year = {2022},
issn = {1367-9120},
doi = {https://doi.org/10.1016/j.jseaes.2022.105400},
url = {https://www.sciencedirect.com/science/article/pii/S1367912022003315},
author = {Yumao Pang and Kaizhen Zou and Xingwei Guo and Yan Chen and Jian Zhao and Fei Zhou and Jun Zhu and Lifeng Duan and Guoxin Yang},
keywords = {Qaidam Basin, Geothermal regime, Heat flow, Thermal structure, Petroleum distribution, Geothermal resources},
abstract = {The Qaidam Basin was formed under the background of the continuous uplift of the Tibetan Plateau, and its tectonic location imparts unique geothermal regime. The geothermal regime of the Qaidam Basin was studied based on oil-test static temperatures from 60 wells, 195 thermal conductivities measured by the optical scanning method, and 142 radiogenic heat production values. The geothermal gradient in the Qaidam Basin is 17.1–47.6 °C/km, with an average of 31.3 °C/km, and the thermal conductivity is 0.523–4.379 W/m‧K. The heat flow is 28.3–83.1 mW/m2, with an average of 59.6 mW/m2, and it is “high in the west and low in the east.” The heat flow can exceed 70 mW/m2 in the northern marginal fold-and-thrust belt of western Kunlun and Nanyishan and generally exceeds 65 mW/m2 in the Mangya Depression. The average radiogenic heat production rate (HPR) is 2.53 μW/m3, which is close to the HPR of granite in northern Tibetan Plateau. The crustal and mantle heat flows in the Qaidam Basin are 32.9 mW/m2 and 26.7 mW/m2, respectively. The crustal contribution to the surface heat flow was approximately 55 %. The geothermal regime may be dominated by lithospheric thickness, HPR of sedimentary cover, and extra heat production related to late Cenozoic tectonic movement. The proven hydrocarbon reserves are primarily distributed around hydrocarbon-generating kitchens, and the existence of abnormally high-temperature zones significantly influences hydrocarbon distribution. The Qaidam Basin satisfies the fundamental temperature conditions for the development of low- and medium-temperature geothermal resources using abandoned oil and gas drilling wells.}
}
@article{COLLINS2022100518,
title = {Review: Smart agri-systems for the pig industry},
journal = {animal},
volume = {16},
pages = {100518},
year = {2022},
note = {Manipulating Pig Production XVIII: Proceedings of the Eighteenth Biennial Conference of the Australasian Pig Science Association (APSA), 15-18 November 2021, Brisbane, Australia},
issn = {1751-7311},
doi = {https://doi.org/10.1016/j.animal.2022.100518},
url = {https://www.sciencedirect.com/science/article/pii/S1751731122000696},
author = {L.M. Collins and L.M. Smith},
keywords = {Digital farming, Pork, Precision Livestock Farming, Production, Systems approach},
abstract = {The projected rise in the global human population and the anticipated increase in demand for meat and animal products, albeit with a greatly reduced environmental footprint, offers a difficult set of challenges to the livestock sector. Primarily, how do we produce more, but in a way that is healthier for the animals, public, and the environment? Implementing a smart agri-systems approach, utilising multiplatform precision technologies, internet of things, data analytics, machine learning, digital twinning and other emerging technologies can support a more informed decision-making and forecasting position that will allow us to move towards greater sustainability in future. If we look to precision agronomy, there are a wide range of technologies available and examples of how digitalisation and integration of platform outputs can lead to advances in understanding the agricultural system and forecasting upcoming events and performance that have hitherto been impossible to achieve. There is much for the livestock sector and animal scientists to learn from the developments of precision technologies and smart agri-system approaches in the arable and horticultural contexts. However, there are several barriers the livestock sector must overcome: (i) the development and implementation of precision livestock farming technologies that can be easily integrated and analysed without the support of a dedicated data analyst in house; (ii) the lack of extensive validation of many developed and available precision livestock farming technologies means that reliability and accuracy are likely to be compromised when applied in commercial practice; (iii) the best smart agri-systems approaches are reliant on large quantities of data from across a wide variety of conditions, but at present the complications of data sharing, commercial sensitivities, data ownership, and permissions make it challenging to obtain or knit together data from different parts of the system into a comprehensive picture; and (iv) the high level of investment needed to develop and scale these technologies is substantial and represents significant risk for companies when a technology is emerging. Using a case study of the National Pig Centre (a flagship pig research facility in the UK) we discuss how a smart agri-systems approach can be applied in practice to investigate alternative future systems for production, and enable monitoring of these systems as a commercial demonstrator site for future pork production.}
}
@article{LIUTKEVICIUS20222404,
title = {Research Roadmap for Designing a Virtual Competence Assistant for the European Labour Market},
journal = {Procedia Computer Science},
volume = {207},
pages = {2404-2413},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.299},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922011875},
author = {Markko Liutkevičius and Sadok Ben Yahia},
keywords = {Virtual Competence Assistant, Job Matching, Upskilling, Proactivity, AI},
abstract = {This research proposal explores how citizen-centered learning and career advancement can benefit from artificial intelligence, occupational classification frameworks, and the concept of proactive services. In the current literature, there are a lot of machine learning methods used in various job and training recommendation systems to tackle scientific or real-life problems. However, only a few use the existing occupational classifications to classify job and training advertisements or enable cross-regional labor mobility. Additionally, the quality of public e-services regarding labour market services in each European country varies greatly. For example, even though Estonia is typically referred to as being at the forefront of public service digitization and automation, it has not implemented machine learning methods to match job offers or training with candidates. The matching process between vacancy and job seeker is currently carried out with outdated International Standard Classification of Occupations (ISCO) codes, altered to the Estonian Unemployment Insurance Fund's needs. The ISCO code is assigned to each job vacancy by a company and job wish by a citizen manually. Such functionality is rigid and requires the users to define an accurate ISCO code. Even when the filled-in CV consists of detailed previous work experience and educational background, if the ISCO code is not accurate, the e-service will not help the citizen find a new job. Moreover, in today's public employment service portals, there is typically no option to insert specific skills that a citizen has to receive an increased number of accurate job or training recommendations. Consequently, despite many well-established frameworks dealing with competencies and occupations, citizen-centered public services supporting upskilling and finding a new job are inefficient. In this paper, we put forth a research roadmap for investigating how to enable a technical ecosystem using occupational classification frameworks so that citizens, both employed and unemployed, can receive proactive recommendations about upcoming training events and job vacancies. Such a system should be tailored to support citizen life events. For example, it could consider citizens’ previous work and educational background to help with retraining, upskilling, or changing one's career path. Therefore we have initiated a project in collaboration with the Estonian Unemployment Insurance Fund, the Estonian Qualifications Authority, other Estonian public organizations, and partner universities in Latvia and Finland. The research is planned as an action design research to design an artificial intelligence-enabled Virtual Competence Assistant (VCA) for the EU labour market.}
}
@article{MAROUFKHANI2022102622,
title = {Digital transformation in the resource and energy sectors: A systematic review},
journal = {Resources Policy},
volume = {76},
pages = {102622},
year = {2022},
issn = {0301-4207},
doi = {https://doi.org/10.1016/j.resourpol.2022.102622},
url = {https://www.sciencedirect.com/science/article/pii/S030142072200071X},
author = {Parisa Maroufkhani and Kevin C. Desouza and Robert K. Perrons and Mohammad Iranmanesh},
keywords = {Digital technologies, Resource sector, Energy, Oil and gas, Mining, Digitalization},
abstract = {The forces of digital transformation have delivered significant benefits like sustainable development and economic growth in a range of early adopter industries such as retail and manufacturing but, despite these potential benefits, the resource and energy sectors have been relative latecomers to digitalization simply because they are frequently slower to absorb new technologies. Here we present the results of a systematic literature review identifying the ways in which digital technologies have been applied in the oil and gas, mining, and energy domains. We applied content and descriptive analysis to evaluate and discuss 151 academic articles selected from the Scopus database. Two particularly interesting trends emerge from the analysis. First, over 75% of the papers were about the energy sector excluding the oil & gas industry, and only a small minority were from the mining or oil & gas sectors. Second, the most frequently discussed objective of digital transformation was the reduction of operational expenses. By surveying the different ways in which these innovations have been used in these industries and identifying trends and patterns in how digital technologies have been applied, the findings of this review deepen our understanding of the current state of digital technologies within the resource and energy sectors and, in so doing, shine a useful amount of light on the contributions that digital transformation has made to businesses in these sectors. This paper also highlights for future scholars, practitioners, and policymakers the six research areas that they should focus on in the future to help the resource and energy sectors accelerate the digital transformation process and improve their ability to deliver value with these innovations.}
}
@article{SHAHAAB2022101759,
title = {Public service operational efficiency and blockchain – A case study of Companies House, UK},
journal = {Government Information Quarterly},
pages = {101759},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101759},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22000958},
author = {Ali Shahaab and Imtiaz A. Khan and Ross Maude and Chaminda Hewage and Yingli Wang},
keywords = {Blockchain, Distributed ledger technology, Public service operation, Design science, Case study},
abstract = {Despite the increasing interest and exploration of the use of blockchain technology in public service organisations (PSOs), academic understanding of its transformative impact on the operational excellence of PSOs remains limited. This study adopts an action design science research methodology to develop a proof of concept (POC) blockchain based application for Companies House, a government agency that is registering companies across UK. The application addresses the operational challenges of Companies House as well as issues citizens face when accessing its services. We draw from the public value framework proposed by Twizeyimana and Andersson (2019) and demonstrate the significance of the emerging blockchain technology in relation to their democratic practices based on six dimensions. We further discuss the related challenges and barriers for its implementation and evaluate the POC with the stakeholders of Companies House. We also present an illustrative case study, where we explored the appropriateness of the POC in relation to the draft legislation, “Registration of Overseas Entities and Beneficial Owners” (ROEBO) bill which proposes the introduction of a register of the beneficial owners of overseas legal entities that own real estate in the UK. Our research is one of the few studies that will provide in-depth empirical insights about the relationship between blockchain and operational excellence of PSOs.}
}
@article{YU2022104002,
title = {Developing an ETL tool for converting the PCORnet CDM into the OMOP CDM to facilitate the COVID-19 data integration},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {104002},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104002},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000181},
author = {Yue Yu and Nansu Zong and Andrew Wen and Sijia Liu and Daniel J. Stone and David Knaack and Alanna M. Chamberlain and Emily Pfaff and Davera Gabriel and Christopher G. Chute and Nilay Shah and Guoqian Jiang},
abstract = {Objective
The large-scale collection of observational data and digital technologies could help curb the COVID-19 pandemic. However, the coexistence of multiple Common Data Models (CDMs) and the lack of data extract, transform, and load (ETL) tool between different CDMs causes potential interoperability issue between different data systems. The objective of this study is to design, develop, and evaluate an ETL tool that transforms the PCORnet CDM format data into the OMOP CDM.
Methods
We developed an open-source ETL tool to facilitate the data conversion from the PCORnet CDM and the OMOP CDM. The ETL tool was evaluated using a dataset with 1000 patients randomly selected from the PCORnet CDM at Mayo Clinic. Information loss, data mapping accuracy, and gap analysis approaches were conducted to assess the performance of the ETL tool. We designed an experiment to conduct a real-world COVID-19 surveillance task to assess the feasibility of the ETL tool. We also assessed the capacity of the ETL tool for the COVID-19 data surveillance using data collection criteria of the MN EHR Consortium COVID-19 project.
Results
After the ETL process, all the records of 1000 patients from 18 PCORnet CDM tables were successfully transformed into 12 OMOP CDM tables. The information loss for all the concept mapping was less than 0.61%. The string mapping process for the unit concepts lost 2.84% records. Almost all the fields in the manual mapping process achieved 0% information loss, except the specialty concept mapping. Moreover, the mapping accuracy for all the fields were 100%. The COVID-19 surveillance task collected almost the same set of cases (99.3% overlaps) from the original PCORnet CDM and target OMOP CDM separately. Finally, all the data elements for MN EHR Consortium COVID-19 project could be captured from both the PCORnet CDM and the OMOP CDM.
Conclusion
We demonstrated that our ETL tool could satisfy the data conversion requirements between the PCORnet CDM and the OMOP CDM. The outcome of the work would facilitate the data retrieval, communication, sharing, and analysis between different institutions for not only COVID-19 related project, but also other real-world evidence-based observational studies.}
}
@incollection{SCHAAP2022131,
title = {Chapter Three - Data management infrastructures and their practices in Europe},
editor = {Giuseppe Manzella and Antonio Novellino},
booktitle = {Ocean Science Data},
publisher = {Elsevier},
pages = {131-193},
year = {2022},
isbn = {978-0-12-823427-3},
doi = {https://doi.org/10.1016/B978-0-12-823427-3.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234273000074},
author = {Dick M.A. Schaap and Antonio Novellino and Michele Fichaut and Giuseppe M.R. Manzella},
keywords = {Data infrastructures, Data management, Marine data, Monitoring services, Standards},
abstract = {Many marine observation and data collection initiatives are promoting data sharing and open data, and their main goals are to support research, make data available to the public, advance innovation, and support the blue economy. Data reuse is the basic concept behind data sharing and open data, a concept also supported by the political agenda. This paper presents and analyses the programs that have enabled coordination of the initiatives relating to the science for sustainable development of the ocean, particularly those infrastructures that allow access to data and information. This paper is using major European programs for the practical implementation of observation data organization and management. These programs and equivalent ones conducted in other areas are providing opportunities to start a new roadmap in the integration of data management infrastructures at the international level.}
}
@article{VAGHARI2022119344,
title = {A multi-site, multi-participant magnetoencephalography resting-state dataset to study dementia: The BioFIND dataset},
journal = {NeuroImage},
volume = {258},
pages = {119344},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119344},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922004633},
author = {Delshad Vaghari and Ricardo Bruna and Laura E. Hughes and David Nesbitt and Roni Tibon and James B. Rowe and Fernando Maestu and Richard N. Henson},
abstract = {Early detection of Alzheimer's Disease (AD) is vital to reduce the burden of dementia and for developing effective treatments. Neuroimaging can detect early brain changes, such as hippocampal atrophy in Mild Cognitive Impairment (MCI), a prodromal state of AD. However, selecting the most informative imaging features by machine-learning requires many cases. While large publically-available datasets of people with dementia or prodromal disease exist for Magnetic Resonance Imaging (MRI), comparable datasets are missing for Magnetoencephalography (MEG). MEG offers advantages in its millisecond resolution, revealing physiological changes in brain oscillations or connectivity before structural changes are evident with MRI. We introduce a MEG dataset with 324 individuals: patients with MCI and healthy controls. Their brain activity was recorded while resting with eyes closed, using a 306-channel MEG scanner at one of two sites (Madrid or Cambridge), enabling tests of generalization across sites. A T1-weighted MRI is provided to assist source localisation. The MEG and MRI data are formatted according to international BIDS standards and analysed freely on the DPUK platform (https://portal.dementiasplatform.uk/Apply). Here, we describe this dataset in detail, report some example (benchmark) analyses, and consider its limitations and future directions.}
}
@article{PRATI2022104791,
title = {Correlates of quality of life, happiness and life satisfaction among European adults older than 50 years: A machine‐learning approach},
journal = {Archives of Gerontology and Geriatrics},
volume = {103},
pages = {104791},
year = {2022},
issn = {0167-4943},
doi = {https://doi.org/10.1016/j.archger.2022.104791},
url = {https://www.sciencedirect.com/science/article/pii/S0167494322001789},
author = {Gabriele Prati},
keywords = {Well-being, Machine learning, Random forest, Gradient boosting},
abstract = {Background and objectives
Previous research has documented the role of different categories of psychosocial factors (i.e., sociodemographic factors, personality, subjective life circumstances, activity, physical health, and childhood circumstances) in predicting subjective well-being and quality of life among older adults. No previous study has simultaneously modeled a large number of these psychosocial factors using a well-powered sample and machine learning algorithms to predict quality of life, happiness, and life satisfaction among older adults. The aim of this paper was to investigate the correlates of quality of life, happiness, and life satisfaction among European adults older than 50 years using machine learning techniques.
Research design and methods
Data drawn from the Survey of Health, Ageing and Retirement in Europe (SHARE) Wave 7 were used. Participants were 62,500 persons aged 50 years and over living in 26 Continental EU Member States, Switzerland, and Israel. Multiple machine learning regression approaches were used.
Results
The algorithms captured 53%, 33%, and 18% of the variance of quality of life, life satisfaction, and happiness, respectively. The most important categories of correlates of quality of life and life satisfaction were physical health and subjective life circumstances. Sociodemographic factors (mostly country of residence) and psychological variables were the most important categories of correlates of happiness.
Discussion and implications
This study highlights subjective poverty, self-perceived health, country of residence, subjective survival probability, and personality factors (especially neuroticism) as important correlates of quality of life, happiness, and life satisfaction. These findings provide evidence-based recommendations for practice and/or policy implications.}
}