@article{LIU2022158461,
title = {Soil tungsten contamination and health risk assessment of an abandoned tungsten mine site},
journal = {Science of The Total Environment},
volume = {852},
pages = {158461},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.158461},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722055607},
author = {Sijia Liu and Rongxiao Yuan and Xuedong Wang and Zengguang Yan},
keywords = {Tungsten mine site, Soil contamination, Leaching, Geo-accumulation index, Health risk assessment},
abstract = {The mining and beneficiation of tungsten ores, including waste treatment and tailings disposal, may cause soil contamination in the mining area and environments. Few studies have addressed soil contamination in tungsten mine sites. The current research quantitated the leachates in the surface and subsurface soil samples from mining and beneficiation areas, peripheral area, sand-making area, dumping area, and tailings pond of an abandoned tungsten mine site in Ganzhou City, Jiangxi Province of China. We further evaluated the degree of soil tungsten pollution and the risk to human health. The results showed that soil tungsten contamination mainly occurred in the sand-making area where tailings were used to make sand. The highest tungsten content in the surface and subsurface soils of the sand-making area was 1250 and 3020 mg/kg, respectively, exceeding the EPA's Regional Screening Level of tungsten (930 mg/kg) for industrial land use. The leaching concentrations of soil tungsten had similar distribution patterns to that of total soil tungsten, with the highest leaching concentration (0.860 mg/L) found in the sand-making area. The geo-accumulation index evaluation indicated heavy tungsten contamination at the sand-making area and tailings pond. The hazard quotient (HQ = 1.34) of tungsten contamination in the surface soils of the sand-making area exceeded the acceptable level (HQ = 1), implying a significant risk to human health. The present study provided valuable information for pollution control and risk management of soil contamination in tungsten mine sites.
Capsule
We studied the degree of soil tungsten pollution and health risk assessment in an abandoned tungsten mining area to provide helpful information for soil pollution control and risk management in China's tungsten mining areas.}
}
@incollection{SMITH2022319,
title = {1.13 - Geovisualization},
editor = {John (Jack) F. Shroder},
booktitle = {Treatise on Geomorphology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {319-361},
year = {2022},
isbn = {978-0-12-818235-2},
doi = {https://doi.org/10.1016/B978-0-12-818234-5.00147-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128182345001474},
author = {Mike J. Smith and Jan-Christoph Otto and Antoni B. Moore and Carlos H. Grohmann and John Hillier and Martin Geilhausen},
keywords = {Augmented reality, Cartography, DEM, Filter, Globe, HCI, Interactivity, Kernel, Legend, Map, Symbol, Terrain, Virtual reality, Visualization},
abstract = {Geovisualization, the depiction of, and visual interaction with, geospatial data, is key to facilitating the generation of observational datasets through which Earth surface and solid-Earth processes may be understood. This article focuses upon the visualization of terrain morphology using satellite imagery and digital elevation models (DEMs), where manual interpretation remains prevalent in the study of geomorphological processes. Techniques to enhance satellite images and DEMs are covered in order to improve landform identification, as part of the manual mapping process, are presented. Visual interaction with geospatial data is also an important part of exploring and understanding landforms and geomorphological systems, and a variety of methods ranging from simple overlay, panning and zooming are discussed, along with 2.5D, 3D and temporal analyses. Visualization output products are outlined in the final section, which focuses on static and interactive methods of dissemination. Geomorphological mapping legends and the cartographic principles for map design are then introduced, followed by details of dynamic web-map systems that allow a greater immersive use by end users, as well as the dissemination of data and information.}
}
@article{PAN2022100754,
title = {Distribution patterns of lake-wetland cultural ecosystem services in highland},
journal = {Environmental Development},
volume = {44},
pages = {100754},
year = {2022},
issn = {2211-4645},
doi = {https://doi.org/10.1016/j.envdev.2022.100754},
url = {https://www.sciencedirect.com/science/article/pii/S2211464522000562},
author = {Jianfeng Pan and Yuewei Ma and Siqing Cai and Yan Chen and Yumei Chen},
keywords = {Cultural ecosystem services, Lake-wetland ecosystem, SolVES, Transfer value, Environmental variables},
abstract = {Environmental and ecological degradation are more prominent within lake-wetland ecosystem than any other ecosystem on Earth, especially in the highlands. The continued pressures of population growth and rapid urbanization on cultural ecosystem services (CES) provided by lake-wetland ecosystem in highlands present ongoing challenges to decision-makers and managers. In this paper, Social Values for Ecosystem Services model (SolVES) was used to map, quantify and assess CES in Dianchi lake basin (DLB) and Erhai lake basin (ELB) to understand the spatial dynamics of CES perceived by residents and tourists. After combining the field survey data with four environmental variables, our analysis shows that (1) The Maximum Value Index (M-VI) ranking of the three CES in descending order within DLB was aesthetic (M-VI = 10), recreation (M-VI = 10), cultural (M-VI = 8). (2) Different stakeholders (residents and tourists) had different perceptions of CES. Recreation value between residents and tourists had the same M-VI (M-VI = 10), but the M-VI of aesthetic value (M-VI = 10) perceived by tourists was higher than those perceived by residents. (3) The four environmental variables significantly influenced CES, especially distance to water and dominant landcover. (4) CES were transferable from DLB to ELB due to the potential transferability of Maxent's DLB models for each CES, and CES hotspots in ELB generated from SolVES were highly consistent with high kernel-density areas. In conclusion, SolVES incorporating CES can benefit the basin resource managers when seeking to integrate a social perspective into the resource management decision-making process. In particular, the involvement of various stakeholders ensures that they are not marginalized in environmental planning and management.}
}
@article{WANG2022113136,
title = {A new object-class based gap-filling method for PlanetScope satellite image time series},
journal = {Remote Sensing of Environment},
volume = {280},
pages = {113136},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.113136},
url = {https://www.sciencedirect.com/science/article/pii/S0034425722002504},
author = {Jing Wang and Calvin K.F. Lee and Xiaolin Zhu and Ruyin Cao and Yating Gu and Shengbiao Wu and Jin Wu},
keywords = {Gap-filling, CubeSats, Image reconstruction, Cloud removal, Object-based segmentation},
abstract = {PlanetScope CubeSats data with a 3-m resolution, frequent revisits, and global coverage have provided an unprecedented opportunity to advance land surface monitoring over the recent years. Similar to other optical satellites, cloud-induced data missing in PlanetScope satellites substantially hinders its use for broad applications. However, effective gap-filling in PlanetScope image time series remains challenging and is subject to whether it can 1) consistently generate high accuracy results regardless of different gap sizes, especially for heterogeneous landscapes, and 2) effectively recover the missing pixels associated with rapid land cover changes. To address these challenges, we proposed an object-class based gap-filling (‘OCBGF’) method. Two major novelties of OCBGF include 1) adopting an object-based segmentation method in conjunction with an unsupervised classification method to help characterize the landscape heterogeneity and facilitate the search of neighboring valid pixels for gap-filling, improving its applicability regardless of the gap size; 2) employing a scenario-specific gap-filling approach that enables effective gap-filling of areas with rapid land cover change. We tested OCBGF at four sites representative of different land cover types (plantation, cropland, urban, and forest). For each site, we evaluated the performance of OCBGF on both simulated and real cloud-contaminated scenarios, and compared our results with three state-of-the-art methods, namely Neighborhood Similar Pixel Interpolator (NSPI), AutoRegression to Remove Clouds (ARRC), and Spectral-Angle-Mapper Based Spatio-Temporal Similarity (SAMSTS). Our results show that across all four sites, OCBGF consistently obtains the highest accuracy in gap-filling when applied to scenarios with various gap sizes (RMSE = 0.0065, 0.0090, 0.0092, and 0.0113 for OCBGF, SAMSTS, ARRC, and NSPI, respectively) and with/without rapid land cover changes (RMSE = 0.0082, 0.0112, 0.0119, and 0.0120 for OCBGF, SAMSTS, ARRC, and NSPI, respectively). These results demonstrate the effectiveness of OCBGF for gap-filling PlanetScope image time series, with potential to be extended to other satellites.}
}
@article{RAHMAN2022100249,
title = {A pilot study towards a smart-health framework to collect and analyze biomarkers with low-cost and flexible wearables},
journal = {Smart Health},
volume = {23},
pages = {100249},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100249},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000635},
author = {Md Juber Rahman and Bashir I. Morshed and Brook Harmon and Mamunur Rahman},
keywords = {Artificial intelligence, Edge computing, Events of interest, Heart rate variability, Inkjet-printed sensors, Smart health, Smart and connected community, Spatiotemporal monitoring, Flexible wearables},
abstract = {Artificial intelligence-enabled applications on edge devices have the potential to revolutionize disease detection and monitoring with smart health (sHealth) applications. The major challenges are user-friendly and flexible sensors for seamless collection of physiological data for long hours, online and on-device processing of sensitive medical data to facilitate privacy protection, reliable extraction of disease-related biomarkers, and implementation of lightweight artificially intelligent algorithms for inference at the edge without degrading the system performance. In this pilot project, we conducted a yearlong field study with 9 participants conducting 480 data collection sessions in the “living lab” environment. We used smartphones as the edge computing device and implemented pre-trained machine learning algorithms in the smartphone app for computing disease-related Events-of-Interest (EoI). We considered real-time data processing on the smartphone itself without sharing raw data with the cloud or any other computing facility to minimize privacy concerns, and network bandwidth requirements. We used a commercial smart band and a custom-designed zero-power inkjet-printed sensor for physiological sensing and capturing health biomarkers such as heart rate variability (HRV) and core body temperature. The extracted HRV feature values are within the 95% confidence interval of normative values. On top of that, the extracted HRV shows some informative trends i.e. hammock pattern for healthy subjects which may be helpful in subsequent research studies. Moreover, we used core body temperature with user-reported outcomes for estimating flu-related symptoms severity and visualizing the spatiotemporal trend in a cloud-server to facilitate personalized as well as community-wide health monitoring. Inference at the edge provided a data reduction of 3 order while the runtime latency, power consumption, memory requirement, and storage size of the smartphone app were 500 ms, 51.90 mAH, 9.4 MB, and 2.4 MB, respectively. Our developed framework of sHealth enables automated community-wide monitoring of symptoms severity in addition to personalized monitoring which paves the way for early monitoring of a disease outbreak for a smart and connected community.}
}
@article{LU2022115540,
title = {Trace elements in public drinking water in Chinese cities: Insights from their health risks and mineral nutrition assessments},
journal = {Journal of Environmental Management},
volume = {318},
pages = {115540},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.115540},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722011136},
author = {Taotao Lu and Hao Peng and Feifei Yao and Aira Sacha {Nadine Ferrer} and Shuang Xiong and Geng Niu and Zhonghua Wu},
keywords = {Public drinking water, Trace element, Health risk, Nutritional assessment},
abstract = {The trace elements in the public drinking water have a duality: on the one hand, trace elements play an important role in maintaining human metabolism; on the other hand, high trace elements levels lead to significant health risks. To determine the impacts of trace elements in the public drinking water on physical health in China, water samples were collected from 314 Chinese cities to analyze the concentrations and spatial distributions of trace elements on a national scale. On this basis, the non-carcinogenic health risk assessments and the nutrient-based scores of trace elements (NSTEs) were applied to evaluate the public drinking water quality in terms of safety and nutrition. Most of the water samples were weakly alkaline: pH values fell in the range of 6.62–8.54, with a mean of 7.80. The results indicated that Sr and F− had the highest concentrations in public drinking water, with averages of 0.3604 mg/L and 0.2351 mg/L, respectively. Moreover, hazard index (HI) values in different regions followed the order: northwest China (NWC) > northern China (NC) > Qinghai-Tibetan Plateau (QT) > southern China (SC). The percentages of water samples with HI > 1 in SC, NC, NWC, and QT were 5.49%, 16.82%, 25.81%, and 16.67%, respectively, indicating that the public drinking water in some cities had significant non-carcinogenic health risks. In addition, the intakes of Mn, Fe, Cu, and Rb through public drinking water made negligible contributions to their recommended nutrient intakes. In contrast, trace elements like Sr, F, B, Li, Mo, etc., contributed a lot. The NSTEs in NWC and most parts of NC were relatively high with averages of 8.0300 and 11.2082, respectively; however, the NSTEs in SC and the northeast part of NC were low with averages of 3.3284 and 5.2106, respectively. The results from this study provide a reference for establishing the public drinking water standards and improving drinking water quality.}
}
@article{SENAY2022113011,
title = {Mapping actual evapotranspiration using Landsat for the conterminous United States: Google Earth Engine implementation and assessment of the SSEBop model},
journal = {Remote Sensing of Environment},
volume = {275},
pages = {113011},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.113011},
url = {https://www.sciencedirect.com/science/article/pii/S0034425722001250},
author = {Gabriel B. Senay and MacKenzie Friedrichs and Charles Morton and Gabriel E.L. Parrish and Matthew Schauer and Kul Khand and Stefanie Kagone and Olena Boiko and Justin Huntington},
keywords = {Landsat thermal, Evapotranspiration, SSEBop, Cloud computing, Google Earth Engine, Evaluation, Eddy covariance, Water balance},
abstract = {The estimation and mapping of actual evapotranspiration (ETa) is an active area of applied research in the fields of agriculture and water resources. Thermal remote sensing-based methods, using coarse resolution satellites, have been successful at estimating ETa over the conterminous United States (CONUS) and other regions of the world. In this study, we present CONUS-wide ETa from Landsat thermal imagery-using the Operational Simplified Surface Energy Balance (SSEBop) model in the Google Earth Engine (GEE) cloud computing platform. Over 150,000 Landsat satellite images were used to produce 10 years of annual ETa (2010–2019) at unprecedented scale. The accuracy assessment of the SSEBop results included point-based evaluation using monthly Eddy Covariance (EC) data from 25 AmeriFlux stations as well as basin-scale comparison with annual Water Balance ETa (WBET) for more than 1000 sub-basins. Evaluations using EC data showed generally mixed performance with weaker (R2 < 0.6) correlation on sparsely vegetated surfaces such as grasslands or woody savanna and stronger correlation (R2 > 0.7) over well-vegetated surfaces such as croplands and forests, but location-specific conditions rather than cover type were attributed to the variability in accuracy. Croplands performed best with R2 of 0.82, root mean square error of 29 mm/month, and average bias of 12%. The WBET evaluation indicated that the SSEBop model is strong in explaining the spatial variability (up to R2 > 0.90) of ETa across large basins, but it also identified broad hydro-climatic regions where the SSEBop ETa showed directional biases, requiring region-specific model parameter improvement and/or bias correction with an overall 7% bias nationwide. Annual ETa anomalies over the 10-year period captured widely reported drought-affected regions, for the most part, in different parts of the CONUS, indicating their potential applications for mapping regional- and field-scale drought and fire effects. Due to the coverage of the Landsat Path/Row system, the availability of cloud-free image pixels ranged from less than 12 (mountainous cloud-prone regions and U.S. Northeast) to more than 60 (U.S. Southwest) per year. However, this study reinforces a promising application of Landsat satellite data with cloud-computing for quick and efficient mapping of ETa for agricultural and water resources assessments at the field scale.}
}
@article{STROMMER2022134322,
title = {Forward-looking impact assessment – An interdisciplinary systematic review and research agenda},
journal = {Journal of Cleaner Production},
volume = {377},
pages = {134322},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134322},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203894X},
author = {Kiia Strömmer and Jarrod Ormiston},
keywords = {Impact assessment, Forward-looking, Temporality, Futures thinking},
abstract = {New and established ventures are under increasing pressure to consider how their current actions impact our future world. Whilst many practitioners are paying greater attention to their future impact, most impact assessment research focuses on the retrospective measurement of impact. Limited studies have explored how impact assessment is used as a tool to forecast or predict the intended impact of organisational action. This study aims to overcome this gap by exploring forward-looking approaches to impact assessment. An interdisciplinary systematic review of the impact assessment literature was conducted to answer the question: “How and why do organisations utilise forward-looking, future-oriented approaches to impact assessment?“. The findings elaborate on the common research themes, challenges, and gaps in understanding forward-looking impact assessment. An integrated process model is developed to show the relationships between various antecedents, methods, and effects of forward-looking impact assessment. Based on the review, the paper puts forward a research agenda to provoke further inquiry on forward-looking, future-oriented approaches to impact assessments related to four research themes: uncertainty, values and assumptions, stakeholder cooperation, and learning. The study contributes to the impact assessment literature by providing an overview of how the current literature comprehends forward-looking approaches and insights into how a more holistic view of temporality in impact assessment can be developed.}
}
@article{PLJAKIC2022,
title = {The influence of traffic-infrastructure factors on pedestrian accidents at the macro-level: The geographically weighted regression approach},
journal = {Journal of Safety Research},
year = {2022},
issn = {0022-4375},
doi = {https://doi.org/10.1016/j.jsr.2022.08.021},
url = {https://www.sciencedirect.com/science/article/pii/S0022437522001359},
author = {Miloš Pljakić and Dragan Jovanović and Boško Matović},
keywords = {Macro-level modeling, Pedestrian accidents, GWPR, Traffic analysis zones, Transportation planning},
abstract = {Introduction: Walking is an active way of moving the population, but in recent years there have been more pedestrian casualties in traffic, especially in developing countries such as Serbia. Macro-level road safety studies enable the identification of influential factors that play an important role in creating pedestrian safety policies. Method: This study analyzes the impact of traffic and infrastructure characteristics on pedestrian accidents at the level of traffic analysis zones. The study applied a geographically weighted regression approach to identify and localize all factors that contribute to the occurrence of pedestrian accidents. Taking into account the spatial correlations between the zones and the frequency distribution of accidents, the geographically Poisson weighted model showed the best predictive performance. Results: This model showed 10 statistically significant factors influencing pedestrian accidents. In addition to exposure measures, a positive relationship with pedestrian accidents was identified in the length of state roads (class I), the length of unclassified streets, as well as the number of bus stops, parking spaces, and object units. However, a negative relationship was recorded with the total length of the street network and the total length of state roads passing through the analyzed area. Conclusion: These results indicate the importance of determining the categorization and function of roads in places where pedestrian flows are pronounced, as well as the perception of pedestrian safety near bus stops and parking spaces. Practical Applications: The results of this study can help traffic safety engineers and managers plan infrastructure measures for future pedestrian safety planning and management in order to reduce pedestrian casualties and increase their physical activity.}
}
@article{SIANG2022100021,
title = {Self-service analytics and the processing of hydrocarbons},
journal = {Digital Chemical Engineering},
volume = {3},
pages = {100021},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100021},
url = {https://www.sciencedirect.com/science/article/pii/S2772508122000126},
author = {Lim C. Siang and Shams Elnawawi and Darren Steele},
keywords = {Hydrocarbon processing, Self-service analytics, Data visualization, Process control},
abstract = {This paper describes the use of self-service analytics on time series process data for the troubleshooting and optimization of refinery operations in the context of data visualization principles and best practices. Refining-relevant examples are used to demonstrate how end-users can access real-time and historical process data and apply the following analytics operations across several refining functions, including (1) incident troubleshooting – identifying periods of interest and methods available to investigate related plant data, patterns, events and disturbances leading up to the incident, and (2) data cleansing – filtering sensor data to remove outliers and bad quality data, splicing and aligning data streams for more accurate analysis and to improve the confidence in the outputs of subsequent analysis, such as the outputs of multivariate, regression-based system identification. The paper also provides examples of how ad hoc analyses can be scaled up to plantwide analytics and evolve into routine, automated tasks. The importance of analytic provenance and collaboration in sharing new insights from data is also discussed. To address the human factors associated with self-service analytics innovation, the paper concludes with lessons learnt, observations and adaptations compared to the traditional “business-as-usual approaches, best practices for data governance, and the implications for engineers that operate in a safety-critical environment.}
}
@article{ZHANG2022231110,
title = {A machine learning-based framework for online prediction of battery ageing trajectory and lifetime using histogram data},
journal = {Journal of Power Sources},
volume = {526},
pages = {231110},
year = {2022},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2022.231110},
url = {https://www.sciencedirect.com/science/article/pii/S0378775322001331},
author = {Yizhou Zhang and Torsten Wik and John Bergström and Michael Pecht and Changfu Zou},
keywords = {Lithium-ion batteries, State of health prediction, Remaining useful life, Machine learning, Online adaptive learning, Real-world fleet data},
abstract = {Accurately predicting batteries’ ageing trajectory and remaining useful life is not only required to ensure safe and reliable operation of electric vehicles (EVs) but is also the fundamental step towards health-conscious use and residual value assessment of the battery. The non-linearity, wide range of operating conditions, and cell to cell variations make battery health prediction challenging. This paper proposes a prediction framework that is based on a combination of global models offline developed by different machine learning methods and cell individualised models that are online adapted. For any format of raw data collected under diverse operating conditions, statistic properties of histograms can be still extracted and used as features to learn battery ageing. Our framework is trained and tested on three large datasets, one being retrieved from 7296 plug-in hybrid EVs. While the best global models achieve 0.93% mean absolute percentage error (MAPE) on laboratory data and 1.41% MAPE on the real-world fleet data, the adaptation algorithm further reduced the errors by up to 13.7%, all requiring low computational power and memory. Overall, this work proves the feasibility and benefits of using histogram data and also highlights how online adaptation can be used to improve predictions.}
}
@article{YANG2022119415,
title = {Long short-term memory suggests a model for predicting shale gas production},
journal = {Applied Energy},
volume = {322},
pages = {119415},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119415},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922007504},
author = {Run Yang and Xiangui Liu and Rongze Yu and Zhiming Hu and Xianggang Duan},
keywords = {Shale gas, Exponential smoothing method, LSTM model, Production prediction, Deep learning},
abstract = {Predicting the production behaviors of shale gas wells is of great importance for further developing future unconventional hydrocarbon strategies. An accurate prediction production, as well as reliable shale gas production models, are required to fully understand the shale gas exploitation budget. However, a major problem with classical analytic methods is the insufficient accuracy of the existing models, the time-consuming collection of historical production data, and the costly computational expense. To minimize this problem, a combination of the exponential smoothing method, autoregressive integrated moving average (ARIMA) model, and long short-term memory (LSTM) model was proposed to provide robust support for the production behaviors of shale gas. In this paper, we employed shale gas well production data to establish a database for model training and optimized the predicted model. Hereby, we sought to evaluate the production data predicted by conventional analytical methods, the exponential smoothing method, the ARIMA model, and the LSTM model. Shortly afterward, we objectively compared the predicted results obtained by the novel LSTM model and traditional analytical methods, such as Arps, stretched exponential decline (SEPD), and the Duong model. Herein, we compared the computational cost between the LSTM model and traditional numerical simulation. The combined interpretation of the proposed model demonstrates that the LSTM model achieved scientific accuracy and outstanding results in both short-term and long-term predictions, and realized production prediction of the adjacent well, with excellent agreement with the real shale gas production and a low error, making it an effective tool in forecasting shale gas production. This assay could be used as a potential approach for evaluating deep learning in the petroleum industry and for predicting the future production of unconventional hydrocarbons.}
}
@article{HSU2022134758,
title = {A mixed spatial prediction model in estimating spatiotemporal variations in benzene concentrations in Taiwan},
journal = {Chemosphere},
volume = {301},
pages = {134758},
year = {2022},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2022.134758},
url = {https://www.sciencedirect.com/science/article/pii/S0045653522012516},
author = {Chin-Yu Hsu and Hong-Xin Xie and Pei-Yi Wong and Yu-Cheng Chen and Pau-Chung Chen and Chih-Da Wu},
keywords = {Benzene, Land-use regression (LUR), Machine learning, Mixed spatial prediction},
abstract = {It is well known benzene negatively impacts human health. This study is the first to predict spatial-temporal variations in benzene concentrations for the entirety of Taiwan by using a mixed spatial prediction model integrating multiple machine learning algorithms and predictor variables selected by Land-use Regression (LUR). Monthly benzene concentrations from 2003 to 2019 were utilized for model development, and monthly benzene concentration data from 2020, as well as mobile monitoring vehicle data from 2009 to 2019, served as external data for verifying model reliability. Benzene concentrations were estimated by running six LUR-based machine learning algorithms; these algorithms, which include random forest (RF), deep neural network (DNN), gradient boosting (GBoost), light gradient boosting (LightGBM), CatBoost, extreme gradient boosting (XGBoost), and ensemble algorithms (a combination of the three best performing models), can capture how nonlinear observations and predictions are related. The results indicated conventional LUR captured 79% of the variability in benzene concentrations. Notably, the LUR with ensemble algorithm (GBoost, CatBoost, and XGBoost) surpassed all other integrated methods, increasing the explanatory power to 92%. This study establishes the value of the proposed ensemble-based model for estimating spatiotemporal variation in benzene exposure.}
}
@incollection{KIDO2022161,
title = {Chapter 10 - Explore the RNA-sequencing and the next-generation sequencing in crops responding to abiotic stress},
editor = {Pradeep Sharma and Dinesh Yadav and Rajarshi Kumar Gaur},
booktitle = {Bioinformatics in Agriculture},
publisher = {Academic Press},
pages = {161-175},
year = {2022},
isbn = {978-0-323-89778-5},
doi = {https://doi.org/10.1016/B978-0-323-89778-5.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323897785000052},
author = {Éderson Akio Kido and José Ribamar Costa Ferreira-Neto and Eliseu Binneck and Manassés {da Silva} and Wilson {da Silva} and Ana Maria Benko-Iseppon},
keywords = {Transcriptomic, bioinformatic, RNA-Seq, SuperSAGE, crop, breeding, abiotic stress, drought, salinity, environmental stress, gene expression, RT-qPCR, plant biotechnology},
abstract = {Most of the environmental stresses damage crops provoking economic losses. Molecular characterization of genotypes identifying suitable genes helps breeders to develop abiotic stress-tolerant crops. Among the omics technologies, transcriptomics represents the most informative techniques for global gene expression profiling of an organism, organ/tissue, or even cell at a given snapshot in time. In this context the next-generation sequencing technologies revolutionized plant transcriptomics, particularly nonmodel plants of economic interest, enhancing the high-throughput data generated from biological samples. This chapter shows an updated RNA-Seq (sequencing of RNA) overview covering scientific reports published in the last 5 years. The assortment of the data-mined RNA-Seq studies concerning relevant issues identified the most studied crops, abiotic stresses, plant organs/tissues, stress-treatment exposition time, global or specific gene expression profiling, noncoding RNA approach, extra omics data, supplementary physiological data, studied accession(s), transcriptome assembly approach, gene expression validation process, relative quantification method, and reference gene(s) in the qPCR assays. Besides, an overall RNA-Seq analysis workflow, covering data generation, raw data processing, and data analysis, was provided, together with some information involving transcriptome assembly, quality of the transcriptome assembly, transcript quantification, differential expression analysis, and annotation/functional analysis. Finally, in the functional genomics topic, the integration of multiomic data stands out, improving our understanding of genetic networks, which helps us to envision machine learning strategies and genome editing applications involving agronomic characteristics. These approaches may help plant breeders identify new genes and related metabolic subpathways associated with relevant agronomic traits.}
}
@article{ZHANG2022103574,
title = {Information fusion for automated post-disaster building damage evaluation using deep neural network},
journal = {Sustainable Cities and Society},
volume = {77},
pages = {103574},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103574},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721008398},
author = {Limao Zhang and Yue Pan},
keywords = {Deep neural network, Factorization machine, Earthquake-induced building damage, Automated evaluation, Decision making},
abstract = {This paper develops a hybrid neural network architecture named multi-class factorization machine with deep neural network (multi-FMDNN) to fuse multi-source information for the automatic post-earthquake building damage evaluation. The novel algorithm is a combination of the factorization machine (FM) and the deep neural network (DNN), which adopts the one-vs-all strategy to fuse results from multiple base classifiers. 39,352 buildings affected by the 2015 Nepal earthquake are taken as a case study to validate the effectiveness of the proposed multi-FMDNN. Experimental results confirm that the proposed model outperforms over many other popular machine learning methods due to the powerful feature learning ability, ultimately reaching an overall accuracy, macro F1-score, and weighted F1-score in the value of 0.703, 0.737, and 0.702, respectively. Features associated with building structural characteristics are found to contribute more to classifying damage grades precisely. Besides, data preprocessing for data cleaning, encoding, and transformation is a necessary step to bring additional performance enhancement. For significance in the knowledge aspect, a novel multi-FMDNN algorithm is developed, which is superior in extracting both low- and high-order feature representation automatically from large volumes of destroyed buildings-related data and learning the optimal feature interactions simultaneously to pursue more accurate classification. For significance in the application aspect, the predicted results provide deep insights into a better understanding of the building vulnerability in seismic areas and inform data-driven decisions in disaster relief efforts. A promising future scope is to make full use of the available pre-event data along with some post-event data, which is possible to return fairly promising predictions and reduce the burden in earthquake field investigations for rapid responses. In future work, advanced techniques associated with data augment, hyperparameter optimization, and others will be implemented to constantly improve the overall accuracy and generalizability of the prediction model.}
}
@article{DOUVILLE2022100099,
title = {Airway driving pressure is associated with postoperative pulmonary complications after major abdominal surgery: a multicentre retrospective observational cohort study},
journal = {BJA Open},
volume = {4},
pages = {100099},
year = {2022},
issn = {2772-6096},
doi = {https://doi.org/10.1016/j.bjao.2022.100099},
url = {https://www.sciencedirect.com/science/article/pii/S2772609622000983},
author = {Nicholas J. Douville and Timothy L. McMurry and Jennie Z. Ma and Bhiken I. Naik and Michael R. Mathis and DouglasA. Colquhoun and Sachin Kheterpal and Nathan L. Pace and Traci L. Hedrick and Randal S. Blank and Marcel E. Durieux and Stephen Patrick Bender and Stefan D. Holubar},
keywords = {abdominal surgery, driving pressure, lung protective ventilation, postoperative pulmonary complications, predictive models, ventilation-induced lung injury},
abstract = {Background
High airway driving pressure is associated with adverse outcomes in critically ill patients receiving mechanical ventilation, but large multicentre studies investigating airway driving pressure during major surgery are lacking. We hypothesised that increased driving pressure is associated with postoperative pulmonary complications in patients undergoing major abdominal surgery.
Methods
In this preregistered multicentre retrospective observational cohort study, the authors reviewed major abdominal surgical procedures in 11 hospitals from 2004 to 2018. The primary outcome was a composite of postoperative pulmonary complications, defined as postoperative pneumonia, unplanned tracheal intubation, or prolonged mechanical ventilation for more than 48 h. Associations between intraoperative dynamic driving pressure and outcomes, adjusted for patient and procedural factors, were evaluated.
Results
Among 14 218 qualifying cases, 389 (2.7%) experienced postoperative pulmonary complications. After adjustment, the mean dynamic driving pressure was associated with postoperative pulmonary complications (adjusted odds ratio for every 1 cm H2O increase: 1.04; 95% confidence interval [CI], 1.02–1.06; P<0.001). Neither tidal volume nor PEEP was associated with postoperative pulmonary complications. Increased BMI, shorter height, and female sex were predictors for higher dynamic driving pressure (β=0.35, 95% CI 0.32–0.39, P<0.001; β=–0.01, 95% CI –0.02 to 0.00, P=0.005; and β=0.74, 95% CI 0.63–0.86, P<0.001, respectively).
Conclusions
Dynamic airway driving pressure, but not tidal volume or PEEP, is associated with postoperative pulmonary complications in models controlling for a large number of risk predictors and covariates. Such models are capable of risk prediction applicable to individual patients.}
}
@article{JIN2022104733,
title = {Machine learning predicts cancer-associated deep vein thrombosis using clinically available variables},
journal = {International Journal of Medical Informatics},
volume = {161},
pages = {104733},
year = {2022},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104733},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622000478},
author = {Shuai Jin and Dan Qin and Bao-Sheng Liang and Li-Chuan Zhang and Xiao-Xia Wei and Yu-Jie Wang and Bing Zhuang and Tong Zhang and Zhen-Peng Yang and Yi-Wei Cao and San-Li Jin and Ping Yang and Bo Jiang and Ben-Qiang Rao and Han-Ping Shi and Qian Lu},
keywords = {Neoplasms, Deep vein thrombosis, Machine learning, Decision making, Risk stratification},
abstract = {Purpose
To develop and validate machine learning (ML) models for cancer-associated deep vein thrombosis (DVT) and to compare the performance of these models with the Khorana score (KS).
Methods
We randomly extracted data of 2100 patients with cancer between Jan. 1, 2017, and Oct. 31, 2019, and 1035 patients who underwent Doppler ultrasonography were enrolled. Univariate analysis and Lasso regression were applied to select important predictors. Model training and hyperparameter tuning were implemented on 70% of the data using a ten-fold cross-validation method. The remaining 30% of the data were used to compare the performance with seven indicators (area under the receiver operating characteristic curve [AUC], sensitivity, specificity, accuracy, balanced accuracy, Brier score, and calibration curve), among all five ML models (linear discriminant analysis [LDA], logistic regression [LR], classification tree [CT], random forest [RF], and support vector machine [SVM]), and the KS.
Results
The incidence of cancer-associated DVT was 22.3%. The top five predictors were D-dimer level, age, Charlson Comorbidity Index (CCI), length of stay (LOS), and previous VTE (venous thromboembolism) history according to RF. Only LDA (AUC = 0.773) and LR (AUC = 0.772) outperformed KS (AUC = 0.642), and combination with D-dimer showed improved performance in all models. A nomogram and web calculator https://webcalculatorofcancerassociateddvt.shinyapps.io/dynnomapp/ were used to visualize the best recommended LR model.
Conclusion
This study developed and validated cancer-associated DVT predictive models using five ML algorithms and visualized the best recommended model using a nomogram and web calculator. The nomogram and web calculator developed in this study may assist doctors and nurses in evaluating individualized cancer-associated DVT risk and making decisions. However, other prospective cohort studies should be conducted to externally validate the recommended model.}
}
@article{HOLZINGER2022263,
title = {Information fusion as an integrative cross-cutting enabler to achieve robust, explainable, and trustworthy medical artificial intelligence},
journal = {Information Fusion},
volume = {79},
pages = {263-278},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521002050},
author = {Andreas Holzinger and Matthias Dehmer and Frank Emmert-Streib and Rita Cucchiara and Isabelle Augenstein and Javier Del Ser and Wojciech Samek and Igor Jurisica and Natalia Díaz-Rodríguez},
keywords = {Artificial intelligence, Information fusion, Medical AI, Explainable AI, Robustness, Explainability, Trust, Graph-based machine learning, Neural-symbolic learning and reasoning},
abstract = {Medical artificial intelligence (AI) systems have been remarkably successful, even outperforming human performance at certain tasks. There is no doubt that AI is important to improve human health in many ways and will disrupt various medical workflows in the future. Using AI to solve problems in medicine beyond the lab, in routine environments, we need to do more than to just improve the performance of existing AI methods. Robust AI solutions must be able to cope with imprecision, missing and incorrect information, and explain both the result and the process of how it was obtained to a medical expert. Using conceptual knowledge as a guiding model of reality can help to develop more robust, explainable, and less biased machine learning models that can ideally learn from less data. Achieving these goals will require an orchestrated effort that combines three complementary Frontier Research Areas: (1) Complex Networks and their Inference, (2) Graph causal models and counterfactuals, and (3) Verification and Explainability methods. The goal of this paper is to describe these three areas from a unified view and to motivate how information fusion in a comprehensive and integrative manner can not only help bring these three areas together, but also have a transformative role by bridging the gap between research and practical applications in the context of future trustworthy medical AI. This makes it imperative to include ethical and legal aspects as a cross-cutting discipline, because all future solutions must not only be ethically responsible, but also legally compliant.}
}
@article{ZANELLA2022107279,
title = {CEIFA: A multi-level anomaly detector for smart farming},
journal = {Computers and Electronics in Agriculture},
volume = {202},
pages = {107279},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107279},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005919},
author = {Angelita Rettore de Araujo Zanella and Eduardo {da Silva} and Luiz Carlos Pessoa Albini},
keywords = {Smart agriculture, Anomaly detection, Security, Reliability, Internet of Things},
abstract = {Climate change, the water crisis, and population growth add new challenges for food production. The modernization of agricultural methods is essential to increase production rates and preserve natural resources. Smart agriculture provides resources that can enhance farming tasks by efficiently controlling actuators, optimizing utility and resource use, managing production, maximizing profit, and minimizing costs. For these technologies to become popular, they must have a high level of reliability and safety. To improve the reliability in Smart Agriculture, this paper proposes CEIFA, a low-cost, hybrid anomaly detector capable of identifying failures, faults, errors and attacks that impact these systems. CEIFA operates on local or remote cloud servers, filtering data sent by agricultural system sensors. It can operate on resource-restricted devices and save financial resources related to computing costs. Real tests on a set of faults, failures, and errors point to an efficiency greater than 95% in anomaly detection.}
}
@article{KIM2022,
title = {Redesigning culturally tailored intervention in the precision health era: Self-management science context},
journal = {Nursing Outlook},
year = {2022},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2022.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0029655422001038},
author = {Miyong T. Kim and Elizabeth M. Heitkemper and Emily T. Hébert and Jacklyn Hecht and Alison Crawford and Tonychris Nnaka and Tara S. Hutson and Hyekyun Rhee and Kavita Radhakrishnan},
keywords = {Cultural tailoring, Psychosocial phenotyping, Precision health, Chronic disease, Self-management, Social determinants of health},
abstract = {Background
Nurse scientists have significantly contributed to health equity and ensuring cultural tailoring of interventions to meet unique needs of individuals. Methodologies for cultural tailoring of self-mangament interventions among marginalized populations have limitedly accommodated intersectionality and group heterogeneity when addressing health needs.
Purpose
Identify methodological limitations in cultural tailoring of interventions among priority populations and issue recommendations on cultural elements that researchers can target to ensure valid cultural tailoring approaches.
Methods
Synthesis of literature on health equity, self-management, and implementation and dissemination research.
Findings
Among priority populations, intersectionality and group heterogeneity has made group-based cultural tailoring approaches less effective in eliciting desirable health outcomes. Precision health methodology could be useful for cultural tailoring of interventions due to the methodology's focus on individual-level tailoring approaches.
Discussion
We offer ways to advance health equity research using precision health approaches in cultural tailoring through targeting unique elements of culture and relevant psychosocial phenotypes.}
}
@incollection{HUSSAIN2022179,
title = {Chapter 8 - Application of artificial intelligence and information and communication technology in the grid agricultural industry: business motivation, analytical tools, and challenges},
editor = {B.D. Deebak and Fadi Al-Turjman},
booktitle = {Sustainable Networks in Smart Grid},
publisher = {Academic Press},
pages = {179-205},
year = {2022},
isbn = {978-0-323-85626-3},
doi = {https://doi.org/10.1016/B978-0-323-85626-3.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323856263000028},
author = {Adedoyin A. Hussain and Barakat A. Dawood and Chadi Altrjman and Sinem Alturjman and Fadi Al-Turjman},
keywords = {Agriculture industry, information and communication technology, AI, big data, HetNet},
abstract = {Recently, the use of Artificial Intelligence (AI) and Information and Communication Technology (ICT) has been clear in farming applications. A few examinations address the subject of ICT appropriation in flooded agribusiness. The principle idea of AI in agribusiness is its adaptability, precision, and cost-adequacy. This area faces various difficulties for yield growth including inappropriate soil treatment, sickness and irritation pervasion, large information necessities, low yield, and information gaps among ranchers and innovation. This paper tackles these issues by presenting a survey of the uses of AI and ICT in the agricultural business. We summarize and arrange different territories and procedures in which AI and ICT can be applied in rural areas. A unique spotlight is laid on the strength and constraints of the application and the path in using scalable frameworks for higher efficiency. Also, the usage of AI and computer networks in the agricultural industry is presented and discussed. The overview put forward will provide understudies and specialists with up to 90% effectiveness at whatever point considered.}
}
@article{CHEN2022104318,
title = {hECA: The cell-centric assembly of a cell atlas},
journal = {iScience},
volume = {25},
number = {5},
pages = {104318},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.104318},
url = {https://www.sciencedirect.com/science/article/pii/S2589004222005892},
author = {Sijie Chen and Yanting Luo and Haoxiang Gao and Fanhong Li and Yixin Chen and Jiaqi Li and Renke You and Minsheng Hao and Haiyang Bian and Xi Xi and Wenrui Li and Weiyu Li and Mingli Ye and Qiuchen Meng and Ziheng Zou and Chen Li and Haochen Li and Yangyuan Zhang and Yanfei Cui and Lei Wei and Fufeng Chen and Xiaowo Wang and Hairong Lv and Kui Hua and Rui Jiang and Xuegong Zhang},
keywords = {Cell biology, Stem cells research, Bioinformatics},
abstract = {Summary
The accumulation of massive single-cell omics data provides growing resources for building biomolecular atlases of all cells of human organs or the whole body. The true assembly of a cell atlas should be cell-centric rather than file-centric. We developed a unified informatics framework for seamless cell-centric data assembly and built the human Ensemble Cell Atlas (hECA) from scattered data. hECA v1.0 assembled 1,093,299 labeled human cells from 116 published datasets, covering 38 organs and 11 systems. We invented three new methods of atlas applications based on the cell-centric assembly: “in data” cell sorting for targeted data retrieval with customizable logic expressions, “quantitative portraiture” for multi-view representations of biological entities, and customizable reference creation for generating references for automatic annotations. Case studies on agile construction of user-defined sub-atlases and “in data” investigation of CAR-T off-targets in multiple organs showed the great potential enabled by the cell-centric ensemble atlas.}
}
@article{LI2022100009,
title = {Fault diagnosis for lithium-ion batteries in electric vehicles based on signal decomposition and two-dimensional feature clustering},
journal = {Green Energy and Intelligent Transportation},
volume = {1},
number = {1},
pages = {100009},
year = {2022},
issn = {2773-1537},
doi = {https://doi.org/10.1016/j.geits.2022.100009},
url = {https://www.sciencedirect.com/science/article/pii/S2773153722000093},
author = {Shuowei Li and Caiping Zhang and Jingcai Du and Xinwei Cong and Linjing Zhang and Yan Jiang and Leyi Wang},
keywords = {Electric vehicle, Fault diagnosis, Extended average voltage, Dynamic time warping, Feature clustering},
abstract = {Battery fault diagnosis is essential for ensuring the reliability and safety of electric vehicles (EVs). The existing battery fault diagnosis methods are difficult to detect faults at an early stage based on the real-world vehicle data since lithium-ion battery systems are usually accompanied by inconsistencies, which are difficult to distinguish from faults. A fault diagnosis method based on signal decomposition and two-dimensional feature clustering is introduced in this paper. Symplectic geometry mode decomposition (SGMD) is introduced to obtain the components characterizing battery states, and distance-based similarity measures with the normalized extended average voltage and dynamic time warping distances are established to evaluate the state of batteries. The 2-dimensional feature clustering based on DBSCAN is developed to reduce the number of feature thresholds and differentiate flaw cells from the battery pack with only one parameter under a wide range of values. The proposed method can achieve fault diagnosis and voltage anomaly identification as early as 43 days ahead of the thermal runaway. And the results of four electric vehicles and the comparison with other traditional methods validated the proposed method with strong robustness, high reliability, and long time scale warning, and the method is easy to implement online.}
}
@incollection{YADAV2022367,
title = {Chapter 22 - Topological parameters, patterns, and motifs in biological networks},
editor = {Dev Bukhsh Singh and Rajesh Kumar Pathak},
booktitle = {Bioinformatics},
publisher = {Academic Press},
pages = {367-380},
year = {2022},
isbn = {978-0-323-89775-4},
doi = {https://doi.org/10.1016/B978-0-323-89775-4.00012-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323897754000122},
author = {Arvind Kumar Yadav and Rohit Shukla and Tiratha Raj Singh},
keywords = {Topological parameters, motif, biological network, graph theory, systems biology, biological pathways, protein–protein interaction, patterns, system biology},
abstract = {Systems biology is an emerging field to study the complex interactions in biological systems. The enormous amount of data generated through high-throughput technologies allows the reconstruction of the biological pathways in a structured and dynamic way at the systems level. Hence, for understating the dynamics of these biological pathways, it is necessary to convert the molecular data in the form of biological pathways. Graph theory is utilized for the modeling of complex biological data. Graphs are used to analyze, simulate, and visualize the biological pathways, such as protein–protein interaction and protein metabolite interaction. Motifs are used for the analysis of complex biological networks. The network motifs describe the local properties of a network and are represented as a small connected subgraph, which is frequently appearing in a network. In this chapter, we have described the structure of biological networks, graph theory including its statistical parameters, network motifs, different types of algorithms, and its role in the biological application.}
}
@article{NIU2022124384,
title = {Point and interval forecasting of ultra-short-term wind power based on a data-driven method and hybrid deep learning model},
journal = {Energy},
volume = {254},
pages = {124384},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124384},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222012877},
author = {Dongxiao Niu and Lijie Sun and Min Yu and Keke Wang},
keywords = {Wind power forecasting, Data-driven modeling, Bidirectional long short-term memory, Attention mechanism, Interval forecasting},
abstract = {Accurate and reliable wind power forecasting (WPF) is significant for ensuring power systems’ economic operation and safe dispatching and for reducing the technical and economic risks faced by power market participants. Based on data-driven and deep-learning methods, we propose a hybrid ultra-short-term WPF framework that can achieve accurate point and interval WPF. First, the multi-sourced and multi-dimensional data sets of wind power plant are preprocessed. Second, feature selection (FS) is conducted to eliminate redundant features. Third, the wind power sequence is decomposed through the variational modal decomposition improved by grey wolf optimization (GWO-VMD). Then, the BiLSTM-Attention model is established to predict each subsequence of wind power. Finally, the prediction intervals of wind power under different confidence levels are estimated by kernel density estimation with the Gaussian kernel function (KDE-Gaussian). The proposed FS-GWO-VMD-BiLSTM-Attention forecasting framework is compared with benchmark models to verify its practicability and reliability. Compared with the BPNN, the mean absolute error, mean absolute percentage error, and mean square error of the FS-GWO-VMD-BiLSTM-Attention model are reduced by 94.03%, 85.82%, and 99.51%, respectively. Furthermore, according to the coverage width-based criterion, KDE-Gaussian is superior to other interval forecasting methods, which can achieve more reliable forecasting of prediction interval.}
}
@article{WU2022125939,
title = {Understanding multi-scale spatiotemporal energy consumption data: A visual analysis approach},
journal = {Energy},
pages = {125939},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125939},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222028250},
author = {Junqi Wu and Zhibin Niu and Xiang Li and Lizhen Huang and Per Sieverts Nielsen and Xiufeng Liu},
keywords = {Demand-side management, Visual analysis, Spatiotemporal patterns, Consumption variability, Segmentation, Demand shift patterns},
abstract = {Understanding energy consumption patterns is crucial for energy demand-side management. Unlike traditional data mining or machine learning-based methods, this paper presents visual analysis methods for exploring energy consumption data from spatial, temporal, and spatiotemporal dimensions, including variability, segmentation, and energy demand shifts. To support the proposed methods, we develop a visual analysis tool that allows users to explore consumption data and validate their hypotheses based on visual results through human-client–server interactions. In particular, we propose a novel potential flow-based method to model energy demand shift patterns and have integrated it into the proposed analysis tool. We comprehensively evaluate the proposed methods and the tool using real-world electricity consumption data from the Shanghai Pudong district, and compare with traditional data mining methods. The results demonstrated the effectiveness and superiority of the proposed visual analysis methods, including its ability to discover the spatiotemporal variability of energy demand, customer groups, and demand shift patterns across different geographical areas and time horizons. All results can be well explained by knowledge of the energy consumption in the study region.}
}
@article{YE2022117674,
title = {Multi-objective optimization of hydrocyclone by combining mechanistic and data-driven models},
journal = {Powder Technology},
volume = {407},
pages = {117674},
year = {2022},
issn = {0032-5910},
doi = {https://doi.org/10.1016/j.powtec.2022.117674},
url = {https://www.sciencedirect.com/science/article/pii/S003259102200568X},
author = {Qing Ye and Peibo Duan and Shibo Kuang and Li Ji and Ruiping Zou and Aibing Yu},
keywords = {Hydrocyclone, Two-fluid model, Steepest ascent, Artificial neural network, Multi-objective optimization},
abstract = {This paper presents a cost-effective method to optimize hydrocyclones used for particle separation. It integrates a mechanistic model for data generation with data-driven models for prediction and optimization. The mechanistic model is based on a validated two-fluid model (TFM), and the data-driven models are the artificial neural network (ANN) and non-dominated sorting genetic algorithm II (NSGA-II). In this integration, the response surface methodology (RSM), coupled with the steepest ascent, is used to design the numerical experiments based on the TFM, aiming to achieve reliable prediction through limited numerical experiments or training data. The applicability of the proposed method is demonstrated by multi-variable and multi-objective optimization of hydrocyclone geometry to achieve low pressure drop and accurate separation, especially for fine particles. The optimization result is elucidated using the multiphase flows predicted by the TFM.}
}
@article{LIU2022,
title = {New metabolic alterations and predictive marker pipecolic acid in sera for esophageal squamous cell carcinoma},
journal = {Genomics, Proteomics & Bioinformatics},
year = {2022},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2021.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1672022922000286},
author = {Lei Liu and Jia Wu and Minxin Shi and Fengying Wang and Haimin Lu and Jibing Liu and Weiqin Chen and Guanzhen Yu and Dan Liu and Jing Yang and Qin Luo and Yan Ni and Xing Jin and Xiaoxia Jin and Wen-Lian Chen},
keywords = {Esophageal squamous cell carcinoma, Serum metabolome, Esophagectomy, Predictive potential, Pipecolic acid},
abstract = {Esophageal squamous cell carcinoma (ESCC) is a major histological subtype of esophageal cancer with a poor prognosis. Although several serum metabolomic investigations have been reported, ESCC tumor-associated metabolic alterations and predictive biomarkers in sera have not been defined. Here, we enrolled 34 treatment-naive patients with ESCC and collected their pre- and post-esophagectomy sera together with the sera from 34 healthy volunteers for a metabolomic survey. Our comprehensive analysis identified ESCC tumor-associated metabolic alterations as represented by a panel of 12 serum metabolites. Notably, postoperative abrosia and parenteral nutrition substantially perturbed the serum metabolome. Furthermore, we performed an examination using sera from carcinogen-induced mice at the dysplasia and ESCC stages and identified three ESCC tumor-associated metabolites conserved between mice and humans. Notably, among these metabolites, the level of pipecolic acid was observed to be progressively increasing in mouse sera from dysplasia to cancerization, and it could be used to accurately discriminate between mice at the dysplasia stage and healthy control mice. Furthermore, this metabolite is essential for ESCC cells to restrain oxidative stress-induced DNA damage and cell proliferation arrest. Together, this study revealed a panel of 12 ESCC tumor-associated serum metabolites with potential for monitoring therapeutic efficacy and disease relapse, presented evidence for refining parenteral nutrition composition, and highlighted serum pipecolic acid as an attractive biomarker for predicting ESCC tumorigenesis.}
}
@article{HERRERA2022121466,
title = {The manipulation of Euribor: An analysis with machine learning classification techniques},
journal = {Technological Forecasting and Social Change},
volume = {176},
pages = {121466},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121466},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100901X},
author = {Rubén Herrera and Francisco Climent and Pedro Carmona and Alexandre Momparler},
keywords = {Euribor, Rate-fixing, Manipulation, Collusion, Panel bank, Machine learning, Classification},
abstract = {The manipulation of the Euro Interbank Offered Rate (Euribor) was an affair which had a great impact on international financial markets. This study tests whether advanced data processing techniques are capable of classifying Euribor panel banks as either manipulating or non-manipulating on the basis of patterns found in quotes submissions. For this purpose, panel banks’ daily contributions have been studied and monthly variables obtained that denote different contribution patterns for Euribor panel banks. Thus, in accordance with the court verdict, banks are categorized as manipulating and non-manipulating and Machine Learning classification techniques such as Supervised Learning, Anomaly Detection and Cluster Analysis are applied in order to discriminate between convicted and acquitted banks. The results show that out of seven manipulative banks, five are detected by Machine Learning using Deep Learning algorithms, all five presenting very similar contribution patterns. This is consistent with Anomaly Detection which confirms that several manipulating banks present similar levels of abnormality in their contributions. In addition, the Cluster Analysis facilitates gathering the five most active banks in illicit actions. In conclusion, administrators and supervisors might find these techniques useful to detect potentially illicit actions by banks involved in the Euribor rate-setting process.}
}
@article{ZHANG2022103462,
title = {Towards automation of in-season crop type mapping using spatiotemporal crop information and remote sensing data},
journal = {Agricultural Systems},
volume = {201},
pages = {103462},
year = {2022},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2022.103462},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X22000981},
author = {Chen Zhang and Liping Di and Li Lin and Hui Li and Liying Guo and Zhengwei Yang and Eugene G. Yu and Yahui Di and Anna Yang},
keywords = {Crop mapping, Agriculture 4.0, Remote sensing, Cropland Data Layer, Sentinel-2, Google Earth Engine},
abstract = {CONTEXT
Mapping crop types from satellite images is a promising application in agricultural systems. However, it is a challenge to automate in-season crop type mapping over a large area because of the insufficiency of ground truth and issues of scalability, reusability, and accessibility of the classification model. This study introduces a framework for automatic crop type mapping using spatiotemporal crop information and Sentinel-2 data based on Google Earth Engine (GEE). The main advantage of the framework is using the trusted pixels extracted from the historical Cropland Data Layer (CDL) to replace ground truth and label training samples in satellite images.
OBJECTIVE
This paper will achieve three objectives: (1) assessing spatiotemporal crop information derived from the historical crop cover maps; (2) mapping crop cover, mainly crop fields without regular historical crop rotation patterns, from remote sensing data using supervised learning classification and validating mapping results; and (3) automating in-season crop mapping and exploring the scalability of the framework.
METHODS
The proposed crop mapping workflow consists of four stages. The data preparation stage preprocesses CDL and Sentinel-2 data into the required structure. The spatiotemporal crop information sampling stage extracts trusted pixels from the historical CDL time series and labels Sentinel-2 data. Then a crop type classification model can be trained using the supervised learning classifier in the model training stage. In the mapping/validation stage, an in-season crop cover map over the full Sentinel-2 tile will be produced using the trained model and the classification performance will be validated using CDL or other ground truth data.
RESULTS AND CONCLUSIONS
We systematically perform a group of experiments for in-season mapping of five major crop types (corn, cotton, rice, soybeans, and soybeans-wheat double cropping) over the Mississippi Delta region. The result indicates that the crop cover map of the study area is expected to reach 80%–90% agreement with CDL within the growing season. To further facilitate the use of the framework, we also develop a GEE-enabled online prototype, In-season Crop Mapping Kit, and explore its scalability over agricultural fields in various ecoregions including California, Idaho, Kansas, and Illinois.
SIGNIFICANCE
The mapping-without-ground-truth approach described in this paper can significantly reduce ground truthing process and save substantial resource needs and labor costs, which is applicable to the production of in-season CDL-like data for the entire United States. The findings and outputs will benefit the agriculture community and other agricultural sectors ranging from government, academia, and companies.}
}
@article{FESSENMAYR20221349,
title = {Selection of traceability-based, automated decision-making methods in global production networks},
journal = {Procedia CIRP},
volume = {107},
pages = {1349-1354},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.156},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122004401},
author = {Franziska Fessenmayr and Martin Benfer and Patrizia Gartner and Gisela Lanza},
keywords = {Automated decision-making, framework, global production networks, traceability, supply chain, method selection, data, decisions, production, adaptable PPS},
abstract = {Automating traceability-based decision-making can shorten the reaction time to supply chain disruptions. This paper develops a framework for choosing automated decision-making (ADM) methods based on traceability data. It contains a toolbox comprising methods suitable for ADM, respective selection criteria and a new process to select a suitable ADM method based on companies’ requirements. This process is based on an evaluation matrix matching methods and criteria. As a result, the ADM framework suggests the most suitable method to automate a specifically chosen decision. The developed framework is validated in the supply chain of a globally operating truck manufacturer.}
}
@article{TIAN2022157730,
title = {Exploring a multisource-data framework for assessing ecological environment conditions in the Yellow River Basin, China},
journal = {Science of The Total Environment},
volume = {848},
pages = {157730},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.157730},
url = {https://www.sciencedirect.com/science/article/pii/S004896972204829X},
author = {Yuqing Tian and Zongguo Wen and Xiu Zhang and Manli Cheng and Mao Xu},
keywords = {Ecological environment conditions, Landscape ecological risk, Road network density, Industry density, Knowledge-based raster mapping},
abstract = {Ecological environment conditions (EEC) assessment plays an important role in watershed management. However, due to insufficient field data, EEC assessment in large-scale watersheds faces challenges. Our study was conducted to develop an effective EEC assessment method framework that was capable of reducing the use of field data. Three indicators were developed from multisource data, including landscape ecological risk index (LERI), road network density (RND), and industry density (ID). The knowledge-based raster mapping approach integrated the three indicators into an overall score of the EEC. Then model validation was conducted with principal components of water quality from field sampling data by Pearson correlation analysis methods. Finally, we applied and demonstrated the constructed method framework in the EEC assessment of the YRB.The results showed that bad EEC (0.5326 < Overall score ≤ 0.7679) areas were mainly distributed in the northern part of the YRB, showing a circular distribution pattern. The areas with bad EEC were 15.84 million km2, accounting for 19.87 % of the YRB. The area of the highest LERI (0.157 < LERI≤0.246), the highest RND (4.4435 < RND ≤ 8.5574), and the highest ID (0.1403 < ID≤0.2597) finally converted to bad EEC was 7.22 million km2, 0.78 million km2, and 0.91 million km2, respectively. The results indicated that the ecological risk factors were the primary challenges for improving EEC, followed by industrial agglomeration and road network factors. The primary factors affecting EEC varied between the provinces in the YRB, suggesting that provinces take the management strategies and measures should be adaptive. The correlation coefficients between EEC and the principal components of water quality characteristics were between 0.022 and 0.241, P < 0.05. These findings validated that our method framework could distinguish the spatial variation of EEC in detail and further provide effective support for watershed management.}
}
@article{BARNWAL2022106692,
title = {Sugar and stops in drivers with insulin-dependent type 1 diabetes},
journal = {Accident Analysis & Prevention},
volume = {173},
pages = {106692},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2022.106692},
url = {https://www.sciencedirect.com/science/article/pii/S0001457522001282},
author = {Ashirwad Barnwal and Pranamesh Chakraborty and Anuj Sharma and Luis Riera-Garcia and Koray Ozcan and Sayedomidreza Davami and Soumik Sarkar and Matthew Rizzo and Jennifer Merickel},
keywords = {Naturalistic driving, Unsafe stopping, Driver risk, Type 1 diabetes, Hypoglycemia, Hyperglycemia},
abstract = {Background
Diabetes is a major public health challenge, affecting millions of people worldwide. Abnormal physiology in diabetes, particularly hypoglycemia, can cause driver impairments that affect safe driving. While diabetes driver safety has been previously researched, few studies link real-time physiologic changes in drivers with diabetes to objective real-world driver safety, particularly at high-risk areas like intersections. To address this, we investigated the role of acute physiologic changes in drivers with type 1 diabetes mellitus (T1DM) on safe stopping at stop intersections.
Methods
18 T1DM drivers (21–52 years, μ = 31.2 years) and 14 controls (21–55 years, μ = 33.4 years) participated in a 4-week naturalistic driving study. At induction, each participant’s personal vehicle was instrumented with a camera and sensor system to collect driving data (e.g., GPS, video, speed). Video was processed with computer vision algorithms detecting traffic elements (e.g., traffic signals, stop signs). Stop intersections were geolocated with clustering methods, state intersection databases, and manual review. Videos showing driver stop intersection approaches were extracted and manually reviewed to classify stopping behavior (full, rolling, and no stop) and intersection traffic characteristics.
Results
Mixed-effects logistic regression models determined how diabetes driver stopping safety (safe vs. unsafe stop) was affected by 1) disease and 2) at-risk, acute physiology (hypo- and hyperglycemia). Diabetes drivers who were acutely hyperglycemic (≥ 300 mg/dL) had 2.37 increased odds of unsafe stopping (95% CI: 1.26–4.47, p = 0.008) compared to those with normal physiology. Acute hypoglycemia did not associate with unsafe stopping (p = 0.537), however the lower frequency of hypoglycemia (vs. hyperglycemia) warrants a larger sample of drivers to investigate this effect. Critically, presence of diabetes alone did not associate with unsafe stopping, underscoring the need to evaluate driver physiology in licensing guidelines.
Conclusion
This study links acute, abnormal physiologic fluctuations in drivers with diabetes to driver safety based on unsafe stopping at stop-controlled intersections, providing recommendations for clinicians aimed at improving patient safety, fair licensing guidelines, and targets for developing advanced driver assistance systems.}
}
@article{KRETZSCHMAR2022100546,
title = {Challenges for modelling interventions for future pandemics},
journal = {Epidemics},
volume = {38},
pages = {100546},
year = {2022},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2022.100546},
url = {https://www.sciencedirect.com/science/article/pii/S1755436522000081},
author = {Mirjam E. Kretzschmar and Ben Ashby and Elizabeth Fearon and Christopher E. Overton and Jasmina Panovska-Griffiths and Lorenzo Pellis and Matthew Quaife and Ganna Rozhnova and Francesca Scarabel and Helena B. Stage and Ben Swallow and Robin N. Thompson and Michael J. Tildesley and Daniel Villela},
keywords = {Mathematical models, Pandemics, Pharmaceutical interventions, Non-pharmaceutical interventions, Policy support},
abstract = {Mathematical modelling and statistical inference provide a framework to evaluate different non-pharmaceutical and pharmaceutical interventions for the control of epidemics that has been widely used during the COVID-19 pandemic. In this paper, lessons learned from this and previous epidemics are used to highlight the challenges for future pandemic control. We consider the availability and use of data, as well as the need for correct parameterisation and calibration for different model frameworks. We discuss challenges that arise in describing and distinguishing between different interventions, within different modelling structures, and allowing both within and between host dynamics. We also highlight challenges in modelling the health economic and political aspects of interventions. Given the diversity of these challenges, a broad variety of interdisciplinary expertise is needed to address them, combining mathematical knowledge with biological and social insights, and including health economics and communication skills. Addressing these challenges for the future requires strong cross-disciplinary collaboration together with close communication between scientists and policy makers.}
}
@article{NWAILA202271,
title = {Artificial intelligence-based anomaly detection of the Assen iron deposit in South Africa using remote sensing data from the Landsat-8 Operational Land Imager},
journal = {Artificial Intelligence in Geosciences},
volume = {3},
pages = {71-85},
year = {2022},
issn = {2666-5441},
doi = {https://doi.org/10.1016/j.aiig.2022.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666544122000272},
author = {Glen T. Nwaila and Steven E. Zhang and Julie E. Bourdeau and Yousef Ghorbani and Emmanuel John M. Carranza},
keywords = {Anomaly detection, Iron deposit, Lansat-8, Remote sensing, Machine learning, Exploration, Prospecting},
abstract = {Most known mineral deposits were discovered by accident using expensive, time-consuming, and knowledge-based methods such as stream sediment geochemical data, diamond drilling, reconnaissance geochemical and geophysical surveys, and/or remote sensing. Recent years have seen a decrease in the number of newly discovered mineral deposits and a rise in demand for critical raw materials, prompting exploration geologists to seek more efficient and inventive ways for processing various data types at different phases of mineral exploration. Remote sensing is one of the most sought-after tools for early-phase mineral prospecting because of its broad coverage and low cost. Remote sensing images from satellites are publicly available and can be utilised for lithological mapping and mineral exploitation. In this study, we extend an artificial intelligence-based, unsupervised anomaly detection method to identify iron deposit occurrence using Landsat-8 Operational Land Imager (OLI) satellite imagery and machine learning. The novelty in our method includes: (1) knowledge-guided and unsupervised anomaly detection that does not assume any specific anomaly signatures; (2) detection of anomalies occurs only in the variable domain; and (3) a choice of a range of machine learning algorithms to balance between explain-ability and performance. Our new unsupervised method detects anomalies through three successive stages, namely (a) stage I – acquisition of satellite imagery, data processing and selection of bands, (b) stage II – predictive modelling and anomaly detection, and (c) stage III – construction of anomaly maps and analysis. In this study, the new method was tested over the Assen iron deposit in the Transvaal Supergroup (South Africa). It detected both the known areas of the Assen iron deposit and additional deposit occurrence features around the Assen iron mine that were not known. To summarise the anomalies in the area, principal component analysis was used on the reconstruction errors across all modelled bands. Our method enhanced the Assen deposit as an anomaly and attenuated the background, including anthropogenic structural anomalies, which resulted in substantially improved visual contrast and delineation of the iron deposit relative to the background. The results demonstrate the robustness of the proposed unsupervised anomaly detection method, and it could be useful for the delineation of mineral exploration targets. In particular, the method will be useful in areas where no data labels exist regarding the existence or specific spectral signatures of anomalies, such as mineral deposits under greenfield exploration.}
}
@article{XU2022113223,
title = {Generating 5 km resolution 1981–2018 daily global land surface longwave radiation products from AVHRR shortwave and longwave observations using densely connected convolutional neural networks},
journal = {Remote Sensing of Environment},
volume = {280},
pages = {113223},
year = {2022},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2022.113223},
url = {https://www.sciencedirect.com/science/article/pii/S0034425722003297},
author = {Jianglei Xu and Shunlin Liang and Han Ma and Tao He},
keywords = {Surface longwave radiation, AVHRR, Deep neural network, Long time series, Shortwave observation},
abstract = {Surface longwave radiation (SLWR) components, including downward longwave radiation (DLR), upward longwave radiation (ULR), and net longwave radiation (NLR), are major contributors to the Earth's surface radiation budget and play important roles in ecological, hydrological, and atmospheric processes. Previous SLWR products have different drawbacks, such as being temporally short (after 2000), spatially coarse (≥ 25 km), and instantaneous values, which hinder their in-depth applications in land surface process modeling and climate trends analysis. Here, we reported the Advanced Very High-Resolution Radiometer (AVHRR)-based Global LAnd Surface Satellites (GLASS-AVHRR) SLWR products over the global land surface at a 5 km spatial resolution and 1 day temporal resolution between 1981 and 2018. These products were generated using multiple densely connected convolutional neural networks (DesCNNs) from the AVHRR top-of-atmosphere (TOA) reflected and emitted observations and European Centre for Medium-Range Weather Forecasts (ECMWF) fifth generation reanalysis (ERA5) near-surface meteorological data. DesCNNs were trained using integrated SLWR samples derived from the Moderate Resolution Imaging Spectroradiometer (MODIS)-based GLASS, Clouds and the Earth's Radiant Energy System Synoptic (CERES-SYN), and ERA5 SLWR products. In situ measurements from 231 globally distributed sites were used to evaluate the GLASS-AVHRR SLWR estimates. The results illustrated the overall high accuracies of GLASS-AVHRR SLWR products with root-mean-square-errors (RMSEs) of 18.66, 14.92, and 16.29 Wm−2, and mean bias errors (MBEs) of −2.69, −3.77, and 0.49 Wm−2 for all-sky DLR, ULR, and NLR, respectively. We found good correlation and consistency between GLASS-AVHRR and both CERES-SYN and ERA5 in terms of spatial patterns, latitudinal gradient, and temporal evolution. Our results revealed the significant contribution of shortwave observations to SLWR estimation owing to the high amounts of clouds over polar regions and water vapor and clouds in tropical areas, which was not previously widely recognized by the remote sensing community. GLASS-AVHRR SLWR products were updated, documented, and made available to the public at www.glass.umd.edu and www.geodata.cn.}
}
@article{HE2022694,
title = {How digitalized interactive platforms create new value for customers by integrating B2B and B2C models? An empirical study in China},
journal = {Journal of Business Research},
volume = {142},
pages = {694-706},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322000054},
author = {Jiaxun He and Shuang Zhang},
keywords = {Digitalized interactive platform, Platform value, Customer share, B2B2C},
abstract = {The traditional B2B model can’t meet the firm’s marketing and customer relationship management demands in the digital environment, and B2B and B2C separately is insufficient to the industry. Thus, we propose a digitalized interactive platform, which is a hybrid of B2B and B2C business models. This B2B2C (business-to-business-to-consumer) business model has transitioned from a traditional channel-driven mode to the integration of platform resources, reflecting the platform value. This study analyzes survey data collect from manufacturing and service industries based in China. The study finds that multi-dimensional platform value positively affects overall platform value, which in turn positively affects platform brand engagement. Moreover, platform brand engagement positively influences platform brand loyalty, ultimately increasing customer share. Thus, the study provides a new theoretical explanation and managerial implication for the value creation of digitalized interactive platforms.}
}
@article{RAHAMAN2022101162,
title = {Identifying the effect of monsoon floods on vegetation and land surface temperature by using Google Earth Engine},
journal = {Urban Climate},
volume = {43},
pages = {101162},
year = {2022},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2022.101162},
url = {https://www.sciencedirect.com/science/article/pii/S2212095522000803},
author = {Sk Nafiz Rahaman and Nishat Shermin},
keywords = {Flood mapping, Vegetation, LST, Google Earth Engine, Satellite imagery},
abstract = {Flood is one of the most devastating climatic disasters around the world. The physical and infrastructural damage of floods is uncontrollable and challenging to recover. Though the mismanagement of the water system is one cause of flood, some countries face a seasonal natural flood, which is impossible to avoid. The history of flood affection in these countries are long, and people don't have any other choice but to adapt to this circumstance every year. The continuous flood event has several climatic impacts that are not broadly documented and remain in the shadows of severe infrastructural damage. This research aims to identify the effect of monsoon floods on vegetation and land surface temperature (LST). The study area is the northeast part of Bangladesh, a highly flood-prone area. The research incorporates Google Earth Engine (GEE) to manage the satellite image data related to this research which are Sentinel-1 SAR imagery and Landsat-8 imagery. Six years of data from 2015 to 2020 have been taken to continuously monitor the flooded area, vegetation, and LST dynamics. Primary results indicate a yearly increase and decrease of the flooded area with 57.3% highest increase rate in 2019. A continuous increase of Enhanced Vegetation Index (EVI) value and decrease of LST has a changing pattern similarity with flooded area fluctuation over the year. Also, the flooded areas have around 50% less mean EVI value than the non-flooded areas, eventually rising average LST in flooded areas. 10,024 grids of 1 km × 1 km have been used to extensively analyze the relationship of flood and EVI through correlation and linear regression. The final result reveals a clear negative correlation value (less than 0.56 for all the years) of EVI with flooded areas, having the highest R-squared value of 0.4325 in 2017.}
}
@article{XIAO2022117829,
title = {Status quo and opportunities for building energy prediction in limited data Context—Overview from a competition},
journal = {Applied Energy},
volume = {305},
pages = {117829},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117829},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921011570},
author = {Tong Xiao and Peng Xu and Ruikai He and Huajing Sha},
keywords = {Building energy, Energy prediction, Cross-building prediction, Hybrid model, Data-driven model, Data Preparation},
abstract = {With the evolution of new energy and carbon trading systems, it is important to accurately predict building energy consumption to help energy arrangements. Additionally, the widespread use of smart meters has introduced a new data context for building energy prediction. Building energy prediction techniques need improvement but the ideas of various new prediction methods are still on the way and have not yet been compared and tested side-by-side in the reported studies. Thus, we held a competition called ‘Energy Detective’. To investigate the status quo of the current prediction techniques, we designed a representative prediction case: cross-building prediction with limited physical parameters and historical data. A total of 195 participants formed 89 teams to participate in the competition. This paper describes the models presented in the competition. By analysing the methods and results, we identified strategies for the future development of energy prediction in hybrid modelling and data-driven modelling. For hybrid modelling, we discuss the basic strategies for hybrid models and suggest that more hybrid models can be developed by combining a wide variety of individual models in sequence or parallel or via feedback methods to achieve accurate and interpretable models. For data-driven modelling, we analyse and discuss the areas of improvement for the current data-driven workflow and suggest that processes other than model application are also important and should be carefully considered. Considering the increasing amount of data available for prediction, we discuss the shortcomings and suggestions for improving the current data preparation process. We recommend comprehensive consideration of the anomaly types in data pre-processing and a focus on feature engineering for higher accuracy and model interpretability, while emphasising the vital role of data selection in cross-building energy prediction.}
}
@article{LIU20221998,
title = {The Interaction between Dual Sourcing and Blockchain Adoption under Yield Uncertainty},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1998-2004},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322020109},
author = {Yang Liu and Dmitry Ivanov},
keywords = {coronavirus, yield uncertainty, intertwined supply chain, dual sourcing, blockchain},
abstract = {An unprecedented global crisis caused by the COVID-19 pandemic has revealed multiple challenges for operation management. One of them is yield uncertainty. In this paper, we examine the effect of the dual sourcing strategy and blockchain adoption on the performance of supply chain members in the presence of yield uncertainty. We develop a stylish model in which a decentralized supply chain consists of a downstream firm and the supplier and a centralized supply chain are competing. The downstream firm faced with yield uncertainty decides whether to procure from the competitor and decided whether to adopt blockchain to enhance the information quality about yield rate. Our results show that the dual sourcing strategy is a dominant strategy and the decision-maker benefits from the blockchain adoption if the volatility level of the yield rate is low.}
}
@article{HAO202228428,
title = {Exploration of the oxidation and ablation resistance of ultra-high-temperature ceramic coatings using machine learning},
journal = {Ceramics International},
volume = {48},
number = {19, Part A},
pages = {28428-28437},
year = {2022},
issn = {0272-8842},
doi = {https://doi.org/10.1016/j.ceramint.2022.06.156},
url = {https://www.sciencedirect.com/science/article/pii/S0272884222021599},
author = {Jie Hao and Lihong Gao and Zhuang Ma and Yanbo Liu and Ling Liu and Shizhen Zhu and Weizhi Tian and Xiaoyu Liu and Zhigang Zhou and Alexandr A. Rogachev and Hanyang Liu},
keywords = {Ultra-high-temperature ceramic coatings, Oxidation and ablation resistance, Machine learning, Property prediction},
abstract = {Carbon fiber-reinforced carbon matrix composites (C/C) will be easily oxidized in high temperatures, which will have a great negative effect on their performance. Preparing ultra-high-temperature ceramic (UHTC) coatings is a well-established method to improve the oxidation and ablation resistance of C/C. However, it is time-consuming and costly to obtain these coatings through the traditional experimental method. Motivated by the outstanding performance of machine learning (ML) algorithms in many fields, this study adopts ML algorithms based on historical experimental datasets to build a model. This model will predict the oxidation and ablation resistance, represented by mass ablation rate. For this purpose, variables that affect the mass ablation rate and are easily accessible were used as input features. That includes the chemical composition and essential physics/chemistry properties of coatings and experimental parameters. Seven different ML algorithms were used to establish the model; namely, ridge regression (Ridge), lasso regression (Lasso), kernel ridge regression (KRR), support vector regression (SVR), random forest regression (RFR), AdaBoost regression (ABR), and bagging regression (Bagging). The results show that RFR has the optimal generalization performance with a mean absolute error (MAE) of 0.55, mean-squared error (MSE) of 0.71 and coefficient of determination (R2) of 0.87 on the testing set. SHapley Additive exPlanations (SHAP) analysis of the RFR model explained how these input features affect the mass ablation rate and further provided the critical features for performance prediction. The model established in this study can predict coating performance accurately and accelerate the development of UHTC-coated C/C composites from a data-driven perspective.}
}
@article{ANGELIDOU2022121915,
title = {Emerging smart city, transport and energy trends in urban settings: Results of a pan-European foresight exercise with 120 experts},
journal = {Technological Forecasting and Social Change},
volume = {183},
pages = {121915},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.121915},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522004371},
author = {M. Angelidou and C. Politis and A. Panori and T. Bakratsas and K. Fellnhofer},
keywords = {Foresight, Delphi survey, Urban development, Urban policy, Responsible Research and Innovation (RRI)},
abstract = {Forecasting future trends constitutes a key process for supporting urban and territorial policy making in general. In this work, we explore how the domains of smart cities, smart transport, and smart energy will evolve until 2030 from a scientific and technological perspective, as a means to inform future policies for urban development in Europe. We started our work with an extensive review of recent and relevant research, covering policy and market reports, scientific journal articles, and other scientific publications. Then, a two-round Delphi survey with 120 field experts was conducted to assess the plausibility of the literature review findings to materialize until 2030. According to our empirical findings, there will be several speedy and structural changes in the three domains: we were able to identify a set of 18 statements that are highly probable to become reality in the next decade, whereas 17 statements were classified as plausible but not highly probable, and three statements raised controversiality. This work provides significant added value in supporting territorial policymakers' and stakeholders' decision making under uncertainty, as well as in designing highly relevant research agendas, attuned to contemporary and emerging trends.}
}
@article{NISO2022119056,
title = {Good scientific practice in EEG and MEG research: Progress and perspectives},
journal = {NeuroImage},
volume = {257},
pages = {119056},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119056},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922001859},
author = {Guiomar Niso and Laurens R. Krol and Etienne Combrisson and A. Sophie Dubarry and Madison A. Elliott and Clément François and Yseult Héjja-Brichard and Sophie K. Herbst and Karim Jerbi and Vanja Kovic and Katia Lehongre and Steven J. Luck and Manuel Mercier and John C. Mosher and Yuri G. Pavlov and Aina Puce and Antonio Schettino and Daniele Schön and Walter Sinnott-Armstrong and Bertille Somon and Anđela Šoškić and Suzy J. Styles and Roni Tibon and Martina G. Vilas and Marijn {van Vliet} and Maximilien Chaumon},
keywords = {Magnetoencephalography (MEG), Electroencephalography (EEG), Good scientific practice},
abstract = {Good scientific practice (GSP) refers to both explicit and implicit rules, recommendations, and guidelines that help scientists to produce work that is of the highest quality at any given time, and to efficiently share that work with the community for further scrutiny or utilization. For experimental research using magneto- and electroencephalography (MEEG), GSP includes specific standards and guidelines for technical competence, which are periodically updated and adapted to new findings. However, GSP also needs to be regularly revisited in a broader light. At the LiveMEEG 2020 conference, a reflection on GSP was fostered that included explicitly documented guidelines and technical advances, but also emphasized intangible GSP: a general awareness of personal, organizational, and societal realities and how they can influence MEEG research. This article provides an extensive report on most of the LiveMEEG contributions and new literature, with the additional aim to synthesize ongoing cultural changes in GSP. It first covers GSP with respect to cognitive biases and logical fallacies, pre-registration as a tool to avoid those and other early pitfalls, and a number of resources to enable collaborative and reproducible research as a general approach to minimize misconceptions. Second, it covers GSP with respect to data acquisition, analysis, reporting, and sharing, including new tools and frameworks to support collaborative work. Finally, GSP is considered in light of ethical implications of MEEG research and the resulting responsibility that scientists have to engage with societal challenges. Considering among other things the benefits of peer review and open access at all stages, the need to coordinate larger international projects, the complexity of MEEG subject matter, and today's prioritization of fairness, privacy, and the environment, we find that current GSP tends to favor collective and cooperative work, for both scientific and for societal reasons.}
}
@article{OKORO2022,
title = {Predicting the effects of selected reservoir petrophysical properties on bottomhole pressure via three computational intelligence techniques},
journal = {Petroleum Research},
year = {2022},
issn = {2096-2495},
doi = {https://doi.org/10.1016/j.ptlrs.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2096249522000473},
author = {Emmanuel E. Okoro and Samuel E. Sanni and Tamunotonjo Obomanu and Paul Igbinedion},
keywords = {Computational intelligence, Bottomhole pressure, Petrophysical properties, Heuristic search optimizer, Volvo field data},
abstract = {This study investigates the effects of selected petrophysical properties on predicting flowing well bottomhole pressure. To efficiently situate the essence of this investigation, genetic, imperialist competitive and whale optimization algorithms were used in predicting the bottomhole pressure of a reservoir using production data and some selected petrophysical properties as independent input variables. A total of 15,633 data sets were collected from Volvo field in Norway, and after screening the data, a total of 9161 data sets were used to develop apt computational intelligence models. The data were randomly divided into three different groups: training, validation, and testing data. Two case scenarios were considered in this study. The first scenario involved the prediction of flowing bottomhole pressure using only eleven independent variables, while the second scenario bothered on the prediction of the same flowing bottomhole pressure using the same independent variables and two selected petrophysical properties (porosity and permeability). Each of the two scenarios involved as implied in the first scenario, the use of three (3) heuristic search optimizers to determine optimal model architectures. The optimizers were allowed to choose the optimal number of layers (between 1 and 10), the optimal number of nodal points (between 10 and 100) for each layer and the optimal learning rate required per task/operation. the results, showed that the models were able to learn the problems well with the learning rate fixed from 0.001 to 0.0001, although this became successively slower as the leaning rate decreased. With the chosen model configuration, the results suggest that a moderate learning rate of 0.0001 results in good model performance on the trained and tested data sets. Comparing the three heuristic search optimizers based on minimum MSE, RMSE, MAE and highest coefficient of determination (R2) for the actual and predicted values, shows that the imperialist competitive algorithm optimizer predicted the flowing bottomhole pressure most accurately relative to the genetic and whale optimization algorithm optimizers.}
}
@article{WANG2022106226,
title = {Recognition on the working status of Acetes chinensis quota fishing vessels based on a 3D convolutional neural network},
journal = {Fisheries Research},
volume = {248},
pages = {106226},
year = {2022},
issn = {0165-7836},
doi = {https://doi.org/10.1016/j.fishres.2022.106226},
url = {https://www.sciencedirect.com/science/article/pii/S0165783622000030},
author = {Shuxian Wang and Shengmao Zhang and Yang Liu and Jiaze Zhang and Yongwen Sun and Yuhao Yang and Huijuan Hu and Ying Xiong and Wei Fan and Fei Wang and Fenghua Tang},
keywords = {Acetes chinensis, Quota fishing, Fishing vessels status recognition, 3D convolutional neural network},
abstract = {In the present study, a method for identifying the status of Acetes chinensis fishing vessels based on a 3D convolutional neural network is proposed, so as to protect marine biodiversity, monitor the working status of Acetes chinensis fishing vessels and assist in the realization of quota fishing. The Vessel Monitoring System (VMS) was installed on the quota fishing vessels to collect work data from June 16, 2021 to July 13, 2021. According to the characteristics of the fishing vessels, the work status of the fishing vessels was divided into five statuses such as stopping, sailing, putting net, waiting and pulling net. The 3D convolutional neural network Acetes3DNet was designed to extract the multi-dimensional and multi-level features of the data and trained in the training set. Finally, the effectiveness of the model was verified in the validation set. The training results were combined with the Beidou ship position data to restore the working process of the fishing vessel. The experimental results reveal that after 150 epochs of training, the precision, recall, and f1 score of Acetes3DNet on the training set reached 99.02%, 99.19%, and 99.09%, respectively, while the precision, recall, and f1 score on the validation set reached 97.09%, 96.82%, and 96.68%. Research shows that Acetes3DNet can circumvent the limitations of traditional 2D neural networks in dynamic target detection, complete recognition of the working status of Acetes chinensis quota fishing vessels, and show the historical work process of the ship in an intuitive manner. The experimental results are conducive to standardizing the management of fishing vessels and protecting marine life.}
}
@article{PLOTNIKOVA2022102013,
title = {Applying the CRISP-DM data mining process in the financial services industry: Elicitation of adaptation requirements},
journal = {Data & Knowledge Engineering},
volume = {139},
pages = {102013},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102013},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X22000258},
author = {Veronika Plotnikova and Marlon Dumas and Fredrik P. Milani},
keywords = {Data mining, CRISP-DM, Case study, Requirements},
abstract = {Data mining techniques have gained widespread adoption over the past decades, particularly in the financial services domain. To achieve sustained benefits from these techniques, organizations have adopted standardized processes for managing data mining projects, most notably CRISP-DM. Research has shown that these standardized processes are often not used as prescribed, but instead, they are extended and adapted to address a variety of requirements. To improve the understanding of how standardized data mining processes are extended and adapted in practice, this paper reports on a case study in a financial services organization, aimed at identifying perceived gaps in the CRISP-DM process and characterizing how CRISP-DM is adapted to address these gaps. The case study was conducted based on documentation from a portfolio of data mining projects, complemented by semi-structured interviews with project participants. The results reveal 18 perceived gaps in CRISP-DM alongside their perceived impact and mechanisms employed to address these gaps. The identified gaps are grouped into six categories. Next, they were triangulated and augmented with the gaps discovered in the other studies. Then, the requirements for adapting CRISP-DM to address the gaps were derived, and the directions for the potential adaptations were outlined. The study presents a two-fold contribution. It provides practitioners with a structured set of gaps to be considered when applying CRISP-DM, or similar processes, in the financial services sector. Additionally, the study elicits the requirements and sketches the potential solutions to address these gaps. Also, the number of the identified gaps is generic and applicable to other sectors with similar concerns (e.g. privacy), such as telecom or e-commerce.}
}
@article{LANGER2022119348,
title = {A benchmark for prediction of psychiatric multimorbidity from resting EEG data in a large pediatric sample},
journal = {NeuroImage},
volume = {258},
pages = {119348},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119348},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922004670},
author = {Nicolas Langer and Martyna Beata Plomecka and Marius Tröndle and Anuja Negi and Tzvetan Popov and Michael Milham and Stefan Haufe},
abstract = {Psychiatric disorders are among the most common and debilitating illnesses across the lifespan and begin usually during childhood and adolescence, which emphasizes the importance of studying the developing brain. Most of the previous pediatric neuroimaging studies employed traditional univariate statistics on relatively small samples. Multivariate machine learning approaches have a great potential to overcome the limitations of these approaches. On the other hand, the vast majority of existing multivariate machine learning studies have focused on differentiating between children with an isolated psychiatric disorder and typically developing children. However, this line of research does not reflect the real-life situation as the majority of children with a clinical diagnosis have multiple psychiatric disorders (multimorbidity), and consequently, a clinician has the task to choose between different diagnoses and/or the combination of multiple diagnoses. Thus, the goal of the present benchmark is to predict psychiatric multimorbidity in children and adolescents. For this purpose, we implemented two kinds of machine learning benchmark challenges: The first challenge targets the prediction of the seven most prevalent DSM-V psychiatric diagnoses for the available data set, of which each individual can exhibit multiple ones concurrently (i.e. multi-task multi-label classification). Based on behavioral and cognitive measures, a second challenge focuses on predicting psychiatric symptom severity on a dimensional level (i.e. multiple regression task). For the present benchmark challenges, we will leverage existing and future data from the biobank of the Healthy Brain Network (HBN) initiative, which offers a unique large-sample dataset (N = 2042) that provides a wide array of different psychiatric developmental disorders and true hidden data sets. Due to limited real-world practicability and economic viability of MRI measurements, the present challenge will permit only resting state EEG data and demographic information to derive predictive models. We believe that a community driven effort to derive predictive markers from these data using advanced machine learning algorithms can help to improve the diagnosis of psychiatric developmental disorders.}
}
@article{WANG2022122309,
title = {Research on thermal load prediction of district heating station based on transfer learning},
journal = {Energy},
volume = {239},
pages = {122309},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122309},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221025573},
author = {Chendong Wang and Jianjuan Yuan and Ke Huang and Ji Zhang and Lihong Zheng and Zhihua Zhou and Yufeng Zhang},
keywords = {District heating, District heating station, Heating energy consumption prediction, Machine learning, Transfer learning},
abstract = {Precise prediction of thermal load plays a critical role in China to fulfill the demand of energy saving, carbon emission reduction and environmental protection, and to realize the "3060″ target. This study proposed layer transfer model and merged transfer model for thermal load prediction of the district heating station. Experiment schemes were elaborated to simulate cross-year and cross-site scenarios, and practical data was collected serving the experiments. The prediction accuracy can be maintained without degradation in cross-year scenario, specifically, the coefficient of variation of the root mean squared error fluctuated between −1.09% and +0.45% compared to previous heating season when proposed two models were used. In the cross-site scenario, proposed models can achieve good prediction performance when the training data is insufficient. The coefficient of variation of the root mean squared error of the new model with insufficient training data was reduced by 7.62% on average when merged transfer model was used, which is equivalent to an overall reduction of 41.67%. Furthermore, proposed models can be applied to further optimize the prediction performance, even if beyond the scenarios discussed in this study.}
}
@incollection{CHANDRA2022177,
title = {Chapter 10 - Data visualization: existing tools and techniques},
editor = {Sourav De and Sandip Dey and Siddhartha Bhattacharyya and Surbhi Bhatia},
booktitle = {Advanced Data Mining Tools and Methods for Social Computing},
publisher = {Academic Press},
pages = {177-217},
year = {2022},
series = {Hybrid Computational Intelligence for Pattern Analysis},
isbn = {978-0-323-85708-6},
doi = {https://doi.org/10.1016/B978-0-32-385708-6.00017-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857086000175},
author = {Tej Bahadur Chandra and Anuj Kumar Dwivedi},
keywords = {Dataset, Data model, Data source, Data visualization, Tools, Techniques, Visualization},
abstract = {Over time, data have become the essential competitive factor for businesses/enterprises to grow and develop. Selected businesses/enterprises such as information industrial businesses will put more focus on product innovation or technology for solving the challenges of gigantic data, i.e., capture, storage, analysis, and presentation/application. Enterprises/businesses like banking, manufacturing, and other enterprises will also benefit from analysis and management of huge data and provide more prospects for management/strategy/marketing innovations. For centuries, persons/societies have depended on visual illustrations such as maps and charts to grasp information quickly. Due to the way the human brain processes information, it is faster for people to grasp the meaning of many data points when they are displayed in charts and graphs rather than in piles of spreadsheets or long reports. Data visualization is the presentation of data in a graphical or pictorial format. Over time, as data are collected, stored, and analyzed, decision makers at all stages rely on data visualization/presentation software that enables them to see and visually present fruitful analytical results, find significance among the heaps/millions of variables, communicate established concepts and hypotheses to others, and even forecast/predict the future. By exploring each aspect of existing tools and techniques related to data visualization, the major objective of this chapter is to present essential theoretical aspects in an analytical way with a profound focus on challenges to represent data in visual form and limits in terms of pros and cons of existing tools and techniques.}
}
@incollection{TREIBLMAIER2022127,
title = {Chapter 8 - Blockchain technologies in the digital supply chain},
editor = {Bart L. MacCarthy and Dmitry Ivanov},
booktitle = {The Digital Supply Chain},
publisher = {Elsevier},
pages = {127-144},
year = {2022},
isbn = {978-0-323-91614-1},
doi = {https://doi.org/10.1016/B978-0-323-91614-1.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323916141000083},
author = {Horst Treiblmaier and Abderahman Rejeb and Wafaa A.H. Ahmed},
keywords = {Blockchain adoption barriers, Blockchain adoption drivers, Blockchain technology, Distributed ledger technology, Literature review, Supply chain management},
abstract = {The application of blockchain or, more generally, distributed ledger technology in logistics and supply chain management has created a huge amount of interest among academics and practitioners. Blockchain's inherent characteristics include immutable data, seamless information flows, and shared access to data. Also, the potential to deploy smart contracts (program code executed automatically) with blockchain has raised high hopes for improving the effectiveness, efficiency, and sustainability of supply chains. In this chapter, we clarify the meaning of terms used in the blockchain ecosystem and present the results from an extensive literature review from which we summarize current findings. We highlight the principal drivers influencing blockchain adoption—traceability, trust and transparency, supply chain integration, data security, privacy, and sustainability. We note technical, organizational, and regulatory barriers that may inhibit adoption of blockchain in supply chain applications, including scalability, investment costs versus perceived benefits, data sharing and interoperability challenges, and lack of standards or regulations. Although blockchain adoption is still in its early stages, an increasing number of applications are being reported. We identify reported supply chain applications classified by industry sector. This chapter equips scholars and practitioners with an understanding of how blockchain can provide value in contemporary supply chains.}
}
@article{ROUSIS2022107436,
title = {Socioeconomic status and public health in Australia: A wastewater-based study},
journal = {Environment International},
volume = {167},
pages = {107436},
year = {2022},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2022.107436},
url = {https://www.sciencedirect.com/science/article/pii/S0160412022003634},
author = {Nikolaos I. Rousis and Zhe Li and Richard Bade and Michael S. McLachlan and Jochen F. Mueller and Jake W. O'Brien and Saer Samanipour and Benjamin J. Tscharke and Nikolaos S. Thomaidis and Kevin V. Thomas},
keywords = {Wastewater-based epidemiology, Metoprolol, Atenolol acid, Venlafaxine, Sotalol, Sitagliptin},
abstract = {Analysis of untreated municipal wastewater is recognized as an innovative approach to assess population exposure to or consumption of various substances. Currently, there are no published wastewater-based studies investigating the relationships between catchment social, demographic, and economic characteristics with chemicals using advanced non-targeted techniques. In this study, fifteen wastewater samples covering 27% of the Australian population were collected during a population Census. The samples were analysed with a workflow employing liquid chromatography high-resolution mass spectrometry and chemometric tools for non-target analysis. Socioeconomic characteristics of catchment areas were generated using Geospatial Information Systems software. Potential correlations were explored between pseudo-mass loads of the identified compounds and socioeconomic and demographic descriptors of the wastewater catchments derived from Census data. Markers of public health (e.g., cardiac arrhythmia, cardiovascular disease, anxiety disorder and type 2 diabetes) were identified in the wastewater samples by the proposed workflow. They were positively correlated with descriptors of disadvantage in education, occupation, marital status and income, and negatively correlated with descriptors of advantage in education and occupation. In addition, markers of polypropylene glycol (PPG) and polyethylene glycol (PEG) related compounds were positively correlated with housing and occupation disadvantage. High positive correlations were found between separated and divorced people and specific drugs used to treat cardiac arrhythmia, cardiovascular disease, and depression. Our robust non-targeted methodology in combination with Census data can identify relationships between biomarkers of public health, human behaviour and lifestyle and socio-demographics of whole populations. Furthermore, it can identify specific areas and socioeconomic groups that may need more assistance than others for public health issues. This approach complements important public health information and enables large-scale national coverage with a relatively small number of samples.}
}
@article{WU2022102897,
title = {Spatiotemporal assessments of nutrients and water quality in coastal areas using remote sensing and a spatiotemporal deep learning model},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102897},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102897},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222000991},
author = {Sensen Wu and Jin Qi and Zhen Yan and Fangzheng Lyu and Tao Lin and Yuanyuan Wang and Zhenhong Du},
keywords = {Aquatic environment, Spatiotemporal deep learning, Water quality, Remote sensing, Coastal restoration},
abstract = {Revealing the spatiotemporal variations of nutrients in coastal waters is crucial to the understanding and evaluation of coastal environment, thereby providing efficient guidance for the aquatic environmental treatment. This study proposed a spatiotemporal-incorporated deep learning model, which is easily applicable to establish the quantitative relationships between measured environmental factors and large-scale satellite maps, and can reduce estimation errors by more than 40% compared with non-spatiotemporal-incorporated deep learning model. The spatiotemporal distributions of dissolved inorganic nitrogen (DIN) and dissolved inorganic phosphate (DIP) over 44400 km2 of the East China Sea on 8-day scale from 2010 to 2018 were obtained. Based on the spatiotemporal variations, the water quality patterns were depicted, and the fluctuation variations of the two essential nutrients were found in the harbors with complex anthropogenic influences, in the typical estuaries with multiple river inputs, and in the open seas with important fisheries. Although the concentration of DIN and DIP decreased by 24% and 19% in 9 years, respectively, the water quality level in the inshore sea has not been significantly improved, especially in autumn and winter. Further, we quantitatively analyzed the main factors of deteriorated water and provided scientific suggestions for targeted monitoring and regional cooperative governances.}
}
@article{GE2022106582,
title = {Spatial heterogeneity of long-term environmental changes in a large agricultural wetland in North China: Implications for wetland restoration},
journal = {CATENA},
volume = {219},
pages = {106582},
year = {2022},
issn = {0341-8162},
doi = {https://doi.org/10.1016/j.catena.2022.106582},
url = {https://www.sciencedirect.com/science/article/pii/S0341816222005689},
author = {Yawen Ge and Xin Mao and Zijing She and Linjing Liu and Lei Song and Yuecong Li and Changhong Liu},
keywords = {Sedimentary records, Spatial heterogeneity, Agricultural wetland, Hydrological processes, Land use, Lake Baiyangdian},
abstract = {Understanding long-term environmental changes and man-land interactions within wetlands is important for their ecological maintenance and restoration. However, there has been little focus on the spatial heterogeneity of environmental variations of wetlands, especially large agricultural wetlands, which hinders the development of targeted management programs for specific areas. We conducted a multi-proxy study of the sediments of a typical agricultural wetland, Lake Baiyangdian in North China, to assess the spatial heterogeneity of environmental changes over the past 70 years. The results reveal that the environmental changes in Lake Baiyangdian were spatially heterogeneous, mainly reflected by the response to hydrological variations before the 1970 s, and by changes in trophic level and land cover transitions since the 1990 s, which are strongly linked to changes in agricultural land use. Pollen and grain-size analyses revealed the large-scale spatial heterogeneity of changes in the sedimentary environment. Areas with greater openness, shallower water depths, and proximity to inflowing rivers were generally more sensitive to hydrological processes and provided regional records of environmental change. In contrast, populated areas far from inflows tended to capture more local changes related to anthropogenic impacts. Intensive agricultural activity has led to a continuous increase in nutrient enrichment and significant land cover transitions in Lake Baiyangdian, mainly the loss of natural wetland. Variations in Humulus pollen were a key indicator of these anthropogenic impacts. Hydrological processes and land use changes were together responsible for the spatial heterogeneity of the environmental variations in Lake Baiyangdian over the last 70 years, indicating the dominant role of human impacts on the spatio-temporal pattern of the evolution of this wetland ecosystem, and the complexity of its environmental management.}
}
@article{HAQUE2022108877,
title = {Short-term electrical load forecasting through heuristic configuration of regularized deep neural network},
journal = {Applied Soft Computing},
volume = {122},
pages = {108877},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108877},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622002575},
author = {Ashraful Haque and Saifur Rahman},
keywords = {Short-term electrical load forecasting, Neural networks, Deep learning, Regularization, Times series analysis},
abstract = {An accurate electrical load forecasting is essential for optimal grid operation. The paper presents a methodology for the short-term commercial building electrical load forecasting through a regularized deep neural network: Long Short-Term Memory Recurrent Neural Network (LSTM-RNN). Detailed heuristic analysis regarding relevant input feature selection, the volume of training data, hyperparameter tuning and regularizer selection of an optimal LSTM-RNN network configuration is presented. The regularized LSTM-RNN is used to forecast 30-min and 24-h ahead electrical loads of two commercial buildings in Virginia, USA. The forecast is performed for one week each over four different months in 2019: January, April, July and October to represent four different seasons in North America. The performance of electrical load forecasts has been compared against actual smart meter data from the electric utility of these buildings. For the case study presented, Mean Absolute Percentage Error (MAPE%) with the regularized LSTM-RNN is 4.9%, compared to 6.4%, 9.2% and 13.3% with Shallow-ANN (Artificial Neural Network), Support Vector Regression (SVR) and Linear Regression (LR) respectively for 30-min ahead electrical load forecast. For 24-h ahead electrical load forecast, MAPE (%) is 11.6%, compared to 12.7%, 13.4% and 14.3% with shallow-ANN, SVR and LR respectively. The methodology to configure a deep neural network (LSTM-RNN) for electrical load forecasting presented in this paper can be utilized for optimal forecasting performance.}
}
@article{LENTZ2022103242,
title = {How do information problems constrain anticipating, mitigating, and responding to crises?},
journal = {International Journal of Disaster Risk Reduction},
volume = {81},
pages = {103242},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103242},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922004617},
author = {Erin C. Lentz and Daniel Maxwell},
keywords = {Early warning, Anticipatory action, Humanitarian action, Crisis},
abstract = {The explosion in data availability and new analytical tools combined with increasing humanitarian need and the imperative of anticipatory action compel us to rethink humanitarian information systems and humanitarian action for the future. Synthesizing interviews with humanitarian practitioners, donors, analysts, and researchers and analyses of early warning (EW) information systems and their linkages to Anticipatory Action (AA), we describe six information challenges within the current system: abundant but confusing information, the difficulty of predicting conflict, politicized information, limitations of new analytical tools, varying information needs, and limited data sharing. We then propose an approach to improve the timeliness and appropriateness of action for humanitarian crises and disasters. Rather than ask, “What can we do with the information (early warning and otherwise) that we have to inform action?” we propose asking, “What information do we need for anticipatory (and other) action?” In other words, we propose planning from known and likely hazards and actions back to information needs. Such an approach should help to mitigate shocks before they cause major humanitarian crises. While not all crises can be prevented, this approach could also support responsive action, which is equally important for protecting human life and dignity.}
}
@article{MU2022119683,
title = {A two-stage scheduling method for integrated community energy system based on a hybrid mechanism and data-driven model},
journal = {Applied Energy},
volume = {323},
pages = {119683},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119683},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922009813},
author = {Yunfei Mu and Yurui Xu and Yan Cao and Wanqing Chen and Hongjie Jia and Xiaodan Yu and Xiaolong Jin},
keywords = {Dynamic energy hub (DEH), integrated community energy system (ICES), Off-design performance of equipment, Hybrid mechanism and data-driven model, Two-stage scheduling method},
abstract = {The integrated community energy system (ICES) is an effective means to promote the synergies among multiple energy carriers. However, the off-design performance of equipment challenges the accurate and economical scheduling of the ICES. To solve this problem, a two-stage scheduling method for the ICES based on a hybrid mechanism and data-driven model is proposed in this paper. Combing the mechanism energy hub (EH) model with a data-driven efficiency correction model, a hybrid-driven dynamic energy hub (DEH) with variable equipment efficiency is built first. The EH describes the multi-energy coupling relationship; the embedded efficiency correction model adopts data-driven approaches of polynomial regression (PR) and backpropagation neural networks (BPNNs) to accurately extract nonlinear characteristics of equipment efficiency. On this basis, a two-stage scheduling model for the ICES is developed. In the day-ahead stage, the PR method is applied to calculate equipment efficiency which varies with load rate. The day-ahead scheduling model is established with the aim of minimizing the operating cost. In the intraday stage, considering the effects of load rate, temperature, and atmospheric pressure, the BPNNs method is employed to further correct equipment efficiency using the latest data. Furthermore, a rolling optimization (RO) strategy is used to address the uncertainties of equipment efficiency and load demand to improve the accuracy and economy of the scheduling scheme. Case studies demonstrate that the proposed method can improve the solution speed and accuracy of the scheduling model, and enhance the operating economy of the ICES.}
}
@article{MAN2022106511,
title = {Transfer learning for spatio-temporal transferability of real-time crash prediction models},
journal = {Accident Analysis & Prevention},
volume = {165},
pages = {106511},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106511},
url = {https://www.sciencedirect.com/science/article/pii/S000145752100542X},
author = {Cheuk Ki Man and Mohammed Quddus and Athanasios Theofilatos},
keywords = {Transferability, Transfer learning, Imbalanced dataset, Generative adversarial network, Oversampling},
abstract = {Real-time crash prediction is a heavily studied area given their potential applications in proactive traffic safety management in which a plethora of statistical and machine learning (ML) models have been developed to predict traffic crashes in real-time. However, one of the fundamental issues relating to the application of these models is spatio-temporal transferability. The present paper attempts to address this gap of knowledge by combining Generative Adversarial Network (GAN) and transfer learning to examine the transferability of real-time crash prediction models under an extremely imbalanced data setting. Initially, a baseline model was developed using Deep Neural Network (DNN) with crash and microscopic traffic data collected from M1 Motorway in the UK in 2017. The dataset utilised in the baseline model is naturally imbalanced with 257 crash cases and 16,359,163 non-crash cases. To overcome data imbalance issue, Wasserstein GAN (WGAN) was utilised to generate synthetic crash data. Non-crash data were randomly undersampled due to computational limitations. The calibrated model was then applied to predict traffic crashes for five other datasets obtained from M1 (2018), M4 (2017 & 2018 separately) and M6 Motorway (2017 & 2018 separately) by using transfer learning. Model transferability was compared with standalone models and direct transfer from the baseline model. The study revealed that direct transfer is not feasible. However, models become transferable temporally, spatially, and spatio-temporally if transfer learning is applied. The predictability of the transferred models outperformed existing studies by achieving high Area Under Curve (AUC) values ranging between 0.69 and 0.95. The best transferred model can predict nearly 95% crashes with only a 5% false alarm rate by tuning thresholds. Furthermore, the performances of transferred models are on par with or better than the standalone model. The findings of this study proves that transfer learning can improve model transferability under extremely imbalanced settings which helps traffic engineers in developing highly transferable models in future.}
}
@article{JUNGBLUTH202215,
title = {A gaps-and-needs analysis of vaccine R&D in Europe: Recommendations to improve the research infrastructure},
journal = {Biologicals},
volume = {76},
pages = {15-23},
year = {2022},
issn = {1045-1056},
doi = {https://doi.org/10.1016/j.biologicals.2022.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1045105622000173},
author = {Stefan Jungbluth and Hilde Depraetere and Monika Slezak and Dennis Christensen and Norbert Stockhofe and Laurent Beloeil},
keywords = {Vaccines, Research and development, Research infrastructure, Science policy, Platform technologies, Manufacturing},
abstract = {The COVID-19 pandemic has brought into sharp focus the importance of strategies supporting vaccine development. During the pandemic, TRANSVAC, the European vaccine–research-infrastructure initiative, undertook an in-depth consultation of stakeholders to identify how best to position and sustain a European vaccine R&D infrastructure. The consultation included an online survey incorporating a gaps-and-needs analysis, follow-up interviews and focus-group meetings. Between October 2020 and June 2021, 53 organisations completed the online survey, including 24 research institutes and universities, and 9 pharmaceutical companies; 24 organisations participated in interviews, and 14 in focus-group meetings. The arising recommendations covered all aspects of the vaccine-development value chain: from preclinical development to financing and business development; and covered prophylactic and therapeutic vaccines, for both human and veterinary indications. Overall, the recommendations supported the expansion and elaboration of services including training programmes, and improved or more extensive access to expertise, technologies, partnerships, curated databases, and-data analysis tools. Funding and financing featured as critical elements requiring support throughout the vaccine-development programmes, notably for academics and small companies, and for vaccine programmes that address medical and veterinary needs without a great potential for commercial gain. Centralizing the access to these research infrastructures via a single on-line portal was considered advantageous.}
}
@article{ZHANG2022112704,
title = {A review on occupancy prediction through machine learning for enhancing energy efficiency, air quality and thermal comfort in the built environment},
journal = {Renewable and Sustainable Energy Reviews},
volume = {167},
pages = {112704},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112704},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122005937},
author = {Wuxia Zhang and Yupeng Wu and John Kaiser Calautit},
keywords = {Occupancy prediction, Machine learning, Thermal comfort, Energy efficiency, Building model simulation, Occupancy detection},
abstract = {The occupants' presence, activities, and behaviour can significantly impact the building's performance and energy efficiency. Currently, heating, ventilation, and air-conditioning (HVAC) systems are often run based on assumed occupancy levels and fixed schedules, or manually set by occupants based on their comfort needs. However, the unpredictability and variability of occupancy patterns can lead to over/under the conditioning of space when using such approaches, affecting indoor air quality and comfort. As a result, machine learning-based models and methodologies are progressively being used to forecast occupancy behaviour and routines in buildings, which may subsequently be used to aid in the design and operation of building systems. The present work reviews recent studies employing machine learning methods to predict occupancy behaviour and patterns, with a special focus on its related applications and benefits to building systems, improving energy efficiency, indoor air quality and thermal comfort. The review provides insight into the workflow of a machine learning-based occupancy prediction model, including data collection, prediction, and validation. An organised evaluation of the applicability or suitability of the different data collection methods, machine learning algorithms, and validation methods was carried out.}
}
@incollection{KILINTZIS2022213,
title = {Chapter 7 - Respiratory data management},
editor = {Rui Pedro Paiva and Paulo de Carvalho and Vassilis Kilintzis},
booktitle = {Wearable Sensing and Intelligent Data Analysis for Respiratory Management},
publisher = {Academic Press},
pages = {213-237},
year = {2022},
isbn = {978-0-12-823447-1},
doi = {https://doi.org/10.1016/B978-0-12-823447-1.00009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234471000099},
author = {Vassilis Kilintzis and Nikolaos Beredimas},
keywords = {Data modeling, HL7 FHIR, Problems of respiratory data management, Web services},
abstract = {Respiratory data management includes all the operations and provisions that ensure secure and unambiguous persistent storage and exchange of all the information that pertains to the respiratory function of an individual. Failure of the health data management framework negatively impacts the quality of the provided health-care services in several aspects. Proper design of the system and modeling of the data can reduce the probability of adverse events and enhance security, data safety, maintainability, and sustainability of the system. In this chapter, a set of modern technologies and practices are discussed along with the leading standards and regulation that pertain to the health data management.}
}
@article{ZHOU2022114636,
title = {T-DNA integration and its effect on gene expression in dual Bt gene transgenic Populus ×euramericana cv. Neva},
journal = {Industrial Crops and Products},
volume = {178},
pages = {114636},
year = {2022},
issn = {0926-6690},
doi = {https://doi.org/10.1016/j.indcrop.2022.114636},
url = {https://www.sciencedirect.com/science/article/pii/S0926669022001194},
author = {Xinglu Zhou and Yachao Ren and Shijie Wang and Xinghao Chen and Chao Zhang and Minsheng Yang and Yan Dong},
keywords = { gene, Poplar 107, T-DNA preferentially inserts, RNA-seq, Unintended effect},
abstract = {The integration of Bacillus thuringiensis (Bt) genes is often accompanied by unintended effects along with the improved resistance to targeted pests. The insertion information of transfer DNA (T-DNA) and the expression information of upstream and downstream genes are of great significance for related research on unexpected effects and molecular-level mechanisms. In this study, six dual Bt transgenic Populus × euramericana cv. Neva (poplar 107) lines were used as research objects. We determined growth and physiological indices, and characterized the T-DNA integration using next-generation sequencing technology. The transgenic and non-transgenic lines showed no significant difference in growth index. However, while insect resistance was enhanced, effects related to the integration sites of the Bt gene occurred. A total of 15 insertion sites were detected among the six transgenic lines, and T-DNA preferentially inserted into AT-rich regions of the poplar genome. RNA sequencing and weighted gene co-expression network analysis were used to explore the effects of T-DNA insertion on the gene expression of poplar 107. As a result, the metabolic pathways most affected by the Bt gene were starch and sucrose metabolism and pentose and gluconate conversion. The expression of exogenous Bt had a greater impact than the insertion position or copy number on poplar 107 gene expression, and the Cry1Ac toxin protein also played a major role. This study provides theoretical support for the cultivation and management of poplar 107 new varieties, and provides reference for Poplar insect resistance breeding and its molecular response to Bt gene.}
}
@incollection{MIERONKOSKI202225,
title = {CHAPTER 2 - Smart home technology for geriatric rehabilitation and the Internet of Things},
editor = {Mohamed-Amine Choukou and Shabbir Syed-Abdul},
booktitle = {Smart Home Technologies and Services for Geriatric Rehabilitation},
publisher = {Academic Press},
pages = {25-42},
year = {2022},
isbn = {978-0-323-85173-2},
doi = {https://doi.org/10.1016/B978-0-323-85173-2.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851732000060},
author = {Riitta Mieronkoski and Iman Azimi and Lydia Sequeira and Laura-Maria Peltonen},
keywords = {Smart home technology, geriatric rehabilitation, Internet of Things},
abstract = {Geriatric rehabilitation is an interactive process between a patient and health professional aiming to maintain or restore the best possible function in various areas of life. The Internet of Things (IoT)-based systems offer new means to support geriatric rehabilitation. IoT-based systems consist of multiple technologies embedded with sensors and software to gather and exchange information with systems over the Internet. In addition to offering means for measuring and monitoring health conditions, these technologies may enable real-time and smooth data exchange between professionals to support care provision. IoT-based systems may have a major role in ensuring effective and equally accessible rehabilitation to the geriatric population. However, many issues warrant attention for the safe and efficient development and implementation of these IoT-based systems. This chapter explores contemporary IoT-based technologies available for geriatric rehabilitation, the geriatric rehabilitation service user as IoT user, and potential and challenges with the use of these technologies in geriatric rehabilitation.}
}
@article{ZHANG2022109441,
title = {Automatic tracking for seismic horizons using convolution feature analysis and optimization algorithm},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109441},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109441},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521010846},
author = {Kai Zhang and Niantian Lin and Dong Zhang and Jianbin Zhang and Jiuqiang Yang and Gaopeng Tian},
keywords = {Seismic reflection horizons, Auto-tracking, Machine learning, Convolution feature analysis, Viterbi algorithm},
abstract = {Seismic horizon tracking is a fundamental aspect of seismic data interpretation. However, seismic horizons are typically obtained using manual tracking or a combination of manual tracking and traditional auto-tracking techniques, either of which is a time-consuming and error-prone process. To improve the efficiency and the accuracy of seismic horizon tracking, we developed a convolution feature analysis method on the basis of the traditional coherent technology combined with the Viterbi algorithm, and proposed a method for auto-tracking seismic horizons in complex exploration areas. Firstly, the local seismic waveform data of the target horizon passing through the drilling area have been extracted as the convolution kernel (i.e. the standard seismic trace). Then, the waveform data of each seismic trace in the whole region have been treated with the convolution to obtain the convolution feature in a sliding time window (i.e, the similarity seismic attribute profile). Finally, the convolution feature data have been taken as the inputs, and constraint optimization is performed for the automatic tracking of the seismic horizon. It has the ability to search for the maximum value by integrating the maximum similarity forward and searching for the shortest path method on synthetic and real seismic data. The obtained results show that the proposed method performs effectively for seismic horizons auto-tracking of low signal-to-noise ratio seismic data in complex exploration areas, and also traces the seismic horizons with good continuity and high accuracy.}
}
@article{WEI2022120057,
title = {Dynamics of microbial communities during inulin fermentation associated with the temporal response in SCFA production},
journal = {Carbohydrate Polymers},
volume = {298},
pages = {120057},
year = {2022},
issn = {0144-8617},
doi = {https://doi.org/10.1016/j.carbpol.2022.120057},
url = {https://www.sciencedirect.com/science/article/pii/S0144861722009626},
author = {Siyu Wei and Cheng Wang and Qifan Zhang and Hui Yang and Edward C. Deehan and Xin Zong and Yizhen Wang and Mingliang Jin},
keywords = {Dietary fiber, Gut microbiota, Individual variability, Butyrate, Swine},
abstract = {The ecology driving the remodeling of gut microbial consortia with dietary fiber intervention remains incomplete. We investigated the short-term dynamics of the gut microbiota and metabolic function during inulin fermentation with distinct microbiota from two swine breeds using an in vitro fermentation model. Different gut microbiota at a transient temporal time displayed a similar response to inulin intervention such as the similar fermentation stage and a rapid response followed by gradual stabilization of microbial diversity. Inulin-induced bacterial succession and individual metabolic change were determined by the original microbial compositions, in particular the α-diversity. Levels of short-chain fatty acids (SCFAs) were predictable with the key bacteria by the regression model, especially butyrate was associated with the abundance and ecological interactions of Lactobacillus delbrueckii, Bifidobacterium thermophilum and Megasphaera elsdenii. This study emphasizes the importance of complex ecology to understand fiber-induced microbiome and metabolic changes, thus providing a reference for predictable dietary responses.}
}
@article{MARIKYAN2022572,
title = {“Alexa, let’s talk about my productivity”: The impact of digital assistants on work productivity},
journal = {Journal of Business Research},
volume = {142},
pages = {572-584},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S014829632200025X},
author = {Davit Marikyan and Savvas Papagiannidis and Omer F. Rana and Rajiv Ranjan and Graham Morgan},
keywords = {Digital assistant, Artificial intelligence, Digitalisation, Satisfaction, Job engagement, Productivity},
abstract = {Digital assistants based on artificial intelligence (AI) have been increasingly used in contexts beyond home-oriented services to support individuals in carrying out work-related tasks. Given the lack of empirical evidence on this fast-developing area, this paper aims (1) to explore the factors which can lead to individuals' satisfaction with the use of technology, and (2) to examine the impact of satisfaction on productivity and job engagement. The model was tested using 536 responses from individuals who used digital assistants for work purposes. Results showed that performance expectancy, perceived enjoyment, intelligence, social presence and trust were positively related to satisfaction with digital assistants. Satisfaction with the digital assistants was found to correlate with productivity and engagement. The findings contribute to the literature focusing on the use of AI-based technology supporting and complementing work tasks. They also offer practical recommendations as to how digital assistants could be used in the workplace.}
}
@article{LEWONIEWSKI20223290,
title = {Identification of Important Web Sources of Information on Wikipedia across various Topics and Languages},
journal = {Procedia Computer Science},
volume = {207},
pages = {3290-3299},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.387},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012777},
author = {Włodzimierz Lewoniewski},
keywords = {Wikipedia, reliability, websites, reliable sources, information quality, topic classification},
abstract = {Despite the fact, that over 21 years Wikipedia is edited by volunteers from all over the world with different views, culture, education and competences, this free encyclopedia is one of the most popular source of knowledge in the Internet. Freedom to edit does not mean, that you can put there whatever content you want. One of the core rules of Wikipedia says, that information in its articles should be based on reliable sources and Wikipedia readers must be able to verify particular facts in text. However, reliability is a subjective concept and a reputation of the same source can be assessed diffidently depending on a person (or group of persons), language and topic. So each language version of Wikipedia may have own rules or criteria on how the website must be assessed before it can be used as a source in references. At the same time, nowadays there are over 1 billion websites on the Internet and only few developed language chapters of the encyclopedia contains non-exhaustive lists of less than 1 thousand popular websites with reliability assessment. Additionally, since reputation of the source can be changed during the time, such lists must be updated regularly. This study presents result of important web sources identification based on analysis of over 230 million references that were extracted from over 40 million Wikipedia article of 42 most developed language version. Additionally, general statistics on references usage for each considered Wikipedia language were counted, including average number of references, number of unique references, scientific score, number of websites in references. Next, Wikipedia articles were assigned to different topics in each considered language. This allows to find differences in reliability and popularity of the same sources of information between Wikipedia languages, as well as find important websites in specific areas of knowledge.}
}
@article{PREGNOLATO2022104421,
title = {Towards Civil Engineering 4.0: Concept, workflow and application of Digital Twins for existing infrastructure},
journal = {Automation in Construction},
volume = {141},
pages = {104421},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104421},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002941},
author = {M. Pregnolato and S. Gunner and E. Voyagaki and R. {De Risi} and N. Carhart and G. Gavriel and P. Tully and T. Tryfonas and J. Macdonald and C. Taylor},
keywords = {Digital Twin, Civil Engineering, Infrastructure, Monitoring, Bridge},
abstract = {Digital Twins (DTs) are forecasted to be used in two-thirds of large industrial companies in the next decade. In the Architecture, Engineering and Construction (AEC) sector, their actual application is still largely at the prototype stage. Industry and academia are currently reconciling many competing definitions and unclear processes for developing DTs. There is a compelling need to establish DTs as practice in AEC by developing common procedures and standards tailored to the sector's procedures and use cases. This paper proposes a step-by-step workflow process for developing a DT for an existing asset in the built environment, providing a proof-of-concept case study based on the Clifton Suspension Bridge in Bristol (UK). To achieve its aim, this paper (i) reviews the state-of-the-art of DTs in Civil Engineering, (ii) proposes a working DT-based workflow framework for the built environment applicable to existing assets, (iii) applies the framework and develops of the physical-virtual architecture to a case study of bridge management, and finally (iv) discusses insights from the application. The main novelty lies in the development of a versatile methodological framework that can be applied to the broad context of civil infrastructure. This paper's importance resides in the knowledge challenge, value proposition and operation dictated by developing a DT workflow for the built environment, which ultimately represents a relevant use case for the digital transformation of national infrastructure.}
}
@article{NTAMO2022100025,
title = {Industry 4.0 in Action: Digitalisation of a Continuous Process Manufacturing for Formulated Products},
journal = {Digital Chemical Engineering},
volume = {3},
pages = {100025},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100025},
url = {https://www.sciencedirect.com/science/article/pii/S2772508122000163},
author = {D. Ntamo and E. Lopez-Montero and J. Mack and C. Omar and M.I. Highett and D. Moss and N. Mitchell and P. Soulatintork and P.Z. Moghadam and M. Zandi},
keywords = {Digital Twin, Smart Manufacturing, Continuous manufacturing, Formulated Products, Digital Chemical Industry, Data visualisation, Continuous wet granulation process},
abstract = {The pharmaceutical industry is going through a significant change to adopt smart manufacturing for more integrated supply chains and improved sustainability. Today's competitive market demands have put pressure on healthcare systems to take a comprehensive assessment of the drug life cycle, its environmental effect, industrial use of energy and resources, supply chain, and impact on end-users. The exploitation of emerging Industry 4.0 technologies will allow a sustainable process design and personalised health care system through the realisation of digital twins, which could transform the pharmaceutical sector to be more flexible, robust, adaptive, and smart. A significant level of research and development has been applied to pharmaceutical manufacturing especially in existing, outdated design and scale-up paradigms in isolated unit operations. However, addressing the key challenges in pharmaceutical manufacturing requires whole systems approaches to incorporate Industry 4.0 concepts. This paper aims to share the latest development of an advanced digital twin of a continuous wet granulation and tableting process at The University of Sheffield. These include the delivery of a digital platform consisting of an Advanced Process Control system (APC), mechanistic model platform and industrial IoT platform for data analytics and visualisation. The combined solution aligns with the concepts of Industry 4.0 by providing a digital twin, cloud integration, sophisticated statistical, as well as hybrid and mechanistic models. The models are in turn, used for soft-sensors, Model Predictive Control and Optimisation algorithms to predict and control product Quality Attributes. The potential application of digital twins in the pharmaceutical industry will also be explored.}
}
@article{WANG2022104512,
title = {State of health estimation based on modified Gaussian process regression for lithium-ion batteries},
journal = {Journal of Energy Storage},
volume = {51},
pages = {104512},
year = {2022},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.104512},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X22005333},
author = {Jiwei Wang and Zhongwei Deng and Tao Yu and Akihiro Yoshida and Lijun Xu and Guoqing Guan and Abuliti Abudula},
keywords = {Lithium-ion battery, State of health, Health indicator, Data-driven, Gaussian processes regression},
abstract = {State of health (SOH) estimation of lithium-ion batteries is a challenging and crucial task for consumer electronics, electric vehicles, and micro-rids. This study presents a data-driven battery SOH estimation method based on a novel integrated Gaussian process regression (GPR) model. First, the aging characteristics of batteries are analyzed from multiple perspectives, and three health indicators (HIs) are extracted from battery charging and discharging curves. Then, the Pearson correlation analysis method is used to quantitatively analyze the correlation between the selected HIs and SOH. Next, a novel compound kernel function is proposed for battery SOH estimation, and different pairs of mean function and kernel function chosen from four mean functions and sixteen kernel functions are used to construct GPR models, and their estimation accuracy is compared subsequently. Finally, four different batteries with various initial health conditions from the NASA battery dataset are used to verify the performance of the proposed method. Experiments show that the method proposed in this paper has satisfactory estimation results in terms of accuracy, generalization ability, and robustness. Specifically, its estimated mean-absolute-error (MAE) and root-mean-square-error (RMSE) is only 1.7%, and 2.41%, respectively.}
}
@article{SHEN2022299,
title = {An improved method for investigating urban municipal infrastructures carrying capacity},
journal = {Sustainable Production and Consumption},
volume = {29},
pages = {299-310},
year = {2022},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2021.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S2352550921002992},
author = {Liyin Shen and Xi Chen and Xiaoyun Du and Zhenchuan Yang},
keywords = {Urban municipal infrastructures carrying capacity (UMICC), UMICC carrier, UMICC load, Chinese large cities, Sustainable urban development},
abstract = {Urban municipal infrastructures play an essential role in urban social and economic development. However, the imbalance between the supply and demand of urban municipal infrastructures appears a common problem in cities. This presents an urgent need for a proper method to investigate the carrying capacity of urban municipal infrastructures in the process of pursuing sustainable urban development. In line with this background, this paper defines the concept of urban municipal infrastructures carrying capacity (UMICC) and establishes an improved method for investigating this capacity. The performances of UMICC are designed to four levels, namely, Low-Utilization, Medium-Utilization, High-Utilization, and Over-Utilization. The effectiveness of this method is proven through an empirical analysis by using the data collected from 35 large cities in China. The empirical results reveal that the utilization intensity of UMICC has been decreasing in recent years in the large Chinese cities, in which UMICC carrier has been increasing at a higher speed than that of UMICC load. In particular, some better-developed cities present a “Low-Utilization” performance level, which is considered that urban policy-makers in some metropolises in China should pay more attention to the better utilization of existing municipal infrastructures. By applying the improved UMICC method introduced in this study, decision-makers can identify the specific areas that affecting the UMICC performance, thus tailor-made policy can be designed for improving UMICC performance by adjusting UMICC carrier and load.}
}
@article{GULMA2022101748,
title = {A new geodemographic classification of the influence of neighbourhood characteristics on crime: The case of Leeds, UK},
journal = {Computers, Environment and Urban Systems},
volume = {92},
pages = {101748},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101748},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001551},
author = {Usman Lawal Gulma},
keywords = {Community, Classification, Neighbourhood, Clustering, Social cohesion, Crime},
abstract = {Understanding spatial composition and characteristics of urban communities is an important factor determining the potential for social interactions between residents, greater chances for neighbourhood safety and policy planning. Until recently, neighbourhood classifications were traditionally census-based. This study presents a different approach in developing a community-based area classification for exploring the relationship between neighbourhood characteristics and crime through the integration of new data sources from social media with contextual variables from the traditional census. In this study, partition around medoids (PAM) clustering algorithm is used to partition 105 Leeds community areas into four distinct groups based on their crime rates. Silhouette index and adjusted rand index (ARI) were used to evaluate the validity of clusters internally and externally. The new typology developed, has revealed new insights demonstrating the relationship between social cohesion and crime rates in Leeds community areas. The new partitioned clusters also provide area-based specific information, on criminal activities that could help in policy planning for community building and better resource allocation towards reducing crime rates.}
}
@article{FISHER2022108836,
title = {BEAUT: An ExplainaBle Deep LEarning Model for Agent-Based PopUlations With Poor DaTa},
journal = {Knowledge-Based Systems},
volume = {248},
pages = {108836},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108836},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122003987},
author = {Andrew Fisher and Bart Gajderowicz and Eric Latimer and Tim Aubry and Vijay Mago},
keywords = {Deep q-learning, Neural fitted q-iteration, Simulation, Homelessness, Agent-based},
abstract = {When working with an agent-based system, it may be desirable to develop an algorithm that can predict future time periods for each individual. One such approach that works to accomplish this is the Markov decision process, which takes as its input available actions, unique agent reward functions, and a model of the overall environment. However, it can be difficult to identify the underlying factors that influence decision making when attempting to simulate the behavior of real-world populations. For example, when modeling the transitions of homeless individuals between states such as street and shelter, it can be a challenging task as the external factors impacting them (e.g., weather) may not be readily apparent. Therefore, this paper proposes and evaluates an approach to capture this information in an explainable way from aggregate, real-life data produced by the At Home/Chez Soi project. The proposed algorithm “BEAUT” is a consolidation of a modified deep q-learning (MDQL) and modified neural fitted q-iteration (MNFQ) algorithm that work together to generate a set of probabilistic transition matrices to describe state transitions. BEAUT is evaluated with experimental results that compare its accuracy against similar methods for time-series forecasting using real-world data. Our tests show that BEAUT provides an explainable forecasting model without a loss of accuracy, and in some instances results in higher accuracy.}
}
@article{SUHAIL2022103699,
title = {Towards situational aware cyber-physical systems: A security-enhancing use case of blockchain-based digital twins},
journal = {Computers in Industry},
volume = {141},
pages = {103699},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103699},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000963},
author = {Sabah Suhail and Saif Ur Rehman Malik and Raja Jurdak and Rasheed Hussain and Raimundas Matulevičius and Davor Svetinovic},
keywords = {Anomaly detection, Blockchain, Cyber-Physical Systems (CPSs), Digital Twins (DTs), Industrial Control Systems (ICSs), Internet of Things (IoT), Industry 4.0},
abstract = {The complexity of cyberattacks in Cyber-Physical Systems (CPSs) calls for a mechanism that can evaluate critical infrastructures’ operational behaviour and security without affecting the operation of live systems. In this regard, Digital Twins (DTs) provide actionable insights through monitoring, simulating, predicting, and optimizing the state of CPSs. Through the use cases, including system testing and training, detecting system misconfigurations, and security testing, DTs strengthen the security of CPSs throughout the product lifecycle. However, such benefits of DTs depend on an assumption about data integrity and security. Data trustworthiness becomes more critical while integrating multiple components among different DTs owned by various stakeholders to provide an aggregated view of the complex physical system. This article envisions a blockchain-based DT framework as Trusted Twins for Securing Cyber-Physical Systems (TTS-CPS). With the automotive industry as a CPS use case, we demonstrate the viability of the TTS-CPS framework through a proof of concept. To utilize reliable system specification data for building the process knowledge of DTs, we ensure the trustworthiness of data-generating sources through Integrity Checking Mechanisms (ICMs). Additionally, Safety and Security (S&S) rules evaluated during simulation are stored and retrieved from the blockchain, thereby establishing more understanding and confidence in the decisions made by the underlying systems. Finally, we perform formal verification of the TTS-CPS.}
}
@article{WIRTZ2022101685,
title = {Governance of artificial intelligence: A risk and guideline-based integrative framework},
journal = {Government Information Quarterly},
volume = {39},
number = {4},
pages = {101685},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2022.101685},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X22000181},
author = {Bernd W. Wirtz and Jan C. Weyerer and Ines Kehl},
keywords = {Artificial intelligence, Risks, Guidelines, Governance, Regulation, Framework},
abstract = {This study addresses the growing challenge of governing artificial intelligence (AI) arising from the risks that it increasingly poses to the public sector and society. Based on an in-depth literature analysis, we first identify AI risks and guidelines and classify them into six categories, including technological, data, and analytical risks and guidelines, informational and communicational risks and guidelines, economic risks and guidelines, social risks and guidelines, ethical risks and guidelines, as well as legal and regulatory risks and guidelines. These risks and guidelines are then elaborated and transferred into a four-layered conceptual framework for AI governance. The framework interrelates AI risks and AI guidelines by means of a risk management and guidance process, resulting in an AI governance layer depicting the process for implementation of customised risk mitigation guidelines. The framework constitutes a comprehensive reference point for developing and implementing AI governance strategies and measures in the public sector.}
}
@article{GHADIMI2022121394,
title = {The successful implementation of industry 4.0 in manufacturing: An analysis and prioritization of risks in Irish industry},
journal = {Technological Forecasting and Social Change},
volume = {175},
pages = {121394},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121394},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521008258},
author = {Pezhman Ghadimi and Oisin Donnelly and Kubra Sar and Chao Wang and Amir Hossein Azadnia},
keywords = {Industry 4.0, Risk factor, Manufacturing, Best-worst method, Irish industry},
abstract = {Industry 4.0 is anticipated to revolutionize the manufacturing sector through a digital transformation. With this transformation, many benefits are expected, such as the automation and decentralization of production processes. Nevertheless, enterprises face considerable risks upon successful implementation of Industry 4.0. The uncertainties regarding these risks are currently hindering enterprises’ implementation of Industry 4.0. Although several studies have investigated the adoption of Industry 4.0-related technologies, far too little attention has been devoted to identifying and analyzing the risk factors associated with the adoption of these technologies in manufacturing, especially in Irish industry. Therefore, this study contributes to the existing knowledge by proposing a systematic approach to identifying and ranking these risk factors along with recommending policies to mitigate the highest risks. Fourteen risk factors are identified, and the opinions of 12 industry experts across the Irish manufacturing sector are used to rank these risk factors using an adjusted best-worst method. The lack of standards and lack of methodological approaches was the highest-ranking risk factor, with the risk to capital investment, the lack of talent, the uncertainty in economic benefits and the potential delay to the manufacturing process ranking in the top 5. Policy recommendations to mitigate the highest-ranking risks are proposed based on an analysis of the Irish government's current Industry 4.0 policy. Governments should aim to assist industries in establishing comprehensive standards to increase the rate of successful Industry 4.0 implementation.}
}
@article{LIAO2022102323,
title = {Simplified estimation modeling of land surface solar irradiation: A comparative study in Australia and China},
journal = {Sustainable Energy Technologies and Assessments},
volume = {52},
pages = {102323},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102323},
url = {https://www.sciencedirect.com/science/article/pii/S2213138822003757},
author = {Xuan Liao and Rui Zhu and Man Sing Wong},
keywords = {Land surface solar irradiation, Machine learning, Cloud optical thickness, Aerosol optical thickness, Himawari-8 satellite images, Meteorological data},
abstract = {Solar irradiation maps are fundamental geospatial datasets that have been used in a variety of research fields. However, it is difficult to estimate the continuous distribution of solar irradiation over large areas accurately by using conventional interpolation or extrapolation methods based on only a few observation stations. To tackle this problem, this study proposed a method to estimate spatially continuous land surface solar irradiation based on four machine learning models, namely, Gradient Boosting Machine (GBM), Random Forest (RF), Support Vector Regression (SVR), and Multilayer Perceptron (MLP). Clear-sky solar irradiation data were computed based on time and location, cloud optical thickness (COT) and aerosol optical thickness (AOT) that significantly influence solar irradiation were retrieved from Himawari-8 meteorological satellite images, and land surface solar irradiation data were obtained from observation stations for training and evaluation. To explore the weather effects on land surface solar irradiation, air temperatures, humidity, wind, and atmospheric pressure were also quantified and integrated into the models. As a comparative study, this study collected six-year historical data and estimated solar distribution at a 5-km spatial resolution in Australia and China. Based on the coefficient of determination (R2), normalized Root Mean Square Error (nRMSE), normalized mean bias error (nMBE), and consumption of time (t), the results show that GBM achieved the highest accuracy with R2 >0.7 at all stations, followed by RF, SVR, and MLP. It suggests that the proposed method can provide an accurate and reliable estimation of land surface solar irradiation, compared with the theoretical solar irradiation without the obstacle of the atmosphere. The annual solar distribution maps created by the built methods indicate that the proposed method is simple and effective for large geographical regions and can be used worldwide when similar datasets are obtained.}
}
@article{MIRAMIRKHANI2022101679,
title = {Enabling 5G indoor services for residential environment using VLC technology},
journal = {Physical Communication},
volume = {53},
pages = {101679},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2022.101679},
url = {https://www.sciencedirect.com/science/article/pii/S1874490722000465},
author = {Farshad Miramirkhani and Mehdi Karbalayghareh and Engin Zeydan and Rangeet Mitra},
keywords = {Visible light communication (VLC), Ray-tracing, Adaptive transmission, 5G services},
abstract = {Visible light communication (VLC) has emerged as a viable complement to traditional radio frequency (RF) based systems and as an enabler for high data rate communications for beyond-5G (B5G) indoor communication systems. In particular, the emergence of new B5G-based applications with quality of service (QoS) requirements and massive connectivity has recently led to research on the required service-levels and the development of improved physical (PHY) layer methods. As part of recent VLC standards development activities, the IEEE has formed the 802.11bb “Light Communications (LC) for Wireless Local Area Networking” standardization group. This paper investigates the network requirements of 5G indoor services such as virtual reality (VR) and high-definition (HD) video for residential environments using VLC. In this paper, we consider such typical VLC scenarios with additional impairments such as light-emitting diode (LED) nonlinearity and imperfect channel feedback, and propose hyperparameter-free mitigation techniques using Reproducing Kernel Hilbert Space (RKHS) methods. In this context, we also propose using a direct current biased optical orthogonal frequency division multiplexing (DCO-OFDM)-based adaptive VLC transmission method that uses precomputed bit error rate (BER) expressions for these RKHS-based detection methods and performs adaptive BER-based modulation-order switching. Simulations of channel impulse responses (CIRs) show that the adaptive transmission method provides significantly improved error rate performance, which makes it promising for high data rate VLC-based 5G indoor services.}
}
@article{MORAN2022794,
title = {Curious instance selection},
journal = {Information Sciences},
volume = {608},
pages = {794-808},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.07.025},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522007149},
author = {Michal Moran and Tom Cohen and Yuval Ben-Zion and Goren Gordon},
keywords = {Intrinsic motivation learning, Curiosity loop, Reinforcement learning, Machine learning, Data science, Instance selection},
abstract = {In the process of building machine learning models data sometimes must be sampled before the learning process can be applied. This step, known as instance selection, is mostly done to reduce the amount of data in a volume that will allow the computing resources required for the learning phase to be reduced. In addition, it also removes noisy data that can affect the learning quality. While the two objectives are often in conflict, in most current approaches, it is impossible to control the balance between them. We propose a reinforcement learning-based approach for instance selection, called curious instance selection (CIS), which evaluates clusters of instances using the curiosity loop architecture. The output of the algorithm is a matrix that represents the value of adding a cluster of instances to existing instances. This matrix enables the computation of the Pareto front and demonstrates the ability to balance the noise and volume reduction objectives. CIS was evaluated on five datasets, and its performance was compared with the performance of three state-of-the-art algorithms. Our results show that CIS not only provides enhanced flexibility but also achieves higher effectiveness (reduction times accuracy). This approach strengthens the appeal of using curiosity-based algorithms in data science.}
}
@incollection{KIRAN2022153,
title = {Chapter 11 - Internet of things and wearables-enabled Alzheimer detection and classification model using stacked sparse autoencoder},
editor = {Hemanth {D. Jude} and Deepak Gupta and Ashish Khanna and Aditya Khamparia},
booktitle = {Wearable Telemedicine Technology for the Healthcare Industry},
publisher = {Academic Press},
pages = {153-168},
year = {2022},
isbn = {978-0-323-85854-0},
doi = {https://doi.org/10.1016/B978-0-323-85854-0.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323858540000125},
author = {Siripuri Kiran and S. Neelakandan and A. Pratapa Reddy and Sonali Goyal and Balajee Maram and V. Chandra Shekhar Rao},
keywords = {Deep learning, IoT, wearables, Alzheimer disease, Classification, Feature extraction},
abstract = {Presently, Internet of Things (IoT) and wearables have become advanced technologies that intend to ease life and aid people in every aspect of life. They find use in smart healthcare where several wearables and IoT devices are used to monitor the health status of the patients. At the same time, Alzheimer’s disease (AD) is a dynamic, permanent, and neuro-degenerative illness that mainly affects elder people and it progressively degrades the efficiency of the brain, thereby affects memories, learning, and behaviors. The advent of machine learning algorithms has led to the design of IoT and wearables-enabled automated AD diagnosis and classification model. In this view, this chapter introduces an effective IoT and wearables-enabled AD detection and classification model using stacked sparse autoencoder (ADC-SSAE). The proposed ADC-SSAE model enables the wearables to collect patient data and medical examination takes place on the captured data. In addition, the region of interest extraction and bilateral filtering based preprocessing take place to raise the image quality. Besides, local texton XOR patterns (LTxXORP) model is applied as a feature extractor to derive a useful set of features from the preprocessed data. At last, SSAE model is utilized as a classification model to determine the proper class labels of the applied input data. An extensive range of simulations was performed to highlight the better outcome of the ADC-SSAE model. The experimental values obtained by the ADC-SSAE model have ensured the efficacy of the ADC-SSAE model over the compared methods.}
}
@incollection{EMETERE202253,
title = {Chapter 4 - Generating environmental data: Progress and shortcoming},
editor = {Moses Eterigho Emetere},
booktitle = {Numerical Methods in Environmental Data Analysis},
publisher = {Elsevier},
pages = {53-77},
year = {2022},
isbn = {978-0-12-818971-9},
doi = {https://doi.org/10.1016/B978-0-12-818971-9.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128189719000107},
author = {Moses Eterigho Emetere},
keywords = {Data, Environment data, Inset, Remote},
abstract = {Environmental data is that which is based on the measurement of environmental parameters, and its impacts on the ecosystems. Environmental data is a set of quantitative, qualitative, or geographically referenced facts about the state of the environment and how it is changing. There are progress made that needs more improvement and shortcomings that requires urgent action. This chapter is an overview of the progress and shortcoming made in data curation in environmental research.}
}
@article{IBRAHIM2022100063,
title = {A mapping towards a unified municipal platform: An investigative case study from a Norwegian municipality},
journal = {Sustainable Futures},
volume = {4},
pages = {100063},
year = {2022},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2022.100063},
url = {https://www.sciencedirect.com/science/article/pii/S2666188822000016},
author = {Ahmed M. Ibrahim},
keywords = {Digital transformation, Governance, Municipality, Platforms, Smart city, Sustainability, Sustainable development goals (SDGS)},
abstract = {Norwegian municipalities set citizens’ wellbeing a top priority, through transparent social practices and democratic conduct of public services. In 2015, the United Nations (UN) have set a collection of 17 sustainable development goals (SDGs), which are interlinked towards achieving a sustainable future by 2030. In smart sustainable cities context, a multi-stakeholder, physical and virtual platforms are considered pivotal to achieve the UN SDGs. They are core initiator for transformative actions, endeavoring on public service excellence. This research aims to investigate the digital governance platforms utilized, by a case study Norwegian municipality, to achieve the UN SDGs; towards a smart sustainable city. An empirical systematic approach was followed. The approach commenced by the conduct of a 1) narrative review and synthesis of the literature, and the municipal strategic development documents, 2) collection of a developed semi-structured survey, and 3) interviews with the municipal decision makers (including directors, and departmental professional advisors). As part of the study, broad aspects related to municipal governance and services have been discussed with the interviewees, where effectiveness measures have been identified and prioritized. Further, recommendations for improving the municipal platforms and initiatives were identified and discussed. The municipal investigated platforms and the strategic documents have led to formulating a novel conceptual model for the behavior needed towards making a unified municipal platform. Discussion and recommendations were presented to identify future development prospects, towards achieving a transformative integrated ecosystem. Further, the research presents main concepts, to articulate organizational administrative practices, and to inform municipal stakeholders, decision makers and professionals about the existing digital platforms, while proposing a model for a unified municipal governance transformation.}
}
@article{KONG2022378,
title = {Process intensification from conventional to advanced distillations: Past, present, and future},
journal = {Chemical Engineering Research and Design},
volume = {188},
pages = {378-392},
year = {2022},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2022.09.056},
url = {https://www.sciencedirect.com/science/article/pii/S0263876222005536},
author = {Zong Yang Kong and Eduardo Sánchez-Ramírez and Ao Yang and Weifeng Shen and Juan Gabriel Segovia-Hernández and Jaka Sunarso},
keywords = {Process intensification, Advanced distillation, Reactive distillation, Extractive distillation, Reactive-extractive distillation, Sustainability},
abstract = {This perspective paper features the process intensification (PI) application for advanced distillation-based processes. Starting with the historical background of generic PI, we subsequently narrow down the discussion to extractive distillation (ED), reactive distillation (RD), and hybrid reactive-extractive distillation (RED). We categorize the existing PI techniques onto internal and external intensification, where the former does not involve altering the distillation configuration while the latter does. Instead of deliberating the technical aspects, we explicitly highlight the contribution of PI applied to ED, RD, and RED towards societal impact covering energy, economic, environmental, control, and safety perspectives. The future perspectives of PI are discussed in the last section, covering the development of hybrid PI technologies, exploring the energy efficiency of different PI configurations, prioritizing PI beyond energy by considering some other sustainability aspects, and linking PI with the ever-increasing Industry 4.0 applications.}
}
@incollection{KURNI2022297,
title = {Chapter 11 - A forefront insight into the integration of AI and blockchain technologies},
editor = {SK Hafizul Islam and Arup Kumar Pal and Debabrata Samanta and Siddhartha Bhattacharyya},
booktitle = {Blockchain Technology for Emerging Applications},
publisher = {Academic Press},
pages = {297-320},
year = {2022},
series = {Hybrid Computational Intelligence for Pattern Analysis},
isbn = {978-0-323-90193-2},
doi = {https://doi.org/10.1016/B978-0-323-90193-2.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901932000053},
author = {Muralidhar Kurni and K. Saritha and D. Nagadevi and K. {Somasena Reddy}},
keywords = {AI and blockchain fusion, App development industry, Artificial intelligence (AI), Blockchain, Industry 4.0, Integration of AI and blockchain, Shared records, Smart contracts},
abstract = {Artificial intelligence (AI) and blockchain: these two technologies have recently been the trendiest and most revolutionary, progressive technologies. Blockchain technology can automate payments in cryptocurrency and provide admittance in a decentralized, safe, and trustworthy way to shared records, transactions, and log ledgers. Also, with smart contracts, blockchain can handle interactions between participants without an interceder. Furthermore, AI provides intelligence and smart decision-making for human-like machines. This chapter presents a thorough analysis of AI and blockchain integration ability, possibilities, and applications. The incorporation of AI was evaluated as well as how blockchain will influence industry 4.0. We also recognize and address the open issues of AI and blockchain fusion.}
}
@article{FREDRIKSSON2022104038,
title = {Construction related urban disturbances: Identification and linking with an IoT-model},
journal = {Automation in Construction},
volume = {134},
pages = {104038},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104038},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521004891},
author = {Anna Fredriksson and Ahmet Anil Sezer and Vangelis Angelakis and David Gundlegård},
keywords = {Construction transport, IoT modelling, Construction disturbances, Stakeholder management},
abstract = {While being a significant part of the urban development, construction projects disturb different stakeholders in various ways. There are three problems associated with construction disturbances: (i) most of these disturbances are not recognised by the people causing them, (ii) they are not monitored and (iii) if they are to be monitored, data is spread among stakeholders. This paper defines what a disturbance is, presents a list of disturbances, linking disturbances to stakeholders and, categorising them based on their distance from construction sites (responding to (i)). Next, a IoT domain model is developed, demonstrating how IoT in construction needs to be combined with the sensors of smart cities to capture the primitives of these disturbances (responding to (iii)). This is a first step towards enabling large-scale data-gathering of construction transport disturbances (responding to (ii)), which is a necessity to predict them and allow better construction transport planning to decrease disturbances.}
}
@article{HUSAK2022102609,
title = {CRUSOE: A toolset for cyber situational awareness and decision support in incident handling},
journal = {Computers & Security},
volume = {115},
pages = {102609},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102609},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822000086},
author = {Martin Husák and Lukáš Sadlek and Stanislav Špaček and Martin Laštovička and Michal Javorník and Jana Komárková},
keywords = {Cyber situational awareness, OODA Loop, Decision support, Network monitoring, Incident response},
abstract = {The growing size and complexity of today’s computer network make it hard to achieve and maintain so-called cyber situational awareness, i.e., the ability to perceive and comprehend the cyber environment and be able to project the situation in the near future. Namely, the personnel of cybersecurity incident response teams or security operation centers should be aware of the security situation in the network to effectively prevent or mitigate cyber attacks and avoid mistakes in the process. In this paper, we present a toolset for achieving cyber situational awareness in a large and heterogeneous environment. Our goal is to support cybersecurity teams in iterating through the OODA loop (Observe, Orient, Decide, Act). We designed tools to help the operator make informed decisions in incident handling and response for each phase of the cycle. The Observe phase builds on common tools for active and passive network monitoring and vulnerability assessment. In the Orient phase, the data on the network are structured and presented in a comprehensible and visually appealing manner. The Decide phase opens opportunities for decision-support systems, in our case, a recommender system that suggests the most resilient configuration of the critical infrastructure. Finally, the Act phase is supported by a service that orchestrates network security tools and allows for prompt mitigation actions. Finally, we present lessons learned from the deployment of the toolset in the campus network and the results of a user evaluation study.}
}
@article{HATAMZAD2022108682,
title = {Intelligent cost-effective winter road maintenance by predicting road surface temperature using machine learning techniques},
journal = {Knowledge-Based Systems},
volume = {247},
pages = {108682},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108682},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122003136},
author = {Mahshid Hatamzad and Geanette Cleotilde Polanco Pinerez and Johan Casselgren},
keywords = {Decision-making units, Decision support systems, Machine learning techniques, Road surface temperature, Winter road maintenance},
abstract = {Since Winter Road Maintenance (WRM) is an important activity in Nordic countries, accurate intelligent cost-effective WRM can create precise advance plans for developing decision support systems to improve traffic safety on the roads, while reducing cost and negative environmental impacts. Lack of comprehensive knowledge and inaccurate WRM information would lead to a certain loss of WRM budget, safety reduction, and irreparable environmental damage. This study proposes an intelligent methodology that uses data envelopment analysis and machine learning techniques. In the proposed methodology, WRM efficiency is calculated by data envelopment analysis for different decision-making units (roads), and inefficient units need to be considered for further assessments. Therefore, road surface temperature is predicted by means of machine learning methods, in order to achieve efficient and effective WRM on the roads during winter in cold regions. In total, four different methods have been used to predict road surface temperature on an inefficient road. One of these is linear regression, which is a classical statistical regression technique (ordinary least square regression); the other three methods are machine-learning techniques, including support vector regression, multilayer perceptron artificial neural network, and random forest regression. Graphical and numerical results indicate that support vector regression is the most accurate method.}
}
@article{MENDES2022157428,
title = {Evaluating the BFAST method to detect and characterise changing trends in water time series: A case study on the impact of droughts on the Mediterranean climate},
journal = {Science of The Total Environment},
volume = {846},
pages = {157428},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.157428},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722045260},
author = {Maria Paula Mendes and Victor Rodriguez-Galiano and David Aragones},
keywords = {Water resources, LOESS, BFAST, Time series analysis, Climate variability, Climate resilience},
abstract = {Mediterranean climate regions are facing increased aridity conditions and water scarcity, thus needing integrated management of water resources. Detecting and characterising changes in water resources over time is the natural first step towards identifying the drivers of these changes and understanding the mechanism of change. The aim of this study is to evaluate the potential of Breaks For Additive Seasonal and Trend (BFAST) method to identify gradual (trend) and abrupt (step- change) changes in the freshwater resources time series over a long-term period. This research shows an alternative to the Pettitt's test, LOESS (locally estimated scatterplot smoothing) filter, Mann–Kendall trend test among other common methods for change detection in hydrological data, and paves the way for further scientific investigation related to climate variability and its influence on water resources. We used the monthly accumulated stored water in three reservoirs, the monthly groundwater levels of three hydrological settings and a standardized precipitation index to show BFAST performance. BFAST was successfully applied, enabling: (1) assessment of the suitability of past management decisions when tackling drought events; (2) detection of recovery and drawdown periods (duration and magnitude values) of accumulated stored water in reservoirs and groundwater bodies after wet and dry periods; 3) measurement of resilience to drought conditions; (4) establishment of similarities/differences in trends between different reservoirs and groundwater bodies with regard to drought events.}
}
@article{DIETRICH202253,
title = {Towards a model for holistic mapping of supply chains by means of tracking and tracing technologies},
journal = {Procedia CIRP},
volume = {107},
pages = {53-58},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002256},
author = {Fabian Dietrich and Moritz Hoffmann and Mario Angos Mediavilla and Louis Louw and Daniel Palm},
keywords = {Supply Chain Mapping, Tracking, Tracing, Transparency, Industry 4.0},
abstract = {The usage of tracking and tracing technologies not only enables transparency and visibility of supply chains but also offers far-reaching advantages for companies, such as ensuring product quality or reducing supplier risks. Increasing the amount of shared information supports both internal and external planning processes as well as the stability and resilience of globally operating value chains. This paper aims to differentiate and define the functionalities of tracking and tracing technologies that are frequently used interchangeably in literature. Furthermore, this paper incorporates influencing factors impacting a sequencing of the connected world in Industry4.0 supply chain networks. This includes legal influences, the embedment of supply chain-related standards, and new possibilities of emerging technologies. Finally, the results are summarized in a model for the holistic mapping of supply chains by means of tracking and tracing technologies. The resulting technological solutions that can be derived from the model enable companies to address missing elements in order to enable the holistic mapping of supply chain events as well as the transparent representation of a digital shadow throughout the entire supply chain.}
}
@article{LI2022111817,
title = {Distance measures in building informatics: An in-depth assessment through typical tasks in building energy management},
journal = {Energy and Buildings},
volume = {258},
pages = {111817},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111817},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821011014},
author = {Ao Li and Cheng Fan and Fu Xiao and Zhijie Chen},
keywords = {Distance measure, Clustering, Pattern recognition, Time-series analysis},
abstract = {Distance measurement (also known as similarity measurement) is used to evaluate pairwise similarities between data samples. It has been widely used in diverse building informatics research and applications to classify or cluster massive building data with the aim of improving prediction accuracy, identifying operation patterns, benchmarking and diagnosing building performance, etc. Various distance measures have been adopted to measure the distance/similarity of building data. However, the intrinsic complexity and diversity of building operational data bring considerable difficulties to the selection of a suitable distance measure for a specific task. There is a strong and urgent need for a comprehensive review and systematic comparison of existing distance measures in building informatics. This study provides a comprehensive review of various distance measures and their applications in building operational data analysis. A systematic comparison is undertaken based on two typical tasks relying on building informatics, i.e., building energy usage pattern recognition, and clustering-based weather data segmentation for the customized development of building energy prediction models. Nine widely adopted distance measures have been reviewed and compared, including Euclidean distance, Chebyshev distance, Manhattan distance, Mahalanobis distance, Hausdorff distance, Pearson correlation distance, Dynamic Time Warping, Edit distance on Real Sequence, and Cosine distance. Novel internal and external clustering validation approaches based on the cross-test and prediction accuracy are proposed and adopted to compare the clustering performance. The results in case studies showed that weather data clustering using the Cosine distance and Pearson correlation distance helps to obtain better energy prediction results in terms of MAPE (13.22% and 12.91%, respectively) than the commonly-used Euclidean distance (13.99%). The results also revealed that better clustering performance does not necessarily lead to higher prediction accuracy. The research results and insights obtained are valuable to guide distance-based research in building informatics.}
}
@article{ALEXANDERWHITE2022105094,
title = {A 10-step framework for use of read-across (RAX) in next generation risk assessment (NGRA) for cosmetics safety assessment},
journal = {Regulatory Toxicology and Pharmacology},
volume = {129},
pages = {105094},
year = {2022},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2021.105094},
url = {https://www.sciencedirect.com/science/article/pii/S027323002100235X},
author = {Camilla Alexander-White and Dagmar Bury and Mark Cronin and Matthew Dent and Eric Hack and Nicola J. Hewitt and Gerry Kenna and Jorge Naciff and Gladys Ouedraogo and Andreas Schepky and Catherine Mahony and Cosmetics Europe},
keywords = {Next generation read-across (RAX), New approach methodology (NAM), Next generation risk assessment (NGRA), Cosmetics safety assessment, Systemic toxicity, Physiologically-based biokinetic modelling (PBK), Caffeine, Parabens},
abstract = {This paper presents a 10-step read-across (RAX) framework for use in cases where a threshold of toxicological concern (TTC) approach to cosmetics safety assessment is not possible. RAX builds on established approaches that have existed for more than two decades using chemical properties and in silico toxicology predictions, by further substantiating hypotheses on toxicological similarity of substances, and integrating new approach methodologies (NAM) in the biological and kinetic domains. NAM include new types of data on biological observations from, for example, in vitro assays, toxicogenomics, metabolomics, receptor binding screens and uses physiologically-based kinetic (PBK) modelling to inform about systemic exposure. NAM data can help to substantiate a mode/mechanism of action (MoA), and if similar chemicals can be shown to work by a similar MoA, a next generation risk assessment (NGRA) may be performed with acceptable confidence for a data-poor target substance with no or inadequate safety data, based on RAX approaches using data-rich analogue(s), and taking account of potency or kinetic/dynamic differences.}
}
@article{COUET2022153425,
title = {Integrated high-throughput research in extreme environments targeted toward nuclear structural materials discovery},
journal = {Journal of Nuclear Materials},
volume = {559},
pages = {153425},
year = {2022},
issn = {0022-3115},
doi = {https://doi.org/10.1016/j.jnucmat.2021.153425},
url = {https://www.sciencedirect.com/science/article/pii/S0022311521006450},
author = {Adrien Couet},
abstract = {Arguably one of the most important factors in the fast deployment of advanced nuclear reactors, with major improvements in safety, is the development and qualification of radiation and corrosion tolerant materials, that serve as the structural components in reactor cores. However, the discovery, improvement, and assessment of materials resistant to radiation and corrosion in the advanced reactors’ extreme environments is quite demanding, time-consuming, and costly, which represents a significant barrier to materials innovation and qualification for nuclear energy. This short review highlights a novel, integrated, high-throughput (HTP) research framework to develop understanding and predictive models for irradiation at high doses and molten salt corrosion responses of structural Compositionally Complex Alloys (CCAs), with the objective to accelerate materials discovery for high-temperature nuclear structural applications. Using a novel in situ alloying technique, arrays of additively manufactured bulk CCAs are processed, heat-treated, and characterized, while still attached to the build plate. Leveraging recent development in automation of heavy-ion irradiation experiments at the University of Wisconsin Ion Beam Laboratory, arrays of CCAs can be rapidly irradiated to hundreds of dpa up to 800 °C. An innovative droplet corrosion method is also used to test molten salt corrosion behavior of CCAs arrays. Automated and rapid characterization methods are used to assess the irradiation and molten salt corrosion resistance of CCAs. Finally, a brief discussion of the results is presented considering future use of machine-learning-based methods to develop useful trends and highlight features of importance. Using this novel HTP approach, a robust and reliable database containing literally hundreds of data points for irradiation and corrosion responses of CCAs can be established within a year, which is considered a significant increase in the pace of nuclear structural materials research and discovery.}
}
@article{ZHU2022116187,
title = {An ensemble machine learning model for water quality estimation in coastal area based on remote sensing imagery},
journal = {Journal of Environmental Management},
volume = {323},
pages = {116187},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.116187},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722017601},
author = {Xiaotong Zhu and Hongwei Guo and Jinhui Jeanne Huang and Shang Tian and Wang Xu and Youquan Mai},
keywords = {Machine learning, Ensemble model, Water quality, Remote sensing, Coastal area},
abstract = {The accurate estimation of coastal water quality parameters (WQPs) is crucial for decision-makers to manage water resources. Although various machine learning (ML) models have been developed for coastal water quality estimation using remote sensing data, the performance of these models has significant uncertainties when applied to regional scales. To address this issue, an ensemble ML-based model was developed in this study. The ensemble ML model was applied to estimate chlorophyll-a (Chla), turbidity, and dissolved oxygen (DO) based on Sentinel-2 satellite images in Shenzhen Bay, China. The optimal input features for each WQP were selected from eight spectral bands and seven spectral indices. A local explanation strategy termed Shapley Additive Explanations (SHAP) was employed to quantify contributions of each feature to model outputs. In addition, the impacts of three climate factors on the variation of each WQP were analyzed. The results suggested that the ensemble ML models have satisfied performance for Chla (errors = 1.7%), turbidity (errors = 1.5%) and DO estimation (errors = 0.02%). Band 3 (B3) has the highest positive contribution to Chla estimation, while Band Ration Index2 (BR2) has the highest negative contribution to turbidity estimation, and Band 7 (B7) has the highest positive contribution to DO estimation. The spatial patterns of the three WQPs revealed that the water quality deterioration in Shenzhen Bay was mainly influenced by input of terrestrial pollutants from the estuary. Correlation analysis demonstrated that air temperature (Temp) and average air pressure (AAP) exhibited the closest relationship with Chla. DO showed the strongest negative correlation with Temp, while turbidity was not sensitive to Temp, average wind speed (AWS), and AAP. Overall, the ensemble ML model proposed in this study provides an accurate and practical method for long-term Chla, turbidity, and DO estimation in coastal waters.}
}
@article{HU2022128558,
title = {Linking electron ionization mass spectra of organic chemicals to toxicity endpoints through machine learning and experimentation},
journal = {Journal of Hazardous Materials},
volume = {431},
pages = {128558},
year = {2022},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2022.128558},
url = {https://www.sciencedirect.com/science/article/pii/S0304389422003466},
author = {Song Hu and Guohong Liu and Jin Zhang and Jiachen Yan and Hongyu Zhou and Xiliang Yan},
keywords = {Chemical structure identification, Machine learning, Mass spectra, Chemical toxicity prediction, Environmental health and safety},
abstract = {Quantitative structure-activity relationship (QSAR) modeling has been widely used to predict the potential harm of chemicals, in which the prediction heavily relies on the accurate annotation of chemical structures. However, it is difficult to determine the accurate structure of an unknown compound in many cases, such as in complex water environments. Here, we solved the above problem by linking electron ionization mass spectra (EI-MS) of organic chemicals to toxicity endpoints through various machine learning methods. The proposed method was verified by predicting 50% growth inhibition of Tetrahymena pyriformis (T. pyriformis) and liver toxicity. The optimal model performance obtained an R2 > 0.7 or balanced accuracy > 0.72 for both the training set and test set. External experimentation further verified the application potential of our proposed method in the toxicity prediction of unknown chemicals. Feature importance analysis allowed us to identify critical spectral features that were responsible for chemical-induced toxicity. Our approach has the potential for toxicity prediction in such fields that it is difficult to determine accurate chemical structures.}
}
@article{VENKATRAMAN2022100430,
title = {Exploring health-analytics adoption in indian private healthcare organizations: An institutional-theoretic perspective},
journal = {Information and Organization},
pages = {100430},
year = {2022},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2022.100430},
url = {https://www.sciencedirect.com/science/article/pii/S1471772722000434},
author = {Sathyanarayanan Venkatraman and Rangaraja P. Sundarraj and Ravi Seethamraju},
keywords = {Health-analytics, Case study, Adoption Journey, Institutional theory, IT adoption},
abstract = {In India, private hospitals are at the cusp of adopting health-analytics (HA) technology to manage their organizational performance through data-driven decision-making. Past studies have analyzed the applications and benefits of HA. Our study builds on this descriptive base to investigate the patterns of HA adoption and the institutional factors which impact adoption. We conducted a cross-sectional field study that involved ten Indian private healthcare organizations and four health-ecosystem partners and analyzed the case study data using an institutional theory lens. Our cases reveal that the breadth and depth of HA adoption varies and falls into three distinct patterns: far-sighted, conservative, and niche. We assess how these patterns are influenced by the two key dimensions of institutional environments (material-resource environment and institutional environment). We highlight distinctive factors (management support for building organizational trust on data-driven decisions and IT-Medical practitioner collaboration) that exert important contextual influences on HA adoption. Our study identifies areas of commonalty for HA adoption across national healthcare settings as well as contextual aspects representative of the burgeoning Indian healthcare field.}
}
@article{NAWROCKI2022102491,
title = {Modeling adaptive security-aware task allocation in mobile cloud computing},
journal = {Simulation Modelling Practice and Theory},
volume = {116},
pages = {102491},
year = {2022},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2022.102491},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X22000041},
author = {Piotr Nawrocki and Jakub Pajor and Bartlomiej Sniezynski and Joanna Kolodziej},
keywords = {Security, Mobile Cloud Computing, Machine learning, Simulation},
abstract = {Security is one of the most important criteria in the management of cloud resources. In Mobile Cloud Computing (MCC), secure allocation of tasks remains challenging due to the limited storage, battery life and computational power of mobile devices connected to the core cloud cluster infrastructure. Secure wireless communication channels and protocols for protecting the data and information sent to the cloud, and remote access to secure cloud services, are other important problems related to task scheduling and processing in dynamic MCC. In this paper, we developed a new security-aware task allocation model strategy in Mobile Cloud Computing. In this model, we define an allocation algorithm which generates an optimal and secure configuration of communication protocols in order to meet the specific data confidentiality requirements defined by end users. Resource utilization is predicted using Machine Learning methods, and the optimal secure service for task execution is selected. We developed a simulation environment (MocSecSim) for the evaluation of the algorithms proposed in several scenarios based on the users’ requirements. The results of simulations and experiments have demonstrated that the model proposed significantly improves the level of security of calculations in comparison with a configuration where processing time and energy consumption are the main criteria for optimizing task allocation.}
}
@article{ZHANG2022281,
title = {QAPP: A quality-aware and privacy-preserving medical image release scheme},
journal = {Information Fusion},
volume = {88},
pages = {281-295},
year = {2022},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522000707},
author = {Xu Zhang and Yufeng Wang and Jianhua Ma and Qun Jin},
keywords = {Differential privacy, Discrete cosine transform, Image quality improvement, Privacy protection},
abstract = {The direct release of medical image may face the dilemma: the privacy protection of medical images inevitably affects the visual quality of images. To balance medical image quality and privacy, this paper proposes a quality-aware and privacy-preserving medical image release scheme, QAPP, which effectively integrates the discrete cosine transform (DCT) with differential privacy (DP). Specifically, QAPP is composed of three phases. First, DCT is applied to each medical image to obtain its cosine coefficients matrix. Second, the original cosine coefficients matrix is compressed into k*k cosine coefficients matrix, which can retain the main features of each image. Third, the appropriate Laplace noise is injected into the formed k*k matrix to achieve differential privacy, and these noise-added coefficients are used to reconstruct the noise-added medical images through inverse DCT. Especially, considering there two error sources affecting the image quality in our work: the compression error caused by DCT, and the injected noise error caused by DP, Therefore, a selection function is proposed to determine the optimal compression dimension k, which can minimize the influence of these two errors to improve the visualization quality of the medical image. Subjective and objective image quality evaluation, and extensive experiments of image classification and segmentation using the real medical image dataset demonstrate that the proposed method QAPP can better balance medical image quality and privacy than other similar DP-based methods.}
}
@article{CHEN2022110008,
title = {Imbalance fault diagnosis under long-tailed distribution: Challenges, solutions and prospects},
journal = {Knowledge-Based Systems},
volume = {258},
pages = {110008},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110008},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122011017},
author = {Zhuohang Chen and Jinglong Chen and Yong Feng and Shen Liu and Tianci Zhang and Kaiyu Zhang and Wenrong Xiao},
keywords = {Intelligent fault diagnosis, Class imbalance, Long-tailed distribution, Information augmentation, Module design},
abstract = {Intelligent fault diagnosis based on deep learning has yielded remarkable progress for its strong feature representation capability in recent years. Nevertheless, in engineering scenarios, machines often operate under normal condition and the probability of various faults is also different, which leads to the extreme class imbalance and long-tailed distribution between different health conditions. Long-tailed distribution usually degrades the performance of the model, since the model tends to focus on dominant classes and exhibits a poor performance on tail classes. To address this problem, numerous researches have been carried out and fruitful achievements have been achieved in recent years. However, a comprehensive summary of the latest achievements is still lacking. In addition, existing studies solely concentrate on the issue of class imbalance, and lack of analysis and future research directions of long-tailed problem. Hence, this paper specifically defines the long-tailed distribution and discusses the relations between long-tailed problem and class imbalance problem. Then, research achievements on class imbalance problems are reviewed. Specifically, we divide the existing research achievements into three categories: class rebalance, information augmentation and module design. Finally, this paper summarizes the challenges of the long-tailed problem in fault diagnosis and provides some future research directions.}
}
@article{ZHOU2022104344,
title = {Smartphone-based road manhole cover detection and classification},
journal = {Automation in Construction},
volume = {140},
pages = {104344},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104344},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002175},
author = {Baoding Zhou and Wenjian Zhao and Wenhao Guo and Linchao Li and Dejin Zhang and Qingzhou Mao and Qingquan Li},
keywords = {Smartphone, Manhole cover, Automatic detection},
abstract = {Road surface condition detection is an important application for many intelligent transportation systems (ITSs). A manhole cover depression is one of the common factors affecting road conditions. Smartphones are equipped with different sensors, which can be used to collect image data and inertial data. A new large-scale manhole cover detection dataset is developed by using smartphones to collect road image data, and a hierarchical classification method based on the convolutional neural network is proposed in this paper. The proposed method first coarsely classifies the images into nonrainy and rainy types and then performs manhole cover detections based on the coarse classification results. As a result, the proposed method achieves an accuracy of approximately 86.3% for road manhole cover detection. Based on the observation that different degrees of manhole cover subsidence produce different degrees of inertial sensor data, this paper used a machine learning method, which can automatically classify the detected manhole covers into different degrees of subsidence, namely good, average, and poor. The average recalls, average precisions, and average F1-measures achieve approximately 87.3%, 86.9%, and 87.2% accuracy, respectively. The results show that the proposed approach can effectively detect manhole covers in different weather and road conditions, which can effectively reduce the cost of road manhole cover data collection and detection, providing a new method for road manhole cover detection.}
}
@article{CRONIN2022,
title = {Creating an Automated Contemporaneous Cohort in Sickle Cell Anemia to Predict Survival After Disease-Modifying Therapy},
journal = {Blood Advances},
year = {2022},
issn = {2473-9529},
doi = {https://doi.org/10.1182/bloodadvances.2022008692},
url = {https://www.sciencedirect.com/science/article/pii/S2473952922007352},
author = {Robert M. Cronin and Kristin Wuichet and Djamila Labib and Brock Hodges and Maya Chopra and Jing He and Xinnan Niu and Adetola A. Kassim and Karina Wilkerson and Mark Rodeghier and Michael R. DeBaun},
keywords = {rare diseases, data warehouse, electronic health record, mortality, sickle cell anemia, longitudinal cohorts, epidemiology},
abstract = {For participants receiving experimental gene therapy or gene editing clinical trials, the FDA requires contemporaneous controls to compare clinical outcomes. However, developing a contemporaneous cohort of rare diseases is costly and requires multiple person-hours. In a single referral center for sickle cell disease, we tested the hypothesis that we could create an automated contemporaneous cohort of children and adults with sickle cell anemia (SCA) to predict mortality. Data were obtained between 1/1/2004 and 4/30/2021. We identified 419 individuals with SCA with consistent medical care (i.e., followed continuously for >0.5 years with no visit gaps >3.0 years). The median age was 10.2 years (IQR 1.0 – 24.0 years), with a median follow-up of 7.4 years (IQR 3.6-13.5 years) and 47 deaths. A total of 98% (274 of 277) of the children remained alive at 18 years of age, and 34.3% (94 of 274) of those children were followed into adulthood. For adults, the median age of survival was 49.3 years of age. Treatment groups were mutually exclusive and in a hierarchical order: hematopoietic stem cell transplant (n=22)>regular blood transfusion for at least two years (n=56)>hydroxyurea for at least one year (n=243)>no disease-modifying therapy (n=98). Compared to those receiving no disease-modifying treatment, those treated with hydroxyurea therapy had significantly lower hazard of mortality (hazard ratio=0.38, p=0.016), but no statistical difference for those receiving regular blood transfusions compared to no disease-modifying therapy (hazard ratio=0.71, p=0.440). An automated contemporaneous SCA cohort can be generated to estimate mortality in children and adults with SCA.}
}
@article{SANCHEZ202246,
title = {The impact of visualizing operational deviations on overall quality in assembly lines},
journal = {Procedia CIRP},
volume = {107},
pages = {46-52},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002244},
author = {Ebly Sanchez and Axel Joelsson and Matthias Andersson Baumgartner and Knut Åkesson},
keywords = {Operational deviations, Quality performance, Volvo Production System},
abstract = {A framework for data collection and visualization of operational deviation at a Volvo truck manufacturing plant implementing Volvo Group’s production system, Volvo Production System (VPS), is presented. This includes visualisation of daily quality performance indicators to support decision making and improvement actions at the shop foor team level. The approach is evaluated in a qualitatively study using a survey instrument to collect responses from managers and team leaders, which acted as input for the actual use of the data and as validation of the framework. The results from this evaluation show that potentially operational deviations can be reduced, impacting positively quality performance indicators such as frst-time through (FTT). This paper also provides a brief description of the VPS in connection with data analytics and visualization.}
}