@ARTICLE{9040614,
author={Yu, Xinquan and Ma, Yuanyuan and Jin, Ruixia and Xu, Lige and Duan, Xintao},
journal={IEEE Access},
title={A Multi-Scale Feature Selection Method for Steganalytic Feature GFR},
year={2020},
volume={8},
number={},
pages={55063-55075},
abstract={The Rich Model of the Gabor filter (referred to as the GFR steganalytic feature) can detect JPEG-adaptive steganography objects. However, feature dimensionality that is too high will lead to too much computation and will correspondingly reduce the detection efficiency. To reduce the dimensionality and the operating time of GFR steganalytic features and to improve the stego image detection accuracy, this paper proposes a multi-scale feature selection method for steganalytic feature GFR. First, we use the SNR criterion to measure the uselessness of each feature component and to provide a basis for the removal of useless steganalytic feature components. Second, we improve the Relief algorithm to measure the importance of feature components in detecting stego images, which provides a basis for the selection of important feature components. Then, we set the threshold value for deleting the useless feature components, and we select the important feature components as the final feature. Finally, we conduct experiments on feature selection for GFR with high-dimensional steganalytic features, and we compared the proposed method with the Fisher-based method, the PCA-based method, the SSFC method, and the Steganalysis-α method. The results show that the method proposed in this paper is effective and fast.},
keywords={Feature extraction;Signal to noise ratio;Media;Reliability;Training;Dispersion;Fast feature selection;GFR;steganalytic features;multi-scale},
doi={10.1109/ACCESS.2020.2981738},
ISSN={2169-3536},
month={},}
@ARTICLE{9086449,
author={Tang, Jianguo and Wang, Jianghua and Wu, Chunling and Ou, Guojian},
journal={IEEE Access},
title={On Uncertainty Measure Issues in Rough Set Theory},
year={2020},
volume={8},
number={},
pages={91089-91102},
abstract={Rough set theory is a tool for dealing with uncertainty problems. How to measure the uncertainty of a knowledge is an important issue in the theory. However, the existing uncertainty measures may not accurately reflect the uncertainty degree. This study analyzes the causes of it and explores a reasonable solution to it. Firstly, the existing accuracy models only focuses on some factors related to the target set while neglecting its own important influence on the model. Secondly, since no one gives a clear definition of knowledge uncertainty in approximation space, it is difficult to evaluate the accuracy and rationality of a knowledge uncertainty measure. Thirdly, most uncertain measures of knowledge are constructed based on the structure of knowledge itself, while neglecting other factors in the approximation model. In view of these, we first propose a new accuracy model which fully considers the important role of the target set itself. Second, two definitions of accuracy measure of knowledge are proposed to explain what the uncertainty of a knowledge is. And then, two uncertainty measures of knowledge are proposed and a method for quickly calculating them is designed. At last, an uncertain entropy is constructed for more conveniently calculating of knowledge uncertainty.},
keywords={Uncertainty;Measurement uncertainty;Rough sets;Entropy;Extraterrestrial measurements;Information entropy;Mathematical model;Accuracy measure;approximation accuracy;approximation quality;rough sets;uncertainty measure},
doi={10.1109/ACCESS.2020.2992582},
ISSN={2169-3536},
month={},}
@ARTICLE{8936442,
author={Jin, Xin and Zhou, Xinghui and Li, Xiaodong and Zhang, Xiaokun and Sun, Hongbo and Li, Xiqiao and Liu, Ruijun},
journal={IEEE Access},
title={Incremental Learning of Multi-Tasking Networks for Aesthetic Radar Map Prediction},
year={2019},
volume={7},
number={},
pages={183647-183655},
abstract={It is difficult and challenging to evaluate the aesthetics quality of images from multiple angles. Since humans' perception of images comes from many factors, the integrated image aesthetic quality assessment cannot be easily summarized by few attributes. A comprehensive evaluation is supposed to predict many aesthetic attributes across not only one dataset. This requires the model to have not only high accuracy, but also strong generalization ability, resulting in a better prediction on multiple models and datasets. Recent work shows that deep convolution neural network can be used to extract image features and further evaluate the total score of images, and the method of evaluation are lacking of sufficient detailed features. In this paper, we propose a multi-task convolution neural network with more incremental features. We show the results in the way of a hexagon map, which is called aesthetic radar map. This allows the network model to fit different attributes in various datasets better.},
keywords={Task analysis;Radar imaging;Quality assessment;Image color analysis;Feature extraction;Computer vision;Neural network;multitasking;computer vision;incremental learning},
doi={10.1109/ACCESS.2019.2958119},
ISSN={2169-3536},
month={},}
@ARTICLE{7508921,
author={Vukobratovic, Dejan and Jakovetic, Dusan and Skachek, Vitaly and Bajovic, Dragana and Sejdinovic, Dino and Karabulut Kurt, Güneş and Hollanti, Camilla and Fischer, Ingo},
journal={IEEE Access},
title={CONDENSE: A Reconfigurable Knowledge Acquisition Architecture for Future 5G IoT},
year={2016},
volume={4},
number={},
pages={3360-3378},
abstract={In forthcoming years, the Internet of Things (IoT) will connect billions of smart devices generating and uploading a deluge of data to the cloud. If successfully extracted, the knowledge buried in the data can significantly improve the quality of life and foster economic growth. However, a critical bottleneck for realizing the efficient IoT is the pressure it puts on the existing communication infrastructures, requiring transfer of enormous data volumes. Aiming at addressing this problem, we propose a novel architecture dubbed Condense which integrates the IoT-communication infrastructure into the data analysis. This is achieved via the generic concept of network function computation. Instead of merely transferring data from the IoT sources to the cloud, the communication infrastructure should actively participate in the data analysis by carefully designed en-route processing. We define the Condense architecture, its basic layers, and the interactions among its constituent modules. Furthermore, from the implementation side, we describe how Condense can be integrated into the Third Generation Partnership Project (3GPP) machine type communications (MTCs) architecture, as well as the prospects of making it a practically viable technology in a short time frame, relying on network function virtualization and software-defined networking. Finally, from the theoretical side, we survey the relevant literature on computing atomic functions in both analog and digital domains, as well as on function decomposition over networks, highlighting challenges, insights, and future directions for exploiting these techniques within practical 3GPP MTC architecture.},
keywords={Internet of things;Computer architecture;3GPP;Cloud computing;Data analysis;Smart devices;5G mobile commuinication;Big data;Machine learning;Wireless communication;Internet of things (IoT);big data;network coding;network function computation;machine learning;wireless communications},
doi={10.1109/ACCESS.2016.2585468},
ISSN={2169-3536},
month={},}
@ARTICLE{9893181,
author={Liu, Yang and Cao, Kejing and Li, Rui and Zhang, Hongxia and Zhou, Liming},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={Hyperspectral Image Classification of Brain-Inspired Spiking Neural Network Based on Approximate Derivative Algorithm},
year={2022},
volume={60},
number={},
pages={1-16},
abstract={Recently, deep learning methods have made significant progress in solving hyperspectral image (HSI) classification problems of high-dimensional features, band redundancy, and spectral mixture. However, the deep neural network is too complex, with a long training time and high energy consumption, making it difficult to deploy on edge computing devices. In order to solve the above problems, this article proposes a brain-inspired computing framework based on the spiking leaky integrate-and-fire neuron model for HSIs’ classification. Then, we design an approximate derivative algorithm to solve the nondifferentiable spike activity of the spiking neuron. The framework uses direct coding to generate spatiotemporal spikes for input HSI and achieves efficient extraction of spatial–spectral features through spiking standard convolution and spiking depthwise separable convolution. Extensive experiments are performed on four benchmark hyperspectral datasets and two public unmanned aerial vehicle-borne hyperspectral datasets. Experiments show that the proposed model has the advantages of high classification accuracy and fewer spiking time steps. The proposed model can save about ten times computational energy consumption compared with the CNN of the same architecture. This research has great significance for overcoming the technical bottleneck of HSI classification based on brain-inspired computing, solving the critical problems of mobile computing in unmanned autonomous systems, and realizing the engineering application of unmanned aerial vehicles and software-defined satellites. The source code will be made available at https://github.com/Katherine-Cao/HSI_SNN.},
keywords={Neurons;Feature extraction;Computational modeling;Training;Convolutional neural networks;Hyperspectral imaging;Brain modeling;Approximate derivative algorithm;brain-inspired computing;hyperspectral image~(HSI) classification;neuromorphic computing;spiking neural network (SNN)},
doi={10.1109/TGRS.2022.3207098},
ISSN={1558-0644},
month={},}
@ARTICLE{9265432,
author={Florencias-Oliveros, Olivia and González-de-la-Rosa, Juan-José and Sierra-Fernández, Jose-María and Agüera-Pérez, Agustín and Espinosa-Gavira, Manuel-Jesús and Palomares-Salas, José-Carlos},
journal={Journal of Modern Power Systems and Clean Energy},
title={Site Characterization Index for Continuous Power Quality Monitoring Based on Higher-order Statistics},
year={2022},
volume={10},
number={1},
pages={222-231},
abstract={The high penetration of distributed generation (DG) has set up a challenge for energy management and consequently for the monitoring and assessment of power quality (PQ). Besides, there are new types of disturbances owing to the uncontrolled connections of non-linear loads. The stochastic behaviour triggers the need for new holistic indicators which also deal with big data of PQ in terms of compression and scalability so as to extract the useful information regarding different network states and the prevailing PQ disturbances for future risk assessment and energy management systems. Permanent and continuous monitoring would guarantee the report to claim for damages and to assess the risk of PQ distortions. In this context, we propose a measurement method that postulates the use of two-dimensional (2D) diagrams based on higher-order statistics (HOSs) and a previous voltage quality index that assesses the voltage supply waveform in a continous monitoring campaign. Being suitable for both PQ and reliability applications, the results conclude that the inclusion of HOS measurements in the industrial metrological reports helps characterize the deviations of the voltage supply waveform, extracting the individual customers' pattern fingerprint, and compressing the data from both time and spatial aspects. The method allows a continuous and robust performance needed in the SG framework. Consequently, the method can be used by an average consumer as a probabilistic method to assess the risk of PQ deviations in site characterization.},
keywords={Monitoring;Indexes;Current measurement;Power measurement;Energy measurement;Standards;Voltage measurement;Continuous statistical monitoring;big data;data compression;higher-order statistics (HOSs);power quality (PQ)},
doi={10.35833/MPCE.2020.000041},
ISSN={2196-5420},
month={January},}
@ARTICLE{9286384,
author={Dou, Yuqiang and Li, Ming},
journal={IEEE Access},
title={An Image Encryption Algorithm Based on Compressive Sensing and M Sequence},
year={2020},
volume={8},
number={},
pages={220646-220657},
abstract={In this article, a new image encryption algorithm based on compressive sensing (CS) and M sequence is proposed to decrease the image communication load and improve the security of image communication in the internet of things. Most of the available image encryption schemes are based on chaotic systems to shuffle the image pixels. Before shuffling the image pixels, the random sequence, which is produced by a chaotic system, need to be sorted. This sorting operation is avoided by utilizing a modified linear feedback shift register (LFSR) state sequence. Then, the security of the proposed scheme is improved by combining CS with an improved 1D chaotic system, which is used to construct a measurement matrix. The computational complexity is reduced by the use of the improved 1D chaotic system. Simultaneously, the amount of image data is reduced. Simulation results and performance analyses demonstrate that the proposed encryption scheme can greatly reduce the amount of image data and has good security and robustness.},
keywords={Encryption;Chaotic communication;Matching pursuit algorithms;Compressed sensing;Image coding;Sparse matrices;Logistics;M sequence;compressive sensing;image encryption;improved 1D chaotic map},
doi={10.1109/ACCESS.2020.3043240},
ISSN={2169-3536},
month={},}
@ARTICLE{9229407,
author={Cai, Youcheng and Cao, Mingwei and Li, Lin and Liu, Xiaoping},
journal={IEEE Access},
title={An End-to-End Approach to Reconstructing 3D Model From Image Set},
year={2020},
volume={8},
number={},
pages={193268-193284},
abstract={Large-scale 3D reconstruction from imagery has received much attention from the computer vision community. However, recovering 3D structures from 2D images is a notoriously complex process that requires expertise with often limited results. This paper presents an end-to-end 3D reconstruction system that can produce high-quality 3D models from a set of unordered image collections. Our workflow is a typical 3D reconstruction architecture that consists of structure from motion (SFM), multi-view stereo (MVS), and surface reconstruction, and can automatically recover desirable 3D models without any interactive operations. Finally, a comprehensive experiment is conducted on several benchmark datasets to assess the presented system. Experimental results show that the presented system achieves significant improvements in reconstruction accuracy and completeness over the existing state-of-the-art approach.},
keywords={Three-dimensional displays;Image reconstruction;Surface reconstruction;Solid modeling;Computational modeling;Cameras;Structure from motion;3D reconstruction;feature tracking;structure from motion;multi-view stereo;surface reconstruction},
doi={10.1109/ACCESS.2020.3032169},
ISSN={2169-3536},
month={},}
@ARTICLE{8993765,
author={Zuo, Jia and Zhang, Xiaojuan and Lu, Jianju and Gui, Zhiguo and Shang, Yu},
journal={IEEE Access},
title={Impact of Reconstruction Algorithms on Diffuse Correlation Tomography Blood Flow Imaging},
year={2020},
volume={8},
number={},
pages={31882-31891},
abstract={Near-infrared diffuse correlation tomography (DCT) is an emerging technology for non-invasive imaging of the tissue blood flow. The flow imaging quality relies on the image reconstruction algorithm, which, however, is little studied thus far. In this study, we conducted the first investigation of reconstruction algorithm impact on DCT blood flow imaging. Two reconstruction algorithms, i.e., the finite element method (FEM) representing the imaging framework of partial differential equation, and the Nth-order linear (NL) approach, representing the imaging framework of integral equation that was recently proposed by us to incorporate the tissue morphological information, were compared. Both computer simulations and phantom experiment outcomes show that the NL approach performs much better in image accuracy and homogeneity over anomaly or background, when compared with the FEM at the same source-detector configuration and spatial resolution. This study demonstrates that the DCT blood flow imaging is substantially influenced by the reconstruction algorithm, thus it has great potential in future algorithm design and optimization.},
keywords={Imaging;Discrete cosine transforms;Blood;Correlation;Finite element analysis;US Department of Transportation;Photonics;Diffuse correlation tomography;blood flow;image reconstruction;finite element method;Nth-order linear approach},
doi={10.1109/ACCESS.2020.2973209},
ISSN={2169-3536},
month={},}
@ARTICLE{9826585,
author={Shi, Yongpeng and Zhang, Junjie and Gao, Ya and Xia, Yujie},
journal={Journal of Communications and Networks},
title={Inter-server computation offloading and resource allocation in multi-drone aided space-air-ground integrated IoT networks},
year={2022},
volume={24},
number={3},
pages={324-335},
abstract={Combining mobile edge computing (MEC), the multi-drone aided space-air-ground integrated Internet of things (SAG-IoT) networks can provide ground IoT devices (GIDs) high-quality wireless access and computing services. However, the diverse tasks, moving drones, and limited network resources reveal great challenges for the task offloading and resource allocation scheme exploitation. Especially, given the restricted computation resources, how to make full use of available applications deployed on MEC servers (MECSs) to compute various types of tasks, is even an important issue. To the best of our knowledge, it is an entirely new problem since most existing works in this line assume that all types of applications can be deployed on one MECS so as to process various offloaded tasks. Toward this end, we present this paper to investigate inter-server computation offloading, resource allocation, and drone deployment to minimize the overall computation overhead of all GIDs. An iteratively optimization algorithm is proposed which alternately utilizes heuristic greedy and successive convex approximation methods. Simulation results verify that, for different GID numbers, optimization schemes, and computing models, our devised schemes can not only significantly reduce the overall computation overhead but also achieve optimal decisions of computation offloading, spectrum allocation, and drone deployment.},
keywords={Drones;Task analysis;Internet of Things;Resource management;Optimization;Servers;Computational efficiency;Bandwidth allocation;inter-server computation offloading;multi-drone;space-air-ground integrated IoT network},
doi={10.23919/JCN.2022.000016},
ISSN={1976-5541},
month={June},}
@ARTICLE{9792257,
author={Wang, Yidong and Song, Rui and He, Shiwei and Song, Zilong and Chi, Jushang},
journal={IEEE Access},
title={Optimizing Train Routing Problem in a Multistation High-Speed Railway Hub by a Lagrangian Relaxation Approach},
year={2022},
volume={10},
number={},
pages={61992-62010},
abstract={As the intersection of multiple high-speed railway lines, the multi-station high-speed railway hub is the key to improve the transport efficiency of the high-speed railway network. This paper focuses on the optimization of the multi-station high-speed railway hub and models it as a train routing problem (TRP). Considering the capacity of railway infrastructures and the demand of passengers, a mixed integer linear programming model is proposed to minimize the total cost of train routes and passenger routes. The optimized train routes include the macroscopic routes between stations and the microscopic track allocation inside stations and Electric Multiple Units (EMUs) depots. A Lagrangian relaxation (LR) approach is developed to dualize the hard constraints and decompose the origin model into train and passenger subproblems, then a shortest path algorithm is designed to solve the subproblems independently. Numerical experiments based on an illustrative railway hub network and a real-world network are implemented to demonstrate the effectiveness of the model and algorithm. The solution results prove that the LR approach can obtain high-quality solutions within an acceptable computational time. Compared with the existing fixed scheme, the optimization scheme can reduce the total cost by 37.18% and utilize the railway lines and tracks more reasonably.},
keywords={Rail transportation;Microscopy;Optimization;Resource management;Maintenance engineering;Heuristic algorithms;Numerical models;Multi-station high-speed railway hub;train routing problem;EMUs depot;Lagrangian relaxation;passenger demand},
doi={10.1109/ACCESS.2022.3181815},
ISSN={2169-3536},
month={},}
@ARTICLE{9319232,
author={Miloudi, Salim and Wang, Yulin and Ding, Wenjia},
journal={IEEE Access},
title={A Gradient-Based Clustering for Multi-Database Mining},
year={2021},
volume={9},
number={},
pages={11144-11172},
abstract={Multinational corporations have multiple databases distributed throughout their branches, which store millions of transactions per day. For business applications, identifying disjoint clusters of similar and relevant databases contributes to learning the common buying patterns among customers and also increases the profits by targeting potential clients in the future. This process is called clustering, which is an important unsupervised technique for big data mining. In this article, we present an effective approach to search for the optimal clustering of multiple transaction databases in a weighted undirected similarity graph. To assess the clustering quality, we use dual gradient descent to minimize a constrained quasi-convex loss function whose parameters will determine the edges needed to form the optimal database clusters in the graph. Therefore, finding the global minimum is guaranteed in a finite and short time compared with the existing non-convex objectives where all possible candidate clusterings are generated to find the ideal clustering. Moreover, our algorithm does not require specifying the number of clusters a priori and uses a disjoint-set forest data structure to maintain and keep track of the clusters as they are updated. Through a series of experiments on public data samples and precomputed similarity matrices, we show that our algorithm is more accurate and faster in practice than the existing clustering algorithms for multi-database mining.},
keywords={Databases;Itemsets;Clustering algorithms;Data models;Prototypes;Computer science;Computational modeling;Multi-database mining;graph clustering;dual gradient descent;quasi-convex optimization;similarity measure},
doi={10.1109/ACCESS.2021.3050404},
ISSN={2169-3536},
month={},}
@ARTICLE{9431193,
author={Yustiawan, Yoga and Ramadhan, Hani and Kwon, Joonho},
journal={IEEE Access},
title={A Stacked Denoising Autoencoder and Long Short-Term Memory Approach With Rule-Based Refinement to Extract Valid Semantic Trajectories},
year={2021},
volume={9},
number={},
pages={73152-73168},
abstract={Indoor location-based services have been widely investigated to take advantage of semantic trajectories for providing user oriented services in indoor environments. Although indoor semantic trajectories can provide seamless understanding to users regarding the provided location-based services, studies on the application of deep learning approaches for robust and valid semantic indoor localization are lacking. In this study, we combined a stacked denoising autoencoder and long short term memory technique with a rule-based refinement method applying a rule-based hidden Markov model (HMM) to perform robust and valid semantic trajectory extraction. In particular, our rule-based HMM approach incorporates a direct set of rules into HMM to resolve invalid movements of the extracted semantic trajectories and is extensible to various deep learning techniques. We compared the performance of our proposed approach with that of other cutting-edge deep learning approaches on two different real-world data sets. The experimental results demonstrate the feasibility of our proposed approach to produce more robust and valid semantic trajectories.},
keywords={Semantics;Trajectory;Location awareness;Hidden Markov models;Deep learning;Noise measurement;Indoor environment;Deep learning;indoor localization;the Internet of Things;rule-based refinement;semantic trajectories},
doi={10.1109/ACCESS.2021.3080288},
ISSN={2169-3536},
month={},}
@ARTICLE{9235583,
author={Liu, Dong and Zhang, Yunping and Zhang, Jun and Li, Qinpeng and Zhang, Congpin and Yin, Yu},
journal={IEEE Access},
title={Multiple Features Fusion Attention Mechanism Enhanced Deep Knowledge Tracing for Student Performance Prediction},
year={2020},
volume={8},
number={},
pages={194894-194903},
abstract={Student performance prediction is a fundamental task in online learning systems, which aims to provide students with access to active learning. Generally, student performance prediction is achieved by tracing the evolution of each student's knowledge states via a series of learning activities. Every learning activity record has two types of feature data: student behavior and exercise features. However, most methods use features that are related to exercises, such as correctness and concepts, while other student behavior features are usually ignored. The few studies that have focused on student behavior features through subjective manual selection argue that different student behavior features can be used in an equivalent manner to predict student performance. In this paper, we assume that the integration of student behavior features and exercise features is crucial to improve the precision of prediction, and each feature has a different impact on student performance. Therefore, this paper proposes a novel framework for student performance prediction by making full use of both student behavior features and exercise features and combining the attention mechanism with the knowledge tracing model. Specifically, we first exploit machine learning to capture feature representation automatically. Then, a fusion attention mechanism based on recurrent neural network architecture is used for student performance prediction. Extensive experiments on a real-world dataset show the effectiveness and practicability of our approach. The accuracy of our method is up to 98%, which is superior to previous methods.},
keywords={Recurrent neural networks;Knowledge engineering;Deep learning;Predictive models;Education;Task analysis;Student performance prediction;knowledge tracing;recurrent neural network;attention mechanism},
doi={10.1109/ACCESS.2020.3033200},
ISSN={2169-3536},
month={},}
@ARTICLE{9454474,
author={Dhawan, Sachin and Chakraborty, Chinmay and Frnda, Jaroslav and Gupta, Rashmi and Rana, Arun Kumar and Pani, Subhendu Kumar},
journal={IEEE Access},
title={SSII: Secured and High-Quality Steganography Using Intelligent Hybrid Optimization Algorithms for IoT},
year={2021},
volume={9},
number={},
pages={87563-87578},
abstract={Internet of Things (IoT) is a domain where the transfer of big data is taking place every single second. The security of these data is a challenging task; however, security challenges can be mitigated with cryptography and steganography techniques. These techniques are crucial when dealing with user authentication and data privacy. In the proposed work, a highly secured technique is proposed using IoT protocol and steganography. This work proposes an image steganography procedure by utilizing the combination of various algorithms that build the security of the secret data by utilizing Binary bit-plane decomposition (BBPD) based image encryption technique. Thereafter a Salp Swarm Optimization Algorithm (SSOA) based adaptive embedding process is proposed to increase the payload capacity by setting different parameters in the steganographic embedding function for edge and smooth blocks. Here the SSOA algorithm is used to localize the edge and smooth blocks efficiently. Then, the hybrid Fuzzy Neural Network with a backpropagation learning algorithm is used to enhance the quality of the stego images. Then these stego images are transferred to the destination in the highly secured protocol of IoT. The proposed steganography technique shows better results in terms of security, image quality, and payload capacity in comparison with the existing state of art methods.},
keywords={Security;Particle swarm optimization;Encryption;Image edge detection;Payloads;Optimization;Medical diagnostic imaging;Steganography;encryption;embedding process;salp swarm optimization;hybrid fuzzy neural network},
doi={10.1109/ACCESS.2021.3089357},
ISSN={2169-3536},
month={},}
@ARTICLE{8844663,
author={Hussain, Bilal and Du, Qinghe and Zhang, Sihai and Imran, Ali and Imran, Muhammad Ali},
journal={IEEE Access},
title={Mobile Edge Computing-Based Data-Driven Deep Learning Framework for Anomaly Detection},
year={2019},
volume={7},
number={},
pages={137656-137667},
abstract={5G is anticipated to embed an artificial intelligence (AI)-empowerment to adroitly plan, optimize and manage the highly complex network by leveraging data generated at different positions of the network architecture. Outages and situation leading to congestion in a cell pose severe hazard for the network. High false alarms and inadequate accuracy are the major limitations of modern approaches for the anomaly—outage and sudden hype in traffic activity that may result in congestion—detection in mobile cellular networks. This indicates wasting limited resources that ultimately leads to an elevated operational expenditure (OPEX) and also interrupting quality of service (QoS) and quality of experience (QoE). Motivated by the outstanding success of deep learning (DL) technology, our study applies it for detection of the above-mentioned anomalies and also supports mobile edge computing (MEC) paradigm in which core network (CN)’s computations are divided across the cellular infrastructure among different MEC servers (co-located with base stations), to relief the CN. Each server monitors user activities of multiple cells and utilizes $L$ -layer feedforward deep neural network (DNN) fueled by real call detail record (CDR) dataset for anomaly detection. Our framework achieved 98.8% accuracy with 0.44% false positive rate (FPR)—notable improvements that surmount the deficiencies of the old studies. The numerical results explicate the usefulness and dominance of our proposed detector.},
keywords={Anomaly detection;Microprocessors;Computer architecture;Quality of service;Quality of experience;Servers;Cellular networks;Cellular network;anomaly detection;call detail record;deep learning;big data analytics;sleeping cell;congestion detection},
doi={10.1109/ACCESS.2019.2942485},
ISSN={2169-3536},
month={},}
@ARTICLE{9703317,
author={Woo, Jiyoung and Lee, Ji-Hyun and Kim, Yeonjin and Rudasingwa, Guillaume and Lim, Dae Hyun and Kim, Sungroul},
journal={IEEE Access},
title={Forecasting the Effects of Real-Time Indoor PM2.5 on Peak Expiratory Flow Rates (PEFR) of Asthmatic Children in Korea: A Deep Learning Approach},
year={2022},
volume={10},
number={},
pages={19391-19400},
abstract={We built a deep learning algorithm to predict the deterioration of health symptoms among asthmatic children between 8–12 years of age. It is based on Peak Expiratory Flow Rates (PEFR) and indoor air pollution data, as well as meteorological data collected at their indoor residences every 2 minutes using portable monitoring devices with a low-cost sensor between November 2018 and March 2019. The PEFR results collected twice a day were matched with daily PM2.5. A personalized model has been developed to predict the peak expiratory flow rate of the next day, considering indoor air quality data including PM2.5, humidity, temperature, and CO2 level in previous days. Two models were developed incorporating Indoor Air Quality (IAQ) with the PEFR-only model. The IAQ uses the daily IAQ, and 10-minute basis IAQ in predicting the future PEFR. Recurrent Neural Networks (RNN) and Deep Neural Networks (DNN) models were trained using 4 months of linked data to predict PEFR for the next days during the study period. The 10-minute RNN model was found to predict better PEFR with a Root Mean Square Error (RMSE) of 42.5 and a Mean Absolute Percentage Error (MAPE) of 14.0, as it consolidates the cumulative effects of PM2.5 concentrations over time. The highly accurate estimation showed that indoor air quality significantly affects PEFR.},
keywords={Respiratory system;Atmospheric modeling;Predictive models;Pediatrics;Deep learning;Data models;Atmospheric measurements;Asthma;big data;machine learning;recurrent neural network;peak expiratory flow rates (PEFR)},
doi={10.1109/ACCESS.2022.3148294},
ISSN={2169-3536},
month={},}
@ARTICLE{9530389,
author={Khan, Farhat Ullah and Aziz, Izzatdin B. and Akhir, Emilia Akashah P.},
journal={IEEE Access},
title={Pluggable Micronetwork for Layer Configuration Relay in a Dynamic Deep Neural Surface},
year={2021},
volume={9},
number={},
pages={124831-124846},
abstract={The classical convolution neural network architecture adheres to static declaration procedures, which means that the shape of computation is usually predefined and the computation graph is fixed. In this research, the concept of a pluggable micronetwork, which relaxes the static declaration constraint by dynamic layer configuration relay, is proposed. The micronetwork consists of several parallel convolutional layer configurations and relays only the layer settings, incurring a minimum loss. The configuration selection logic is based on the conditional computation method, which is implemented as an output layer of the proposed micronetwork. The proposed micronetwork is implemented as an independent pluggable unit and can be used anywhere on the deep learning decision surface with no or minimal configuration changes. The MNIST, FMNIST, CIFAR-10 and STL-10 datasets have been used to validate the proposed research. The proposed technique is proven to be efficient and achieves appropriate validity of the research by obtaining state-of-the-art performance in fewer iterations with wider and compact convolution models. We also naively attempt to discuss the involved computational complexities in these advanced deep neural structures.},
keywords={Convolution;Training;Logic gates;Feature extraction;Computational modeling;Adaptive systems;Relays;Convolution neural network;deep learning;dynamic neural structure;micronetwork;multilayer perceptron},
doi={10.1109/ACCESS.2021.3110709},
ISSN={2169-3536},
month={},}
@ARTICLE{9580651,
author={Tam, Prohim and Math, Sa and Nam, Chaebeen and Kim, Seokhoon},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Adaptive Resource Optimized Edge Federated Learning in Real-Time Image Sensing Classifications},
year={2021},
volume={14},
number={},
pages={10929-10940},
abstract={With the exponential growth of the Internet of things (IoT) in remote sensing image applications, network resource orchestration and data privacy are significant aspects to handle in bigdata cellular networks. The image data sharing procedure toward central cloud servers in order to perform real-time classifications has leaked client personalization and heavily burdened the communication networks. Thus, the deployment of IoT image sensors in privacy-constrained sectors requires an optimized federated learning (FL) scheme to efficiently consider both aspects of securing data privacy and maximizing the model accuracy with sufficient communication and computation resources. In this article, an adaptive model communication scheme with virtual resource optimization for edge FL is proposed by converging a deep q-learning algorithm to enforce a self-learning agent interacting with network functions virtualization orchestrator and software-defined networking based architecture. The agent targets to optimize the resource control policy of virtual multi-access edge computing entities in virtualized infrastructure manager. The proposed scheme trains the learning model and weighs the optimal actions for particular network states by using an epsilon-greedy strategy. In the exploitation phase, the scheme considers multiple spatial-resolution sensing conditions and allocates computation offloading resources for global multiconvolutional neural networks model aggregation based on the congestion states. In the simulation results, the quality of service and global collaborative model performance metrics were evaluated in terms of delay, packet drop ratios, packet delivery ratios, loss values, and overall accuracy.},
keywords={Computational modeling;Sensors;Data models;Servers;Real-time systems;Adaptation models;Resource management;Convolutional neural networks (CNN);deep q-learning (DQL);federated learning (FL);quality of service (QoS);real-time image classifications},
doi={10.1109/JSTARS.2021.3120724},
ISSN={2151-1535},
month={},}
@ARTICLE{9767588,
author={Won, Dong-Ok and Jang, Yong-Nam and Lee, Seong-Whan},
journal={IEEE Transactions on Emerging Topics in Computing},
title={PlausMal-GAN: Plausible Malware Training Based on Generative Adversarial Networks for Analogous Zero-day Malware Detection},
year={2022},
volume={},
number={},
pages={1-1},
abstract={Zero-day malicious software (malware) refers to a previously unknown or newly discovered software vulnerability. The fundamental objective of this paper is to enhance detection for analogous zero-day malware by efficient learning to plausible generated data. To detect zero-day malware, we proposed a malware training framework based on the generated analogous malware data using generative adversarial networks (PlausMal-GAN). Thus, the PlausMal-GAN can suitably produce analogous zero-day malware images with high quality and high diversity from the existing malware data. The discriminator, as a detector, learns various malware features using both real and generated malware images. In terms of performance, the proposed framework showed higher and more stable performances for the analogous zero-day malware images, which can be assumed to be analogous zero-day malware data. We obtained reliable accuracy performances in the proposed PlausMal-GAN framework with representative GAN models (i.e., deep convolutional GAN, least-squares GAN, Wasserstein GAN with gradient penalty, and evolutionary GAN). These results indicate that the use of the proposed framework is beneficial for the detection and prediction of numerous and analogous zero-day malware data from noted malware when developing and updating malware detection systems.},
keywords={Malware;Generative adversarial networks;Generators;Training;Training data;Big Data;Linear programming;Zero-day Malware;Analogous Malware Detection;Malware Augmentation;Malware Data;Generative Adversarial Networks},
doi={10.1109/TETC.2022.3170544},
ISSN={2168-6750},
month={},}
@ARTICLE{9905561,
author={Li, Shiqi and Lang, Maoxiang and Li, Siyu and Chen, Xinghan and Yu, Xueqiao and Geng, Yixuan},
journal={IEEE Access},
title={Optimization of High-Speed Railway Line Planning With Passenger and Freight Transport Coordination},
year={2022},
volume={10},
number={},
pages={110217-110247},
abstract={This paper studies the line planning optimization problem based on the coordinate mode of high-speed railway (HSR) passenger trains and freight trains. The multi objective nonlinear mixed integer programming model of HSR passenger train and freight train line planning with passengers and freight is designed on the basis of comprehensive consideration of passenger and freight transport demand. Then, in order to simultaneously determine the types, origin and destination stations, operation sections, stop schemes, operation frequencies, and demand allocation of HSR passenger trains and freight trains, the model is solved iteratively using a hybrid heuristic algorithm combining a column generation algorithm and a genetic algorithm. Finally, a numerical experiment based on the operation data of China’s Dalian-Harbin HSR line is implemented to verify the effectiveness of the proposed model and algorithm, and the solution performance of the CPLEX solver and the hybrid heuristic algorithm is compared. The results show that both the CPLEX solver and the hybrid heuristic algorithm can obtain the global optimal solution set. With the expansion of the scale of the problem, the solution quality and convergence efficiency of the hybrid heuristic algorithm have significantly improved, and it can solve large-scale problems and obtain satisfactory solutions within a shorter time.},
keywords={High-speed rail transportation;Freight handling;Rail transportation;Optimization;Resource management;Genetic algorithms;Heuristic algorithms;Generation algorithm;genetic algorithm;high-speed railway;passenger and freight transport coordination;the line planning;train candidate set},
doi={10.1109/ACCESS.2022.3210578},
ISSN={2169-3536},
month={},}
@ARTICLE{8528409,
author={Xiong, Jinbo and Chen, Xiuhua and Tian, Youliang and Ma, Rong and Chen, Lei and Yao, Zhiqiang},
journal={IEEE Access},
title={MAIM: A Novel Incentive Mechanism Based on Multi-Attribute User Selection in Mobile Crowdsensing},
year={2018},
volume={6},
number={},
pages={65384-65396},
abstract={In the user selection phase of mobile crowdsensing, most existing incentive mechanisms focus on either single-attribute selection or random selection, which possibly lead to serious consequences such as low user enthusiasm, decreased task completion rate, and increased cost of platform consumption. To tackle these issues, in this paper, we propose a novel incentive mechanism MAIM, which is based on multi-attribute user selection and participation intention analysis function in mobile crowdsensing. In this mechanism, the sensing platform employs the analytic hierarchy process to determine the weights of three attributes: participation threshold, cost, and reputation. The weight calculation results of each sensing user with respect to each attribute are then integrated to obtain the sorted weight of each user, with which the sensing platform will then obtain the optimal user set. From the users' perspective, they can autonomously decide whether to accept task processing requests, as enabled by the participation intention analysis function, thereby voiding the absolute authority and control of the sensing platform over users and achieving a two-way selection between the sensing platform and the sensing users. Furthermore, the sensing platform establishes a score-based reputation reward to inspire active performers and utilizes a punishment mechanism to overawe malicious vandals, which substantially helps activize enthusiasm of user participation and improve sensing data quality. Simulation results indicate that the proposed MAIM has significantly improved the sensing task completion ratio and the budget surplus ratio compared with the existing incentive mechanisms in mobile crowdsensing.},
keywords={Sensors;Task analysis;Analytic hierarchy process;Training;Technological innovation;Data integrity;Simulation;Mobile crowdsensing;incentive mechanism;analytic hierarchy process;multi-attribute user selection;participation intention analysis},
doi={10.1109/ACCESS.2018.2878761},
ISSN={2169-3536},
month={},}
@ARTICLE{9594816,
author={Zhou, Yueli and Lin, Guoping},
journal={IEEE Access},
title={Local Generalized Multigranulation Variable Precision Tolerance Rough Sets and its Attribute Reduction},
year={2021},
volume={9},
number={},
pages={147237-147249},
abstract={In the era of big data, as for an important granular computing model, rough set model is an important tool for us to deal with data. As a kind of extension of classical rough sets, multigranulation rough sets have two forms, including optimistic and pessimistic cases. However, these two models have their shortcomings, one is too loose, and the other is too strict. To overcome the above shortcomings, based on the concept of local multigranulation tolerance rough sets in set-valued information systems, the local generalized multigranulation variable precision tolerance rough sets model by introducing characteristic function is established. Then the related properties are studied and proved. In addition, we define the concepts of lower approximate quality, inner and outer importance of attribute according to different granularity structures in set-valued decision information systems because different granularity structures have different effectives on the decision classes. Finally, the local attribute reduction algorithm and the global attribute reduction algorithm of local generalized multigranulation variable precision tolerance rough sets in set-valued decision information systems are given, and the effectiveness of the algorithms is proved by using UCI data sets.},
keywords={Rough sets;Information systems;Data models;Approximation algorithms;Granular computing;Computational modeling;Big Data;Tolerance relation;local rough sets;attribute reduction;set-valued information systems},
doi={10.1109/ACCESS.2021.3124339},
ISSN={2169-3536},
month={},}
@ARTICLE{8782454,
author={Sun, Jianfei and Hu, Shengnan and Nie, Xuyun},
journal={IEEE Access},
title={Fine-Grained Ranked Multi-Keyword Search Over Hierarchical Data for IoT-Oriented Health System},
year={2019},
volume={7},
number={},
pages={101969-101980},
abstract={With the rapid advance of the Internet of Things (IoT) and cloud computing technologies, the IoT-oriented health is expected to greatly improve the quality of healthcare service. However, data security and privacy concerns have become one of the biggest issues in smart health applications. As a potential and promising solution, attribute-based keyword search (ABKS) can provide fine-grained keyword search and access control over the encrypted data at the same time. Nevertheless, prior ABKS schemes cannot simultaneously support fine-grained, effective, and accurate data retrieval over hierarchical data. In this paper, to tackle these issues, we propose a fine-grained ranked multi-keyword search scheme over hierarchical data by leveraging ciphertext-policy hierarchical attribute-based encryption (CP-HABE) and ranked multi-keyword search (RMKS) technologies. Then, we prove that our proposed scheme is selectively secure through security analysis and we also show the practicability and feasibility of the proposed scheme by performance evaluation.},
keywords={Access control;Encryption;Medical services;Keyword search;Internet of Things;Internet of Things;fine-grained and ranked;multi-keyword retrieval;hierarchical data},
doi={10.1109/ACCESS.2019.2928441},
ISSN={2169-3536},
month={},}
@ARTICLE{8482117,
author={Ke, Wenjun and Wu, Chunxue and Wu, Yan and Xiong, Neal N.},
journal={IEEE Access},
title={A New Filter Feature Selection Based on Criteria Fusion for Gene Microarray Data},
year={2018},
volume={6},
number={},
pages={61065-61076},
abstract={In machine learning and data mining, feature selection aims to seek a compact and discriminant feature subset from the original feature space. It is usually used as a preprocessing step to improve the prediction performance, understandability, scalability, and generalization capability of classifiers. A typical gene microarray data set has the characteristics of high dimensionality, limited samples, and most irrelevant features, and these characteristics make it difficult to discover a compact set of features that really contribute to the response of the model. In this paper, a score-based criteria fusion feature selection method (SCF) is proposed for cancer prediction, and this method aims at improving the prediction performance of the classification model. The SCF method is evaluated on five open gene microarray data sets and three low-dimensional data sets, and it shows superior performance over many well-known feature selection methods when employing two classifiers SVM and KNN to measure the quality of selected features. Experiments verify that SCF is able to find more discriminative features than the competing methods and can be used as a preprocessing algorithm to combine with other methods effectively.},
keywords={Feature extraction;Cancer;Mutual information;Redundancy;Time complexity;Predictive models;Prediction algorithms;Dimension reduction;feature selection;high-dimensional data;criteria fusion;cancer prediction},
doi={10.1109/ACCESS.2018.2873634},
ISSN={2169-3536},
month={},}
@ARTICLE{9103202,
author={Horota, Rafael Kenji and Aires, Alysson Soares and Marques, Ademir and Rossa, Pedro and de Souza, Eniuce Menezes and Gonzaga, Luiz and Veronez, Mauricio Roberto},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Printgrammetry—3-D Model Acquisition Methodology From Google Earth Imagery Data},
year={2020},
volume={13},
number={},
pages={2819-2830},
abstract={This article proposes a technique named Printgrammetry, a structured workflow that allows the extraction of 3-D models from Google Earth platform through the combination of image captures from the screen monitor with structure from motion algorithms. This technique was developed to help geologists and other geoscientists in acquiring 3-D photo-realistic models of outcrops and natural landscapes of big proportions without the need of field mapping and expensive equipment. The methodology is detailed aiming to permit easy reproducibility and focused on achieving the highest resolution possible by working with the best images that the platform can provide. The results have shown that it is possible to obtain visually high-quality models from natural landscapes from Google Earth by acquiring images at high level of detail regions of the software, using a 4K monitor, multidirectional screenshots, and by marking homogeneously spaced targets for georeferencing and scaling. The geometric quality assessment performed using light detection and ranging ground truth data as comparison shows that the Printgrammetry dense point clouds have reached 98.1% of the total covered area under 5 m of distance for the Half Dome case study and 96.7% for the Raplee Ridge case study. The generated 3-D models were then visualized and interacted through an immersive virtual reality software that allowed geologists to manipulate this virtual field environment in different scales. This technique is considered by the authors to have a promising potential for research, industrial, and educational projects that do not require high-precision models.},
keywords={Three-dimensional displays;Solid modeling;Geology;Software;Earth;Germanium;Data models;Digital outcrop model;geosciences;Google Earth (GE);photogrammetry},
doi={10.1109/JSTARS.2020.2997239},
ISSN={2151-1535},
month={},}
@ARTICLE{8063957,
author={Bosse, Sebastian and Maniry, Dominique and Müller, Klaus-Robert and Wiegand, Thomas and Samek, Wojciech},
journal={IEEE Transactions on Image Processing},
title={Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment},
year={2018},
volume={27},
number={1},
pages={206-219},
abstract={We present a deep neural network-based approach to image quality assessment (IQA). The network is trained end-to-end and comprises ten convolutional layers and five pooling layers for feature extraction, and two fully connected layers for regression, which makes it significantly deeper than related IQA models. Unique features of the proposed architecture are that: 1) with slight adaptations it can be used in a no-reference (NR) as well as in a full-reference (FR) IQA setting and 2) it allows for joint learning of local quality and local weights, i.e., relative importance of local quality to the global quality estimate, in an unified framework. Our approach is purely data-driven and does not rely on hand-crafted features or other types of prior domain knowledge about the human visual system or image statistics. We evaluate the proposed approach on the LIVE, CISQ, and TID2013 databases as well as the LIVE In the wild image quality challenge database and show superior performance to state-of-the-art NR and FR IQA methods. Finally, cross-database evaluation shows a high ability to generalize between different databases, indicating a high robustness of the learned features.},
keywords={Feature extraction;Image quality;Distortion;Databases;Optimization;Computational modeling;Full-reference image quality assessment;no-reference image quality assessment;neural networks;quality pooling;deep learning;feature extraction;regression},
doi={10.1109/TIP.2017.2760518},
ISSN={1941-0042},
month={Jan},}
@ARTICLE{8576506,
author={Shuai, Chunyan and Yang, Hengcheng and Ouyang, Xin and He, Mingwei and Gong, Zeweiyi and Shu, Wanneng},
journal={IEEE Access},
title={Analysis and Identification of Power Blackout-Sensitive Users by Using Big Data in the Energy System},
year={2019},
volume={7},
number={},
pages={19488-19501},
abstract={With the further liberalization of the electricity market of China, customers’ requirements, characteristics, and distribution, as well as the quality, security, and reliability of power supplies without interruption, have received considerable attention from power companies, policymakers, and researchers. How to deeply explore the distribution characteristics of electricity customers and analyze their sensitivities to electricity blackouts has become an especially important problem. This paper takes over 0.1 billion data, collected by various smart devices of the Internet of Things in the power system of China, such as smart meters, intelligent power consumption interactive terminals, data concentrators, and other cross-platform data, for example, 95 598 telephone records, complaint information, user bills, user information, and maintenance records, as study objects, to analyze the consumption characteristics of power users. It has been found that there is a wide range of power users who pay different electricity bills; a long-tail distribution following a power law lies in the number of users versus their paid electricity bills. Meanwhile, there are two Pareto effects (2-8 rule): the number of residents and non-residents versus their electricity bills, and the number of large industrial users and general industry (business users) versus in their electricity consumption and bills. Then, a decision tree algorithm is proposed to capture the characteristics of electricity consumers and to recognize the crowd who is power blackout sensitive. The evaluation indexes and parameters of the decision tree are discussed in detail, and a comparison with other intelligent algorithms shows that the decision tree has a good recognition performance over that of others, and the characteristics used to identify the blackout-sensitive crowd is various. All the results state that except for economic factors, positive social effects should also be considered. Various marketing strategies to satisfy different requirements of power users should be provided to promote long-term relationships between the power companies and power customers.},
keywords={Power system reliability;Electricity supply industry;Data mining;Economics;Decision trees;Security;Blackout sensitivity;big data;decision tree;electricity market;Internet of Things;long-tailed;Pareto effect},
doi={10.1109/ACCESS.2018.2886551},
ISSN={2169-3536},
month={},}
@ARTICLE{9238029,
author={Wensheng, Dai},
journal={IEEE Access},
title={Rural Financial Information Service Platform Under Smart Financial Environment},
year={2020},
volume={8},
number={},
pages={199944-199952},
abstract={The development and improvement of agricultural financial information service system is of great significance to the development of rural modernization, the improvement of rural comprehensive competitiveness and the construction of new socialist countryside. The construction of rural financial information service platform is directly related to the quality of rural financial information service, and directly affects the construction of new socialist countryside. In order to solve the problems of information collection, processing and integration of rural financial information service platform in China, the diversification, personalization, timeliness and accuracy of information demand are difficult to be met, and the organization and operation mode of the platform are not perfect. In this paper, based on the intelligent sensor, the whole digital transformation is realized through the reference of big data. Based on this, this paper establishes the research model of rural financial information service platform under the smart financial environment. From the current situation of the construction and application of rural financial information service platform in China, it studies the basic situation of the construction of rural financial information service platform in China from three aspects of functional scope, service mode and operation mode, and draws the improvement conclusion. The results show that the efficiency of rural financial information service is increased by 20% after using the improved method in this paper, which has certain use value.},
keywords={Information services;Itemsets;Financial services;Data mining;Software as a service;Technological innovation;Market research;Smart sensor;smart financial environment;financial information service platform;big data integration},
doi={10.1109/ACCESS.2020.3033279},
ISSN={2169-3536},
month={},}
@ARTICLE{8911489,
author={Zhong, Han and Qi, Geqi and Guan, Wei and Hua, Xiaochen},
journal={IEEE Access},
title={Application of Non-Negative Tensor Factorization for Airport Flight Delay Pattern Recognition},
year={2019},
volume={7},
number={},
pages={171724-171737},
abstract={With the rapid development of civil aviation transportation in China, huge demand growth has broken the balance between supply and demand, resulting in airspace congestion and increasing flight delays. The delays of large airports have been increasing year by year, which has seriously affected the air travel experience of passengers. Obtaining their flight delay patterns can help identify defects in flight scheduling and airspace utilization. The investigation based on the actual flight operation data of Tianjin Binhai International Airport (TSN) is conducted, in order to capture the relationship and impact between the factors such as traffic flow direction, airline attributes and hourly average delay distribution. Furthermore, Non-negative Tensor Factorization (NTF) is applied to pattern recognition by introducing CP (CANDECOMP/PARAFAC) decomposition and Block Coordinate Descent (BCD) algorithm for selected data set. Numerical experiments show that the designed method has good performance in terms of computation speed and solution quality. Recognition results indicate the significant pattern characteristics of the Tianjin airport delay are extracted, which can provide some new perspectives for air traffic management unit to alleviate airspace congestion and improve service quality.},
keywords={Delays;Airports;Pattern recognition;Atmospheric modeling;Meteorology;Air traffic control;Air traffic;flight delay;non-negative tensor factorization;pattern recognition},
doi={10.1109/ACCESS.2019.2955735},
ISSN={2169-3536},
month={},}
@ARTICLE{8920046,
author={Zhang, Tong and Mao, Baohua and Xu, Qi and Feng, Jia},
journal={IEEE Access},
title={Timetable Optimization for a Two-Way Tram Line With an Active Signal Priority Strategy},
year={2019},
volume={7},
number={},
pages={176896-176911},
abstract={Modern trams typically run along semi-exclusive right-of-way. Although tram lanes isolate trams from other traffic in the running sections, the operation process will be affected by signal control. To improve the service quality of trams and reduce the negative impact on intersections caused by bidirectional priority requests, we propose a timetable optimization method for a single two-way tram line based on active transit signal priority strategy. Combining with the characteristics of bidirectional signal priority strategy, trams can pass through the intersections without stopping by adjusting the running times and dwell times. A multiobjective optimization model of a tram timetable is established to minimize the total travel time, dwell time increment, and negative effect of the signal priority strategy. For obtaining a timetable with equal satisfaction for the three objectives, we adopt the fuzzy mathematical programming approach to transform the problems into mixed integer linear programming (MILP) problems, which can be solved by using standard solvers. The case study of Nanjing Qilin Tram Line 1 shows that the timetable optimization method designed in this paper can effectively improve the service efficiency of trams, and reduce the negative impact of the signal priority strategy on social vehicles. These empirical findings can give us some useful insights on the optimum design of tram timetable.},
keywords={Delays;Rails;Optimization methods;Roads;Process control;Tram;timetable;bidirectional signal priority strategy;multiobjective optimization;fuzzy mathematical programming},
doi={10.1109/ACCESS.2019.2957437},
ISSN={2169-3536},
month={},}
@ARTICLE{8865058,
author={Wang, Jin and Wu, Wenbing and Liao, Zhuofan and Sangaiah, Arun Kumar and Simon Sherratt, R.},
journal={IEEE Access},
title={An Energy-Efficient Off-Loading Scheme for Low Latency in Collaborative Edge Computing},
year={2019},
volume={7},
number={},
pages={149182-149190},
abstract={Mobile terminal users applications, such as smartphones or laptops, have frequent computational task demanding but limited battery power. Edge computing is introduced to offload terminals' tasks to meet the quality of service requirements such as low delay and energy consumption. By offloading computation tasks, edge servers can enable terminals to collaboratively run the highly demanding applications in acceptable delay requirements. However, existing schemes barely consider the characteristics of the edge server, which leads to random assignment of tasks among servers and big tasks with high computational intensity (named as “big task”) may be assigned to servers with low ability. In this paper, a task is divided into several subtasks and subtasks are offloaded according to characteristics of edge servers, such as transmission distance and central processing unit (CPU) capacity. With this multi-subtasks-to-multi-servers model, an adaptive offloading scheme based on Hungarian algorithm is proposed with low complexity. Extensive simulations are conducted to show the efficiency of the scheme on reducing the offloading latency with low energy consumption.},
keywords={Servers;Task analysis;Edge computing;Delays;Computational modeling;Energy consumption;Optimization;Latency;energy;offloading;edge computing},
doi={10.1109/ACCESS.2019.2946683},
ISSN={2169-3536},
month={},}
@ARTICLE{9310346,
author={Petri, Dario and Carbone, Paolo and Mari, Luca},
journal={IEEE Transactions on Instrumentation and Measurement},
title={Quality of Measurement Information in Decision-Making},
year={2021},
volume={70},
number={},
pages={1-16},
abstract={This article introduces a general-purpose framework aimed at capturing the elusive concept of quality of measurement information (MI), a critical issue for both researchers and practitioners when dealing with MI-enabled decision-making. The framework is a blueprint for the definition, assessment, communication, and improvement of MI quality, as analyzed through a set of general criteria, classified according to the syntactic, semantic, and pragmatic layers of semiotics, as suggested in the ISO 8000-8:2015 technical standard. The top-down analysis, where each criterion is specified in terms of characteristics and each characteristic in terms of domain-related indicators, is complemented with a bottom-up synthesis and operationalized by means of a flowchart. An application example, about the quality of information provided by the networks of measurement instruments reporting pollutants in the air, is presented to test the usefulness and the limitations of the framework.},
keywords={Syntactics;Semantics;Pragmatics;Measurement uncertainty;Decision making;Pollution measurement;Big Data;Decision-making;measurement;measurement information (MI);quality management;semiotic criteria;semiotics},
doi={10.1109/TIM.2020.3047954},
ISSN={1557-9662},
month={},}
@ARTICLE{9353479,
author={Carta, Salvatore M. and Consoli, Sergio and Podda, Alessandro Sebastian and Recupero, Diego Reforgiato and Stanciu, Maria Madalina},
journal={IEEE Access},
title={Ensembling and Dynamic Asset Selection for Risk-Controlled Statistical Arbitrage},
year={2021},
volume={9},
number={},
pages={29942-29959},
abstract={In recent years, machine learning algorithms have been successfully employed to leverage the potential of identifying hidden patterns of financial market behavior and, consequently, have become a land of opportunities for financial applications such as algorithmic trading. In this paper, we propose a statistical arbitrage trading strategy with two key elements: an ensemble of regression algorithms for asset return prediction, followed by a dynamic asset selection. More specifically, we construct an extremely heterogeneous ensemble ensuring model diversity by using state-of-the-art machine learning algorithms, data diversity by using a feature selection process, and method diversity by using individual models for each asset, as well models that learn cross-sectional across multiple assets. Then, their predictive results are fed into a quality assurance mechanism that prunes assets with poor forecasting performance in the previous periods. We evaluate the approach on historical data of component stocks of the S&P500 index. By performing an in-depth risk-return analysis, we show that this setup outperforms highly competitive trading strategies considered as baselines. Experimentally, we show that the dynamic asset selection enhances overall trading performance both in terms of return and risk. Moreover, the proposed approach proved to yield superior results during both financial turmoil and massive market growth periods, and it showed to have general application for any risk-balanced trading strategy aiming to exploit different asset classes.},
keywords={Predictive models;Forecasting;Data models;Machine learning algorithms;Heuristic algorithms;Prediction algorithms;Portfolios;Stock market forecast;statistical arbitrage;machine learning;ensemble learning},
doi={10.1109/ACCESS.2021.3059187},
ISSN={2169-3536},
month={},}
@ARTICLE{9120049,
author={Yang, Xiaowei and He, Lin and Zhao, Yong and Sang, Haiwei and Yang, Zu Liu and Cheng, Xian Jing},
journal={IEEE Access},
title={Multi-Attention Network for Stereo Matching},
year={2020},
volume={8},
number={},
pages={113371-113382},
abstract={In recent years, convolutional neural network (CNN) algorithms promote the development of stereo matching and make great progress, but some mismatches still occur in textureless, occluded and reflective regions. In feature extraction and cost aggregation, CNNs will greatly improve the accuracy of stereo matching by utilizing global context information and high-quality feature representations. In this paper, we design a novel end-to-end stereo matching algorithm named Multi-Attention Network (MAN). To obtain the global context information in detail at the pixel-level, we propose a Multi-Scale Attention Module (MSAM), combining a spatial pyramid module with an attention mechanism, when we extract the image features. In addition, we introduce a feature refinement module (FRM) and a 3D attention aggregation module (3D AAM) during cost aggregation so that the network can extract informative features with high representational ability and high-quality channel attention vectors. Finally, we obtain the final disparity through bilinear interpolation and disparity regression. We evaluate our method on the Scene Flow, KITTI 2012 and KITTI 2015 stereo datasets. The experimental results show that our method achieves state-of-the-art performance and that every component of our network is effective.},
keywords={Feature extraction;Three-dimensional displays;Convolution;Data mining;Image edge detection;Convolutional neural networks;Neural network;stereo matching;multi-scale attention module;feature refinement module;3D attention aggregation module},
doi={10.1109/ACCESS.2020.3003375},
ISSN={2169-3536},
month={},}
@ARTICLE{8730359,
author={Mao, Dianhui and Wang, Fan and Wang, Yalei and Hao, Zhihao},
journal={IEEE Access},
title={Visual and User-Defined Smart Contract Designing System Based on Automatic Coding},
year={2019},
volume={7},
number={},
pages={73131-73143},
abstract={Smart contract applications based on Ethereum blockchain have been widely used in many fields. They are developed by professional developers using specialized programming languages like solidity. It requires high requirements on knowledge of the specialized field and the proficiency in contract programming. Thus, it is hard for normal users to design a usable smart contract based on their own demands. Most current studies about smart contracts focus on the security of coding while lack of friendly tools for users to design the specialized templates of contracts coding. This paper provides a visual and user-defined smart contract designing systems. It makes the development of domain-specific smart contracts simpler and visualization for contract users. The system implements the domain-specific features extraction about the crawled data sets of smart contract programs by TF-IDF and K-means++ clustering algorithm. Then, it achieves the automatic generation of unified basic function codes by Char-RNN (improved by LSTM) based on the domain-specific features. The system adopts Google Blockly and links the generated codes with UI controls. Finally, it provides a set of specialized templates of basic functions for users to design smart contracts by the friendly interface. It reduces the difficulty and costs of contract programming. The paper offers a case study to design contracts by users. The designed contracts were validated on the existing system to implement the food trading and traders' credit evaluation. The experimental results show that the designed smart contracts achieve good integration with the existing system and they can be deployed and compiled successfully.},
keywords={Smart contracts;Blockchain;Encoding;Programming;Visualization;Security;Smart contract;Char-RNN;LSTM;automatic coding},
doi={10.1109/ACCESS.2019.2920776},
ISSN={2169-3536},
month={},}
@ARTICLE{8936445,
author={Kim, Dohyeong and Cho, Sunghwan and Tamil, Lakshman and Song, Dae Jin and Seo, Sungchul},
journal={IEEE Access},
title={Predicting Asthma Attacks: Effects of Indoor PM Concentrations on Peak Expiratory Flow Rates of Asthmatic Children},
year={2020},
volume={8},
number={},
pages={8791-8797},
abstract={Despite ample research on the association between indoor air pollution and allergic disease prevalence, public health and environmental policies still lack predictive evidence for developing a preventive guideline for patients or vulnerable populations mostly due to limitation of real-time big data and model predictability. Recent popularity of IoT and machine learning techniques could provide enabling technologies for collecting real-time big data and analyzing them for more accurate prediction of allergic disease risks for evidence-based intervention, but the effort is still in its infancy. This pilot study explored and evaluated the feasibility of a deep learning algorithm for predicting asthma risk. It is based on peak expiratory flow rates (PEFR) of 14 pediatric asthma patients visiting the Korea University Medical Center and indoor particulate matter PM10 and PM2.5 concentration data collected at their residence every 10 minutes using a PM monitoring device with a low-cost sensor between September 1, 2017 and August 31, 2018. We interpolated the PEFR results collected twice a day for each patient throughout the day so that it can be matched to the PM and other weather data. The PEFR results were classified into three categories such as `Green' (normal), `Yellow' (mild to moderate exacerbation) and `Red' (severe exacerbation) with reference to their best peak flow value. Long Short-Term Memory (LSTM) model was trained using the first 10 months of the linked data and predicted asthma risk categories for the next 2 months during the study period. LSTM model is found to predict the asthma risk categories better than multinomial logistic (MNL) regression as it incorporates the cumulative effects of PM concentrations over time. Upon successful modifications of the algorithm based on a larger sample, this approach could potentially play a groundbreaking role for the scientific data-driven medical decision making.},
keywords={Respiratory system;Real-time systems;Monitoring;Deep learning;Predictive models;Air pollution;Atmospheric measurements;Asthma;indoor particulate matter;deep learning;peak expiratory flow rates;real-time monitoring},
doi={10.1109/ACCESS.2019.2960551},
ISSN={2169-3536},
month={},}
@ARTICLE{9583290,
author={Wang, Mimi and He, Xudong and Zhao, Peihai},
journal={IEEE Access},
title={Process Model Enhancement Through Capturing Important Behaviors and Rating Trace Variants},
year={2021},
volume={9},
number={},
pages={143634-143660},
abstract={In the field of process discovery, it is worth noting that most process discovery algorithms assume that event logs are clean, i.e., event logs should not contain infrequent behaviors. However, real-life event logs often contain infrequent behaviors (i.e., outliers) and lead to quality issues of the discovered process model. On the other hand, driven by recent trends such as big data and process automation, the volume of event data is rapidly increasing: an event log may contain billions of event data. Unfortunately, some process mining algorithms and platforms may have difficulties handling such event logs. The ever-increasing size of event data and infrequent behaviors in the event log are two main challenges in the field of process discovery nowadays. However, little research has been conducted on simultaneously filtering infrequent behaviors and decreasing the size of the event log: Various filtering methods can filter infrequent behaviors, whereas the volume of the filtered log is still considerable. On the other hand, sampling methods can reduce the size of the event log, but the processed event log may still contain infrequent behaviors. Therefore, this paper proposes a technique to simultaneously filter infrequent behaviors and control the volume of input logs by capturing important behaviors and rating trace variants. Our experiments show that our approach can significantly improve the quality of the discovered process models. Furthermore, our approach can obtain a better process model from 0.001% trace variants than the complete event log and significantly improves the runtime of discovery algorithms.},
keywords={Data mining;Task analysis;Solid modeling;Roads;Reactive power;Power capacitors;Process mining;process discovery;filtering infrequent behaviors;event log preprocessing;process model enhancement},
doi={10.1109/ACCESS.2021.3121997},
ISSN={2169-3536},
month={},}
@ARTICLE{9381206,
author={Tian, Yuan and Song, Biao and Ma, Tinghuai and Al-Dhelaan, Abdullah and Al-Dhelaan, Mohammed},
journal={IEEE Access},
title={Bi-Tier Differential Privacy for Precise Auction-Based People-Centric IoT Service},
year={2021},
volume={9},
number={},
pages={55036-55044},
abstract={With the fast proliferation of device sensing and computing, crowed sensing has become the building block of the Internet of things. Consequently, various data collection and incentive mechanisms are investigated for people-centric services. In this paper, we have investigated the problem of privacy-aware people-centric IoT service based on a tailored auction approach. We applied a bi-tier differential privacy methodology on the data collected from crowdsensing IoT devices. A corresponding pricing scheme is also proposed to ensure the property of incentive compatibility, precise service data, and anonymized query results. Comparing to traditional privacy-aware auction schemes which only focus on the cost, our corresponding precise privacy-aware auction scheme provides a tailored IoT service based on the customers' request. The proposed trial query technique is able to provide a precise assessment of service quality, thus improves the efficiency of the people-centric IoT service. The customer could enjoy the convenience of service evaluation before making a bid, while the actual service data is anonymized to guarantee the service providers' interests. We evaluate the proposed bi-tier differential privacy schema for auction-based service by conducting extensive simulations. The experimental results show that our proposed method yields higher data utility and accuracy for the IoT service customers with privacy concerns.},
keywords={Differential privacy;Internet of Things;Data privacy;Privacy;Sensors;Incentive schemes;Databases;Data protection;Internet of Things;differential privacy;crowd sensing IoT system},
doi={10.1109/ACCESS.2021.3067138},
ISSN={2169-3536},
month={},}
@ARTICLE{8552343,
author={Li, Shun and Wen, Junhao and Luo, Fengji and Ranzi, Gianluca},
journal={IEEE Access},
title={Time-Aware QoS Prediction for Cloud Service Recommendation Based on Matrix Factorization},
year={2018},
volume={6},
number={},
pages={77716-77724},
abstract={Prediction of quality of service (QoS) is a critical area of research for cloud service recommendation. The disadvantage of QoS values is that they are directly related to time series of service status and network condition and thus instantly vary over time. The main contribution of this paper is to consider service invocation time as a dynamic factor in the collaborative filtering model and recommend high-quality services for target user. In particular, this paper proposes a time-aware matrix factorization (TMF) model that integrates QoS time series to provide two-phase QoS predictions for cloud service recommendation. The TMF model uses an adaptive matrix factorization model on a sparse QoS dataset to predict the missing QoS values. A temporal smoothing method is then developed and applied to the predicted result to perform the time-varying QoS prediction that accounts for the dependence of QoS values at different time intervals. The numerical experiments presented are conducted to validate the accuracy of the proposed method on a public QoS dataset.},
keywords={Quality of service;Cloud computing;Sparse matrices;Smoothing methods;Adaptation models;Predictive models;Time series analysis;Could service;recommender system;QoS prediction;time-aware;matrix factorization},
doi={10.1109/ACCESS.2018.2883939},
ISSN={2169-3536},
month={},}
@ARTICLE{8822928,
author={Xie, Li and He, Jiqun and Cheng, Pengfei and Xiao, Runsha and Zhou, Xianghong},
journal={IEEE Access},
title={A Multi-Criteria 2-Tuple Linguistic Group Decision-Making Method Based on TODIM for Cholecystitis Treatments Selection},
year={2019},
volume={7},
number={},
pages={127967-127986},
abstract={Cholecystitis is a common disease with a high incidence, and attracts much attention. It not only harms human health, but also affects quality of work and life. Therefore, the choice of a suitable treatment is badly important for patients. In this paper, a novel selection model of treatments for cholecystitis based on hybrid multiple-criteria group decision-making (MCGDM), which is helpful to choose the most suitable treatment in the case of asymmetric information between doctors and patients. Subsequently, subjective and objective criteria are comprehensively taken into account in the index system of the selection model for cholecystitis, and combines 2-tuple linguistic with quantitative data analysis. Besides, the evaluation information obtained from the patient's conditions, the treatment and the hospital's medical status, etc., including real numbers, interval numbers, and linguistic labels with multi-granularity, is more complete and real. And the 2-tuple linguistic model is used to unify the non-homogeneous information, so the treatment selection is accurate and reliable. Simultaneously, for the unknown index and criteria weight, the improved entropy weight method and the BWM (best-worst-method) are utilized to figure out the index weight and criteria weight, respectively. Further, TODIM (an acronym in Portuguese for interactive and multi-criteria decision-making model) method based on the prospect theory is applied to solve the prioritization of cholecystitis treatments, and give full consideration to the decision maker of risk aversion. Eventually, an empirical study of treatment selection for cholecystitis is conducted. Sensitivity analysis and comparative analysis indicate that the proposed selection model of treatments for cholecystitis patients is reliable and effective.},
keywords={Decision making;Linguistics;Indexes;Hospitals;Surgery;Entropy;Computational modeling;Cholecystitis;best-worst method (BWM);entropy weight method;2-tuple linguistic;group decision-making (MCGDM);TODIM},
doi={10.1109/ACCESS.2019.2939211},
ISSN={2169-3536},
month={},}
@ARTICLE{8573120,
author={Niu, Yuzhen and Zhong, Yini and Guo, Wenzhong and Shi, Yiqing and Chen, Peikun},
journal={IEEE Access},
title={2D and 3D Image Quality Assessment: A Survey of Metrics and Challenges},
year={2019},
volume={7},
number={},
pages={782-801},
abstract={Image quality is important not only for the viewing experience, but also for the performance of image processing algorithms. Image quality assessment (IQA) has been a topic of intense research in the fields of image processing and computer vision. In this paper, we first analyze the factors that affect two-dimensional (2D) and three-dimensional (3D) image quality, and then provide an up-to-date overview on IQA for each main factor. The main factors that affect 2D image quality are fidelity and aesthetics. Another main factor that affects stereoscopic 3D image quality is visual comfort. We also describe the IQA databases and give the experimental results on representative IQA metrics. Finally, we discuss the challenges for IQA, including the influence of different factors on each other, the performance of IQA metrics in real applications, and the combination of quality assessment, restoration, and enhancement.},
keywords={Measurement;Image quality;Two dimensional displays;Visualization;Three-dimensional displays;Distortion;Indexes;Image quality assessment;image aesthetics assessment;visual comfort;image quality enhancement},
doi={10.1109/ACCESS.2018.2885818},
ISSN={2169-3536},
month={},}
@ARTICLE{9253992,
author={Ju, Mingye and Ding, Can and Guo, Y. Jay},
journal={IEEE Photonics Journal},
title={VROHI: Visibility Recovery for Outdoor Hazy Image in Scattering Media},
year={2020},
volume={12},
number={6},
pages={1-15},
abstract={Additive haze model (AHM), due to its high simplicity, has a potential to increase the efficiency of the restoration procedure of images degraded by scattering media. However, AHM is designed for hazy remote sensing data and is not suitable to be used on outdoor images. In this paper, according to the low-frequency feature (LFC) of haze, AHM is modified via gamma correction technique to make it suitable for modeling outdoor images. Benefitting from the modified AHM (MAHM), a simple yet effective method called VROHI is proposed to enhance the visibility of an outdoor hazy image. In specific, a low complexity LFC extraction method is designed by utilizing characteristic of the discrete cosine transform. Subsequently, by constructing the linear function of unknown parameters and imposing the saturation prior on MAHM, the image dehazing problem can be derived into a global optimization function. Experiments reveal that the proposed VROHI is superior to the other state-of-the-art techniques in terms of both the processing efficiency and recovery quality.},
keywords={Scattering;Real-time systems;Image restoration;Additive haze model;global optimization dehazing;haze thickness map;low-frequency component;propagation and scattering},
doi={10.1109/JPHOT.2020.3036873},
ISSN={1943-0655},
month={Dec},}
@ARTICLE{9458302,
author={Al-Dayyeni, Wisam Subhi and Al-Yousif, Shahad and Taher, Mayada M. and Al-Faouri, Ahmad Wahib and Tahir, Nooritawati Md and Jaber, Mustafa Musa and Ghabban, Fahad and Najm, Ihab A. and Alfadli, Ibrahim M. and Ameerbakhsh, Omair Z. and Mnati, Mohannad Jabbar and Al-Shareefi, Nael A. and Saleh, Abbadullah H.},
journal={IEEE Access},
title={A Review on Electronic Nose: Coherent Taxonomy, Classification, Motivations, Challenges, Recommendations and Datasets},
year={2021},
volume={9},
number={},
pages={88535-88551},
abstract={Context: Quality Control (QC) has been constantly an essential concern in many fields like food industry production, medical drugs, environmental protection, and so on. An odor or flavor, as a global fingerprint, can be implemented as a non-invasive mechanism for quality assurance. This computer-based approach can assure accurate detection and precise identification of the product quality or manufactured goods. Objective: This paper aims to achieve a systematic review about e-nose by introducing the achievements made by researchers in this area, to summarize their findings, to provide motivations and challenges to new researchers in the field of e-nose. Methods: The articles that were being utilized in the e-nose field were systematically achieved using three search engines: The online library of IEEE Explore, Web of Science and Science Direct for time span of 7 years (from 2013 to 2020). Both medical literature reviews and technical reviews were considered in the criteria of the research for wider understanding in the field of e-nose. The articles were categorized according to the objective of the research and projected into four classes. Upon completion of screening process 333 research papers using the exclusion and inclusion conditions, as the final set 54 articles were selected. Results: The taxonomy of this research was classified into four categories. The first one included the suggested methods that introduced the utilization of the e-nose for classification purposes (9/54 papers). The second category comprises the methods related to the development of e-nose (24/54 papers). The third one included the review studies about the e-nose (8/54 papers). The fourth group comprises comparative studies and evaluation (13/54 papers). Discussion: This systematic review contributes for a clearer understanding and a full insight in the e- nose research field by surveying and categorizing pertinent research efforts. Conclusion: This review paper will help to address the up-to-date research opportunities, challenges, problems, motivations and recommendations related to the utilization of e-nose in all fields of sciences and industries.},
keywords={Pattern recognition;Electronic noses;Olfactory;Taxonomy;Food industry;Sensor arrays;Quality assessment;Artificial olfaction;electronic nose;feature classification;food quality;machine learning;pattern recognition},
doi={10.1109/ACCESS.2021.3090165},
ISSN={2169-3536},
month={},}
@ARTICLE{9256314,
author={Li, Ling-Fang and Wang, Xu and Hu, Wei-Jian and Xiong, Neal N. and Du, Yong-Xing and Li, Bao-Shan},
journal={IEEE Access},
title={Deep Learning in Skin Disease Image Recognition: A Review},
year={2020},
volume={8},
number={},
pages={208264-208280},
abstract={The application of deep learning methods to diagnose diseases has become a new research topic in the medical field. In the field of medicine, skin disease is one of the most common diseases, and its visual representation is more prominent compared with the other types of diseases. Accordingly, the use of deep learning methods for skin disease image recognition is of great significance and has attracted the attention of researchers. In this study, we review 45 research efforts on the identification of skin disease by using deep learning technology since 2016. We analyze these studies from the aspects of disease type, data set, data processing technology, data augmentation technology, model for skin disease image recognition, deep learning framework, evaluation indicators, and model performance. Moreover, we summarize the traditional and machine learning-based skin disease diagnosis and treatment methods. We also analyze the current progress in this field and predict four directions that may become the research topic in the future. Our results show that the skin disease image recognition method based on deep learning is better than those of dermatologists and other computer-aided treatment methods in skin disease diagnosis, especially the multi deep learning model fusion method has the best recognition effect.},
keywords={Deep learning;Analytical models;Image recognition;Skin;Data models;Medical diagnosis;Diseases;Deep learning;image recognition;review;skin disease},
doi={10.1109/ACCESS.2020.3037258},
ISSN={2169-3536},
month={},}
@ARTICLE{8835895,
author={Long, Yanhua and Li, Yijie and Wei, Shuang and Zhang, Qiaozheng and Yang, Chunxia},
journal={IEEE Access},
title={Large-Scale Semi-Supervised Training in Deep Learning Acoustic Model for ASR},
year={2019},
volume={7},
number={},
pages={133615-133627},
abstract={This study investigated large-scale semi-supervised training (SST) to improve acoustic models for automatic speech recognition. The conventional self-training, the recently proposed committee-based SST using heterogeneous neural networks and the lattice-based SST were examined and compared. The large-scale SST was studied in deep neural network acoustic modeling with respect to the automatic transcription quality, the importance data filtering, the training data quantity and other data attributes of a large quantity of multi-genre unsupervised live data. We found that the SST behavior on large-scale ASR tasks was very different from the behavior obtained on small-scale SST: 1) big data can tolerate a certain degree of mislabeling in the automatic transcription for SST. It is possible to achieve further performance gains with more unsupervised fresh data, and even the automatic transcriptions have a certain degree of errors; 2) the audio attributes, transcription quality and importance of the fresh data are more important than the increased data quantity for large-scale SST; and 3) there are large differences in performance gains on different recognition tasks, such that the benefits highly depend on the selected data attributes of unsupervised data and the data scale of the baseline ASR system. Furthermore, we proposed a novel utterance filtering approach based on active learning to improve the data selection in large-scale SST. The experimental results showed that the SST with the proposed data filtering yields a 2-11% relative word error rate reduction on five multi-genre recognition tasks, even with the baseline acoustic model that was already well trained on a 10000-hr supervised dataset.},
keywords={Acoustics;Training;Hidden Markov models;Data models;Task analysis;Neural networks;Deep learning;Semi-supervised learning;data preprocessing;acoustic modeling;speech recognition},
doi={10.1109/ACCESS.2019.2940961},
ISSN={2169-3536},
month={},}
@ARTICLE{9793359,
author={Adebayo, Glory O. and Yampolskiy, Roman V.},
journal={Big Data Mining and Analytics},
title={Estimating Intelligence Quotient Using Stylometry and Machine Learning Techniques: A Review},
year={2022},
volume={5},
number={3},
pages={163-191},
abstract={The task of trying to quantify a person's intelligence has been a goal of psychologists for over a century. The area of estimating IQ using stylometry has been a developing area of research and the effectiveness of using machine learning in stylometry analysis for the estimation of IQ has been demonstrated in literature whose conclusions suggest that using a large dataset could improve the quality of estimation. The unavailability of large datasets in this area of research has led to very few publications in IQ estimation from written text. In this paper, we review studies that have been done in IQ estimation and also that have been done in author profiling using stylometry and we conclude that based on the success of IQ estimation and author profiling with stylometry, a study on IQ estimation from written text using stylometry will yield good results if the right dataset is used.},
keywords={Estimation;Psychology;Reinforcement learning;Big Data;Data mining;Task analysis;Testing;stylometry;IQ estimation;authorship attribution;intelligence;IQ;author profiling;machine learning},
doi={10.26599/BDMA.2022.9020002},
ISSN={2096-0654},
month={Sep.},}
@ARTICLE{7352306,
author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
journal={Proceedings of the IEEE},
title={Taking the Human Out of the Loop: A Review of Bayesian Optimization},
year={2016},
volume={104},
number={1},
pages={148-175},
abstract={Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
keywords={Big data;Bayes methods;Linear programming;Decision making;Design of experiments;Optimization;Genomes;Statistical analysis;decision making;design of experiments;optimization;response surface methodology;statistical learning;genomic medicine;Decision making;design of experiments;optimization;response surface methodology;statistical learning},
doi={10.1109/JPROC.2015.2494218},
ISSN={1558-2256},
month={Jan},}
@ARTICLE{9427182,
author={Zhu, Lilu and Huang, Kai and Hu, Yanfeng and Tai, Xianqing},
journal={IEEE Access},
title={A Self-Adapting Task Scheduling Algorithm for Container Cloud Using Learning Automata},
year={2021},
volume={9},
number={},
pages={81236-81252},
abstract={With the rapid development of cloud computing and container technology, more and more applications are deployed to the cloud, and the scale of cloud platform is expanding. Due to the large number of container instances running in the platform, complex dependency relationship, fast version iteration and other characteristics, the update of business can often cause the change of the whole cloud resource environment, which triggers the repetitive scheduling problem of related tasks and affects stability of the business. In this paper, we propose a self-adapting task scheduling algorithm (ADATSA) using learning automata to solve these problems. Firstly, we design a learning automata model and objective function for the system on task scheduling problem. Then, we realize an effective reward-penalty mechanism for scheduling actions in combination with the idle state of resources and the running state of tasks in the current environment. Meanwhile, the environment is modeled by cluster, node and task, and the probability of action selected is optimized by scheduling execution, thus enhancing the adaptability to the cloud environment of the scheduling and accelerating convergence. Finally, we construct a framework of task load monitoring with buffer queue to achieve dynamic scheduling based on priority. The experimental part verifies the effectiveness of proposed algorithm with different angles such as resource imbalance degree, resource residual degree and QoS. Compared with other learning automata scheduling models such as LAEAS, non-automata technology based algorithms such as PSOS and K8S scheduling engine, ADATSA shows the better performance of environment adaptability, resource optimization efficiency and QoS in dynamic scheduling. The theoretical analysis was consistent with the experimental results.},
keywords={Task analysis;Containers;Job shop scheduling;Cloud computing;Optimal scheduling;Learning automata;Dynamic scheduling;Container cloud;learning automata;self-adapting scheduling;reward-penalty strategy},
doi={10.1109/ACCESS.2021.3078773},
ISSN={2169-3536},
month={},}
@ARTICLE{7109106,
author={Gruber, Astrid and Wessel, Birgit and Martone, Michele and Roth, Achim},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={The TanDEM-X DEM Mosaicking: Fusion of Multiple Acquisitions Using InSAR Quality Parameters},
year={2016},
volume={9},
number={3},
pages={1047-1057},
abstract={Since 2010, TanDEM-X and its twin satellite TerraSAR-X fly in a close orbit formation and form a single-pass synthetic aperture radar (SAR) interferometer. The formation was established to acquire a global high-precision digital elevation model (DEM) using SAR interferometry (InSAR). In order to achieve the required height accuracy of the TanDEM-X DEM, at least two global coverages have to be acquired. However, in difficult and mountainous terrain, up to five coverages are present. Here, acquisitions from ascending and descending orbits are needed to fill gaps and to overcome geometric limitations. Therefore, a strategy to properly combine the available height estimates is mandatory. The objective of this paper is the presentation of the operational TanDEM-X DEM mosaicking approach. In general, multiple InSAR DEM heights are combined by means of a weighted average with the height error as weight. Apart from this widely used mosaicking approach, one big challenge remains with the handling of larger height discrepancies between the input data, which are mainly caused by phase unwrapping errors, but also by temporal changes between acquisitions. In the case of inconsistencies, the TanDEM-X mosaicking approach performs a grouping into height levels. A priority concept is set up to evaluate the different groups of heights considering the number of DEMs and several InSAR quality parameters: the height error, the phase unwrapping method, and the height of ambiguity. This allows the identification of the most reliable height level for mosaicking. This fusion concept is verified on different test areas affected by phase unwrapping errors in flat and mountainous terrain as well as by height discrepancies in forests. The results show that the quality of the final TanDEM-X DEM mosaic benefits a lot from this mosaicking approach.},
keywords={Reliability;Orbits;Synthetic aperture radar;Calibration;Accuracy;Coherence;Earth;Digital elevation models (DEMs);image fusion;interferometric synthetic aperture radar (InSAR);mosaicking;TanDEM-X;Digital elevation models (DEMs);image fusion;interferometric synthetic aperture radar (InSAR);mosaicking;TanDEM-X},
doi={10.1109/JSTARS.2015.2421879},
ISSN={2151-1535},
month={March},}
@ARTICLE{8501920,
author={Mohammadi, Mehdi and Al-Fuqaha, Ala},
journal={IEEE Access},
title={Exploiting the Spatio-Temporal Patterns in IoT Data to Establish a Dynamic Ensemble of Distributed Learners},
year={2018},
volume={6},
number={},
pages={63316-63328},
abstract={Internet of Things applications can greatly benefit from accurate prediction models. The performance of prediction models is highly dependent on the quantity and quality of their training data. In this paper, we investigate the creation of a dynamic ensemble from distributed deep learning models by considering the spatiotemporal patterns embedded in the training data. Our dynamic ensemble does not depend on offline configurations. Instead, it exploits the spatiotemporal patterns embedded in the training data to generate dynamic weights for the underlying weak distributed deep learners to create a stronger learner. Our evaluation experiments using three real-world datasets in the context of the smart city show that our proposed dynamic ensemble strategy leads to an improved error rate of up to 33% compared to the baseline strategy even when using31of the training data. Moreover, using only 20% of the training data, the error rate of the model slightly increased by up to 2 in terms of mean square error. This increase is 82% less than the 11.3 increase seen in the baseline model. Therefore, our approach contributes to the reduced network traffic while not hindering the accuracy significantly.},
keywords={Data models;Predictive models;Training data;Load modeling;Computational modeling;Sensors;Distributed deep neural networks;spatio-temporal analysis;ensemble deep learning;bloom filter;Internet of Things;smart city},
doi={10.1109/ACCESS.2018.2877153},
ISSN={2169-3536},
month={},}
@ARTICLE{8787775,
author={Zheng, Yang-Yang and Kong, Jian-Lei and Jin, Xue-Bo and Wang, Xiao-Yi and Su, Ting-Li and Wang, Jian-Li},
journal={IEEE Access},
title={Probability Fusion Decision Framework of Multiple Deep Neural Networks for Fine-Grained Visual Classification},
year={2019},
volume={7},
number={},
pages={122740-122757},
abstract={Fine-grained visual classification tasks often suffer from that the subordinate categories within a basic-level category have low inter-class discrepancy and high intra-class variances, which is still challenging research for traditional deep neural networks (DNNs). However, different models extract local parts' features in isolation and neglect the inherent correlations and distribution in high-dimensional space, which limit the single model to achieve better accuracy. In this paper, we propose a novel probability fusion decision framework (named as PFDM-Net) for fine-grained visual classification. More specifically, it first employs data-augmented tricks to enlarge the dataset and pretrain the basic VGG19 and ResNet networks on high-quality images datasets to learn common and domain knowledge simultaneously while fine-tuning with professional skill. Next, refined multiple DNNs with transfer learning are applied to design a multi-stream feature extractor, which utilizes the mixture-granularity information to exploit high-dimensionality features for distinguishing interclass discrepancy and tolerating intra-class variances. Finally, a probability fusion module equipped with gating network and probability fusion layer is developed to fuse different components model with Gaussian distribution as a unified probability representation for the ultimate fine-grained recognition. The input of this module is the various features of multi-models and the output is the fused classification probability. The end-to-end implementation of our framework contain an inner loop about the EM algorithm within an outer loop with the gradient back-propagation optimization of the whole network. Experimental results demonstrate the outperforming performance of PFDM-Net with higher classification accuracy on different fine-grained datasets compared with the state-of-the-arts methods. More discussions are provided to indicate the potential applications in combination with other work.},
keywords={Feature extraction;Training;Visualization;Neural networks;Task analysis;Learning systems;Streaming media;Deep neural network;weakly-supervised learning;multi-steam feature extractor;probability fusion module},
doi={10.1109/ACCESS.2019.2933169},
ISSN={2169-3536},
month={},}
@ARTICLE{8242357,
author={Zhao, Feng and Tian, Zeliang and Jin, Hai},
journal={IEEE Access},
title={Entity-Based Language Model Smoothing Approach for Smart Search},
year={2018},
volume={6},
number={},
pages={9991-10002},
abstract={Smart search plays an important role in all walks of life, for example, according to business needs, accurate search of required knowledge from massive resources is an important way to enhance industrial intelligence. Smoothing of the language model is essential for obtaining high-quality search results because it helps to reduce mismatching and overfitting problems caused by data sparseness. Traditional smoothing methods lexically focus on the global corpus and locally cluster documents information without semantic analysis, which leads to deficiency of the semantic correlations between query statements and documents. In this paper, we propose an entity-based language model smoothing approach for smart search that uses semantic correlation and takes entities as bridges to build the entity semantic language model using a knowledge base. In this approach, entities in the documents are linked to an external knowledge base, such as Wikipedia. Then, the entity semantic language model is generated by using soft-fused and hardfused methods. A two-level merging strategy is also presented to smooth the language model according to whether a given word is semantically relevant to the document or not, which integrates the Dir-smoothing and JM-smoothing methods. Experimental results show that the smoothed language model more closely approximates the word probability distribution under the document semantic theme and more accurately estimates the relevance between query and document.},
keywords={Smoothing methods;Semantics;Computational modeling;Correlation;Information retrieval;Knowledge based systems;Probability distribution;Language model smoothing;entity;knowledge base;semantic relevance},
doi={10.1109/ACCESS.2017.2788417},
ISSN={2169-3536},
month={},}
@ARTICLE{8884208,
author={Zhang, Liyuan and Zhang, Pengcheng and Yang, Jie and Li, Jie and Gui, Zhiguo},
journal={IEEE Access},
title={Aperture Shape Generation Based on Gradient Descent With Momentum},
year={2019},
volume={7},
number={},
pages={157623-157632},
abstract={Direct aperture optimization (DAO) is an effective method to generate high-quality intensity-modulated radiation therapy treatment plans. In generic DAO, the direction of negative gradient descent is generally used to determine the aperture shape. However, this strategy can reduce the convergence rate, especially near the optimal value. We propose aperture shape generation based on the direction of gradient descent with momentum, where column generation is implemented as carrier. During aperture shape generation of column generation, the current aperture gradient map is first calculated. Then, the gradient with momentum is calculated based on the existing gradient information. Finally, the direction of gradient descent with momentum is constructed for obtaining the deliverable aperture shape by solving the pricing problem. To verify the effectiveness of the proposed method, we conducted comparative experiments on two head and neck and two prostate tumor cases. Compared with generic column generation, the proposed method can effectively protect the organs at risk while ensuring the required dose distribution to the target. Using the proposed method, the number of apertures and optimization time can be reduced by up to 30.95 and 32.96%, respectively, compared to the conventional approach. The experimental results suggest that the proposed method can accelerate the search speed and improve the quality of treatment plans.},
keywords={Apertures;Shape;Optimization;Pricing;Linear programming;Acceleration;Convergence;Aperture shape;column generation;direct aperture optimization;gradient descent direction;gradient descent with momentum},
doi={10.1109/ACCESS.2019.2949871},
ISSN={2169-3536},
month={},}
@ARTICLE{8630825,
author={Zhang, Liyuan and Gui, Zhiguo and Yang, Jie and Zhang, Pengcheng},
journal={IEEE Access},
title={A Column Generation Approach Based on Region Growth},
year={2019},
volume={7},
number={},
pages={31123-31139},
abstract={In intensity-modulated radiation therapy (IMRT), a network flow is adopted to solve the pricing problem of the generic column generation approach in order to obtain a deliverable aperture. However, excessive computation results from the direct use of a network flow. In addition, a decline in plan quality may result from the direct determination of the leaf position using the gradient information. To overcome these problems, a column generation approach based on region growth is proposed. The proposed method is designed to reduce the computational cost of solving the pricing problem and improve the IMRT plan quality. First, the gradients of the beamlets are obtained by an objective function constructed under the constraint conditions of the organs. Second, the gradients are transformed nonlinearly. Third, the positions of the continuous negative gradient regions in each row of the aperture are determined and stored. Fourth, these gradients are taken as a whole and added to the aperture network flow, which is solved as a shortest-path problem. Finally, the deliverable aperture is obtained and added to the treatment plan. To verify the effectiveness of the proposed method, experiments involving five five-field prostate cancer cases and five nine-field head and neck cancer cases were conducted. Compared with the generic column generation method, the dose distribution of the target is ensured by the proposed method, which also effectively protects organs at risk and reduces the running time. Specifically, in ten groups of comparative experiments, the normal tissue complication probability of the proposed method is reduced by up to 3.37%, and the maximum acceleration rate is 20.44%. According to the experimental results, the proposed method is more consistent with clinical requirements compared with the generic column generation method.},
keywords={Apertures;Optimization;Pricing;Linear programming;Shape;Biomedical applications of radiation;Sequential analysis;Column generation;direct aperture optimization;image processing;intensity-modulated radiation therapy;region growth},
doi={10.1109/ACCESS.2019.2896175},
ISSN={2169-3536},
month={},}
@ARTICLE{7484255,
author={Wu, Qihui and Ding, Guoru and Du, Zhiyong and Sun, Youming and Jo, Minho and Vasilakos, Athanasios V.},
journal={IEEE Access},
title={A Cloud-Based Architecture for the Internet of Spectrum Devices Over Future Wireless Networks},
year={2016},
volume={4},
number={},
pages={2854-2862},
abstract={The dramatic increase in data rates in wireless networks has caused radio spectrum usage to be an essential and critical issue. Spectrum sharing is widely recognized as an affordable, near-term method to address this issue. This paper first characterizes the new features of spectrum sharing in future wireless networks, including heterogeneity in sharing bands, diversity in sharing patterns, crowd intelligence in sharing devices, and hyperdensification in sharing networks. Then, to harness the benefits of these unique features and promote a vision of spectrum without bounds and networks without borders, this paper introduces a new concept of the Internet of spectrum devices (IoSDs) and develops a cloud-based architecture for IoSD over future wireless networks, with the prime aim of building a bridging network among various spectrum monitoring devices and massive spectrum utilization devices, and enabling a highly efficient spectrum sharing and management paradigm for future wireless networks. Furthermore, this paper presents a systematic tutorial on the key enabling techniques of the IoSD, including big spectrum data analytics, hierarchal spectrum resource optimization, and quality of experience-oriented spectrum service evaluation. In addition, the unresolved research issues are also presented.},
keywords={Internet of things;Spectrum management;Data analytics;Resource management;Quality of experience;Cognitive radio;Wireless networks;Cloud computing;Systematics;Internet of spectrum devices (IoSD);cognitive radio;data analytics;resource optimization;quality of experience (QoE);Internet of spectrum devices (IoSD);cognitive radio;data analytics;resource optimization;quality of experience (QoE)},
doi={10.1109/ACCESS.2016.2576286},
ISSN={2169-3536},
month={},}
@ARTICLE{9530686,
author={Mansouri, Majdi and Trabelsi, Mohamed and Nounou, Hazem and Nounou, Mohamed},
journal={IEEE Access},
title={Deep Learning-Based Fault Diagnosis of Photovoltaic Systems: A Comprehensive Review and Enhancement Prospects},
year={2021},
volume={9},
number={},
pages={126286-126306},
abstract={Photovoltaic (PV) systems are subject to failures during their operation due to the aging effects and external/environmental conditions. These faults may affect the different system components such as PV modules, connection lines, converters/inverters, which can lead to a decrease in the efficiency, performance, and further system collapse. Thus, a key factor to be taken into consideration in high-efficiency grid-connected PV systems is the fault detection and diagnosis (FDD). The performance of the FDD method depends mainly on the quality of the extracted features including real-time changes, phase changes, trend changes, and faulty modes. Thus, the data representation learning is the core stage of intelligent FDD techniques. Recently, due to the enhancement of computing capabilities, the increase of the big data use, and the development of effective algorithms, the deep learning (DL) tool has witnessed a great success in data science. Therefore, this paper proposes an extensive review on deep learning based FDD methods for PV systems. After a brief description of the DL-based strategies, techniques for diagnosing PV systems proposed in recent literature are overviewed and analyzed to point out their differences, advantages and limits. Future research directions towards the improvement of the performance of the DL-based FDD techniques are also discussed. This review paper aims to systematically present the development of DL-based FDD for PV systems and provide guidelines for future research in the field.},
keywords={Circuit faults;Feature extraction;Tools;Iron;Data models;Photovoltaic systems;Deep learning;Fault diagnosis;deep learning;photovoltaic systems},
doi={10.1109/ACCESS.2021.3110947},
ISSN={2169-3536},
month={},}
@ARTICLE{9775658,
author={Yang, Bin and Hu, Shunshi and Guo, Qiandong and Hong, Danfeng},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Multisource Domain Transfer Learning Based on Spectral Projections for Hyperspectral Image Classification},
year={2022},
volume={15},
number={},
pages={3730-3739},
abstract={Hyperspectral image classification is an important topic for hyperspectral remote sensing with various applications. Hyperspectral image classification accuracy. It has been greatly improved with the introduction of deep neural networks, while the idea of transfer learning provides an opportunity to solve the problem even with the lack of training samples. In this article, we propose an effective transfer learning approach for hyperspectral images, projecting hyperspectral images with different sensors and different number of bands into a general spectral space, preserving the relative positions of each band for spectral alignment, and designing a hierarchical depth neural network for shallow feature transfer and deep feature classification. The experiments show that the proposed method can effectively preserve the source domain features, especially for the scenarios with very few samples in the target domain, which can significantly improve the classification accuracy and reduce the risk of model overfitting. Meanwhile, this strategy greatly reduces the requirement of source domain data, using multisensor data to jointly train a more robust general feature model. The proposed method can achieve high accuracies even with few training samples compared to currently many state-of-the-art classification methods.},
keywords={Hyperspectral imaging;Feature extraction;Transfer learning;Sensors;Neural networks;Convolution;Image classification;Classification;convolutional neural network (CNN);generalized feature extraction network (GFEN);hyperspectral imaging;spectral projection;transfer learning},
doi={10.1109/JSTARS.2022.3173676},
ISSN={2151-1535},
month={},}
@ARTICLE{9291393,
author={Liu, Yang and Ma, Shuaifeng and Du, Xinxin},
journal={IEEE Access},
title={A Novel Effective Distance Measure and a Relevant Algorithm for Optimizing the Initial Cluster Centroids of K-means},
year={2020},
volume={},
number={},
pages={1-1},
abstract={The traditional K-means algorithm is very sensitive to the selection of the initial clustering point and the calculation of the distance measure, which is likely to result in the convergence of only partly optimal solutions. An improved k-means algorithm is proposed to solve the problem of unbalanced clustering effect caused by the fact that the first initial clustering centre falls in the non-dense region of the boundary in the initial clustering centre optimisation process. An improved k-means algorithm for initial clustering centres is proposed, namely, the optimal matching algorithm for K-means clustering, and related experimental analysis of the algorithm is carried out. The improved algorithm first selects the initial points of the traditional K-means clustering algorithm and analyses the clustering results. Then, the initial clustering centre selection and distance determination were tested and the clustering effect was evaluated by introducing the contour coefficient. Experiments on both artificial data sets and UCI data sets show that the algorithm can achieve better clustering results. The experimental results indicate that the improved algorithm has a much higher clustering quality than the traditional K-means algorithm and other improved algorithms.},
keywords={Clustering algorithms;Classification algorithms;Machine learning algorithms;Partitioning algorithms;Optimization;Euclidean distance;Data mining;K-means;local optimum;Initial center;UCI},
doi={10.1109/ACCESS.2020.3044069},
ISSN={2169-3536},
month={},}
@ARTICLE{6797876,
author={Yu, Yang and Chen, Jian and Lin, Shangquan and Wang, Ying},
journal={IEEE Transactions on Emerging Topics in Computing},
title={A Dynamic QoS-Aware Logistics Service Composition Algorithm Based on Social Network},
year={2014},
volume={2},
number={4},
pages={399-410},
abstract={The public logistics platform aims to provide customers with end-to-end logistics services by finding and composing a huge quantity of web services from logistics service providers. But, traditional service composition required predefined business process so that its flexibility is far from satisfactory in the problem. Path planning can be a solution of finding a suitable business path during service composition, but the search space will increase dramatically with the growth of service quantity and is hard to get a result within a tolerable interaction time. In the context of big data, to quickly build a service path with the optimal global QoS has become a problem demanding prompt solution. Sociologists point out that companies prefer familiar partners in the commercial environment. Using this principle, a concept of partner circle is defined, which can significantly reduce the search space in path planning. Combining path planning with service composition, a PartnerFirst algorithm is presented based on the social network, which is the cooperation network of service providers here. Simulation experiment shows that the PartnerFirst algorithm outperforms current approaches over 10 times in efficiency, with just about 10% loss in QoS. The relationship between efficiency and service quantity of the PartnerFirst algorithm is nearly linear. It proves that using social network in dynamic service composition is efficient and effective.},
keywords={Quality of service;Logistics;Algorithm design and analysis;Heuristic algorithms;Optimization;Urban areas;Social network services;Logistics path planning;service composition;social network;QoS;big data},
doi={10.1109/TETC.2014.2316524},
ISSN={2168-6750},
month={Dec},}
@ARTICLE{8793128,
author={Li, Chunling and Chen, Longyi and Feng, Jie and Wu, Duanpo and Wang, Zimeng and Liu, Junbiao and Xu, Weifeng},
journal={IEEE Access},
title={Prediction of Length of Stay on the Intensive Care Unit Based on Least Absolute Shrinkage and Selection Operator},
year={2019},
volume={7},
number={},
pages={110710-110721},
abstract={Length of stay (LoS) in the intensive care unit (ICU) is a common outcome measure used as an indicator of both quality of care and resource use. However, the existing analysis methods of LoS are poorly interpretable and extensible, and there is controversial for the predictive performance of LoS. In this paper, the study includes data from 1,214 unplanned ICU admissions to participate in the ICU of Sichuan Provincial People’s Hospital between Dec. 11, 2015 and Dec. 6, 2018. On the basis of these data, this study creates a highly accurate and predictive model using advanced preprocessing techniques, exploratory data analysis (EDA) and least absolute shrinkage and selection operator (LASSO) algorithm. Next, this study evaluates the predictive performance of the proposed model by 10-fold cross validation and external validation method using the root mean square prediction error (RMSPE), mean absolute error (MAE), and coefficient of determination ( $R^{2}$ ). The predictive performance of the proposed model is 0.88±0.13 day for RMSPE, 0.87±0.07 day for MAE and 0.35±0.09 for $R^{2}$ . Experimental results show that the performance of the proposed method are competitive with the state-of-the-art methods and results. Furthermore, this study explores the risk factors for ICU LoS in survivors and non-survivors and compare their predictive performance.},
keywords={Predictive models;Hospitals;Prediction algorithms;Red blood cells;Data analysis;Length of stay;intensive care unit;exploratory data analysis;least absolute shrinkage and selection operator},
doi={10.1109/ACCESS.2019.2934166},
ISSN={2169-3536},
month={},}
@ARTICLE{8390920,
author={Zhang, Pengcheng and Zhang, Liyuan and Yang, Jie and Gui, Zhiguo},
journal={IEEE Access},
title={The Aperture Shape Optimization Based on Fuzzy Enhancement},
year={2018},
volume={6},
number={},
pages={35979-35987},
abstract={The aperture shape optimization (ASO) is a critical step in the direct aperture optimization (DAO) method. During ASO, the gradient of objective function is calculated with respect to the beamlet weight. These gradient components are directly utilized to generate the new aperture shape. In this way, the beamlet of the large positive gradient value may be grouped into the generated aperture shape. The treatment quality may be deteriorated by adding this aperture into the treatment plan. In order to overcome this drawback, a novel method based on the fuzzy enhancement was proposed to generate the aperture shape. We apply the fuzzy enhancement method to enhance the gradient map that is composed of the gradients of objective function in a beam. The enhanced gradient map was then employed to form a network flow, which was solved to generate the new aperture shape. The optimal aperture shape was generated by removing the beamlet of the large positive gradient value from the new generated aperture shape. To verify the effectiveness, the proposed method was compared with the conventional column generation (CG) method on a prostate cancer case and on a head-and-neck cancer case. Experimental results demonstrate that the new algorithm has a better performance than the CG algorithm. The proposed method can further reduce the dose delivered to the critical structures, when the similar dose coverage is delivered on the targets.},
keywords={Apertures;Shape;Optimization;Linear programming;Shortest path problem;Sequential analysis;Search methods;Optimization;intensity modulated radiation therapy;fuzzy enhancement;aperture shape optimization},
doi={10.1109/ACCESS.2018.2849208},
ISSN={2169-3536},
month={},}
@ARTICLE{9328097,
author={Mokhtari, Ichrak and Bechkit, Walid and Rivano, Hervé and Yaici, Mouloud Riadh},
journal={IEEE Access},
title={Uncertainty-Aware Deep Learning Architectures for Highly Dynamic Air Quality Prediction},
year={2021},
volume={9},
number={},
pages={14765-14778},
abstract={Forecasting air pollution is considered as an essential key for early warning and control management of air pollution, especially in emergency situations, where big amounts of pollutants are quickly released in the air, causing considerable damages. Predicting pollution in such situations is particularly challenging due to the strong dynamic of the phenomenon and the various spatio-temporal factors affecting air pollution dispersion. In addition, providing uncertainty estimates of prediction makes the forecasting model more trustworthy, which helps decision-makers to take appropriate actions with more confidence regarding the pollution crisis. In this study, we propose a multi-point deep learning model based on convolutional long short term memory (ConvLSTM) for highly dynamic air quality forecasting. ConvLSTM architectures combines long short term memory (LSTM) and convolutional neural network (CNN), which allows to mine both temporal and spatial data features. In addition, uncertainty quantification methods were implemented on top of our model's architecture and their performances were further excavated. We conduct extensive experimental evaluations using a real and highly dynamic air pollution data set called Fusion Field Trial 2007 (FFT07). The results demonstrate the superiority of our proposed deep learning model in comparison to state-of-the-art methods including machine and deep learning techniques. Finally, we discuss the results of the uncertainty techniques and we derive insights.},
keywords={Atmospheric modeling;Predictive models;Air pollution;Forecasting;Vehicle dynamics;Uncertainty;Deep learning;Conv-LSTM;spatio-temporel prediction;highly dynamic air quality;accidental pollutant release;uncertainty;FFT-07;WSN},
doi={10.1109/ACCESS.2021.3052429},
ISSN={2169-3536},
month={},}
@ARTICLE{8506340,
author={Wang, Hongling and Liu, Gang},
journal={IEEE Access},
title={Two-Level-Oriented Selective Clustering Ensemble Based on Hybrid Multi-Modal Metrics},
year={2018},
volume={6},
number={},
pages={64159-64168},
abstract={The purpose of selective clustering ensemble is to select a subset of base clustering partitions with predictive performance and combine these partitions into more accurate and stable final results. Traditional approaches tend to utilize the well-known validity criteria such as NMI to evaluate the quality and diversity of base clustering partitions in the selection process. However, the characteristics of the original data and the data structure itself are commonly neglected. Furthermore, the generation process of base clustering partitions is more concerned with diversity and less consideration of quality. To tackle these problems, we propose a new selective clustering ensemble scheme. In the process of generating base clustering partitions, k-means and hierarchical clustering algorithm alternately combined with random projection method are employed to generate diverse base partitions. Meanwhile, in order to improve the quality of base clustering partitions, we propose a new selection strategy for the number of clusters k in k-means algorithm. In the clustering selection process, both diversity and quality of the base clustering partitions are evaluated by multi-modal metrics from two levels: clustering labels and data structure. Based on five UCI benchmark datasets, experimental results demonstrate that the proposed method not only can generate but also select base clustering partitions with both diversity and quality. Experimental analyses show the validity and stability of the proposed scheme.},
keywords={Diversity reception;Clustering algorithms;Partitioning algorithms;Measurement;Classification algorithms;Geology;Data structures;Diversity;multi-modal metrics;quality;selective clustering ensemble},
doi={10.1109/ACCESS.2018.2877666},
ISSN={2169-3536},
month={},}
@ARTICLE{8125076,
author={Li, Wenrui and Zhang, Pengcheng and Leung, Hareton and Ji, Shunhui},
journal={IEEE Access},
title={A Novel QoS Prediction Approach for Cloud Services Using Bayesian Network Model},
year={2018},
volume={6},
number={},
pages={1391-1406},
abstract={Cloud computing is the next generation computing model, which has a significant position in the field of scientific and business computing. By predicting cloud service's QoS in next period, it is helpful for end users to choose the most suitable cloud service that meets their needs. The underlying hardware/software resources of cloud architecture may have a certain influence on cloud service QoS. However, existing cloud service QoS prediction approaches do not take this influence into account. As these effects are real during the process of cloud service QoS prediction, ignoring the impact of these effects may create a big gap between the prediction results and the actual results. Therefore, in this paper interactive information is first used to describe the correlation between the hardware/software resources and the QoS attributes of the cloud service. Then, a Bayesian network model is established to predict cloud QoS. Bayesian network prediction reasoning algorithm is used to predict and reason about the future QoS values. A set of dedicated experiments is conducted to validate that our approach can accurately predict QoS of cloud service and the accuracy rate is better than state-of-the-art approaches.},
keywords={Cloud computing;Quality of service;Bayes methods;Cognition;Computational modeling;Predictive models;Time factors;Web service;quality of service;cloud service;Bayesian network model},
doi={10.1109/ACCESS.2017.2779045},
ISSN={2169-3536},
month={},}
@ARTICLE{9400395,
author={Tohidi, Faranak and Paul, Manoranjan and Hooshmandasl, Mohammad Reza},
journal={IEEE Access},
title={Detection and Recovery of Higher Tampered Images Using Novel Feature and Compression Strategy},
year={2021},
volume={9},
number={},
pages={57510-57528},
abstract={Due to the availability of powerful image-editing software and the growing amount of multimedia data that is transmitted via the Internet, integrity verifications and confidentiality of the data are becoming critical issues. However, currently, the accuracy of detecting and the recovery capability of the tampered images by the existing methods through watermarking strategy is still not at the required level, especially at a higher tampered rate. This paper proposes a new blind and fragile watermarking method to detect tampering and better recovery of tampered images. To improve the quality of both the watermarked and the recovered images, a new feature extraction scheme is introduced which will produce a short but comprehensive recovery code using a new compression strategy. If a block in the image tampers, the proposed embedded feature allows the original data to be extracted for recovery. To overcome tamper coincidence, every block’s watermarked data contains not only the recovery code belonging to the block itself but also its neighbor’s data as a second layer of recovery. Various size blocks were investigated to see the performance and compare their efficiency for recovering an image after different tampering rates. The test showed the smaller block sizes may be more suitable for locating tampering, where the bigger ones are more suitable when the tampering rate is higher. The bigger block sizes in the proposed method can recover an image even after a 60% tampering rate with high quality (more than 31 dB). The experimental results prove that the proposed method can have better efficiency for detecting tampering, and recovery of the original image, compared to the relevant existing methods.},
keywords={Watermarking;Image coding;Feature extraction;Authentication;Discrete cosine transforms;Data mining;Tampered image;image recovery;image authentication;feature extraction;watermarking;image compression},
doi={10.1109/ACCESS.2021.3072314},
ISSN={2169-3536},
month={},}
@ARTICLE{9815595,
author={Pradhan, Nilam M. and Chaudhari, Bharat S. and Zennaro, Marco},
journal={IEEE Access},
title={6TiSCH Low Latency Autonomous Scheduling for Industrial Internet of Things},
year={2022},
volume={10},
number={},
pages={71566-71575},
abstract={With the advancements in the Internet of Things (IoT), machine-to-machine communication, big-data, and the associated environment, a new model of the Industrial Internet of Things (IIoT) has emerged. The IIoT brings sensors, intelligent machines and tools, instruments, and analytics together for applications like manufacturing, robotics, and many others. Most of these applications require adaptive and autonomous behavior, Quality of Service (QoS), efficient resource allocation, and reservation. One of these crucial challenges is to build and maintain a data communication schedule. Time Slotted Channel Hopping (TSCH) based network operation promises required QoS for low-power applications and enables high reliability. 6TiSCH layer is being developed by standardizing the protocol stack to achieve industrial performance requirements by using IPv6 over IEEE802.15.4e TSCH MAC. 6TiSCH aims to manage the schedule and configure it with the topology and traffic requirements in the industrial environment. This paper proposes a novel low latency autonomous scheduling scheme for the 6TiSCH networks. It generates a segmented schedule for the network where all source nodes can send application data packets to the root node in a single slotframe. The performance of the proposed technique is compared with existing scheduling techniques. Our scheme outperforms the other techniques. The result shows that the latency is reduced up to 41% in comparison with the other scheduling schemes. The proposed scheme has a lower radio duty cycle as the node’s ON time is reduced, making it more energy efficient and reliable.},
keywords={Job shop scheduling;Schedules;Industrial Internet of Things;Reliability;IEEE 802.15 Standard;Quality of service;Standards;6TiSCH;autonomous scheduling;industrial IoT;latency;RPL;TSCH},
doi={10.1109/ACCESS.2022.3188862},
ISSN={2169-3536},
month={},}
@ARTICLE{9104693,
author={Zhao, Peng and Wang, Peizhe and Yang, Xinyu and Lin, Jie},
journal={IEEE Access},
title={Towards Cost-Efficient Edge Intelligent Computing With Elastic Deployment of Container-Based Microservices},
year={2020},
volume={8},
number={},
pages={102947-102957},
abstract={With the tremendous growth of the Internet of Things (IoT), big data, and artificial intelligence (AI), the edge computing-based service paradigm has been introduced to meet the increasing demand of applications. To provide efficient computing services at the network edge, the algorithms and applications are generally deployed based on the container-based microservice strategy, which significantly impacts the system efficiency and QoS. Considering the fundamental system uncertainties, including the dynamic workload and service rate, we investigate how to minimize the long-term system cost through the elastic microservice deployment in this paper. To this end, we formulate the container-based microservice deployment as a stochastic optimization problem to minimize the system cost while maintaining the system QoS and stability. We develop a cost-aware elastic microservice deployment algorithm to solve the formulated problem, which balances the tradeoff between system cost and QoS. Our algorithm makes the real-time decisions based on current queue backlogs and system states without predicting the future knowledge. Finally, we conduct the theoretical analysis and extensive simulations based on data traces from the ResNet-50 model-based visual recognition application. The results demonstrate that our algorithm outperforms the baseline strategies with respect to the system cost, queue backlogs, and the number of Pod replicas.},
keywords={Containers;Heuristic algorithms;Edge computing;Computational modeling;Virtualization;Artificial intelligence;Quality of service;Edge computing system;microservices;elastic deployment;system cost;container},
doi={10.1109/ACCESS.2020.2998767},
ISSN={2169-3536},
month={},}
@ARTICLE{9875288,
author={Farooq, Muhammad Shoaib and Javid, Rizwan and Riaz, Shamyla and Atal, Zabihullah},
journal={IEEE Access},
title={IoT Based Smart Greenhouse Framework and Control Strategies for Sustainable Agriculture},
year={2022},
volume={10},
number={},
pages={99394-99420},
abstract={In recent years, the Internet of Things (IoT) has become one of the most familiar names creating a benchmark and scaling new heights. IoT an indeed future of the communication that has transformed the objects (things) of the real world into smarter devices. With the advent of IoT technology, this decade is witnessing a transformation from traditional agriculture approaches to the most advanced ones. In perspective to the current standing of IoT in agriculture, identification of the most prominent application of IoT-based smart farming i.e. greenhouse has been highlighted and presented a systematic analysis and investigated the high quality research work for the implementation of greenhouse farming. The primary objective of this study is to propose an IoT-based network framework for a sustainable greenhouse environment and implement control strategies for efficient resources management. A rigorous discussion on IoT-based greenhouse applications, sensors/devices, and communication protocols have been presented. Furthermore, this research also presents an inclusive review of IoT-based greenhouse sensors/devices and communication protocols. Moreover, we have also presented a rigorous discussion on smart greenhouse farming challenges and security issues as well as identified future research directions to overcome these challenges. This research has explained many aspects of the technologies involved in IoT-based greenhouse and proposed network architecture, topology, and platforms. In the end, research results have been summarized by developing an IoT-based greenhouse farm management taxonomy and attacks taxonomy.},
keywords={Farming;Green products;Sensors;Monitoring;Wireless sensor networks;Taxonomy;Protocols;Security;Internet of Things;Communication protocols;Big Data;Data analysis;Cloud computing;Internet of Things (IoT);greenhouse;applications;sensors;communication protocols;cloud computing;big data analytics;security attacks},
doi={10.1109/ACCESS.2022.3204066},
ISSN={2169-3536},
month={},}
@ARTICLE{7349100,
author={Chen, Bo-Wei and Ji, Wen and Jiang, Feng and Rho, Seungmin},
journal={IEEE Access},
title={QoE-Enabled Big Video Streaming for Large-Scale Heterogeneous Clients and Networks in Smart Cities},
year={2016},
volume={4},
number={},
pages={97-107},
abstract={The rapid growth of the next-generation communication and networks is bringing video services into more pervasive environments. More and more users access and interact with video content using different devices, such as smart televisions, personal computers, tablets, smartphones, and wearable equipments. Providing heterogeneous Quality of Experience (QoE) that supports a wide variety of multimedia devices is critical to video broadcasting over the next-generation wireless network. This paper reviews practical video broadcasting technologies and examines current requirements ranging from heterogeneous devices to transmission technologies. Meanwhile, various coding methodologies, including QoE modeling, scalable compression efficiency, and flexible transmission, are also discussed. Moreover, this paper presents a typical paradigm as an example for video broadcasting with large-scale heterogeneity support, which enables QoE mapping, joint coding, flexible forward error coding, and cross-layer transmission, as well as optimal and dynamic adaptation to improve the overall receiving quality of heterogeneous devices. Finally, a brief summary of the key ideas and a discussion of interesting open areas are summarized at the end of this paper along with a future recommendation.},
keywords={Streaming media;Multimedia communication;Broadcasting;Quality of service;Encoding;Computer architecture;Next generation networking;Quality of service;video coding;broadcast technology;communications technology},
doi={10.1109/ACCESS.2015.2506648},
ISSN={2169-3536},
month={},}
@ARTICLE{8720195,
author={Yang, Yun and Cao, Lijuan and Liu, Qing and Yang, Po},
journal={IEEE Access},
title={A Stacked Multi-Granularity Convolution Denoising Auto-Encoder},
year={2019},
volume={7},
number={},
pages={83888-83899},
abstract={With the development of big data, artificial intelligence has provided many intelligent solutions to urban life. For instance, an image-based intelligent technology, such as image classification of diseases, is widely used in daily life. However, the image in real life is mostly unlabeled, so the performance of many image-based intelligent models shows limitations. Therefore, how to use a large amount of unlabeled image data to build an efficient and high-quality model for better urban life has been an urgent research topic. In this paper, we propose an unsupervised image feature extraction method that is referred to as a stacked multi-granularity convolution denoising auto-encoder (SMGCDAE). The algorithm is based on a convolutional neural network (CNN), yet it introduces a multi-granularity kernel. This approach resolved issues with image unicity by extracting a diverse category of high-level features. In addition, the denoising auto-encoder ensures stability and improves the classification accuracy by extracting more robust features. The algorithm was assessed using three image benchmark datasets and a series of meningitis images, achieving higher average accuracy than other methods. These results suggest that the algorithm is capable of extracting more discriminative high-level features and thus offers superior performance compared with the existing methodologies.},
keywords={Feature extraction;Convolution;Noise reduction;Kernel;Classification algorithms;Neural networks;Data mining;Unsupervised learning;feature extraction;denoising auto-encoder;convolutional neural network},
doi={10.1109/ACCESS.2019.2918409},
ISSN={2169-3536},
month={},}
@ARTICLE{8588980,
author={Ju, Mingye and Ding, Can and Guo, Y. Jay and Zhang, Dengyin},
journal={IEEE Access},
title={Remote Sensing Image Haze Removal Using Gamma-Correction-Based Dehazing Model},
year={2019},
volume={7},
number={},
pages={5250-5261},
abstract={Haze is evident in most remote sensing (RS) images, particularly for the RS scenes captured in inclement weather, which severely hinders image interpretation. In this paper, two simple yet effective visibility restoration formulas are proposed for RGB-channel RS (RRS) images and multi-spectral RS (MSRS) images, respectively. More specifically, a robust gamma-correction-based dehazing model (RGDM) is first defined, which can better address the non-uniform illumination problem in hazy images. Then, the scene albedo restoration formula (SARF) used for the RRS images is obtained by imposing the existing prior knowledge on this RGDM, which enables us to simultaneously eliminate the interferences of haze and non-uniform illumination. In subsequence, according to Rayleigh’s law, an expanded restoration formula (E-SARF) is further developed for MSRS data. Using the proposed E-SARF, the spatially varying haze in each band can be thoroughly removed without using any extra information. The experiments are conducted on the challenging RRS and MSRS images, including images with non-uniform illumination, non-uniform haze distribution, and heavy haze. The results reveal that the SARF and the E-SARF are superior to most other state-of-the-art techniques in terms of both the recover quality and the implementation efficiency.},
keywords={Atmospheric modeling;Image restoration;Image color analysis;Remote sensing;Scattering;Meteorology;Lighting;Heavy haze;image dehazing;implementation efficiency;non-uniform haze;non-uniform illumination;remote sensing},
doi={10.1109/ACCESS.2018.2889766},
ISSN={2169-3536},
month={},}
@ARTICLE{9200608,
author={Khaidem, Luckyson and Luca, Massimiliano and Yang, Fan and Anand, Ankit and Lepri, Bruno and Dong, Wen},
journal={IEEE Access},
title={Optimizing Transportation Dynamics at a City-Scale Using a Reinforcement Learning Framework},
year={2020},
volume={8},
number={},
pages={171528-171541},
abstract={Urban planners, authorities, and numerous additional players have to deal with challenges related to the rapid urbanization process and its effect on human mobility and transport dynamics. Hence, optimize transportation systems represents a unique occasion for municipalities. Indeed, the quality of transport is linked to economic growth, and by decreasing traffic congestion, the life quality of the inhabitants is drastically enhanced. Most state-of-the-art solutions optimize traffic in specific and small zones of cities (e.g., single intersections) and cannot be used to gather insights for an entire city. Moreover, evaluating such optimized policies in a realistic way that is convincing for policy-makers can be extremely expensive. In our work, we propose a reinforcement learning frameworks to overtake these two limitations. In particular, we use human mobility data to optimize the transport dynamics of three real-world cities (i.e., Berlin, Santiago de Chile, Dakar) and a synthesized one (i.e., SynthTown). To this end, we transform the transportation dynamics' simulator MATSim into a realistic reinforcement learning environment able to optimize and evaluate transportation policies using agents that perform realistic daily activities and trips. In this way, we can assess transportation policies in a manner that is convincing for policy-makers. Finally, we develop a model-based reinforcement learning algorithm that approximates MATSim dynamics with a Partially Observable Discrete Event Decision Process (PODEDP) and, with respect to other state-of-art policy optimization techniques, can scale on big transportation data and find optimal policies also on a city-scale.},
keywords={Learning (artificial intelligence);Urban areas;Heuristic algorithms;Optimization;Roads;Adaptation models;Transportation dynamics;human mobility data;reinforcement learning;partially observable discrete event decision process;MATSim},
doi={10.1109/ACCESS.2020.3024979},
ISSN={2169-3536},
month={},}
@ARTICLE{9422830,
author={Sun, Li and Liang, Kaibo and Song, Yanxing and Wang, Yuzhi},
journal={IEEE Access},
title={An Improved CNN-Based Apple Appearance Quality Classification Method With Small Samples},
year={2021},
volume={9},
number={},
pages={68054-68065},
abstract={Apple quality classification is an important means to refine apple sales market and promote apple sales. At present, most of classification methods based on a convolutional neural network (CNN) depend on the quantity of training samples to get good performance. But due to the lack of large-scale public apple appearance dataset, it is a big challenge to obtain high accuracy of apple appearance quality classification with small samples. Therefore, we propose an improved method based on CNN for apple appearance, quality classification with small samples. Firstly, support vector machine (SVM) is used for image segmentation to avoid the decrease of recognition accuracy caused by environmental noise. Secondly, the segmented image data are input into deep convolutional generative adversarial networks (DCGAN) model, which is used for data expansion. Thirdly, the improved ResNet50 (Imp-ResNet50) is proposed as follows: Replace the fully-connected layer with global average pooling layer; Add the dropout algorithm and batch normalization algorithm at the fully-connected layer; Replace the activation function ReLU with Swish. Through comparative experiments with 360 apple images, we verify the performance of the proposed method including the training image quality, the running time, and classification accuracy. The result shows that the proposed method can obtain high quality training samples and reduce the running time of the method effectively. At the same time, it can realize higher classification accuracy that is up to 96.5%, which is higher than the previous classification method.},
keywords={Support vector machines;Image segmentation;Data models;Feature extraction;Training;Mathematical model;Image recognition;Apple quality classification;SVM;DCGAN;ResNet50},
doi={10.1109/ACCESS.2021.3077567},
ISSN={2169-3536},
month={},}
@ARTICLE{7935490,
author={Liu, Xiaolong and Yuan, Shyan-Ming and Luo, Guo-Heng and Huang, Hao-Yu and Bellavista, Paolo},
journal={IEEE Access},
title={Cloud Resource Management With Turnaround Time Driven Auto-Scaling},
year={2017},
volume={5},
number={},
pages={9831-9841},
abstract={Cloud resource management research and techniques have received relevant attention in the last years. In particular, recently numerous studies have focused on determining the relationship between server-side system information and performance experience for reducing resource wastage. However, the genuine experiences of clients cannot be readily understood only by using the collected server-side information. In this paper, a cloud resource management framework with two novel turnaround time driven auto-scaling mechanisms is proposed for ensuring the stability of service performance. In the first mechanism, turnaround time monitors are deployed in the client-side instead of the more traditional server-side, and the information collected outside the server is used for driving a dynamic auto-scaling operation. In the second mechanism, a schedule-based auto scaling preconfiguration maker is designed to test and identify the amount of resources required in the cloud. The reported experimental results demonstrate that using our original framework for cloud resource management, stable service quality can be ensured and, moreover, a certain amount of quality variation can be handled in order to allow the stability of the service performance to be increased.},
keywords={Cloud computing;Resource management;Measurement;Monitoring;Servers;Time factors;Schedules;Network;resource management;big data;turnaround time;service management},
doi={10.1109/ACCESS.2017.2706019},
ISSN={2169-3536},
month={},}
@ARTICLE{8326701,
author={Liu, Xiao-Ying and Liang, Yong and Wang, Sai and Yang, Zi-Yi and Ye, Han-Shuo},
journal={IEEE Access},
title={A Hybrid Genetic Algorithm With Wrapper-Embedded Approaches for Feature Selection},
year={2018},
volume={6},
number={},
pages={22863-22874},
abstract={Feature selection is an important research area for big data analysis. In recent years, various feature selection approaches have been developed, which can be divided into four categories: filter, wrapper, embedded, and combined methods. In the combined category, many hybrid genetic approaches from evolutionary computations combine filter and wrapper measures of feature evaluation to implement a population-based global optimization with efficient local search. However, there are limitations to existing combined methods, such as the two-stage and inconsistent feature evaluation measures, difficulties in analyzing data with high feature interaction, and challenges in handling large-scale features and instances. Focusing on these three limitations, we proposed a hybrid genetic algorithm with wrapper-embedded feature approach for selection approach (HGAWE), which combines genetic algorithm (global search) with embedded regularization approaches (local search) together. We also proposed a novel chromosome representation (intron+exon) for global and local optimization procedures in HGAWE. Based on this “intron+exon” encoding, the regularization method can select the relevant features and construct the learning model simultaneously, and genetic operations aim to globally optimize the control parameters in the above non-convex regularization. We mention that any efficient regularization approach can serve as the embedded method in HGAWE, and a hybrid L1/2 + L2 regularization approach is investigated as an example in this paper. Empirical study of the HGAWE approach on some simulation data and five gene microarray data sets indicates that it outperforms the existing combined methods in terms of feature selection and classification accuracy.},
keywords={Feature extraction;Genetic algorithms;Optimization;Genetics;Focusing;Data models;Correlation;Feature selection;wrapper−embedded method;memetic framework;genetic algorithm;L½ + L₂ regularization},
doi={10.1109/ACCESS.2018.2818682},
ISSN={2169-3536},
month={},}
@ARTICLE{9262875,
author={Yao, Dunhong and Deng, Xiaowu},
journal={IEEE Access},
title={Teaching Teacher Recommendation Method Based on Fuzzy Clustering and Latent Factor Model},
year={2020},
volume={8},
number={},
pages={210868-210885},
abstract={Colleges and universities attach great importance to the quality of undergraduate teaching. To virtually guarantee the course's teaching quality, the key lies in recommending suitable teachers for the course scientifically. It is a seemingly simple but very complicated problem. Moreover, with the development of colleges and universities, new courses are continually set up, and new teachers are introduced, which further complicates the problem. The problem has not been solved well for many years. Therefore, we propose a course teacher recommendation model (FCTR-LFM) based on fuzzy clustering and the latent factor model (LFM) to solve this problem. Firstly, under the guidance of pedagogy theories and methods, we conduct quantitative modeling for teachers and courses' relevant characteristics and combine the quantitative results with historical teaching scores to establish a large-scale sparse course teaching evaluation matrix as the recommendation dataset. Next, we adopt the improved fuzzy clustering model to realize teachers' automatic clustering according to their characteristics and use the teacher cluster to reconstruct the teaching evaluation matrix, significantly reducing the dataset's size and reducing the sparsity. Then, we used the improved LFM to predict the score items in the evaluation matrix, including the missing score items. Finally, the prediction evaluation scores are sorted according to the course, and the TOP-N recommendation of the course teachers is realized. The experimental results show that FCTR-LFM can realize the prediction and recommendation well using the optimized parameters. It effectively solves the problem that there is no scientific basis for recommending suitable teachers for the course for a long time.},
keywords={Education;Matrix decomposition;Sparse matrices;Predictive models;Task analysis;Prediction algorithms;Biological system modeling;Teacher characteristics;course characteristics;sparse evaluation matrix;FCTR-LFM;TOP-N recommendation;teaching quality},
doi={10.1109/ACCESS.2020.3039011},
ISSN={2169-3536},
month={},}
@ARTICLE{9189762,
author={Shen, Limin and Pan, Maosheng and Liu, Linlin and You, Dianlong and Li, Feng and Chen, Zhen},
journal={IEEE Access},
title={Contexts Enhance Accuracy: On Modeling Context Aware Deep Factorization Machine for Web API QoS Prediction},
year={2020},
volume={8},
number={},
pages={165551-165569},
abstract={Service-oriented computing (SOC) promises a world of cooperating services loosely connected, constructing agile Web applications in heterogeneous environments conveniently. Web application interface (API) as an emerging technique attracts more and more enterprises and organizations to publish their deep computing functionalities and big data on the Internet, Web API has become the backbone to promote the development of SOC, thus forming the prosperous Web API economy. However, the number of available Web APIs on the Internet is massive and growing constantly, which causes the Web API overload problem. Quality of service (QoS) as an indicator is able to well differentiate the quality of Web APIs and has been widely applied for high quality Web API selection. Since testing QoS for massive Web APIs is resource-consuming, and the QoS performance depends on contextual information such as network and location, hence accurate QoS prediction has become very crucial for personalized Web API recommendation and high quality Web application construction. To address the above issue, this paper presents a context aware deep factorization machine model (CADFM for short) for accurate Web API QoS prediction. Specifically, we first carry out detailed data analysis using real-world QoS dataset and discover a positive relationship between QoS and contextual information, which motivates us to incorporate beneficial contexts for enhancing QoS prediction accuracy. Then, we treat QoS prediction as a regression problem and propose a context aware CADFM framework that integrates the contextual information via embedding technique. Particularly, we adopt MF and MLP for high-order and nonlinear interaction modeling, so as to learn the complex interaction between users and Web APIs accurately. Finally, the experimental results on real-world QoS dataset demonstrate that CADFM outperforms the classic and the state-of-the-art baselines, thereby generating the most accurate QoS predictions and increasing the revenue of Web APIs recommendation.},
keywords={Quality of service;Context modeling;Predictive models;Context-aware services;Internet;Organizations;Software;Service-oriented computing;Web API;quality of service prediction;context aware;deep factorization machine},
doi={10.1109/ACCESS.2020.3022891},
ISSN={2169-3536},
month={},}
@ARTICLE{8819913,
author={Zhang, Jiarui and Wang, Xuesong and Cheng, Yuhu},
journal={IEEE Access},
title={Broad Attribute Prediction Model With Enhanced Attribute and Feature},
year={2019},
volume={7},
number={},
pages={124606-124620},
abstract={For the zero-shot image classification without intersection between training and testing sets, the high-quality representation of image attributes and features plays a key role to improve the classification performance. In order to overcome the limitations related to insufficient attribute and feature expression in zero-shot image classification, we propose a broad attribute prediction model with enhanced attribute and feature (EAF-BAP) based on broad learning and elastic net constraint. Firstly, the EAF-BAP enhances pre-defined attributes by elastic net constraint to obtain hybrid attributes, which effectively improves the finiteness of semantic attributes. Secondly, the enhanced features are constructed by broad learning to increase the discrimination ability of features in different classes. Meanwhile, the broad learning is employed to train multiple attribute classifiers synchronously, which is more efficient compared to traditional support vector machines. Finally, the similarity between predicted attributes and hybrid attributes in testing classes is calculated by Manhattan distance, which is further used to implement image classification. Experiments on both AwA and Shoes datasets show that the proposed EAF-BAP model is capable of improving the accuracy of zero-shot image classification efficiently.},
keywords={Semantics;Predictive models;Training;Testing;Image reconstruction;Feature extraction;Learning systems;Zero-shot image classification;broad learning;elastic net constraint;enhanced feature;enhanced attribute},
doi={10.1109/ACCESS.2019.2938349},
ISSN={2169-3536},
month={},}
@ARTICLE{8733786,
author={Huo, Yongkai and Wang, Xu and Zhang, Peichang and Jiang, Jianmin and Hanzo, Lajos},
journal={IEEE Access},
title={Unequal Error Protection Aided Region of Interest Aware Wireless Panoramic Video},
year={2019},
volume={7},
number={},
pages={80262-80276},
abstract={Panoramic video with its flawless immersive tele-presence is considered to be the near-future video format of choice since they carry 360 degree coverage of the designated scenes. However, the viewers may focus their specific attention on perfectly lip-synchronized video as part of the panoramic video scene, hence only have a peripheral vision of the remaining parts of a frame. Therefore, it is intuitive to allocate stronger protection to the panoramic video region of interest. As a solution, we propose Region of Interest Aware Unequal Error Protection (ROI-UEP) for wireless transmission of high-efficiency video code (HEVC) sequences. Specifically, the ROI of a panoramic frame may be deemed to be within the 120° angular range of the viewing center, which can be estimated from the viewing trajectory of a head mounted display. Then, the most appropriate unequal forward error correction (FEC) coding rates will be found for the ROI signals by minimizing the expected video distortion. Moreover, the so-called weighted peak signal-to-noise ratio (WPSNR) is proposed for evaluating the quality of the reconstructed panoramic video, where the weights of pixels are taken into account for calculating the distortion caused by the related pixels. Our simulation results show that the ROI based equal error protection (ROI-EEP) scheme substantially outperforms the EEP by a WPSNR of more than 10 dB, while the ROI-UEP scheme further improves its ROI-EEP counterpart by a WPSNR of 9.4 dB at a channel Eb/N0 of 6 dB.},
keywords={Streaming media;Forward error correction;Wireless communication;Encoding;Distortion;Error correction codes;Trajectory;Panoramic video;wireless video;unequal error protection (UEP);equi-rectangular projection (ERP);region of interest (ROI)},
doi={10.1109/ACCESS.2019.2921880},
ISSN={2169-3536},
month={},}
@ARTICLE{9122518,
author={Zhou, Yuhe and Bai, Yun and Guo, Haiyang and Li, Tang and Qiu, Yu and Zhang, Zhao},
journal={IEEE Access},
title={Metro Scheduling to Minimize Travel Time and Operating Cost Considering Spatial and Temporal Constraints on Passenger Boarding},
year={2020},
volume={8},
number={},
pages={114190-114210},
abstract={Passengers on metro platforms can board a train only when the train has surplus capacity and the dwell time is sufficient, while the latter condition is omitted in previous studies. Taking into account the impacts of train capacity and dwell time on passengers boarding, this study develops a model on optimizing metro timetable to reduce passenger travel time and metro operating cost, through regulating trains' inter-station run-time, dwell time and headway. The NSGA-II algorithm is employed to obtain the near-optimal Pareto Frontier of the proposed model. To address insufficient dwell time scheduled in the timetable, three operating strategies are proposed and compared: a. sticking to nominal timetable; b. extending dwell time only; c. extending dwell time and recovering delay as soon as possible by compressing train inter-station run-time. Case studies on real-life metro line prove that some passengers cannot board the train during peak hours due to insufficient dwell time. In this context, strategy a brings low-quality service because passengers are stranded at platform even though the train has surplus capacity. In contrast, more passengers can board the train with strategies b and c because dwell time is extended for passengers' boarding when train has surplus capacity. Compared to strategy b, strategy c reduces the average in-vehicle time of passengers by 2.5% through compressing inter-station run-time to recover the delay. The timetable optimized based on strategy c saves total travel time of passengers by 3.1% without increasing operating cost when compared to the practical timetable.},
keywords={Optimization;Companies;Delays;Scheduling;Energy consumption;Stakeholders;Mathematical model;Public transportation;urban railway;train scheduling;heuristic algorithms;operating cost;passenger travel time},
doi={10.1109/ACCESS.2020.3004274},
ISSN={2169-3536},
month={},}
@ARTICLE{9079860,
author={Wang, Di and Liu, Chao and Li, Nan-Nan and Wang, Qiong-Hua},
journal={IEEE Access},
title={Holographic Zoom System With Large Focal Depth Based on Adjustable Lens},
year={2020},
volume={8},
number={},
pages={85784-85792},
abstract={In this paper, we propose a holographic zoom system based on two adjustable lenses. Different from the traditional holographic system, a digital conical lens and a liquid lens are used as the zoomable lenses. The liquid lens with large effective image aperture is designed and produced by a 3D printer. By mechanically controlling the curvature of the liquid-liquid surface, the focal length of the liquid lens can be changed easily. Compared with the other lenses, the conical lens has a larger focal depth. By encoding the phase information of the conical lens on the liquid crystal on silicon, the focal length and focal depth of the conical lens can be adjusted easily. The liquid lens and conical lens cooperate with each other so as to realize the high quality of holographic zoom projection. With such a system, the size and depth of the reconstructed image can be changed easily according to the requirement. Experimental results verify the feasibility of the proposed system.},
keywords={Lenses;Liquids;Image reconstruction;Holography;Holographic optical components;Optical imaging;Substrates;Liquid crystal on silicon;holographic projection;lens},
doi={10.1109/ACCESS.2020.2990992},
ISSN={2169-3536},
month={},}
@ARTICLE{9767708,
author={Wang, Pu and Cao, Di and Xia, Shaobo and Wang, Cheng},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={A Crown Guess and Selection Framework for Individual Tree Detection From ALS Point Clouds},
year={2022},
volume={15},
number={},
pages={3533-3538},
abstract={Individual tree detection from airborne laser scanning (ALS) point clouds is the basis for forestry inventory and further applications. In the past decade, many methods have been developed to localize tree instances in ALS point clouds. These methods rely on empirical rules and field measurements that may change from plot to plot. Besides, most existing methods cannot consider multiple clues (e.g., shape priors and neighboring trees) under the same framework, which makes them not flexible and extensible. In this letter, we devise a new point-based and model-driven framework named “crown guess and selection”. This framework first generates crown candidates automatically, and then the qualities of candidates and their neighboring information are both considered. Finally, expected crowns are selected from candidates simultaneously. The proposed framework is tested and evaluated in a benchmark dataset. We also compare the new framework with several existing methods, and it turns out that the proposed framework outperforms others in terms of model flexibility and detection accuracy.},
keywords={Vegetation;Point cloud compression;Forestry;Three-dimensional displays;Benchmark testing;Licenses;Laser modes;Airborne laser scanning (ALS) point clouds;model-driven;tree crown;tree instances;tree localization},
doi={10.1109/JSTARS.2022.3171771},
ISSN={2151-1535},
month={},}
@ARTICLE{9873922,
author={Mao, Ruihan and Li, Hua and Ren, Gaofeng and Yin, Zhangcai},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Cloud Removal Based on SAR-Optical Remote Sensing Data Fusion via a Two-Flow Network},
year={2022},
volume={15},
number={},
pages={7677-7686},
abstract={Optical remote sensing imagery plays an important role in observing the Earth's surface today. However, it is not easy to obtain complete multitemporal optical remote sensing images because of the cloud cover, how reconstructing cloud-free optical images has become a big challenge task in recent years. Inspired by the remote sensing fusion methods based on the convolutional neural network model, we propose a two-flow network to remove clouds from optical images. In the proposed method, synthetic aperture radar images are used as auxiliary data to guide optical image reconstruction, which is not influenced by cloud cover. In addition, a novel loss function called content loss is introduced to improve image quality. The ablation experiment of the loss function also proves that content loss is indeed effective. To be more in line with a real situation, the network is trained, tested, and validated on the SEN12MS-CR dataset, which is a global real cloud-removal dataset. The experimental results show that the proposed method is better than other state-of-the-art methods in many indicators (RMSE, SSIM, SAM, and PSNR).},
keywords={Remote sensing;Clouds;Optical imaging;Optical sensors;Convolutional neural networks;Radar polarimetry;Image reconstruction;Cloud removal;data fusion;deep learning;optical;remote sensing;synthetic aperture radar (SAR)},
doi={10.1109/JSTARS.2022.3203508},
ISSN={2151-1535},
month={},}
@ARTICLE{9095336,
author={Qu, Jiantao and Liu, Feng and Ma, Yuxiang and Fan, Jiaming},
journal={IEEE Access},
title={Temporal-Spatial Collaborative Prediction for LTE-R Communication Quality Based on Deep Learning},
year={2020},
volume={8},
number={},
pages={94817-94832},
abstract={In recent years, long term evolution for railway (LTE-R) has been a promising technology to meet the growing demand for railway wireless communication. To realize the active maintenance of LTE-R base station, it is of great significance to precisely predict the communication quality (CQ) of LTE-R base station. Given that the existing LTE CQ prediction methods can not support the active maintenance of LTE-R base station. Furthermore, the LTE-R base station has its unique characteristics in time relationship and regional impact, one of the most challenging problems is to effectively integrate the temporal and spatial information to improve the effect of CQ prediction. To solve the above problems, we choose daily evolved radio access bearer (E-RAB) abnormal release ratio as the CQ indicator, and propose a new deep learning-based CQ prediction approach for LTE-R. Considering the influence of adjacent base stations, this method conducts temporal-spatial collaborative prediction on multivariate time series collected from the CQ data of these stations. First, to eliminate the negative effect of redundant variables, a new variable filter method based on max-relevance, and min-redundancy (MRMR) criterion and binary particle swarm optimization (BPSO) is proposed to select a variable set from the CQ data of related base stations. Second, a new recurrent convolutional neural network (RCNN) model with a self-attention mechanism is proposed to extract temporal-spatial features from the selected variable set. With these features, we build a collaborative prediction model for CQ prediction. Experimental results on real-world LTE-R CQ datasets demonstrate the superiority of the proposed method in CQ prediction.},
keywords={Rail transportation;Base stations;Long Term Evolution;Feature extraction;Maintenance engineering;Time series analysis;Prediction methods;Time series forecasting;deep learning;LTE-R;communication quality prediction},
doi={10.1109/ACCESS.2020.2995478},
ISSN={2169-3536},
month={},}
@ARTICLE{9777962,
author={Ma, Xinmin and Chen, Zhenyu and Chen, Pan and Jin, Yuan and Wang, Yue and Yang, Liyun and Zhang, Zhaoran},
journal={IEEE Access},
title={Intelligent Quality Evaluation System for Vertical Shaft Blasting and its Application},
year={2022},
volume={10},
number={},
pages={61175-61191},
abstract={Blasting quality is a key factor in determining the productivity and total cost of the shaft blasting excavation construction, so it is of great engineering and theoretical importance to evaluate blasting quality rationally. The existing evaluation methods rely more on previous experience and the knowledge level of technicians, which are more subjective and cannot be judged by quantitative or unified standards, so the evaluation results have limitations. This paper proposes the Analytic Hierarchy Process (AHP) based on Particle Swarm Optimization (PSO) to obtain the weights of each index for evaluating the blasting quality of shafts, then combine expert knowledge, field engineering experience and statistical data for a comprehensive analysis to determine the quantitative interval of blasting quality evaluation index levels and construct a blasting quality evaluation index system, which makes the evaluation indexes more accurate and more in line with reality. The PSO-AHP combined with fuzzy comprehensive evaluation technique has constructed a blasting quality evaluation matrix more in line with the engineering reality and established a shaft blasting quality evaluation model adapted to different geological conditions. Finally, the established blasting quality evaluation model is combined with computer programming and artificial intelligence technology to develop a visualized shaft blasting quality intelligent evaluation system, which meets the practical needs of front-line operators in the field to evaluate the blasting quality objectively and reasonably, and achieves the accuracy, objectivity and intelligence of shaft blasting quality evaluation.},
keywords={Shafts;Indexes;Costs;Rocks;Excavation;Safety;Fuel processing industries;Particle swarm optimization (PSO);analytic hierarchy process (AHP);fuzzy mathematics;shaft blasting;result assessment},
doi={10.1109/ACCESS.2022.3176373},
ISSN={2169-3536},
month={},}
@ARTICLE{9254004,
author={Amani, Meisam and Brisco, Brian and Mahdavi, Sahel and Ghorbanian, Arsalan and Moghimi, Armin and DeLancey, Evan R. and Merchant, Michael and Jahncke, Raymond and Fedorchuk, Lee and Mui, Amy and Fisette, Thierry and Kakooei, Mohammad and Ahmadi, Seyed Ali and Leblon, Brigitte and LaRocque, Armand},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={Evaluation of the Landsat-Based Canadian Wetland Inventory Map Using Multiple Sources: Challenges of Large-Scale Wetland Classification Using Remote Sensing},
year={2021},
volume={14},
number={},
pages={32-52},
abstract={The first Canadian wetland inventory (CWI) map, which was based on Landsat data, was produced in 2019 using the Google Earth Engine (GEE) big data processing platform. The proposed GEE-based method to create the preliminary CWI map proved to be a cost, time, and computationally efficient approach. Although the initial effort to produce the CWI map was valuable with a 71% overall accuracy (OA), there were several inevitable limitations (e.g., low-quality samples for the training and validation of the map). Therefore, it was important to comprehensively investigate those limitations and develop effective solutions to improve the accuracy of the Landsat-based CWI (L-CWI) map. Over the past year, the L-CWI map was shared with several governmental, academic, environmental nonprofit, and industrial organizations. Subsequently, valuable feedback was received on the accuracy of this product by comparing it with various in situ data, photo-interpreted reference samples, land cover/land use maps, and high-resolution aerial images. It was generally observed that the accuracy of the L-CWI map was lower relative to the other available products. For example, the average OA in four Canadian provinces using in situ data was 60%. Moreover, including reliable in situ data, using an object-based classification method, and adding more optical and synthetic aperture radar datasets were identified as the main practical solutions to improve the CWI map in the future. Finally, limitations and solutions discussed in this study are applicable to any large-scale wetland mapping using remote sensing methods, especially to CWI generation using optical satellite data in GEE.},
keywords={Wetlands;Remote sensing;Earth;Monitoring;Artificial satellites;Biodiversity;Synthetic aperture radar;Big data;Canada;Google Earth Engine;Landsat;remote sensing (RS);wetlands},
doi={10.1109/JSTARS.2020.3036802},
ISSN={2151-1535},
month={},}
@ARTICLE{8626088,
author={Fu, Yonggui and Zhu, Jianming},
journal={IEEE Access},
title={Big Production Enterprise Supply Chain Endogenous Risk Management Based on Blockchain},
year={2019},
volume={7},
number={},
pages={15310-15319},
abstract={In view of the influence of information's “incompleteness” and “asymmetry” to supply chain operation efficiency, we make big production enterprise as the object and apply blockchain to its supply chain endogenous risk management, to research the specific operation mechanism and application value. In the operation process of big production enterprise supply chain, because of the information's asymmetry, the fraud problem will produce among the business subjects; blockchain is a decentralized distributed accounting and data storage technology, and with blockchain technology, we can resolve the business subjects' fraud problem and can provide more accurate decision information basis for each business section, and realize group decision. This paper has described the system structure and intelligent contract operation mechanism under consensus authentication of blockchain applying in big production enterprise supply chain and analyzed by the case. In view of the limitation of classical blockchain technology applying in big production enterprise supply chain, we constructed the corresponding blockchain data storage mechanism and data access mechanism. Analyzed the economic value of this paper researching from the aspects of response speed, supply accuracy, cooperation integrity, business interaction economic cost, supply quality, and supply price. This paper research will provide ideas and model structure for developing supply chain area's blockchain system and will promote the application research development of blockchain in specific area.},
keywords={Supply chains;Blockchain;Risk management;Authentication;Contracts;Blockchain;big production enterprise;supply chain;endogenous risk management},
doi={10.1109/ACCESS.2019.2895327},
ISSN={2169-3536},
month={},}
@ARTICLE{8538878,
author={Chen, Ju and Yang, Dianxing and Cao, Yue and Ma, Yiyi and Wen, Chuanbiao and Huang, Xiwei and Guo, Jinhong},
journal={IEEE Access},
title={Syndrome Differentiation and Treatment Algorithm Model in Traditional Chinese Medicine Based on Disease Cause, Location, Characteristics and Conditions},
year={2018},
volume={6},
number={},
pages={71801-71813},
abstract={Traditional Chinese medicine (TCM) is based on a unique disease diagnosis and treatment system that has been developed over the last 2,300 years. In the TCM, “syndrome differentiation and treatment”(SDAT) is a core method for doctors to deal with diseases. This diagnostic and therapeutic technique that infer the occurrence and the development of diseases by observing symptoms as a whole, not only has its own uniqueness but also has been recognized by the public in oriented medical fields for its clinical efficacy. With recent developments in computer science, the Internet, big data, and artificial intelligence, a study based on the SDAT algorithm has aroused much attention. This paper encompasses three stages spanning 30 years to accomplish the following: 1) the TCM data and the modern SDAT system were collated and summarized based on 35,706 reference data on the TCM, starting from the syndrome differentiation of four aspects, such as the cause, location, characteristics, and conditions of the disease (CLCC), we constructed a quantitative model of the TCM SDAT regarding the CLCC of the disease, collected the symptom information on the diagnosed subject, and transferred them to the SDAT assistant algorithm for calculation and analysis, to determine the CLCC, Based on the therapy recommended by the differentiation results in the knowledge base and the prescription and traditional Chinese medicines recommended by the therapy, any stage of all diseases could determine a syndrome type by differentiating the CLCC, we constructed the basic SDAT algorithm integrating theory, method, prescription, and medicine and realized the calculability in the TCM diagnosis and treatment process; 2) based on the SDAT algorithm, we developed the TCM doctor's workstation software and introduced it to more than 80 TCM institutions in Sichuan province, China, we collated a large-scale trove of samples of the TCM data platform that was established with more than 2.9 million TCM electronic medical records (EMRs) and reference data, and had the compliance tested and algorithm verified on the 9,300 EMRs of the common diseases in the TCM; and 3) based on the dimension reduction and degree elevation optimization of the technology with a directed graph to the basic algorithm, the algorithm complexity was reduced and the accuracy of the algorithm was improved. It was demonstrated that the coincidence rate of the basic model was 80.47% and the basic coincidence rate was 96.19%. After optimizing the basic algorithm (for example, for gastric abscess), the coincidence rate increased by 7.04%. The test results demonstrated the efficacy of the model study. This model realized a computable SDAT to specify and assist in the differentiation diagnosis and in the treatment processes of the TCM and improve the service quality of the TCM diagnosis and treatment.},
keywords={Diseases;Water heating;Medical diagnostic imaging;Knowledge based systems;Electronic mail;Software algorithms;Cause;location;characteristics;conditions of diseases;syndrome differentiation and treatment;algorithm;traditional Chinese medicine},
doi={10.1109/ACCESS.2018.2881535},
ISSN={2169-3536},
month={},}
@ARTICLE{9144301,
author={Chowdhury, Mostafa Zaman and Shahjalal, Md. and Ahmed, Shakil and Jang, Yeong Min},
journal={IEEE Open Journal of the Communications Society},
title={6G Wireless Communication Systems: Applications, Requirements, Technologies, Challenges, and Research Directions},
year={2020},
volume={1},
number={},
pages={957-975},
abstract={The demand for wireless connectivity has grown exponentially over the last few decades. Fifth-generation (5G) communications, with far more features than fourth-generation communications, will soon be deployed worldwide. A new paradigm of wireless communication, the sixth-generation (6G) system, with the full support of artificial intelligence, is expected to be implemented between 2027 and 2030. Beyond 5G, some fundamental issues that need to be addressed are higher system capacity, higher data rate, lower latency, higher security, and improved quality of service (QoS) compared to the 5G system. This paper presents the vision of future 6G wireless communication and its network architecture. This article describes emerging technologies such as artificial intelligence, terahertz communications, wireless optical technology, free-space optical network, blockchain, three-dimensional networking, quantum communications, unmanned aerial vehicles, cell-free communications, integration of wireless information and energy transfer, integrated sensing and communication, integrated access-backhaul networks, dynamic network slicing, holographic beamforming, backscatter communication, intelligent reflecting surface, proactive caching, and big data analytics that can assist the 6G architecture development in guaranteeing the QoS. Besides, expected applications with 6G communication requirements and possible technologies are presented. We also describe potential challenges and research directions for achieving this goal.},
keywords={5G mobile communication;Wireless communication;Artificial intelligence;Quality of service;Market research;Sensors;5G;6G;artificial intelligence;automation;beyond 5G;data rate;massive connectivity;virtual reality;terahertz},
doi={10.1109/OJCOMS.2020.3010270},
ISSN={2644-125X},
month={},}
@ARTICLE{8979339,
author={Chen, Liangfeng and Zhang, Shutao and Tan, Haibo and Lv, Bo},
journal={IEEE Access},
title={Progressive RSS Data Augmenter With Conditional Adversarial Networks},
year={2020},
volume={8},
number={},
pages={26975-26983},
abstract={Accuracies of most fingerprinting approaches for WiFi-based indoor localization applications are affected by the qualities of fingerprint databases, which are time-consuming and labor-intensive. Recently, many methods have been proposed to reduce the localization accuracy reliance on the qualities of the established fingerprint databases. However, studies on establishing fingerprint databases are relatively rare under the condition of sparse reference points. In this paper, we propose a novel data augmenter based on the adversarial networks to build fingerprint databases with sparse reference points. Additionally, two conditions of these networks are designed to generate data effectively and stably, which are 0-1 sketch and Gaussian sketch. Based on the networks, we design two augmenters with different cyclic training strategies to evaluate the augmenting effects comparatively. Meanwhile, five quantitative evaluation metrics of the augmenters are proposed from two perspectives of the artificial experiences and the data features, and some of them are also used as the gradient penalties for generators. Finally, experiments corresponding to these metrics and localization accuracies demonstrate that the data augmenter with the 0-1 sketch adversarial network is more efficient, effective and stable totally.},
keywords={Databases;Training;Generators;Measurement;Neural networks;Generative adversarial networks;Gallium nitride;CGAN;sketch;quantitative evaluation metrics;RSS},
doi={10.1109/ACCESS.2020.2971269},
ISSN={2169-3536},
month={},}
@ARTICLE{9430113,
author={Xiaolong, Xu and Wen, Chen and Xinheng, Wang},
journal={Journal of Systems Engineering and Electronics},
title={RFC: A feature selection algorithm for software defect prediction},
year={2021},
volume={32},
number={2},
pages={389-398},
abstract={Software defect prediction (SDP) is used to perform the statistical analysis of historical defect data to find out the distribution rule of historical defects, so as to effectively predictdefects in the new software. However, there are redundant and irrelevant features in the software defect datasets affecting the performance of defect predictors. In order to identify and remove the redundant and irrelevant features in software defectdatasets, we propose Relief F-based clustering (RFC), a cluster-based feature selection algorithm. Then, the correlation between features is calculated based on the symmetric uncertainty. According to the correlation degree, RFC partitions features into kclusters based on the k-medoids algorithm, and finally selects the representative features from each cluster to form the final feature subset. In the experiments, we compare the proposed RFC with classical feature selection algorithms on nine National Aeronautics and Space Administration (NASA) software defectprediction datasets in terms of area under curve (AUC) and F-value. The experimental results show that RFC can effectively improve the performance of SDP.},
keywords={Software;Software algorithms;Prediction algorithms;Feature extraction;Partitioning algorithms;Correlation;Clustering algorithms;software defect prediction (SDP);feature selection;cluster},
doi={10.23919/JSEE.2021.000032},
ISSN={1004-4132},
month={April},}
@ARTICLE{8675275,
author={Kuru, Kaya and Yetgin, Halil},
journal={IEEE Access},
title={Transformation to Advanced Mechatronics Systems Within New Industrial Revolution: A Novel Framework in Automation of Everything (AoE)},
year={2019},
volume={7},
number={},
pages={41395-41415},
abstract={The recent advances in cyber-physical domains, cloud, cloudlet, and edge platforms along with the evolving Artificial Intelligence (AI) techniques, big data analytics, and cutting-edge wireless communication technologies within the Industry 4.0 (4IR) are urging mechatronics designers, practitioners, and educators to further review the ways in which mechatronics systems are perceived, designed, manufactured, and advanced. Within this scope, we introduce the service-oriented cyber-physical advanced mechatronics systems (AMSs) along with current and future challenges. The objective in AMSs is to create remarkably intelligent autonomous products by 1) forging effective sensing, self-learning, Wisdom as a Service (WaaS), Information as a Service (InaaS), precise decision making, and actuation using effective location-independent monitoring, control and management techniques with products and 2) maintaining a competitive edge through better product performances via immediate and continuous learning, while the products are being used by customers and are being produced in factories within the cycle of Automation of Everything (AoE). With the advanced wireless communication techniques and improved battery technologies, the AMSs are capable of getting independent and working with other massive AMSs to construct robust, customizable, energy-efficient, autonomous, intelligent, and immersive platforms. In this regard, rather than providing technological details, this paper implements philosophical insights into 1) how mechatronics systems are being transformed into AMSs; 2) how robust AMSs can be developed by both exploiting the wisdom created within cyber-physical smart domains in the edge and cloud platforms and incorporating all the stakeholders with diverse objectives into all phases of the product life-cycle; and 3) what essential common features AMSs should acquire to increase the efficacy of products and prolong their product life. Against this background, an AMS development framework is proposed in order to contextualize all the necessary phases of AMS development and direct all stakeholders to rivet high-quality products and services within AoE.},
keywords={Mechatronics;Automation;Industries;Production;Cloud computing;Wireless communication;Sensors;Advanced mechatronics systems;Wisdom as a Service (WaaS);Information as a Service (InaaS);Industry 4.0 (4IR);cyber-physical domains;cloud and edge/fog platforms;Automation of Everything (AoE)},
doi={10.1109/ACCESS.2019.2907809},
ISSN={2169-3536},
month={},}
@ARTICLE{9254003,
author={Wen, Yuqiang and Chen, Yuqiang and Shao, Meng-Liang and Guo, Jian-Lan and Liu, Jia},
journal={IEEE Access},
title={An Efficient Content Distribution Network Architecture Using Heterogeneous Channels},
year={2020},
volume={8},
number={},
pages={210988-211006},
abstract={With the popularization and development of the IoT(Internet of Things), more and more data needs to be transmitted over the Internet, which leads to the deterioration of network quality. The CDN (Content Distribution Network) technology is an important theoretical model to ensure network QoS (Quality of Service). To improve the QoS, we extend current CDN system to the hierarchical CDN system, that traditional CDN system is just a special case of hierarchical CDN system. In the traditional CDN system and the hierarchical CDN system, by analyzing the bandwidth between subsystems, we found the inter-system bandwidth is the bottleneck that impedes CDN system expansion. To address this problem, a new kind of distributed system architecture is proposed in this paper. This new architecture uses broadcast channels to distribute broadcast type data and still using bidirection uni-cast channels for other type of data, so we call the new architecture as CHCDN (Channel Heterogeneous CDN) in this paper. The new architecture is analyzed and compared with the traditional CDN system architecture and hierarchical CDN system architecture. Moreover, the experimental simulation result has shown that the CHCDN system features better in real-time cache updating and features with higher data transmission efficiency than the hierarchical CDN system, which indicating that the new architecture has great potential for being widely used in distributed computing.},
keywords={Content distribution networks;Computational modeling;Systems architecture;Computer architecture;Quality of service;Bandwidth;Real-time systems;Real time CDN cache update;Internet of Things (IoT);channel heterogeneous CDN (CHCDN);network QoS;distributed computing},
doi={10.1109/ACCESS.2020.3037164},
ISSN={2169-3536},
month={},}
@ARTICLE{9481217,
author={Baviskar, Dipali and Ahirrao, Swati and Kotecha, Ketan},
journal={IEEE Access},
title={Multi-Layout Unstructured Invoice Documents Dataset: A Dataset for Template-Free Invoice Processing and Its Evaluation Using AI Approaches},
year={2021},
volume={9},
number={},
pages={101494-101512},
abstract={The daily transaction of an organization generates a vast amount of unstructured data such as invoices and purchase orders. Managing and analyzing unstructured data is a costly affair for the organization. Unstructured data has a wealth of hidden valuable information. Extracting such insights automatically from unstructured documents can significantly increase the productivity of an organization. Thus, there is a huge demand to develop a tool that can automate the extraction of key fields from unstructured documents. Researchers have used different approaches for extracting key fields, but the lack of annotated and high-quality datasets is the biggest challenge. Existing work in this area has used standard and custom datasets for extracting key fields from unstructured documents. Still, the existing datasets face some serious challenges, such as poor-quality images, domain-related datasets, and a lack of data validation approaches to evaluate data quality. This work highlights the detailed process flow for end-to-end key fields extraction from unstructured documents. This work presents a high-quality, multi-layout unstructured invoice documents dataset assessed with a statistical data validation technique. The proposed multi-layout unstructured invoice documents dataset is highly diverse in invoice layouts to generalize key field extraction tasks for unstructured documents. The proposed multi-layout unstructured invoice documents dataset is evaluated with various feature extraction techniques such as Glove, Word2Vec, FastText, and AI approaches such as BiLSTM and BiLSTM-CRF. We also present the comparative analysis of feature extraction techniques and AI approaches on the proposed multi-layout unstructured invoice document dataset. We attained the best results with BiLSTM-CRF model.},
keywords={Task analysis;Layout;Organizations;Artificial intelligence;Data mining;Standards organizations;Text analysis;Artificial Intelligence (AI);information extraction;key field extraction;named entity recognition (NER);template-free invoice processing;unstructured data},
doi={10.1109/ACCESS.2021.3096739},
ISSN={2169-3536},
month={},}
@ARTICLE{6894550,
author={Xia, Feng and Chen, Zhen and Wang, Wei and Li, Jing and Yang, Laurence T.},
journal={IEEE Transactions on Emerging Topics in Computing},
title={MVCWalker: Random Walk-Based Most Valuable Collaborators Recommendation Exploiting Academic Factors},
year={2014},
volume={2},
number={3},
pages={364-375},
abstract={In academia, scientific research achievements would be inconceivable without academic collaboration and cooperation among researchers. Previous studies have discovered that productive scholars tend to be more collaborative. However, it is often difficult and time-consuming for researchers to find the most valuable collaborators (MVCs) from a large volume of big scholarly data. In this paper, we present MVCWalker, an innovative method that stands on the shoulders of random walk with restart (RWR) for recommending collaborators to scholars. Three academic factors, i.e., coauthor order, latest collaboration time, and times of collaboration, are exploited to define link importance in academic social networks for the sake of recommendation quality. We conducted extensive experiments on DBLP data set in order to compare MVCWalker to the basic model of RWR and the common neighbor-based model friend of friends in various aspects, including, e.g., the impact of critical parameters and academic factors. Our experimental results show that incorporating the above factors into random walk model can improve the precision, recall rate, and coverage rate of academic collaboration recommendations.},
keywords={Collaboration;Social network services;Vectors;Educational institutions;Recommender systems;Context;Data models;Most valuable collaborators;academic recommendation;big scholarly data;random walk;link prediction},
doi={10.1109/TETC.2014.2356505},
ISSN={2168-6750},
month={Sep.},}
@ARTICLE{8662571,
author={Liu, Shubo and Guo, Liqing and Webb, Heather and Ya, Xiao and Chang, Xiao},
journal={IEEE Access},
title={Internet of Things Monitoring System of Modern Eco-Agriculture Based on Cloud Computing},
year={2019},
volume={7},
number={},
pages={37050-37058},
abstract={In order to enhance the efficiency and safety of production and management of modern agriculture in China, problems, such as the quality and safety of agricultural products and the pollution of the environment from agricultural activities, should be unraveled. Based on the new generation of information technology (IT), an integrated framework system platform incorporating the Internet of Things (IoT), cloud computing, data mining, and other technologies is investigated and a new proposal for its application in the field of modern agriculture is offered. The experimental framework and simulation design suggest that the basic functions of the monitoring system of the IoT for agriculture can be realized. In addition, the innovation derived from integrating different technologies plays an important role in reducing the cost of system development and ensuring its reliability as well as security.},
keywords={Cloud computing;Internet of Things;Logic gates;Monitoring;Production;Agricultural products;Management system;modern agriculture;big data;cloud computing;Internet of Things (IoT)},
doi={10.1109/ACCESS.2019.2903720},
ISSN={2169-3536},
month={},}
@ARTICLE{9145574,
author={Shi, Lin and Xu, Shoukun},
journal={IEEE Access},
title={UAV Path Planning With QoS Constraint in Device-to-Device 5G Networks Using Particle Swarm Optimization},
year={2020},
volume={8},
number={},
pages={137884-137896},
abstract={Unmanned Ariel Vehicles (UAVs) are tasked to collect sensory data which are typically retrieved after the flight. The emergence of 5G and Device-to-Device (D2D) networks enables high speed network communication for UAVs to transfer data during a flight mission instead of post flight. UAVs are now subject to constraints of area coverage, battery capacity and network quality of service, making their path planning more challenging. In this paper, we formulate the issue as a combinatorial optimization problem which minimizes the flight cost of multiple UAVs covering the entire area. We show this problem is NP-hard, therefore we propose a Particle Swarm Optimization heuristic along with path encoding and local search techniques to solve the problem. Our numerical simulations demonstrate the effectiveness of the approach and how the size of the area and D2D link affect the number of UAVs needed and their flight time.},
keywords={5G mobile communication;Device-to-device communication;Quality of service;Path planning;Base stations;Unmanned aerial vehicles;Particle swarm optimization;UAV;5G networks;QoS;coverage path planning;device-to-device communication;particle swarm optimization},
doi={10.1109/ACCESS.2020.3010281},
ISSN={2169-3536},
month={},}
@ARTICLE{8794523,
author={Wang, Yubiao and Wen, Junhao and Zhou, Wei and Tao, Bamei and Wu, Quanwang and Tao, Zhiyong},
journal={IEEE Access},
title={A Cloud Service Selection Method Based on Trust and User Preference Clustering},
year={2019},
volume={7},
number={},
pages={110279-110292},
abstract={In order to achieve universal and personalized cloud service choices in a social network environment, we propose a cloud service selection method based on trust and user preference clustering. The method performs a comprehensive trust evaluation, which is used to evaluate and select cloud services. Meanwhile, we propose an improved condensed hierarchical clustering method based on user preference similarity to further improve the accuracy of recommendation trust. A cloud model-based approach is used to measure similarities between users, and then a hierarchical clustering method is used to divide users into different domains according to user similarity. The final recommendation trust will be obtained, which includes the intra-domain recommendation trust and the extra-domain recommendation trust. The comprehensive trust of cloud services, which consists of direct trust and recommended trust. Simulation experiments verify the accuracy and superiority of the clustering algorithm. Experimental results show that the cloud service selection method improves the transaction success rate and enables users to select more satisfactory cloud services.},
keywords={Cloud computing;Peer-to-peer computing;Entropy;Quality of service;Clustering algorithms;Computational modeling;Face;Cloud service;selection method;user preference clustering;cloud model;trust},
doi={10.1109/ACCESS.2019.2934153},
ISSN={2169-3536},
month={},}
@ARTICLE{9131799,
author={Wang, Chia-Hung and Lee, Chia-Jung and Wu, Xiaojing},
journal={IEEE Access},
title={A Coverage-Based Location Approach and Performance Evaluation for the Deployment of 5G Base Stations},
year={2020},
volume={8},
number={},
pages={123320-123333},
abstract={It has become a strategic consensus of the international community for accelerating the deployment of 5G network. This paper presents an approach for the deployment of 5G base stations under the considerations of both the cost and the signal coverage. We formulate an optimization problem for the site selection and location of 5G macro and micro base stations. An implementation procedure is proposed in the paper for the cooperative operation and deployment scheme of optimizing the location of 5G heterogeneous base stations, which aims to optimally reduce the setup cost and strengthen the signal coverage while deploying 5G base stations. A series of numerical examples are solved in the paper to demonstrate the proposed approach, and a cost-benefit analysis is also conducted to determine the optimal deployment plan for the number of macro and micro base stations. In the conclusion, a balanced executable solution is presented to make the signal strength of all demand points in the studied 5G network reach the strongest under the budget constraint.},
keywords={5G mobile communication;Base stations;Optimization;Investment;Quality of service;Performance evaluation;5G network optimization;computational intelligence;cost benefit analysis;heterogeneous network;algorithm design and analysis;decision making},
doi={10.1109/ACCESS.2020.3006733},
ISSN={2169-3536},
month={},}